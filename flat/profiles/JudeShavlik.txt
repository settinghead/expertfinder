curriculum vitae jude william shavlik professor departments computer sciences biostatistics medical informatics wisconsin madison addresses computer sciences department hillington wisconsin madison west dayton street madison shavlik wisc wisc shavlik academic degrees doctor philosophy computer science illinois master science molecular biophysics biochemistry yale bachelor science electrical engineering massachusetts institute technology bachelor science biology massachusetts institute technology professional experience -present professor computer sciences department biostatistics medical informatics department wisconsin madison member wisconsin center genomics wisconsin comprehensive cancer center associate professor computer sciences department wisconsin madison visiting associate professor institute advanced computer studies computer sciences department maryland college park half-time visitor national library medicine bethesda profesor visitante universidad nacional san luis argentina december assistant professor computer sciences department wisconsin madison research assistant artificial intelligence research group coordinated science laboratory illinois teaching assistant departments physics computer science fall electrical computer engineering illinois member technical staff mitre corporation bedford massachusetts pasadena california summer national institutes health trainee yale summer research associate advanced technology division united states department transportation washington research interests artificial intelligence intelligent agents machine learning adaptive information retrieval computational biology computer security professional societies international society machine learning american association artificial intelligence international society computational biology association computing machinery primary grants awards machine learning improved mammography screening national cancer institute joint beth burnside uw-radiology co-pis integrated learning darpa subcontract bbn goal-oriented privacy preservation national science foundation joint david dewitt uw-computer sciences co-pis machine learning visualization structural biology national library medicine joint co-pi george phillips biochemistry interactive learning advice reinforcements broadening communication channel machine learners human teachers defense advanced research programs agency joint co-pi raymond mooney ut-austin jan knowledge-intensive interactive efficient relational pattern learning air force joint david page raghu ramakrishnan uw-madison developing advice module porting robocup simulator tielt environment naval research laboratory rich maclin visiting professor uw-madison co-pi knowledge-intensive interactive efficient relational pattern learning defense advanced research programs agency joint mark craven david page uw-madison raymond mooney ut-austin development maskless array synthesizer national institutes health sbir program subcontractor nimblegen support summer month year ras adaptive information monitoring extraction national library medicine joint mark craven uw-madison selection combination evaluation effective software sensors detecting abnormal usage computers running windows defense advanced research programs agency joint mark shavlik ceo shavlik technologies vilas associate wisconsin vilas trust months summer support learning instruction experience national science foundation providing verbal advice connectionist learners office naval research simplification association rule sets extracted database mining international business machines corporation san jose neural networks automatically refine expert system knowledge bases nynex corporation applying machine learning techniques dna sequence analysis department energy subcontracted noordewier rutgers knowledge-based neural networks learn dynamic environments office naval research integrating explanation-based neural approaches machine learning national science foundation combining explanation-based neural approaches machine learning office naval research development computer-configuration system learns international business machines corporation rochester grants computation informatics medicine national library medicine training grant initially funded approximately annually renewed fall years rate co-program director comprehensive cancer center support national cancer institute co-supervisor cancer informatics portion graduate student researchers program national aeronautics space administration support cherkauer support intl conf machine learning daimler-benz research technology office naval research microsoft research research nec research support intl conf intelligent systems molecular biology national library medicine american association artificial intelligence aaai biomatrix society equipment support algorithm visualization foundation ioannidis midship managing image datasets scalable high performance national science foundation infrastructure grant investigator determining dna sequence coli national institutes health participant blattner department genetics support distinguished lecture series machine learning international business machines corporation rochester theses supervised empirical study machine learning algorithms applied modeling player behavior person shooter video game benjamin geisler masters initially raven software madison building intelligent agents learn retrieve extract information tina eliassi-rad lawrence livermore laboratories computational methods fast accurate dna fragment assembly carolyn allex dna madison extracting comprehensible models trained neural networks mark craven associate professor biostatistics medical informatics computer sciences wisconsin madison learning instruction experience methods incorporating procedural domain theories knowledge-based neural networks richard maclin professor computer science minnesota duluth anytime approach connectionist theory refinement refining topologies knowledge-based neural networks david opitz associate professor computer science montana missoula sift self-improving fractions tutor eric gutstein initially assistant professor mathematics education depaul chicago symbolic knowledge neural networks insertion refinement extraction geoffrey towell initially siemens research laboratory princeton refining pid controllers neural networks gary scott masters initially forest products laboratory madison patents dna sequence assembly system allex shavlik blattner patent number april courses taught machine learning graduate introduced introduction artificial intelligence advanced artificial intelligence programming languages tools neural network approaches machine learning special-topics computational problems molecular biology special-topics distinguished lecturer series machine learning special-topics machine learning computer vision special-topics machine learning information retrieval special-topics machine learning text analysis special-topics learning modeling biological networks special-topics statistical relational learning special-topics computation informatics biology medicine seminar training grant professional activities conference program committees international machine learning conference area chair area chair area chair national conference artificial intelligence area chair area chair area chair international conference intelligent systems molecular biology acm sigkdd international conference knowledge discovery data mining ieee international conference data mining siam conference data mining area chair european conference machine learning international conference intelligent user interfaces international conference inductive logic programming international conference knowledge capture neural information processing systems area chair additional program committees conference organizing co-organizer hunter searls penn intelligent systems molecular biology ismb july largest conference bioinformatics chair international conference machine learning icml july general chair ieee international conference data mining november co-chair blockeel belgium tadepalli oregon state international conference inductive logic programming june co-organizer gil usc isi musen stanford international conference knowledge capture october chair three-day workshop learning theory data workshops constituted international machine learning conference mooney univ texas hirsh rutgers co-organizers co-organizer hunter national library medicine nsf-nih invited workshop creating infrastructure intelligent systems molecular biology november co-organizer petsche judd siemens corp research international workshop computational learning theory natural learning systems august steering committee member nsf darpa workshop machine learning vision june co-organizer poggio mit sessions workshop october co-organizer stolorz los alamos santa institute post-nips workshop computational approaches biological sequence analysis neural net versus traditional perspectives december co-organizer gordon naval 
research lab subramanian cornell teccuci george mason univ workshop titled agents learn agents twelfth international conference machine learning july board directors steering committees elected member inaugural board directors intl machine learning society inaugural board directors international society computational biology member steering committee international conference intelligent systems molecular biology international conference machine learning international conference knowledge capture member board directors great lakes bioinformatics consortium review panels nsf review panel division information robotics intelligent systems doe site review team lawrence berkeley laboratory genome center january onr research options evaluation panel february nih genome research review committee june december hoc member national institute environmental health sciences niehs board scientific counselors site review team nsf biological research training grants program june nih site review team proposed computational biology resource june national library medicine study section member reviewer biotechnology information proposals july june meetings hoc member march august march national research council panel reviewed nasa computing information communications technology january teleconference reviews nih referee national science foundation artificial intelligence machine learning neural computation science cognitive science connection science computing surveys computer communications acm molecular biology acm artificial intelligence research machine learning research data mining knowledge discovery learning sciences combinatorial optimization ieee trans systems man cybernetics ieee trans neural networks ieee trans pattern analysis machine intelligence ieee trans knowledge data engineering ieee expert ieee parallel distributed technology intl computational intelligence applications magazine knowledge-based systems future generation computing systems orsa computing informs computing genomics bioinformatics journal bioinformatics computational biology tetrahedron computer methodology molecular biology journal comparative functional genomics computer applications biosciences bioinformatics computational biology acm doctoral dissertation awards intl joint conf artificial intelligence italian assoc artificial intelligence hawaii international conference systems sciences biocomputing track computer vision pattern recognition conf neural information processing conf editor hunter searls special issue machine learning machine learning molecular biology appeared september major invited talks combining symbolic neural learning ninth international conference machine learning aberdeen scotland july invited speakers combining symbolic connectionist approaches artificial intelligence fourth congress italian association artificial intelligence florence october sole invited speaker scientific track talking neural networks putting inference rules rules brazilian symposium neural networks belo horizonte december invited speakers talking neural network enriching dialog human teachers machine learners distinguished lecture series speaker institute cognitive science colorado boulder october invited annually creating instructible self-adaptive web browser triangle computer science distinguished lecture series speaker co-sponsored duke north carolina north carolina state computer science departments november invited annually knowledge-based neural networks create instructible self-adaptive web browser seventh international conference neural information processing november taejon korea plenary speakers scaling ilp experiences extracting relations biomedical text fourteenth international conference inductive logic programming september porto portugal plenary speakers editorial boards magazine editor-in-chief editorial board machine learning action editor editorial board journal machine learning research acm transactions knowledge discovery data cognitive science journal artificial intelligence research journal data mining knowledge discovery journal bioinformatics computational biology neural information processing letters reviews journal cognitive systems research action editor international journal computational intelligence applications applied intelligence international journal artificial intelligence tools intl journal computational intelligence organizations editorial advisory board tutorials overview machine learning arthur andersen chicago october january machine learning ieee international computer science conference hong kong december practical applications machine learning ieee conference applications artificial intelligence santa barbara march applying machine learning classification tasks national conference artificial intelligence aaaiwith hirsh rutgers san jose august symbolic neural network approaches machine learning national conference artificial intelligence aaaiwith hirsh rutgers washington july learning examples recent topics symbolic connectionist learning national conference artificial intelligence aaaiwith hirsh rutgers seattle july introduction machine learning molecular biologists international conference intelligent systems molecular biology ismbwith salzberg johns hopkins louis june machine learning genetic microarrays international conference machine learning icmlwith page uw-madison washington august journal articles dimaio shavlik phillips probabilistic approach protein backbone tracing electron density maps bioinformatics special issue based papers presented intl conf intelligent systems molecular biology ismbfortaleza brazil goadrich oliphant shavlik gleaner creating ensembles first-order clauses improve recall-precision curves machine learning mangasarian shavlik wild knowledge-based kernel approximation machine learning research molla waddell page shavlik machine learning design interpret gene-expression microarrays magazine bockhorst craven page shavlik glasner bayesian network approach operon prediction bioinformatics tobler molla nuwaysir green shavlik evaluating machine learning approaches aiding probe selection gene-expression arrays bioinformatics supplement papers international conference intelligent systems molecular biology molla andreae glasner blattner shavlik interpreting microarray expression data text annotating genes information sciences eliassi-rad shavlik system building intelligent agents learn retrieve extract information international journal user modeling user-adapted interaction special issue user modeling intelligent agents allex shavlik blattner neural network input representations produce accurate consensus sequences dna fragment assemblies bioinformatics opitz shavlik connectionist theory refinement genetically searching space network topologies artificial intelligence research craven shavlik understanding time-series networks case study rule extraction invited paper intl neural systems craven shavlik neural networks data mining invited paper future generation computer systems opitz shavlik actively searching effective neural-network ensemble connection science extended version appears combining artificial neural nets sharkey springer maclin shavlik creating advice-taking reinforcement learners machine learning reprinted learning learn thrun pratt eds kluwer opitz shavlik dynamically adding symbolically meaningful nodes knowledgebased neural networks knowledge-based systems towell shavlik knowledge-based neural networks artificial intelligence craven shavlik machine learning approaches gene recognition ieee expert shavlik combining symbolic neural learning extended abstract machine learning longer version appears artificial intelligence neural networks steps principled integration honavar uhr eds academic press towell shavlik extracting refined rules knowledge-based neural networks machine learning maclin shavlik knowledge-based neural networks improve algorithms refining chou-fasman algorithm protein folding machine learning scott shavlik ray refining pid controllers neural networks neural computation shavlik towell noordewier neural networks refine biological knowledge intl genome research craven shavlik visualizing learning computation neural networks intl tools shavlik mooney towell symbolic neural network learning algorithms experimental comparison machine learning reprinted readings knowledge acquisition machine learning buchanan wilkins eds shavlik dejong learning mathematically-based domains understanding generalizing obstacle cancellations artificial intelligence shavlik acquiring recursive iterative concepts explanation-based learning machine learning shavlik towell approach combining explanation-based neural learning algorithms connection science reprinted applications learning planning methods bourbakis books gil musen shavlik eds proc intl conf knowledge capture acm press 
oct shavlik proc intl conf machine learning morgan kaufmann madison july petsche hanson shavlik eds computational learning theory natural learning systems vol mit press cambridge hunter searls shavlik eds proc intl conf intelligent systems molecular biology aaai press bethesda july shavlik dietterich eds readings machine learning morgan kaufmann san mateo shavlik extending explanation-based learning generalizing structure explanations research notes artificial intelligence series pitman publishing london extended version phd thesis rigorously reviewed conference papers dimaio shavlik belief propagation large highly connected graphs partbased object recognition proc ieee international conference data mining icdm hong kong chen ramakrishnan shavlik tamma bellwether analysis predicting global aggregates local regions proc international conference large data bases vldb seoul korea maclin shavlik walker torrey simple effective method incorporating advice kernel methods proc national conference artificial intelligence aaai boston torrey shavlik walker maclin skill acquisition transfer learning advice taking proc european conference machine learning ecml berlin germany maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression proc national conference artificial intelligence aaai pittsburgh davis burnside dutra page ramakrishnan santos costa shavlik view learning statistical relational learning application mammography proc intl joint conf artificial intelligence ijcai edinburgh scotland torrey walker shavlik maclin advice transfer knowledge acquired reinforcement learning task proc european conference machine learning ecml porto portugal bravo page ramakrishnan shavlik costa framework set-oriented computation inductive logic programming application generalizing inverse entailment proc international conference inductive logic programming ilp bonn germany dimaio shavlik phillips pictorial structures molecular modeling interpreting density maps advances neural information processing systems nips vancouver shavlik shavlik selection combination evaluation effective software sensors detecting abnormal computer usage proc intl conf knowledge discovery data mining kdd seattle goadrich oliphant shavlik learning ensembles first-order clauses recallprecision curves case study biomedical information extraction proc intl conf inductive logic programming ilp porto portugal dimaio shavlik learning approximation inductive logic programming clause evaluation proc intl conf inductive logic programming ilp porto portugal molla shavlik albert richmond smith self-tuning method one-chip snp identification proc ieee conf computational systems bioinformatics stanford fung mangasarian shavlik knowledge-based nonlinear kernel classifiers proc intl conf computational learning theory colt washington baiao mattoso shavlik zaverucha applying theory revision design distributed databases proc intl conf inductive logic programming ilp szeged hungary fung mangasarian shavlik knowledge-based support vector machine classifiers proc conf neural information processing systems nips vancouver canada dutra costa page shavlik empirical evaluation bagging inductive logic programming proc intl conf inductive logic programming ilp sydney australia eliassi-rad shavlik theory-refinement approach information extraction proc intl conf machine learning icml williamstown craven page shavlik bockhorst glasner probabilistic learning approach whole-genome operon prediction proc intl conf intelligent systems molecular biology ismb san diego craven page shavlik bockhorst glasner multiple levels learning diverse evidence sources uncover coordinately controlled genes proc intl conf machine learning icml stanford goecks shavlik learning users interests unobtrusively observing normal behavior proc intl conf intelligent user interfaces iui orleans shavlik calcari eliassi-rad solock instructable adaptive interface discovering monitoring information world-wide web intl conf intelligent user interfaces iui los angeles allex baldwin shavlik blattner increasing consensus accuracy dna fragment assemblies incorporating fluorescent trace representations proc intl conf intelligent systems molecular biology ismb halkidiki greece allex baldwin shavlik blattner improving quality automatic dna sequence assembly fluorescent trace-data classifications proc intl conf intelligent systems molecular biology ismb louis cherkauer shavlik growing simpler decision trees facilitate knowledge discovery proc intl conf knowledge discovery data mining kdd portland craven shavlik extracting tree-structured representations trained networks advances neural information processing systems nips touretzky mozer hasselmo eds mit press cambridge opitz shavlik generating accurate diverse members neural-network ensemble advances neural information processing systems touretzky mozer hasselmo eds mit press cambridge expanded version appears article connection science cherkauer shavlik rapid quality estimation neural network input representations advances neural information processing systems touretzky mozer hasselmo eds mit press cambridge maclin shavlik combining predictions multiple classifiers competitive learning initialize neural networks proc intl joint conf montreal aug maclin shavlik incorporating advice agents learn reinforcements proc nat conf seattle expanded version appears article machine learning craven shavlik sampling queries extract rules trained neural networks proc intl conf machine learning brunswick opitz shavlik genetic search refine knowledge-based neural networks proc intl conf machine learning brunswick expanded version appears article artificial intelligence research craven shavlik learning represent codons challenge problem constructive induction proc intl joint conf chambery france opitz shavlik heuristically expanding knowledge-based neural networks proc intl joint conf chambery france expanded version appears article knowledge-based systems craven shavlik learning symbolic rules artificial neural networks proc intl conf machine learning amherst presented intl workshop multistrategy learning harper ferry cherkauer shavlik protein structure prediction selecting salient features large candidate pools proc intl conf intelligent systems molecular biology bethesda towell shavlik symbolic inductive learning improve knowledge-based neural networks proc nat conf san jose maclin shavlik knowledge-based neural networks improve algorithms refining chou-fasman algorithm protein folding proc nat conf san jose expanded version appears article machine learning towell shavlik interpretation artificial neural networks mapping knowledgebased neural networks rules advances neural information processing systems moody hanson lippmann eds morgan kaufmann san mateo expanded version appears article machine learning scott shavlik refining pid controllers neural networks advances neural information processing systems moody hanson lippmann eds morgan kaufmann san mateo expanded version appears article neural computation noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences advances neural information processing systems lippmann moody touretzky eds morgan kaufmann san mateo towell shavlik noordewier refinement approximate domain theories knowledge-based artificial neural networks proc nat conf boston shavlik acquiring recursive concepts explanation-based learning proc intl joint conf detroit expanded version appears article machine learning mooney shavlik towell gove experimental comparison symbolic connectionist learning algorithms proc intl joint conf detroit expanded version appears article machine learning shavlik dejong explanation-based approach generalizing number proc intl joint conf milan italy aug shavlik dejong bagger ebl system extends generalizes explanations proc nat conf seattle july shavlik dejong ross acquiring special case schemata explanation-based learning proc annual conf cognitive science society seattle shavlik dejong analyzing variable cancellations generalize symbolic mathematical calculations proc ieee conf applications orlando shavlik dejong computer 
understanding generalization symbolic mathematical calculations case study physics problem solving proc acm-sigsam symp symbolic algebraic computation waterloo ontario canada july shavlik dejong model attention focussing problem solving proc annual conf cognitive science society amherst shavlik learning momentum conservation proc intl joint conf los angeles shavlik dejong building computer model learning classical mechanics proc annual conf cognitive science society irvine book chapters mooney melville tang shavlik dutra page costa relational data mining inductive logic programming link discovery data mining generation challenges future directions kargupta joshi eds aaai mit press eliassi-rad shavlik intelligent web agents learn retrieve extract information intelligent exploration web szczepaniak segovia kacprzyk zadeh eds springer-verlag shavlik learning artificial intelligence neural network methods synthesis handbook brain theory neural networks arbib mit press cambridge craven shavlik investigating good input representation computational learning theory natural learning systems vol petsche hanson shavlik eds mit press cambridge opitz shavlik heuristic search expand knowledge-based neural networks computational learning theory natural learning systems vol petsche hanson shavlik eds mit press cambridge version appeared article knowledge-based systems shavlik combining symbolic neural learning artificial intelligence neural networks steps principled integration honavar uhr eds academic press york expanded version article machine learning towell shavlik refining symbolic knowledge neural networks machine learning integrated approach vol michalski tecuci eds morgan kaufmann san mateo version appeared proc intl workshop multistrategy learning shavlik machine learning encyclopedia science technology gear mcgrawhill york edition shavlik finding frame-shift errors anonymous dna automated dna sequencing analysis adams fields venter eds academic press york cherkauer shavlik selecting salient features machine learning large candidate pools parallel decision-tree construction massively parallel artificial intelligence kitano hendler eds aaai mit press menlo park towell shavlik knowledge-based neural networks refine roughlycorrect information computational learning theory natural learning systems vol petsche hanson kearns rivest eds mit press cambridge maclin shavlik refining algorithms knowledge-based neural networks improving chou-fasman algorithm protein folding computational learning theory natural learning systems vol hanson drastal rivest eds mit press cambridge expanded version article machine learning shavlik mooney generalizing explanation structures investigating explanationbased learning dejong kluwer academic publishers hingham shavlik rorke empirically evaluating ebl investigating explanation-based learning dejong kluwer academic publishers hingham shavlik physics learning mathematically-based domains investigating explanation-based learning dejong kluwer academic publishers hingham bennett shavlik aspects operationality investigating explanationbased learning dejong kluwer academic publishers hingham towell shavlik hybrid symbolic-neural methods improved recognition highlevel visual features neural networks human machine perception wechsler academic press york shavlik dejong acquiring general iterative concepts reformulating explanations observed examples machine learning artificial intelligence approach vol iii kodratoff michalski eds morgan kaufmann los altos shavlik learning classical physics machine learning guide current research mitchell carbonell michalski eds kluwer academic publishers hingham selected conference workshop papers torrey shavlik walker maclin relational skill transfer advice taking icml workshop structural knowledge transfer machine learning pittsburg torrey walker shavlik maclin knowledge transfer advice taking proc international conference knowledge capture k-cap banff canada goadrich oliphant shavlik learning extract genic interactions gleaner icml workshop learning language logic lll bonn germany maclin shavlik torrey walker knowledge based support vector regression reinforcement learning ijcai workshop reasoning representation learning computer games edinburgh scotland walker shavlik maclin relational reinforcement learning sampling space first-order conjunctive features proc icml workshop relational reinforcement learning banff canada kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer proc aaai workshop supervisory control learning adaptive systems san jose dimaio shavlik speeding relational data mining learning estimate candidate hypothesis scores proc icdm workshop foundations directions data mining melbourne dimaio shavlik phillips pictorial structures identify proteins x-ray crystallographic electron density maps working notes icml workshop machine learning bioinformatics washington mooney melville tang shavlik dutra page costa relational data mining inductive logic programming link discovery proc national science foundation workshop generation data mining baltimore molla andreae glasner blattner shavlik interpreting microarray expression data text annotating genes proc conf computational biology genome informatics durham shavlik shavlik fahland evaluating software sensors actively profiling windows users proc conf recent advances intrusion detection davis cassel allex shavlik blattner dna fragment assembly linear time recomb satellite meeting dna sequence assembly extended abstract goecks shavlik automatically labeling web pages based normal user actions workshop machine learning information finding intl joint conf conf stockholm shavlik eliassi-rad middleware web-based tasks theory-refinement approach cmu workshop learning text web pittsburgh shavlik eliassi-rad intelligent agents web-based tasks advice-taking approach aaai icml workshop learning text categorization madison opitz craven shavlik neural networks automatically refine expert system knowledge bases experiments nynex max domain proc intl conf neural networks houston volume shavlik overview research wisconsin knowledge-based neural networks proc intl conf neural networks washington craven shavlik extracting comprehensible concept representations trained neural networks proc ijcai workshop comprehensibilty machine learning montreal cherkauer shavlik rapidly estimating quality input representations neural networks proc ijcai workshop data engineering inductive learning montreal opitz shavlik genetically refining topologies knowledge-based neural networks proc intl symp integrating knowledge neural heuristics pensacola beach craven shavlik understanding neural networks rule extraction pruning proc connectionist models summer school boulder craven shavlik learning predict reading frames coli dna sequences proc hawaii intl conf systems science biocomputing track maui shavlik towell noordewier knowledge-based neural networks refine existing biological theories proc intl conf bioinformatics supercomputing complex genome analysis petersburg towell craven shavlik constructive induction knowledge-based neural networks proc intl conf machine learning evanston maclin shavlik refining domain theories expressed finite-state automata proc intl conf machine learning evanston shavlik finding genes case-based reasoning presence noisy case boundaries proc darpa cased-based reasoning workshop san mateo shavlik empirical analysis ebl approaches learning plan schemata proc intl workshop machine learning ithaca maclin shavlik enriching vocabularies generalizing explanation structures proc intl workshop machine learning ithaca shavlik towell combining explanation-based learning artificial neural networks proc intl workshop machine learning ithaca fisher mckusick mooney shavlik towell processing issues comparisons symbolic connectionist learning systems proc intl workshop machine learning ithaca shavlik generalizing structure explanations explanation-based learning thesis dept computer science univ illinois urbana january appears uilu-eng- research group coordinated science laboratory univ illinois urbana-champaign version published pitman publishing london 
uncorr ected proo interpreting microarray expression data text annotating genes michael molla peter andreae jeremy glasner frederick blattner jude shavlik department computer sciences wisconsin madison madison usa department genetics wisconsin madison madison usa department biostatistics medical informatics wisconsin madison madison usa received june received revised form november accepted february abstract undoubted exponential increases annotated genomic data rapidly pouring public databases goal develop automated ways combining sources information produce insight operation cells conditions approach machine-learning techniques identify characteristics genes up-regulated down-regulated microarray experiment seek models accurate easy interpret stable small variations training data paper explores ectiveness standard machine-learning algorithms task bayes based probability pfoil based building rules anticipate learned models predict expression levels genes cast task predictive framework genes held training paper reports experiments actual coli microarray data discussing strengths weaknesses algorithms demonstrating trade-o accuracy comprehensibility stability published elsevier science information sciences xxx xxx xxx elsevier locate ins correspondingauthor e-mail addresses molla wisc molla pondy wisc andreae jeremy genome wisc glasner fred genome wisc blattner shavlik wisc shavlik leave victoria wellington front matter published elsevier science pii ins pages dtd august type sps chennai article press uncorr ected proo introduction sgenesproducespeci cproteins building blocks life understanding organism regulates production speci rna sequences crucial understanding mechanism organism functions expression level time microarrays quickly inexpensively measure expression levels thousands genes simultaneously microarrays employ uorescently labeled fragments rna bind locations microarray surface scanning laser measures intensity rnas inferred intensity values measured laser microarraysareoftenusedtomeasureexpressionlevelsbeforeandafteraspeci thatevent up-regulated levels decreased down-regulated development microarrays large collections interpretation microarray-based biomedical experiments present method creating interpretations microarray experiments combines expression-level data textual information individual genes interpretations consist models characterize genes expression levels upor downregulated goal models assist human scientist understanding results experiment approach machine learning create models accurate comprehensible stable small microarray experiment english words text descriptions individual genes descriptions curated swissprot protein database annotations proteins text protein generated gene description gene models consist sets words descriptions characterize up-regulated down-regulated genes note usethe text descriptions genes togenerate interpretationsofmanydi erent microarray experiments ineachexperiment erent genes up-regulated down-regulated text description gene experiments work related recent attempts machine learning predict gene-regulation levels focus erent goal predict gene-regulation levels automatically generate human scientist generate hypotheses explaining experiment molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo key aspect approach interpreting microarray experiments evaluate accuracy potential models experimental data statistical technique called cross-validation goal make predictions genes criteria makes good explanatory model genes expression levels measured experiment model accurately predicted expression levels investigate standard successful algorithms machinelearning literature evaluating satisfy desiderata accuracy comprehensibility stability present evaluate variants combinations approaches standard approach investigate bayes based probability pfoil mooney rule learner based propositional logic bayes proven successful wide range text problems produces learners tend produce human-readable models describing learning algorithms evaluate rst describe basic task assign learners input learner consists parts numeric rna-expression levels gene gene array conditions aftera event antibiotic treatment swissprot text describing protein produced gene microarray output learner model accurately characterizes genes up-regulated down-regulated response event rna rna gene expression ratio greater ratio down-regulated commonly cross-validation gene-regulation examples training data learning algorithm remaining held examples toestimate accuracy learned model fold cross-validation examples divided subsets subset successively held-aside test set subsets pooled create training set desired property human comprehensibility cult measure crude approximation counting number distinct swissprot words appearing model important property stability ideally models learned change gene-array experiment repeated slightly erent conditions actual replicates microarray experiment measure stability repeatedly creating molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo eachother forexample ifamicroarraycontains genes wewouldseveral times randomly select genes learn model random sample apply stability measure ned set learned models section presents machine-learning algorithms investigate ourexperiments section section presents discusses experimental results obtained data blattner coli laboratory wisconsin section describes related work nal section describes planned follow-up research summarizes lessons learned work create tool english-text protein annotations assist interpretation microarray experiments algorithmdescriptions section describes algorithms bayes pfoil mooney bothalgorithmstake input collection training instances case genes labeled belonging classes call present absent text description gene algorithms produce model categorize unlabeled gene basis feature values words describing bayes bayes algorithm works computing estimates conditional ruleto determine probable class unlabeled instance algorithm classes compute probabilities word probabilities combined likelihood ratios word represent signi cance presence absence word present presentjup presentjdown absent presentjup absentjdown ituses astandardlaplace estimatortodeal infrequent words thatis added numerator denominator estimate probabilities preventing estimating probabilities algorithm classify unlabeled instance gene instance molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo ith word vocabulary present absent depending word present absent instance algorithm called ignore dependencies words calculating probabilities assumption independence hasprovene involving english words features major problem bayes classi lack comprehensibility characterization positive class vocabulary words characterization involves likelihood ratios variant pruning characterization signi words likelihood ratios note pruning reduce accuracy experiments section explore trade-o accuracy comprehensibility pruned bayes algorithm phases construct subset vocabulary consisting words highest score information-gain measure prune subset incrementally removing words time result characterization class terms smaller set words pseudocode version algorithm table information-gain measure rst phase standard measure ectiveness word separating classes decision-tree algorithms words scored information-gain words highest scores selected table pruned bayes algorithm disjointly split data set training testing sets disjointly subdivide training set train set tuning set word train set compute infogain set set words largest infogain empty foreach remove construct bayes classi train words evaluate classi accuracy tuning set return remove resulted tuning-set accuracy report current tuning testing set accuracies classi molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo vebayesalgorithm thetrainingsetis separated train set tuning set data tuning set choose word prune algorithm constructs subset obtained removing word current subset words generates bayes classi train set instance descriptions restricted remaining words determines average accuracy resulting classi tuning set iteration chooses word results accurate classi prunes current subset interested trade-o accuracy comprehensibility test set iteration subset empty pfoil pfoil mooney prepositional version 
foil rulebuilding algorithm incrementally builds rules characterize instances class data set foil builds rules rst-order logic language rules conjunctions literals logical variables recursive interpreted rst-order pfoil usesasimpler propositionallanguage builds rules conjunctions features pfoil rules interpretedstraightforwardly true instance domain rule speci words present gene annotation pfoil itconstructseach ateachstep itchooses feature maximizes performance rule foilgain measure instances remaining features positive foilgain ruleiscomplete rule data set starts build rule foilgain feature rule trade-o coverage rule number positive instances class covered rule increase precision rule fraction instances covered rule positive foilgain rule log log number positive negative instances covered rule added rule mooney pfoil prune rule set pfoil constructing rules covered positive instances data set noise result large set rules molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo beveryspeci ctoparticularinstances rulesare comprehensible vectors likelihood ratios produced bayes large rule set comprehensible toaddressthis problem wehaveextended pfoil toinclude arule pruning stage lines pruning foil table inthepruningstage rules choosing feature removal results highest accuracy remaining rule set features removed rule rule removed rule set pruning version bayes separate training data set train set tuning set tuning set evaluate predictive accuracy rules learned train set thanhalting thepruningwhen experiments continue pruning rule set empty order explore trade-o comprehensibility accuracy improving stability pfoil bayes algorithm fairly stable small variations data based estimated probabilities pfoil greedy rulebased algorithm stable set rules produces considerablywithjustasmallchangeinthedata pfoil encourage greater stability algorithm rst runs pfoil number collects words rule sets reruns pfoil rule-set pruning training set restricted rststage thepseudocodeis table table pfoil pruning disjointly split data set training test sets disjointly subdivide training set train tuning sets construct pfoil rule set completely train whilethe rule set empty foreach feature rule current rule set temporarily remove feature rule evaluate rule set accuracy tuning set return feature current rule set remove rule set feature resulted accuracy remove empty rule rule set report current rule set accuracy train tuning test sets molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo evaluate algorithm vefold cross-validation compute measure similarity rule sets folds coarse measure considers words rule sets ignores thestructureoftherules rms averageof number times word duplicated rule sets stability count juj isthenumberofrulesets andcount word appears rmsaverage weights words occur data sets heavily sets words identical count equal foreach stabilitywouldbe ifthe count equal case stability experimentalmethodology data microarray experiments performed blattner coli sequencing laboratory wisconsin current computational experiments biological event antibiotic treatment gene expressions measured min treatment vefold cross-validation times training data evaluating models held text elds swissprot database include comment elds theexception ofthedatabase cdb andmassspectrometry cms topics table pseudocode stabilized pfoil times disjointly split data set training test sets generate set words training set initialize counts word repeat times choose random training set train set run pfoil train set pruning increment count word rule rule set set words count generate rule sets words disjointly split training set train tuning sets run pfoil pruning train set restricted words record rule set accuracy test set compute stability average accuracy rule sets folds molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo description eld theorganismclassi cation eld thekeyword eld title elds exception titles complete sequence coli genome common text convert words annotations root form porter stemming algorithm brackets paper signify word stemmed residu aljec refers residual residue porter stemmer implemented algorithms rst set experiments explores trade-o accuracy comprehensibility algorithms fig shows result vefold cross-validation experiment running pfoil bayes pruning average error rate plotted number words left model baseline error rate data set predicting frequent class algorithms perform bayes consistently pfoil consistently baseline initial decline pfoil error rate increases words pruned rule set increase surprisingly slight wordsremain unexpectedly overmostoftheexperiment pruningwordsfrom bayes model decreases error rate toaminimum words error rate increases error rate unpruned bayes swissprot words higher error rate graph result initial pruning phase informative features pfoil bayes accuracy high models pruned comprehensible size fig test set error function model size molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo table vebayes thepfoil section disjunctive rules bayes section shows likelihoodratios word presence absence gene upregulated low likelihood ratios gene down-regulated models shown chosen pruning reduced models words share word surprising characterizing experiment word biosynthesis ects underrepresentation basic biosynthetic genes up-regulated note pfoil rules dominated words absent bayes model words high likelihood ratio word absent appears algorithms erent kinds characterizations data set experiments explored trade-o accuracy stability pfoil algorithm fig shows averaged vefold result runningstabilizedpfoil onvariousvaluesof onthesame data set fig represents constrained words octable sample small models experimental data pfoil disjunctive rules rule map biosynthesis proteins gene rule biosynthesis map encoded rule hypothetical rule -kb bayes word present absent flagellar deriv ativejationc ubiquinone nuo mitochondri ajalc aspart atejylc nadh complex jesjedc biosynthesis permeas molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo curred rule set runs average words phase represents constrained wordsthatoccurredin rulesetswere availableforthe phase stability measure represents constraint words occurred rule sets phase stability measure represents similarity rule sets maximum observed stability result words appeared rule sets rest appearing rule sets minimum stability result half words appearing rule sets half appearing stability rule sets increased signi cantly number words reduced stabilized pfoil stable original version error rate change result stabilizing algorithm error rate considerably worse unstabilized version suggests forcing stability signi accuracy cost discussion pruning experiments suggest greedy algorithm trade-o accuracy comprehensibility severe fact decreasing rule set size bene cial signi cantly hamper accuracy range comprehensible rules words information-gain-based pruning prune adverse ect error rate increases pruning drops back greedy pruning fact pfoil error rate higher bayes suggests pfoil ective discovering real regularities gene data fig trade-o rule set stability test set error eate pfoil molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo evaluating bene adding features time dependencies features valleys search space mislead pfoil direction future work 
explore ways smoothing search space bayes attempt identify dependencies features itsmodel evenwhen pruned manageable number words helpful identifying dependencies inthemodelintable permeas andflagellar strongly correlated up-regulated genes model genes characterized permeas erent genes characterized flagellar spite bayes model consistently higher accuracy pfoil models speculate reason pfoil rules crisp sense instance feature speci rule satisfy rule bayes model softer range alternative features adirectionforfuturework explore rule-building algorithms softer rules allowing rules asnotedabove erentcharacterizations ofthedata thisispartlyduetothedi erent defaultrules inthealgorithms genes pfoil sdefaultistoclassifyany gene covered rule explanation abundance rules mat pfoil attempts rules exclude genes easily achieved excluding words commonindowngenes prunedna vebayes ontheotherhand hasabiastowards larger class default data set genes pruning advantage keeping words pick genes classify correctly pfoil retains words pick genes infrequent genes means present smaller wenotealsothatalthoughna words account pruning tend retain group dependent words generally accuracy advantage keeping words algorithms initially produce incomprehensible models features human make sense experiments show heavy pruning result comprehensible models moderate loss accuracy results bayes striking model words moderate loss accuracy -fold increase comprehensibility comprehensibility essential task results encouraging pfoil produces unstable models vary signi cantly genes training data experiments shown algorithm improving stability pfoil ective producing stable molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo rule sets cost reduced accuracy promising result intend technique sophisticated rule-building algorithms relatedwork great deal research text mining involves biomedical tasks hearst lindi system searches medical literature text relating subject problem make logical connections form hypothesis bene approach researchers advance expression data textual descriptions genes represented expressiondata rstpass atdiscoveringwhat interesting pubgene tool interprets gene-expression data based textual data big erence pubgene compiles clusters text ahead time match expression data already-made cluster tool designed expression data models masys text genes explain experiments cluster expression values experiments text explain clusters text directly learning process explain single experiments presented approach aiding interpretation microarrayexperiments thatisbased onmachinelearninganduses swissprot textas representation microarray genes argue important properties computational tool produce modelsthatare accurate readable microarray data empirically studied widely successful algorithms bayes pfoil coli microarray experiment evaluating approaches variants respect desiderata shown algorithms characterizations data reasonable accuracy adding pruning algorithms presented modi cation pfoil increased stability cost decreased accuracy noted algorithms construct erent kinds characterizations intend explore ways combining desirable properties bayes pfoil develop algorithms task molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press uncorr ected proo current limitation approach relies text sequence data plan toexplore methods make sequence data text annotating genes enhancement increase textual data abstracts articles logical step finally plan empirically evaluate approach additional microarray experiments acknowledgements research funded grants nlm nsf irinih nih bairoch apweiler swiss-prot protein sequence database supplement trembl nucl acids res brown lin cristianini sugnet furey ares haussler knowledgebased analysis microarray gene expression data support vector machines proc natl acad sci craven shavlik learning symbolic rules arti cial neural networks proc intl conf machine learning morgan kaufmann dudoit yang callow speed statistical methods identifying erentially expressed genes replicated cdna microarray experiments uc-berkeley statistics dept hearst untangling text data mining proc annual meeting assoc computational linguistics jenssen greid komorowski hovig high-throughput gene-expression analysis nat genet manning sch uutze themit press masys keyword hierarchies interpret gene expression patterns bioinformatics mitchell machine learning mcgraw-hill porter algorithm stripping program quinlan learning logical descriptions relations mach learn xing jordan karp feature selection high-dimensional genomic microarray data proc intl conf machine learning morgan kaufmann molla information sciences xxx xxx xxx ins pages dtd august type sps chennai article press 
promoter terminator feature nodes operon node intermediate nodes codon usagemin internal internalspacing operon operon length spacing promoter edge terminator spacing spacingmax internal internal consistency edge codon usage expression codon usage expression expressionmean internal min internal codon usagemean internal expression true positive rate false positive rate bayes net naive bayes vol pages doi bioinformatics btl bioinformatics probabilistic approach protein backbone tracing electron density maps frank dimaio jude shavlik george phillips computer sciences dept biostatistics medical informatics dept biochemistry dept wisconsin madison usa abstract time-consuming step protein crystallography interpreting electron density map fitting complete molecular model protein image protein produced crystallographic process poor-quality electron density maps interpretation require significant amount crystallographer time work investigates automatingthe time-consuming initial backbone trace poor-quality density maps describe acmi automatic crystallographic map interpreter probabilistic model knownas markovfield representthe protein residuesof protein modeled nodes graph edges model pairwise structural interactions modeling protein manner model flexible infinite number conformations rejecting physically impossible efficient algorithm approximate inference belief propagation probable trace protein backbone density maptobedetermined resolution compare results alternative approaches resolutions acmi offers accurate backbone trace current approaches contact dimaio wisc introduction determining folding protein three-dimensional spatial configuration atoms protein long important problem biochemistry exceptions protein structure uniquely determined linear amino-acid sequence algorithm determine unique structure sequence scientists forced rely laboratory methods order determine protein structures experimental methods exist popular accounting protein structures determined date x-ray crystallography significant recent interest high-throughput structure determination time-consuming step crystallography interpretation electron map finding location protein atoms three-dimensional image protein paper describe acmi automatic crystallographic map interpreter algorithm automates process tracing backbone electron density maps acmi consists main components local matching component locates individual amino acids density map global constraint component prior knowledge protein structure eliminating false positives local matching acmi combines efficient inference algorithm infer protein backbone electron density map acmi model probabilistic interpretation represents residue probability distribution electron density map property contrained force residue single location advantageous naturally handles noise map errors input sequence disordered regions protein crystallography background protein crystallography labor-intensive undertaking protein produced large quantities purified protein crystals grown requires testing significant number crystallization conditions solvents crystals finally beam x-rays fired crystal lattice protein molecules comprise crystal diffracts x-ray beam produces pattern spots plate spots represent intensities fourier-transformed picture protein laboratory experiments determine phases intensities finally fourier transform converts intensities electron density map three-dimensional image protein final step x-ray crystallography interpreting electron density map converting representation usable biologists interpretation crystallographer locate amino-acid sequence protein coordinates centers protein atoms interpretation extremely time-intensive crystallographer spend weeks months interpreting poor-quality electron density map electron density map defined lattice points covering unit cell basic repeating unit protein crystal crystal unit cell multiple copies protein related crystallographic symmetry regular ways protein pack unit cell rotation translation operators relate region unit cell asymmetric unit symmetric copies protein form multimeric complex dimer tetramer asymmetric unit cases crystallographer isolate interpret single copy protein overview interpretation task illustrated figures figures electron density map function unit cell illustrated isocontoured surface figure correspondence addressed author published oxford press rights reserved permissions journals permissions oxfordjournals online version article published open access model users entitled reproduce disseminate display open access version article non-commercial purposes provided original authorship properly fully attributed journal oxford press attributed original place correct citation details article subsequently reproduced disseminated entirety part derivative work commercial re-use contact journals permissions oxfordjournals illustrates small portion electron density map sticks figure show location bonds atoms figure shows lines adjacent atoms protein trace backbone trace main concern paper finally figure shows scale problem illustrating complete unit cell electron density map crystallographically symmetric copies protein measure quality electron density map resolution map x-ray beam protein crystals diffract beam general crystal diffracts beam quality map illustrated figure shows short protein electron density variety resolutions lower values resolution higherquality electron density map high resolutions resolution individual atoms visible automated interpretation straightforward details individual atoms smeared atom-based methods tend fail approaches attempted automatically interpreting maps met success interpretations produced methods messy require significant crystallographer effort fill gaps overview algorithm high-level overview acmi main components illustrated table acmi includes local matching component individual residues probabilistically located map independent residues global constraint component backbone chain built probabilistically local matches taking account chemical laws governing physical structure proteins local-matching component algorithm makes library existing sequence-specific -mer templates searching individual residue common conformations -mer centered residue local search high sensitivity matching residue correct location suffers low specificity producing significant number false positives acmi global-constraint component probabilistically refines local search results takes account prior knowledge protein structure prior knowledge adjusts local-match probabilities based local match probabilities residues produces physically feasible interpretation maximizes probabilities local matching acmi models physical feasibility pairwise markov field represents probability conformation product probabilities pairs residues pairwise potential analogous pairwise potential energy calculations molecular dynamics model optimize physical energy statistical energy local matching local matching acmi locate individual protein residues electron density map poor-quality maps acmi designed simple atom-based-refinement methods perform poorly empirically methods rotamer searching skeletonization critical points perform poorly fig electron density map protein fragment map resolution fig view electron density map interpretation amino acid sequence protein density map crystallographer goal find positions proteins atoms alternatively backbone trace reduces residue single point acmi automates determination backbone trace fig electron density map entire unit cell copy protein unit cell symmetric copies wrap map boundary dimaio low-resolution maps methods success low-resolution maps based finding large fragments protein electron density sequencespecific -mer search locate individual residues electron density map method divided basic parts illustrated figures previously solved structures protein data bank construct basis set sequence-specific -mer templates perform rotation translation search map -mers basis set output local search residue estimated probability distribution residue presence unit cell constructing sequence-specific -mer basis set acmi begins step illustrated figure walking onedimensional protein sequence -mer centered residue -mer search non-redundant subset pdb restricted sequence similarity three-dimensional instances -mer instances search neighbors -mer increasing pam distance structures infeasible search conformations electron density map cluster structures represent cluster centroid fragment weight clustering fragments rotationally-aligned all-atom rms deviation fragments distance metric quickly computed optimization problem complete-linkage hierarchical fig overview -mer template matching process extracted representative -mers residue perform rotation translation search fragment density map matching fragment tuning set structures bayes rule equation determine probability distribution residue density map table pseudocode overview acmi algorithm procedure acmi sequence seq electron density map find putative backbone trace foreach residue mjw 
dolocalmatch seqi enforceglobalconstraints mjw optimal trace argmax procedure dolocalmatch seq sequence seq electron density map find prob dist mjw residue map -mer centered residue extract instances -mer pdb cluster characterize -mer conformational space perform search -mer density map tuning set convert squared density differences probabilities mjw residue procedure enforceglobalconstraints mjw individual residue probability distributions find marginal probabilities structure constraints model protein backbone structure graph nodes model a-carbon positions edges enforce structural constraints probability interpretation ggiven product node potentials edge potentials wjm residues residues mjw infer marginal probs structural constraints fig -mer clustering process walking amino-acid sequence -mer centered position search database instances -mer cluster finally extract representative member cluster characterizes conformational space -mer sequence probabilistic approach protein backbone tracing electron density maps clustering limiting clusters maximum diameter cluster representation thrown limit cpu time step remaining clusters find centroid representative fragment record cluster weight centroid fragment percent structures fell cluster depending sequence structural entropy -mer clusters resultant centroid fragments produced cluster centroids weights determined acmi represent conformational space specific -mer fragment fragments length balancing trade-off template size template specificity larger fragments preferred recognition poor-quality maps larger fragments lower representation set already-solved structures non-redundant pdb subset -mers searching -mer centroid fragments clustering complete cluster centroids extracted search instances centroids electron density map process illustrated figure fragment target resolution build map expect fragment map location compute squared electron density difference map fragment compute difference points electron density map distance fragment map searching standardized fragment electron density masking function nonzero points fragment scales standard deviations fragment map densities perform fragment search search rotations translations fortunately compute quickly single rotation ffts additionally position store best-matching -mer fragment rotation electron density difference function good measure similarity regions density convert scores probability distributions probability jscore instance specific -mer cluster location match score score acmi computes tuning set application bayes rule bayes rule states probability jscore score score terms right-hand side computed estimated probability distribution match scores map score derived actual distribution match scores unsolved map prior probability residue location map simply normalization term knowing protein sequence number copies -mer electron density map normalize probabilities map sum term distribution scores -mer matches map trickier compute acmi estimates term tuning set derived protein structures pdb tuning set instances -mer cluster searching match cluster centroid density map tuneset density map centroid cluster estimate distribution scores -mer match end local matching procedure acmi computed residue probability distribution unit cell point space probability specific -mer positioned location remainder paper describes algorithm prior knowledge structure protein estimate probable backbone trace probability distributions run times local matching significant fragment searchc rotations -degree discretization entire electron density map total compute time order cpu-weeks -mer matching trivially parallelized global constraints section computed residue probability distribution position unit cell probability probability map generated residue location rotation select residue maximized probability resultant trace figure acmi account structural probability model ensure proposed structure physically feasible protein molecule ultimately find map configuration fig backbone traces trace maximizes product -mer match probabilities resultant protein physically impossible prefer trace lower -mer match probability corresponds physically-possible structure dimaio residues arg max wjm term accounts physical feasibility proposed structure figure higher probability configuration figure markov field model model global constraint probability acmi pairwise markov field model pairwise markov field modelg consists set nodes vconnected edges node graph hidden random variable graph conditioned observation variables vertex observation potential edge conformational potential represent full joint probability wjm concerned finding maximizing probability figure shows encode protein markov field model node represents amino-acid residue protein label amino-acid residue consists terms cartesian coordinates residue alpha carbon internal parameters alternate parameterization rotational parameters bend angle formed consecutive residues observation potential residue -mer probability computed section conformation potentials model probability conformation residues protein divided basic types suddereth hand-tracking model acmi defines adjacency potentials edge connecting neighboring residues figure potentials ensure adjacent residues maintain proper sspacing proper angle acmi defines occupancy potentials non-adjacent residues figure prevent residues occupying region three-dimensional space joint probability defined adjacent adj nonadjacent occ residues distant protein chain necessarily distant space graph fully connected pair residues joined edge markov field model adjacency potentials adjacency potentials connect adjacent pair residues broken product constraining functions distance constraint function rotational constraint function adj distance constraint based physical fact proteins distance invariant potential takes form tight gaussian ideal internal parameters model rotation residue angle formed residue triple centered residue simplify definition choose parameterize degrees freedom pairs spherical coordinates direction forward residue backward residue local -mer matching section addition computing probability specific location remembers -mer centroid rotation centroid location map store values indicating direction adjacent residues based direction residues rotated best-matching -mer angular constraint function illustrated figure position map fixed-width gaussian sphere centered preferred orientation residue center sphere highest potential residue located lightest points sphere athc occupancypotentials occupancy potentials place ensure residues occupy location space defined independently orientation step function constrains nonadjacent closest distance nonadjacent residues occ structural potential function acmi deals crystallographic symmetry slightly modify potential function symmetric operators residues occupy space fig structure graphical model joint probability conformation residues product observation potential node adjacency potential adjacent residues occupancy potential pairs non-adjacent residues probabilistic approach protein backbone tracing electron density maps symmetric copies occ min symmetric transforms multiple chains asymmetric unit handled acmi separate chains fully connected edges enforcing occupancy constraints acmi inference algorithm finding probable backbone trace ultimate goal acmi producing backbone trace finding labels maximize probability local observational potentials global conformational potentials arg max residues residue solving arbitrary graphs infeasible dynamic programming solve quadratic time treestructured graphs alternative acmi belief propagation compute approximation marginal probability residue chooses maximum marginal label residue final trace belief propagation inference algorithm based pearl polytree algorithm computes marginal probabilities series local messages iteration node residue computes estimate marginal distribution estimate residue location unit cell product incoming messages residue passes convolution product edge potential outgoing edge true positive rate false positive rate features gene spacing feature group expression feature group codon usage feature group promoter feature group operon length feature group terminator feature group features unitcell denotes estimation marginal iteration figure illustrates message-passing simple twodimensional residues prior probabilities probability mass split peaks structural knowledge tells residue residue iteration residue passes message residue residue expects find residue essentially ring residue peaks messages probability distributions marginalized message 
recipient random variables message residue residue function residue position unit cell residue passes message back residue indicating expects find shows iterations reduce number peaks structural priors graphs cycles exact graphs arbitrary topologies acmi protein model guarantees convergence correctness empirical results show loopy produces good approximation true marginal technical challenges computational savings afforded size complexity graph space labels presented acmi number implementation challenges fig angular component acmi adjacency potential performing -mer matching acmi remembers positions adjacent residues most-likely match potential gaussian sphere surface centered location adjacent residue figure shows single location unit cell fig simple message passing belief propagation prior probabilities iteration node passes message nodejindicatingi belief ofj position residue adjacent residue saway residue message consists bubbles peaks iterates matches structurally supported residues begin emerge dimaio scope paper modifications order scale problem discussed paper authors section briefly discuss scaling issues representation potentials label residue continuously-valued -dimensional variable nonparametric belief propagation nbp variant handle continuous-valued labels previous work represented belief sum-of-gaussians work introduces fourier-series nbp variant nbp represents messages belief set fourier coefficients cartesian space offer number benefits problem domain benefits include natural treatment periodic boundary conditions symmetry explicit initialization required required sum-of-gaussians efficient message-passing implementation efficient message passing message passed requires integrating entire unit cell vely takes running time order number fourier coefficients typical protein adjacency messages problem integrate thin spherical shell adj nonzero occupancy messages message computation time significant fortunately occupancy potential function distance connected residues pass message log multiplication fourier-space structural message aggregation graph fully connected iteration messages computed stored number amino-acid residues protein message probability distribution entire unit cell demanding computationally storage-wise outgoing structural messages equation node similar differ denominator serves avoid double-counting making method exact tree-structured graphs loopy graphs doublecounting unavoidable structural potentials diffuse high-entropy potentials authors suggested approximation errors graphs type potential tend stabilize save significant amount work aggregate non-bonded residues sending single structural message dropping denominator acmi sending messages iteration combined optimizations acmi handle large proteins large unit cells typical run times inference vary hours day experiments obtained set ten model-phased electron density maps center eukaryotic genomics wisconsinmadison maps fairly good resolution natively solutions test acmi performance poor-quality data downsampled maps smoothly diminishing intensities higher-resolution reflections avoid truncation effects give realistic model low-resolution data scaled structure factors exp resolution structure factor scaling constant chosen based desired resolution higher values smooth map down-sampled maps resolutions giving total maps test chose signal strength weakened point truncation compared performance acmi maps automated techniques specialized low-resolution maps ioerger textal terwilliger resolve approaches success handling interpretation poor-quality maps textal based ideas pattern recognition ioerger constructs set rotation-invariant density features features radii textal trains neural network identify atoms sidechains identified electron density putative alpha carbon efficiently finding similar region database laying sidechain terwilliger takes approach resolve resolve large secondary-structure elements places map extends rotamer search places sidechains aligning sequence backbone methods success true positive rate false positive rate features promoter terminator operon length codon usage expression inter-gene spacing features maps running algorithms test set measured results metrics rms error predicted true structure percent chain solved percent correct residue identity ideally method find trace low rms error high percent chain solved high residue identity results resolution summarized figure textal unable run protein density maps resolution including terrible score map gave benefit doubt textal report results maps ran terms rms error figure algorithm consistently performs textal resolutions tested two-tailed pair test acmi outperforms textal values resolve performs roughly equivalent acmi resolution acmi performance two-tailed test yields values tests account rms error chain coverage figure shows percent chain covered roughly equivalent approaches figure shows approach identifying proper residue type location important point related methods optimizing residue-identification accuracy resolve return long chain alanine residues identify sidechains correct backbone structure illustrates significant difference acmi alternate approaches textal resolve build backbone model attempt align protein sequence acmi alternatively sequence protein construct model result identification amino acids map probabilistic approach protein backbone tracing electron density maps additionally figure shows scatterplots individually solved electron density map point x-axis acmi error y-axis textal resolve points diagonal line correspond maps acmi outperformed textal resolve majority structures interpretation lower rms error algorithms acmi outperformed resolve highresolution maps acmi perform post-processing predicted backbones real-space refinement energy minimization residues restricted grid limiting accuracy grid spacing advantage acmi probabilistic framework addition returning putative trace acmi returns confidence probability level predicted residue confidence informs crystallographer areas map improvement alternatively high confidence partial trace improve phasing figure illustrates trace resolution structure consisting chains residues sixth-best ten traces resolution acmi finds segments rms deviation covering backbone trace color likelihood prediction residue location fig graphs showing comparison algorithms average interpretation terms rms error percent chain located percent residues correctly identified fig scatterplot showing performance protein-by-protein basis acmi versus textal resolve mark interpreted map points diagonal cases acmi provided moreaccurate backbone trace dimaio conclusions future work describe acmi tool automatically tracing protein backbones designed poor-quality electron density maps acmi combines local matching procedure global constraint procedure probabilistic framework efficiently infer locations backbone atoms electron density map algorithm accurate traces poor resolution electron density maps outperforming textal resolve map resolution major shortcoming acmi significant compute time required local -mer matching procedure search approximately -mer fragments residue fragment rotations medium-sized unit cells takes order cpu-weeks larger proteins months acmi exploits parallelism running overnight spare cycles desktop computers investigate machine learning algorithms support vector machines neural networks quickly match -mer density map explore alternative feature representations additionally post-processing step augment acmi refinement sidechain tracing algorithm previous work pictorial structures place sidechain atoms trace combining tool acmi produce complete molecular model finally explore probabilistic model phase improvement maps initial phasing poor maps partial structure significantly improve initial phasing revealing previously blurred-out regions electron density high-confidence trace iteratively improve phasing future research direction acmi providing accurate interpretations lower-resolution maps acmi reduces burden crystallographers poor-quality density map data obtaining higher-resolution electron density map data acmi significant cost savings making poorer-quality maps speeding process high-throughput protein structure determination acknowledgements acknowledge support national library medicine training grant -lm nlm -lm national institutes health protein structure initiative grant berman westbrook impact structural genomics protein data bank pharmacogenomics perrakis sixma wilson lamzin warp improvement extension crystallographic phases 
acta crystallographica terwilliger automated main-chain model-building templatematching iterative fragment extension acta crystallographica ioerger sacchettini textal system artificial intelligence techniques automated protein model building methods enzymology mackerell wio rkiewicz-kuczera karplus all-atom empirical energy function simulation nucleic acids chem soc terwilliger automated side-chain model-building sequence assignment template-matching acta crystallographica greer three-dimensional pattern recognition molecular biology leherte glasgow baxter steeg fortier analysis three-dimensional protein images journal research cowtan fast fourier feature recognition acta crystallographica wang dunbrack pisces protein sequence culling server bioinformatics jones taylor thornton rapid generation mutation data matrices protein sequences cabios kabsch solution rotation relate sets vectors acta crystallographica huang hwang computation conformational entropy protein sequences proteins structure function bioinformatics cowtan modified phased translation functions application molecular-fragment location acta crystallographica thain tannenbaum livny distributed computing practice condor experience concurrency computation practice experience geman geman stochastic relaxation gibbs distributions bayesian restoration images pami sudderth mandel freeman willsky visual hand tracking nonparametric belief propagation mit lids technical report pearl probabilistic reasoning intelligent systems morgan kaufman san mateo weiss freeman correctness belief propagation gaussian graphical models arbitrary topology neural comp murphy weiss jordan loopy belief propagation approximate inference proc uai dimaio shavlik improving effciency belief propagation large highly-connected graphs working paper research group ihler fisher willsky message errors belief propagation proc nips dimaio shavlik phillips pictorial structures molecular modeling interpreting density maps proc nips fig comparison predicted versus actual structure sixth-best ten interpretation sresolution thin continuous coil actual structure thicker segmented chain acmi prediction predicted structure colored log-likelihood least-likely residues shown red most-likely blue probabilistic approach protein backbone tracing electron density maps 

bioinformatics vol suppl pages evaluating machine learning approaches aiding probe selection gene-expression arrays tobler molla nuwaysir green shavlik department computer science wisconsin west dayton street madison departments computer science biostatistics medical informatics wisconsin west dayton street madison nimblegen systems science madison received january revised accepted march abstract motivation microarrays fast cost-effective method performing thousands dna hybridization experiments simultaneously dna probes typically measure expression level specific genes probes greatly vary quality hybridizations choosing good probes difficult task accurately choose probes hybridize fewer probes needed represent gene gene-expression microarray genes array physical size goal empirically evaluate successfully standard machine-learning algorithms bayes decision trees artificial neural networks applied task predicting good probes fortunately easy training examples learning task place probes gene chip add sample genes highly expressed record probe measures presence gene training examples accurate predictor probe quality learned results learning algorithms investigate bayes neural networks learn predict probe quality surprisingly top ten predicted probes gene training average rank top gene hundreds probes decision-tree induction simple approach predicted melting temperature rank probes perform significantly worse algorithms features represent probes easily computed time score candidate probe training minor training bayes algorithm takes time takes times long train neural network time correspondence addressed substantial order hours desktop workstation report information contained features describe probes find fraction cytosine probe informative feature find surprisingly nucleotides middle probes sequence informative ends sequence contact molla wisc keywords microarrays probe selection artificial neural networks decision trees bayes introduction overview oligonucleotide microarrays commonly gene chips fast cost-effective method performing thousands dna hybridization experiments simultaneously general procedure simple short typically base pairs -bp short strands dna sequence called probes affixed specific positions chip surface fluorescently labeled rna sample washed surface rna hybridize complementary strands dna amount rna hybridized position microarray sample inferred fluorescence measurements measured laser scans surface chip returning fluorescence intensity snustad simmons order measure expression level specific gene sample design microarray dna strands complimentary gene interest typical probe shorter typical gene probe entire complement gene fact due issues dilution economy total length probes hybridize gene typically small fraction length gene choosing good probes difficult task sequences hybridization characteristics oxford press machine learning probe selection breslauer expression levels accurately measured good set probes chosen inadequate nonspecific hybridization occur confounding results experiment effective computational aids probe selection substantially improve design gene chips computationally aiding probe-selection task focus article task definition apply well-established machine-learning techniques problem probe selection order judge probe quality predicted learning algorithm performs important task specifically compare performance artificial neural network ann training bayes algorithm decision-tree learner mitchell order frame problem probe selection categoryprediction task -bp probe called features derived sequence bases make probe goal predict quality probe strictly features set probes set -bp probes genes coli subtilis genomes genes genome tiling genes measure probe quality measured fluorescence levels probes exposed sample genes highly expressed summarize task algorithmically learn perform -bp probe gene training predict probe fluorescence level rank top probes gene exposed sample gene highly expressed accurately predicted information combined constraints prove highly beneficial deciding probes represent gene designing microarray increase probability chosen probe bound strongly probe gene expressed current sample fewer probes needed gene detect microarray genes measured gene chip physical size related work heuristics developed discard probes based based knowledge hybridization characteristics lockhart self-hybridization degenerate repeats attempted melting point equations derived genetic material solution kurata suyama stormo kurata suyama investigate predictions stable secondary structures probe uniqueness create criteria selecting good probes contribution successful empirical evaluation standard machine-learning methods applied task learning predict good probes systems methods data experimental data consists tilings genes coli subtilis measured maskless microarrays singh-gasson produced nimblegen order standardize data genes measured fluorescence intensity probe normalized bottom top measured intensities labeled values linearly interpolated mapping bottom top reduces impact outliers datasets supervised machine-learning algorithms bayes decision trees anns typically require training set data labeled categorized examples set data called training set classifier produced learning algorithm training tested dataset test set predicted classifications trained machine-learning classifier compared correct outputs test set order estimate accuracy classifier microarray data genes generate training-set test-set pairs commonly applied leave-one-out method method training set consists probes tilings genes test set remaining gene probes simple features describe -bp probes called mers data set features chose table expressive data descriptions features determine information contained basic mer structure normalized intensity chose label probe discrete output low medium high sought roughly evenly distribute low medium high normalized intensity discrete-output mappings chose interval maps low examples data medium high experiments training discard probes labeled medium arguably ambiguous fair ignore medium probes testing phase objective testing predict output probes training tobler table features describe probes feature description fraca fracc fracg fract fraction mer fracaa fracac fracag fracat fracca fraccc fraccg fracct fracga fracgc fracgg fracgt fracta fractc fractg fractt fraction dimers mer nucleotide position mer prime end dimer position mer prime end remove probes test sets mismatches -bp region full genome probe gene probes insufficiently indicative single gene leave insufficiently unique probes training sets provide information learning algorithms case number discarded test sets genes small small fraction probes close match genome methods mentioned evaluate well-established machine learning algorithms bayes decision trees artificial neural networks anns task bayes practical successful machine-learning algorithm assumes independence features independence assumption simple ratio compute relative likelihood test feature values labeled high low nbratio high producttext high low producttext low high low number training examples labeled high low high estimated simply counting number examples training dataset output labeled high low terms denominator feature equal avoid bias underestimating probability output occurrence feature training data m-estimate probability estimate probabilities high low low high number occurrence feature equal output number examples output chosen equal experiment settings features discrete feature values mitchell discretized non-discrete features fraca fracat binning equally distributed bins bayes algorithm fast algorithm training classification training classification train test fold average minutes standard desktop classifier evaluate probe selection decision tree algorithm generate decision trees quinlan successor quinlan algorithm selects node place tree computing information gain candidate features choosing feature gains information output category current information gain measure feature separates remaining training examples based shannon 
information theory information gain calculated equations entropy low log low high log high set examples low high estimated computing fractions low high labeled examples infogain entropy summationtext values entropy values set values feature subset feature mitchell machine learning probe selection waikato weka machine learning algorithms java package http waikato weka index html run decisiontree experiments weka algorithm decision-tree learning named algorithm quinlan quinlan reduced-error-pruning algorithm avoid overfitting training data quinlan procedure removes portions initially induced decision tree overfitting data leaf pruned tree fraction training set reaching node high low recorded slightly modified weka code report fractions classifying test crude estimate probability current test-set called high weka software package training classification decision tree train-test pair takes minutes standard experiments approach evaluate multilayered ann trained backpropagation standard algorithm training neural networks algorithm attempts minimize squared-error network output values target outputs algorithm searches weight space defined weight values arc network error minimum nonlinear multi-layered network algorithm guaranteed find global minimum error find local minimum mitchell networks train consist input units produced input unit real-valued feature fracta -of-n encoding discrete-valued feature -of-n encoding requires boolean-valued input unit feature input network input unit representing discrete-valued feature set inputs set represented values input units single layer hidden units general hidden units leads overfitting training data hidden units lead underfitting data hidden units free ann constraints feature set network discover intermediate non-linear representations data mitchell sarle assert large numbers hidden units avoid finding bad local minima decided hidden units number input categorization tasks rumelhart argue crossentropy error function work found difference predictive accuracy comparing squared-error cross-entropy error functions project selected commonly squared error units employing standard sigmoidal activation function finally output units sigmoid activation functions output units represent estimated probability output high low network fully connected input unit connected hidden units hidden unit connected output units arc network initialized random weight standard practice training consists cycles training set early stopping avoid overfitting training data training long decide stop training create tuning set data randomly removing training data cycle measure current accuracy ann predictions tuning set ann weight settings cycle performs tuning set weight settings classify test set training classification run takes order magnitude longer bayes decision trees results algorithm run training set sort predicted scores probe test set bayes ann classifiers sorting highest lowest ratio prob label high testset prob label low testset decision-tree sorting solely based prob label high test-set mentioned estimated distribution high low examples leaf pruned decision tree reached test-set sorting manner produce ordering test-set examples predicted machinelearning classifier question answer assume good probes gene sorted list question similar asked informationretrieval systems search engines order nrelevant articles highest-scoring articles returned definitions good probe figure panel define good measuring higher normalized scale probes normalized measured intensities testset probes highest predicted scores turns tobler ann bayes normalized measured intensities ideal curve -degree line probes top considered good results fall ideal curve panels report information increasingly strict definitions good probes normalized intensities construction normalize cases decision-tree learner performs poorly neural networks bayes perform surprisingly neural networks slightly included figure curve presents results ordering probes simply predicted melting point calculated melting points formula presented aboul-ela predictors based neural networks bayes substantially accurate simply predicted melting point metric figure presents visualization predicted intensities learning algorithms typical gene region generated curves manipulating ratios determine probes reported figure important note figure visualization purposes functions generate figure curves partially fitted testing data invalidates quantitative evaluation curves generated bayes anns log ratio prob label high prob label low create predicted output probe gene similarly decision trees simply prob label high compute squared error predicted values normalized measured intensities probes gene raising predicted values increasing powers power minimum squared error chosen visualization graphs selected power generate curves shown figure legend quantitative experiments interested relative predicted values probes figure raising scores increasing exponents impact sorted order visualize predictions found manipulate predictions bayes ann predicted curves closely fit measured probe intensities high low intensity values decision-tree curve fit actual probe curve algorithms intensity ranges figure shows short -bp region probes results typical produced genes figure offers insight features providing information classification information gain computed equation curve generated computing information gain individual feature training datasets values normalized information content highest-scoring feature fracc informative feature double information informative feature bell-shaped curves features show nucleotides middle probe substantially informative end mer expect discussion threshold normalized intensity experiments show ann bayes competently pick efficiently hybridizing probes normalizedintensity threshold algorithms choose probes normalized intensity threshold time average strictest normalized intensity threshold probes chosen meet standard experimentation needed discern normalized intensity threshold probes needed order accurately sense gene results highly encouraging encouraging results ann bayes task predicting normalized intensity gene task similar choosing probes threshold subtle difference consistency prediction relative intensities weaker probes top twenty required order make landscapes line decision tree performs consistently worse algorithms tasks suggests probabilistic representation preferable allor-nothing nature decision-tree model variants decision-tree induction boosting bagging quinlan regression trees learn predict real values directly quinlan perform task relative information gain features interesting bases ends probe important middle intuitively kinetics situation makes sense imagine strand target rna bound complimentary mer dna probe bases end strand leaves -base-long rna arm free float twist generate torques break duplex imagine middle bases complementary strands bound case swinging ends ten bases long torques smaller free rna order rna strand stay connected probe long hybridize fully critical middle probe make good bond rna machine learning probe selection num predicted highest scoring probes considered average ber estset obes easure ann naive bayes dtree primer melting point ideal predicted highest scoring probes considered average ber estset obes asure predicted highest scoring probes considered average ber estset obes asure liz predicted highe scoring probe considered average ber estset obes asure nsi fig number test-set probes highest-scoring predictions exceed threshold normalized measured intensity learning algorithm averaged test sets starting nucleotide position -mer probe liz predicted values log normalized high low annpredicted values log normalized high low predicted alues high actual probe values normalized probe intensity fig probe intensity classifier output versus starting nucleotide position mer probe gene relative 
information gain base content informative content informative feature content important single bases surprising relative stability g-c bond a-t bond tobler ature liz ature liz fig normalized average information gain probe feature training sets table definitions features dimer frequencies interesting difficult explain approximate symmetry importance frequency similar importance frequency reflects approximate symmetry stacking energies duplex exact reverse switching prime prime ends stacking energies great influence energetic characteristics hybridization information content dimers roughly relative stacking energies dimers exception dimers high stacking energies show low information content features subject study future work developed strategies successfully find probes bind rna rna present sample limitation approach filter probes bind non-specifically mismatch probes probes target probe small number exceptions alongside target probe common method identifying behaviour binding nonspecific mismatch probes high intensity values extending experiments address non-specific probes investing task predicting amount fluorescence measured sample applied microarray rna probe gene specifically investigating learning tasks converting categorical real-valued prediction probe predict measured fluorescence conditions probe fluorescence level gene rna substantially present sample probe fluorescence level gene srnaisabsent sample maximum fluorescence level probe mismatch probes gene srna substantially present sample maximum fluorescence level probe mismatch probes gene srna absent sample predicted quality potential probe combination predicted values plan evaluate machine learning conjunction methods heuristic method filter probes learner self-complementarity proves machine learning probe selection difficult characteristic learner infer simple computational test self-complementarity eliminate probes topic addressing evaluating machine learners larger set probes including eukaroyotic genes finally enriched sets features represent probes instance zuker mfold algorithm predict secondary structures probes targets devising features based mfold predictions anns bayes highly accurate simple easy compute features table richer features order obtain acceptable level predictive accuracy interested judging features combinations features prove helpful making accurate computational models probe quality order obtain insight improving probe-design process conclusion address important task predicting quality short probes represent gene microarray approach apply well-established machine-learning algorithms task order machine learning works problem determine evaluated learning approaches accurate task obtained training data problem putting mers prokaryotic genes gene chip measuring fluorescence probe sample genes highly expressed applied microarray ideally probes hybridize strongly sample probes hybridized weakly basically background-noise level trained learning algorithms predict probes hybridize strongly learning algorithms performed remarkably held test data bayes artificial neural networks worked usage decision-tree-induction algorithm performed calculated melting point predict quality probes training bayes neural networks quickly rate candidate probes time needed judge probe quasiuniqueness needed rate probe hybridization quality results strongly suggest shelf machine-learning methods greatly aid important task probe selection gene-expression arrays acknowledgements darryl roy helping creating training data research funded grants nih grant nlm nsf irinih nih aboul-ela koh tinoco base-base mismatches thermodynamics double helix formation dca dct nucleic acids res breslauer frank blocker marky predicting dna duplex stability base sequence proc natl acad sci usa stormo selection optimal dna oligos gene expression arrays bioinformatics lockhart dong byrne follettie gallo chee mittmann wang kobayashi horton borwn expression monitoring hybridization high density oligonucleotide arrays nat biotechnol kurata suyama probe design dna chips japanese soc bioinformatics mitchell machine learning mcgraw-hill boston quinlan induction decision trees machine learning quinlan programs machine learning morgan kaufman san mateo quinlan boosting bagging proceeding national conference artificial intelligence aaaimit aaai press rumelhart durbin golden chauvin backpropagation basic theory back-propagation theory architecture applications chauvin rumulhart eds lawrence erlbaum associates hillsdale jersey sarle stopped training remedies overfitting proceedings symposium interface computing science statistics shannon mathematical theory communication bell system technical journal july october singh-gasson green yue nelson blattner sussman cerrina maskless fabrication light-directed oligonucleotide microarrays digital micromirror array nat biotechnol snustad simmons principles genetics edition wiley york zuker mathews turner algorithms thermodynamics rna secondary structure prediction practical guide barciszewski clark eds rna biochemistry biotechnology nato asi series kluwer academic publishers 
neural network input representations produce accurate consensus sequences dna fragment assemblies allex shavlik blattner computer sciences dept wisconsin madison west dayton madison usa tel dnastar south park madison usa tel genetics dept wisconsin madison henry mall madison usa tel correspondence addressed keywords dna fragment assembly sequencing neural networks consensus bioinformatics abstract motivation inputs extracted aligned column dna bases underlying perkin elmer applied biosystems abi fluorescent traces goal train neural network correctly determine consensus base column choosing network input representation critical success task empirically compare representations base calls include trace information results attained accurate results networks incorporate trace information input representations based estimates derived -fold cross-validation network topology produces consensus accuracies ranging coverages aligned sequences coverage makes errors consensus calls contrast network base calls input representation double error rate errors consensus calls contact allex wisc introduction applied neural networks task determining consensus base column aligned dna sequences problem addressed referred consensus calling briefly figure accuracy consensus sequences important concern national human genome research institute nhgri set standard sequencing accuracy nhgri error rate sequences genbank estimated lawrence solovyev higher standard imperfect dna sequences translated effect resulting protein sequence substantial mutation single amino acid critical character predicted protein deletion insertion bases result frame shifts lead dramatically increased error rates failure recognize open reading frames dna translated sequencing accuracy significantly dependent careful human examination editing consensus sequences fragment assemblies hand process time-consuming expensive error-prone making unsuitable large-scale sequencing projects automatic methods produce highly accurate consensus calls reduce errors alleviate human editing significant system consensus calling differs existing methods directly processes information shape intensity perkin elmer applied biosystems abi fluorescent traces methods tigr assembler sutton staden package bonfield examine previously determined base calls calculating consensus existing assemblers trace characteristics phrap green dnastar seqman make consensus call phrap chooses base call aligned column highest-quality trace determined companion base-calling program phred ewing seqman consensus determined method developed earlier work allex method extracts sums information shape intensity traces alignment sums evidence determining consensus call difference system neural networks figure description operation neural networks details found mcclelland rumelhart neural networks powerful data analysis tool problems molecular biology baldi brunak strength ability learn complex patterns found types problems neural networks tasks dna sequencing scarcely explored promising neural networks make base calls individual dna sequences golden note golden work calls bases single sequences work describe determines consensus multiple aligned sequences system methods ability neural network correctly categorize instances problem critically dependent input representation baldi brunak work problem expressed aligned column base calls traces represent column numerical inputs define features aligned column singly combination form input representations neural network features information extracted fluorescent traces valuable information lost traces reduced base calls hypothesis neural network exploit trace information make consensus calls accurate made networks base calls inputs inputs trace information weighted quality trace emphasis data description calculation quality values appears allex input features fluorescent trace information captures shape traces employ trace classification scores allex summarized figure input features defined aligned column listed base call fraction fraction occurrences gap fraction fraction occurrences gaps trace peak intensities base trace peak intensity weighted quality averaged number aligned sequences trace peak shapes base strong medium trace classification scores weighted quality averaged number aligned sequences figures details calculating numerical inputs features tested network topologies hidden units outputs desired outputs networks consist single represents bases gap input representations combinations input features simplest network referred base call input representation consists base call fraction gap fraction features base call network control testing hypothesis inputs include trace information produce accurate results base calls network called trace shape inputs include trace peak shapes gap fraction input features network trace intensity inputs trace peak intensities gap fraction input features fourth network referred trace shape intensity trace peak intensities trace peak shapes gap fraction features thirteen inputs finally tested network included input features base call trace peak intensities trace peak shapes gap fraction network topologies summarized figure make consensus call networks find highest output base gap consensus call ambiguous calls made setting threshold output exceeds threshold ambiguous call made output threshold call unambiguous non-heterozygote dna sequences human editors resolve ambiguous calls bases submission genbank ambiguous calls serve focus editors attention areas consensus warrant closer examination case heterozygote genomes ambiguous calls pinpoint differences alleles implementation tested effectiveness networks examples distinct amounts coverage number aligned sequences reasonable algorithm make correct calls coverage high criterion identify superior method accuracy coverage low addition step required sequence fragment adds expense sequencing reducing needed coverage means substantial reduction sequencing costs large sequencing projects typical produce coverage areas ensure accurate consensus sequences coverage needed method highly accurate fewer aligned sequences compare input representations varying amounts coverage created sets examples set coverage chose examples coverages form sets set examples categorized data ten training test sets constructed set network trained examples tested remaining occurs test set training sets disjoint test set sets examples desired output gap outnumbered examples desired outputs enable networks learn recognize gaps gap examples duplicated training sets occur frequency examples base note gap examples duplicated test sets sets extracted fragment assemblies section coli supplied coli genome project wisconsin blattner assemblies created dnastar seqman fragment assembly program data alignments assemblies good sequence traces vary quality areas present challenge consensus calling figure aligned region test assemblies fair amount discrepancies indicating imperfect underlying trace data difficulties consensus calling data subsequent alignments included testing results cover wide range quality perfect inexact shown figure correct base calls categorize data coli sequences submitted genbank neuralware neuralworks professional software neural network tests ran software pentium pro running windows discussion trained tested neural network topologies examples sets coverage -fold crossvalidation report accuracies averaged test sets training phase training set processed accuracy results topologies graphed figure networks found trace shape intensity produces accurate consensus calls coverage makes errors calls range accuracies coverage coverage network base call information inputs base call lowest accuracies coverage aligned sequences network substantially poorer results networks coverage sequences differences base call trace shape intensity networks statistically significant paired one-tailed t-test confidence level networks results base call network achieved coverage aligned sequences error rate double network trace 
information additional tests experimented alternative plausible input representations experiment extracted inputs broader context single column premise accuracy consensus calls increased extending inputs include trace information bases base interest parker golden reported intensity values base affected adjacent bases parker show intensity peak low patterns dye-primer dye-terminator labeled data perkin elmer parker believed neural networks trained recognize patterns practice found improvement accuracy extended inputs experiment provided single intensity input trace intensities window surrounding center base peaks values calculating trace classification scores transforming algorithmically network process network alternate input representation required inputs yielded results similar trace shape intensity network work demonstrates neural networks effective tool determining consensus aligned dna sequences networks trained input representations fluorescent trace information ignore base calls highly accurate studies utilizing traces neural networks consensus calling related tasks warranted allex baldwin shavlik blattner increasing consensus accuracy dna fragment assemblies incorporating fluorescent trace representations proceedings international conference intelligent systems molecular biology halkidiki greece aaai press allex baldwin shavlik blattner improving quality automatic dna sequence assembly fluorescent trace-data classifications proceedings fourth international conference intelligent systems molecular biology louis aaai press ansorge sproat stegemann schwager non-radioactive automated method dna sequence determination journal biochemical biophysical methods baldi brunak bioinformatics machine learning approach cambridge mit press blattner plunkett bloch perna burland riley collado-vides glasner rode mayhew gregor davis kirkpatrick goeden rose mau shao complete genome sequence escherichia coli science bonfield smith staden dna sequence assembly program nucleic acids research ewing hillier wendl green base-calling automated sequencer traces phred accuracy assessment genome research ewing green base-calling automated sequencer traces phred error probabilities genome research green genome sequence assembly annual conference computational genomics program abstract book herndon tigr science education foundation golden iii torgersen tibbetts pattern recognition automated dna sequencing on-line signal conditioning feature extraction base calling proceedings international conference intelligent systems molecular biology bethesda aaai press lawrence solovyev assignment position-specific error probability primary dna sequence data nucleic acids research mcclelland rumelhart parallel distributed processing cambridge mit press national human genome research institute nhgri standard quality human genomic sequence http nhgri nih gov grant info funding statements rfa quality standard html parker deng zakeri carlson nickerson kwok peak height variations automated sequencing pcr products taq dye-terminator chemistry biotechniques perkin elmer dna sequencing chemistry guide foster city smith sanders kaiser hughes dodd connell heiner kent hood fluorescence detection automated dna sequence analysis nature sutton white adams kerlavage tigr assembler tool assembling large shotgun sequencing projects genome science technology cwak ctt gcg cggc gct consensus figure consensus calling state-of-the-art sequencers perkin elmer applied biosystems abi fluorescent-dye labeling determine dna fragment sequences ansorge smith fragment sequencing process produces dye intensities sets fluorescent traces fragments sequenced aligned fragment traces base calls output abi software shown sequences aligned consensus sequence listed alignment calculated columns base calls traces exhibit total agreement highlighted columns base calls traces conflict ambiguity code listed consensus call rightmost highlighted column base call erroneously inserted fragment consensus shows gap meaning base exists weighted connections inputs avg peak intensities outputs consensus call hidden units categorized examples inputs desired outputs inputs outputs average relative trace peak intensities consensus call aligned column figure neural networks feed-forward backpropagation neural network learns categorize patterns inputs inputs numerical representations features problem typically output category problem desired output correct category network trained processing set categorized examples training set categorized instance problem includes inputs desired outputs training weighted connections network adjusted error actual output reduced hidden units network aid allowing input representation transformed difference desired actual inputs sufficiently low training halted network categorize previously unseen instances problem future accuracy trained network estimated measuring trained network performance disjoint set testing examples figure simple neural network function call consensus single aligned column dna bases inputs extracted fluorescent traces network inputs relative trace intensity averages outputs consensus call peaks negative curvature sign change slope strong weak shoulder slope medium strong medium weak score figure trace classifications peak trace classification set scores capture shape intensity traces single base call classes criteria distinguish listed illustrated score assigned classes reflects amount strong medium weak peak characteristic exhibited trace sets traces shown scores trace combination strong-medium peak inputs figure base call fraction aligned sequences highlighted column base call divide number occurrences number sequences base call occurs sequences input set likewise inputs input figure gap fraction aligned sequences highlighted column input interested gaps single input number gap occurrences divided number sequences gap occurs sequences input inputs maximum intensity aligned column quality weighted average peak intensity relative maximum tattattctcac tattattctcac tattattctcac figure trace peak intensities sequences aligned highlighted column bases sequence intensity center column trace divided maximum trace fraction multiplied quality allex assigned sequence average weighted values forms input base maximum trace typical abi traces sequence intensity trace intensity relative maximum values bases sequence calculated values weighted quality results parenthesis relative intensity averaged values yield inputs inputs aligned column trace classification scores quality weighted average tattattctcac tattattctcac tattattctcac figure trace peak shapes form inputs aligned sequences highlighted column extract trace information trace classification scores allex compute strong medium peak scores traces sequence found weak scores irrelevant score multiplied quality score trace scores weighted quality parenthesis scores inputs base average sequences weighted strong scores average weighted medium scores neural network input features trace shape trace peak shapes gap fraction inputs trace intensity trace peak intensities gap fraction trace shape intensity trace peak shapes trace peak intensities gap fraction base call base call fraction gap fraction base call fraction trace peak shapes trace peak intensities gap fraction figure network topologies networks hidden units outputs number inputs range figure test assembly alignment data testing varying quality displayed region aligned sequences test assemblies columns base calls total agreement marked fair amount disagreement base calls implying poorer-quality underlying trace data consensus calling region difficult areas near-perfect data base call trace intensity trace shape trace shape intensity coverage accuracy figure results trace shape intensity network produces accurate results coverage coverage accuracies networks trace information 
network nnetwork combine network outputs ensemble output input network state unit activation state unit activation gleaner creating ensembles first-order clauses improve recall-precision curves mark goadrich louis oliphant jude shavlik department computer sciences department biostatistics medical informatics wisconsin-madison avenue madison usa march abstract domains eld inductive logic programming ilp involve highly unbalanced data common measure performance domains precision recall simply accuracy goal research approaches ilp suited large highly-skewed domains propose gleaner randomized search method collects good clauses broad spectrum points recall dimension recall-precision curves employs clauses thresholding method combine sets selected clauses research focuses multi-slot information extraction task typically involves negative examples positive examples formulate problem relational domain large testbeds involving extraction important relations abstracts biomedical journal articles compare gleaner ensembles standard theories learned aleph nding gleaner produces comparable testset results fraction training time keywords inductive logic programming ensembles recall-precision curves biomedical information extraction introduction large relational domains recently addressed eld inductive logic programming ilp intrinsically involve highly unbalanced data negative examples greatly outnumber positive examples tang taskar popescul richardson domingos domains present problems traditional ilp algorithms learning focused obtaining accurate interpretable theories small datasets goal research approaches ilp suited large highly-skewed domains standard approach ilp covering algorithm rst-order logical clauses learned sequentially covering subset positive examples positive examples covered clause urnkranz clauses combined form called theory traditional approach combine clauses test match learned clauses classi positive individual theory produce set true false predictions testset examples predictions evaluated wide number evaluation metrics accuracy true positive rate false positive rate precision recall ned table kluwer academic publishers printed netherlands camera-goadrich tex goadrich oliphant shavlik applying ilp large relational datasets major problem covering algorithm approach amount time needed generate theory theory search time intensive due repeated sequential process examining hundreds thousands clauses clause add theory pronounced large datasets days weeks complete theory large training set problem involves evaluation metrics ilp domains common measure performance large highly-skewed domains precision recall manning sch utze evaluation metrics focus correct classi cation positive examples standard ilp approach biased producing high-precision low-recall clauses combined typically create high-recall low-precision theory evaluation create recall-precision curve illustrate trade-o measurements address goal ciently producing good recall-precision curves ilp developed evaluated gleaner algorithm gleaner parallel randomized-search method quickly collects good clauses broad spectrum points recall dimension recall-precision curves employs simple clauses thresholding method combine selected clauses create disjoint theories focused erent region recall-precision space datasets exploring gleaner biomedical informationextraction large relational highly unbalanced speci cally set medline journal abstracts manually tagged proteinlocalization genetic-disorder relationships goal learn theory extracts relations training set abstracts performs unseen abstracts datasets compare gleaner ensembles standard aleph theories nding gleaner produces comparable testset results fraction training time rest paper organized rst review background material ilp evaluation metrics recall precision present algorithm gleaner discussion biomedical information-extraction dataset detail describe comparison algorithm aleph ensembles discuss results experiments report extensions gleaner comparison control algorithms nally conclude proposals future work area interpolating recall-precision space versus roc space making predictions machine learning classi ers commonly label test positive negative predictions true false giving classi cation matrix found table true positives examples classi correctly predicts positive label camera-goadrich tex gleaner ensembles clauses improve recall-precision curves table evaluation metrics nitions classification metric nition accuracy tntp true positive rate tptp false positive rate fpfp precision tptp recall tptp -score precision recallprecision recall false positives occur classi incorrectly predicts positive label similar nitions true negatives false negatives datasets unbalanced class distributions present number problems ilp systems domains tend large number objects relations causing large explosion search space clauses rst approach sample objects reduce space reasonable size moderate number objects brings problem large skew data negative examples suppose people friends people positive examples assuming friendship relationship necessarily symmetric negative examples friendships negative examples positive negative skew table nitions common evaluation metrics accuracy true positive rate false positive rate precision recall issue skewed data leads standard performance measure accuracy evaluate results positive class small relative negative class trivial achieve high accuracy labeling test examples negative precision recall concentrate positive examples precision measures accurate predicting positive class recall measures total positives identify common ways evaluate performance algorithms roc curves fawcett recall-precision curves manning sch utze typically generated ranked list examples points commonly created rst ning threshold examples ranked higher threshold classi positive examples lower threshold classi negative classi cations calculate values true positive rate false positive rate recall precision threshold threshold varied highest rank lowest rank giving meaningfully distinct threshold values ranking points camera-goadrich tex goadrich oliphant shavlik curve thresholding generate classi ers ranked list examples general classi generate curve article area curve auc traditionally analyze roc curves bradley plot true positive rate versus false positive rate equivalent wilcoxon-mann-whitney statistic cortes mohri calculates chance randomly selected positive randomly selected negative ranked list positive negative examples optimal auc positive examples ranked negative examples score random classi auc quick summary roc graph helpful comparing performance erent machine learning algorithms variety thresholds area recall-precision curve aurpc gather single score algorithm involved research figure roc curves mask true performance algorithm ignoring class distribution curves account skew recall precision axis calculate aurpc rst standardize precision-recall curves cover full range recall values interpolate points rst point designate rfirst pfirst curve extended horizontally point pfirst point achievable randomly discard fraction extracted relations expect precision smaller recall examples setting determine recall ending point total postotal pos total neg found classifying positive give continuous curve extending recall dimension remember point precision-recall curve generated underlying true positive tpa false positive fpa counts suppose points precision-recall space intermediate values curve interpolate counts tpa tpb fpa fpb negative examples takes equal positive local skew ned fpb fpa tpb tpa create points recall tpa integervalues tpa tpa tpa tpb calculate precision linearly increasing false positives point local skew resulting precision-recall points tpa total pos tpa tpa fpa fpb fpatpb tpa points approximate aurpc graphical interpolation recall-precision curve erent roc curve roc interpolation linear connection points recall-precision space connection curved depending actual number positive negative examples camera-goadrich tex gleaner ensembles clauses improve recall-precision curves false positive rate roc curve cisi recall curve figure roc curves computed set predictions dataset section curves produced identical underlying predictions visually strikingly erent covered point 
curve pronounced classi predicts tied con dence scores causing adjacent points recall precision curve figure constructed single point extended endpoints dataset positives negatives interpolating produce aurpc camera-goadrich tex goadrich oliphant shavlik cisi recall correct interpolation incorrect interpolation figure interpolation erences roc space linear connection severely overestimate aurpc aurpc advantages breakeven point statistic ned point curve precision recall curves necessarily generated ranked list examples aurpc exact measure -point average precision statistic manning sch utze calculate interpolate precision points roc curves optimal aurpc aurpc random classi equal totalpostotalpos totalneg expected precision classifying random sample examples positive discussion aurpc metric relation roc curves found davis goadrich inductive logic programming inductive logic programming ilp combines machine learning logic programming zeroski lavrac process learning rstorder clauses correctly categorize training set data typical machine learning algorithms propositional xed-sized feature-vector representation ilp relations mathematical logic describe examples handle large variable-sized structures sequences advantage ilp incorporation related background knowledge data open problems ilp include ciently searching greatly increased hypothesis space compared propositional domains appropriately calibrating probability estimates boolean-valued logical rules refer reader nilsson maluszy nski nitions standard logic programming terminology camera-goadrich tex gleaner ensembles clauses improve recall-precision curves east-bound west-bound trains mutagenic molecules friendship social networks noo figure relational object domains versus link-learning domains domains suitable ilp roughly divided main groups figure group tasks internal relational structure classic domain trains dataset michalski larson goal discriminate types trains east west trains relational nature varying length number cars types objects carried car realistic mutagenesis dataset srinivasan goal classify chemical compound mutagenic relational nature atomic structure chemical ilp proven successful domains bringing inherently relational attributes hypothesis space group tasks examples addition relational structure relations examples goal domains classify links objects objects domain learning friendship social networks taskar classifying people determine structural relationships people based combination personal attributes attributes friends domain type learning suggest relevant citations scienti popescul link-learning domains typi signi overlap background knowledge large skew negative examples article focused link-learning domains aleph srinivasan top-down ilp covering algorithm developed oxford written completely prolog open source aleph based earlier ilp system called progol muggleton input aleph takes background knowledge form intensional extensional facts list modes declaring literals camera-goadrich tex goadrich oliphant shavlik chained designation literal head predicate learned lists positive negative examples head literal required high-level overview aleph sequentially generates clauses positive examples picking random seed saturated create bottom clause speci clause relative background knowledge covers clause created chaining literals facts seed added speci limit reached bottom clause determines search space clauses aleph heuristically searches space clauses clause found time runs clauses learned cover positive training examples learned clauses combined form theory uckert zelezn explored stochastic local search sls explore hypothesis space propositional ilp settings aleph stochastic search functions stochastic clause selection scs gsat walksat selman rapid random restart rrr scs randomly picks clauses subsets bottom clause user-speci distribution clause lengths scs hard time nding high quality clauses biased select long clauses due heavy-tailed distribution clause lengths local search gsat selects initial clause random chooses add remove randomly selected literal altered clause evaluation function walksat modi gsat allowing percent bad moves rrr works similarly scs gsat walksat initial random clause selection takes time evaluate hypothesis space initial clause rrr nes clauses bestrst search conjunction heuristic evaluation function restarts random clause speci number evaluations gsat walksat make moves hypothesis space rrr makes investigation choosing move stated earlier standard ilp approach biased producing high-precision low-recall clauses combined typically create high-recall low-precision theory number clauses theory recall clause assuming independence clauses theory probability positive classi positive theory probability classi positive clause words minus probability classi positive clauses recall theory written large values approaches camera-goadrich tex gleaner ensembles clauses improve recall-precision curves table iii gleaner algorithm initialize bins create recall bins bin bin bin uniformly divide recall range populate bins parallel pick seed generate bottom clause randomized local search clauses generation clause find recall binr training set precision recall seed binr store binr discard clause seed binr clauses generated determine bin threshold binj find theory binm highest precision tuneset recall clauses match examples recall binj evaluate testset find precision recall testset bin decision process entire equation approaches recall entire theory unrealistically assuming independence order negative correctly labeled match clauses probability clause correctly classifying negative true negative rate equals probability clauses correctly calling negative probability false positive approaches large values keeping focus skewed data suppose positive negative examples true negative rate implies precision clause precision theory independent clauses algorithm gleaner order rapidly produce good recall-precision curves developed gleaner two-stage algorithm learn broad spectrum clauses combine thresholded theory aimed maximizing precision choice recall pseudo-code algorithm appears table iii gleaner gathers grain left reapers call algorithm gleaner sifts clauses discarded standard heuristic search form theories gleaner algorithm aleph underlying engine generating clauses camera-goadrich tex goadrich oliphant shavlik recall figure hypothetical run gleaner seed bins training set showing considered clause small circle chosen clause bin large circle repeated seeds gather clauses assuming clause found falls bin seed initialization rst stage gleaner learns wide spectrum clauses illustrated figure aleph search clauses seed examples encourage diversity experiments section recall dimension uniformly divided equal sized bins seed clauses stochastic local-search methods hoos stutzle clauses generated compute recall clause determine bin clause falls bin track clause appearing bin current seed heuristic function precision recall determine clause increase generality clauses end search process clauses collected seed seed examples total clauses assuming clause found falls bin seed perform stochastic local search considered search methods discussed earlier stochastic clause selection scs gsat walksat rapid random restart rrr found gsat walksat make uphill moves search space removing predicates clause rrr due internal workings aleph adding predicates clause cient removing testbeds rrr takes time produces higher quality clauses methods gleaner search method remainder article stage takes place clauses gathered random search gleaner combines clauses bin create large thresholded theory form clauses cover camera-goadrich tex gleaner ensembles clauses improve recall-precision curves recall figure twenty complete recall-precision curves gleaner bin evaluated fold protein-localization dataset order classify positive learned theories generate recall-precision curves exploring values shown figure curves overlap recall precision results 
save highest points combined curve irrespective bin generated points bin record theory threshold generated highest points bin tuning set evaluate saved thresholded theory testset record precision recall end recall-precision points generated bin broadly span recall-precision curve unique aspect gleaner point recall-precision curve generated separate thresholded theory opposed usual setup create curve standard theory transformed ranking examples nding erent thresholds classication separate-theory method related roc convex hull created separate classi ers fawcett separate theories strength gleaner approach theory point curves hindered mistakes previous points theory totally independent end-user gleaner choose preferred operating point recall-precision curve algorithm generate testset classi cations closest bin desired recall results found threshold produce con dence score number clauses cover selected bin reason performed macro-averaging lewis results calculate aurpc aurpc rst calculated fold averaged produce camera-goadrich tex goadrich oliphant shavlik pubmed abstract sentence deduced amino acid sequence fre protein exhibits hydrophobic regions compatible transmembrane domains significant similarity sequence plasma membrane cytochrome critical component human phagocyte oxidoreductase suggesting fre structural component yeast ferric reductase clear cytochrome plasma membrane ambiguous fre transmembrane co-occurance combinations figure sample biomedical sentence correct extractions protein localization task selection dataset article focuses dataset learning location yeast proteins cell illustrated figure biomedical information extraction domain research genetic-disorder dataset similar structure examined section section overview domain proceeds explain dataset collected labeled additional background information information extraction information extraction process scanning unstructured text objects interest facts objects framed learning task ned information unstructured text documents extract relevant objects relationships main tasks single-slot extraction multi-slot extraction named entity recognition ner common subtask single-slot extraction ner identifying single type object individual corporation gene weapon multi-slot extraction builds objects found ner relationship items text examples parent-child relationship individuals ceo company interaction proteins cell multi-slot extraction typically harder objects relation identi semantic relationship objects recently biomedical journal articles major source interest community number reasons amount data enormous objects proteins genes standard naming camera-goadrich tex gleaner ensembles clauses improve recall-precision curves conventions interest biomedical practitioners quickly relevant information blaschke shatkay feldman ray craven bunescu focused learning multi-slot protein localization medline abstracts task identify links phrases correspond protein location protein cell biomedical journals typically highly domain-speci language figure gure sentence protein-localization dataset proteins black text boxes locations white text boxes named entities dataset discern evidence sentence suggest protein found cell location relational data task multi-slot falls link-learning category section domain typically unbalanced data small number phrases protein names learning relation entities protein location increases imbalance number positive examples subset cross-product entities negative examples entity-entity pairing dataset ilp well-suited information-extraction biomedical domains link-learning tasks ilp ers advantages straight-forward incorporate domain knowledge expert advice produce logical clauses suitable analysis revision humans improve performance multi-slot appealing challenge task ilp due large amount examples background knowledge substantial skew examples large heavily skewed datasets ilp research information-extraction task provide testbed ilp research data labeling testbed initially ray craven data consist sentences abstracts found medline database positive phrase-phrase relations formalize dataset ilp task sentence protein phrases location phrases satisfy relation protein location original dataset labeled semi-automatically order avoid laborious task labeling human protein localizations gathered yeast protein database ypd hodges sentences contained instances protein location pair marked positive simple computer program early exploration dataset found signi number false positives http ncbi nlm nih gov pubmed camera-goadrich tex goadrich oliphant shavlik looked true positives apparently missed automated labeling algorithm labels ambiguous nding protein word location word human-judged semantics sentence involve localization addition labeling scheme data yeast proteins corpus listed ypd issues decided relabel dataset hand current dataset found ftp ftp wisc machinelearning shavlik-group datasets ie-protein-location label positive examples manually performed rst protein location named-entity labeling relational labeling found named-entities labeled proteins locations independently proteins locations believed part relational tuple data previously divided folds abstract abstract partitioned sentences simple heuristics labeled sentence individually abstract deleted merged sentences incorrectly split previous heuristic program medical subject headings mesh biological words unknown time labeling labeling standards groups focus nding general words describing proteins task extract locations speci yeast proteins disagreement labelers tag protein location make training set precise expense recall protein labeling strove speci general labeled words directly referred protein gene molecule included gene names smf protein names fet full chemical names enzymes -cytochome reductase label sec sec mutant label isp delta rrp gene products defective give rise functioning protein molecule label protein families hsp adjective protein hsp dnak fusion proteins gene combined uorescent tag labeled proteins protein complexes antibodies open reading frames labeled positive protein examples proteins exist yeast labeled found species dataset dealt localization yeast proteins labeling location words direct list cellular locations listed introductory cellular biology text book becker including locations abbreviations cytoskeleton membrane lumen npc bud labeled location adjectives nucleoporin ribosomal protein location pair labeled co-occurrence ambiguous tuple clear tuple co-occurrence protein location occurred sentence http nlm nih gov mesh meshhome html camera-goadrich tex gleaner ensembles clauses improve recall-precision curves belong tuple sentence imply protein found marked location ambiguous sentence evidence localization information needed surrounding context determine automatically tuple present clear sentence directly implies relationship clear relationships directly stated text protein location yrb cytosol sentence yrb located cytosol ambiguous relationships protein location implied stated protein location lip mitochondrial sentence lip mutants undergo high frequency mitochondrial dna deletions co-occurrence pairing protein location relationship context sentence protein location erd sentence cells lacking erd gene secrete endogenous protein bip classi cation agreed labelers experiments clear category positive examples phrase pairings negative examples unbalanced data filtering previously mentioned culties face domain large number examples dataset approximately sentences sentence approximately phrases total number phrase-phrase pairings million pairings positive leads positive negative ratio domain prior knowledge reduce number false positive examples observe positive relations noun phrases positions ratio limit size training data candidate extractions arguments noun phrases reduces positive negative ratio data necessarily track discarded positives testing set non-noun phrase record false negatives recall-precision results reduce positive negative ratio randomly under-sample negatives retaining fourth training ltering shown figure faster clause learning camera-goadrich tex goadrich oliphant shavlik pos pos pos neg neg neg sampling noun phrase filter figure lter training testing sets noun phrase lter training set 
sampling lter sentence fragment text part speech phrase type haved named yfh localizes mitochondria figure sample sentence parse sundance sentence analyzer noun verb preposition noun phrase verb phrase prepositional phrase background knowledge standard feature-vector machine learning setup mitchell ilp logical relations describe data algorithms attempt construct logical clauses based background structure separate positive negative examples information-extraction task construct background knowledge sentence structure statistical word frequency lexical properties biomedical dictionaries table shows sample sentence resulting prolog facts create capture structure semantics rst set predicates sentence structure sundance sentence parser rilo derive parse tree sentences dataset part-of-speech words phrases tree atten tree nested phrases phrases sentence root words members phrase figure shows sample sentence parse divided level phrases word phrase sentence unique identi constant based ordering abstract create predicates relate sentences phrases words based actual text document structure sentence child phrase previous word tree structure sequence words predicates camera-goadrich tex gleaner ensembles clauses improve recall-precision curves table translation sample sentence yrb located cytosol prolog sentence abstract pubmed facts created listed background prolog facts created knowledge sentence sentence sen structure phrase sen phrase sen word sen word sen word sen phrase child sen sen word sen sen word string sen yrb target arg target arg sen part segment sen speech segment sen unk sen cop sen sen medical phrase mesh term sen cytosol ontologies phrase meddict term sen cytosol phrase term sen cytosol lexical phrase alphabetic word sen properties phrase specific word sen phrase originally leading cap sen word phrase arg word sen frequency phrase arg word sen nounphrase article verb describe part speech structure include actual text sentence background knowledge predicate word string maps identi ers word constants addition words sentence stemmed porter stemmer porter stemmed version words group background predicates frequency words appearing target phrases training set per-fold basis prevent learning test set created boolean predicates ratios times times times general word frequency abstracts training set formula determine words matched ratios ratio wordjwi target phrase wordjwi target phrase words body npc membrane times location phrases phrases general training set gradations calculated target arguments protein location words frequently camera-goadrich tex goadrich oliphant shavlik arguments create semantic classes consisting high frequency words semantic classes mark occurrences words training testing set source background knowledge derived lexical properties word alphanumeric words numbers alphabetic characters yrb lip alphabetic words alphabetic characters lexical morphological features include singlechar hyphenated -cytochrome capitalized nip words classi novelword phosphatidylinositol standard usr dict words dictionary unix fourth source incorporate semantic knowledge biology medicine background knowledge medical subject headings mesh gene ontology online medical dictionary sentence structure simpli hierarchies level picked categories mesh protein peptide cellular structure cellular-localization category cellular-biology category online medical dictionary phrases labeled predicates phrase mesh term phrase term words phrase match words category table word cytosol found categories labeled sentence-structure predicates word phrase added allowing navigation parse tree phrases tagged rst phrase sentence likewise words length phrases calculated explicitly turned predicate length words phrases sentences phrases classi short medium long additional piece information predicate phrases true arguments distinct phrases lexical predicates augmented make applicable phrase level phrase alphabetic word phrase predicate phrase alphabetic word similarly phrases marked predicate actual word text phrase specific word lumen equivalent adding phrase child word string lumen predicates created pairs triplets words assert phrase word golgi labeled noun step search hypothesis space finally predicates added denote ordering phrases target arg target arg asserts protein phrase occurs location phrase similarly target arg target arg created adjacent target args true protein location phrases adjacent sentence phrase http nlm nih gov mesh meshhome html http geneontology http cancerweb ncl omd camera-goadrich tex gleaner ensembles clauses improve recall-precision curves nucleoporin npl identical target args noun phrase protein location count phrases target arguments ned predicates describing training examples informally evaluated impact types background knowledge table performing subsets background knowledge standard run aleph computing score fold testset theory created rst clauses aleph induces background knowledge results score leaving mesh-related biomedical features produces leave lexical part-of-speech properties words drops leave features related statistics words shrinks experimental controls main experimental control work aleph ensembles discussed addition compare weighting methods call singletheory ensembles bayes structural hmms aleph ensembles bagging breiman popular ensemble approach machine learning multiple classi ers trained erent subsamples training data introduces bias learned hypothesis training set classi ers vote classi cation testset examples majority class selected output classi cation vote user-dependent common schemes equal voting weighted tuneset accuracy voter dietterich bagging create con dence scores percentage classi ers voting majority class general con dence score ensemble calculated summing result weight classi multiplied prediction bagging ilp previously investigated dutra demonstrate bagging helpful modest improvements accuracy straight-forward calculate con dence boosting freund schapire learns multiple classi ers erent method produce diverse classi ers examples initially assigned uniform weight classi ers learned sequentially classi misclassi cations training set examples examples classi incorrect up-weighted add emphasis correct examples down-weighted forcing subsequent classi ers focus harder harder examples additionally potential classi score based covers examples higher scores correlated correctly covering highly weighted camera-goadrich tex goadrich oliphant shavlik examples long classi greater accurate ciently erent classi ers boosting theoretically converge highly accurate classi quinlan investigated boosting relation combining theories learned foil investigate random seeds approach creating ensembles dutra approach shown essentially equivalent predictive accuracy bagging breiman produces diversity learned models starting run underlying ilp system erent seed compare gleaner approach section random seeds aleph experimental control call aleph times create theories sets clauses cover positive training examples negative create recall-precision curve theories simply classify positive theories classify positive varying produces family ensembles ensembles produces point recall-precision curve discussed earlier aleph exible ilp system wide variety learning parameters modi cation train test sets fold protein-localization dataset choose good parameter settings experimental control compare gleaner algorithm fair testset tune parameters limit number clauses considered thousand seed processed limit number reductions million call counting predicate yap prolog explicitly stated parameter choices made initially empirically tuned major aleph parameters minimum accuracy place lower bound accuracy clause learned system note accuracy clause positive examples words precision settings minimum accuracy learned clauses 
minimum positives prevent aleph learning overly narrow clauses cover examples acceptable clause cover number positives require clauses cover positive examples clause length size clause constrained clause length limiting length explore wider breadth clauses prevent clauses speci required clauses longer ten literals including head settings random sampling hypothesis space gleaner approach search strategy aleph user choose search function include standard search methods breadthrst search http ncc vsc yap yap html camera-goadrich tex gleaner ensembles clauses improve recall-precision curves depthrst search iterative beam search iterative deepening heuristic methods requiring evaluation function heuristic search scales large size task investigated number erent evaluation functions evaluation function ways calculate node exploration default heuristic aleph coverage ned number positives covered clause minus number negatives highly skewed domain coverage bias search clauses cover small number false positives matter true positives cover similar heuristic compression coverage minus length clause compression biases search minimum description-length hypothesis rissanen shorter clause improve clause quality correct accuracy estimates clauses cover small number examples laplace estimate working domains generate precision recall curves explored heuristicsearch evaluation function precision recall -score precision recallprecision recall metrics provide balance precision recall clause coverage coverage tune set encourage clauses general added parameter aleph requiring recorded clause cover positive examples tuneset clauses unseen examples test set low computational overhead training parameter evaluations fold obtained area recall-precision curve aleph ensembles laplace evaluation function minimum clause accuracy shown table setting average number clauses considered constructed theory approximately nding encountered reported dutra limit size theories figure plots aurpc function maximum number clauses learned theories running aleph normal completion parameters leads theories clauses average limit rst clauses aurpc drastically reason larger theories diversity smaller diversity key ensembles dietterich subsequent experiments stop clause learning theory clauses convenient side-e ect limiting theory size runtime individual aleph executions substantially reduced considered parameter settings algorithm designs aleph create ensemble theories evaluated substantial number variants feel chosen camera-goadrich tex goadrich oliphant shavlik table aurpc results testset fold protein-localization dataset clauses theory theories minimum heuristic accuracy function aurpc laplace coverage -score precision recall laplace coverage -score precision recall urpc number clauses theory figure aurpc aleph ensembles varying number clauses protein-localization dataset settings provide satisfactory experimental control compare algorithm gleaner single-theory ensembles mentioned earlier theory learned inductive logic programming viewed ensemble disjunction clauses view extended exploring weight uence clause classi cation standard theory interpretation clauses positive weight decision threshold set approach call single-theory ensemble achieve camera-goadrich tex gleaner ensembles clauses improve recall-precision curves results examines fewer clauses aleph ensembles approach explore weighting schemes comparison gleaner input output fawcett compares number propositional-rule weighting methods relation area curve auc performance number erences dataset examined fawcett ilp learn clauses cover positive class propositional-rule learners examined earlier fawcett rules positive negative examples reason unable compare number weighting schemes data highly skewed negative examples fawcett previous work examined datasets fairly balanced distribution nal erence note aurpc recall-precision curves auc roc curves determine score test investigated methods protein-localization dataset tuneset gather statistics ranked list method treats theory list clauses ordered m-estimate precision clause tuneset testset score generated nding set clauses cover score highest-scoring clause fawcett calls method employed craven slatterly ilp lowest false positive rate lfpr fawcett proposed schemes similar ranked list method falsepositive rate tuning data m-estimate score clause lowest highest-ranked clause compared unordered rule resolution method mentioned clark boswell propositional-rule learner set clauses match found separately sum true positives false positives matching clause tuneset score assigned resulting aggregated precision weighted vote lines fawcett weighted vote method rst nds precision matching clause score average precision scores matching clauses cumulative class weighting schemes examined size set matching clauses score single-theory ensemble approach partially inspired blockeel dehaspe proposal cumulativity ilp call method equal weighting clause vote score number matching clauses explored methods determine weight clause camera-goadrich tex goadrich oliphant shavlik weighting schemes precision diversity equal -score recall bayes tan ranked list lfpr weighted vote figure comparison aurpc weighting approaches protein-localization datasets error bars standard deviation folds vote precision recall score clause diversity metric adapted opitz shavlik bayes tan method rst learning theory learning weights combine feature selection propositionalization compare propositional learners discussed davis bayes tree augmented networks tan friedman augments bayes account dependence features compared erent weighting schemes protein localization set good weighting scheme control experiment gleaner standard aleph learn theories training set fold minimum accuracy setting maximum nodes setting learned theories fold averaged learned clauses figure shows results erent weighting schemes protein localization task ordered performance leftmost columns cumulative weighting schemes propositional learning methods ranking schemes averaging schemes experiments found highest scoring scheme cumulative weighting precision fact cumulative weighting schemes outscored approaches erence bayes tan cumulative schemes barely statistically signi slightly results contrast fawcett found lfpr weighted vote scored equally ranked list lagged noted experiments involve protein-localization dataset camera-goadrich tex gleaner ensembles clauses improve recall-precision curves additional controls compare results ray craven structural hmms retrained evaluated cleaned dataset propositional bayes approach text classi cation found mitchell ray craven hmm approach phrase protein location pif pif counted multiple times part positive relation due erent problem representation hmm approach slightly positive examples ilp framework examples based phrase constants constituent words bayes created feature sets bag words phrases relation bags words phrase relation words target phrases mitchell m-estimate equation ocabularyj values found results relational features experiment stemmed words sentences experimental results main hypothesis dividing recall-precision area collecting clauses combining clauses theories quickly theories high area recall-precision curve explore hypothesis experiments biomedical information extraction domains protein localization dataset divided protein-localization data folds training set consisted folds fold held tuning testing experiments require clause learned training set cover positive examples tuning set gleaner tuning set pick threshold bin sample clause chosen gleaner shown table clause picked tendency protein phrase alphanumeric words location part sentence words previously marked locations training set familiar pattern starting article important clause sentence structure requiring protein phrase location phrase location phrase phrase sentence aleph-based method producing ensembles parameters vary number theories size ensemble number clauses theory produce ensemble points experiments choose average camera-goadrich tex goadrich oliphant shavlik table sample clause recall precision testset protein location target arg target arg 
rst word phrase phrase art phrase marked location phrase alphanumeric words phrase alphanumeric words sentence target phrases variable protein phrase location phrase sentence variables clause positive extraction npl encodes nuclear protein rna recognition motif similarities family proteins involved rna metabolism protein location npl nuclear protein negative extraction false positive subcellular fractionation studies demonstrate amino acid vps peripherally cytoplasmic face late golgi vesicle compartment protein location amino acid vps cytoplasmic face nodes explored clause learned extend analysis lower numbers clauses generated choose compare experiments scenario drastically limit nodes explored experiment nodes approximately seed examples theory result singletons unable learn suitable clause time allowed wasted clause evaluations factored comparisons attempts limit nodes explored resulted approximately singletons theory singletons factored learning time expensive limit nodes compare gleaner single-theory ensembles cumulative precision weighting performed highest weighting schemes make comparison competitive limited maximum nodes learned clause theory calculated aurpc points rst clauses complete theory parameters gleaner equal-sized recall bins rapid random restart zelezn precision recall heuristic function construct clauses derived initial random clause restarting random clause generate aurpc data points gleaner choosing seed examples values number candidate clauses generated seed reduce number seed camera-goadrich tex gleaner ensembles clauses improve recall-precision curves number clauses generated logarithmic scale aleph ensembles aleph ensembles gleaner precision weighting figure comparison aurpc gleaner aleph ensembles varying number clauses generated examples explore performance lower numbers clauses generated results comparison found figure points averaged folds note graph logarithmic scale number clauses generated gleaner comparable aurpc numbers generating orders magnitude fewer clauses aleph ensembles nodes learned clause aleph ensembles improve limited nodes learned clause gleaner order magnitude faster interesting note gleaner curve consistent number clauses allowed aleph ensemble method increases clauses considered demonstrates bene saving clause searching hypothesis space showing gleaner resistant tting figure gleaner order magnitude faster method weighting theory single-theory ensembles employ covering algorithm halts learning positive examples singletons covered clause explore behavior large numbers considered clauses note gleaner aleph ensembles executed parallel give large savings running time theory-weighting method learns clauses sequentially figures show comparison curves gleaner aleph ensembles number total clauses evaluated results generated averaging precision folds equally-spaced recall values clauses bene saving high-recall clauses gleaner quickly spans recall-precision space aleph ensembles initially limited recall ability aleph ensembles achieve higher recall camera-goadrich tex goadrich oliphant shavlik recall gleaner aleph ensembles curves clauses gleaner aleph ensembles aleph ensembles recall curves clauses gleaner aleph ensembles recall curves clauses figure comparison curves gleaner aleph ensembles numbers clauses generated curves averaged folds precision clauses major bene gleaner increased precision low high recall gleaner clauses theoretically produce higher precision individual clauses recall long coverage positives greater coverage negatives clauses independent practice clauses independent tendency cover negatives true high-recall bins learned clauses identical overlap degrades performance results comparing structural hmms bayes shown table vii bayes performs slightly random guessing domain partially due relational nature dataset protein phrase positive repeated negative examples correctly paired location phrase protein words classi testset receive data-free m-estimate score hmm approach camera-goadrich tex gleaner ensembles clauses improve recall-precision curves table vii aurpc results averaged folds protein-localization dataset bayes hmm aleph ensembles single-theory ensembles gleaner aleph ensembles single-theory ensembles gleaner right-most point curve figure learning algorithm testset aurpc bayes bags bayes bags structural hmm single-theory ensembles aleph ensembles gleaner number clauses generated logarithmic scale gleaner aleph ensembles figure comparison aurpc gleaner aleph ensembles varying number clauses generated genetic-disorder dataset ray craven fares ers low recall achieving highest recall testset fold genetic disorder dataset finally evaluate gleaner genetic-disorder biomedical information extraction dataset ray craven original labelings folds construct background knowledge fashion protein-localization dataset substituting mesh categories related diseases due memory size limitations yap prolog uniformly sampled abstracts fold ray craven create dataset left positive negative examples compare aleph ensembles gleaner algorithm parameter settings previous experiment camera-goadrich tex goadrich oliphant shavlik figure shows comparison results genetic-disorder dataset gleaner consistently achieves higher aurpc aleph ensembles values number candidate clauses notice gleaner consistently improves clauses examined reaching maximum aurpc score compared aleph ensembles peak gleaner performance clauses bene pruning clauses found gleaner point found seeds clauses generated seed domain early stopping clauses theory improve nal aurpc aleph ensembles show data points completeness related work applied gleaner algorithm information extraction task addition article discusses ways single-theory viewed ensemble clauses erent methods assigning weights clauses section related work gleaner algorithm domains information extraction work focused named-entity recognition successful rule-based approaches task include rapier cali mooney system learns clauses format pre extraction post boosted wrapper induction bwi freitag kushmerick method boosting weak rule-based classi ers extraction boundaries powerful extraction method bwi examined kauchak showing results high recall high precision wide variety tasks previous machine learning work biomedical multi-slot domain includes number erent approaches ray craven hidden markov model hmm modi include part speech tagging analyze method protein localization genetic disorder protein-protein interaction tasks probabilistic approach achieves high precision low areas recall maximum recall limited limitation approach culty adding background knowledge hypothesis space human interpretability results datasets eliassi-rad shavlik implemented neural network primed domain-speci prior knowledge achieved signi improvements recall precision work ray craven approach background knowledge directly incorporated form initial weights structure network resulting neural-network structure weights uninterpretable sequential nature data required awkward sliding window implementation fully capture desired background knowledge camera-goadrich tex gleaner ensembles clauses improve recall-precision curves aitken foil quinlan perform ilp dataset working closed ontology entities means relationships learned previously identi objects bagof-words representation sentence type information incorporate semantic knowledge data results limited small dataset recall-precision results point opposed analysis curve bunescu propose extraction longest common subsequence elcs bottom-up approach nding protein interactions rule templates sentences greedy covering algorithm repeatedly generalize sentence templates templates found cover positive examples bunescu extended rapier cali mooney bwi freitag kushmerick handle multislot extractions approaches assume named entity recognizer label proteins involved relationship thing notably absent incorporation background knowledge part speech word properties results show extension recall elcs modi rapier bwi algorithms fared poorly protein-interaction domain due low recall single-theory ensembles typical approach weighting theory propositionalization clause theory translated boolean feature number propositional learning algorithms learning weights output input output input output input output input networks resulting networks original crossed clause pompe kononenko bayes classi 
weights srinivasan king logistic regression technique weights maximize likelihood data koller pfe learn weights clauses theory rst creating bayesian network model theory expectation maximization algorithm set parameters maximize likelihood data results toy dataset clauses unknown extend large datasets propose investigate richardson domingos extend work relational markov networks taskar formulate markov logic networks setup clauses ilp domain expert translate markov network learn weights clauses logistic regression davis compare bayes tan sparse candidate algorithm alternate methods learning weight parameters methods attempt modify learned theory weights urnkranz flach explore ect erent heuristic functions resulting learned theories researchers made progress learning clauses probabilities concert hoche wrobel implement version boosting foil sequentially learn clauses examples covered clause reweighted positive examples down-weighted negative examples up-weighted camera-goadrich tex goadrich oliphant shavlik clause learned con dence score created clause based current weights clause coverage popescul proposed upgrade logistic regression called structural logistic regression slr top-down search ilp hypothesis space aggregates gather clauses weighted logistic regression srinivasan king slr guides search clauses improve current aic score compare algorithm data representation methods learning weights clauses landwehr davis concurrently proposed nfoil sayu algorithms scoring features based contribution growing bayesian classi muggleton adds probability distribution ground predicates incorporates methods learning probabilities data muggleton common approach learning weights clauses perform statistical relational learning srl combines probabilistic bene bayesian networks structural data representation ilp friedman approach problem database perspective upgrade bayesian networks operate aggregate relational database schema made progress learning parameters joint probability tables correct database structure kersting raedt propose bayesian logic program framework show logical probabilistic approaches expressed common formalism conclusions future work eld inductive logic programming matured point large real-world problems formulated attempted research addresses issue unbalanced data frequently arises large datasets main strengths gleaner algorithm ability quickly retain large number clauses wide range performance areas ability combine collected clauses separate theories providing high precision full recall range major bene gleaner erent seeds rapid random restart search helps achieve unique clauses low-recall bins search space constrained cover seed necessarily overlap clauses learned erent seeds noticed lack diversity collected clauses high-recall bins gleaner semantically duplicate clauses identical coverage training set removed left clauses bin low recall bins retain clauses hampers theories ability achieve higher recall plan investigate ways keeping diverse clauses bins gleaner clause seed recall bin saving clauses give exibility increase camera-goadrich tex gleaner ensembles clauses improve recall-precision curves aurpc clauses processed remember rapid random restart search selects random clause performs heuristic search set number moves jumping place hypothesis space experiments searched clauses jumping random clause retain larger number clauses record found bin seed jump jump examine clauses erent area hypothesis space alternate ways save clauses store ten highest scoring clauses bin seed increase number bins training phase create diversity seed addition seeds clauses gathered gleaner algorithm limited rapid random restart approach direct search clauses areas recall byproduct generating clauses high-recall bins active approach needed high-recall clauses tend general found beginning top-down searches additional literal added clause decrease positive negative coverage lower recall propose incorporate gleaner top-down approach breadthrst search heuristic search search strategy guided nding general speci clauses original gleaner algorithm clauses sorted recall bins possibility statistics gathered searching hypothesis space uence choice initial clause jump encouraging search seed unexplored clauses bottom clause search diverse high-recall clauses relevant searching low-recall clauses low-recall bins bottom clause bias gleaner diverse clauses nition clause covers seed recall cover positives leaving ample room erent clause found erent seed high-recall bins recall clause clause approximately positive examples limiting diversity search bias search space avoid randomly selected subset negative examples plan explore heuristic functions negative examples cover giving high cost encourage clauses cover positive examples parallel erent subsets negatives end result large number high-recall clauses cover erent negative examples combined produce higher precision high recall plan examine information-extraction datasets part future research include protein-interaction datasets ray craven bunescu reported results recent learning language logic challenge task dataset goadrich datasets gleaner camera-goadrich tex goadrich oliphant shavlik include nuclear smuggling dataset tang social network dataset taskar citeseer citation dataset popescul relation dataset richardson domingos datasets link-learning tasks relevant ilp plan compare datasets severe skew gleaner general-purpose algorithm recall-precision type problems balanced-data problems gleaner approach modi partition roc space recall-precision space techniques optimizing recall-precision graph applicable ilp plan explore adapt gleaner methodology work propositional datasets straightforward translation clark niblett aleph clause-learning engine finally plan explore aurpc evaluation metric current methods comparing algorithms recall-precision curve uninterpolated average precision uap manning sch utze maximum -score uninterpolated average precision essentially aurpc smoothed interpolation recall point preliminary work davis goadrich demonstrate similarities erences roc curves formally answer questions optimizing roc curve optimize recall-precision curve made existing algorithms focus aurpc acknowledgements gratefully acknowledge funding usa nlm grant usa nlm grant usa darpa grant usa air force grant ines dutra vitor santos costa condor group soumya ray marios skounakis david page jesse davis anonymous reviewers helpful comments research article extended version goadrich protein-localization dataset discussed article downloaded ftp ftp wisc machine-learning shavlik-group datasets ieprotein-location aitken learning information extraction rules inductive logic programming approach proceedings european conference arti cial intelligence amsterdam becker reece poenie world cell benjamin cummings camera-goadrich tex gleaner ensembles clauses improve recall-precision curves blaschke hirschman valencia information extraction molecular biology brie ngs bioinformatics blockeel dehaspe cumulativity inductive bias pkdd workshop data mining decision support meta-learning ilp lyon france bradley area roc curve evaluation machine learning algorithms pattern recognition breiman bagging predictors machine learning bunescu kate marcotte mooney ramani wong comparative experiments learning information extractors proteins interactions journal arti cial intelligence medicine cali mooney relational learning pattern-match rules information extraction working notes aaai spring symposium applying machine learning discourse processing menlo park aaai press clark boswell rule induction recent improvements proceedings european working session machine learning porto portugal springer-verlag york clark niblett induction algorithm machine learning cortes mohri auc optimization error rate minimization neural information processing systems nips mit press craven slattery relational learning statistical predicate invention models hypertext machine learning davis burnside dutra page costa integrated approach learning bayesian networks rules european conference machine learning springer davis dutra page costa establish entity equivalence multi-relation 
domains proceedings international conference intelligence analysis vienna davis goadrich note precision-recall curves technical report wisconsin madison camera-goadrich tex goadrich oliphant shavlik castro dutra page costa shavlik empirical evaluation bagging inductive logic programming twelfth international conference inductive logic programming sydney australia dietterich machine-learning research current directions magazine zeroski lavrac introduction inductive logic programming relational data mining springer-verlag eliassi-rad shavlik theory-re nement approach information extraction proceedings international conference machine learning fawcett rule sets maximize roc performance ieee international conference data mining icdm fawcett roc graphs notes practical considerations researchers technical report labs hpl- freitag kushmerick boosted wrapper induction proceedings national conference arti cial intelligence aaai freund schapire experiments boosting algorithm international conference machine learning friedman geiger goldszmidt bayesian network classi ers machine learning friedman getoor koller pfe learning probabilistic relational models proceedings international conference arti cial intelligence ijcai urnkranz separate-and-conquer rule learning arti cial intelligence review urnkranz flach roc rule learning understanding covering algorithms machine learning goadrich oliphant shavlik learning ensembles firstorder clauses recall-precision curves case study biomedical information extraction proceedings international conference inductive logic programming ilp porto portugal goadrich oliphant shavlik learning extract genic interactions gleaner proceedings learning language logic workshop international conference machine learning bonn germany camera-goadrich tex gleaner ensembles clauses improve recall-precision curves hoche wrobel relational learning constrained con dence-rated boosting international conference inductive logic programming strasbourg france hodges payne garrels yeast protein database ypd curated proteome database saccharomyces cerevisiae nucleic acids research hoos stutzle stochastic local search foundations applications morgan kaufmann guidelines protein tagging technical report georgetown kauchak smarr elkan sources success boosted wrapper induction journal machine learning research kersting raedt bayesian logic programs proceedings work-in-progress track international conference inductive logic programming koller pfe learning probabilities noisy first-order rules fifteenth international joint conference arti cial intelligence ijcai nagoya japan landwehr kersting raedt nfoil integrating naive bayes foil national conference arti cial intelligene aaai lewis evaluating text categorization proceedings speech natural language workshop morgan kaufmann manning sch utze foundations statistical natural language processing mit press michalski larson inductive inference decision rules proceedings workshop pattern-directed inference systems hawaii mitchell machine learning york mcgraw-hill muggleton inverse entailment progol generation computing journal muggleton stochastic logic programs proceedings international workshop inductive logic programming department computer science katholieke universiteit leuven muggleton learning stochastic logic programs proceedings aaai workshop learning statistical models relational data nilsson maluszy nski logic programming prolog john wiley sons camera-goadrich tex goadrich oliphant shavlik opitz shavlik actively searching ective neuralnetwork ensemble connection science pompe kononenko naive bayesian classi ilpr international workshop inductive logic programming popescul ungar lawrence pennock statistical relational learning document mining ieee international conference data mining icdmporter algorithm stripping program quinlan learning logical nitions relations machine learning quinlan relational learning boosting relational data mining springer-verlag ray craven representing sentence structure hidden markov models information extraction proceedings international joint conference arti cial intelligence ijcai richardson domingos markov logic networks machine learning rilo sundance sentence analyzer http utah projects nlp rissanen modeling shortest data description automatica uckert kramer stochastic local search k-term dnf learning proceedings international conference machine learning icmlwashington usa uckert kramer tight bounds rule learning proceedings international conference machine learning icmlban canada uckert kramer raedt phase transitions stochastic local search k-term dnf learning proceedings european conference machine learning ecmlhelsinki finland selman kautz cohen local search strategies satis ability testing proceedings dimacs challange cliques coloring satis ability providence shatkay feldman mining biomedical literature genomic era overview journal computational biology camera-goadrich tex gleaner ensembles clauses improve recall-precision curves srinivasan aleph manual version http web comlab oucl research areas machlearn aleph srinivasan king feature construction inductive logic programming study quantitative predictions biological activity aided structural attributes proceedings international workshop inductive logic programming stockholm royal institute technology srinivasan muggleton sternberg king theories mutagenicity study first-order feature-based induction arti cial intelligence tang mooney melville scaling ilp large examples results link discovery counter-terrorism kdd workshop multi-relational data mining taskar abbeel wong koller label link prediction relational data ijcai workshop learning statistical models relational data zelezn srinivasan page lattice-search runtime distributions heavy-tailed proceedings international conference inductive logic programming syndey australia zelezn srinivasan page monte carlo study randomized restarted search ilp proceedings international conference inductive logic programming ilpporto portugal camera-goadrich tex camera-goadrich tex 
decrease false negatives decrease false positives node node node node node node newnode existing node node decrease false negatives decrease false positives node node node node node node newnode existing node node mhz internal nodes validation-set fidelity test-set fidelity test-set accuracy profit loss trading days network trepan naive predictor profit loss trading days -ofid -ofreduced feature set reduced feature set output input output input output input output input networks resulting networks original crossed roi yen future interest rate false dem future dem future sfr future dem usd dem future dem usd sfr future dem future sfr future sfr future dem future 

promoters splice junctions ribosome binding sites kbann topgen regent test set error networks considered splice junctions promotersr key topgen standard regent kbann test set error test set error networks considered promoters splice junctions mutation mutation mutation mutation ribosome binding sites great life meaning universal turing machine advice behavior reinforcement action state learner environment observer actions sensor inputs hidden units current hidden units advice hidden units parser language interface advice mapper rule network advice reformulated advice observer agent behavior environment action operationalizer 

iii movenorthmoveeast state enemy west obstacle north moveeast movenorth state inputs outputs surrounded moveeastpusheast oktopusheast enemy inputs pusheast moveeast definition definitionsurrounded surrounded hidden units action moveeast pusheast movenorth previous action moveeast noaction pusheast agent enemy obstacle empty key food sensor inputs actions sector wall empty food enemy obstacle occluded sector wall empty food enemy obstacle occluded number training episodes average cumulative testset reinforcement simplemoves number training episodes elimenemies number training episodes average cumulative testset reinforcement nonlocalmoves number training episodes surrounded advice advice episodes advice episodes advice episodes midvale blvd nakoma ave park beltline work home technical report computer sciences department wisconsin madison nov framework combining symbolic neural learning jude shavlik computer sciences department wisconsin madison shavlik wisc abstract article describes approach combining symbolic connectionist approaches machine learning three-stage framework presented research groups reviewed respect framework stage involves insertion symbolic knowledge neural networks addresses refinement prior knowledge neural representation concerns extraction refined symbolic knowledge experimental results open research issues discussed keywords knowledge-based neural networks theory refinement prior knowledge rule extraction neural networks kbann algorithm nofm algorithm shorter version paper machine learning framework combining symbolic neural learning jude shavlik computer sciences department wisconsin madison introduction ten years produced explosion amount research machine learning rapid growth occurred largely independently symbolic connectionist neural network machine learning communities fortunately years communities separate increasing amount research considered hybrid approaches paper reviews research combines symbolic neural network approaches artificial intelligence presents framework combining paradigms attempt define precisely essential differences symbolic connectionist approaches lead lengthy debate scope article distinction needed make coarse approximation symbolic approaches focus producing discrete combinations features neural approaches adjust continuous non-linear weightings inputs assume understanding fundamental differences paradigms future research issue focus research incorporates traditionally considered aspects camps large number ways combine symbolic connectionist utgoff developed algorithm closely integrates decision trees perceptrons loosely-coupled hybrid system high level decisions made symbolically low level made neural networks gallant pomerleau gowdy thorpe recent special issues journals hendler hinton present additional approaches attempting comprehensive review symbolic connectionist hybrid methods explored focus framework figure illustrates framework learner inserts symbolic information sort neural network increasingly clear learner make effective prior knowledge order perform geman bienenstock doursat neural representation training examples refine initial knowledge finally extracts symbolic information trained network research groups fits nicely framework promising results achieved remainder paper discusses research points open issues phases continuing noted steps independent researchers studied combinations remainder article organized questions neural networks symbol-oriented learning tasks review research addresses questions figure phases insertion refinement extraction symbolic information page combining symbolic neural learning insert initial neural network symbolic information final extract neural network final symbolic information initial refine examples figure framework combining symbolic neural learning neural networks symbol-oriented learning tasks avoid connectionist methods learn tasks inherently deal symbols neural networks primarily applicable low-level perceptual tasks argue section answer related questions years starting papers published simultaneously ijcaifisher mckusick mooney weiss kapouleas studies atlas dietterich hild bakiri groups empirically compared symbolic learning algorithms quinlan decision-tree algorithm connectionist approaches rumelhart hinton williams backpropagation method training neural networks studies produce consistent results coarse summary trained neural networks comparable accuracies induced decision trees tasks considered symbol oriented appears worthwhile investigate neural learning methods produce refine symbolic information addition neural network approaches proven successful wide range real world tasks speech understanding lippmann handwritten-character recognition cun control dynamic systems jordan rumelhart gene finding uberbacher mural language learning touretzky experiments strongly suggest connectionist learning powerful approach neural networks symbolic knowledge merits exploration finally important note connectionist architectures simple feed-forward single-hidden-layer neural networks recurrent networks elman jordan feedback loops memory appealing application symbolic tasks sequential nature page combining symbolic neural learning symbolic information neural networks assuming convinced merit figure framework techniques inserting symbolic information neural network needed preexisting information prior knowledge task hand question neural networks effectively hints abu-mostafa answer kbann approach towell shavlik noordewier towell creates knowledge-based artificial neural networks producing neural networks topological structure matches dependency structure rules approximately-correct domain theory collection inference rules current task table shows correspondences domain theory neural network figure alcontains simple approach mapping domain theory neural networks kbann applied successfully refining domain theories real-world problems gene finding towell protein folding maclin shavlik press control simple chemical plant scott shavlik ray appendix paper presents application kbann problem human genome project appendix shows complicated mapping figure reports kbann approach generalizes examples training groups found knowledge-based neural networks train faster standard neural networks berenji oliver schneider omlin giles shavlik towell initial information choose good starting point network importantly experiments shown knowledge-based networks generalize future examples standard networks methods inductive learning theory refinement omlin giles maclin shavlik press mcmillan roscheisen hofmann tresp scott towell towell tresp hollatz ahmad attribute improved generalization aspects insertion process domain theory produces inductive bias focusing attention relevant input features indicating intermediate conclusions suggest good network topology figure schematically illustrates general performance kbann system plots generalization performance testset accuracy function number examples table correspondences domain theory neural network domain theory neural network final conclusion output units intermediate conclusions hidden units supporting facts input units antecedents rule highly-weighted links page combining symbolic neural learning hgfed hgfed iii figure sample application kbann rule-insertion algorithm frame simple domain theory frame shows dependency structure rules frame shows network kbann creates thick lines iii correspond dependencies rules kbann sets weights links manner nodes highly active domain theory supports deduction thin lines frame iii represent zeroweighted links kbann adds network refinement domain theory neural training training qualitatively similar results obtained domains towell noordewier towell shavlik scott maclin shavlik press specific generalization curves domains case kbann imperfect domain theory domain-specific information kbann produced good 
starting network small numbers training examples resulted testset accuracy obtained standard artificial neural network ann initialized small random weights asymptotically testset accuracy learning systems converged experimental domains collect training examples asymptotic convergence suggests view domain theory initial knowledge worth number training examples domains collecting large number examples impossible costly protein folding utilize alternate sources information prove valuable towell shown kbann knowledge-based networks refine domain theory purely symbolic theory-refinement systems holds compares rules extracted trained network refined rules produced symbolic theory-refinement systems results provide justification complex representational shifts figure framework convert rules kbann extracts disjunctions conjunctive rules great increase number rules approaches searching hypothesis space towell empirical results page combining symbolic neural learning testset errors amount training data kbann domain theory standard ann learning figure generalization examples function number training examples problem-specific broader conclusion searching continuous weight space neural networks real-world problems searching combinatorial space discrete rules complex concepts representation simpler deeper understanding relative merits symbolic connectionist purely symbolic approaches theory refinement important open research issue addition simple propositional rules shown figure early kbann work researchers produced 
techniques mapping forms prior knowledge networks mahoney mooney map rules certainty factors berenji masuoka map fuzzy-logic rules mcmillan mozer smolensky gating networks jacobs map production rules scott roscheisen map mathematical equations demonstrating kbann approach require logic-oriented domain theories finally groups mapped generalized finite-state grammars recurrent neural networks fransconi maclin shavlik press omlin giles scott generalized finite-state grammars interesting theory-refinement community view state-dependent domain theories richer type domain theory studied subfield machine learning approaches differ kbann degrees essential idea prior knowledge decide initialize neural network domain theories studied theory-refinement literature propositional address simple classification tasks mentioned groups studying finite-state automata means expressing approximately-correct domain knowledge finite-state domain theories system memory means page combining symbolic neural learning base decisions current input summary recent inputs figure shows architecture connectionist approach refining state-dependent domain theories recurrent neural networks system bases output current input internal state world model planning problems addition calculating current output system select state state-dependent domain theory initially configure network manner analogous shown figure open questions knowledge-insertion process types prior knowledge inserted networks methods lacking inserting first-order theories years progress inductive logic programming muggleton quinlan neural networks refine rules variables devise methods dealing unbounded symbolic structures neural networks size fixed training recurrent networks provide method dealing unbounded structures pollack recursive auto-associative memories provide relevant research teaching networks recognize context-free grammars learn stack das giles mozer das unbounded structures stacks handled fixed-size networks altering resolution sense product information stored resolution equals constant state current output hidden unit topology determined domain theory current state current input system boundary figure refining finite-state domain theories page combining symbolic neural learning towell shown knowledge-based networks good deleting irrelevant information approximately-correct domain theories handle impoverished domain theories figure shows qualitative nature experiments demonstrate domain theories approximately correct prove beneficial towell added spurious antecedents rules existing domain theory found expansion corrupted domain theory lead generalization standard hidden-layer network domain theory delete antecedents domain theory lead worse testset accuracy words kbann approach discarding erroneous information discovering missing knowledge open issue knowledge-based neural networks deal domain theories incomplete return topic section converting symbolic information neural-network representation connectionist learning shown research groups leaves central question insertion phase convert symbolic knowledge learning tasks powerful numeric optimization search methods applicable testset errors amount domain theory noise standard ann drop antecedents add antecedents figure effect generalization corrupting domain theory page combining symbolic neural learning network refinement guided symbolic knowledge prior knowledge inserted network refined enhanced simple run backpropagation standard connectionist training procedure training examples ways symbolic information improve training symbolic learning methods ideas focus adjustment network weights topology alter backpropagation match symbolic nature problem section discuss approaches symbolic learning approaches presented answer domain-theory refinement knowledge-based networks addresses incorrect-theory problem explanation-based learning fact perspective initial motivation kbann research shavlik towell recently mitchell thrun proposed explanation-based non-symbolic method training neural networks reinforcement learning tasks performing symbolic inductive learning conjunction neural learning mentioned utgoff perceptron trees method algorithm applicable refinement prior knowledge recall knowledge-based networks input features fall classes mentioned domain theory domain theory imperfect ignore unmentioned input features typically connected units lowweighted links towell shavlik proposed technique symbolic inductive learning identify good input features weighted heavily found preprocessing network led generalization mentioned domain theory missing number rules mapped network small order learn missing rules additional nodes added network training opitz shavlik developed algorithm interprets networks symbolically decide add nodes standard connectionist learning motivated symbolic problems minimizing mean-squared error cross-entropy error function hinton choice knowledge-based networks towell explanation refining rules certainty factors requires activation function nodes mahoney mooney constrain weight maintain symbolic interpretation network mcmillan finally networks decay weights training hinton weights knowledge-based networks decay initial values hinton personal communication tresp encouraging network preserve knowledge initial domain theory open questions symbolic information aid refinement step detect extra nodes needed generalize places add folk wisdom backpropagation work networks layers hidden units error signal diffuse symbolic information focus back-propagated error signal deep networks deep networks occur basing network topology dependency structure rule base problem exacerbated knowledge-based networks finally prevent distributed representations hinton evolving training hidden units knowledge-based networks initially symbolic meaning page combining symbolic neural learning distributed representations undesirable advantage distributed representations summary central question refinement phase symbolic knowledge task hand guide network refinement extract symbolic knowledge trained neural networks phase figure framework involves extracting symbolic information rules trained network originally knowledge-based important rule extraction understand black box network learned network produced discovery nice made explicit trained system produce explanations future decisions finally manipulate results learning system planner people developed methods extracting rules standard networks gallant saito nakano proposed algorithms ways node weighted input exceed threshold convert situations rule approaches require exponential number rules terms number network weights re-represent node towell shavlik developed method produces rule node found algorithm extracted comprehensible rules maintaining accuracy trained network approach works knowledge-based networks requires weights cluster groups soft-weight sharing technique nowlan hinton improve performance towell shavlik algorithm standard networks finally mcmillan simply project trained nodes closest valid rule hayashi extracts small number fuzzy-logic rules trained network figure sketch towell shavlik nofm algorithm algorithm extracts rules trained knowledge-based networks assumes nodes trained network values boolean constraint easily achieved steepening sigmodal activation function final stages training step upper middle panel figure clusters incoming weights unit standard clustering algorithm hartigan weights links cluster replaced average cluster weights regularizes network algorithm analyzes cluster determine clusters irrelevant figure cluster average weight discarded due boolean constraint stated activation unit qualitatively effected settings nodes simple case analysis demonstrates active maximum weighted input node active minimum weighted input node case specific activations impact final step rewrite regularized simplified node rule shown bottom panel figure nofm algorithm produce rules simple clean figure towell shavlik real-world figure illustrate essential aspects nofm rule-extraction technique page combining symbolic neural learning rewrite initial unit bias reduce bias bias cluster average figure sketch nofm rule-extraction algorithm methods analyze weights nodes cleermans servan-schreiber mcclelland giles perspective investigate extracting finite-state automata recurrent networks methods focus activation patterns hidden units approaches assume patterns represent sort internal state extraction algorithms cluster patterns view cluster state automaton step runs training examples trained 
network obtain state transitions traditional algorithms minimize automaton major question rule extraction measure comprehensibility extraction algorithm produce comprehensible rules good measure hard compare alternative approaches open issue relates refinement phase task altered support rule extraction possibly network constrained lie comprehensible portion weight space related hidden units knowledge-based networks generally symbolic names attached labels extracted rules ensure symbol-node correspondence altered training reason formation distributed representations training harmful finally conceptually clustering hidden-unit activations promising area symbolic machine learning finding describing clusters provide insight distinctions made network sejnowski rosenberg manually analyzed clusters developed nettalk task success page combining symbolic neural learning wrap section central question rule extraction extract small comprehensible symbolic version trained network losing accuracy conclusion connectionist machine learning proven fruitful approach makes sense investigate systems combine strengths symbolic connectionist approaches past years researchers successfully developed number systems article summarizes view endeavor framework encompasses approaches research groups framework figure views combination symbolic neural learning three-stage process insertion symbolic information neural network partially determining topology initial weight settings network refinement network numeric optimization method backpropagation possibly guidance symbolic knowledge extraction symbolic rules accurately represent knowledge contained trained network components form appealing complete picture approximately-correct symbolic information more-accurate symbolic information stages independently studied conclusion research summarized paper demonstrates combining symbolic connectionist methods promising approach machine learning acknowledgements material article based invited talk presented international machine learning conference held aberdeen scotland geoff towell mick noordewier rich maclin gary scott mark craven dave opitz derek zahn charlie squires kevin cherkauer members wisconsin machine learning research group time major contributors ideas presented chapter discussions ray mooney tom dietterich geoff hinton substantially influenced discussion work partially supported office naval research grant -jnational science foundation grant iriand department energy grant de-fg atlas cole connor el-sharkawi marks muthusamy barnard performance comparisons backpropagation networks classification trees real-world applications advances neural information processing systems vol touretzky san mateo morgan kaufmann abu-mostafa learning hints neural networks journal complexity berenji refinement approximate reasoning-based controllers reinforcement learning proceedings eighth international machine learning workshop evanston morgan kaufmann cleermans servan-schreiber mcclelland finite state automata simple recurrent networks neural computation page combining symbolic neural learning das giles sun hints successfully learn context-free grammars neural network pushdown automaton advances neural information processing systems vol hanson cowans giles eds san mateo morgan kaufmann dietterich hild bakiri comparative study backpropagation english textto-speech mapping proceedings seventh international conference machine learning austin morgan kaufmann elman finding structure time cognitive science fisher mckusick empirical comparison back-propagation proceedings eleventh international joint conference artificial caintelligence detroit morgan kaufmann frasconi gori maggini soda unified approach integrating explicit knowledge learning recurrent networks proceedings international joint conference neural networks seattle ieee press integration neural heuristics knowledge-based inference connection science rule learning searching adapted nets proceedings ninth national conference artificial intelligence anaheim aaai press gallant connectionist expert systems communications acm gemen bienenstock doursat neural networks bias variance dilemma neural computation giles sun chen lee chen higher order recurrent networks grammatical inference advances neural information processing systems vol touretzky san mateo morgan kaufmann giles miller chen chen sun lee learning extracting finite state automata second-order recurrent neural networks neural computation hartigan clustering algorithms york wiley hawley mcclure compilation analysis escherichia coli promoter dna sequences nucleic acids research hayashi neural expert system automated extraction fuzzy if-then rules application medical diagnosis advances neural information processing systems vol lippmann moody touretzky eds san mateo morgan kaufmann hendler special issue hybrid systems symbolic connectionist connection science hinton learning distributed representations concepts proceedings eighth annual conference cognitive science society amherst erlbaum hinton connectionist learning procedures artificial intelligence hinton special issue connectionist symbol processing artificial intelligence jacobs jordan nowlan hinton adaptive mixtures local experts neural computation jordan attractor dynamics parallelism connectionist sequential machine proceedings eighth annual conference cognitive science society amherst erlbaum jordan rumelhart forward models supervised learning distal teacher cognitive science cun boser denker henderson howard hubbard jackel backpropagation applied handwritten zip code recognition neural computation lippmann review neural networks speech recognition neural computation maclin shavlik press knowledge-based neural networks improve algorithms refining chou-fasman algorithm protein folding machine learning mahoney mooney combining neural symbolic learning revise probabilistic rule bases advances neural information processing systems vol hanson cowans giles eds san mateo morgan kaufmann page combining symbolic neural learning masuoka watanabe kawamura owada asakawa neurofuzzy system fuzzy inference structured neural network proceedings international conference fuzzy logic neural networks iizuka japan mcmillan mozer smolensky rule induction integrated symbolic subsymbolic processing advances neural information processing systems vol moody hanson lippmann eds san mateo morgan kaufmann mitchell thrun explanation-based neural network learning robot control advances neural information processing systems vol hanson cowans giles eds san mateo morgan kaufmann mooney shavlik towell gove experimental comparison symbolic connectionist learning algorithms proceedings eleventh international joint conference artificial intelligence detroit morgan kaufmann extended version appeared machine learning mozer das connectionist chunker induces structure context-free languages advances neural information processing systems vol hanson cowans giles eds san mateo morgan kaufmann muggleton inductive logic programming london academic press noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences advances neural information processing systems vol lippmann moody touretzky eds san mateo morgan kaufmann nowlan hinton simplifying neural networks soft weight-sharing advances neural information processing systems vol moody hanson lippmann eds san mateo morgan kaufmann oliver schneider rules task division augment connectionist learning proceedings tenth annual conference cognitive science society montreal erlbaum omlin giles training second-order recurrent neural networks hints proceedings ninth international conference machine learning aberdeen scotland morgan kaufmann neill escherichia coli promoters journal biological chemistry opitz shavlik heuristically expanding knowledge-based neural networks technical report madison wisconsin computer sciences department pollack recursive distributed representations artificial intelligence pomerleau gowdy thorpe combining artificial neural networks symbolic processing autonomous robot guidance engineering applications artificial intelligence quinlan induction decision trees machine learning quinlan learning logical definitions relations machine learning roscheisen hofmann tresp neural control rolling mills incorporating domain theories overcome data deficiency advances neural information processing systems vol moody hanson lippmann eds san mateo morgan kaufmann rumelhart hinton williams learning internal representations error propagation parallel distributed processing vol rumelhart mcclelland eds cambridge mit press saito nakano medical diagnostic expert system 
based pdp model proceedings ieee international conference neural networks ieee press scott shavlik ray refining pid controllers neural networks neural computation sejnowski rosenberg parallel networks learn pronounce english text complex systems shavlik towell approach combining explanation-based neural learning algorithms connection science page combining symbolic neural learning touretzky special issue connectionist approaches language learning machine learning towell symbolic knowledge neural networks insertion refinement extraction doctoral dissertation madison wisconsin computer sciences department towell shavlik noordewier refinement approximately correct domain theories knowledge-based neural networks proceedings eighth national conference artificial intelligence boston aaai press towell shavlik symbolic inductive learning improve knowledge-based neural networks proceedings tenth national conference artificial intelligence san jose aaai press towell shavlik interpretation artificial neural networks mapping knowledge-based neural networks rules advances neural information processing systems vol moody hanson lippmann eds san mateo morgan kaufmann longer version paper machine learning title extracting refined rules knowledge-based neural networks tresp hollatz ahmad network structuring training rule-based knowledge advances neural information processing systems vol hanson cowans giles eds san mateo morgan kaufmann uberbacher mural locating protein coding regions human dna sequences multiple sensor neural network approach proceedings national academy sciences utgoff perceptron trees case study hybrid concept representations proceedings seventh national conference artificial intelligence paul morgan kaufmann weiss kapouleas empirical comparison pattern recognition neural nets machine learning classification methods proceedings eleventh international joint conference artificial intelligence detroit morgan kaufmann appendix application kbann appendix summarizes study kbann produce method recognizing promoters coli dna towell promoters sites process expressing gene create protein begins table initial domain theory promoter recognition task underscore position filled dna nucleotide rule promoter involves subcategories contact conformation region rule states contact involves regions subsequent rules define alternative ways regions set rules derived straightforward fashion biological literature describes dna sequences training neill input features sequential dna nucleotides special notation symbol simplify specification locations dna sequence biological literature counts locations relative site gene expression begins fifty nucleotides location constitute rule antecedents refer input features state starting location list sequence follow rule conformation adenine nucleotides start site adenine position nucleotides finally thymine location kbann translates domain theory neural network topology shown figure clarity connections conformation input units shown notice order classify subsequence promoter region intermediate concepts page combining symbolic neural learning table imperfect domain theory dna promoters promoter contact conformation contact minus minus minus cttgac minus tataat minus ttg minus minus ttgaca minus tataat minus ttgac minus conformation caa gcgcc conformation conformation conformation promoter contact conformation minus minus dna sequence figure initial neural network promoter recognition kbann produces domain theory recognized recall algorithm adds additional lowly-weighted links shown additional sequence information relevant algorithm capture information backpropagation training fifty-three sample promoters nonpromoter sequences refine initial neural network sample promoters obtained compilation produced hawley mcclure negative training examples derived selecting contiguous substrings kilobase sequence provided prof record wisconsin chemistry department virtue fact fragment bind rna polymerase protein initiates gene expression believed promoter sites record personal communication order estimate kbann learned concept promoter standard experimental methodology called leave-one-out jackknife testing cross-validation technique operates training examples page combining symbolic neural learning testing left procedure repeated times excluded training set error rate number errors single test cases divided methodology applied learning algorithms standard backpropagation quinlan standard backpropagation runs number hidden units network kbann produced convention input units connected hidden unit hidden unit connected output unit weights randomly initialized number nonconnectionist empirical learning algorithm training data construct decision tree determining category step node added decision tree partitioning training examples based single most-informative feature feature chosen chosen partition examples maximizes statistical measure information gain table average error rate training examples learning algorithms original domain theory error rate fails match examples cases algorithm correctly classified members training sets algorithm fully accounted training data kbann job generalization error rate previously unseen examples substantially lower investigated nearest-neighbor classification algorithm distance measure number mismatched nucleotides nearest neighbors selected majority class neighbors chosen classification testing values lead lowest error finally table reports results neill hoc approach neill tailored specific task promoter recognition table error rates dna promoter experiment kbann standard backpropagation neill nearest neighbor towell reports results additional learning algorithms including symbolic machine learning techniques refine domain theories alternates matched kbann generalization performance towell shavlik results nofm algorithm promoter problem promoter domain theory training examples anonymous ftp ics uci california irvine archive machine learning datasets domain theories page 
international journal user modeling user-adapted interaction special issue user modeling intelligent agents system building intelligent agents learn retrieve extract information tina eliassi-rady center applied scienti computing lawrence livermore national laboratory box livermore usa eliassi llnl gov jude shavlik computer sciences department wisconsin-madison west dayton street madison usa shavlik wisc abstract present system rapidly easily building instructable self-adaptive software agents retrieve extract information wisconsin adaptive web assistant wawa constructs intelligent agents accepting user preferences form instructions user-provided instructions compiled neural networks responsible adaptive capabilities intelligent agent agent neural networks modi user-provided system-constructed training examples users create training examples rating web pages documents importantly wawa agents techniques reinforcement learning internally create examples users provide additional instruction life agent experimental evaluations home-page nder agent seminar-announcement extractor agent illustrate instructable adaptive agents retrieving extracting information keywords instructable adaptive software agents web mining machine learning neural networks information retrieval information extraction introduction popularity world wide web created surge interest tools retrieve extract information online documents perfect world internet instantaneously retrieve precisely information document fragments option assistant rapidly easily builds instructable self-adaptive software agents information retrieval information extraction tasks intelligent software agents process interests respect information receive automatically work performed rst author computer sciences department wisconsin-madison model preferences time mission spend hours day documents interest answering speci questions goal build assistant call assistant wawa short wisconsin adaptive web assistant wawa interacts user on-line textual environment web build intelligent agent retrieving extracting information figure illustrates overview wawa wawa sub-systems information retrieval sub-system called wawa-ir information extraction subsystem called wawa-ie wawa-ir general search engine agent trained produce specialized personalized agents wawa-ie general extractor system creates specialized agents extract pieces information documents domain interest wawa wawa-ir information retrieval sub-system wawa-ie information extraction sub-system environment user agentir agent figure overview wawa wawa builds agents based ideas theory-re nement community machine learning pazzani kibler ourston mooney towell shavlik users prior knowledge desired task knowledge compiled knowledge based neural networks towell shavlik allowing subsequent nement training examples advantages theory-re nement approach build intelligent agents wawa agents perform initially utilize users prior knowledge users prior knowledge correct ned learning prior knowledge continual dialog user agent decreases large number training examples human-machine communication limited binary representation positive negative examples wawa appealing middle ground non-adaptive agent programming languages systems solely learn user preferences training examples wawa agents intelligent adapt behavior users instructions feedback environments words learning agents neural networks store modify knowledge figure illustrates interaction user intelligent wawa agent agent environment user observes agent behavior quality pages retrieved helpful instructions agent maclin shavlik refer users instructions advice emphasizes agent blindly follow user-provided instructions nes advice based experiences user inputs advice user-friendly advice interface advice processed mapped agent knowledge base neural networks ned based agent experiences agent represent user model neural networks representations ective learning algorithms mitchell article organized present wawa fundamental operations section wawa information-retrieval system discussed section section describes case study wawa information-retrieval system rapid creation ective home-page nder agent section discuss wawa information-extraction system section presents case study wawa information-extraction system seminar announcements extractor agent related work future directions discussed sections section summarizes material paper envision types potential users system application developers build intelligent agent top wawa application users resulting agent phrase user article types users provide advice underlying neural networks envision application users indirectly specialized interface application developers create scenario discussed section advice user agent behavior environment action reformulated advice figure interaction user intelligent agent agent environment wawa core section presents wawa fundamental operations subsystems wawa sections details operations include handling wawa advice language representing web pages internally agent scoring arbitrary long pages neural networks figure illustrates agent operations score page page processor page environment web produces internal representation page representation page agent knowledge-base agent neural network produces score page forward-propagation rumelhart finally agent neural network incorporates user advice environment feedback ect score page knowledge base wawa agent centered basic functions scorelink scorepage fig highly accurate functions standard heuristic search lead ective retrieval text documents best-scoring links traversed highest-scoring pages collected users tailor agent behavior providing advice functions advice compiled knowledge based neural networks towell shavlik implementing functions scorelink scorepage figure functions guide agent wandering web judge pages encountered subsequent reinforcements web encountering dead links ratings rein article terms page document interchangeably interaction system generated examples score page internal representation page page processor parsing tagging wawa agent environment interaction advice training examples user web page figure scoring page wawa agent link score page score figure central functions wawa agents score web pages hyperlinks trieved pages user wishes provide linkand page-scoring functions department people academic information contact information http wisc home page scorepage scorelink departmenthttp wisc html figure wawa central functions score web pages hyperlinks wawa agent scorepage network supervised learner mitchell learns user-provided training examples advice wawa agent scorelink network reinforcement learner sutton barto network automatically creates training examples user-provided training examples advice design scorelink network important advantage producing self-tuning agents training examples created agent section details wawa advice language user-provided instructions mapped scorepage scorelink networks web-based language called advice expression advice language instruction basic form precondition action preconditions represent aspects contents structure web pages table lists actions advice language backusnaur form bnf aho notation strength levels actions represent degree user increase decrease score page link table permissible actions advice statement actions strength show page strength avoid showing page strength follow link strength avoid link strength show page follow link strength avoid showing page link strength weakly moderately strongly nitely wawa extracts features html plain-text web pages input features constitute primitives advice language primitive constructs combined create complicated constructs section presents wawa feature-extraction method advice language extracting features web pages standard representation text bag-of-words representation salton bag-of-words representation word order lost vector records words present page scaled number occurrences properties top-right part figure illustrates representation generally systems belew reduce dimensionality number features problem discarding common stop words stem words root form walked walk wawa typically performs preprocessing steps cal ize s-o f-w ord standard approach spacerent page sample bag-of-w ords aspects representation pagesample words title words window space rent words url compage 
wwwslidingwindow url page title sample page space rent original web page url page title sample page space rent stop words removal stemming figure internally representing web pages information provided word order important word-order information express instructions phrase green bay page show page neural networks score pages links approach capturing word-order information recurrent networks borrow idea nettalk sejnowski rosenberg basic unit word alphabetic letter nettalk wawa reads page sliding xed-size window page word time figure -word sliding window page features represent page ned respect current center sliding window sliding window capture word order page bags words size sliding window capture instructions green bay packers show page addition preserving word-order information advantage structure html documents fetched page formatted augment bag-of-words model localized bags illustrated bottomtypically sliding window words sample page green bay packers played chicago bears yesterday positions -word sliding window step step step step step step greenstep yesterday bears chicago bears chicago played chicago played packers played bay packers green bay packers green bay figure -word sliding window capture word-order information page part figure bag words page word bags title page url sliding window left sides sliding window current hyperlink window inside hypertext current section title wawa parser web pages records parent section title word parent words standard section-header constructs html indicators table captions table-column headings bags words grandparent great-grandparent sections current window nested deeply knowledge context words limit advice describing relations nearby words wawa brill tagger brill annotate word page part-of-speech pos tag noun proper noun verb information represented agent neural networks input features words sliding window adding pos tags distinguish erent grammatical word user give advice form bug page nouns show page rule match pages words bug verbs advantage inherent hierarchy pos tags proper noun noun present participle verb verb user interest word noun presence noun proper noun user interest word web page url urls page contents refer url cases hyperlinks attempt reduce confusion bill proper noun presence bill proper noun noun table lists wawa extracted input features features anywhereonpage hwordi anywhereintitle hwordi word input return true word page inside title page features act word bags page title addition feature representing bag-of-words word order represent xed positions obvious case positions sliding window represent rst words xed title url section titles due important role web specially represent elds delimited dots server portion urls hyperlinks wisc http wisc news table sample extracted input features anywhereonpage hwordi anywhereintitle hwordi isnthwordintitle hni hwordi isnthwordfromendoftitle hni hwordi nthfromendofurlhostname hni hwordi leftnwordinwindow hni hwordi centerwordinwindow hwordi numberofwordsintitle insideemphasizedtext timepagewaslastmodified posatrightspotinwindow hni hpos tagi posatcenterofwindow hpos tagi posatleftspotinwindow hni hpos tagi input features related words positions page wawa agent input vector includes features eliassi-rad full description wawa input features length page date page created modi page server provide information window inside emphasized html text sizes word bags words mentioned advice present bags part-of-speech tags words sliding window represented features table advice consecutive yellow posatcenterofwindow noun show page expresses interest pages include phrase yellow word noun features posatrightspotinwindow posatleftspotinwindow desired pos tag nth position left center sliding window boolean-valued features represent web page ranging anywhereonpage aardvark anywhereonpage zebra rightnwordinwindow aaai nthfromendofurlhostname design leads large number input features expressive advice language assuming typical vocabulary tens thousands words number features order million learning system hope large space input features dealing input features infeasible wawa agent solely learned labeled examples valiant fortunately advice means users indirectly select subset huge set implicit input features indirectly select features involve words appearing advice full set input features weights input features advice high values weights absent words values potential words mentioned advice impact network output lots training deal enormous input space explicitly representing page zero-valued features anywhereonpage aardvark false implicitly represented fortunately nature weighted sums forward backward propagation phases neural networks rumelhart means zero-valued nodes impact current version wawa tfidf methods salton buckley due manner compile advice networks section complex advice constructs predicates features extracted page link constitute basic constructs predicates advice language basic constructs predicates combined boolean operators create complex predicates phrases croft desired properties consecutive words play central role creating complex predicates primitive features extract web pages table iii complicated predicates wawa nes terms basic input features advice rules table correspond instructions user provide interested nding joe smith home-page table iii sample advice consecutiveintitle anyof joseph joe smith home page strongly suggest showing page hyperlinkendswith anyof joseph joe smith jsmith anyof joseph joe smith jsmith index home homepage anyof htm html strongly suggest link titlestartswith joseph joe titleendswith smith suggest showing page anywhereonpage smith strongly suggest avoid showing page rule system sliding window page title plausible variants joe smith rst apostrophe phrase home page rule demonstrates piece advice home-page nding compiled nthfromendofhyperlink input features true speci word nth predicate function returns true false construct function returns numerical values anyof construct table satis listed words present end current hyperlink note wawa treats urls separate word rule depicts interest pages titles starting plausible variants joe smith rst end rule shows advice follow link show page negations avoid instructions negative weights neural networks advice variables wawa advice language variables variables range kinds things names places advice variables relevance wawa system section details understand variables wawa assume system create home-page nder give system good advice title page phrase firstname lastname home page show page leading question marks variables bound receiving request speci person home page variables advice applied task nding home pages number erent people compilation advice neural networks advice compiled scorepage scorelink networks variant kbann algorithm towell shavlik mapping process table analogous compiling traditional program machine code system compiles advice rules intermediate language expressed neural networks important advantage machine code automatically ned based feedback provided user web apply backpropagation algorithm rumelhart learn training set illustrate mapping advice rule variables suppose advice rule phrase professor firstname lastname page show page advice compilation wawa maps phrase centering sliding window figure phrase sequence words maps positions input units sliding window variable firstname center sliding window variables input units bound network units turned match bindings words sliding window bindings table general algorithm mapping advice neural networks construct and-or dependency 
graph based advice rules node and-or dependency graph unit neural network insert additional units nodes set biases unit weights coming unit unit activated inputs true set biases unit weights coming unit unit activated inputs true add links low weights unconnected nodes adjacent layers network learning long run left inwindow professor true word centerinwindow bound firstname true word inwindow bound lastname score page bias figure mapping advice scorepage network firstname joe lastname smith input unit true word centerinwindow bound firstname true set current word center window joe similarly input unit true inwindow bound lastname set current word immediately center window smith wawa connects referenced input units newly created hidden unit weights bias threshold hidden unit set required predicates true order weighted sum inputs exceed bias produce activation sigmoidal hidden unit additional zero-weighted links added hidden unit subsequent learning standard kbann finally wawa links hidden unit output unit weight determined strength rule action wawa interprets phrase suggest showing page moderately increase page score mapping advice rules variables process variable-binding step scoring arbitrary long pages fixed-sized neural networks wawa neural networks means mechanism processing arbitrarily long web pages xed-sized input vectors sliding window assists problem recall sliding window moves page word time html tags hpi hbri hhri act window breakers window breakers sliding window cross window breaker encountered unused positions sliding window left lled score page computed stages stage set input units represent global features page number words page slide window sliding window page window position rst set values input units representing positions window word center window calculate values hidden units hus directly connected input units call hus level-one hus words perform forwardpropagation input units level-one hus process list values level-one hus position sliding window level-one pick highest represent activation stage values level-one hus values input units global features compute values hus output unit perform forward-propagation initial weights bias threshold unit satisfy equation bias initial weight unnegated antecedents level-one hus global input units output unit evaluate values hus process produced scorepage network stage returned page score note scan sliding window page occurs stage forward-propagating level-one hus rst stage ectively values complex features stage values global input units values score page two-stage process capture advice phrase milwaukee brewers phrase chicago cubs page show page one-stage process correctly capture advice rule phrases sliding window simultaneously figure illustrates point hyperlink computed similarly scorelink network sliding window slid hypertext hyperlink words surrounding hypertext sides wawa retrieve information information retrieval systems input set documents corpus query consisting bunch keywords keyphrases ultimate goal system return documents relevant query section describes design creating specialized personalized intelligent agents retrieving information web system description table high-level description wawa system wawa-ir wawa-ir general search engine agent training specialized personalized neural networks created section techniques read disk resumption previous session basic operation wawa-ir heuristic search scorelink network acting heuristic function solely nding goal node collect pages scorepage rates highest user choose seed queue pages fetch ways set starting urls providing simple query wawa-ir converts query urls user-chosen subset selectable search engine sites altavista excite infoseek lycos yahoo centerinwindow cubs left inwindow chicago inwindow inwindow centerinwindow brewers left inwindow milwaukee bias bias bias score page level-one hidden unit highest activation produced words page correspond chicago cubs level-one hidden unit highest activation produced words page correspond milwaukee brewers level-two hidden unit advice phrase milwaukee brewers phrase chicago cubs page show page sample web page brewers cubs milwaukee brewers play chicago cubs friday september wrigley field figure scoring page sliding window stage sliding window scanned page determine highest values level-one hidden units stage highest activations level-one hidden units score level-two hidden unit output unit produces score page mentioned table user values parameters upper bound distance agent wander initial urls default table wawa information retrieval algorithm saved disk previous session create scorelink scorepage neural networks reading user initial advice start adding user-provided urls search queue initialize search queue urls query user chosen set web search engine sites execute concurrent processes independent process search queue empty maximum number urls visited urltov isit pop search queue fetch urltov isit evaluate urltov isit scorepage network score high insert urltov isit sorted list pages found score urltov isit improve predictions scorelink network section details evaluate hyperlinks urltov isit scorelink network score links session insert urls sorted search queue max-length bound independent process user additional advice insert neural network independent process person rates fetched page rating create training scorepage neural network minimum score hyperlink receive order put search queue default scale maximum number hyperlinks add page default maximum kilobytes read page default kilobytes maximum retrieval time page default seconds training wawa neural networks ways train wawa-ir neural networks system-generated training examples advice user iii user-generated training examples fetching page wawa-ir predicts retrieving predicted based text surrounding hyperlink global information referring page title url fetching analyzing actual text wawa-ir re-estimates erences estimates score constitute error backpropagation rumelhart improve scorelink neural network details process section addition system-internal method automatically creating training examples user improve scorepage scorelink neural networks ways user provide additional advice observing agent behavior invoke thoughts good additional instructions repeatedly happened case studies wawa-ir agent accept advice augment neural networks time simply adds network additional hidden units represent compiled advice technique ectiveness demonstrated tasks maclin shavlik providing additional hints rapidly drastically improve performance wawa-ir agent provided advice relevant maclin shavlik showed algorithm robust advice incrementally bad advice agent quickly learn ignore tedious user rate pages mechanism providing training examples backpropagation type training performed pages constitute initial search queue user unable articulate agent misscoring pages links standard methodology previously investigated researchers pazzani discuss aspect wawa-ir section conjecture improvement wawa-ir neural networks scorepage result users providing advice personal experience easy simple advice require large number labeled examples order learn purely inductively words advice rule typically covers large number labeled examples rule found avoid showing page cover pages phrase found deriving training examples scorelink temporal erence methods sutton automatically train scorelink wawa-ir employs form q-learning watkins type reinforcement learning sutton barto recall erence wawa-ir prediction link fetching url estimate serves error backpropagation reduce wawa-ir collected information 
re-estimate link invokes backpropagation addition periodically reuses training examples times network notice wawa-ir automatically constructs training examples direct user intervention typical reinforcement learning typical reinforcement learning action link case solely determined result action page retrieved minus retrievaltime penalty reward links lead pages additional good links figure equation illustrate point page scoring link page page link score scoring link scoring link page page figure reestimating link equation estimate link best-first search scorelink estimate scorelink fetchpenalty scorepage fetchpenalty scorepage max scorelink scorelink estimate scorelink fetchpenalty scorepage task scorelink function estimating discounted sum scores pages fetched assuming system started bestrst search page referred hyperlink cost fetching pages words figure page root bestrst search wawa-ir visit depending referring hyperlink scored higher rst terms sum root page discounted time step recursively estimate remainder sum higher score urls front search queue discounting predicted time steps future bestrst search visiting moving wawa-ir promising url search queue order calculation reestimated scorelink function localized largely ignore aspect system behavior partially capture phenomenon adjusting calculation assuming links negative predicted scenario localizing re-estimation scorelink function apply url fetched dead link occurrence scorelink receives large penalty nition function equation represent bestrst beam-search strategy erent traditional nition equation essentially assumes hill climbing default discounted wawa-ir networks produce negative values output units simply output weighted sum inputs linear units note hidden units wawa-ir networks sigmoidal activation functions equation estimate link hill-climbing search scorelink estimate scorelink fetchpenalty scorepage fetchpenalty scorepage max scorelink estimate scorelink fetchpenalty scorepage figure illustrate erence approach traditional ning function traditional hill climbing approach bestscoring link reconsidered calculation score search strategy optimal nding relevant pages web traverse link highest-score set encountered links encountered links follow link highest score discard previous step highest step discussion wawa information retrieval main advantage wawa system theory renement utilize user prior knowledge perfectly correct wawa-ir learning system correct user instructions manner rapidly transform wawa-ir general search engine specialized personalized agent section describes rapid creation ective home-page nder agent generic wawa-ir system user continually provide advice agent characteristic wawa-ir enables user observe agent guide behavior user feels wawa-ir agent user model incorrect finally learning scorelink function wawa-ir agent ectively search web learning relevant links automatically create training examples reinforcement learning turn improves accuracy agent respect relevancy pages returned cost approach fetch analyze web pages focused speed case study ignoring questions fetching rst characters web pages capsule summaries search engines return making wawa-ir faster part future work due arti cial neural networks cult understand learned craven shavlik nice wawa-ir agent explain reasoning user attempt alleviate problem built visualizer neural network wawa-ir visualizer draws neural network graphically displays information nodes links network experimental study wawa system section describes case study evaluate wawa system built home-page nder agent wawa advice language appendix presents complete advice home-page nder agent results empirical study illustrate build ective agent web-based task quickly instructable adaptive home-page finder chose task building home-page nder existing system named ahoy shakes valuable benchmark ahoy technique called dynamic sifting lters output web indices generates guesses urls promising candidates found wrote simple interface layered top wawa-ir figure asks relevant information person home page sought rst nicknames middle initial miscellaneous phrases partial url ibm wrote small program reads elds creates advice wawa-ir wrote general advice rules related home-page nding slight variants middle names initials specializing wawa-ir task creating initial general advice day spent parts days tinkering advice examples training set describe step allowed manually advice process expect typical future users wawa-ir learn general concept home-page nding advice language variable binding mechanism home-page nder altavista excite infoseek lycos yahoo figure interface wawa-ir home-page finder accepts instructions words bound variables rst names names wrote general-purpose advice home-page nding variables rule table iii written advice variables illustrated figure names speci people wawa-ir refer advice variables sliding window technical limitation address future advice refers aspects web page specially created subsequently retracted request speci person home page number speci c-person rules home-page nder creates depends information provided target person experiments provided information people names leads generation dozen rules depending middle names initials provided users retract advice wawa-ir neural networks retract advice rule wawa-ir removes network nodes links rule motivation methodology randomly selected people aha list machine learning case-based reasoning cbr researchers aic nrl navy mil aha people html run experiments evaluate wawa-ir reduce computational load experiments limited people united states people selected randomly picked train wawa-ir remaining test set training phase steps step manually run system people randomly picked training set refer set advice-training set ning advice hand freezing advice-giving phase step split remaining people set people backpropagation training refer set training set set consisting people tuning refer set tuning set perform backpropagation-based training rst step advice-training phase accurate make advice machine learning scorepage function trained backpropagation training set tuning set avoid tting training examples ing scorelink function wawa-ir automatically generate training examples person temporal-di erence learning section finally evaluate trained home-page nder test set testing phase learning takes place person training person rate pages actual number erent examples processed backpropagation larger size training set factor table describes technique neural-network learning associate desired score page encounter compare score output scorepage network page nally perform error backpropagation rumelhart simple heuristic desired score page target page actual home page person recall score page real number interval heuristic page encountered target page desired score follow standard machine learning methodology dividing data set subsets subset training testing purposes network performs training data poorly data table supervised-learning technique training scorepage network text explanation desired output values training error tuning set increasing person training set times person home page found train scorepage network pages scored higher home page actual home page pages scored actual home page train network highest scoring pages calculate error tuning set page encountered host target page desired score desired score page encountered suppose target person alan turing target page http turing turing homepage encountering page http turing set desired score page host alan turing home page judge wawa-ir performance task nding homepages provide advice discussed presented appendix important note experiment intentionally provide advice speci cbr research build generalized homepage nder 
specializes nding cbr researchers wawa-ir options ect performance amount execution time accuracy results choose small numbers parameters maximum number pages fetched maximum distance travel pages returned search engines start wawa-ir providing person aha web page partially standardize examples common variants rst names joseph joe wawa-ir converts initial query paragraph search engines mentioned earlier altavista excite infoseek lycos yahoo compare performance wawa-ir performances ahoy hotbot search engine wawa-ir performed home-page experiments shakes provide names test set ahoy web interface ahoy metacrawler search engine queries search engines opposed wawa-ir queries search engines run hotbot erent conditions rst setting performs specialized hotbot search people aha page queries variant provide hotbot general-purpose disjunctive query person required word variants person rst query wawa-ir initially sends search engines experiments rst pages hotbot returns assume people results returned search engine people erent links home pages comparing urls provided aha page exact comparison contents fetched pages contents page linked aha site running wawa-ir fetch urls server matched aha page preventing visiting aha site results discussion table vii lists performance wawa-ir home-page nder results ahoy hotbot refer supervised reinforcement learning reporting percentage test set home-pages found report average ordinal position rank page found wawa-ir ahoy hotbot return sorted lists results provide strong evidence version wawa-ir specialized home-page nder adding simple advice produces home-page nder proprietary peoplender created hotbot ahoy erence percentage home-pages found wawa-ir hotbot experiment statistically signi con dence level erence wawa-ir ahoy statistically signi con dence level recall specialize wawa-ir generic system task days investigate home-page nder performance supervised reinforcement learning motivation gain performance learning remove initial google study exist ran experiments eliassi-rad details experiments google aspects wawa-ir table vii empirical results wawa-ir ahoy hotbot supervised learning reinforcement learning system found rank page found wawa-ir advice rules ahoy hotbot person search hotbot general search rules performance degrades advice rules removed refer words home page person resume address table viii reports performance wawa-ir home-page nder trained advice rules trained supervised reinforcement learning table viii empirical results erent versions wawa-ir home-page finder supervised learning reinforcement learning advice rules found rank page found erences wawa-ir runs advice rules learning learning statistically signi reduce number advice rules wawa-ir performance deteriorates results show wawa-ir learn increase accuracy percentage points erence statistically signi con dence level surprising wawa-ir reach performance increase size training data compensate reduction advice rules nonetheless rules erence wawa-ir hotbot experiment statistically signi con dence level cases target page speci person found rank target page similar runs recall rank target page refers ordinal position list pages returned user rank typically lower runs included training training target page high score trained network assuming wawa-ir nds home page table lists average number pages fetched actual home page table average number pages fetched wawa-ir target home page supervised learning reinforcement learning advice rules avg pages fetched home page learning reduces number pages fetched target page found intuitive learning wawa-ir classify pages target page quicker table average number pages fetched target page found lower advice rules advice rules rules average rules average rst glance intuitive reason discrepancy found advice rules recall remove advice rules refer words home page person resume address rules scorepage scorelink networks rate pages links promising home pages pages fetched processed target page found wawa extract information information extraction process pulling desired pieces information document price product author article building system requires large number annotated examples expert provide cient correct knowledge domain interest requirements make time-consuming cult build system similar case wawa theory-re nement mechanism build system wawa-ie theory nement strike balance needing large number labeled examples complete correct set domain knowledge wawa-ie takes advantage intuition inverse problems system set keywords asked rate relevance documents system set documents asked slots template explore essentially system address task building agent task straightforward wawa user set advice rules wawa-ie describe system score bindings slots lled process call names slots lled variables binding variable synonym lling slot initial advice compiled scorepage network rates goodness document context variable bindings recall scorepage supervised learner learns trained user-provided instructions user-labeled pages scorelink network wawa-ie interested extracting pieces text pages linked wawa-ir agents wawa-ie agents blindly follow user advice agents advice based training examples user-provided advice typically leads higher accuracy fewer user-provided training examples eliassirad shavlik eliassi-rad wawa-ie generate-and-test approach extract information generation step user rst speci slots lled part-of-speech tags parse structures wawaie generates large list candidates document test step wawa-ie scores candidate candidates annotated examples result tedious process reading training documents tagging extraction hand produce scores greater system-de ned threshold returned extracted information mentioned section relevance approach fact wawa-ie advice language variables understand wawa-ie variables assume extract speaker names collection seminar announcements give system good advice page phrase speaker firstname nnp lastname nnp score page highly leading question marks slots lled matches single word recall advice language user required part speech slot nnp denotes proper noun precondition rule matches phrases speaker joe smith speaker jane doe figure illustrates extracting speaker names seminar announcement wawa-ie announcement fed candidate generator selector produces list speaker candidates entry candidates list bound variables advice output trained network real number interval represents con dence speaker candidate correct slot ller document candidate generator selector seminar announcement don miss jane doe john smith talk doe smith talk turing tarpit building score jane doe speakerextractor speaker speaker candidates jane doe john smith doe smith generation step test step figure extraction speaker names wawa-ie variable speaker record holds multiple variables firstname lastname system description wawa-ie candidate generator selector algorithm scorepage network build agents require user provide information wawa-ie set on-line documents information extracted extraction slots speaker names part-of-speech pos tags noun proper noun verb parse structures noun phrase verb phrase extraction slot set advice rules refer extraction slots variables set annotated examples training documents extraction slots marked user explicitly provide extraction slots pos tags separately advice extracted advice rules case study extract speaker names set seminar announcements give wawa-ie collection cmu seminar-announcements slots extract speaker names speaker phrase word words word phrase tagged noun proper noun set advice rules refer speaker variable advice rules phrase firstname nnp 
lastname nnp talk appears document strongly suggest showing page variables firstname lastname represent speaker nnp trailing variables required pos tag variables nnp refers proper noun precondition rule matches phrases john smith talk action document seminar announcement satis precondition complete set rules seminar-announcement domain listed appendix set word marked speaker names training set seminar-announcement domain phrases jane doe john smith marked speaker names seminar announcement depicted figure training wawa-ie rst compiles user advice scorepage network wawa-ie call individualslot candidate generator combination-slots candidate selector create training examples scorepage network candidate generation selection process training generate extractions trained network scores testing document extract information generate large number candidate bindings turn provide set bindings trained network neural network produces numeric output set bindings finally extraction process returns bindings greater system-de ned threshold figure showed seminar announcement don miss jane doe john smith talk doe smith talk turing tarpit building extracted list candidates announcement jane doe jane doe john smith john smith talk tarpit building building words tagged nouns proper nouns word turing included list incorrectly tagged verb present participle form suppose compiled rule earlier section scorepage network phrase firstname nnp lastname nnp talk appears document strongly suggest showing page assume trained network training process section candidate slot ller bind variable candidate ller word bound variable lastname jane bound lastname candidate ller words bound variables firstname lastname phrase jane doe word jane bound terms trained network trained agent interchangeably sections network represents agent knowledge-base brain firstname word doe bound lastname finally candidate binding calculate score scorepage network resides agent knowledge-base seminar announcement case network produce high score candidate john smith satis precondition rule assuming training network signi cantly alter rule section variables learn speaker names variables learn location names variables speaker names refer rst names nicknames middle names initials names variables location refer cardinal number variables representing non-numerical portion location section sample rules extracting speaker names location names complete sets rules extraction slots appendix sections describe candidate generator selector training phase testing phase detail candidate generation selection rst step wawa-ie takes training generate individual llers slot document candidate llers individual words phrases recall wawa-ie extraction slot represented user-provided variables initial advice user tag variables representing parts extraction slot wawa-ie slot tag information part-of-speech pos tagger brill sentence analyzer rilo collect slot candidate llers cases user-speci pos tags slot noun proper noun verb rst annotate word document pos brill tagger slot collect word document pos tag tag assigned variable task advice user parse structure slot noun phrase verb phrase sundance rilo builds shallow parse tree segmenting sentences noun verb prepositional phrases collect phrases match parse structure extraction slot generate subphrases consecutive words sundance shallow parsing pos tags provided user extraction slot pos tag ned brill tagger user speci extraction slot word tagged proper noun consecutive words tagged proper nouns brill tagger userprovided document collect words tagged proper nouns addition sequence words tagged proper nouns phrase jane doe appeared document tagger marked words proper nouns collect jane doe jane doe point typically lengthy lists candidate llers slot focus selecting good combinations slots process combinatorially demanding eliassi-rad shavlik present evaluate heuristic methods choosing good combinations generate combinations llers task template slot case experiments presented section desirable trim list candidate llers training process training iteratively heuristically select slot list training candidate llers scoring candidate ller untrained scorepage network returning highest scoring candidates randomly sampled candidates process picking informative candidate llers training data bene cial side ects detail section training agent figure shows process building trained agent positive training examples provided domains rst generate negative training examples end candidate generator selector list negative training examples collected selector informative negative examples misses heuristic search selector scores training documents untrained scorepage network user-provided prior knowledge scored miss extractions highly true extractions highest-scoring negative examples collected train scorepage neural network negative examples small number randomly selected negative examples provided positive examples training network recognize candidate llers training set untrained network compiled initial advice training backpropagation labeled examples produce high output score correct extraction context document section advantage global layout information documents interest training set initial advice individual-slot candidate generator combination-slots candidate selector slots pos tags parse structures lists candidate combination-slots extractions scorepage agent trained network agent knowledge-base figure building trained agent scorepage network outputs real number wawa-ie threshold output bindings scores threshold returned user extractions rest discarded note threshold manipulate performance agent threshold set high number agent miss lot correct llers slot low recall number extracted llers correct higher high precision recall van rijsbergen ratio number correct llers extracted total number llers correct extraction slots precision van rijsbergen ratio number correct llers extracted total number llers extracted finally -measure combines precision recall formula precision recallprecision recall -measure van rijsbergen regularly compare performances systems weights precision recall equally produces single number avoid tting scorepage network threshold output training divide training set disjoint sets sets train scorepage network set tuning set rst stop training scorepage network speci cally cycle training examples times iteration training examples lists candidate llers tuning set evaluate -measure produced network settings threshold pick network produced highest -measure tuning set nal trained network utilize tuning set time optimal threshold output trained scorepage network speci cally perform threshold increments run tuning set trained scorepage network -measure threshold set optimal threshold threshold maximum -measure testing trained agent figure depicts steps trained agent takes produce extractions entry list extraction candidates rst bind variables candidate values perform forward propagation trained scorepage network output score network test document based candidate bindings output network greater threshold ned tuning step record bindings extraction bindings discarded discussion wawa information extraction aspect wawa-ie exploitation relationship build agents treat extractions keywords turn judged context entire document network performs training data poorly data slots pos tags parse structures test set unseen docs trained agent individual-slot candidate generator combination-slots candidate selector extractions lists candidate combination-slots extractions figure testing trained agent theory nement advantage user prior knowledge perfectly correct wawa-ie learning system turn reduces labeled examples expensive task compiling users prior knowledge scorepage network good method nding informative negative training examples misses cost approach require 
user provide pos tags parse structures extraction slots assume brill tagger sundance perfect tag words parse sentences accuracy brill tagger annotates words document accuracy brill error rate propagates results accuracy estimates sundance recall subphrases phrases sundance produces approach computationally demanding due generate-and-test approach cpu cycles abundant experiments eliassi-rad shavlik eliassi-rad eliassirad shavlik shown wawa-ie performs subset combinations slot llers experimental study wawa system section test wawa-ie cmu seminar-announcements domain freitag domain consists pages task extract start time end time speaker location announcement report results extracting speaker location omit start end times experiments systems perform slots addition follow existing methodology independently extract speaker location names document assumed announcement announcement pair speakers locations return list speakers separate list locations seminar announcements tagged brill part-of-speech tagger brill common stop words discarded stem words study converting words base forms removes information extraction process experimental methodology compare system information extraction systems cmu seminar announcements domain freitag systems hmm freitag mccallum bwi freitag kushmerick srv freitag naive bayes freitag whisk soderland rapier cali rapier-wt cali systems exploits prior knowledge naive bayes hmm bwi rest systems relational learning algorithms rapier-wt variant rapier information semantic classes utilized hmm freitag mccallum employs hidden markov model learn extraction slots bwi freitag kushmerick combines wrapper induction techniques adaboost solve task freitag rst randomly divided documents seminar announcements domain ten splits randomly divided ten splits approximately training examples testing examples whisk results systems based data splits results whisk single trial documents training set documents testing set give wawa-ie advice rules backus-naur form bnf aho notation speakers locations rewe chose task widely literature directly compare performance wawa-ie existing systems spectively appendix wrote advice rules speci cmu seminar announcements mind rules describe prior knowledge speaker location general seminar announcement half day write rules manually rules time domain create number negative training examples speaker location independently number positive examples choose negatives complete list possibilities collecting score highest untrained scorepage network remaining chosen randomly complete list table describes rules domain theories speaker location slots rule matches phrases length start word professor proper nouns remaining words rule phrases length rst word speaker word care trailed proper nouns matches phrases speaker joe smith speaker jane doe rules match phrases room ers requires words room cardinal number proper noun subset con dent phrases matching describe locations sends higher weight output unit scorepage network table sample rules domain theories speaker location slots professor firstname nnp lastname nnp strongly suggest showing page speaker firstname nnp lastname nnp strongly suggest showing page room locnumber locname suggest showing page room locnumber locname nnp strongly suggest showing page results tables xii show results wawa-ie agent systems speaker location slots results reported averaged precision recall values ten splits precision recall -measure split determined optimal threshold found split tuning set section details ten splits optimal thresholds wawa-ie untrained agent speaker slot location slot optimal threshold wawa-ie trained agent vary split speaker slot location slot speaker slot optimal thresholds wawa-ie trained agent vary location slot optimal thresholds wawa-ie trained agent range speaker location seminar multiple forms announcement extraction considered correct long correct forms extracted speaker john doe smith words smith joe smith john doe smith smith smith document extractions considered correct method marking correct extractions systems compare approach precision recall -measure compare erent systems section nitions terms ideal system precision recall discussion -measure versatile precision recall explaining relative performance erent systems takes account inherent tradeo exists precision recall speaker location slots -measure system considerably higher naive bayes relational learners trained agent performs competitively bwi hmm learner -measures high generate extraction candidates generate-and-test model extract lot correct llers data set turn leads higher recall systems training reject candidates obtain reasonable precision figure illustrates fact speaker slot point graph represents averaged precision recall values speci network output ten splits points curve figure representing network outputs table results speaker slot seminar announcements task system precision recall hmm wawa-ie trained agent bwi srv rapier-wt rapier wawa-ie untrained agent naive bayes whisk table xii results location slot seminar announcements task system precision recall wawa-ie trained agent hmm bwi rapier-wt srv rapier whisk rapier-w naive bayes wawa-ie untrained agent increments trained agent generates precision scores untrained agent recall precision trained wawa-ie agent untrained wawa-ie agent figure precision recall curves wawa-ie untrained trained agents extracting speaker slots increase performance wawa-ie untrained agent wawa-ie trained agent shows agent hard-wired perform domain training helped performance finally note systems higher precision depending user tradeo recall precision erent systems preferred testbed related work wawa-ir syskill webert pazzani webwatcher joachims web agents machine learning techniques bayesian classi rl-tfidf hybrid learn drummond drummond created similar system assists users browsing software libraries learns unobtrusively observing users actions letizia lieberman system similar drummond look-ahead search current location user web browser unlike wawa-ir systems unable accept advice simple provide lead learning manually rating visiting web pages eliassi-rad eliassi-rad shavlik details experiments wawa-ie system literature applies theory nement problem feldman system feldman takes set approximate rules training examples incrementally revise inaccuracies initial rules revision algorithm heuristics place type revision performed unlike wawa-ie advice rules rules provide advice existing rules system manipulates rules directly wawa-ie compiles rules neural networks standard neural training rules finally approach suggest revisions human user wawa-ie approach make revisions automatically systems break groups rst group kind relational learning learn extraction patterns cali freitag soderland group learns parameters hidden markov models hmms hmms extract information bikel freitag mccallum leek ray craven seymore recently freitag kushmerick combined wrapper induction techniques kushmerick adaboost algorithm schapire singer create extraction system named bwi short boosted wrapper induction system out-performed relational learners competitive systems hmms wawa-ie leek hmms extracting information biomedical text system lot initial knowledge build hmm model training data learn parameters hmm system knowledge authors statistical methods reduce lot training examples freitag mccallum hmms extract information text employ statistical technique called shrinkage problem cient labeled examples seymore hmms extract information on-line text problem cient training data data labeled purpose system similarly craven kumlien weakly labeled training data reduce labeled training examples advantage system utilize prior knowledge reduces large number labeled training examples depend initial knowledge 
correct easy users articulate domain-speci advice user-friendly interface provided converts advice speci wawa advice language advantage system entire content document estimate correctness candidate extraction learn extraction slots documents advantage wawa-ie utilize untrained scorepage network produce informative negative training examples misses current work future directions order understand people instructable web agent wawa started work research groups medical school build personalized easily customized intelligent agents capable returning relevant information web based interactions hope improve advice language started running additional experiments study wawa-ir learn general concepts nding research-group pages domain harder problem nding home pages syntactical clues nding home pages exist anymore person home page searching pages research group pages research operating systems actual phrase operating systems actual page research operating systems ways embed wawa major existing web browser minimizing interface features users learn order interact system related developing methods wawa automatically infer plausible training examples observing users normal browsers goecks shavlik experiments reported trained network nding speaker slots location slots needed train separate networks testing set crossproducts speaker location candidates computationally expensive posed problem domain document contained announcement match multiple speakers multiple locations document problem instance template document multi-slot problem extended catalog candidate-selector algorithms include heuristic methods modi versions walksat selman gsat random local search hill climber multiple random starts eliassi-rad shavlik eliassi-rad functionality reduce computational problem candidate bindings solve multislot extraction problem wawa-ie eliassi-rad shavlik empirical results multi-slot problem extracting proteins locations cell set biological abstracts wawa-ie agent out-perform state-of-the-art method ray craven domains started measuring performance function number positive training examples function number advice rules addition starting experiments domain webkb domain freitag finally incorporating candidate generation selection steps directly connectionist framework current scorepage network candidate extractions training process conclusions argue promising create intelligent agents involve user ability direct programming provide approximately correct instructions sort agent ability accept automatically create training examples due largely unstructured nature size web hybrid approach appealing solely based non-adaptive agent programming languages users rate mark desired extractions large number web pages rst present evaluate wawa information retrieval system appealing approach creating personalized informationnding agents web central aspect design machine learner core users create specialized agents articulating interests advice language wawair compiles instructions neural networks allowing subsequent nement system creates training examples reinforcement learning supervised training user rate information wawa-ir agent nds process continuous learning makes wawa-ir agents adaptive home-page nder case study demonstrates cacy system-generated training examples improve evaluation potential hyperlinks traverse describe evaluate system theory nement perform information extraction wawa information extraction system neural network accepts advice variables rate candidate variable bindings content document extraction process rst generates large set candidate variable bindings slot selects subset slot bindings heuristic search nally trained network judge bindings score higher systemcomputed threshold returned extracted information theory nement advantage prior knowledge domain interest produce informative training examples lead increase performance agent cmu seminar-announcements domain -measures wawa-ie agent signi cantly higher approaches competitive state-of-the-art system boosted wrapper induction technique recent work eliassi-rad shavlik eliassi-rad empirically show bene intelligent algorithm selecting candidates multiple slots provide additional evidence approach improves state art wawa utilizes user knowledge build agents retrieve extract information important characteristics wawa agents ability receive instructions knowledgebases learning instructions provided user perfectly correct ability receive user advice continually iii ability create informative training examples empirical studies information retrieval information extraction tasks illustrate advantages building instructable adaptive agents acknowledgements research reported article supported part national science foundation nsf grant iriuniversity wisconsin vilas trust national library medicine nlm grant material presented sections partially reported shavlik eliassi-rad shavlik material covered sections partially reported eliassirad shavlik appendix advice rules home-page finder section appendix presents rules home-page nder section details case study rules variables firstname person robert firstinitial initial person nicknamea nickname person rob nicknameb nickname person bob middlename middle person eric middleinitial middle initial person lastname person smith miscword miscellaneous word miscword miscellaneous word miscword miscellaneous word urlhostword word end host url http wisc urlhostword word end host url wisc http wisc urlhostword word host http wisc experiments variables numbered wanted fairly compare wawa home-page nder existing alternative approaches provide values variables numbered introduced variables wrote rules illustrate information person helpful nding home-page actions home-page nder rules suggest action adds moderately weighted link rule hidden unit scorepage scorelink networks output units networks suggest showing page action adds moderately weighted link rule hidden unit scorepage network network output unit suggest link action adds moderately weighted link rule hidden unit scorelink network network output unit avoid action adds link moderately negative weight rule hidden unit scorepage scorelink networks output units networks avoid showing page action adds link moderately negative weight rule hidden unit scorepage network network output unit avoid link action adds link moderately negative weight rule hidden unit scorelink network network output unit actions pre xed modi ers nitely assuming preconditions nite rule fully met link sigmoidal hidden unit representing rule weight actions actions strongly assuming preconditions strong rule fully met link sigmoidal hidden unit representing rule weight actions actions moderately assuming preconditions moderate rule fully met link sigmoidal hidden unit representing rule weight actions actions action modi moderately default modi weakly assuming preconditions weak rule fully met link sigmoidal hidden unit representing rule weight actions actions present home-page nder rules non-terminal tokens backus-naur form bnf aho rules non-terminal tokens rst names firstname firstinitial nicknamea nicknameb middle names middlename middleinitial full rst names middle names lastname regular rst names lastname home page words home homepagej home-page person full namejregular rules pages person phrase home page words home homepage home-page title function consecutiveintitle takes sequences words returns true form phrase inside title page symbol wawa wild card symbol placeholder matches single word punctuation home page rules consecutiveintitle home page person nitely suggest showing pagej consecutiveintitle person home page words nitely suggest showing pagej consecutiveintitle person home page words strongly suggest showing pagej consecutiveintitle rst names lastname home page words suggest showing pagej consecutiveintitle home page rst names lastname suggest showing pagej consecutiveintitle home page lastname suggest showing pagej consecutiveintitle home page lastname weakly suggest showing pagej consecutiveintitle person home page words suggest showing page set rules 
links person home-page function consecutive takes sequences words returns true words phrase page home page rules consecutive person home page words nitely suggest link consecutive home page person nitely suggest link consecutive rst names lastname home page words strongly suggest link consecutive home page rst names lastname strongly suggest link consecutive home page lastname suggest link consecutive person strongly suggest link consecutive lastname suggest link rules pages links leading pages person title question format don person question mark page title represent query person person home-page home page rules anywhereintitle consecutiveintitle regular noneof home page words strongly suggest anywhereintitle consecutiveintitle rst names lastname noneof home page words strongly suggest anywhereintitle consecutiveintitle rst names anywhereintitle lastname suggest anywhereintitle consecutiveintitle lastname rst names suggest consecutive rst names home page words suggest consecutive lastname home page words suggest consecutive home page words suggest rule home-pages lead home-pages home page rules anywhereintitle anywhereintitle home page anywhereintitle homepage anywhereintitle home-page suggest link rule seeks pages person image conjecture image person picture home page rules insideimagecaption consecutive lastname suggest set rules pages links include query words user bindings variables rules functions numberofquerywordsonpage returns number query words page home page rules insideemailaddress insideaddress weakly suggest numberofquerywordsonpage avoid link nitely avoid showing pagej anywhereinurl strongly avoid link nitely avoid showing pagej strongly avoid link anywhereinurl anywhereinurl numberofquerywordsinurl weakly suggest set rules pages links include query words user function called scalelinearlyby takes set conditions returns linear sum depending number conditions satis home page rules scalelinearlyby suggest scalelinearlyby suggest showing page weakly suggest linkj rules pages links include combination words home page home-page homepage home page directory people page url home page rules consecutive home page words directory strongly suggest link consecutiveinasection home page words directory suggest link consecutiveinhypertext home page words suggest link consecutiveinasection home page words weakly suggest consecutive home page consecutive anyof homepage home-page weakly suggest anywhereinurl home anywhereinurl page anywhereinurl people anywhereinurl homepage anywhereinurl home-page suggest subsequent rules attempt page modied function pagelastmodi edat determines number days today page modi page modi function pagelastmodiedat page reports time modi convert time interval means means page changed today pagelastmodi edat minus scaled home page rules scalelinearlyby pagelastmodi edat weakly suggest showing page weakly suggest link rules function consecutiveinurl function takes sequences words returns true words form phrase url page rules pages url person possibly words htm html home page rules consecutiveinurl anyof rst names lastname weakly suggest consecutiveinurl person anyof htm html nitely suggest showing pagej rules pages links phrase firstname lastname title url hypertexts hyperlinks home page rules consecutive firstname lastname strongly consecutiveinurl firstname lastname strongly consecutiveintitle firstname lastname strongly consecutiveinhypertext firstname lastname strongly consecutiveinhyperlink firstname lastname strongly rules avoid pages phrase found title home page rules titlestartswith anyof found strongly avoid titleendswith anyof found strongly avoid anywhereintitle found avoid rules advice commonly words person homepage resume function nofm set rules takes integer list conditions size returns true conditions true function anywhereonpage true telephone number page returns false rules removed original set advice rules create set initial advice rules section details home page rules insidemetawords consecutive home page consecutive anyof homepage home-page consecutive personal anyof info information suggest showing pagej consecutive curriculum anyof vitae vita weakly suggest consecutiveinhypertext curriculum anyof vitae vita suggest consecutiveinhypertext anyof vitae vita suggest link consecutiveinhypertext anyof resume suggest link consecutive anyof resume vita vitae suggest marked non-terminal representing rules symbol distinguish rules consecutive anyof homepage home suggest consecutiveinasection personal anyof info information weakly suggest nofm anywhereinsections personal anywhereinsections information anywhereinsections info anywhereinsections projects anywhereinsections interests weakly suggest scalelinearlyby consecutive anyof lastname miscword miscword miscword anywhereinwindow anywhereinwindow e-mail anywhereinwindow anywhereinwindow anywhereinwindow anywhereinwindow anywhereinwindow telephone anywhereinwindow department anywhereinwindow work anywhereinwindow anywhereinwindow dept anywhereinwindow address anywhereinwindow mailing strongly suggest scalelinearlyby consecutive anyof lastname miscword miscword miscword anywhereonpage anywhereonpage e-mail anywhereonpage anywhereonpage anywhereonpage anywhereonpage anywhereonpage telephone anywhereonpage department anywhereonpage work anywhereonpage anywhereonpage dept anywhereonpage address anywhereonpage mailing suggest consecutive anyof research recent anyof summary weakly suggest consecutive recent weakly suggest insideemailaddress consecutive urlhostword urlhostword urlhostword suggest insideemailaddress consecutive urlhostword urlhostword suggest consecutiveinurl urlhostword urlhostword urlhostword suggest showing page consecutiveinurl urlhostword urlhostword suggest showing pagej consecutiveinhyperlink urlhostword urlhostword urlhostword suggest link consecutiveinhyperlink urlhostword urlhostword suggest link scalelinearlyby anywhereonpage bio anywhereonpage interests anywhereonpage hobbies anywhereonpage resume anywhereonpage anywhereonpage vita anywhereonpage vitae anywhereonpage degrees anywhereonpage employment anywhereonpage anywhereonpage courses anywhereonpage classes anywhereonpage education anywhereonpage dept strongly suggest showing pagej scalelinearlyby anywhereinwindow bio anywhereinwindow interests anywhereinwindow hobbies anywhereinwindow resume anywhereinwindow anywhereinwindow vita anywhereinwindow vitae anywhereinwindow degrees anywhereinwindow employment anywhereinwindow anywhereinwindow courses anywhereinwindow classes anywhereinwindow education anywhereinwindow dept strongly suggest link anywhereinwindow links consecutive anyof interests interesting cool suggest showing pagej anywhereinwindow links consecutive anyof recommended stu suggest showing pagej consecutiveinasection anyof topics areas interest suggest consecutiveintitle main page weakly suggest consecutive contact information anywhereonpage lastname strongly consecutive check spelling strongly avoid consecutive search tips strongly avoid set advice rules home-page nder union rules non-terminal tokens home page rules home page rules advice rules seminar-announcement extractor section appendix present advice rules speaker location slots seminar-announcement extractor agent recall function named consecutive takes sequence words returns true phrase page returns false extract speaker variables firstname initial person nickname nickname person middlename middle initial person lastname person advice rules speaker slot backus-naur form aho listed non-terminal tokens talk talk vbg rules refer verbs base form present participle form recall choose stemming words study spk rules consecutive title spk strongly suggest showing pagej consecutive spk degree strongly suggest showing pagej consecutive spk intro spk strongly suggest showing pagej consecutive spk talk noun strongly suggest showing pagej consecutive spk talk strongly suggest showing pagej consecutive spk talk vbg strongly suggest showing pagej consecutive presented spk strongly suggest showing pagej consecutive talk spk strongly suggest showing pagej spk weakly suggest showing page spk lastname nnp firstname nnp lastname nnp nickname nnp lastname nnp firstname nnp middlename nnp lastname nnp nickname nnp middlename nnp lastname nnp title mrs prof professor mrs prof degree phd spk intro visitor seminar lecturer colloquium speaker talk talk noun talk presentation lecture speech talk talk lecture speak present talk vbg talking lecturing speaking 
presenting extract location variables locnumber cardinal number representing room number building number locnamea word building street locnameb word building street locnamec word building street advice rules location slot backus-naur form aho loc rules consecutive loc tagged strongly suggest showing pagej consecutive loc weakly suggest showing pagej consecutive loc tagged loc tokens strongly suggest showing pagej consecutive loc loc tokens strongly suggest showing pagej consecutive loc tokens loc tagged strongly suggest showing pagej consecutive loc tokens loc suggest showing pagej consecutive loc tokens loc tagged strongly suggest showing pagej consecutive loc tokens loc suggest showing pagej consecutive loc intro loc tagged strongly suggest showing pagej consecutive loc intro loc suggest showing page loc tagged locnumber locnamea nnp locnumber locnamea nnp locnamea nnp locnumber locnamea nnp locnameb nnp locnumber locnumber locnamea nnp locnameb nnp locnamec nnpj locnamea nnp locnameb nnp locnamec nnp locnumber loc locnumber locnamea locnumber locnameaj locnamea locnumber locnumber locnamea locnameb locnamea locnameb locnumber locnumber locnamea locnameb locnamec locnamea locnameb locnamec locnumber loc tokens hall auditorium building bldg center campus school conference conf room oor inst institute wing union college lounge lab laboratory library classroom tower street avenue alley road drive circle trail ave loc intro place location aho sethi ullman compilers principles techniques tools reading addison wesley belew finding cognitive perspective search engine technology york cambridge press bikel schwartz weischedel algorithm learns machine learning special issue natural language learning brill advances rule-based part speech tagging proceedings twelfth national conference arti cial intelligence seattle cali relational learning techniques natural language information extraction thesis department computer sciences texas austin craven kumlien constructing biological knowledge-bases extracting information text sources proceedings seventh international conference intelligent systems molecular biology heidelberg germany craven shavlik extracting tree-structured representations trained networks advances neural information processing systems vol denver croft turtle lewis phrases structured queries information retrieval proceedings fourteenth international acm sigir conference information retrieval chicago drummond ionescu holte learning agent assists browsing software libraries technical report tr- ottawa ottawa canada eliassi-rad building intelligent agents learn retrieve extract information thesis computer sciences department wisconsin madison appears technical report cs-tr- eliassi-rad shavlik intelligent web agents learn retrieve extract information szczepaniak segovia kacprzyk zadeh eds intelligent exploration web springer-verlag eliassi-rad shavlik theory-re nement approach information extraction proceedings eighteenth international conference machine learning williamstown feldman liberzon rosenfeld schler stoppi framework explicit bias revision approximate information extraction rules proceedings sixth acm sigkdd international conference knowledge discovery data mining boston freitag grammatical inference improve precision information extraction proceedings fourteenth international conference machine learning workshop automata induction grammatical inference language acquisition freitag machine learning information extraction informal domains thesis computer science department carnegie mellon pittsburgh freitag kushmerick boosted wrapper induction proceedings seventeenth national conference arti cial intelligence austin freitag mccallum information extraction hmms shrinkage proceedings sixteenth national conference arti cial intelligence workshop machine learning information extraction orlando goecks shavlik learning users interests unobtrusively observing normal behavior proceedings international conference intelligent user interfaces orleans joachims freitag mitchell webwatcher tour guide world wide web proceedings sixteenth international joint conference arti cial intelligence nagoya japan kushmerick wrapper induction ciency expressiveness arti cial intelligence leek information extraction hidden markov models master thesis department computer science engineering california san diego lieberman letzia agent assists web browsing proceedings fourteenth international joint conference arti cial intelligence montreal canada maclin shavlik creating advice-taking reinforcement learners machine learning mitchell machine learning mcgraw-hill ourston mooney theory nement combining analytical empirical methods arti cial intelligence pazzani kibler utility knowledge inductive learning machine learning pazzani muramatsu billsus syskill webert identifying interesting web sites proceedings thirteenth national conference arti cial intelligence portland ray craven representing sentence structure hidden markov models information extraction proc ijcai seattle rilo sundance sentence analyzer http utah projects nlp rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol mit press salton developments automatic text retrieval science salton buckley term-weighting approaches automatic text retrieval information processing management schapire singer improved boosting algorithms con dencerated predictions proceedings eleventh annual conference computational learning theory sejnowski rosenberg parallel networks learn pronounce english text complex systems selman kautz cohen local search strategies satis ability testing dimacs series discrete mathematics theoretical seymore mccallum rosenfeld learning hidden markov model structure information extraction proceedings sixteenth national conference arti cial intelligence workshop machine learning information extraction orlando shakes langheinrich etzioni dynamic sifting case study homepage domain proceedings sixth international world wide web conference santa clara shavlik calcari eliassi-rad solock instructable adaptive interface discovering monitoring information world-wide web proceedings international conference intelligent user interfaces redondo beach shavlik eliassi-rad building intelligent agents web-based tasks theory-re nement approach proceedings conference automated learning discovery workshop learning text web pittsburgh shavlik eliassi-rad intelligent agents web-based tasks advice-taking approach proceedings fifteenth national conference arti cial intelligence workshop learning text categorization madison soderland learning information extraction rules semi-structured free text machine learning special issue natural language learning sutton learning predict methods temporal erences machine learning sutton barto reinforcement learning mit press towell shavlik knowledge-based arti cial neural networks arti cial intelligence valiant theory learnable communications acm van rijsbergen information retrieval london buttersworths edition watkins learning delayed rewards thesis king college cambridge vitae tina eliassi-rad center applied scienti computing lawrence livermore national laboratory box livermore usa tina eliassi-rad computer scientist center applied scienti computing lawrence livermore national laboratory eliassi-rad earned computer sciences minor mathematical statistics wisconsin-madison received computer science illinois urbana-champaign computer sciences wisconsin-madison eliassi-rad research interests include machine learning knowledge discovery data mining arti cial intelligence text web mining information extraction information retrieval intelligent software agents jude shavlik computer sciences department wisconsin-madison dayton street madison usa jude shavlik professor computer sciences biostatistics medical informatics wisconsin madison shavlik wisconsin receipt phd illinois work explanation-based learning current research interests include machine learning computational biology intrusion detection shavlik recently completed three-year term editor-in-chief magazine serves editorial board half-dozen journals chaired international conference machine learning shavlik co-edited tom dietterich readings machine learning 

appears magazine special issue bioinformatics machine learning design interpret gene-expression microarrays michael molla michael waddell david page jude shavlik department computer sciences department biostatistics medical informatics wisconsin madison usa molla mwaddell dpage shavlik wisc abstract gene-expression microarrays commonly called gene chips make simultaneously measure rate cell tissue expressing translating protein thousands genes comprehensive snapshots biological activity infer regulatory pathways cells identify targets drug design improve diagnosis prognosis treatment planning suffering disease amount data technology produces manually analyze automated analysis microarray data offers opportunity machine learning significant impact biology medicine article describes microarray technology data produces types machine-learning tasks naturally arise data reviews recent prominent applications machine learning gene-chip data points related tasks machine learning impact biology medicine describes additional types interesting data recent advances biotechnology biomedical researchers collect introduction cell body organism dna genes portions dna code proteins commonly large biomolecules hunter covers introductory article special issue completeness review section article gene expressed two-step process gene dna transcribed rna translated protein technology gene-expression microarrays development started half revolutionary impact molecular biology monitor dna-to-rna portion fundamental biological process development biology interest researchers machine learning areas artificial intelligence ability measure transcription single gene ability measure transcription genes organism amount data biologists examine overwhelming data sets describe article consist roughly samples sample genes measured gene-expression microarray suppose patients disease disease finding combination genes expression levels distinguish groups patients daunting task human natural machine-learning algorithm illustrates challenge microarray data poses machine-learning algorithms dimensionality data high compared typical number data points preceding paragraph natural apply machine learning microarray data tasks arise analyzing microarray data correspondingly ways machine learning applicable present number tasks effort describe task concisely give concrete examples researchers addressed tasks summaries results discussing tasks approaches summarize relevant biology biotechnology article closes future research directions including analysis types high-throughput biological data similar microarray data based advances biotechnology relevant introductory biology method genes organism expressed production proteins building blocks life true organism bacterium plant human gene encodes specific protein point life cell proteins produced turning production specific proteins organism responds environmental biological situations stress developmental stages cell division genes contained dna organism mechanism proteins produced genes two-step process figure step transcription gene dna temporary molecule rna step translation cellular machinery builds protein rna message blueprint exceptions process steps dna replication central dogma molecular biology property dna rna common chain chemicals bases case dna bases adenine cytosine guanine thymine commonly referred rna set bases thymine rna uracil commonly referred property dna rna common called complementarity base binds complement result complementarity strand dna rna strong affinity reverse complement strand dna rna bases complementary original strand figure illustrates english text directionality reading strand dna rna figure dna read left-to-right rna read right-to-left reverse phrase reverse complement strictly true due process higher organisms called alternate splicing single gene encode multiple proteins purposes gene detection microarrays part genes called exons detected separately discuss detection splice variants article dna rna chains bases technically chains sugars case dna sugar deoxyribose case rna ribose full names deoxyribonucleic acid dna ribonucleic acid rna bases attached sugars figure central dogma molecular biology gene expressed transcribed rna sequence rna translated protein sequence amino acids dna replicated cell divides article focuses dna-to-rna-to-protein process dna gtaaggccctcgttgagtcgtatt rna cauuccgggagcaacucagcauaa figure complementary binding dna rna sequences complementarity central double-stranded structure dna process dna replication vital transcription addition role natural processes molecular biologists decades advantage complementarity detect specific sequences bases strands dna rna synthesizing probe piece dna reverse complement sequence detect introducing probe solution genetic material dna rna searched solution genetic material called sample theory probe bind sample probe finds complement sample discuss detail happen practice imperfect process excellent opportunity machine learning act binding probe sample called hybridization prior experiment labels probes fluorescent tag hybridization experiment easily scan probe hybridized make probes rna tend degrade faster ion translation dna proteinrna transcription reverse complement sample molecular biologist determine presence absence sequence interest sample gene chips recently dna probe technology adapted detection sequence tens thousands simultaneously synthesizing large number probes carefully placing probe specific position glass slide called spotted arrays attaching probes specific positions surface figure illustrates case predominant approach technology matured device called microarray gene chip utilization chips involves labeling sample probe spreading thousands copies labeled sample chip washing copies sample remain bound probe probes attached specific locations chip labeled sample detected position chip easily determined probe hybridized complement common gene chips measure expression level genes organism article focus task reader aware microarrays continually devised offering opportunities machine learning eache expression level snapshot rate protein produced organism cells time ideally biologists measure protein-production rate directly difficult impractical large scale measures expression level genes estimating amount rna gene present cell cell degrades rna quickly level accurately reflect rate cell producing protein order find expression level group genes labels rna cell group cells spreads rna word chip confusing familiar integrated circuits microarrays size computer chip approaches creating masking technology etching integrated circuits single gene chip typically unlike computer chip conceptually view gene chip holding thousands miniature test tubes confuse gene chips dna computing dna solve computational tasks traveling-salesman problem article address computer science solve biomedical tasks molecularbiology processes solve computational tasks chip probes genes interest organism bacterium coli small genome single gene chip probes detect genes organism human single chip subset genes present genome figure hybridization sample probe probes typically order -bases long samples times long large variation due process breaks long sequences rna small samples sonication sound waves data collection preprocessing runs microarray experiment optical scanner records fluorescence-intensity values level fluorescence spot gene chip case gene-expression arrays typically experiments measuring set genes circumstances normal conditions cell heated cooled drug added time points minutes adding antibiotic due steps manually perform produce rna sample sub-minute resolution current feasible perspective machine learning organize measured expression values ways table illustrates tables show view gene expression levels measured conditions constitute features alternatively table view experiment gene chip surface probes dna hybridization labeled sample rna case features expression values genes microarray case examples unlabeled tables labeled tables category interest sets measurements normal cells cancerous cells discuss article specific learning 
task interest dictate perspective data describe scenarios shown table published project views microarray data scenario presenting process measuring gene-expression levels simply creating probe gene computing rna made measuring fluorescence level probe-sample hybrid surprisingly complications remainder section summarizes major probes gene chips figure typically order bases long synthesizing longer probes practical genes order bases long find unique -base-long probe represent gene probes hybridize sample probe partially hybridize samples match perfect sample fold hybridize reasons microarrays typically dozen probes gene algorithm combines measured fluorescence levels probe set estimate expression level gene due nature experiments including fact microarrays nascent technology raw signal values typically great deal noise noise introduced synthesis probes creation labeling samples reading fluorescent signals ideally data illustrated table include replicated experiments gene-chip experiment cost hundred dollars practice replicates experiment small number times replicated experiments table ways representing microarray expression data machine learning panel measured expression levels single gene variety conditions panel measured expression levels thousands genes condition panels illustrate associate categories type cell genes normal diseased panels illustrate structure datasets unsupervised learning panels supervised learning features experiment experiment experiment gene gene xam gene features gene gene gene experiment experiment xam experiment features experiment experiment experiment category gene gene xam gene features gene gene gene category experiment experiment xam experiment accurately estimate absolute expression level gene work-around compute ratio fluorescence levels experimental condition obtained normal control conditions compare gene expression normal circumstances cell heated higher normal temperature called heat shock experimenters things coli heated gene expressed normal rate dealing ratios problem noise exacerbated numerator denominator small numbers newton kendziorski developed bayesian method reliably estimating ratios studies numbers table gene-expression ratios corrected minimize problems arise creating ratios small noisy numbers approach partner probe mismatch probes probes bases probe interest positions gene expression score function fluorescence levels dozen match mismatch probes wong table world-wide web url freely gene-expression data sets discuss article table url publicly microarray data sets url viable description ebi arrayexpress ebi microarray data repository ncbi nlm nih gov geo ncbi microarray data repository genome-www stanford microarray smd stanford microarray database rana lbl gov eisendata htm eisen-lab yeast data spellman genome wisc functional microarray htm wisconsin coli genome project llmpp nih gov lymphoma data shtml diffuse large b-cell lymphoma alzadh llmpp nih gov dlbcl molecular profiling rosenwald rii vantveer htm breast cancer prognosis van veer www-genome mit cgi-bin cancer datasets cgi mit whitehead center genome research including data golub lambertlab uams publicdata htm lambert laboratory data multiple myeloma wisc dpage kddcup kdd cup data task includes correlations genes expression levels biostat wisc craven kddcup kdd cup data task includes gene-expression data clinicalproteomics steem proteomics data mass spectrometry proteins snp cshl single nucleotide polymorphism snp data machine learning aid design microarrays previous section typically dozen probes represent gene probe-sample binding process perfect breslauer frank blocker marky job picking good probes fewer probes gene test genes microarray accurate results tobler molla nuwaysir green shavlik machine learning address task choosing good probes easy training examples task simply place probes set genes -base subsequence gene microarray probes produce strong fluorescence levels gene rna sample applied gene chip figure shows portion data tobler table illustrates cast probe selection machine-learning task figure result actual microarray experiment base-long probes bacterial genes chip shown quadrant chip darker point greater fluorescence original sample ideal case points equally strong fluorescence values mappings probe sequence fluorescence training examples machine-learning system data supplied courtesy nimblegen systems table probe-quality prediction set probes fluorescence tobler represent probe vector feature values specific base positions probe sequence pair adjacent bases positions probe bases probe percentage probe percentage pairs adjacent bases probe discretize fluorescence values groups good ambiguous bad discard ambiguous probes training group bad testing learn choose probes gene tobler microarray supplied nimblegen systems nuwaysir microarray company probes bacterial genes exposed chip sample rna genes measured fluorescence level location chip probes hybridized equally uniformly high signal entire chip clear figure case probes hybridize features table represent probe well-known learning algorithms learn predict candidate probe sequence good tobler found ten probes predicted trained neural network gene satisfy definition good probe randomly selecting probes satisfy good-probe definition machine learning biological applications microarrays section provide examples microarrays address questions molecular biology focusing role played machine learning cover supervised unsupervised learning discuss research microarray data types data machine-learning algorithms supervised learning experimental methodology supervised learning methods train examples categories order produce model classify examples learner evaluation type learner typically method called n-fold cross-validation form hold-out testing hold-out testing examples training data learning algorithm remaining held examples estimate future accuracy learned model n-fold cross validation examples divided subsets subset successively held-aside test set subsets pooled create training set results test-set runs averaged find total accuracy typical fact probe-selection project previous section describes application supervised learning results measured held-aside data project genes times learning algorithms trained genes resulting models tested held-out gene application supervised learning brown deals functional classifications genes representation data similar pictured table genes examples functional classifications classes features gene-expression values experimental conditions functional classifications simply classes genes defined genes function biologists years methods expression profiles multiple experiments multiple genes functional class brown train learner predict functional classification genes functional class table order machine-learning technique support vector machine svm table predicting gene biological function set genes represented similarly table gene features numeric expression levels measured multiple experimental circumstances experimental conditions include stresses temperature shock change introduction antibiotic experimental circumstances include developmental stages organism time points series category gene simply gene functional category set functional categories tca cycle respiration cytoplasmic ribosome proteasome histone helixturn-helix brown explanations classes learn predict functional category additional genes vector expression levels set experimental conditions simplest form support vector machine algorithm attempts find linear separator data points classes figure illustrates svm seek maximize margin separation classes order improve chance accurate predictions future data maximizing margin viewed optimization task solvable linear quadratic programming techniques practice good linear separator data support vector machines based kernel functions efficiently produce separators non-linear kernel functions improve accuracy svm brown empirically found gene-expression data simple linear svm produce accurate predictions linear svm generalize non-svm 
supervised learning methods data genes data set trained svm correctly identifies ribosomal proteins produces false positives supervised learner correctly identifies number produces false positives figure support vector machine differentiating genes involved respiration involved tca cycle maximizing margin n-dimensional space defined expression levels genes experimental conditions simple experimental conditions time time greater paper brown number genes categorize higher brown paper number genes unsupervised learning unsupervised learning learning set examples features categories examples examples type commonly called unlabeled examples context gene chips means learning models biological processes relationships genes based expression levels respiration genes tca cycle genes improve models checking learners answers sort externally provided ground truth clustering methods successful efforts unsupervised learning involve clustering algorithms including work algorithmic analysis microarray data due nature evolution clustering biological data makes sense task long history computational biology past individual protein dna sequences commonly clustered clustering algorithms group cluster examples based similarity feature values gene-expression values eisen spellman brown botstein describe method table presents problem address table clustering genes based expression levels set genes organism represented similarly table gene features gene numeric expression levels experimental circumstances environmental stresses development stage cluster genes based similarity expression values eisen clustered expression patterns number experiments genes yeast saccharomyces cerevisiae spellman experiments measure genetic response environmental stresses cold shock measure transcription stages life cycle organism cell division gene measured expression levels gene experiments features data format table standard statistical technique describe similarity examples terms features distance metric specifically eisen perform hierarchical clustering algorithm clusters repeatedly pairing similar examples removing data set adding average set examples method pairs examples pair average examples producing hierarchy clusters figure shows hypothetical output hierarchical-clustering algorithm axis spans experimental conditions y-axis spans genes measured expression level gene experiment relative organism normal conditions dictates shading graph higher expression level lighter point genes ordered similar genes regard experimentally derived values grouped visually result intuitive visual guide researcher quickly discern blocks similar genes regard set experiments figure graphical output cluster analysis similar representation table integers represented gray-scale intensity unlike table genes sorted similarity similar genes respect vector expression values grouped realistic diagram made real data eisen due flexibility intuitive nature clustering methods proven popular biologists laboratories conduct microarray experiments clustering genes microarray experiments standard practice clustering experiments common practice table thomas ran microarrays rna mice subjected variety toxic compounds microarray compound hierarchically clustered microarray experiments found clusters correspond closely toxicological classes compounds thomas report supervised learning experiments experiments gene hierarchy table clustering experimental conditions based gene-expression levels produce set microarray experiments represented similarly table experiment instance thomas experiment involves subjecting mice toxic compound features numeric expression levels microarray genes cluster experimental conditions based similarity geneexpression vectors produce bayes networks unsupervised learning algorithm analysis microarray data bayesian network bayes net bayes net directed acyclic graph specifies joint probability distribution variables arcs nodes dependencies variables absence arcs infer conditional independencies figure simple capturing conditional independence exists bayes net provide compact representation joint probability distribution full joint table node bayes net conditional probability table specifies probability distribution variable values parents values set nodes arcs denoted probability distribution bayes net variables defined iiipp xpaxxpxxxxp friedman halpern technique area microarray expression data cerevisiae data eisen clustering friedman show statistical methods bayes network representing observed relationships expression levels genes learned automatically expression levels genes variety experiments table table learning bayes networks set genes organism represented similarly table genes numeric expression levels experimental circumstances environmental stresses developmental stage features learn bayesian network captures joint probability distribution expression levels genes figure simple bayesian network illustrative bayes network describes relationships hypothetical genes probabilities refers probability gene expressed note conditional probabilities rely parent variables gene expression levels simplicity figure genes expressed expressed richer model variables correspond numeric expression level application learning bayes nets gene expression microarray data receiving great deal attention resulting bayes nets potentially provide insight interaction networks cells regulate expression genes developed algorithms construct bayes network models data substantial success interpret graph figure gene gene gene expressed turn influencing gene caution exercised gene gene gene gene interpreting arcs causality automatically constructed models presence arc represents correlation variable good predictor correlation arise parent node influences behavior child node arise reverse influence indirect chain influence involving features method addressing causality bayes net learning genetic mutants gene knocked regev elidan friedman approach model expression cerevisiae baker yeast gene cerevisiae biologists created knock-out mutant genetic mutant lacking gene parent gene bayes net knocked child status remains unchanged arc parent child captures causality current limitation approach organism extensive set knock-out mutants method addressing issue causality explored ong glasner page time-series data time-series data simply data organism time points ong time-series data tryptophan regulon coli khodursky regulon set genes co-regulated tryptophan regulon regulates metabolism amino acid tryptophan cell ong data infer temporal direction gene interactions suggesting causal relations order model temporal directionality employ representation dynamic bayesian network dynamic bayesian network genes represented node nodes number time points nodes represents gene expression level time point algorithm learn relationships genes time time makes network identify feedback loops cases gene directly chain influence influences regulation feedback loops common gene regulation additional source data recent trend computational biology microarray data source input learning algorithm section briefly describe investigations recent approaches clustering genes rely expression data background knowledge problem domain hanisch zien zimmer lengauer present approach add term distance metric represents distance genes biological-reaction network biological-reaction network set proteins intermediates reactions chemicals carry cooperative function cell respiration metabolism function assembly lines protein turns chemical chemical adding removing atoms changing conformation protein turns chemical chemical similar fashion depicts entities biological networks edges graph reactions vertices biologists discovered networks experimental means networks understood genes nearer biological network considered purposes clustering similar genes farther biolingua system shrager langley pohorille network graph describing biological pathway updates results microarray experiments algorithm adds removes links biological pathway based link experimental support microarray data form theory revision small subtopic machine learning chapter mitchell network structures biolingua similar dynamic bayes network links imply causality correlation expression gene shrager achieve perspective combination domain knowledge time-series data causal connection events require forward temporal direction representation differs bayesian 
approaches biolingua links qualitative quantitative joint statistical distribution probabilities linked nodes algorithm qualitative representation simply specifies influences positive negative causal links representation mirrors type network description biologists familiar making resulting model source data dna sequence organisms including coli fruit fly yeast mouse humans completely sequenced words sequence entire string millions billions bases constituting genomes complete progress large amounts data dna sequence surrounding gene impact regulation regulation function craven page shavlik bockhorst glasner machine learning integrate coli dna sequence data including geometric properties spacing adjacent genes predicted dna binding sites important regulatory proteins microarray expression data order predict operons operon set genes transcribed operons provide important clues gene function functionally related genes operon dna sequence information method segal taskar gasch friedman koller developed goal jointly model gene-expression data transcription factor binding sites transcription factors proteins bind subsequence dna gene encourage start transcription subsequence transcription factor binds called transcription factor binding site genes similar expression profiles controlled transcription factor similar transcription factor binding sites sequence preceding order model gene expression information sequence information jointly segal probabilistic relational models prm prm thought bayesian network variables fields relational database strength representation prm learned relational database multiple relational tables learning algorithms ordinary bayes nets require data single table tables represent types data sequence data expression data approach segal expectation-maximization algorithm learn prm models clusters genes cluster transcription factor binding sites front genes dna excellent source supplementary material large amount humanproduced text genes microarray proteins contained biomedical digital libraries expert-produced annotations biomedical databases molla andrae glasner blattner shavlik investigate text curated swissprot protein database bairoch apweiler features characterizing gene coli microarray text-based features employ machine-learning algorithm produce rules explain genes expression levels increase coli treated antibiotic wealth data reaction pathways dna sequences genomic structure information gleaned protein-dna protein-protein binding experiments carefully annotated databases scientific literature supplement table meager representation microarray experimental data exploiting richness offers exciting opportunity machine learning machine learning medical applications microarrays supervised unsupervised learning methods proven interpretation microarray data context basic molecular biology turn application microarrays medicine microarrays improving diagnosis disease facilitating accurate prognosis patients guiding understanding response disease drugs ways improve process drug design technologies someday lead medicines personalized genetic level mancinelli cronin sadee section attempt provide sense large number future opportunities machine learning medical applications microarray technology expand disease diagnosis common issue medicine distinguish accurately similar diseases order make accurate diagnosis patient molecular-level classification gene microarrays proven task technique tasks discuss context cancer diagnosis class discovery class prediction class discovery table task identifying classes cancer class prediction table task assigning tumor class accurate diagnosis crucial obtaining accurate prognosis assigning treatment disease table discovering disease classes set microarray experiments cells patient data represented similarly table patients group closely related diseases patient numeric expression levels microarray experiment constitute features disease classification patient patient category clustering ignoring disease category find cells fit current disease classification assume cells belong disease classifications table predicting existing disease classes data table learn model accurately classify cell disease classification golub microarray technology class discovery class prediction types closely related cancers acute lymphoblastic leukemia acute myeloid leukemia aml distinction cancers long established single test sufficient accurately diagnose current medical practice series separate highly specialized tests combined results tests fairly accurate misdiagnoses occur golub group microarrays address diagnostic issue analyzing samples patients tumors time microarrays primarily highly purified cell lines grown laboratories microarrays analyze samples directly patients noise due genetic variation patients obscure results reason working samples patients important large number patients sample genetic variation unrelated disease obscure results supervised-learning techniques induce diagnosis model gene-expression data number patients disease accurate predictive model obtained patients previously undiagnosable classified ensemble weighted voters figure aml diagnosis task golub correctly classify samples test set ensemble rejects samples test set close call type gene microarray data class-discovery task commonly discovers classes unsupervised learning technique cluster examples matches clusters produced disease types considers remaining clusters unstudied disease classes primary challenge class discovery ensuring clustering biologically meaningful unsupervised learning current disease classification clustering based wrong variations patients performing unsupervised learning group patients similar cancers obtaining clustering based ethnicity patients result grouping optimal algorithm offers insight diseases studied important challenge unsupervised learning significantly affect usefulness results obtained granularity examples clustered find optimal clustering number clusters important find clustering accurately captures level differentiation sought case distinction diseases gene-microarray technology patient samples highly purified laboratory samples exercise caution ensure genes chosen predictors biologically relevant process studied relevant solid-tumor analysis due method obtained tumor-biopsy specimens large variations amount surrounding connective tissue obtained tumor cells applying class discoveries predictions made data cells analyzing learned predictive model result making decisions wrong basis skill person performed biopsy desired basis underlying tumor biology reason learning techniques create directly comprehensible models decision trees figure ensembles voters figure bayesian networks types applications preferred problem specific collection specimens solid tumors case dealing cancers blood reason higher accuracies generally found machine learning cancers blood solid tumor cancers induced models easily comprehended humans neural networks support vector machines primarily diagnosis molecular-level classification limited simply distinguishing diseases methods class prediction class discovery predict tumor site origin stage grade figure comprehensible models disease diagnosis two-level decision tree discriminating myeloma cells normal cells based geneexpression levels cells ensemble voting decision stumps one-level decision trees task case unweighted voting decision stump single vote simple majority vote distinguish myeloma cells normal cells case weighted voting decision stumps votes counted choose variety methods determine weight votes disease prognosis discussing molecular-level classification supervised learning accurately diagnose patient set similar diseases types techniques predict future outcome prognosis disease making accurate prognosis complicated task physicians depends large number factors physician time diagnosis accurately diagnosing disorder normal myeloma normal gene myeloma present absent gene normal present absent gene myeloma present absent present gene absent gene present myeloma normal myeloma absent predicting response disorder drugs make accurate prognosis patient microarray analysis predict prognosis patients types cancer investigators chosen cancer model disease variety reasons prognosis patient cancer highly dependant cancer metastasized shown important components biology malignant cell inherited type cell initially gave rise cancer lifecycle stage cell time transformation figure illustrates process finally 
providing accurate prognosis patient crucial deciding aggressive treatment reasons researchers typically employ supervised learning techniques address problem table normal cells dna damage tumor cells dna damage cancer cells metastasis figure transformation development cancerous cells normal cells step transformation dna damage normal cells multiplying uncontrollably forming benign tumor dna damage occurs cells convert benign cancerous final stage progression cells metastasis process cancer gains ability spread locations body group supervised learning approach prognosis prediction van veer employ ensemble voters classify breast cancer patients groups good prognosis metastasis years initial diagnosis poor prognosis distant metastases found years begin select genes genes microarray highest degree association disease outcome calculated correlation coefficient full set examples rank genes correlation coefficients repeat leave-one-out cross-validation examples ensemble sizes found ensemble size genes cross-validated accuracy table predicting prognosis cancer patients set microarray experiments cells patient data represented similarly table patients type cancer stages progression patient numeric expression levels genes microarray features true prognosis patient patient category categories include cancer metastasize prognosis patient patient survive years formulate real-valued prediction task years recurrence cancer learn model accurately predicts category patients belong methodology errors perspective current machine-learning practice chose features entire set examples constitutes information leakage examples including test sets cross validation guide selection features report ensemble size size works cross-validation experiment constitutes information leakage optimized parameters learning system size ensemble examples test sets errors estimated accuracy overestimation overfit test data methodology separately select parameters fold n-fold cross-validation experiments recognizing issues van veer reported modified version algorithm online supplement article order address concerns reduced cross-validated accuracy question revised approach leads overestimate future accuracy true prognosis patient years collecting labeled training examples challenging task fact gene-expression measurement technology rapidly changing complicates creation good training sets prognosis tasks prognosis prediction commonly thought supervised learning task valuable information disease gained unsupervised learning alizadeh utilized unsupervised learning techniques cluster patients diffuse large b-cell lymphoma clusters discovered average five-year survival patients cluster compared cluster average five-year survival patients results illustrate clusters found unsupervised learning biologically medically relevant solely employing clustering algorithms users machine learning task cast form directed supervised learning training examples labeled respect important property interest response drugs drugs typically small molecules bind protein body act inhibit activate activity figure pharmaceutical companies limited designing drugs high level success low level side effects average person individual responds drug complex influenced unique genetic makeup figure summarizes millions cases annually adverse reactions drugs cases drugs ineffective field pharmacogenomics addresses tight interrelation individual genetic makeup response drug table microarrays play role area related pharmacogenomics molecular-level profiling main difference fields pharmacogenomics deals finding genetic variations individual people predict individual person response drug goal molecular-level profiling find genetic variations individual diseased cells predict cell response drug analyzing specific cells important predicting drug response due highly variable nature cancer significant variation exists tumors type cancer significant variation exists organisms species million cases adverse drug reactions deaths united states lazarou figure drug binding protein inhibitor drug shown stick model dark gray bound protein hivprotease mutant shown space-filling model lighter gray molecular-level profiling found effective treating types cancers recent rosenwald lymphoma leukemia project study investigates large-b-cell lymphoma type cancer curable chemotherapy patients thought large-b-cell lymphoma single disease class diseases morphologically differ response types therapy figure major factors affect person response drug genetic variation age nutrition health status environmental exposures concurrent therapy drug response table predicting drug response patients disease set microarray experiments cells patient infected disease data represented similarly table microarray experiment gene numeric expression level experiment serving feature augment gene-expression features additional features age gender race patient drug-response classification patient category typical categories good response improved health bad response bad side effects response build model accurately predicts drug response patients analyzing gene-expression profiles cells large-b-cell lymphoma tumors rosenwald developed method predict survival rates diffuse large-b-cell lymphoma based microarray data training data patents outcomes anthracycline-based chemotherapy predict held-out test-set patients respond type chemotherapy actual five-year survival rate predicted respond predicted respond actual five-year survival rate investigation large-b-cell lymphoma yielded prognosis information type insight genetic variations cells affect response drugs eventually suggest drugs treat types cells respond chemotherapy lead deeper understanding disease mechanism gain deeper insight diseases study lines molecularlevel classification pharmacogenomics molecular-level profiling blur accurate sub-typing single disease ultimately lead considered separate diseases deeper understanding underlying mechanisms diseases lead discovery previously distinct diseases manifestations underlying disease personalized medicine eventually lead classifying patients based drug work designing drug specifically tailored patient exact disorder genetic makeup data types high-throughput biotechnology tools section briefly discuss types high-throughput molecular-level biological data machine learning applicable high-throughput techniques permit scientists make thousands measurements biological sample time effort traditionally make handful measurements data sets arising additional techniques similar gene microarrays similar tabular representation high dimensionality single nucleotide polymorphisms snp genome researchers learned variation individuals result number discrete single-base human genome discovery intense effort catalog discrete genetic differences single positions variation dna called single nucleotide polymorphisms snp illustrated figure presently infeasible obtain sequence dna patient feasible quickly measure patient snp pattern dna bases large number snp positions machine learning applied snp data manner similar application microarray data snp data file table employ supervised learning identify differences snp patterns people respond drug versus respond poorly data points classified disease versus healthy supervised learning identify snp patterns predictive disease highly predictive snp genes genes important conferring disease resistance susceptibility proteins encode potential drug targets figure single nucleotide polymorphism differences genomes individuals generally discrete single-base shown simplified genomes people differences highlighted dna bases identical twos sequences person person challenge snp data collected unphased form suppose coming people dna strands figure refer copies chromosome single person humans copies chromosome current snp technology return row table provide information snp variants chromosome phase information prediction task machine-learning algorithm unsuccessful table sample single nucleotide polymorphism data file humans paired chromosomes record base chromosome snp position notice chromosomes pair dna strands famous double-helix due complementarity paired strands record bases snp position biologists identified million snp positions human genome typical snp data file thousands snp cost data gathering proteomics gene microarrays measure degree gene transcribed measure surrogate gene expression complete process transcription 
translation protein levels difficult measure rna levels increased transcription increased protein production desirable measure protein directly called proteomics contrast genomics rubric gene microarrays falls organism proteome full complement proteins mass spectrometry makes detect presence proteins sample details mass spectrometry scope article figure sense type data convert feature vector perform type peak picking result picking peaks masssnp snp snp response person positive person negative person positive spectrometry data feature vector x-y pairs entry corresponds mass-tocharge ratio x-axis peak height y-axis mass charge tiv ity figure sample mass-spectrometry output protein fragments mass charge values horizontal axis vertical axis reflects amount protein fragment sample plotted peak heights typically normalized relative highest intensity mass-spectrometry data presents major challenges raw form peaks typically correspond pieces proteins peptides entire proteins work features preprocess data attempting map set peaks smaller set proteins mass spectrometry extremely poor giving quantitative values peak heights calibrated sample normalized peak height mass-to-charge ratio greater amount protein ratio greater desirable binary features continuous mass-to-charge ratio peak major challenge mass spectrometry data peaks lower-concentration proteins distinguished background noise discussion focused mass-spectrometry data similarities gene-microarray data phrase proteomics refers broader range data types significantly includes data protein-protein interactions data poses interesting opportunities challenges machine learning kdd cup cheng contained challenging task involving protein-protein interaction data metabolomics tempting data dna snp rna microarrays proteins mass spectrometry access important aspects cell behavior fact aspects remain unmeasured high-throughput techniques aspects include post-translational modifications proteins phosphorylation cell structure signaling cells aspects exist high-throughput measurement techniques present insight aspects cell behavior obtained examining small molecules low molecular weight cell molecules important inputs outputs metabolic pathways cell high-throughput techniques measuring molecules exist area studying data molecules called metabolomics oliver winson kell baganz highthroughput metabolomics data represented naturally feature vectors manner similar gene-microarray data mass-spectrometry data metabolomics data features correspond small molecules feature takes expresses quantity molecule type cell systems biology additional forms high-throughput biological data future motivation developments shift biology systems approach commonly referred systems biology hood galas note past biologists study complex system gene protein time systems approach permits study elements system response genetic digital environmental perturbations state study cellular organismal biology systems approach beginning require integrated teams scientists disciplines biologists chemists computer scientists engineers mathematicians physicists methods acquiring analyzing high-throughput biological data needed hood galas constructing models biological pathways entire cell silico cell goal systems biology preeminent date systems approach gene-regulatory model davidson developed embryonic development sea urchin model developed years data collected benefit high-throughput techniques machine learning potential major player systems biology learning algorithms construct modify models based vast amounts data generated high-throughput techniques conclusion machine learning offer revolutionary technology gene microarrays microarray design basic biology medicine researchers employed machine learning make gene chips practical gene chips changed field biology data years collect takes week biologist aided greatly supervised unsupervised learning methods make sense large amount data additional challenging learning tasks continue arise field matures result rapid increase rate biologists understand molecular processes underlie govern function biological systems impact progress slowly medicine molecular biology microarray technology coupled machine learning variety important medical applications diagnosis prognosis drug response applications similar deal predicting aspect disease differentiating molecular level individuals population patients cells difference applications concerns predicted disease classification focuses distinguishing cells possibly related diseases disease prognosis predicting long-range results pharmacogenomics molecular profiling molecular-level measurements differentiate patients cells disease based reaction drugs vast amount genomic similar types data continues grow role computational techniques machine learning grow algorithms enable handle task analyzing data yield valuable insight biological systems surround diseases affect acknowledgements writing article partially supported grants nih nih nih nlm nsf nlm alizadeh eisen davis lossos rosenwald boldrick hajeer tran powell yang marti moore hudson lewis tibshirani sherlock chan greiner weisenburger armitage warnke levy wyndham wilson grever byrd botstein brown staudt distinct types diffuse large b-cell lymphoma identified gene expression profiling nature bairoch apweiler swiss-prot protein sequence database supplement trembl nucleic acids research breslauer frank blocker marky predicting dna duplex stability base sequence proceedings national academy science usa brown grundy lin cristianini sugnet furey ares haussler knowledge-based analysis microarray gene expression data support vector machines proceedings national academy science usa cheng hatzis hayashi krogel morishita page sese report kdd cup sigkdd explorations craven page shavlik bockhorst glasner multiple levels learning diverse evidence sources uncover coordinately controlled genes proceedings international conference machine learning morgan kaufmann palo alto davidson rast oliveri ransik calestani yuh amore minokawa hynman arenas-mena otim brown livi lee revilla alistair pan schilstra clarke arnone rowen cameron mcclay hood bolouri genomic regulatory network development science eisen spellman brown botstein cluster analysis display genome-wide expression patterns proceedings national academy science usa friedman halpern modeling beliefs dynamic systems part revision update journal research golub slonim tamayo huard gaasenbeek mesirov coller loh downing caligiuri bloomfield lander molecular classification cancer class discovery class prediction gene expression monitoring science hanisch zien zimmer lengauer co-clustering biological networks gene expression data bioinformatics hood galas digital code dna nature hunter introduction molecular biology computer scientists magazine issue khodursky peter cozzarelli botstein brown yanofsky dna microarray analysis gene expression response physiological genetic affect tryptophan escheria coli proceedings national academy science usa lazarou pomeranz corey incidence adverse drug reactions hospitalized patients journal american medical association wong model-based analysis oligonucleotide arrays expression index computation outlier detection proceedings national academy science usa mancinelli cronin sadee pharmacogenomics promise personalized medicine aaps pharmsci article molla andrae glasner blattner shavlik interpreting microarray expression data text annotating genes information sciences mitchell machine learning mcgraw-hill boston oliver winson kell baganz systematic functional analysis yeast genome trends biotechnology ong glassner page modelling regulatory pathways coli time series expression profiles bioinformatics newton kendziorski richmond blattner tsui differential variability expression ratios improving statistical inference gene expression microarray data journal computational biology nuwaysir huang albert singh nuwaysir pitas richmond gorski berg ballin mccormick norton pollock sumwalt butcher porter molla hall blattner sussman wallace cerrina green gene expression analysis oligonucleotide arrays produced maskless lithography genome research regev elidan friedman inferring subnetworks perturbed expression profiles bioinformatics rosenwald wright chan connors campo fisher gascoyne mullerhermelink smeland staudt molecular profiling predict survival chemotherapy diffuse large-b-cell 
lymphoma england journal medicine segal taskar gasch friedman koller rich probabilistic models gene expression bioinformatics shrager langley pohorille guiding revision regulatory models expression data proceedings pacific symposium biocomputing world scientific lihue hawaii spellman sherlock zhang iyer anders eisen brown botstein futcher comprehensive identification cell cycle-regulated genes yeast saccharomyces cerevisiae microarray hybridization molecular biology cell thomas rank penn zastrow hayes pande glover silander craven reddy jovanovich bradfield identification toxicologically predictive gene sets cdna microarrays molecular pharmacology tobler molla nuwaysir green shavlik evaluating machine learning approaches aiding probe selection gene-expression arrays bioinformatics van veer dai van vijver hart mao peterse van der kooy marton witteveen schreiber kerkhoven roberts linsley bernards friend gene expression profiling predicts clinical outcome breast cancer nature 

knowledge-based neural networks improve algorithms refining chou-fasman algorithm protein folding richard maclin jude shavlik computer sciences dept wisconsin dayton madison maclin wisc keywords multistrategy learning theory refinement neural networks finite-state automata protein folding chou-fasman algorithm running head neural networks improve algorithms abstract paper describes connectionist method refining algorithms represented generalized finite-state automata method translates rule-like knowledge automaton artificial neural network refines reformulated automaton applying backpropagation set examples technique translating automaton network extends kbann algorithm system translates set propositional rules neural network extended system fskbann refine large class algorithms represented state-based processes test fskbann improve chou-fasman algorithm method predicting globular proteins fold empirical evidence shows multistrategy approach fskbann leads statistically significantly accurate solution original chou-fasman algorithm neural network trained standard approach extensive statistics report types errors made chou-fasman algorithm standard neural network fskbann network paper appears machine learning journal volume neural networks improve algorithms introduction machine learning applied complex real-world problems researchers found turning systems combine knowledge multiple sources standard approach incorporate existing knowledge domain empirical learning system produce accurate solution artificial neural networks anns shown powerful technique empirical learning recently anns largely unable advantage existing problem-specific knowledge paper describes extension kbann system towell shavlik noordeweir connectionist system refines symbolic domain knowledge extended system called finite-state kbann fskbann translates domain theories state information represented generalized finite-state automata fsas hopcroft ullman neural networks system refines networks backpropagation rumelhart hinton williams set examples application kbann domain gene recognition towell noordeweir towell shavlik showed domain theories refined neural networks accurate unrefined domain knowledge neural networks trained standard manner work demonstrates promise multistrategy approach based combination symbolic representation rules numeric representation inherent neural networks allowing domain theories express state information fskbann greatly extends applicability kbann approach researchers machine learning generally publish algorithms sets rules machine learning researchers refer domain theories algorithms maintain sense state extension makes easier machine learning refine existing real-world knowledge test extended system refining chou-fasman algorithm chou fasman predicting aspect globular proteins fold important difficult problem molecular biology state domain theory represents context problem problem find path room state variables include light rules introduced solve problem account state problem rules turn light considered state light style problem solving problem solved step series actions leading state leads goal state turning light navigating couch protein-folding problem open problem increasingly critical human genome project watson proceeds chou-fasman algorithm focus paper best-known widely-used algorithms field protein-folding problem interest number machine learning techniques applied problem including neural networks holley karplus qian sejnowski inductive logic programming muggleton king case-based reasoning cost salzberg press multistrategy learning zhang work shows multistrategy approach combining chou-fasman algorithm neural network produces accurate result method paper presents empirically analyzes fskbann approach problem solving domains prior state-based knowledge exists section presents basic kbann algorithm discusses extension algorithm handle state information section defines protein-folding problem reviews previous approaches neural networks improve algorithms experiments investigate utility fskbann problem finite-state kbann describing fskbann review basic kbann knowledge-based artificial neural networks algorithm towell kbann translates domain theory represented simple rules promising initial neural network technique neural networks advantage pre-existing knowledge problem kbann takes input set propositional non-recursive rules shown figure figure shows dependencies rules dependency link propositions arcs show conjunctive dependencies set dependencies easy map rules network replacing proposition unit adding units conjunctions combined disjunctions figure displays resulting network network behavior rules input vector setting weights biases units network kbann connects unit unconnected units lower level network small-weight link resulting network appears figure kbann adds connections learn dependencies backpropagation learning details towell figure table type problem solving fskbann applicable state-dependent domain theory goal description repeat set input externally-provided information current internal representation problem-solving state produce domain theory goal description output results specific problem solving step internal representation problem-solving state termination criterion met handle wider class problems work extends kbann translate domain theories represented generalized fsas main extension fskbann type network domain theory mapped fskbann maps domain theories variant simple recurrent networks jordan elman subset network output copied back input network step copied output represents current state neural networks improve algorithms calculated network calculating succeeding state table describes class problem solvers fskbann applicable problem solver determines state basis externally-provided input internal representation current state problem solution externally-provided input involve description initial state changing measurements sensors reactive planner task problem solver produce output step problem solution operator apply choose internal representation state problem solution process repeats termination condition met goal state reached description table essentially definition state-based problem solving contribution fskbann mechanism neural networks improve statedependent domain theory inputs outputs table directly map input output units neural network basic kbann algorithm domain theory determine number connectivity hidden units figure shows diagram type network produced fskbann figure fskbann approach requires user provide sample input output pairs train network backpropagation requires inputs outputs bounded size means domain theory store finite amount state information finally fskbann requires domain theory propositional good mechanism exists dealing predicate calculus variables neural networks current limitations real-world algorithms adequately represented finite-state framework open empirical question neuralbased approach generalizes inductive logic programming approaches muggleton quinlan learn larger class languages protein-folding problem section describes protein-folding problem open problem field molecular biology examined researchers biological machine learning community outline standard algorithm biological community solve problem description algorithm mapped section framework proteins long strings amino acids hundred elements long average amino acids represented capital letters string amino acids making protein constitutes primary structure protein protein forms notion fsa fskbann generalized taking single input step fsa set input values neural networks improve algorithms folds three-dimensional shape protein tertiary structure tertiary structure important form protein strongly influences function present determining tertiary structure protein laboratory costly time consuming alternative solution predict secondary structure protein approximation secondary structure protein description local structure surrounding amino acid prevalent system determining secondary structure divides protein types structures -helix regions -strand regions random coils regions figure shows tertiary structure protein shape divided regions secondary structure purposes secondary structure protein simply sequence primary sequence table shows sample mapping protein primary secondary structures figure table primary secondary structures sample protein primary amino acids secondary local structures table predictive accuracies standard algorithms biological literature 
solving secondary-structure problem chou fasman garnier robson lim data sets test algorithms amino acids proteins part coil structures accuracy achieved trivially predicting coil important note biological researchers algorithms account local information achieve limited accuracy wilson generally believed cohen presnell personal communication table accuracies non-learning prediction algorithms method accuracy comments chou fasman data qian sejnowski lim nishikawa garnier robson data qian sejnowski approach secondary-structure problem learning method neural networks holley karplus qian sejnowski neural networks efforts input window amino acids consisting central amino acid predicted number amino acids sequence similar nettalk networks sejnowski rosenberg output network secondary structure central amino acid figure shows general structure type neural networks improve algorithms network table presents results studies figure approach combine knowledge biological methods neural learning method hopes achieving solution chose chou-fasman algorithm chou fasman domain theory widely chou-fasman approach find amino acids part -helix -strand regions extend predictions neighboring amino acids figure schematic overview algorithm step process find nucleation sites nucleation sites amino acids part -helix -strand structures based neighbors conformation probabilities rules reported chou fasman sites algorithm extends structure forward backward protein long probability part -helix structure remains sufficiently high predicting -helix -strand regions chou-fasman algorithm compares relative probabilities regions resolve predictions overlap figure algorithm easily represented propositional rules prediction amino acid depend predictions neighbors represent algorithm generalized fsa figure start state fsa coil make predictions protein protein scanned input step amino acid classified neighbors window prediction based prediction state depends transitions valid current input window notion table neural network results secondary-structure prediction task number ofmethod accuracy hidden units window size holley karplus qian sejnowski fskbann scans protein left-to-right right-to-left sums results simulate extending nucleation sites directions neural networks improve algorithms transition figure automata complex transition set rules dependent input window current state fsa transition state helix state coil break-helix represented rule coili helixibreak-helix term break-helix input network predicate derived input break-helix defined terms rules break-helix helix-break helix-break break-helix helix-break helix-indiff terms helix-break helix-break helix-indiff defined rules appendix full set rules defining fsa figure table shows algorithm fits fskbann framework table resulting network appears figure shown recall network low-weighted links hidden units network similar figure major differences input network includes copy past prediction made network represents state network topology hidden units determined rules implementing chou-fasman algorithm figure table mapping chou-fasman algorithm fskbann framework domain theory chou-fasman algorithm goal assign secondary structure amino acid external input sliding window amino acids current state predicted secondary structure previous amino acid results predicted secondary structure current amino acid state ditto neural networks improve algorithms experimental study section reports experiments protein-folding problem evaluate fskbann demonstrate fskbann small statistically significant gain accuracy standard artificial neural networks anns non-learning choufasman algorithm section in-depth empirical analysis strengths weaknesses methods experimental details experiments data set qian sejnowski data set consists segments proteins total amino acids average length amino acids segment amino acids part coil structures part -helix structures part -strand structures randomly divided proteins ten times disjoint training test sets contained two-thirds proteins one-third proteins original proteins backpropagation train neural networks approaches fskbann standard anns training terminated patience stopping criterion fahlman lebiere training divided proteins training portions training set tuning set employ training set train network tuning set estimate generalization network epoch system trains network amino acids training set assesses accuracy tuning set retain set weights achieving highest accuracy tuning set set weights measure test set accuracy fskbann randomly chooses representative tuning set considers tuning set representative percentages type structure coil tuning set roughly approximate percentages training proteins note system testing set comparing percentages empirical testing reported found tuning set size proteins achieves results fskbann anns important note style training reported qian sejnowski tested network periodically retained network achieved highest accuracy test set fskbann hidden units represent chou-fasman domain theory qian sejnowski report networks generalized hidden units methodology outlined compared standard anns hidden units found networks hidden units generalized slightly paper experiments hidden units standard anns added advantage fskbann standard networks number hidden units results analysis table results averaged test sets statistics reported percent accuracy percent accuracy secondary structure correlation coefficients structure correlation coefficient good evaluating patience criterion states training continue error rate decreased number training cycles study set number epochs determined empirical testing neural networks improve algorithms effectiveness prediction classes separately resulting gain accuracy fskbann anns non-learning chou-fasman method statistically significant level confidence t-test apparent gain accuracy fskbann ann networks appears fairly small percentage points number misleading correlation coefficients give accurate picture show fskbann -helix coil prediction -strand prediction reason ann solution fairly accuracy predicts large number coil structures largest class predictions gain accuracy fskbann chou-fasman algorithm fairly large exhibits gain correlation coefficients interesting note fskbann chou-fasman solutions produce approximately accuracy -strands correlation coefficients demonstrate chou-fasman algorithm achieves accuracy predicting larger number -strands shown table results anns included state information networks similar qian sejnowski previous output forms part current input vector results show state information increase accuracy network prediction evaluate usefulness domain theory function number training examples estimate collecting proteins performed series tests divided training sets subsets contained proteins contained contained fourth training proteins process produced training sets training sets train fskbann ann networks figure results tests fskbann shows gain accuracy training set size statistically significant level confidence table results prediction methods testset accuracy correlation coefficients method total helix strand coil helix strand coil chou-fasman ann fskbann ann state formula defines correlation coefficient secondary structure problem mathews calculated structure separately number true positives true negatives false positives misses structure neural networks improve algorithms figure results figure demonstrate interesting trends fskbann networks matter large training set shape curve accuracy continue increase proteins training anomaly curve gain accuracy training proteins large expect number training instances small domain knowledge big advantage problem small training set obtain random sets proteins indicative population individual proteins generally reflect distribution secondary structures population proteins large numbers -helix regions -sheets large numbers -sheet regions -helices learn predict skewed population network produce poor solution mitigated proteins introduced causing training population closely match population finally analyze detailed performance approaches gathered number additional statistics fskbann ann chou-fasman solutions statistics analyze results 
terms regions region consecutive sequence amino acids secondary structure regions measure accuracy obtained comparing prediction amino acid adequately capture notion secondary structure biologists view cohen presnell cohen langridge biologists knowing number regions approximate order regions important knowing structure amino acid lies predictions figure adapted cohen prediction completely misses -helix region errors prediction slightly skewed -helix region ends errors appears answer statistics gathered assess solution predicting -helix regions table -strand regions table figure table table give picture strengths weakness approach table shows fskbann solution overlaps slightly fewer actual -helix regions anns overlaps tend longer hand fskbann networks overpredict fewer regions anns predict fewer -helix regions intersect actual -helix regions table fskbann anns accurately predict neural networks improve algorithms table region-oriented statistics -helix prediction occurrence description chou fasman actual helixpredicted predicted helix region overlap actual helix region percentage time helixactual average length actual helix region number regions helixpredicted average length predicted helix region number regions predicted helix actual helix actual helix regionis overlapped predicted helix region length overlap percentage time kbann ann table region-oriented statistics -strand prediction occurrence description chou fasman predicted strand regiondoes overlap actual strand region percentage timeactual predicted strand actual strand region overlapped predicted strand region length overlap percentage time predicted actual average length predicted strand region number regions predicted strand strand average length actual strand region number regions actual strand ann kbann strand occurrence regions chou-fasman table demonstrates fskbann predictions overlap higher percentage actual -strand regions chou-fasman algorithm anns accuracy -strand predictions approximately fskbann chou-fasman method length overlap chou-fasman method longer fskbann cost predicting longer regions ann networks extremely poorly overlapping actual -strand regions fskbann networks anns overpredicting -strands chou-fasman method results fskbann solution significantly ann solution predicting -strand regions sacrifice accuracy predicting -helix regions neural networks improve algorithms results suggest work developing methods evaluating solution quality simple position-by-position count correct predictions capture adequately desired behavior solutions find approximate locations -helix -strand regions accurately predict classes favored favored solutions predicting largest class importantly results show difficult problems protein-folding problem fskbann approach neural learning refine existing algorithm worthwhile future work fskbann domain theory give network good set initial weights search starts location weight space augmenting chou-fasman domain theory information increase solution accuracy training start location information tables weaknesses present system knowledge plan develop domain-theory extensions addressing weaknesses studying biological literature method augmenting knowledge complex encoding scheme present amino acid represented single input unit hunter suggests complex encoding scheme encodes number properties amino acids preliminary tests encoding scheme standard neural networks showed promising results recent domain theories garnier robson prevelige fasman include knowledge fourth type secondary structure -turns data set classified random coils knowledge added present networks partial domain theory coils networks predict classes interesting property fskbann networks magnitude output correlated accurate prediction plan information complex method predicting protein structure scan predict strongest activated areas feed predictions back network scan pass protein system mark amino acids largest predictions predicting step existing method pass system structure previous amino acid structure amino acid amino acid positions ahead basic problem kbann approach extracting information human-readable form trained networks giles press towell shavlik plan address rule extraction augmented networks fskbann extending existing method extracting rules kbann networks towell shavlik extension extract refined fsas rules jacobs jordan nowlan hinton proposed method learning combine knowledge number neural networks produce solution number approaches protein structure prediction investigated working method combine strategies single prediction method combined solution aggregate predictions number neural networks output machine learning approaches biological approaches important area focus applying fskbann domains plan apply fskbann problems molecular biology splice-junction problem noordewier neural networks improve algorithms states network intron exon interest evaluating approach problems fields natural language elman task involve learning recognize simple sentences involving simple regular grammar information grammar missing incorrect related research work shares similarities research areas algorithms predicting protein structure neural networks state information systems combine strategies solving problems methods predicting protein secondary structure number algorithms proposed predicting protein secondary structure loosely divided biological knowledge nonlearning methods learning mechanism non-learning methods widely approaches biological literature fasman predicting protein secondary structure chou-fasman chou fasman prevelige fasman robson garnier robson robson suzuki lim algorithms robson suzuki gorii goriii garnier robson solutions based information theory approaches neural networks base prediction window information central amino acid position central amino acid window position robson algorithm determines relevance amino acid predicting type secondary structure computerized versions chou-fasman robson techniques implemented tested qian sejnowski test data exhibit accuracy table lim method account long-range interactions stereochemical theory secondary structure globular proteins solutions garnier robson prevelige fasman include theories fourth type secondary structure -turns classified coils main advantage fskbann algorithms fskbann biological information mechanism learning learning methods number investigators learning algorithms sample folded proteins predict secondary structure proteins holley karplus qian sejnowski simple hidden-layer neural networks predict secondary structure studies focus varying hidden unit size window size achieving results shown table parameters report test set accuracies qian sejnowski cascaded architecture produces percentage point improvement accuracy single network results stolorz lapedes xia perceptron architecture evaluate error function mutual information produces percentage point gain accuracy standard perceptron interesting thing stolorz measure improves helix strand prediction expense coil prediction desirable effect coil making training data overpredicted neural-network techniques neural networks improve algorithms zhang machine learning method combines information statistical technique memory-based reasoning algorithm neural network divide training set halves components trained half training set training half training set learn combine results components neural network results report training set proteins zhang personal communication learning technique applied problem nearest-neighbor algorithm pebls cost salzberg press report accuracy approximately training set similar size work muggleton king applying inductive logic programming produced test set results accuracy proteins helix coil regions kneller cohen langridge produced similar results accuracy proteins consisting -helices coils domain theory trained solve similar problem predicting -helix showed approximately accuracy major difference learning approaches fskbann fskbann incorporates complete algorithm fskbann differs approaches neural networks studies incorporate state information methods representing state information neural networks researchers proposed neural-network architectures incorporating information state idea retaining state context training patterns occurs primarily work addressing natural language problems cleeremans servan-schreiber mcclelland elman approaches 
provide mechanism preserving past activations units processing input jordan elman introduced recurrent network topology fskbann networks set hidden units called context units preserve state network time step previous context units copied back input system networks possibility keeping multiple past contexts input system idea type network introduced jordan represent finite-state automaton discussed cleeremans show type network perfectly learn recognize grammar derived finite-state automaton giles press complex recurrent network learn fsa extract learned fsa major difference research cleeremans giles focus initial domain theory expressed finite-state automaton attempting learn scratch methods multistrategy learning number researchers recently addressed problem blending strategies produce effective learners ourston mooney system domain theory focus corrections inductive learning system performs tecuci issue describes system includes number mechanisms induction deduction analogy abduction correct domain theory pazzani issue theorydriven learning system search rules explain incorrect examples constrained regularities observed rules domain theory saitta botta issue developed system combines abductive learning based domain model inductive learning neural networks improve algorithms based sample instances major difference work fskbann systems symbolic reasoning learn systems works directly rules form fskbann integration symbolic representation neural learning advantage generality power type learning conclusion paper presents evaluates fskbann system mechanism combining knowledge domain theories represented generalized finite-state automata neural networks networks trained backpropagation refine initial domain knowledge extension kbann domain theories include knowledge state significantly enhances power kbann approach rules expressed domain theory account current problem-solving context state solution tested fskbann refining non-learning chou-fasman algorithm predicting protein secondary structure task challenge problem machine learning computational biology communities fskbann multistrategy approach combining domain knowledge neural network proved accurate standard neural network approach non-learning chou-fasman algorithm fskbann solution proved effective considered terms class secondary structure success fskbann secondary-structure problem suggests tool addressing tasks include state information work improving neural-network refinement process extraction symbolic knowledge trained networks acknowledgements research partially supported national science foundation grant iriand office naval research grant -jthe authors terrence sejnowski providing protein data testing domain theory data testing anonymous ftp california-irvine archive maintained david aha patrick murphy chou fasman prediction secondary structure proteins amino acid sequence advanced enzymology cleeremans servan-schreiber mcclelland finite state automata simple recurrent networks neural computation cohen presnell cohen langridge proposal feature-based scoring protein secondary structure predictions proceedings aaaiworkshop artificial intelligence approaches classification pattern recognition molecular biology anaheim cost salzberg press weighted nearest neighbor algorithm learning symbolic features machine learning neural networks improve algorithms elman finding structure time cognitive science fahlman lebiere cascade-correlation learning architecture touretzky advances neural information processing systems vol denver morgan kaufmann fasman development prediction protein structure fasman prediction protein structure principles protein conformation york plenum press garnier robson gor method predicting secondary structures proteins fasman prediction protein structure principles protein conformation york plenum press giles miller chen chen sun lee press learning extracting finite state automata second-order recurrent neural networks neural computation holley karplus protein structure prediction neural network proceedings national academy sciences usa hopcroft ullman introduction automata theory languages computation reading addison wesley hunter representing amino acids bitstrings proceedings aaaiworkshop artificial intelligence approaches classification pattern recognition molecular biology anaheim jacobs jordan nowlan hinton adaptive mixtures local experts neural computation jordan serial order parallel distributed processing approach technical report san diego california institute cognitive science kneller cohen langridge improvements protein secondary structure prediction enhanced neural network journal molecular biology lim algorithms prediction -helical -structural regions globular proteins journal molecular biology mathews comparison predicted observed secondary structure phage lysozyme biochimica biophysica acta muggleton feng efficient induction logic programs proceedings conference algorithmic theory tokyo muggleton king predicting protein secondary-structure inductive logic programming technical report glasgow scotland turing institute nishikawa assessment secondary-structure prediction proteins comparison computerized chou-fasman method biochimica biophysica acta noordewier towell shavlik training knowledge-based neural networks recognize genes neural networks improve algorithms dna sequences lippmann moody touretzky eds advances neural information processing systems vol denver morgan kaufmann ourston mooney changing rules comprehensive approach theory refinement proceedings eighth national conference artificial intelligence boston mit press pazzani issue learning causal patterns making transition data-driven theory-driven learning machine learning prevelige fasman chou-fasman prediction secondary structure proteins choufasman-prevelige algorithm fasman prediction protein structure principles protein conformation york plenum press qian sejnowski predicting secondary structure globular proteins neural network models journal molecular biology quinlan learning logical definitions relations machine learning richardson richardson principles patterns protein conformation fasman prediction protein structure principles protein conformation york plenum press robson suzuki conformational properties amino acid residues globular proteins journal molecular biology rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume foundations cambridge mit press saitta botta issue multistrategy learning theory revision machine learning sejnowski rosenberg parallel networks learn pronounce english text complex systems stolorz lapedes xia predicting protein secondary structure neural net statistical methods technical report la-ur- los alamos theoretical division los alamos national laboratory tecuci issue plausible justification trees framework deep dynamic integration learning strategies machine learning towell shavlik noordewier refinement approximate domain theories knowledge-based neural networks proceedings eighth national conference artificial intelligence boston mit press towell symbolic knowledge neural networks insertion refinement extraction doctoral dissertation department computer science wisconsin madison towell shavlik interpretation artificial neural networks mapping knowledge-based neural neural networks improve algorithms networks rules lippmann moody touretzky eds advances neural information processing systems vol denver morgan kaufmann watson human genome project past present future science wilson haft getzoff tainer lerner brenner identical short peptide sequences unrelated proteins conformations testing ground theories immune recognition proceeding national academy sciences usa zhang exploration protein structures representation prediction doctoral dissertation department computer science brandeis waltham appendix chou-fasman domain theory chou-fasman algorithm chou fasman involves activities recognizing nucleation sites extending sites resolving overlapping predictions appendix details steps describes representation algorithm collection rules recognize nucleation sites chou fasman assign conformation values amino acids conformation values represent amino acid part helix strand structure higher values group amino acids classes similar conformation classes helix formers highindifferent indifferent breakers strand formers indifferent breakers table defines values types breakers formers table rules represent chou-fasman algorithm true amino acid positions secondary structure algorithm predicting rules predict -helix nucleation site consecutive set amino acids helix formers fewer helix breakers 
helix high-indifferent amino acids count helix rule determine location nucleation site simply adds helix-former helix-breaker values window amino acids wide totals greater rule predicts helix nucleation site proposition init-helix rules nucleation -strands similar -helix nucleation window amino acids wide strand nucleation site predicted strand formers fewer strand breakers step algorithm resolving overlaps reason numbers table making formers breakers boolean properties chou fasman suggest conformation values regions compared resolve overlaps fskbann networks weighting links amino acids numbers table combination alanines produce higher activation init-helix unit combination phenylalanines chou-fasman algorithm continues predict -helix long predicate conthelix true rules define cont-helix terms helix-breaking rules helix continues long break region encountered -helix break region occurs helix-breaker amino acid immediately helix-breaker helix-indifferent amino acid helix broken encountering amino acid proline process extending -strand structures works similarly algorithm predicts coil default neural networks improve algorithms table breaker values amino acids helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-breaker helix-breaker helix-breaker helix-breaker helix-breaker strand-former strand-former strand-former strand-former strand-former strand-former strand-former strand-former strand-former strand-former strand-former strand-breaker strand-breaker strand-breaker strand-breaker strand-breaker strand-breaker strand-breaker produced values tables reported chou fasman chou fasman normalized values formers dividing conformation conformation weakest helix alanine helix conformation alanine conformation weakest helix phenylalanine breaker values work similarly calculate breaker multiplicative inverse conformation directly values chou fasman reasons wanted smaller values decrease number times strong helix-formers add similarly strands breaker conformation values tend numbers stronger breakers close wanted breaker larger stronger breaker inverse breaker conformation restricting result exceed neural networks improve algorithms table chou-fasman algorithm expressed inference rules rules recognizing nucleation sites init-helix position helix amino acid position position helix breaker amino acid position init-strand position strand amino acid position position strand breaker amino acid position rules pairs amino acids terminate helix structures helix-break helix-break helix-indiff break-helix helix-break helix-break break-helix helix-break helix-indiff rules pairs amino acids terminate strand structures strand-break strand-break strand-indiff break-strand strand-break strand-break break-strand strand-break strand-indiff rules continuing structures cont-helix break-helix cont-strand break-strand rules predicting -helix nucleation propagating state helixi init-helix helixi helixicont-helix rules predicting -strand nucleation propagating state strandi init-strand strandi strandicont-strand rules predicting coil default coili helixibreak-helix coili strandibreak-strand coili coiliusing neural networks improve algorithms figure sample kbann set rules dependencies rules neural network neural networks improve algorithms current inputcurrent state current outputnext state hidden unit topology determined domain theory system boundary figure schematic view fskbann network neural networks improve algorithms figure ribbon drawing three-dimensional structure protein richardson richardson areas resembling springs -helix structures flat arrows represent -strands remaining regions random coils neural networks improve algorithms input units input windowprimary structure output units hidden units helix strand coil predicted secondary structure figure neural network architecture qian sejnowski neural networks improve algorithms primary structure predict nucleation sites step step extend regions step resolve overlaps figure steps chou-fasman algorithm neural networks improve algorithms init helix init strand init strand init helix break helix break strand continue helix continue strand helix strand coil figure finite-state automaton interpretation chou-fasman algorithm neural networks improve algorithms copy output input scanning direction predicted secondary structure init helixcont helix helix helix input window hidden units figure general neural-network architecture represent chou-fasman algorithm neural networks improve algorithms number training proteins test set correctness chou-fasman standard ann finite-state kbann figure percent correctness test proteins function training-set size neural networks improve algorithms prediction prediction secondary structure primary structure figure predictions secondary structure 
empty rules added correct theory test set error kbann domain theory antecedents added correct theory test set error kbann domain theory rules deleted correct theory test set error domain theory kbann antecedents deleted correct theory test set error kbann domain theory decrease false negatives decrease false positives node node node node node node newnode existing node node knowledge based portion outputs inputs extra hidden units standard neural network rules deleted correct theory test set error kbann topgen strawman standard neural network antecedents deleted correct theory test set error kbann topgen strawman rules deleted correct theory test set error test set error antecedents deleted correct theory topgen strawman kbann test set error splice junctions promoters terminators key kbann topgen strawman protein mrna dna noncoding region ribosome cys ser ser gly leu rna polymerase promoter transcription translation dnaintron intronexon exon exon transcription splicing mrna mrna position position position position position position classifier dna sequence feature representation promoter position sliding window 
promoter contact conformation minus minus promoter contact conformation minus minus conformation aaxxa cttgac minus tataat minus minus minus axxxxt txxxxaxxtxtg conformation axxxxa txxxtxaaxxtx conformation caaxttxac gxxxtxc gcgccxcc conformation xttgxca xttgaca xttgac xtaxaxt xtataat xxtaxxxt input units contact minus minus minus minus position position position donor donor donor negative negative negative negative negative negative negative dna sequence position frame frame frame refining rules incorporated knowledge-based support vector learners successive linear programming richard maclin edward wild jude shavlik lisa torrey trevor walker computer science department computer sciences department minnesota duluth wisconsin madison kirby drive west dayton street duluth madison rmaclin umn fwildt shavlik ltorrey twalkerg wisc abstract knowledge-based classification regression methods powerful forms learning system advantage prior domain knowledge supplied human user algorithm combining knowledge data produce accurate models limitation prior knowledge occurs provided knowledge incorrect knowledge information knowledge-based learners fully exploit information fact incorrect knowledge lead poorer models result knowledge-free learners present support-vector method incorporating refining domain knowledge learner make knowledge suggests provided knowledge approach built knowledge-based classification regression methods presented fung mangasarian shavlik mangasarian shavlik wild experiments artificial data sets properties real-world data set demonstrate method learns accurate models adjusting provided rules intuitive ways algorithm appealing extension knowledge-based support-vector learning combine knowledge rules data data modify change rules fit data introduction support-vector methods incorporate prior knowledge form rules increasingly popular fung mangasarian shavlik mangasarian shavlik wild maclin smola gaertner general assumption methods advice provided accurate leaves open question advice significantly accurate effectively knowledge-based support vector methods provide mechanism based slack variables data overcome poor advice general means learner adjust rule data overcome poor rule develop model comparable learned copyright american association artificial intelligence aaai rights reserved prior knowledge work develop method overcome poor rules alter rule produce effective model rule structure result models repair poor rule learner outperform learner discards rule addition method mechanism examining made rule made user method analyzing wrong original rule method makes knowledge-based support vector machines kbsvms presented fung mangasarian mangasarian shavlik wild method rule-refining support vector machines rrsvm learner corrects antecedents provided rules part numeric optimization problem formulation solved linear programming feasible region nonlinear due factors correct rule mathematical program bilinear factors solution successive linear programming correction factors resulting process increase accuracy model supplied human user refinements rules knowledge-based support vector methods kbsvms generally capture rules form antecedents consequent promoter domain examine gene sequence promoter dna characters positions start gene shortened rule countof promoter true rules kbsvms generally represented form representation matrices capture antecedents rule representation feature similarly feature feature capture rule row zeros features rule requires count equation negated antecedents appears proceedings twenty-second conference artificial intelligence aaai vancouver british columbia features case set row features capture threshold rule met features right-hand side rule captures consequent terms represents input features weights threshold model learned classification problem conditions matrix data point promoter positive simply setting kbsvms provide methods incorporating rules svm formulations formulation optimization problem linear svm domain knowledge classification min kwk jbj denotes vector dimension prime symbol denotes transpose vector matrix vector denotes sum components absolute scalar denotedjbj -norm vector denoted kxk defined sum absolute values components components kxk jxij matrix set data row data point column feature matrix diagonal elements class labels points off-diagonal elements parameters represent feature weights threshold slack variables terms solution deal noisy data penalties slacks threshold note focusing classification linear kernel model results apply regression kernel methods kbsvm domain knowledge equation added extra constraints optimization problem causing system pick model takes account rules optimization problem equation extended min kwk jbj part process adding rules constraints rules softened adding slack variables representation system partially completely discard rule contradicted data knowledge refinement slacks introduced knowledge kbsvms refinement implication rule figure surfaces learned knowledge knowledge shown green gray box refined knowledge shown smaller green gray box outer box showing original rule refined sense allowing system alter terms desirable refine term lefthand side antecedent implication regionfxjbx dgover inequality righthand side implication enforced simple problem shown figure figure show simple linear surface produced based two-dimensional data figure show data region defining rule light green gray rule region user thinks data positive data contradict rule assume learner closely fit rule decision surface shown figure finally figure show happen approach learner shrinks advice region fit data fits learned surface region data note training errors surface learned training data hope adjusted rule resulting surface accurate test data note adjusted rule returned user analyze refine term rule approach refine learning vector learn add implication optimization formulation original implication added formulation produce formulation add penalty -norm objective trade reducing complexity solution fitting data making consequent rule represented making antecedent rule represented construct formulation method rrsvm extending formulation equation min kwk jbj note refine term leave future work apply complex rules individual antecedents involve feature observe problem nonlinear constraints term appears free variables constraints bilinear suggesting method solution shown algorithm fix solve optimization problem set fix solve process iterated algorithm rrsvm successive linear programming solution return failure end solve min kwk jbj solve min kwk jbj ifk return end end proposition sequence objective values converges computed accumulation point sequence iterates generated algorithm accumulation point satisfies local minimum property argmin kwk jbj proof sequence objective values bounded solution linear program generates feasible point succeeding linear program sequence bounded nonincreasing converges accumulation point sequence satisfies local minimum property noting point sequence satisfies replaced due linear program algorithm accumulation point sequencef satisfies local minimum property algorithm conjunction mangasarian shavlik wild proposition immediately prove result shows refined prior knowledge imposed result algorithm proposition result algorithm slack variables function satisfies note method necessitates additional parameter optimization problem learned parameter controlling termination bilinear process terms maximum steps tolerance measuring process terminated discuss aspect experiments note bilinear process returns learned terms making user examine resulting optimization altered rules experiments tested method rrsvm artificial data sets properties testing allowed determine rrsvm works suggested variety conditions performed experiments promotersdata set towell shavlik noordewier early domain theory outdated knowledge promoters significantly advanced subject significant experimentation theory refinement results demonstrate rrsvm works user rules correct structure exploited methodology critical aspect work addressed selection parameter values original knowledge representation svm formulation parameters penalize size slacks misfit data point threshold weights implicitly penalized kbsvm approach experimental controls fung mangasarian shavlik captured formulation approach addition parameters slack knowledge terms previously finally approach rrsvm includes terms term penalize terms order set parameters supply system range values select data set combination parameters set values addition values set 
ranges values based suggestions previously kbsvm papers represent reasonable range values note experiments restricted values onlyf discuss experiments table methodology artificial data set experiment trainingset random data points testset random data points trainsetsize trainset trainsetsize points trainingset bestscore inf set parameters score -fold cross-val err trainset score bestscore bestscore score bestparams end end bestparams train model trainset score model testset end order determine set parameters learner performs ten-fold cross validation training set set parameter values estimate effectiveness set learner picks parameters produced results ten folds training set parameters learn model entire training set learned model tested data parameter selection subsequent training note effect approach advice methods larger variety parameters find model experiments concentrate cases amount data small knowledge affect cases amount data small overfitting significant possibility larger variety parameters balanced potential problem artificial data test rrsvm controlled situation true function generated data rule class pos class neg experiments include noise results gaussian noise similar slower convergence experiments performed artificial data data sets generated input features meaningful distractor features values chosen randomly labeled rule rule performed repeats methodology shown table method performed experiments rules knowledge rule based correct domain theory class pos call good advice altered threshold class pos call bad advice structure tested learners system advice kbsvm method good bad advice rrsvm good bad advice addition tested kbsvm method supplied set parameters previous section eliminated possibility setting basically training set size svm svm relevant features kbsvm good advice kbsvm good advice rrsvm good advice kbsvm bad advice kbsvm bad advice rrsvm bad advice figure results artificial data set rule methodology text results averaged runs method receiving set data run learner ignore advice wanted results cases learner make advice test include baseline learner advice relevant features form advice results presented figure thing note results kbsvm rrsvm good advice perform fairly small amount data data points statistically significantly paired t-test outperform learner advice learning curve note learners good advice achieve lower error learner relevant features difference statistically significant observation results rrsvm ability refine advice quickly performs bad advice kbsvm systems system good advice observed experiments quickly occurs generally depends noisy data expect rrsvm make advice bad long structure thing note kbsvm bad advice ability ignore advice performs learner advice kbsvm forced advice allowed perform worse learner advice difference statistically significant training examples note rrsvm recover rules altered part learning process experiments altered expect learner recover similar figure shows resulting learned values quickly close deviation number examples grows experiments artificial data method refining advice conditions makes advice accurate structure experiments real-world data set theory refinement training set size figure values averaged runs refining bad advice discussed text standard deviation shown promoters promotersdata set set dna sequences originally presented towell shavlik noordewier extensively theory refinement data set consists set examples dna sequences gene promoters examples strand consists dna characters starting promoter expected start promoter starts chromosome representation simple representation feature dna characters basically feature represents boolean test character occurs position character position start domain theory promotershas major parts conformation portion theory repeatedly rejected researchers part theory rules regions dna sequence sets characters occur regions lead rules combinations regions make learning process fairly unwieldly make issue bad rules complex observations ortega combined rules regions single antecedent rules region called minus promoter tested conditions minus minus minus minus note significant shared structure rules similarly minus region defined minus minus minus minus combined rule takes form countof countof promoter true question set thresholds based examination rules set advice close capturing domain theory call original svm kbsvm original advice rrsvm original advice kbsvm poor advice kbsvm poor advice rrsvm poor advice average error figure average error rates ten-fold cross-validation experiments run promotersdata set rule discussed text advice advice make advice work significantly lowered thresholds made rules match call poor advice tested running repeats ten-fold cross validation promotersdata set methodology select parameters figure shows results experiments note linear model knowledge performed models data note advice models perform compared learners kbann towell shavlik ensembles neural networks terms performance advice note kbsvm ignore poor advice performs poorly expected kbsvm system ignore advice performs learner advice finally note learners original advice perform rrsvm refined poor advice performs difference rrsvm learners original advice statistically significant results rrsvm real-world data set make piece advice advice accurate terms advice adjusted poor advice cases thresholds adjusted upwards remaining cases advice threshold adjusted applied small number cases threshold left generally shifted shifted suggests good rules made strongly focusing region allowing looser fit region promoter results suggest rrsvm refine advice real-world problem give insight data method ability compile advice complex learning method advice changed makes knowledge refinement method main drawbacks approach iterative procedure rrsvm takes time standard kbsvm approaches parameters set experiments found time rrsvm grow linearly number iterations respect kbsvm algorithm advantage fact good solution found previous iteration previous solution hot start optimization significantly reduce time issue setting parameters approaches presented bennett zhu automatically determine effective parameter values related work work relates closely knowledge-based support vector methods fung mangasarian shavlik mangasarian shavlik wild maclin smola gaertner approach differs methods refine antecedents knowledge examine resulting knowledge kbann towell shavlik system sense compiles symbolic knowledge bias numeric learner significant work attempting extract learned knowledge networks fung sandilya rao thrun towell shavlik work differs methods inserting rules svms neural networks method fung sandilya rao apply linear support vector methods process determining rules changed simpler require complex processes extract refined rule significant work directly manipulating rules symbolic form compiling learner pazzani kibler developed mechanisms refining rules represented order logic inductive logic programming methods ourston mooney created method analyzing order logical proofs generated sets rules data correct complete proofs learn knowledge work differs related methods focusing extending support vector methods proven effective wide range problems handle additional sources training information general rules addition labeled examples allowing types information noisy conclusions future work presented method incorporating domain knowledge provided user support vector learning method shown domain knowledge refined learning process method rule-refining support vector machines rrsvm learner make domain knowledge accurate long structure rrsvm reports refinements rules examined analyzed human user rrsvm extends 
previous work knowledge-based support vector methods experiments rrsvm demonstrate effectively refine knowledge provided human user demonstrate artificial data sets real-world promotersdata set future work plan apply rrsvm larger problems test real-world regression problems addition classification plan robocup advice maclin plan examine methods rrsvm expand advice examine methods add variables rules previously mentioned plan ways extend methods extracting rules svms similar fung sandilya rao additional adjustment rules finally methods refining nonlinear knowledge rules develop methods extracting rules bennett kunapuli pang model selection bilevel optimization ijcnn rule learning searching adapted nets aaai fung mangasarian shavlik knowledge-based support vector machine classifiers nips fung mangasarian shavlik knowledge-based nonlinear kernel classifiers colt fung sandilya rao rule extraction linear support vector machines kdd smola gaertner simpler knowledge-based support vector machines icml maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression aaai maclin shavlik walker torrey simple effective method incorporating advice kernel methods aaai mangasarian shavlik wild knowledge-based kernel approximation jmlr ortega informativeness dna promoter sequences domain theory jair ourston mooney theory refinement combining analytical empirical methods artificial intelligence pazzani kibler utility knowledge inductive learning machine learning thrun extracting rules artificial neural networks distributed representations nips towell shavlik extracting refined rules knowledge-based neural networks machine learning towell shavlik knowledge-based artificial neural networks artificial intelligence towell shavlik noordewier refinement approximate domain theories knowledge-based ural networks aaai zhu rosset hastie tibshirani -norm support vector machines nips 
appears artificial intelligence volume submitted final pre-publication revisions knowledge-based artificial neural networks geoffrey towella jude shavlik towell learning scr siemens shavlik wisc wisconsin west dayton madison keywords machine learning connectionism explanation-based learning hybrid algorithms theory refinement computational biology running head knowledge-based artificial neural networks current address siemens corporate research college road east princeton direct correspondence address abstract hybrid learning methods theoretical knowledge domain set classified examples develop method accurately classifying examples training challenge hybrid learning systems information provided source information offset information missing source hybrid learning system learn effectively systems information sources kbann knowledge-based artificial neural networks hybrid learning system built top connectionist learning techniques maps problem-specific domain theories represented propositional logic neural networks refines reformulated knowledge backpropagation kbann evaluated extensive empirical tests problems molecular biology results tests show networks created kbann generalize wide variety learning systems techniques proposed biologists introduction suppose teach class objects recognize members class approach define category student state domain theory describes recognize critical facets class members facets interact domain theory student distinguish members nonmembers class approach teaching recognize class objects show person lots examples shown student member class sufficient examples student classify examples comparison methods teaching roughly characterize approaches achieving problem-specific expertise computer hand-built classifiers expert systems empirical learning hand-built classifiers correspond teaching giving person domain theory extensive set examples call learning told conversely empirical learning corresponds giving person lots examples explanation examples members class reasons listed section approaches achieving machine expertise completely satisfactory suffer flaws preclude generally applicable method flaws method part complementary sections hybrid system effectively combines hand-built classifier empirical learning algorithm student taught combination theoretical information examples student combine sources information fill gaps knowledge exist similarly hybrid learning systems reviewed sections find synergies make effective hand-built classifiers empirical learning algorithms isolation kbann knowledge-based artificial neural networks successor ebl-ann algorithm system approach kbann outlined table briefly idea insert set hand-constructed symbolic rules hand-built classifier neural network network refined standard neural learning algorithms set classified training examples refined network function highly-accurate classifier final step kbann extraction refined comprehensible rules trained neural network subject effort scope paper section describes kbann algorithm empirical tests section dna machine learning domain theory collection rules describes task-specific inferences drawn facts classification problems domain theory prove object member class table kbann approach learning list features describe examples approximately-correct domain theory describing problem solved set classified training examples translate domain theory neural network train kmowledge-based network classified examples trained network classify future examples optionally extract refined domain theory sequence-analysis tasks section show kbann benefits combination hand-built classifier empirical learning tests show datasets examine kbann generalizes methods learn purely examples methods learn theory examples convention assess generalization testing systems examples training testing reveals kbann profitably domain theories significant amounts misinformation tests show broad range conditions kbann yields hoped-for synergies hybrid approach learning hybrid systems describing kbann motivate development hybrid systems listing important weaknesses hand-built classifiers empirical learning systems lists overview reasons hybrid systems active area machine learning research hand-built classifiers hand-built classifiers non-learning systems altered hand simply told learn knowledge level apparent simplicity systems pose problems build typically hand-built classifiers assume domain theory complete correct real-world tasks completeness correctness extremely difficult impossible achieve fact explanation-based learning major issues dealing incomplete incorrect domain theories domain theories intractable make domain theory complete correct write thousands interacting possibly recursive rules rule sets intolerably slow domain theories difficult modify interactions proliferate rule set difficult predict resulting modifying single rule empirical learning empirical learning systems inductively generalize specific examples require theoretical knowledge problem domain require large library examples complete ignorance problem-specific theory means address important aspects induction significant problems unbounded number features describe object user choice features make computer cookie similar features relevant classification context dependent observation paper money flammable relevant bank fire complex features constructed initial features considerably simplify learning feature construction difficult error-prone enterprise large set examples small sets exceptions unrepresented poorly represented result uncommon cases difficult correctly handle artificial neural networks artificial neural networks anns form basis kbann method empirical learning anns proven equal superior empirical learning systems wide range domains evaluated terms generalization ability set problems unique style empirical learning problems training times lengthy initial parameters network greatly affect concepts learned problem-independent choose good network topology considerable research direction training neural networks difficult interpret hybrid learning systems significant gap knowledge-intensive learning-by-being-told approach hand-built classifiers virtually knowledge-free approach empirical learning gap filled hybrid learning methods hand-constructed rules classified examples learning trends made development systems active area machine learning important trends realization knowledgeintensive knowledge-free learning ends spectrum neural learning symbolic knowledge initial initial neural networknetwork rules training examples trained neural network figure flow chart theory-refinement kbann intelligent system operate realization lead recent specialized workshops staying end spectrum learning systems simplifies learning problem allowing strong assumptions made nature learned middle ground appealing offers possibility synergistic combinations theory data result powerful learning systems trend spurring development hybrid systems growth body psychological evidence people rarely learn purely theory examples instance murphy medin suggest feature correlations partly supplied people theories causal mechanisms contained theories means correlational structure represented page theory examples interact closely human learning clear people learn theory examples interaction occurs determined subject research affects work machine learning finally purely practical consideration hybrid systems proven effective real-world problems section kbann section describes kbann methodology figure depicts pair algorithms arcs form system learning theory examples algorithm labeled rules-to-network detailed section algorithm inserts approximately-correct symbolic rules neural network networks created step make classifications rules based algorithm kbann labeled neural learning refines networks backpropagation learning algorithm experiments backpropagation method supervised weight revision conjugate gradient work learning mechanism essentially standard backpropagation network trained standard algorithm kbann constructs initializes network implications training initial symbolic knowledge examples training symbolic learning algorithm final symbolic knowledge figure flow chart all-symbolic theory-refinement discussed section completion step trained network accurate classifier beginning detailed description kbann difference figures figures present alternative architectures systems learn theory examples figure shows architecture kbann contrast figure represents architecture labyrinth-k all-symbolic hybrid learning systems kbann compared section kbann requires algorithms all-symbolic systems require single algorithm underlying empirical learning mechanism operates directly rules re-representation neural 
network tests reported chapter show extra effort entailed kbann rewarded kbann generalizes all-symbolic systems testbeds subsection presents overview type neural networks subsequent high-level overview kbann subsections in-depth descriptions kbann algorithmic steps neural networks neural networks paper feedforward neural networks trained backpropagation algorithm units logistic activation function defined equations roughly speaking net incoming activation unit exceeds bias unit activation unit activation netinputi connected unitsa weightji activationj activationi netinputi biasi overview kbann shown figure kbann consists largely independent algorithms rules-to-network translator refiner neural learning algorithm briefly rules-to-networks translation accomplished establishing mapping rule set neural network mapping table defines topology networks created kbann initial link weights network section defining networks problems inherent neural networks empirical learning ameliorated translation specifies features relevant making correct decision specification features addresses problems spurious correlations irrelevant features untable correspondences knowledge-bases neural networks knowledge base neural network final conclusions output units supporting facts input units intermediate conclusions hidden units dependencies weighted connections boundedness set features rule translation important derived features simplifying learning problem derived features capture contextual dependencies description addition rules refer arbitrarily small regions feature space rules reduce empirical portion hybrid system learn uncommon cases procedure indirectly addresses problems hand-built classifiers instance problem intractable domain theories reduced approximately-correct theories major step kbann refine network standard neural learning algorithms set classified training examples completion step trained network classifier accurate derived machine learning methods section empirical evidence supports claim inserting knowledge neural network step kbann translate 
set approximately-correct rules knowledge-based neural network henceforth kbann-net rules translated kbann-nets expressed horn clauses appendix complete description language accepted kbann constraints rule set rules propositional constraint results neural learning algorithms present unable handle predicate calculus variables rules acyclic cycles constraint simplifies training resulting networks represent fundamental limitation kbann exist algorithms based backpropagation train networks cycles extended kbann handle recursive finite-state grammars addition constraints rule sets provided kbann hierarchically structured rules commonly map directly inputs outputs rules provide intermediate conclusions describe conjunctions input features intermediate conclusions rules determine final conclusion intermediate conclusions hierarchical structure set rules creates derived features example-based learning system domain knowledge hierarchically structured networks created kbann derived features contextual dependencies conjunctions descriptions kbann-net results translating rule set intermediate conclusions hidden units result capable perceptron-like learning rules-to-network translator subsections subsections detailed description translation translation process pair intuitive arguments kbann translator correct full proofs table rules-to-networks algorithm kbann rewrite rules disjuncts expressed set rules antecedent directly map rule structure neural network label units kbann-net level add hidden units network user-specified levels optional add units input features referenced rules add links translation units topologically-contiguous levels perturb network adding near-zero random numbers link weights biases initial rules final rules figure rewriting rules eliminate disjuncts term rules translated network accurately reproduces behavior rules-to-network algorithm table abstract specification seven-step rules-to-network translation algorithm algorithm initially translates set rules neural network augments network learn concepts provided initial rules subsection describe detail steps algorithm step rewriting step algorithm transforms set rules format clarifies hierarchical structure makes directly translate rules neural network rule consequent rule consequent antecedent rewritten rules form disjunction allowed kbann multiple rules consequent rules original consequent single newly-created term antecedent rule newly-created term consequent antecedents original rule antecedents instance figure shows transformation rules format required steps kbann rewriting explained section step mapping step rules-to-network algorithm kbann establishes mapping transformed set rules neural network mapping shown table kbann creates networks one-toone correspondence elements rule set weights links rule set biases units consequents set network responds manner rules based section explanation precise settings completion step kbann-net information set rules relevant input derived features guarantee set rules refers relevant features significant collection derived features steps augment kbann-net additional links inputs units possibly hidden units step numbering step kbann numbers units kbann-nets level number precursor steps kbann defines level unit length longest path input unit step adding hidden units step adds hidden units kbann-nets giving kbann-nets ability learn derived features initial rule set suggested expert step optional initial rules provide vocabulary sufficient obviate adding hidden units hidden units added specific instructions user instruction number distribution levels established previous step added units addition hidden units kbann-nets subject partially explored methods unit addition evaluated step adding input units step kbann augments kbann-nets input features referred rule set domain expert believes relevant addition set rules perfectly correct identify input feature required correctly learning concept step adding links step algorithm adds links weight network numbering units established step links added connect unit numbered unit numbered adding links conjunction numbering technique slightly methods adding links explored step perturbing final step network-to-rules translation perturb weights network adding small random number weight perturbation small effect kbann-net computations prior training sufficient avoid problems caused symmetry sample rules-to-network translation figure shows step-by-step translation simple set rules kbann-net panel shows set rules prolog-like notation panel set rules rewritten step translation algorithm rules affected rewriting form disjunctive definition consequent numbering technique implicitly assumes chain reasoning complete intermediate conclusion part directed path inputs outputs requirement chain complete incomplete chains attach unconnected antecedents directly input unit unconnected consequents directly output unit low-weight links prior step step step step step steps key unnegated dependency negated dependency conjunction key positively weighted link negatively weighted link unit figure sample rules-to-network translation panel graphical representation rules panel shows hierarchical structure rules figure dotted lines represent negated antecedents solid lines represent unnegated antecedents arcs connecting antecedents conjuncts standard tree step translation algorithm step table create neural network mapping hierarchical structure rules network result visual difference representations initial kbann-net panel hierarchical structure rules panel panels illustrate process links input units hidden units set rules added kbann-net steps algorithm panel shows units kbann-net numbered level addition panel shows hidden unit shaded added network level purposes assume user instructed network-torules translator add single hidden unit level panel shows network links weight added connect units separated level note addition providing existing units access information domain knowledge low-weighted links connect added hidden units rest network illustration final step rules-to-network translation algorithm perturbation link weights results minute qualitatively affect calculations initial network figure translation conjunctive rule kbann-net translation rules kbann-nets section describes kbann translates rules logical connectives 
kbann-net recall individual rules assumed conjunctive nonrecursive variable-free disjuncts encoded loss generality multiple rules discussion section assumes features binary-valued restriction binary-valued features made clarity explanation appendix specification language kbann accept rules-to-network translator sets weights links biases units units significant activation activation deduction made domain knowledge likewise deduction made knowledge base unit inactive activation kbann translates conjunctive rules neural network setting weights links positive unnegated antecedents weights links negated antecedents bias unit rule consequent number positive antecedents rule kbann commonly setting empirically found work figure shows network encodes intuitively translation method reasonable weighted input links minus bias exceed negated antecedents true positive antecedents true case equation logistic activation function result instance figure activations approximately equal activation approximately equal net incoming activation greater generally units encoding conjunctive rules significantly active positively-weighted links carry signal negatively-weighted links carry signal kbann handles disjunctive rules exceptions handles conjunctive rules exception kbann rewrites disjuncts multiple rules consequent step rules-to-network algorithm rewriting prevent combinations antecedents activating unit situations consequent deduced assume exists consequent proven rules assume unnegated antecedents labeled antecedents antecedents translation rules neural structures mcculloch pitts kleene units threshold functions original contribution work idea training networks constructed simply construction networks figure translation disjunctive rules kbann-net antecedents connected activate set bias unwanted combinations activate kbann avoids problem rewriting independent rules difference conjunctive case translation disjunctive rules bias unit encoding consequent set conjunctive case kbann sets link weights equal figure shows network results translation disjunctive rules kbann-net intuitively reasonable strategy incoming activation overcomes bias antecedents true refining kbann-nets kbann refines networks backpropagation standard neural learning method methods refinement weights classified examples kbann-nets create problems backpropagation start confident answers output units activation correctness answers problems standard formulation backpropagation answers confident change made network correctness answer rumelhart argue desirable property standard neural networks means tend noise resistant page outputs networks completely incorrect property makes difficult correct aspects networks errors general context neural networks solutions require minor adjustments backpropagation proposed problem significant backpropagation address problem results kbann-nets article cross-entropy error function suggested hinton standard error function cross-entropy function interprets training signal network outputs conditional probabilities attempts minimize difference probabilities equations activation output unit desired activation unit number output units error error log log experience cross-entropy error function yielded improvements training time generalization kbann-nets addition changing definition error experimented augmenting error function term penalizes network making original domain theory adding equation regularization term equation regularizer viniti viniti term equation controls tradeoff ability network learn training set distance initial rules principal effect addition regularization term increase interpretability trained kbann-nets encouraging networks leave original weights unchanged interpreting trained networks scope paper experiments reported equation proven beneficial work experimental testbeds section describes real-world datasets domain molecular biology dna sequence analysis testbeds purposes article sufficient view dna linear sequence characters drawn biologists refer nucleotides molecular biology dna sequence analysis area well-suited computational analysis human genome project large growing database sequenced dna extensively studied large body training examples work addition human genome project promises sequence dna faster studied biological laboratories result computer-based analysis augment efforts biologists previously datasets demonstrate usefulness kbann algorithm description notation paper descriptions promoter splice-junction datasets detailed descriptions notation biological datasets special notation locations dna sequence idea number locations respect fixed biologically-meaningful point negative numbers sites preceding point positive numbers sites point figure illustrates numbering scheme rule antecedents refer input features state location relative point sequence vector dna symbol sequence symbols occur agtc means nucleotides point nucleotides point biological convention position numbers rule specifications nucleotide suffice addition notation locations dna sequence table specifies location number sequence point figure location numbering dna sequences table single letter codes expressing uncertain dna sequence locations code meaning code meaning code meaning standard notation referring combinations nucleotides single letter compatible codes embl genbank pir data libraries major collections data molecular biology promoter recognition testbed article prokaryotic promoter recognition prokaryotes single-celled organisms nucleus coli promoters short dna sequences precede beginnings genes algorithmic method promoter recognition make identify starting locations genes long uncharacterized sequences dna promoters detected wet biological experiments locations protein named rna polymerase binds dna sequence input features promoter recognition sequence consecutive dna nucleotides biological convention point promoter recognition site gene transcription begins promoter point located nucleotides -long sequence positive examples nucleotides transcribed gene table initial rule set promoter recognition task rules sites dna sequence bind rna polymerase minus minus regions regions named distance point rules derived biological literature noordewier rule set table translated kbann neural network topology shown figure recall kbann adds additional low-weighted links shown additional sequence information relevant algorithm capture information training training examples consist sample promoters nonpromoter sequences prior training rules table classify examples promoters rules useless classifier capture significant amount information promoters promoter recognition problem previously introduced table initial rules promoter recognition matches sequence location promoter contact conformation contact minusminus- minus- cttgac minus- minus- ttgaca minus- minus- ttg minus- tataat minus- ttgac minus- tataat conformation conformation conformation conformation caa gcgcc promoter contact conformationminus minus dna sequence figure initial kbann-net promoter recognition low-weighted links shown box bottom figure represents sequence location encoded input units standard problems theory-revision systems test inductive theory-refinement systems labyrinth inductive logic-programming systems focl grendel test problem exemplar-based learning systems knowledge system exceeded performance report article splice-junction determination testbed eukaryotic splice-junction determination unlike prokaryotic cells eukaryotic cells nucleus splice junctions points dna sequence cell removes superfluous dna process protein creation problem posed dataset sequence dna recognize boundaries exons parts dna sequence retained splicing introns intervening parts dna sequence spliced problem consists subtasks recognizing exon intron boundaries referred paper sites biologists call sites donors recognizing intron exon boundaries sites acceptors table initial rules splice-junction determination maggtragt i-stop i-stop taa i-stop taa i-stop taa i-stop tag i-stop tag i-stop tag i-stop tga i-stop tga i-stop tga pyramidine-rich yagg e-stop pyramidine-rich yyyyyyyyyy skipping e-stop taa e-stop taa e-stop taa e-stop tag e-stop tag e-stop tag e-stop tga e-stop tga e-stop tga table meanings letters construct creates rules define disjunct location input consequents antecedent form satisfied parenthesized antecedents true dataset examples approximately remaining due processing constraints tests involving dataset randomly-selected set examples consists dna sequence nucleotides 
long categorized type boundary center sequence center sequence location numbering nucleotides addition examples information splice-junctions includes set rules table count include rules defined iterative construct define meaning noordewier derived set rules biological literature rules networks configured output units categories category considered true exceeds threshold tests reported work threshold rules classify examples correctly promoter recognition success rate initial splice-junction rules due largely tendency rules correctly classify examples figure depicts abstracted version kbann-net constructed rules empirical tests kbann section details sets empirical tests explore kbann works opposed section describes kbann works empirical study address questions stop stop pyramidine rich dna sequence figure initial splice-junction kbann-net box bottom figure represents sequence location encoded input units shaded units represent definitional features choice fixed weights incoming links bias change training kbann compare purely inductive learners hybrid systems solutions proposed biological literature kbann performance dependent identification informative input features identification relevant derived features kbann relative performance depend number training examples quality domain theory briefly tests section show trained kbann-nets effective classifiers comparison method set tests section show relative advantage kbann standard backpropagation kbann underlying empirical algorithm varies inversely number training examples tests section show combination feature identification derived feature construction responsible kbann relative effectiveness finally section lesion studies show approach robust errors initial domain theory kbann versus learning systems tests section explore hypothesis kbann effective efficient learning algorithm terms ability generalize examples training compare kbann section splice-junction promoter testbeds empirical learning systems systems learn theory data kbann versus empirical learners section compares generalization ability kbann systems learn strictly training examples comparisons run ways compare kbann algorithms ability extract information training examples kbann compared backpropagation effective empirical algorithms investigate kbann ability learn small sets examples compare learning generalization kbann backpropagation terms training effort seven-way comparison algorithms section kbann compared empiricallearning algorithms standard backpropagation nearest neighbor pebls perceptron cobweb standard backpropagation networks single completelyconnected layer hidden units weights initialized randomly selected values absolute networks promoters splicejunctions hidden units topology generalizes coarse search topology space results tests show case kbann superior strictly-empirical learners method weiss kulikowski suggestion evaluate systems cross-validation -example promoter dataset leaving-oneout cross-validation leaving-one-out extreme form cross-validation successively left training set promoter recognition leaving-one-out requires training passes training set examples testing set leaving-one-out prohibitively expensive number examples grows weiss kulikowski suggest size training set grows -fold cross-validation yields generalization results equivalent leaving -fold cross-validation -example splice-junction determination dataset n-fold cross-validation initial ordering examples affects split training testing examples result testing splice-junction problem eleven orderings error rates report simple averages eleven trials training testing set membership affected initial ordering examples leaving-one-out testing test set consists single algorithm nearest neighbor sensitive order presentation tests algorithms nearest neighbor eleven orderings promoter examples tests implementations written wisconsin based descriptions cited lone exception cobweb gennari classit code note term cross-validation sense weiss kulikowski testing methodology set examples permuted divided sets division testing remaining divisions training testing division learning algorithm training procedure repeated times partition testing definition cross-validation found neural networks literature cross-validation set training prevent overfitting training set backpropagation sensitive presentation order weights updated presentation training examples tests weights updated simply recording average number errors eleven trials statistics maintained examples counted incorrect incorrectly classified majority eleven trials error-scoring method captures result algorithm results decrease number errors cobweb sensitive presentation order misses examples times misses examples times simple average error rate cobweb double level reported figure conversely perceptron insensitive presentation order definition incorrectness effect perceptron error rate drops effect algorithms extremes reducing error rate examples kbann standard backpropagation classification examples made respect threshold activation output unit greater threshold takes category active output unit falls negative category instance splice-junction problem output activations outputs threshold categorized conversely activations units threshold category assigned network finally initial randomization weights neural network affect learning compensate extra level experimentation required standard backpropagation kbann specifically eleven permutations dataset tested ten initial states test requires separate runs n-fold cross-validation average output unit activations ten trained networks determines classification output unit promoter recognition network trials tenth trial classification network based relationship threshold tests threshold set train networks stopping criteria satisfied training examples activation output unit correct training presented network times network trained epochs network classifying training examples correctly improved ability classify training examples epochs implements patience stopping criterion general networks trained promoter recognition stopped training criterion perfectly learn training data hand incremental system cobweb reasonable count promoter errors method method implies error error occurs time instance orderings hypothesis incremental systems user sort post-collection analysis algorithm expected execute provide answers real-time provide answers running result simple average error rate indicator true error rate cobweb numbers reported cobweb reflect counting method algorithms controlled comparisons due processing time considerations tests promoter domain figure simple average error rates -fold cross-validation promoter dataset leaving-one-out testing meant training networks kbann cobweb pebls perceptron nearest neighbor neill stormo number false positives number false negatives figure test-set performance promoter recognition task assessed leaving-one-out cross-validation kbann-nets splice-junction determination terminate criterion difficult perfectly learn training data termination criteria stringent networks overfit training set impair generalization separate experiment empirically tested overfitting assessing generalization epoch training networks trained periodic generalization assessment reached criteria networks overfit training data observed decline generalization end training period generalization peaked end training observed significant consistent decline unable detect overfitting pursue techniques prevention prune decision trees created tuning cross-validation set decide stop training neural networks results discussion figures present comparisons kbann empirical-learning algorithms listed addition empirical algorithms figure accuracies methods suggested biologists promoter recognition methods suggested stormo method learns presentation set examples category counts times nucleotide appears position counts build scoring matrix applied sequences score threshold threshold predicted members class learned method suggested biologist non-learning hand-refined technique differentiating promoters non-promoters neill analyzed set promoters produced collection filters promoter recognition sequence properly matches filters classified promoter directly implemented approach note derived promoter domain theory conclusions neill paper identical promoter-finding technique proposed kbann generalizes empirical learning system tested biological domains cases differences statistically significant confidence based one-tailed t-test exceptions generalization cost salzburg report making errors promoter testbed unable reproduce results implementation pebls kbann pebls perceptron cobweb nearest neighbor percent errors percent errors 
percent errors figure test-set performance assessed -fold cross-validation splice-junction determination task training examples examples training testing training set training set training set figure partitioning set examples form learning curve kbann marginally standard backpropagation pebls splice-junction problem attribute comparatively poor performance kbann problem sparseness initial theory splice junctions table figure learning small sets examples hypothesis hybrid learning systems efficient system learns data aspect efficiency hypothesis theory data learning system require fewer training examples systems learn data performance algorithms large amount training examples exemplified tests prior section part story ability learn training examples important training examples hard find expensive collect important learning system extract generalizations small set examples method tests compare kbann standard backpropagation tests show backpropagation effective systems learning examplesonly learning curves built splitting set examplesinto subsets approximately examples remaining examples set put testing set partitioned sets increasing size smaller sets subsets larger sets networks trained subsets set tested set figure illustrates method partitioning form learning curves number training examples examples reserved testing test set error rate kbann ann figure learning curves promoter domain examples kbann training examples represents accuracy initial theory ann training examples represents random guessing test set error rate number training examples examples reserved testing ann kbann figure learning curves splice-junction domain examples kbann training examples represents accuracy initial theory ann training examples represents random guessing partitioning dependent ordering examples repeat partitioning eleven times permutations algorithm tests repeated ten initial sets network weights ten initial weight sets eleven permutations seven-way comparison algorithms results discussion results presented figures verify hypothesis kbann efficient standard backpropagation terms ability extract accurate generalizations small set training examples promoter splice-junction datasets test set error rate small sets training data percentage points kbann standard backpropagation difference steadily declines datasets number training examples increases promoter set figure kbann advantage percentage points respect standard backpropagation training largest training set hand kbann percentage point disadvantage respect backpropagation splice junction set training largest set training examples analyzing learning curves terms quickly error rates reach levels attainable training promoter dataset kbann-nets require training examples achieve error rate percentage points achieved full set training examples hand standard backpropagation requires training examples training examples figure kbann percentage point disadvantage standard anns contrasts results reported seven-way comparison show kbann slightly superior standard anns trained examples dichotomy results reflects difference error counting figure simple average figure complex method earlier section approach levels achieves training examples results splicejunction problem similar kbann requires roughly one-third number training examples required backpropagation reach error rates testing examples close levels asymptotically attained system discussion comparison kbann empirical learning algorithms results reported figures describe asymptotic performance system basis comparisons kbann effective learning algorithm problems characterized paucity data performance systems data availableis important performance large collection examples learning curves figures compare performance backpropagation kbann important results figures kbann versus theory data learners tests section compare kbann systems learn theory data system system make minimal corrections domain theories system labyrinth-k extension labyrinth insertion domain knowledge labyrinth turn based cobweb section detailed description algorithms kbann systems initial knowledge incorrect incomplete basis learning method results labyrinth-k supplied authors respective systems published sources fortunately systems tested virtually identical procedures testing kbann closely method systems systems tested promoter dataset knowledge labyrinth-k tested splice-junction problem applied splice-junction problem unable handle negated antecedents methodology constructing learning curves similar prior section specifically initial step partition promoter dataset test set examples training set examples partitioned training set sets examples smaller sets subsets larger sets subsets training smooth statistical fluctuations repeated procedure times results figure plots predictive accuracy hybrid learning systems figure kbann consistently percentage points systems blip initial performance labyrinth-k consistently poorest performance problem discussion empirical comparisons results tests section suggest kbann generalizes systems learn examples empiricallearners systems kbann learn theory data kbann relative weakness splicejunction dataset attributed factors splice-junction domain theory sparse perceptron-like learning expect richer domain theory improve performance kbannnumber training examples examples reserved testing test set error rate kbann labyrinth-k figure accuracy hybrid learning systems promoter recognition nets hypothesis tested domain theory splice junctions hypothesis explain poor performance kbann splice-junction problem standard backpropagation utilizes large dataset splice-junction domain alternate view hypothesis domain theory prevents network learning solution differs significantly solution proposed theory hypothesis suggests standard backpropagation outperform kbann large amounts training data learning curve figure supports hypothesis sources kbann strength taking narrow view results prior section amount training data kbann generalizes systems learn theory data figure systems learn data figures section explores results subsection tests rejects hypothesis difference kbann hybrid learning systems due differences learning algorithms underlying system subsection investigates hypotheses attempt explain kbann improves standard backpropagation underlying empirical learning algorithm comparing kbann theory data learners hypothesis account kbann advantage respect labyrinthk datasets tested difference due wholly kbann underlying learning algorithm words kbann outperforms labyrinth-k backpropagation generalizes labyrinth molecular-biology datasets data presented earlier figures support contention figures show backpropagation outperforms cobweb promoter splice-junction datasets figure compares backpropagation cobweb experimental methodology figure figure lends credence hypothesis relative ability backpropagation addition number empirical studies suggest wide range conditions backpropagation good empirical learning systems classifying examples training reasonable conclude power kbann due simply underlying empirical learning algorithm figure kbann simply backpropagation figure plots improvement hybrid system underlying empirical algorithm percentage total improvement shows kbann labyrinth slightly outperforms cobweb number training examples examples reserved testing test set error rate backpropagation labyrinth figure accuracy empirical algorithms underlying hybrid learning systems promoter data number training examples examples reserved testing improvement kbann labyrinth-k improvement error rate underlying algorithm error rate hybrid algorithm error rate underlying algorithm figure improvement hybrid learning systems underlying empirical algorithms proportionally improves underlying learning algorithm labyrinth-k words kbann makes effective domain knowledge systems result refutes hypothesis kbann effectiveness due wholly relative superiority underlying learning algorithm comparing kbann standard backpropagation results presented kbann superiority hybrid systems due partly wholly backpropagation results address kbann improves backpropagation discussed hypotheses account kbann abilities respect backpropagation structure responsible kbann strength topology kbannnet suited learning domain standard ann ann single layer hidden units complete connectivity layers initial weights responsible kbann strength initial weights kbann-net select critical features domain simplifying learning reducing problems resulting spurious correlations training 
examples subsections test hypotheses promoter domain standard knn structure network figure standard kbann-nets versus structure-only networks structure responsible kbann strength hypothesis correct network structure rules initially random weights effective predicting unseen examples standard kbann-net method test hypothesis creating networks exact structure kbann-net link weights biases randomly distributed learning abilities structure-only networks compared standard kbann-nets figure graphically depicts difference standard structured networks experiment procedure tightly control results create kbann-net duplicate network duplicate network reset link weights set domain knowledge randomly selected values duplicate network reset biases randomly selected values test pair networks eleven repetitions ten-fold cross-validation orderings previous experiments smooth differences resulting slightly initial weight settings repeat procedure ten times compare structure-only networks standard kbann-nets repetitions ten-fold cross-validation results figure results tests reports sets data generalization performance training effort measured terms number epochs required training results refute hypothesis strength kbann arises solely determination network topology structured network worse generalizing promoter testing examples slower train standard kbann-nets domains differences statistically significant confidence one-tailed paired-sample t-test note structured network slower train worse generalizing testing examples standard anns difference generalization standard anns structured networks statistically significant standard kbann structured network standard ann training epochs test set error rate error epochs figure learning standard kbann-nets structure-only nets network links added network missing links added figure creation networks test hypothesis initial weights significant contributor kbann darkened input units features identified important domain theory initial weights responsible kbann strength section tests hypothesis strength kbann results weights links kbann-nets identifying relevant features environment unable devise direct test hypothesis reformulated feature identification source kbann strength ann preferentially connected features identified domain theory effective generalization kbann-net method test reformulated hypothesis standard anns single layer hidden units initially connected input units explicitly mentioned domain knowledge illustrated figure promoter domain creates network number links fully-connected network network added missing links input hidden layers increments random selection hidden input layers fully connected step missing links added network copied copy put experimentation figure illustrates link addition procedure links added smooth fluctuations resulting addition important links reprobability link added test set error rate kbann fully-connected ann figure classification performance standard anns initially links features promoter domain theory peated procedure times creating networks networks trained ten-fold cross-validation standard eleven permutations training examples results figure plots average error rate missing link networks networks links identified initial domain knowledge worse generalizing testing examples kbann-nets standard anns time missing links added missing-link networks outperform standard anns missing-link networks remain superior standard anns missing links added links added missing-link networks slightly inferior fully-connected networks inversion anomalous statistically significant performance missing-link networks peaks missing links added best-case performance significantly inferior kbann confidence one-tailed t-test results improvement kbann standard backpropagation results identification important input features discussion sources kbann strength tests structure weight focusing account superiority kbann-nets standard anns hypothesis combination structure focusing weights give kbann advantage backpropagation true analysis rules-to-network translation algorithm makes surprising combination structure weighting success kbann combination focuses network significant input features set derived features important structure potential derived features weight significant inputs combination structure weight supplies derived features give kbann power noise domain theories tests section address hypothesis kbann-nets insensitive noise domain theories hypothesis correct domain theory provided kbann approximately correct supply information hypothesis tested systematically adding noise domain theories tests hypothesis addition verifying hypothesis suggest bounds correctness initial domain theory kbann expected generalize standard anns work addition noise domain theories existing work completely investigate problem space richards tested effect number errors domain theory forte system pazzani investigated effect types domain-theory problems introducing random faults theories style describe papers extant time experiments types noise methods approximate noise represent independent attempt identify test important types domain-theory noise domain-theory noise split categories rule-level noise antecedentlevel noise categories subcategories discussion assumes sake clarity two-category positive negative domain rules negation-by-failure rule-level noise affects rules appears guises missing rules rules required correct operation domain theory missing effect missing rules predict output class unnegated antecedents rules render chains reasoning impossible result rule set missing rules underpredicts positive class added rules rules required correct operation domain theory present effect added rules make proofs occur rule sets extra rules overpredict positive class antecedent-level noise independent rule set missing rules unnecessary rules antecedent-level noise problems result improper statement antecedents individual rules impropriety occur forms extra antecedents extra antecedents required correct rule effect overconstrain application rule extra antecedents result rule set underpredicts positive class missing antecedents missing antecedents antecedents part rule effect missing antecedents underconstrain application rule result missing antecedents added rules result overpredicting positive class inverted antecedents inverted antecedents antecedents rule opposite negated antecedent unnegated vice versa effect inverted antecedent make rule apply wrong situation result labeling positive examples negative converse method tests antecedent-level noise start initial domain theory probabilistically add noise uniform distribution theory experiments involving increasing quantities add noise incrementally domain theory noise smallest noise fraction added slightly noisy rule set noise added total noise equal smallest fraction repeat procedure times kinds antecedent noise add antecedent-level noise procedure antecedent rule probabilistically decide add noise decision add noise procedures add randomly select antecedent added input features consequents defined labeling procedure section consequent rule considered add selected antecedent rule scanned antecedent member delete simply delete rule member invert negate negated delete negation similar incremental procedure probabilistically add rule-level noise domain theory expand domain theories creating rules simple conjunctions randomly-selected input features number antecedents putative rule random number range add newlycreated rules domain theory existing disjuncts domain theory reason restriction minimal structure rule set kind additions rules added conditions conjunct require adding antecedent-level noise addition rule-level noise rules added create ways prove consequent disjunctive sort addition requires significant alterations hierarchical structure rules rules rule leading final conclusions subject deletion antecedent noise repeat procedure times type noise results discussion figure presents results increasingly adding antecedent-level noise promoter domain theory expected inverting antecedents largest effect approaches inserting antecedentlevel noise addition type noise domain theory resulting kbann-net generalizes worse standard ann hand irrelevant antecedents effect kbann-nets noise antecedents important original domain theory spurious antecedent added performance kbann-nets superior standard ann appearing figure tests noise types added domain theory surprisingly resulting kbann-nets 
perform average types noise individually results rule-level noise figure case initial rules deleted leaving kbann-nets superior anns addition randomly created rules effect accuracy range number antecedents rules promoter splice-junction domain theories probability addition noise antecedent test set error rate invert antecedent drop antecedent fully-connected standard ann add antecedent figure effect antecedent-level noise classification performance kbann-nets promoter domain probability addition rule-level noise test set error rate delete rules add disjuncts fully-connected standard ann figure effect rule-level noise classification performance kbann-nets promoter domain kbann-nets inability added rules influence correctness kbannnets surprising added rules average randomly-chosen antecedents rules expected match patterns assumes dna sequence analysis domain feature values result chances good rules match training testing patterns introduced rules matched training patterns effect similar adding single highly-weighted link input unit figure kbann-nets insensitive type noise effect irrelevant rules expected minimal results show networks created kbann outperform standard anns sizable range domain-theory quality theory promoters incorrect originally provide benefit standard anns moral tests clear overstated rule antecedent include domain theory hand absolutely false general discussion kbann effectiveness results experiments presented section suggest fairly concise definition abilities kbann kbann effective generalizing examples training empirical learners systems learn theory data superiority kbann built effective empirical learning system systems learn theory data kbann combination theory data systems separate set tests reasonably-accurate domain theories sufficient kbann create network generalizes specifically results section show kbann effective learning domain theories overly specific result serendipitous promoter splice-junction domain theories overly specific domain theories testing extracted directly biological literature person unfamiliar property kbann success kbann domains reflect intentional bias favor tests small artificial domains show qualitatively similar results optimistic future tests continue show success kbann algorithm work colleagues biological domain protein secondary-structure prediction process control problem shown generality approach finally tests show kbann due identification informative input features derived features establishing good network topology focusing attention topology selection suffice approaches hybrid learning stated earlier recent work field hybrid learning review field section closely compare approach representative approaches initial systems compared kbann all-symbolic approaches labyrinth-k examples approach all-symbolic approaches compare kbann inductive logic programming focl representative field lastly compare kbann work katz representatives systems kbann base hybrid systems neural networks all-symbolic approaches labyrinth-k similar approach forming hybrid system ways similar kbann kbann systems previously developed empirical learning algorithm modify user-supplied propositional domain theory systems symbolic learning algorithms avoid elaborate translation mechanism required kbann result underlying empirical learner directly modifies initial knowledge rerepresentation knowledge learning complete modified domain theory directly observable labyrinth-k initial knowledge create structure cobweb underlying empirical algorithm create set examples structure acted cobweb cobweb created structure place time theories extracted unaware property labyrinth-k extension cobweb cobweb structured domains contrast initial knowledge sort examples correctly classified incorrectly classified incorrect group broken reflect part initial knowledge failure resulted misclassification underlying learning algorithm executed incorrect examples correct examples determine modification knowledge makes examples groups correct systems initial knowledge arbitrary types errors significant step forward form earlier systems required domain theory classes mistakes systems create knowledge form rules supplied initial information addition systems initial knowledge focus learning relevant features simplifying learning problem expected require fewer examples affected spurious correlations standard empirical learning systems systems based inductive logic programming recent proliferation work inductive logic programming goal kbann refine existing domain theory classified examples addition kbann inductive logic programming applied problems computational biology inductive logic programming systems closely related labyrinth-k limited propositional rules systems focl focl extension foil inductive learning mechanism foil work user-supplied background knowledge focl add delete relations existing rules add rules existing domain theory -like information gain heuristic focl correct arbitrary types errors provided domain theory set positive negative examples problem tests artificial domains show focl kbann tolerant variety errors initial domain theory training examples focl unable directly handle promoter domain theory theory classifies training slight modification focl handle problem generalization inferior kbann examples training focl error rate number examples kbann error rate systems based neural networks hybrid systems developed recently neural networks basis focus systems varied reducing number examples required robot learning probabilistic logics propagation mycinlike certainty factors refinement systems fuzzy logic review diverse system focus systems developed time kbann slightly paths system similar kbann symbolic knowledge form rules establish structure connection weights neural network system non-differentiable activation functions handle disjunctions rule sets tolerate non-differentiability carefully organizes networks layers units layer differentiable training learning mechanisms applied types units empirical study medical diagnosis showed technique effective method closely tied learning mechanism system unable developments methods training neural networks katz describes system kbann knowledge base mapped apparently hand neural network focus katz work improving time required classify object generalization speed increase achieved building connections system bypass intermediate processing stages result initial resemblance network knowledge base rapidly lost learning network make corrections initial knowledge impossible interpret corrections symbolic rules construction future networks trends hybrid systems research pronounced trend development hybrid systems specifically constraints quality form information eased early systems unimem required correct theoretical information subsequent systems eased correctness requirement introduced constraints strict over-generality domain theory recent systems kbann arbitrary imperfections theories data side earlier systems required data noise free recent systems noisy examples reductions quality requirements theory data generally accompanied examples addition recent trend hybrid systems tend built top established empirical learning system case labyrinth-k focl kbann earlier systems tended specially-designed empirical components advantage strong constraints theory data systems made limitations future work empirical results previous section show kbann effective learning system unfinished kbann significant limitations kbann proposals elimination additional open issues concepts kbann learns incomprehensible humans due neural representation difficult identify basis kbann classification future examples trained neural networks explain decisions difficult convince experts reliability system conclusions difficult transfer kbann learned solution related problems transfer method problem partially addressed sophisticated training techniques error functions instance addition penalizing networks incorrect answers penalize squared distance expert-assigned trained weights maintain interpretability trained networks network-to-rules translator developed directly addresses problems kbann rule syntax limited kbann handle rules cycles variables research area expansion kbann admit type cyclic rules future kbann handle quantified variables rules techniques binding variables neural networks inductive logic programming systems foil focl golem relational rule sets avoid restrictions 
inherent kbann mechanism handling uncertainty rules point cuts ways kbann mechanism assigning weights antecedents approach kbann mechanism expressing partial truth consequents mycin-like certainty factors integrated rules-to-network translator factors adjust link weights resulting network links weights depend global link weight certainty factor change definition link weights require modification method setting bias rules-to-network translator changed neural learning kbann ignores symbolic meaning initial network result training networks created kbann unnecessarily difficult tighter integration symbolic neural learning result reduced training times improved generalization preliminary experiments algorithm designed tightly integrate symbolic neural elements kbann support hypothesis mechanism changing topology network addition missing antecedents rules missing domain theory kbann handle problem alter existing rule represents missing effects combination make network difficult interpret combination impair generalization force correct rules change solution add mechanism proposes rules appropriately modifies topology network addition tests reported section repeated artificial domain theory relevant features certainty artificial problem tightly controlled experiments real-world problems studied paper instance artificial problem closely controlled experiment effects irrelevant missing antecedents tests made artificial problem results section show kbann robust learning algorithm final summary article describes empirically analyzes kbann system hybrid method learning domain knowledge classified training examples kbann combination subsymbolic symbolic learning create algorithm learns examples rules generalizes efficient terms number training examples required algorithms make sources information kbann operates translating propositional rule set neural network topology network set network initially reproduces behavior rules network trained set classified examples standard neural learning techniques empirical testing shows network created trained effective generalizing examples training principle empirical results showing effectiveness kbann section section kbann shown generalize empirical learning algorithms hybrid algorithms results presented paper show method effective wide variety conditions ranging training examples low-quality domain knowledge results show kbann effective learning real-world domains dna sequence-analysis domains promoter recognition splice-junction determination small problems field growing rapidly result human genome project ideas underlying kbann protein folding process control work remain kbann general topic learning theory data kbann system represents step significant research path conclusion paper shown profitably combine symbolic connectionist approaches artificial intelligence acknowledgements research partially supported office naval research grant national science foundation grant iriand department energy grant de-fg creating datasets investigated especiallythank michiel noordewier addition david aha patrick murphy irvine maintaining database machine learning problems datasets describe paper finally richard maclin charles squires mark craven derek zahn david opitz sebastian thrun bharat rao lorien pratt anonymous reviewers insightful commentary cases programming assistance ahmad study scaling generalization neural networks technical report ccsr- illinois center complex systems research atlas cole muthusamy lippman connor park el-sharkawi marks peerformance comparison trained multi-layer perceptrons trained classification trees proceedings ieee bachant mcdermott revisited years trenches magazine barnard cole neural-net training program based conjugategradient optimization technical report cse oregon graduate institute beaverton berenji refinement approximate reasoning-based controllers reinforcement learning proceedings eighth international machine learning workshop evanston birnbaum collins eds proceedings eighth international machine learning workshop morgan kaufmann evanston cohen compiling prior knowledge explicit bias proceedings ninth international machine learning workshop aberdeen scotland cost salzberg weighted nearest neighbor algorithm learning symbolic features machine learning dietterich learning knowledge level machine learning fahlman lebiere cascade-correlation learning architecture advances neural information processing systems volume denver fisher knowledge acquisition incremental conceptual clustering machine learning flann dietterich study explanation-based methods inductive learning machine learning integration neural heuristics knowledge-based inference connection science gennari langley fisher models incremental concept formation artificial intelligence hinton connectionist learning procedures artificial intelligence holte acker porter concept learning problem small disjuncts proceedings eleventh international joint conference artificial intelligence detroit iub nomenclature committee ambiguity codes european journal biochemistry katz ebl sbl neural network synthesis proceedings eleventh annual conference cognitive science society ann arbor king muggleton lewis sternberg drug design machine learning inductive logic programming model structure activity relationships trimethoprim analogues binding dihydrofolate reductase proceeding national academy sciences usa kleene representation events nerve nets finite automata automata studies shannon mccarthy eds princeton press princeton lacher hruska kuncicky backpropagation learning expert networks ieee transactions neural networks lebowitz integrated learning controlling explanation cognitive science maclin shavlik refining algorithms knowledge-based neural networks improving chou-fasman algorithm protein folding machine learning mahoney mooney combining connectionist symbolic learning refine certainty-factor rule-bases connection science mcculloch pitts logical calculus ideas immanent nervous activity bulletin mathematical biophysics michalski tecuci eds proceedings international workshop multistrategy learning george mason harpers ferry mingers empirical comparison pruning methods decision tree induction machine learning mitchell keller kedar-cabelli explanation-based generalization unifying view machine learning mitchell thrun explanation-based neural network learning robot control advances neural information processing systems volume denver muggleton inductive logic programming academic press san diego muggleton king sternberg protein secondary structure prediction logic-based machine learning protein engineering murphy medin role theories conceptual coherence psychological review noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences advances neural information processing systems volume denver neill escherichia coli promoters consensus relates spacing class specificity repeat substructure dimensional orgainzation journal biological chemistry opitz shavlik heuristically expanding knowledge-based neural networks proceedings thirteenth international joint conference artificial intelligence chambery france ourston mooney theory refinement combining analytical empirical methods artificial intelligence pazzani kibler utility knowledge inductive learning machine learning pazzani influence prior knowledge concept acquisitionexper- imental computational results journal experimental psychologylearn- ing memory cognition pazzani brunk silverstein knowledge-intensive approach learning relational concepts inductive logic programming muggleton eds academic press pineda generalization back-propagation recurrent neural networks physics review letters pratt mostow kamm direct transfer learned information neural networks proceedings ninth national conference artificial intelligence anaheim quinlan induction decision trees machine learning quinlan learning logical definitions relations machine learning rendell cho empirical learning function concept character machine learning richards mooney refinement first-order horn-clause domain theories machine learning rosenblatt principles neurodynamics perceptrons theory brain mechanisms spartan york rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition volume foundations rumelhart mcclelland eds mit press cambridge schank collins hunter transcending inductive category formation learning behavioral brain sciences scott shavlik ray refining pid controllers neural networks neural computation shavlik mooney towell symbolic neural net learning algorithms empirical comparison machine learning shavlik towell approach combining explanation-based neural learning algorithms connection science shortliffe buchanan model inexact reasoning medicine rule-based expert systems buchanan shortliffe eds 
addison-wesley reading stormo consensus patterns dna methods enzymology volume academic press orlando thompson langley iba background knowledge concept formation proceedings eighth international machine learning workshop evanston towell symbolic knowledge neural networks insertion refinement extraction phd thesis computer sciences department wisconsin madison towell shavlik extracting refined rules knowledge-based neural networks machine learning towell shavlik noordewier refinement approximately correct domain theories knowledge-based neural networks proceedings eighth national conference artificial intelligence boston waterman guide expert systems addison wesley reading watrous learning algorithms connectionist networks applied gradient methods nonlinear optimization proceedings international conference neural networks volume watson hopkins roberts steitz weiner molecular biology gene benjamin-cummings menlo park weiss kulikowski computer systems learn morgan kaufmann san mateo wisniewski medin pocket purse tightly coupled theory data driven learning proceedings eighth international machine learning workshop evanston wolpert overfitting avoidance bias technical report la-ur- santa institute santa kbann language rules-to-network translator section requires user sets information describe problem set information specification features describe examples set information set rules encode knowledge problem supplied system domain theory sections define syntax information types handled kbann rules-to-network translator information features kbann handle examples types features commonly machine learning nominal list red green blue boolean values true false hierarchical values exist isa hierarchy linear values numeric continuous ordered values non-numeric total ordering subsections feature definition description encoded kbann missing values handled consistent manner feature types feature units encode equal activation addition sum activations units set total activation units equal total activation units network learn distinguish missing values basis total activation empirical tests shown method encoding missing values results generalization methods setting activations nominal simplest type feature values feature named kbann translates feature color values red blue green binary units color-is-red color-is-blue color-is-green binary binary-valued features special subclass nominal features values values true false reduce number input units binary-valued features single input unit information binary feature activation note departure consistent total activation approach handling missing variables implemented feature types testing focused nominal binary features material non-insulating insulating ceramic paper styrofoam open-cell foam figure hierarchy cup materials hierarchical hierarchical features features defined context isa hierarchy instance figure illustrates hierarchy materials making cup hierarchical features act nominal features baselevel units active information hierarchical feature activation units level hierarchy reciprocal number units level styrofoam open-cell-foam paper ceramic activations insulating non-insul activations material activation linear linear features numeric values values continuous feature numeric values linear feature linear features kbann requires user define set subranges range feature subranges translated input unit activated equations exp abs upb lowb upb lowb equations upb top subrange lowb bottom subrange exact feature intermediate stage calculation activation activation unit indices ranging units encode feature nominal features information linear feature unit number units encode feature ordered ordered features special type nominal features values totally ordered size represented totally ordered set tiny small medium large huge nominal features ordered features handled creating input unit ordered features treated simple nominal features boundaries subsequent values ordering typically indistinct object medium size equally correctly largeor small account lack precision inherent ordered features values ordered feature activated distance formula equation activation distance information rules section describes syntax rules kbann translate neural network specifies kbann translates rules kbann form propositional horn clauses addition disjuncts expressed multiple rules consequent finally rules cycles rule types kbann recognizes distinct types rules type definitional definitional rules rules changed training phase instance definitional rule blots backgammon result consequents fixed rules connected antecedents mentioned rule weights connections antecedents allowed change training type rule nondefinitional variable variable rules standard rules subject change training consequents connected antecedents mentioned rule weights links entering consequent changed neural learning special antecedents addition simply listing positive antecedents form simple conjunctive rule kbann special predicates antecedents rules predicate description negate antecedent figure greater-than linear ordered features figure less-than linear ordered features figure n-true compact specification concepts form antecedents true figure specification antecedents recursive types antecedents nested size bus bread box car bus house true figure encoding rules neural network 

bias bias bias input input 
output output inputs hidden units input input output input input neural learning trained neural network refined symbolic knowledge symbolic knowledge initial initial neural network network rules rules network bias step initial unit bias step bias bias steps numbertrue returns number true antecedentsnumbertrue steps promoter contact conformationminus minus dna sequence network mofn subset linus error rate promoter domain training set testing set network mofn subset linus error rate splice-junction domain number rules number antecedents promoter domain initial rules nofm linus number rules number antecedents splice-junction domain initial rules nofm linus network mofn subset linus antecedents rule promoter domain positive antecedents negative antecedents network mofn subset linus antecedents rule splice-junction domain del add add neg swap del del del add add add exact exact percent dataset needed learn concept problem problem unable learn network mofn subset del add add neg swap del del del add add add exact exact number antecedents problem problem learned actual height mofn subset 
winput winput light concavity concavity handle flat bottom stable graspable open vessel liftable cup appears proceedings sixth ieee conference data mining icdm belief propagation large highly connected graphs part-based object recognition frank dimaio jude shavlik computer sciences dept wisconsin madison madison dimaio shavlik wisc abstract describe part-based object-recognition framework specialized mining complex objects detailed images objects modeled collection parts pairwise potential function efficient inference algorithm based belief propagation finds optimal layout parts input image introduce aggbp message aggregation scheme groups messages approximated single message objects consisting parts reduce cpu time memory requirements apply aggbp synthetic data real-world task identifying protein fragments three-dimensional images experiments show improvements result minimal loss accuracy significantly time introduction recent explored part-based models recognizing generic objects images models represent physical objects graph collection vertices parts connected edges enforcing pairwise constraints inference algorithm finds probable location part model image previous work considered simple objects parts present part-based object recognition framework capable identifying objects thousands parts rich three-dimensional image data arises applications biological imaging techniques fmri confocal microscopy produce high-quality images tissues x-ray crystallography yields electron density map three-dimensional image macromolecule data sources objects comprised parts connected complex topology rich two-dimensional data detailed satellite imagery complex objects interpreted current methods effectively mine complex objects algorithm belief propagation large highly connected graphs standard offer efficiency fully connected graphs thousands vertices approximations messages compute marginal distributions reasonable amount time describe aggbp aggregate approximates groups messages single message fully connected graphs aggbp reduces running time graph withn nodes finally test approximation techniques real-world synthetic data testbed realworld computer-vision task identifying protein fragments three-dimensional images interpreting protein images important step determining protein structures x-ray crystallography aggbp lets scale interpretation large proteins large images testbed synthetic object generator test aggbp performance finding objects part topologies part-based object recognition framework describes class objects pairwise undirected graphical model pairwise undirected graphical models define joint probability distribution set variables graph product potential functions edge vertex graph undirected graphical models graph associate vertex random variable conditioned observafigure nizing person image thicker lines skeletal edges thinner lines occupancy edges tion variables object recognition thesexs describe position possibly orientation part vertex observation potential edge structural potential parts image productdisplay productdisplay concerned finding labels maximize joint probability describe object part-based framework pieces data part graph node observation potential edge structural potential part graph potential functions typically learned previously solved problem instances detail working paper object-recognition framework part graph fully connected reason illustrated figure shows model recognize people images sparsely connected skeleton thick edges connects highly correlated nodes skeletal potential form part pair arms labels completely conditionally independent parts occupy space occupancy edges connecting pairs parts ensure parts model occupy space occupancy potential typically step function non-zero connected parts sufficiently distant belief propagation image graphical model inference attempts find most-probable location object parts image object graph loops exact inference methods work framework belief propagation messagepassing approximate inference algorithm computes marginal probability part location algorithm belief propagation input observational potentials structural potentials output approximation marginal summationdisplay summationdisplay summationdisplay summationdisplay converged foreach parts foreach partt iftnegationslash sand updated mnt integraltextx bntmn dxt end mnt end end end passing series local messages pseudocode appears algorithm iteration part model computes product incoming messages passes convolution product neighbors clarity message dependence dropped mnt integraldisplay productdisplay dxt message thought indicating node expects find based belief approximation marginal belief product incoming messages observation potential bns productdisplay mnu tree-structured graphs exact graphs cycles guarantees correctness produces good estimates practice working paper discusses related work area aggbp scaling belief propagation modeling objects hundreds parts number messages quickly overwhelming make tractable types graphs propose aggbp approximates subset outgoing messages single node single message replacing message computations undirected graphs object recognition pairs figure aggbp approximates group messages single message nodes skeleton edges highly correlated messages edges high information content graphs majority edges occupancy edges enforce constraint parts occupy space potential functions edges weak uniform messages edges carry information edges make approximations formally message update equation alternately written mnt integraldisplay bnt dxt denominator term avoid double-counting making method exact treestructured graphs loopy graphs double-counting unavoidable occupancy edges denominator carries information aggbp drops mnt integraldisplay bnt dxt key advantage structural potential identical occupancy edges cal refer approximate messages asmt illustrated chain figure approximation reduces number occupancy messages computed updating belief part requires multiplying incoming occupancy messages running time send aggregate messages central accumulator acc nproductdisplay accumulator update node belief constant number operations reducing aggbp runalgorithm aggregate belief propagation initialize accumulatoracc messagesmto converged foreach parts acc acc acc foreach partt skeleton neighbor updated mnt integraltextx bntmn dxt end mnt mnt end end compute agg msg send accumulator mns integraltextx bns acc acc mns end end time chain acc acc acc algorithm pseudocode overview aggbp key difference algorithm loop skeleton neighbor object recognition framework skeleton graph sparsely connected loop rarely entered finally occupancy edges potential functions aggbp advantage approximation additional approximation error case aggbp computes broadcast message partt average potential function outgoing fromt mnt integraldisplay summationtextn bnt dxt experiments section compare standard belief propagation algorithm aggbp real-world synthetic datasets real-world task based locating proteinfragmentsin dimages whileoursyntheticdataset figure protein amino-acid sequence density map interpretation finds atom position backbone trace finds key atom amino acid explore recognizing objects complex graph topologies protein fragment identification application object recognition arises ray crystallography determining three-dimensional structure protein crystallography produces threedimensionalimageoftheprotein knownasanelectrondensity map illustrated figure finding atoms interpreting map final time-consuming step ofx-raycrystallography alternatively abackbonetracefocuses locating key carbon atom alpha carbon contained amino acid aggbp automatically determine backbone trace electron density map protein sequence details task empirical results comparison methods found previous work paper authors briefly describe backbone tracing object-recognition framework construct graph node represents amino-acid protein labelws amino-acid consists terms cartesian coordinatesxs amino acid internal rotational parameters probability distributions cartesian space represented discretized probability density estimate protein-specific structural observation potential functions learned previously solved structures node potential function computed matching learned set small protein-fragment templates electron density map edge potential functions basic types skeletal edges run linear aminoacid chain occupancy edges connect non-adjacent pairs amino acids ensuring don occupy space results density maps provided crystallographer george phillips uw-madison convoluted maps protein fragment length maliz cpu 
time maliz emor cpu time aggbp cpu time memory aggbp memory figure comparison memory cpu time usage approximate-bp standard gaussian simulate poor-quality density map automated interpretation methods produce poor results map provided amino-acid sequence true crystallographerdetermined solution maps compare standard inference exactbp aggbp accuracy tracing protein backbone exactbp unable scale entire protein considered locating fragments amino acids normalized cpu time iteration memory usage techniques illustrated figure normalization sets exact-bp time memory usage locating amino-acid fragment average-sized protein sec results experiment figure plot twodifferentmetrics maximum-marginal interpretation function iteration solutions found methods differ terms rms error versus true trace produce equally accurate traces interestingly figure shows log-likelihood maximum-marginal interpretation metric aggbp produces solution figure shows rms error function protein-fragment length surprisingly methods perform slightly worse searching longer fragments predicted structure fairly accurate quality maps rms error finally scatterplot log-likelihoods fragments represented point illustrated figure figure points diagonal correspond fragments aggbp produced morelikely interpretation fragment aggbp produces solution greater log-likelihood standard difference statistically significant two-tailed paired test iteration aggbp iteration c-alpha rms tion true aggbp aggbp protein fragment length c-alpha rms true true aggbp aggbp true lik eliho figure aggbp exactbp error iteration rms deviation log-likelihood interpretation additionally shows rms error function protein fragment size aggbp log-likelihood lik eliho -mers -mers -mers -mers -mers -mers figure scatterplot showing target fragments log likelihood aggbp trace versus exactbp trace synthetic object recognition previous section illustrated performance limitedtopology andalloccupancy potentials identical section construct synthetic object generator explores aggbp performance identifying objects varying topologies object generator derstand aggbp works range tasks locating objects composed interconnected parts generator lets vary graph topology individual partparameters asinfigure jects predefined number parts tree-structured skeleton branching factor skeleton randomly assembled parts part pairs connected skeleton connected occupancy edges fromwhichthe structural potentials derived parts directly connected vary radii increase branching factor spatial overlap figure illustration parameters variable graph generator radii part pairs closer sum radii softness parameter occupancy constraint violated low probability testbed generator artificially generates observation potentials obs prior probability distribution part location space generated pattern matching algorithm object generator assumes classifier area precision-recall curve auprc discussion generator appears working paper results generator varies model parameters branching-factor average branching factor skeleton graph default softness part softness default radius standard deviation radii default graph average part radius fixed grid point model constructed parts object recognition framework search theoptimal layoutof parts somegeneratedobject observation potentials previous section compare aggbp versus exactbp assumed part parameters radius softness learned algorithm run convergence maximum branching factor rms true true aggbp aggbp softness rms standard-deviation radius rms figure comparison exactbp aggbp synthetic data report error vary skeleton branching factor part softness radius standard deviation iterations taking highest-likelihood solution iteration averaging random part graphs comparison errors exactbp aggbp interpretations appears figure figure shows methods equally accurate varying branching factors figure shows small moderate variance object radii handled aggbp aggregation ignores variations interesting result figure object softness varied non-zero softness aggbp finds accurate solution standard unclear aggbp produce more-accurate results standard aggbp ignores term serves avoid feedback feedback unavoidable graphs loops cases ignoring term produces more-accurate approximation dampening feedback loops inherent loopy belief propagation improved accuracy computational savings makes aggbp approximation important technique mining large images conclusions describe part-based object recognition framework suited mining detailed image data introduce aggbp message approximation aggregation scheme makes tractable large highly connected graphs fully connected graphs objectrecognition framework reduce runtime memory experiments biological vision task synthetic data show aggbp produces solutions good standard acknowledgements work supported nlm grant nlm grant dimaio shavlik phillips probabilistic approach protein backbone tracing electron density maps proc ismb dimaio shavlik improving efficiency belief propagation large highly connected graphs workingpaper uwmlresearchgroup felzenszwalb huttenlocher efficient matching pictorial structures proc cvpr frey graphical models machine learning digital communication mit press ihler sudderth freeman willsky gaussian mixtures proc nips isard pampas real valued graphical models computer vision proc cvpr jordan ghahramani jaakkola saul introduction variational methods graphical models machine learning koller lerner angelov general algorithm approximate inference application hybrid bayes nets proc uai mackay neal good codes based sparse matrices cryptography coding ima conference murphy weiss jordan loopy belief propagation approximate inference empirical study proc uai pearl probabilistic reasoning intelligent systems morgan kaufman san mateo rhodes crystallography made crystal clear academic press sudderth mandel freeman willsky visual hand tracking nonparametric belief propagation mit lids technical report weiss interpreting images propagating bayesian beliefs proc nips 
skill acquisition transfer learning advice taking lisa torrey jude shavlik trevor walker richard maclin wisconsin madison usa minnesota duluth usa abstract describe reinforcement learning system transfers skills previously learned source task related target task system inductive logic programming analyze experience source task transfers rules actions target task learner accepts rules advice-taking algorithm learners bene guidance imperfect system accepts human-provided mapping speci similarities source target tasks include advice erences tasks robocup simulated soccer domain demonstrate system speed reinforcement learning substantially introduction machine learning tasks addressed independently implicit assumption task relation tasks domains reinforcement learning assumption incorrect tasks domain tend related tasks erent speci general similarities shared skills conditions agent action goal transfer general skills source task order speed learning similar target task suppose soccer player learned source task ball opponents passing teammates target task suppose learn work teammates score goals opponents player apply passing skills source task master target task quickly tasks shared skills transfer cult problem erences action sets reward structures create erences shared skills passing skill source task incomplete target task passing progress goal agents transferred information continue learn lling gaps left transfer transfer produce appearing ecml research partially supported darpa grant nrl grant keepaway breakaway movedownfield fig snapshots robocup soccer tasks partially irrelevant incorrect skills agents modify ignore transferred information imperfect facilitate transfer human observer basic domain knowledge provide mapping source target tasks mapping describes structural similarities tasks correspondences player objects include simple advice ects erences tasks tips prefer passing goal shoot close goal helpful present system transfer learning called advice induction instruction constructs relational transfer advice inductive logic programming analyze experience source task learn skills rst-order logic user contributes mapping tasks include user advice target-task learner considers advice learning follow ignore approach performs transfer higher level abstraction previous approaches reason performs transfer scenarios involving distant tasks present empirical results challenging robocup simulated soccer domain demonstrating signi cantly faster learning target task breakaway performing user-guided transfer source tasks keepaway movedown eld figure reinforcement learning robocup reinforcement learning agent navigates environment earn rewards avoid penalties environment state nite number features agent takes actions state change q-learning agent learns q-function estimate taking action state agent policy typically action highest q-value current state occasional exploratory actions taking action receiving reward agent updates q-value estimates current state sarsa reinforcement learning algorithms designed sutton robocup learning task m-on-n keepaway objective reinforcement learners called keepers ball hand-coded players called takers game ends opponent takes ball ball bounds learners receive reward time step team ball keepers ball follow hand-coded strategy receive passes original keepaway task keeper ball choose hold pass teammate introduce version called mobile keepaway keeper move inwards outwards clockwise counterclockwise respect eld center realistic movement version transfer games keepaway state representation based designed stone sutton keepers ordered distance learner takers features listed table note logical variables capitalized typed player keeper simplicity types variable names leaving terms player player keeper keeper constants uncapitalized robocup task m-on-n breakaway objective reinforcement learners called attackers score goal hand-coded defenders hand-coded goalie game ends succeed opponent takes ball ball bounds time limit seconds learners receive reward score goal reward attackers ball follow hand-coded strategy receive passes attacker ball choose move ahead left respect goal pass teammate shoot left center part goal breakaway state representation presented torrey attackers ordered distance learner defenders features listed table introduce robocup task called m-on-n movedown eld objective attackers move opposing team goal maintaining possession ball game ends cross vertical line eld opponent takes ball ball table robocup task feature spaces keepaway features breakaway movedown eld features distbetween player distbetween player distbetween keeper closesttaker distbetween attacker closestdefender anglede nedby keeper closesttaker anglede nedby attacker closestdefender xposition object xposition object yposition object yposition object distbetween keeper eldcenter distbetween attacker goalcenter distbetween goalpart anglede nedby goalpart goalie anglede nedby topright goalcenter distbetween attacker goalie anglede nedby attacker goalie timeleft bounds time limit seconds learners receive symmetrical positive negative rewards horizontal movement forward backward attackers ball follow hand-coded strategy receive passes action set feature set breakaway shoot actions features involving goal system discretizes feature tasks intervals called tiles boolean feature tile denoted distbetween takes units enhancement state space robocup stone sutton adopt give linear q-function model ability represent complex functions robocup games substantial erences features actions rewards q-values require skill passing ball teammates losing opponents transferring skills agents learn actions natural human interpretation agents acquire skills typically acquire q-function highly task-speci readily translate discrete skills researchers proposed methods transferring entire q-function policy present approach q-function perform transfer analyzes games played source task learn skills rst-order logic learning high-level concepts favors transfer general behavioral information speci low-level details q-function framework games collections state-action pairs action classi cation state pairs training examples learn classify states traces keepaway games learn concept states passing teammate good action task arises domain data task exists user identi skills transferred mapping relates logical objects source task target task optionally advice transferred skills table algorithm game traces source task skill transfer list skills transferred collect training examples object mapping tasks learn rules aleph user advice optional select rule highest score translate rule transfer advice learn target task advice ilp mapping state distbetween distbetween distbetween action pass outcome caught training examples pass teammate distbetween teammate distbetween skill concept distbetween distbetween prefer pass advice fig showing transfers skills information performs transfer automatically existing game traces source task system learns skill concepts translates advice target task applies transfer advice user advice learning target task table summarizes algorithm high-level pseudocode figure illustrates transfer part algorithm robocup advice item conjunction conditions constraint applied conditions met shown figure advice ned disagrees learner experience advice-taking algorithm maclin explain section demonstrate experiment section protection imperfect transfer learning skills inductive logic programming ilp learn skills ilp method learning rst-order conjunctive rules works data logical relations robocup feature space presented section rst-order rule unlike propositional rule variables teammate figure advantage rst-order rules general rule pass teammate capture essential elements passing skill rules passing speci teammates expect common skill elements transfer tasks advantage ilp general accommodate background knowledge domain system sophisticated user predicates add search space 
added predicates robocup domain represent aggregate features average distance opponent ning mid-level concepts results simpler rules ilp algorithms searching space rules prolog-based aleph software package conduct random heuristic search hypothesis space selects rule nds highest score generalization familiar metric action pass teammate outcome caught teammate pass teammate good pass teammate action good pass teammate bad positive pass teammate negative pass teammate reject fig showing selects training examples produce datasets search examines states games source task selects positive negative examples positive conditions met skill performed desired outcome occurred expected q-value recent q-function minimum score minqpos ratiopos times predicted q-values actions negative action performed highest q-value minimum score minq neg expected q-value skill learned rationeg times highest q-value state maximum score maxqneg standard settings ratiopos rationeg found q-values stochastic domains robocup widely separated parameters set system positive negative examples found learn reasonable rules figure illustrates sorting process robocup mapping skills produce advice task system translates source-task objects target-task objects based user-provided mapping reasonable mapping -onkeepaway -onbreakaway relate keeper attacker taker defender objects target task mapping objects source task left case simply include objects rules examples mapping breakaway goalie keepaway eldcenter similarly ilp algorithm leave search space predicates domain shared tasks kbkr advice-taking algorithm ultimately requires advice propositionalized speci task nal step takes mapping process propositionalize rules instantiates skills pass teammate target task -onbreakaway produce rules pass pass deals conditions rule body variables rule condition distbetween attacker ectively disjunction conditions distance distance interval disjunctions part advice language tile features represent recall feature range divided boolean tiles feature falls interval disjunction satis tiles active -onbreakaway distbetween distbetween exact tile boundaries exist target task adds tile boundaries feature space transfer advice expressed target task feature space unknown time source task learned multiple conditions rule refer variable distbetween attacker anglede nedby attacker closestdefender variable attacker represents object clauses system propositionalize clauses separately nes boolean background-knowledge predicate newfeature attacker closestdefender dist distbetween attacker ang anglede nedby attacker closestdefender dist ang expresses required condition feature -onbreakaway newfeature newfeature adds boolean features considered multidimensional tiles target task transfer advice enhance feature space target task user advice users optionally include advice source-target mapping guide transfer pointing erences tasks passing skills transferred keepaway breakaway make distinction passing goal goal objective score goals players prefer passing goal user provide guidance instructing system add condition pass teammate skill distbetween goal distbetween teammate goal alternatively expert user make system ability features target task advantage approach formally ning feature tiled user rst write nition prolog goaldistance teammate distteammate distbetween teammate goal dista distbetween goal dista distteammate user instruct system add pass teammate rule goaldistance teammate user advice describe skills needed target task shoot skill breakaway important erence keepaway source task type user advice required natural powerful users facilitate transfer advice implementation rules produced transfer provided users imperfect incorrect obeying prevent effective learning treats advice soft constraint agent selectively ignore advice disagrees agent experience target task incorporates advice q-learning linear optimization method called kbkr linear optimizer creates q-function nding action weight state feature q-value state-action pair training set approximately weighted sum features minimizing quantity modelsize datamis advicemis modelsize sum absolute values feature weights datamis disagreement learned function outputs training examples advicemis disagreement learned function outputs advice constraints numeric parameters relative importance minimizing disagreements versus nding simple model decays time advice fades learner gains experience longer requires guidance essentially stops applying advice altogether training progresses linear program resolved games maclin details empirical results present results skill transfer robocup games challenging transfer scenarios games erent reward structures section include study kbkr advice-taking algorithm handles imperfect incorrect advice player code experiments based amsterdam trilearn players skill transfer experiments rst experiment perform transfer -onmobile keepaway -onbreakaway skill transfer pass teammate mapping section assume user encourages passing goal adding goaldistance condition section approximates skills breakaway distbetween goalleft anglede nedby goalleft goalie prefer shoot goalleft actions distbetween goalright anglede nedby goalright goalie prefer shoot goalright actions distbetween goalcenter prefer moveahead moveaway shoot actions experiment perform transfer -onmovedown eld -onbreakaway similar mapping skills transfer pass teammate moveahead assume user advice includes shoot skills assume passing forward moving ahead learned movedown eld user provide guidance analyze results rst experiment detail learned rule mobile keepaway pass teammate distbetween teammate anglede nedby teammate closesttaker distbetween taker distbetween player rule good pass opponent close teammate opponent blocking pass translates rule items transfer advice breakaway teammate adds user advice figure compares learning curves breakaway transfer mobile keepaway shows learning curves transferred skills user advice separately analyze individual probability goal training games transfer transfer skills user advice transfer fig learning curves breakaway transfer mobile keepaway probability goal training games m-keepaway transfer keepaway transfer movedownfield transfer transfer fig learning curves breakaway transfer tasks contributions curve average independent runs data point smoothed games previous games fewer transferred skills scoring probability higher con dence level based unpaired t-tests games full system scoring probable con dence level point full system performs signi cantly transferred skills user advice con dence level transferred skills user hints perform sum parts experiment figure compares learning curves breakaway transfer movedown eld transfer curve mobile keepaway duplicated include results transfer original -onkeepaway task compare performance transfer keepaway variants expected mobile keepaway transfers slightly non-mobile keepaway variants movedown eld successfully transfer breakaway causing higher scoring probability con dence level point experiments imperfect advice demonstrate cope imperfect incorrect advice include experiment transfer intentionally bad user advice perform transfer mobile keepaway breakaway opposite user advice inequalities reversed bad advice instructs learner pass backwards shoot goal narrow angle move close goal figure shows results advice transfer experiment shows bad advice decrease positive ect transfer system impact learning negatively bad advice initial negative ect kbkr quickly learns ignore advice learning curve recovers completely probability goal training games good advice bad advice bad advice advice fig learning curve breakaway bad user advice related work approach builds previous methods providing advice reinforcement learners maclin shavlik develop if-then advice language incorporate rules neural network adjustment driessens dzeroski human guidance create partial initial q-function relational system kuhlmann propose rule-based advice system increases q-values xed amount aspect work extracting 
explanatory rules complex functions sun studies rule learning neural-network based reinforcement learners fung investigate extracting rules support vector machines address knowledge transfer singh studies transfer knowledge sequential decision tasks taylor stone copy initial functions transfer keepaway games erent sizes torrey introduce transfer keepaway breakaway function advise action q-value mapped model highest contrast work learn advice rules rst-order logic transfer individual skills working keepaway game traces functions achieve transfer detailed study transfer learning robocup online machine learning group working paper conclusions future work reinforcement learners bene signi cantly user-guided transfer skills previous task presented system transfers shared skills learning rst-order rules agent behavior translating user-designed mapping system assume similar reward structure source target tasks robustness imperfect transfer advice-taking experimental results demonstrate ectiveness approach complex domain challenge encountered transfer learning erences action sets reward structures source target task make cult transfer shared actions changing game objective adding action meaning shared skill addressed problem user guidance human domain knowledge apply transferred skills encourage learning skills future hope reach similar levels transfer user guidance underlying issue separation general speci information source task transfer learning transfer general aspects skills domain ltering task-speci aspects ilp learn general rst-order skill concepts step goal future step learning skills multiple games domain lead general rules transfer driessens dzeroski integrating experimentation guidance relational reinforcement learning proc icml fung sandilya rao rule extraction linear support vector machines proc kdd kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer aaai workshop supervisory control learning adaptive systems maclin shavlik creating advice-taking reinforcement learners machine learning maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression proc aaai muggleton raedt inductive logic programming theory methods journal logic programming pages singh transfer learning composing solutions elemental sequential tasks machine learning pages srinivasan aleph manual http web comlab oucl research areas machlearn aleph aleph html stone sutton scaling reinforcement learning robocup soccer proc icml sun knowledge extraction reinforcement learning learning paradigms soft computing pages sutton learning predict methods temporal erences machine learning pages sutton generalization reinforcement learning successful examples sparse coarse coding proc nips sutton barto reinforcement learning introduction mit press taylor stone behavior transfer value-function-based reinforcement learning proc aamas torrey walker shavlik maclin advice transfer knowledge acquired reinforcement learning task proc ecml 

simple effective method incorporating advice kernel methods richard macliny jude shavlikz trevor walkerz lisa torreyz computer science departmenty computer sciences departmentz minnesota duluth wisconsin madison kirby drive west dayton street duluth madison rmaclin umn fshavlik twalker ltorreyg wisc abstract propose simple mechanism incorporating advice prior knowledge form simple rules support-vector methods classification regression approach based introducing inequality constraints datapoints match advice constrained datapoints standard examples training set unlabeled data semi-supervised advice-taking approach approach simpler implement efficiently solved knowledge-based support vector classification methods fung mangasarian shavlik knowledge-based support vector regression method mangasarian shavlik wild performing approximately complex approaches experiments approach synthetic task reinforcementlearning problem robocup soccer simulator show advice-taking method significantly outperform method advice perform similarly prior advice-taking support-vector machines introduction important goal machine learning making additional sources training information traditional input-output pairs domain knowledge human-provided advice interesting approach advice builds support-vector methods knowledge-based support vector machines kbsvms fung mangasarian shavlik classification knowledge-based kernel regression kbkr regression mangasarian shavlik wild develop alternative relies extensional definition advice intensional approach kbkr kbsvm approach incorporates advice form rules allowed kbkr kbsvm examples match advice deriving constraints examples approach call extenkbkr extenkbsvm simpler implement kbkr kbsvm generally results faster optimization similar accuracy advantage unlabeled data points semi-supervised advice-taking learning copyright american association artificial intelligence aaai rights reserved kbkr kbsvm approaches advice specificed simple rules describing region input space assertion made class regression points region piece advice regression problem distanceto goalcenter angle goalcenter goalie qshoot rule maclin shows sample rule defined reinforcementlearning method employing kbkr learn set functions advice range situations center goal angle formed center goal goalie large shooting maclin developed extended form advice called preference rules apply situations learning regression functions simultaneously give advice functions relate method easily capture preference advice knowledge-based support vector methods kbkr kbsvm simple rules discussed translated set constraints added optimization problem describe constraints section regression problem goal learn function solution takes form wtx learn function matrix training examples target values kernel version formulation replace wtx standard kernel trick produce solution form learn function solving optimization problem mangasarian shavlik wild min jbj cjjsjj problem produce solution variables minimizes cost good job approximating function note vectorsrepresents slack variables formulation data point slack variable sense model prediction data point appears proceedings twenty-first national conference artificial intelligence aaai boston slack measures move point predict output correctly optimization problem works minimize combination size parameters model band sum slacks indicating solution components weighted parameter user picks selected tuning set tradeoff data fit model complexity kbkr method rule discussed section represented form htx rule section row column dist goalcenter row column angle goalcenter goalie rule specifies multiply sides row row rule qshoot greater implication directly support-vector machine motzkin theory alternative implication converted complicated set linear constraints mangasarian shavlik wild standard practice constraints slacked possibility advice perfect results optimization problem min jbj cjjsjj jjzjj piece advice bti dtu essence kbkr introduces set variablesudefining space solution solution original space rule violated slacks introduced solution variables system find solution partially meets constraints major advantage approach solution produced system match advice training data matches preconditions advice rapid learning inputoutput pairs typical source training information approach limitations limitation approach introduces large number constraints problem depending size number advice rules introduces number variables system equations solved increasing solution time addition solution fairly complex theuvariables directly part solution indirectly constrain variables solution limitation approach interpretation slacked advice variables training slacks learned model partially contradict advice assume user-provided advice perfectly correct slacks variables variables combine approach encourages types advice account noise specifically slacks measure preconditions advice rule rotated translated feature space order advice consistent learned model recall slacks standard training point amount translated feature space types advice-refinement match advice refined scaling polygon representing rule preconditions finally problematic aspect kernelization matrix valuek assumption kbkr kbsvm kernelized matrix representing constraints match original constraints space antecedents rules altered appropriately kernelized limitation led mangasarian wild introducing method weakness proposed method fairly complex requires extensive sampling feature space making scalability significant question address limitations propose simpler family methods call extenkbkr extenkbsvm extensional knowledge-based kernel regression support vector machines chose term extensional approaches represent polyhedral region rule preconditions sample data points included prior work directly manipulated intensional representation advice extensional knowledge-based methods noted previously kbkr kbsvm methods effective limitations notably mechanism account imperfect advice fairly complex approach simpler involves appealing mechanism modifying imperfect advice extensional methods determine training-set points match piece advice add constraints state advice constraint met points understand approach figure shows examples binary classification problem advice region rectangle feature space assume advice points region belong solid-circle class panel shows decision boundary standard svm produce note penalty proportional distance decision boundary misclassified illustrated dashed line panel illustrates advice slacked fung kbsvm rotation translation advice consistent training data decision boundary notice decision boundary moved slightly penalty misclassified remains panel shows extenkbsvm task pays penalty white circle inside advice advice point black circle decision boundary classifies white equation point mispredicted advice involved constraints extenkbsvm figure standard svms panel compared kbsvm panel extenkbsvm panel linear program advice treating standard input-output pair returning regression extenkbkr version equation min jbj cjjsjj jjmjj piece advice formulation set points match advice item note kernel respect set pointsa whereu set unlabeled points method introduces parameter determine total weighting cost formula slacks points covered advice recall original kbkr representation parameters weight slacks advice notice labeled training examples linear program multiple times standard fitting training input-output pair matches advice rule major advantage approach advice applied points derive constraints unlabeled points unlabeled points exist domain hand artificially generated advice-induced constraints directly constrain solution variables problem advantage exceptions advice captured training examples approach datapoint match advice simply slack advice-constraints related training pay penalty cost function addition approach introduces slack variables problem introduce space theu expression solved derive solution mentioned previously maclin introduced preference advice user function higher learning simultaneously problem domains reinforcement learning situation action preferable pref-kbkr formulation preference advice original form advice call absolute advice captured maclin min ajj jbaj cjjsajj jjzijj action ata preference advice bti dtui absolute advice bti dtui formulation models learned actions simultaneously subscriptsp andn refer 
preferred non-preferred action preference advice refers action absolute advice easily formulate extenkbkr version pieces advice min ajj jbaj cjjsajj jjmjj action piece prefer advice piece absolute advice knowledge transfer advice taking lisa torrey trevor walker jude shavlik richard maclin wisconsin madison minnesota duluth west dayton street kirby drive madison usa duluth usa fltorrey twalker shavlikg wisc rmaclin umn abstract present framework knowledge transfer reinforcement learning task related task advicetaking mechanisms discuss importance transfer complex domains robocup soccer show automatically generated advice perform transfer categories subject descriptors arti cial intelligence learning general terms algorithms experimentation keywords advice knowledge transfer reinforcement learning introduction reinforcement learning task arises human experts provide information task relates tasks domain identifying parallels features actions task task human expert learning algorithm transfer knowledge task addition expert direct tips accomplish task call forms human assistance advice complete fully accurate provide valuable information speed learning suppose machine learner learned play robocup soccer subtask keepaway team robotic players ball opposing team players suppose learner learn play related robocup soccer subtask breakaway team robotic players score goal opposing team defenders goalie human advisor identify parallels copyright held author owner cap october banff alberta canada features representing distances players angles players analogous actions passing teammates analogous action shoot goal analogous action pass teammate pretend teammate standing goal human advisor give direct tips breakaway distance goal shot angle prefer shoot pass forms advice easy human provide learner learn play breakaway quickly advice taking transfer transfer knowledge reinforcement learning tasks advice ways summarized figure developed algorithm accept direct advice action preferences previous work advice if-then rule corresponds optional advice task figure advice provided task transfer note require advisor internal details learning algorithm q-values actions advice expressed terms features actions shown advice substantially improve performance breakaway recent work developed algorithm accept advice parallels tasks advice communicates information feature action parallels corresponds circle figure apply learned model task automatically extract transfer advice task transfer advice form direct advice note means worry ranges q-values tasks advice speci action preferences action q-values human advisor identi parallels providing mapping translates features actions task appears proceedings international conference knowledge capture k-cap banff canada learned functions task transferred functions task user-provided advice relating features actions task task advice task action preferred action learned functions task binteractions task advice solving task optional figure transferring knowledge advice features actions task mapping evaluate situation perspective task action situation create transfer advice recommends taking action task table outlines automated process function linear function features expressing action keepaway keepaway features replaced breakaway action breakaway action corresponds part transfer advice rule extracted breakaway keepaway actions passnearest passing nearest real teammate passgoal passing imaginary teammate located goal holdball analogous breakaway actions passnearest shoot moveahead passgoal passnearest passgoal holdball prefer shoot passnearest prefer shoot moveahead recently investigating mechanisms extracting transfer advice current learned models linear functions features place q-function expressions directly advice shown apply transfer advice nonlinear models neural networks support vector machines easily advice language table algorithm create transfer advice learned model task mapping task task actions taska generate advice actions taska prefer task approach investigating inductive logic programming ilp ilp systems learn concepts training examples form rst-order logic rules ilp learn rules transcripts keepaway games create advice breakaway kind user-provided mapping rst-order logic additional advantage variable teammate applies learner teammates previous transfer rule makes create general rules ilp rule learned breakaway dist goalie dist greater goal left dist range teammate prefer pass teammate move ahead presented results rst transfer approach working summary transfer critical capability system attempting learn complex domain learning task scratch extremely time-consuming redundant developed mechanisms extracting applying learned knowledge human-provided advice parallels task automatically create transfer advice action preference format developed incorporate direct advice task format advice-taking tool achieving knowledge transfer supplies learner partial policy deciding actions situations user thinks skills apply advice ned discarded learner absolutely transfer advice harmful long run user guidance imperfect future work plan examine transfer methods advice lavrac dzeroski inductive logic programming ellis horwood york maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression aaai stone sutton scaling reinforcement learning robocup soccer icml torrey shavlik walker maclin advice transfer knowledge acquired reinforcement learning task ecml 
ideas linear formulation maclin found effective min ajj jbaj cjjsajj jjmjj action awa piece prefer advice miwp miwn piece absolute advice miwa adapt extenkbkr quickly easily forms advice extensional notion applied binary classification problems extenkbsvm formulation min jbj cjjsjj jmijjjmjj piece advice formulation piece advice predicted class compare efficiencies method approach approach size resulting optimization problem kbkr piece advice introduces variable data point thez terms extra variable term total number training examples extenkbkr piece advice introduces variable matches advice refer number examples match advice item general expect addition limit size random subset examples match rule preconditions robocup experiments restricted terms non-zero terms constraint matrix unlabeled data points kbkr piece preference advice introduces order nonzero terms piece absolute advice introduces order terms extenkbkr introduces order nonzero terms piece preference advice ande piece absolute advice expect extenkbkr fewer terms note case unlabeled data points extenkbkr size basis kernel grow leave extenkbkr fewer terms sections report experiments synthetic data underlying function demonstrate extenkbkr method works expected present results difficult problem subtask soccer robocup simulator synthetic experiments order initially validate method performed number simple experiments synthetically generated data present standard support vector regression svr involves parameter relative weight data fit compared model complexity extenkbkr parameter expression relative weight fit prior knowledge kbkr parameters addition expression measure fit prior knowledge selected good parameter values running tuning-set experiments considered extenkbkr considered found wide range experiments worked optimal parameter tuning table advice synthetic dataset experiment prefer prefer prefer prefer training set size svr kbkr extenkbkr extenkbkr training set size figure top graph shows average absolute error svr kbkr extenkbkr extenkbkr pseudo examples advice item bottom graph show average time solve linear program ran fresh set experiments report results synthetic data involves input features feature random test create functions test preference advice functions selected functions highly nonlinear linear overlap significant differences functions allowing advice gaussian kernel variance table shows advice tests involve ten repeated experiments dataset sizes addition test extenkbkr regular situation provided randomly generated pseudo examples piece advice pseudo examples training data match piece advice recall extenkbkr unlabeled case arbitrary random examples sample extension advice preconditions graph average absolute error separate set test points shown figure top graph figure shows extenkbkr outperform kbkr no-advice approach small sample unlabeled pseudo examples labeled training examples top graph kbkr initially performs poorly advice perfect training set sizes small lower graph demonstrates extenkbkr efficient computationally kbkr experiments robocup soccer demonstrate effectiveness formulation experimented task robocup soccer simulator called breakaway torrey m-on-n breakaway figure objective figure sample breakaway game attackers represented light circles dark edges attempting score goalie goal shown black ball large white circle reinforcement learners called attackers score goal hand-coded defenders goalie game ends succeed opponent takes ball ball bounds time limit seconds experiments -onbreakaway attacker ball choose move ball pass teammate score goal shooting limit movement directions ahead left goal point limit shots left side side center goal attackers possession ball follow hand-coded strategy position receive pass goalie defenders follow hand-coded strategies prevent attackers scoring gain possession ball adopted state representation torrey consists distances angles players distances angles involving goal time left game learners receive reward end game score goal reward features discretized intervals called tiles boolean feature tile feature true numeric falls interval false enhancement state space robocup stone sutton adopted give linear q-function model ability represent complex functions create tiles varying width feature give advice playing games randomly chosen moves games retrain models variants q-learning algorithm sarsa estimates sutton barto ran short parameter-tuning experiments times played games settings svr kbkr extenkbkr considered extenkbkr considered kbkr considered setting parameter settings worked parameter-tuning experiments svr kbkr extenkbkr addition runs exponentially increased exponentially decreased number games increased prior knowledge important amount actual experience increases values asymptotic values table advice robocup experiments distanceto goalcenter prefer move ahead actions distanceto goalleft distanceto goalcenter distancebetween teammate goalie distanceto goalie distanceto teammate angle teammate dgoalie angle topright goalcenter prefer passto teammate actions distanceto goalcenter angle goalleft goalie angle topright goalcenter prefer shoot left actions started casymp reached half asymptotic games initial values decay rate games tune slopes exponential curves selected parameter settings process performed repeated trials starting fresh game ensure advice rule covers examples rule covers real examples sample feature space uniformly create random unlabeled pseudo-examples rule covers real examples randomly discard excess terms number pseudo examples extenkbkr adds examples examples leftmost point figure terms efficiency found extenkbkr executes approximately half cpu time table advice basically player ball move goal pass teammate player good shot shoot close good shooting angle addition advice rule designed player ball pass lower-right corner field luring goalie edge goal opening shooting opportunity teammate manually alter advice imperfect experiments figure shows probability scoring goal averaged sets games approaches extenkbkr confidence level no-advice support vector regression games based unpaired two-tailedt-test based test kbkr svr games experiment demonstrates advice-taking significantly improve reinforcement learning challenging task kbkr faster extenkbkr perform equally accurately related work discussed earlier work closely relates techniques kbkr kbsvm pref-kbkr addition number papers examined prior knowledge kernel methods schoelkopf training set size kbkr extenkbkr svr figure probability scoring goal past breakaway games standard support vector regression svr kbkr extenkbkr looked incorporating information images class invariants local translations image information local structure images epshteyn dejong proposed method incorporating prior knowledge user excitatory inhibitory conditions examples presence word medical positive connection word doctor work focuses advice expressed rules providing general platform incorporating advice schoelkopf epshteyn dejong sun dejong domain knowledge select important features examples create generalized examples addition standard examples svm approach advice guide selection important examples associate desired outputs examples muggleton developed method incorporating prior knowledge inductive logic programming support vector methods method differs approach applies regression classification method explicitly addresses imperfect prior knowledge addition number semi-supervised support vector methods developed relate work srihari developed svm method rule labeling unlabeled data provide confidence predictions labeled unlabeled data franz perform regularization solution svr problem unlabeled 
data advice rules provide informative source training approaches conclusions presented family methods incorporating advice represented rules support-vector learners regression classification methods call extenkbkr extensvm depend extensional definition advice information advice learner set sample points meeting preconditions advice algorithm simply adds linear program solved constraints advice places points contrasts kbkr kbsvm methods include advice constraints solution problem intensional manner experiments demonstrate extenkbkr performs similarly empirically kbkr synthetic dataset challenging subtask simulated robotic soccer argue formulation simpler define typically leads smaller optimization problems solved efficiently significant advantages approach adapted rule-based advice needed test datapoint matches advice future work plan broader range advice ideas respect methods advice learning mechanism acknowledgements research partially supported naval research laboratory grant darpa grant epshteyn dejong rotational prior knowledge svms ecml franz kwon rasmussen schoelkopf semi-supervised kernel regression whitened function classes dagm fung mangasarian shavlik knowledge-based svm classifiers nips fung mangasarian shavlik knowledge-based nonlinear kernel classifiers colt maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression aaai mangasarian wild nonlinear knowledge kernel approximation technical report mangasarian shavlik wild knowledge-based kernel approximation jmlr muggleton lodhi amini sternberg support vector inductive logic programming proc int conf discovery science lnai schoelkopf simard smola vapnik prior knowledge support vector kernels nips stone sutton scaling reinforcement learning robocup soccer icml sun dejong explanation-augmented svms icml sutton barto reinforcement learning introduction cambridge mit press torrey walker shavlik maclin advice transfer knowledge acquired reinforcement learning task ecml srihari incorporating prior knowledge weighted margin svms kdd 
relationship precision-recall roc curves jesse davis jdavis wisc mark goadrich richm wisc department computer sciences department biostatistics medical informatics wisconsin-madison west dayton street madison usa abstract receiver operator characteristic roc curves commonly present results binary decision problems machine learning dealing highly skewed datasets precision-recall curves give informative picture algorithm performance show deep connection exists roc space space curve dominates roc space dominates space corollary notion achievable curve properties convex hull roc space show cient algorithm computing curve finally note erences types curves signi algorithm design space incorrect linearly interpolate points algorithms optimize area roc curve guaranteed optimize area curve introduction machine learning current research shifted simply presenting accuracy results performing empirical validation algorithms true evaluating algorithms output probabilities class values provost argued simply accuracy results misleading recommended evaluating binary decision problems receiver operator characteristic roc curves show number correctly classi positive examples varies number incorrectly classi negative examples roc curves present overly optimistic view algorithm performance large skew appearing proceedings international conference machine learning pittsburgh copyright author owner class distribution drummond holte recommended cost curves address issue cost curves excellent alternative roc curves discussing scope paper precision-recall curves information retrieval manning schutze raghavan cited alternative roc curves tasks large skew class distribution bockhorst craven bunescu davis goadrich kok domingos singla domingos important erence roc space space visual representation curves curves expose erences algorithms apparent roc space sample roc curves curves shown figures curves learned models highly-skewed cancer detection dataset highlight visual erence spaces davis goal roc space upper-left-hand corner roc curves figure fairly close optimal space goal upper-right-hand corner curves figure show vast room improvement performances algorithms comparable roc space space algorithm clear advantage algorithm erence exists domain number negative examples greatly exceeds number positives examples large change number false positives lead small change false positive rate roc analysis precision hand comparing false positives true positives true negatives captures ect large number negative examples algorithm performance section nes precision recall reader unfamiliar terms important study connection bethe relationship precision-recall roc curves true positive rate false positive rate algorithm algorithm comparison roc space precision recall algorithm algorithm comparison space figure erence comparing algorithms roc space tween spaces interesting properties roc space hold space show dataset xed number positive negative examples roc curve curve algorithm points curves algorithm algorithm figure sense formally equivalent roc curves algorithm algorithm figure based equivalence roc curves show curve dominates roc space dominates space introduce space analog convex hull roc space call achievable curve show due equivalence spaces ciently compute achievable curve demonstrate space insu cient linearly interpolate points finally show algorithm optimizes area roc curve guaranteed optimize area curve review roc precision-recall binary decision problem classi labels examples positive negative decision made classi represented structure confusion matrix contingency table confusion matrix categories true positives examples correctly labeled positives false positives refer negative examples incorrectly labeled positive true negatives correspond negatives correctly labeled negative finally false negatives refer positive examples incorrectly labeled negative confusion matrix shown figure confusion matrix construct point roc space space confusion matrix metrics space figure roc space plots false positive rate fpr x-axis true positive rate tpr y-axis fpr measures fraction negative examples misclassied positive tpr measures fraction positive examples correctly labeled space plots recall x-axis precision y-axis recall tpr precision measures fraction examples classi positive positive figure nitions metric treat metrics functions act underlying confusion matrix nes point roc space space confusion matrix recall returns recall relationship roc space space roc curves typically generated evaluate performance machine learning algorithm dataset dataset xed number positive negative examples show exists deep relationship roc spaces theorem dataset positive negative examples exists one-to-one correspondence curve roc space curve space curves confusion matrices recall relationship precision-recall roc curves actual actual positive negative predicted positive predicted negative confusion matrix recall tptp precision tptp true positive rate tptp false positive rate fpfp nitions metrics figure common machine learning evaluation metrics proof note point roc space nes unique confusion matrix dataset xed space ignore worry point correspond multiple confusion matrices xed number positive negative examples entries matrix uniquely determined recall unable recover unique confusion matrix one-to-one mapping confusion matrices points space implies one-to-one mapping points ned confusion matrix roc space space translate curve roc space space vice-versa important nition theorem notion curve dominates curve meaning curves beneath equal provost theorem xed number positive negative examples curve dominates curve roc space rst dominates precision-recall space proof claim curve dominates roc space dominates space proof contradiction suppose curve curve shown figure curve dominates roc space translate curves space curve longer dominates curve dominate space exists point curve point curve identical recall lower precision words precision precision recall recall recall recall recall identical tpr tpr tpr curve dominates curve roc space fpr fpr remember total positives total negatives xed tpr tpr tpr tpatotal positives tpr tpbtotal positives tpa tpb denote remember fpr fpr fpr fpatotal negatives fpr fpbtotal negatives implies fpa fpb precision tpfp precision tpfp precision precision contradicts original assumption precision precision claim curve dominates space dominates roc space proof contradiction suppose curve curve shown figure curve dominates curve space translated roc space curve longer dominates curve dominate roc space exists point curve point curve identical tpr fpr tpr recall tpr recall recall curve dominates space precision precision remember relationship precision-recall roc curves true positive rate false positive rate curve curve case fpr fpr true positive rate false positive rate curve curve case fpr fpr figure cases claim theorem precision recall curve curve case precision precision precision recall curve curve case precision precision figure cases claim theorem recall recall recall tpatotal positives recall tpbtotal positives tpa tpb denote simply precision precision precision tptp precision tptp fpa fpb fpr fpatotal negatives fpr fpbtotal negatives implies fpr fpr contradicts original assumption fpr fpr roc space convex hull crucial idea set points roc space convex hull meet criteria linear interpolation adjacent points point lies nal curve relationship precision-recall roc curves true positive rate false positive rate convex hull roc space true positive rate false positive rate curves roc space precision recall equivalent curves space figure 
convex hull analog dominate method curve construction space note achievable curve true convex hull due non-linear interpolation linear interpolation space typically achievable pair points construct curve line segment connecting equal curve figure shows convex hull roc space detailed algorithm ciently construct convex hull cormen space exists analogous curve convex hull roc space call achievable curve achieved linear interpolation issue dominance roc space directly related convex hull analog corollary set points space exists achievable curve dominates valid curves constructed points proof convert points roc space theorem construct convex hull points roc space nition convex hull dominates curves constructed points linear interpolation points converting points roc convex hull back space yield curve dominates space shown figures theorem achievable curve exclude points beneath convex hull roc space convex hull roc space legal curve constructed set roc points researchers included argue curves preferable presented highly-skewed datasets surprising achievable curve legal curve rst computing convex hull roc space converting curve space curve space curve space important methodological issue addressed building convex hull roc space achievable curve space constructing roc curve curve algorithm outputs probability approach rst probability test set positive sort list traverse sorted list ascending order simplify discussion class refer true classi cation position array prob refer probability position positive class class prob prob create classi calling positive examples negative point roc space space represents speci classi threshold calling positive building convex hull constructing classi picks points methodologically incorrect construct convex hull achievable curve performance test data constructing convex hull combat problem convex hull constructed tuning set method candidate set thresholds tuning data build convex hull tuning data finally thresholds selected tuning data relationship precision-recall roc curves building roc curve test data test-data curve guaranteed convex hull preserves split training data testing data interpolation auc key practical issue address interpolate points space straightforward interpolate points roc space simply drawing straight line connecting points achieve level performance line ipping weighted coin decide classi ers end points represent precision-recall space interpolation complicated level recall varies precision necessarily change linearly due fact replaces denominator precision metric cases linear interpolation mistake yields overly-optimistic estimate performance corollary shows achievable curve simply converting analogous roc convex hull yields correct interpolation space curve consists nitely points practical approximate method translation expand method proposed goadrich approximate interpolation points space remember point precision-recall space generated underlying true positive tpa false positive fpa counts suppose points precision-recall space intermediate values interpolate counts tpa tpb fpa fpb negative examples takes equal positive local skew ned fpb fpatpb tpa create points tpa integer values tpb tpa tpa tpa tpb calculate linearly increasing false positives point local skew resulting intermediate precision-recall points tpa total pos tpa tpa fpa fpb fpatpb tpa suppose dataset positive examples negative examples tpa fpa tpb fpb table shows proper interpolation intermediate points local skew negatives table correct interpolation points space dataset positive negative examples rec prec precision recall correct incorrect figure ect incorrect interpolation space positive notice resulting precision interpolation linear area curve simple metric algorithm performs space bradley davis goadrich kok domingos macskassy provost singla domingos area roc curve auc-roc calculated trapezoidal areas created roc point equivalent wilcoxon-mannwhitney statistic cortes mohri including intermediate points composite trapezoidal method approximate area curve auc-pr ect incorrect interpolation auc-pr pronounced points recall precision local skew high curve figure constructed single point extended endpoints dataset positives negatives interpolating auc-pr linear connection severely overestimate auc-pr developed interpolation space give complete algorithm ndthe relationship precision-recall roc curves true positive rate false positive rate curve curve comparing auc-roc algorithms precision recall curve curve comparing auc-pr algorithms figure erence optimizing area curve space ing achievable curve convex hull roc space corollary point selected algorithm included hull confusion matrix nes point construct point space theorem finally perform correct interpolation newly created points optimizing area curve researchers investigated auc-roc inform search heuristics algorithms ferri alter decision trees auc-roc splitting criterion cortes mohri show boosting algorithm rankboost freund well-suited optimize auc-roc joachims presents generalization support vector machines optimize auc-roc ranking metrics prati flach rule selection algorithm directly create convex hull roc space yan herschtal raskutti explore ways optimize auc-roc neural networks ilp algorithms aleph srinivasan changed heuristics related roc space relation individual rule knowing convex hull roc space translated achievable curve precision-recall space leads open question algorithms optimize auc-roc optimize aucpr answer generally prove counter-example figure shows overlapping curves roc space domain positive examples negative examples curve individually convex hull auc-roc curve auc-roc curve algorithm optimizing auc-roc choosing rankings choose curve figure shows curves translated space erence drastic auc-pr curve due high ranking half positive examples auc-pr curve direct opposite choice curve made optimize auc-pr space main contribution achieving lower recall range higher precision based theorem roc curves algorithm optimizes auc-pr algorithm convex hull roc space convert curve space achievable curve score classi area achievable curve conclusions work makes important contributions dataset roc curve curve algorithm points equivalence leads surprising theorem curve dominates roc space dominates space corollary theorem show existence space analog convex hull roc space call achievable curve remarkably constructing achievable curve discards points omitted convex hull roc space ciently compute achievable curve show simple linear interpolation insu cient points space finally show relationship precision-recall roc curves algorithm optimizes area roc curve guaranteed optimize area curve acknowledgements java program calculating discussed metrics found http wisc richm programs auc gratefully acknowledge funding usa nlm grant usa air force grant tor santos costa louis oliphant advisors david page jude shavlik anonymous reviewers helpful comments suggestions bockhorst craven markov networks detecting overlapping elements sequence data neural information processing systems nips mit press bradley area roc curve evaluation machine learning algorithms pattern recognition bunescu kate marcotte mooney ramani wong comparative experiments learning information extractors proteins interactions journal arti cial intelligence medicine cormen leiserson charles rivest introduction algorithms mit press cortes mohri auc optimization error rate minimization neural information processing systems nips mit press davis burnside dutra page ramakrishnan costa shavlik view learning statistical relational learning application mammography proceeding international joint conference arti cial intelligence edinburgh scotland drummond holte explicitly representing expected cost alternative roc representation 
proceeding knowledge discovery datamining drummond holte roc curves cost curves rocai ferri flach henrandez-orallo learning decision trees area roc curve proceedings international conference machine learning morgan kaufmann freund iyer schapire singer cient boosting algorithm combining preferences proceedings international conference machine learning madison morgan kaufmann publishers san francisco goadrich oliphant shavlik learning ensembles rst-order clauses recall-precision curves case study biomedical information extraction proceedings international conference inductive logic programming ilp porto portugal herschtal raskutti optimising area roc curve gradient descent proceedings international conference machine learning york usa acm press joachims support vector method multivariate performance measures proceedings international conference machine learning acm press kok domingos learning structure markov logic networks proceedings international conference machine learning acm press macskassy provost suspicion scoring based guilt-by-association collective inference focused data access international conference intelligence analysis manning schutze foundations statistical natural language processing mit press prati flach roccer algorithm rule learning based roc analysis proceeding international joint conference arti cial intelligence edinburgh scotland provost fawcett kohavi case accuracy estimation comparing induction algorithms proceeding international conference machine learning morgan kaufmann san francisco raghavan bollmann jung critical investigation recall precision measures retrieval system performance acm trans inf syst singla domingos discriminative training markov logic networks proceedings national conference arti cial intelligene aaai aaai press srinivasan aleph manual version http web comlab oucl research areas machlearn aleph yan dodier mozer wolniewicz optimizing classi performance wilcoxonmann-whitney statistics proceedings international conference machine learning 
machine learning kluwer academic publishers boston manufactured netherlands symbolic neural learning algorithms experimental comparison jude shavlik shavlik wisc computer sciences department wisconsin west dayton street madison raymond mooney mooney utexas department computer sciences taylor hall texas austin geoffrey towell towell wisc computer sciences department wisconsin madison editor quinlan abstract fact symbolic neural network connectionist learning algorithms address problem learning classified examples comparative strengths weaknesses experiments comparing symbolic learning algorithm perception backpropagation neural learning algorithms performed large real-world data sets backpropagation performs slightly algorithms terms classification accuracy examples takes longer train experimental results suggest backpropagation work significantly data sets numerical data analyzed empirically effects amount training data imperfect training examples encoding desired outputs backpropagation occasionally outperforms systems small amounts training data slightly accurate examples noisy incompletely finally backpropagation effectively utilizes distributed output encoding keywords empirical learning connectionism neural networks inductive learning perception backpropagation introduction division symbolic neural network approaches artificial intelligence evident machine learning symbolic artificial neural network connectionist learning algorithms developed recently fisher mckusick mooney shavlik towell oove weiss kapouleas atlas dietterich hild bakiri direct comparison basic approaches machine learning fact symbolic connectionist learning systems frequently address general problem comparative strengths weaknesses problem addressed neural network symbolic learning systems inductive acquisition concepts examples problem briefly defined descriptions set examples labeled belonging class determine procedure correctly assigning examples classes neural network literature problem frequently referred supervised associative learning shavlik mooney towell associative learning symbolic neural systems generally require input classified examples represented feature vectors addition performance type learning systems evaluated testing ability correctly classify examples symbolic machine learning numerous algorithms developed perform associative learning systems exist learning decision trees quinlan logical concept definitions mitchell michalski examples classify subsequent examples algorithms tested problems ranging soybean disease diagnosis michalski chilausky classifying chess end games quinlan neural networks algorithms developed training network respond correctly set examples appropriately modifying connection weights rosenblatt rumelhart hinton williams hinton sejnowski training network classify examples neural learning algorithms tested problems ranging converting text speech sejnowski rosenberg dietterich evaluating moves backgammon tesauro sejnowski article presents results experiments comparing performance symbolic learning algorithm quinlan perceptron rosenblatt backpropagation rumelhart neural algorithms systems tested large data sets previous symbolic connectionist experiments learning times accuracies examples measured cases backpropagation classification accuracy statistically significantly perform roughly equivalently cases backpropagation longer train well-known limitations perceptron performed surprisingly performed percentage points methods accuracy learning time paper investigates additional aspects empirical learning dependence amount training data ability handle imperfect data types ability utilize distributed output encodings investigations shed light relative merits approaches inductive learning types imperfect data sets studied type random noise occurs feature values categorizations incorrectly recorded results missing feature values occurs insufficient number features describe examples output encoding issue arises symbolic learning algorithms trained learn category names direct translation approach yields local connectionist representation category output learned category represented single output unit backpropagation algorithm trained complicated output encodings information category naming based distributed encodings output compact local encodings aspects neurally-based algorithms exploit learning results experiments sections neural methods occasionally perform small amounts training data experiments backpropagation slightly accurate data sets imperfections finally results suggest backpropagation comparing symbolic neural learning takes greater advantage domain-specific distributed output encodings successfully distributed encodings section describes data sets representation algorithms implementations experiments results reported analyzed fourth section discusses general issues relationships symbolic neural approaches inductive learning general experimental setup section describes data sets compare learning systems motivates choice algorithms study briefly describes algorithms finally describes representation language encode examples data sets experiments data sets previously test symbolic learning systems test backpropagation soybean data set soybean diseases features weather time year descriptions leaves stems disease examples total examples domain popularized michalski chilausky exact data reinke noted soybean data sets exist full set confused simpler purely conjunctive four-disease data test clustering systems stepp fisher chess data set shapiro consists examples king rook versus king pawn end game kpa categories win win highlevel features total examples randomly selected original examples examples approximately evenly divided categories similar chess end game data sets previously test quinlan audiology data bareiss consist cases baylor college medicine examples categories hearing disorders involving features data set large amount missing information average features values bareiss heart disease data set medical cases cleveland clinic foundation detrano unpublished manuscript data sets numerically-valued features nominally-valued values form finite unordered set numerically-valued features categories healthy diseased heart full data set examples roughly evenly divided categories examples missing feature values experiments nettalk sejnowski rosenberg data set involves text-to-speech conversion consists word dictionary letter word phoneme stress pair training examples nettalk formed passing word time seven-letter window phoneme stress pair middle letter shavlik mooney towell window constitutes category number training examples word equal number letters word word dictionary training examples seven-letter window insufficient uniquely identify phoneme stress pair attributable central letter window instance sound letter asset assess words identical seven-letter windows letter --asse result data considered low levels noise full dictionary extensive analyze tractably manner equivalent domains small training set extracted common english words reported kuchera keeping nettalk dictionary training set called nettalk-jull simply nettalk words produce examples classified phoneme stress categories nettalk corpus categories categories represented training set constitute examples corpus standard test set nettalk-full consists words examples randomly selected remainder dictionary experiment nettalk-full data set pruned keeping examples involving sounds produces examples fall sound stress categories data set called nettalk-a learning algorithms order experimentally compare neural-net symbolic learning algorithms chose perceptron backpropagation algorithms representative algorithms section briefly describes systems reasons choosing representatives chose simple widely symbolic algorithm learning examples extensively tested number large data sets quinlan wirth catlett basis commercial rule-induction systems addition augmented techniques handling numerically-valued features noisy data missing information quinlan finally experimental comparisons symbolic learning algorithms generally performs systems rorke rendell cho seshu training data construct decision tree determining category step node added decision tree partitioning training examples based single most-informative attribute attribute chosen minimizes function comparing symbolic neural learning number values attribute kji number examples jth category ith attribute total number examples number examples ith attribute number categories resulting partition processed recursively examples single category case leaf created labeled category informationgain 
criterion determines splitting attribute acts hill-climbing heuristic minimize size resulting decision tree quinlan chi-squaredpruning handle noisy data prevent overfilling data method terminates growth branch decision tree remaining attribute improves prediction statistically significant manner case leaf created labeled common category partition missing feature values handled techniques recommended quinlan section details numerically-valued features handled examining thresholds height split data partitions examples distinct splits numerical feature split treated information-gain calculation split nominal feature quinlan perceptron perception learning procedure incapable learning concepts linearly separable minsky papert fect performs large data sets test recent learning systems included perceptron neural network learning algorithms study simple fast neural learning algorithm perceptron learning procedure method adjusting weights single linear threshold unit produces correct outputs set training examples procedure performs gradient descent hill climbing weight space attempt minimize sum squares errors training examples specifies weights changed presentation input output pair weight input feature target output pattern current output pattern api input feature pattern training data linearly separable procedure guaranteed converge correct set weights finite number input presentations minsky papert implementation perceptron experiments includes cycle detection set weights repeats system stops data linearly separable perceptron cycling theorem minsky papert guarantees eventually happen data linearly separable due simulation time restrictions perceptron stopped cycles training data times nettalk-full data set times data sets single perceptron trained category distinguish members shavlik mooney towell category categories test classified passing perceptrons assigning category perceptron output exceeds threshold largest amount backpropagation past years neural learning algorithms developed capable learning concepts linearly separable rumelhart hinton sejnowski barnard cole backpropagation called generalized delta rule rumelhart well-known procedure tested large-scale problems sejnowski rosenberg tesauro sejnowski chose backpropagation represent neural learning algorithms perceptron backpropagation gradient descent attempt minimize sum squares errors training inputs capable multi-layer networks hidden units units directly receive external input produce external output method work thresholding function unit altered differentiable common output function presented unit output opi input output pair wji weight unit unit tunable threshold bias unity weights bias unit changed presentation pattern backpropagating error measures layer layer output back input equations parameter called learning rate parameter called momentum term reduces fluctuations hill climbing tpj target output output unit pattern spj measures error output unity pattern unlike perceptron backpropagation stuck local minima guaranteed comparing symbolic neural learning converge correctness training data addition trapped oscillation momentum term helps prevent oscillations version backpropagation experiments supplied volume pdp series mcclelland rumelhart set learning rate momentum term standard values section networks output bit category networks hidden layer totally connected input output layers number hidden units total number input output units empirically found work testing assigned category output unit highest training terminates network correctly classifies training data number passes data number epochs reaches training nettalk-full data terminated epochs due time restrictions representation examples represented examples data sets bit vectors multi-valued feature vectors slight modification numerically-valued features heart-disease data input bit corresponds feature depending feature bit set bits representing single feature feature missing original data occurs audiology data bits set special processing missing features required issue representation missing feature values discussed section numerically-valued features heart-disease data set treated differently values numeric features normalized real numbers range minimum feature mapped maximum binary encodings natural representation neural learning algorithms found binary encoding slightly improved classification performance learning algorithms bit vectors usable systems helps standardize experimental setup improved performance binary encoding due factors feature values binary encoding eliminates gain criterion undesirable preference many-valued features quinlan general branching decisions red non-red requiring nodes branch values feature overcome irrelevant values problem cheng fayyad irani qian result general decision trees binary-encoded examples features run time increased approach experiments results report experiments section experiment compares learning times accuracies algorithms relative performance function shavlik mooney towell amount training data studied experiment experiment investigates performance learning algorithms presence types imperfect data final experiment investigate distributed output encodings perform experiments separated data set collection training testing sets system processes training set performance measured test set reduce statistical fluctuations nettalk-full results averaged training testing sets experiments twothirds examples category randomly training set test set reduce statistical fluctuations run backpropagation seed random number determining initial random network weights nettalk-full single training set runs backpropagation processes nettalk-full data ran backpropagation times seed random number run network performs training set classify test set initial random choice weights effect performance backpropagation tests domains nettalk local minima problems found occur long plateaus correctness remains constant rising small number training epochs nettalk data randomly chosen initial states order minimize effect starting bad state experiment learning time correctness major issues inductive learning time spent learning classification accuracy examples nettalk-full data set randomly permuted divided training test sets ten times experiment training sets processed learning algorithms terminate correctness measured test sets results figures experimental results figure reports training times systems normalized time correctness test data reported figure numbers reported means ten partitionings data sets actual numbers standard deviations statistics appendix geometric means training times soybeans chess audiology heart disease nettalk-a seconds seconds perceptron seconds backpropagation correctness geometric means perceptron backpropagation backpropagation time nettalk-full run test set correctness network performed training set comparing symbolic neural learning figure relative training times algorithms figure accuracies algorithms isolate source variation correctness results two-way analysis variance anova data anova compares variance attributable learning methods variance attributable training sets variance attributed sources analysis learning algorithm chosen significant variations correctness data set confidence conclusion ranges low audiology greater data sets difference ten training sets significant source variation soybeans greater confidence audiology confidence data sets test confirms rejects hypothesis differences training sets contribute significantly variation data addition statistical t-test paired differences compare pairs systems heart data concluded confidence backpropagation superior perceptron superior perceptron t-test results support total ordering data sets soybean data concluded confidence backpropagation perceptron shavlik mooney towell superior chess audiology nettalk-a data concluded confidence backpropagation superior perceptron pair-wise differences statistically significant discussion experiment part learning systems similar respect accurately classifying examples accuracy shortcomings occur perceptron heart disease nettalk data sets close linearly separable compared backpropagation performs poorly heart-disease data set numerically-valued features systems percentage points perceptron train fester backpropagation obvious differences decision trees neural networks ability accurately 
represent concepts correctly classify instances comparable data recent experiments supports conclusion fisher mckusick compared backpropagation natural domains cases found backpropagation percentage points accurate weiss kapouleas compared backpropagation cart breiman friedman olshen stone algorithm similar produces decision trees studied data sets cart outperformed backpropagation data sets backpropagation worked considered pvm algorithm weiss galen tadepalli produces classification rules pvm beat backpropagation domains percentage points atlas compared cart backpropagation data sets cases backpropagation accurate difference statistically significant noted previously found improve slightly examples converted binary representation studies binary representation decision-tree builders differences found reduced finally studies mention slowness backpropagation explanation similarity performance reasonable generalization procedures correctly classify set training instances possibly allowing noise equally classify instance correctly consistent recent work computational learning theory blumer ehrenfeucht haussler warmuth states roughly polynomial algorithm compresses training data polynomial learner sense valiant algorithm guaranteed compress information present training set guaranteed learn approximately correct pac concept average perform test data explanation inductive biases inherent connectionist symbolic representations algorithms reflect implicit biases real categories equally systems share form occam razor bias degree prefer simpler hypotheses standard neural network approaches complexity hypothesis constrained user initially comparing symbolic neural learning select network problem unlike symbolic learning systems explicitly search simple hypothesis neural systems simply search correct hypothesis fits user-specified network generally hill-climbing search guide learning neural systems hill climb correctness space symbolic systems hill climb simplicity space recent interesting developments involving neural learning algorithms explicitly learn simple networks eliminating unnecessary hidden units slowly adding hidden units needed hanson pratt honavar uhr ash cun denker solla systems ease burden initially network problem heart-disease data set backpropagation performed substantially accurately previously mentioned data set study numerically-valued features results suggest backpropagation works training examples numeric values conclusion corroborated study atlas data sets numeric features including accuracy superiority backpropagation cart statistically significant weiss kapouleas study involved numeric features data sets numeric values decision tree-building cart algorithm produced accurate result backpropagation detailed study required understand relative merits symbolic connectionist approaches learning presence numeric features quinlan proposed method learning soft thresholds decisions involving numerically-valued features briefly investigated soft thresholds found statistically significant differences heart-disease data recent study found substantial accuracy difference backpropagation towell shavlik noordewier investigate learning recognize biologically-interesting class dna sequences called promoters experiment involve numeric features backpropagation test set accuracy ids accuracy dna data set contained examples explanation accuracy difference backpropagation outperforms small data sets investigate issue training set size experiment compelling explanation dna task involves learning concept form predicates true fisher mckusick report backpropagation performs problems functions aspects promoter task fit format potential sites hydrogen bonds form dna protein bonds form promoter activity occur final point discussion concerns performance backpropagation nettalkfull data set data set backpropagation learns training set data sets number hidden units allowed training epochs backpropagation nettalk-full test set epochs training set performance backpropagation improved percentage point require prohibitive amount training substantially improve backpropagation performance nettalk-full shavlik mooney towell experiment effect number training examples learning methods perform small amount training data perform large training sets address issue experiment presents graphs plot correctness function training set size learning curves generated performing batch training examples training set gradually increased subsequent training sets increasing size subsume learning non-incremental carryover learning training examples learning training examples learning experiment incremental results provide upper bound performance terms correctness expected incremental algorithms fisher mckusick mooney shavlik towell provide discussion ways apply backpropagation incremental setting results figure presents correctness function amount training data data point represents average ten due simulation time restrictions randomly chosen training sets nettalk curve represents training single data set correctness expected random guessing plotted training examples soybean nettalk data backpropagation perceptron perform small amount training data backpropagation consistently outperforms systems heart-disease data set finally difference systems chess audiology data discussion results small amounts training data backpropagation choice small training sets slower speed backpropagation problem sample domains studied backpropagation results suggest small sample faster full training set backpropagation takes long run technique practical systems perceptron accuracy heart-disease data declines slightly number examples increases attributed fact perceptron terminates decides data set linearly separable perceptron performance heart-disease nettalk data improved detects inseparable data set search hyperplane separates training examples tendency perform worse small training sets related problem small disjuncts holte acker porter holte shown leaves decision tree disjuncts dnf rule cover small number training examples tend higher error rates cover large number comparing symbolic neural learning figure classification performance function amount training data examples present evidence higher error rate due maximum generality bias symbolic induction systems basic problem simplest conjunction covers small set examples frequently overgeneralization small training set build trees composed primarily small disjuncts high shavlik mooney towell error rates unlike symbolic systems backpropagation perceptron build explicit disjuncts maximum generality bias susceptible problem small disjuncts note data large number categories fewer examples category generate smaller disjuncts explain chess data classes explains ids performs backpropagation audiology data audiology categories three-fourths examples fell classes fisher mckusick suggest effective backpropagation small training sets size training set increases backpropagation eventually outperforms discrepancy results procedural difference generating learning curves fisher mckusick backpropagation incremental fashion update weights training learning curves presented figure backpropagation allowed converge training examples encountered run test set running backpropagation convergence leads performance small amounts training data experiment effect imperfect data sets important aspect inductive learning systems sensitivity imperfections data improperly represented examples occur reasons mistakes made recording feature values judging class feature values missing insufficient collection features describe examples experiments section investigate types imperfections report effect learning algorithms noisy incomplete reduced data sets produced performance learning algorithms compared classification performance learning corrupted data measured test sets types rates imperfections training data consistent work theoretical basis learning valiant assumes distribution examples training testing due processing limitations experiments reduced version nettalk-full training set -word training set words examples training data set called nettalk curves experiment result averaging performance random training test divisions random noise type imperfection investigated called random noise noise produced feature values classifications mistakenly recorded probability feature randomly changed legal feature probability classification randomly changed category replaced heart disease numeric features randomly choosing uniformly interval comparing symbolic neural learning note 
noise introduced feature level noise added data sets converted binary representation instance noise level nettalk data feature values bits change feature comprises bits change feature requires bits change values results figure presents effect amount random noise classification performance relative performance obtained feature noise classification errors introduced thin dotted lines graphs represents frequency common item training set line represents level performance obtainable guessing common category training set domain audiology outperforms systems soybeans chess backpropagation backpropagation degrade roughly rate heart-disease nettalk domains discussion backpropagation apears handle random noise slightly explanation difference backpropagation backpropagation makes decisions simultaneously weighing input features sequentially considers values input features traverses decision trees learns single noisy feature early decision tree significantly impact classification results mildly disagree fisher mckusick study comparative effect noise report backpropagation consistently outperforms results handle noise backpropagation recently alternatives chi-squared pruning proposed handling noisy data quinlan mingers generally referred postpruning methods prune tree built terminating growth construction newer methods handling noise competitive backpropagation backpropagation advantage handles noise naturally requiring special-purpose procedures missing feature values type data set imperfections occurs feature values missing presence incompletely examples complicates learning information distinguish examples absent build data sets missing feature values probability feature deleted methods handling missing values learning incomplete data requires effective method handling missing feature values techniques developed dealing unknown attribute values quinlan presents set experiments comparing approaches version method arguably results experiments evaluating potential splitting feature system adjusts information gain distributing examples unknown shavlik mooney towell figure classification performance function amount feature category noise values values frequency quinlan partitioning training set attribute training unknown attribute fraction assigned subset based frequency kononenko bratko roskar comparing symbolic neural learning classifying case unknown attribute tested branches explored results combined reflect relative probabilities outcomes quinlan experimental results confirm approach superior methods replacing unknown values common discarding examples unknown values partitioning method leads higher accuracies examples drawback processing time grows exponentially function number missing feature values approach reducing run time prevent splitting features create partition total quinlan personal communication frequently decreases run time substantial amount reduce accuracy percentage points addition run time grows exponentially number missing values intractable large data sets large amounts missing data reason technique applied nettalk data set apply nettalk data replaced unknown values common attribute classification experiments found data sets method led small degradation test set accuracy knowledge issue representing missing values neural networks investigated previously evaluated representations missing features backpropagation assume values feature inputs represent feature missing inputs nth representation reflects fact values present intermediate input represent unspecified inputs reflects priori probability input sense approach spreads inputs single feature approach extended distributing single distribution feature values training set investigate approach performance approaches averaged soybeans chess audiology data sets reported figure reason nth works techniques provide input activity features figure effect representation missing features backpropagation performance shavljk mooney towell missing nth works backpropagation remainder experiment heart disease numeric features replaced missing features tested perceptron approaches nth approach works method perceptron earlier mentioned audiology data set missing feature values features missing lost missing domain means expert producing decided feature irrelevant details case duran experiments represent type absent feature zeroes produce curves represented randomly discarded feature values techniques results figure performance learning algorithms function percentage missing features values perceptron perform poorly data sets backpropagation performs domains performs substantially worse methods soybean data perceptron performs poorly chess data surprisingly perceptron outperforms systems audiology data discussion results backpropagation handles missing feature values perceptron perceptron frequently poorly complicated representation missing values lead concepts linearly separable reason backpropagation outperforming backpropagation naturally supports representation partial evidence feature discussed analysis performance noisy data backpropagation makes decisions summing weighted evidence features locally bases decisions single feature appears missing feature values naturally matched inductive biases inherent backpropagation interesting question consistently poorly soybean data possibly high number input features domain large number categories distribution examples categories interacts poorly information gain measurement training set examples categories completely dropped features type imperfection arises insufficiently rich vocabulary represent examples features simplify classification included examples produced investigate sensitivity learning algorithms number features randomly dropping percentage features feature dropped dropped examples training set test set nettalk training sets chose random collection dropped features set nettalk clear ordering features letters dropped ends window sufficient number comparing symbolic neural learning figure classification performance function amount missing feature values features discarded odd number features discarded training sets produced case extra letter dropped front dropped back results averaged shavlik mooney towell results figure presents results experiment fraction features completely dropped backpropagation degrade roughly rate number features reduced large numbers dropped features perceptron degrades drastically figure classification performance function number features totally dropped comparing symbolic neural learning discussion backpropagation surprisingly number features reduced perceptron poor performance due training sets linearly inseparable interesting aspect figure apparent redundancies domains randomly dropping half features slightly impairs performance interesting occasions performance improves small number features dropped illustrating extra features degrade inductive learning algorithms nettalk data set performs considers threeletter window experiment effect output encoding nettalk examples neural network systems concept representations distributed collection units sejnowski rosenberg encode phoneme stress outputs nettalk-full sequence bits opposed bit categories bits distributed encoding phonemes contained dictionary bit unary feature describing position tongue mouth phoneme type vowel height punctuation remaining bits form local encoding types stresses dictionary final experiment repeat sejnowski methodology nettalk-full data set investigate merits distributed output encodings ascertain learning algorithms utilize type output representation experiment compares classification performance learning localized method output encoding distributed encoding output encoding experiment involves nettalk data set data set producers rendered distributed output encoding note distributed output vector requires information data sets methodology backpropagation algorithm easily handles distributed encoding output backpropagation tests involving distributed outputs hidden units output units training epochs sejnowski rosenberg backpropagation handles distributed output representation naturally implement representation requires building separate decision tree output bits similarly implement representation perception perceptron learned output bits categorize test string outputs formed single pass backpropagation network passes decision trees perceptrons backpropagation produces vector numbers perceptron produces vector numbers entry perceptron output minus threshold discuss section produces binary vector vector numbers category determined finding phoneme stress pair encoding smallest angle outputs 
guess metric sejnowski rosenberg shavlik mooney towell results figure compares classification performance learning systems nettalk data local output encoding distributed output encoding training table -word set classification accuracy measured -word test set previously experiment initially chi-squared version poor performance correctness training set correctness test set investigated hypothesis producing binary vector making guess led loss information knowing amount evidence output advantageous answer question applied buntine method giving probabilistic interpretation outputs input guess routine probabilities produced method results training set correctness test set correctness turned chi-squared pruning fitted data closely correctness training set test set buntine classification method trees built chi-squared pruning produced results configuration training set accuracy test set accuracy figure statistics refer final method discussion distributed encoding output substantially improves performance backpropagation distributed encoding backpropagation correctness training set testing set local encoding output bit category results backpropagation correctness percent training set testing set successfully distributed encoding performance enconding styles roughly equivalent test-set accuracy distributed encoding backpropagation performs substantially nettalk task dietterich report similar results comparison backpropagation nettalk data figure classification correctness nettalk data set output encoding methods comparing symbolic neural learning perceptron similar improvement performance degrades occurs individual concepts easier learn concepts local encoding perception converged bits distributed encoding converged cost distributed encoding backpropagation range correctness varies widely training data imperfect nettalk data corrupted noise backpropagation correctness runs ranged varied seed random number noise accuracy runs ranged problem worse increased noise level data sets nettalk local output encoding variation classification performance randomly-initialized networks distributed encoding introduce problems local minima absent local encoding dietterich performed extensive series tests attempt isolate reasons superiority backpropagation distributed output encoding considered hypotheses individually jointly account differences backpropagation experiments hypotheses account observed differences backpropagation overfits training data backpropagation ability share intermediate conclusions hidden units factor concluded primary difference backpropagation numerical parameters continuous output values capture statistical information missed propose block decoding scheme substantially reduces accuracy difference backpropagation general discussion study investigates relative performance symbolic neural approaches learning experiment shows backpropagation performs slightly algorithms terms classification correctness examples takes longer train results heart-disease data set suggest backpropagation perform substantially numerically-valued features result experiment neural algorithms tend perform accurately small amounts training data experiment investigates handling imperfect data sets backpropagation handles noisy missing feature values slightly perception backpropagation perform presence reduced numbers features fourth experiment backpropagation utilize distributed output encoding advantage representation style discuss additional aspects experiments section symbolic subsymbolic distinction supposed difference traditional symbolic neural-net learning systems operate distinct subsymbolic level smolensky experiments shavlik mooney towell reported previous section demonstrate distinction misleading fodor pylyshyn langley discuss issue symbolic systems subsymbolic systems backpropagation essentially solving learning problem applied problems typically test set input vectors produce classification function accept input features grain size ability microfeatures unique neural nets demonstrated section distributed representations current neural-net methods backpropagation greater advantage distributed representations current symbolic systems important note assuming initial inputs final outputs finite-valued concept representations decision trees disjunctive normal form dnf expressions multi-layer neural nets equivalent terms representational power formalisms represent functions binary inputs well-known property dnf expressions two-layer and-or networks muroga dnf expression easily converted equivalent decision tree weights linear threshold unit set model gates assuming sufficient number hidden units dnf expression converted equivalent neural net neural net implicitly represents logical rule functions decision trees dnf formulae simply explicit representations rules difference symbolic connectionist systems lies inherent inductive biases determine logical rules consistent set data chosen concept definition symbolic learning systems prefer syntactically simple rules functions easily learned neural nets correspond complex dnf formulae decision trees functions output inputs surprisingly experiments artificial data show backpropagation performs problems fit natural bias ofn functions fisher mckusick current experiments inductive biases symbolic neural net systems equally suitable real world problems perceptron performance problem difficulty surprising result experiments simple perceptron algorithm performs perceptron largely abandoned general learning mechanism twenty years ago inherent limitations inability learn concepts linearly separable minsky papert performs experiments heart-disease nettalk data sets presence imperfect training data accuracy perceptron distinguishable complicated learning algorithms addition efficient perceptron training time comparable results presence large amount regularity training sets chosen representative data previously test symbolic learning systems categories present soybean audiology data linearly separable comparing symbolic neural learning ten randomly chosen training sets categories chess data linearly separable ten training sets linearly separable rest average correctness training sets fact data sets large represent real categories regularity makes easy simple learning algorithms perception explanation regularity data reflecting regularity real-world categories words members real category naturally great deal common easy distinguish examples categories explanation features present data carefully engineered reflect important differences categories formulating features chess end games learning required considerable human effort quinlan difficult problems studied order determine true power current inductive learning systems nettalk data set difficult studied researchers machine learning beginning benchmark dietterich reason data number real problems consist linearly separable categories perceptron simple efficient learning algorithm case perceptron initial test system good idea set categories linearly separable perceptron cycling theorem minsky papert gurarantees algorithm eventually repeat set weights terminated case complicated algorithm backpropagation perceptron tree error correction procedure utgoff hybrid approach algorithm perceptron learning procedure fails splits data subsets information-theoretic measure applies recursively subset slowness backpropagation backpropagation performs systems classifying examples consistently takes lot time train averaged data sets experiment backpropagation takes times long train testing backpropagation takes times longer systems factors increase optimized common lisp versions perceptron coded efficiently compared version backpropagation researchers developing faster methods neural network training fahlman barnard cole fahlman quickprop algorithm ran order magnitude faster backpropagation sample task fahlman report algorithm performs relative backpropation terms accuracy examples obvious backpropagation made fester exploit intrinsic parallelism networks experiments contained average units assuming processor unit perfect speedup training time backpropagation possibly made competitive recursive shavlik mooney towell divide-and-conquer algorithm great deal intrinsic parallelism addition perceptron output bit learned independently simple source parallelism method comparing training time parallel implementations algorithms fair address issue scalability computational complexity learning algorithm important theoretical practical issue issue generally important parallelism fixed number processors decrease run time constant fector increasing size problem increase run time arbitrary amount depending computational complexity algorithm learning time grows linearly number 
examples worst case quadratically number features schlimmer fisher utgoff learning times dropped features experiment section support hypothesis average learning time grows linearly number features scale speculations neural networks scale poorly minsky papert recent theoretical work shows neural network learning np-complete blum rivest judd empirical evidence average backpropagation time reach asymptotic performance grows cube number weights hinton evidenced nettalk-full data backpropagation produce impressive results training terminated training set fully learned incremental learning issue investigate learning incrementally incremental versions proposed schlimmer fisher utgoff backpropagation performed processing number times discarding comparison incremental approaches area future research fisher issues differences learning algorithms symbolic connectionist approaches general collection input output training pairs directly run perceptron hand order run backpropagation network architecture chosen art science choose number hidden units number hidden layers addition choose settings learning rate momentum term performance depend greatly initial randomlyselected weights runs initial weights good final result finally criterion stopping training chosen parameters comparing symbolic neural learning inappropriately set initial weights unfavorable backpropagation fail converge efficiently noted symbolic learning systems parameters appropriately set insure good performance rendell issue human interpretability acquired rules symbolic learning produce interpretable rules networks weights harder interpret large decision trees difficult interpret shapiro learned classifiers implemented unreliable components sensitivity component failure concern neural models sum partial evidence robust presence component mure mcclelland sensitivity component failure seldom addressed symbolic systems finally connectionist models intended neurallyplausible models symbolic models connectionist models shed light neurophysiology future research issues experiments suggest number issues future research mentioned interesting research topics comparing parallel implementations comparing incremental learning approaches understanding poorly soybean data investigating hypothesis backpropagation addresses problem small disjuncts holte remainder section describes additional topics involving empirical theoretical psychological comparison symbolic connectionist learning obvious extension comparison additional algorithms domains connectionist rule-based algorithms instance-based genetic probabilistic algorithms included experiments give additional insight algorithms suitable types problems interest detailed experiments isolate algorithmic differences backpropagation observed performance differences inductive learning algorithm sort inductive bias interest biases work real-world learning tasks mentioned section dietterich performed comparative analysis nettalk data represented distributed encoding similar analysis numerical data noisy data data missing values small data sets lead additional insights development learning algorithms types data relative performance backpropagation presence numerical data interest results experiments atlas weiss kapouleas provide clear answer question effect parameter settings relative performance algorithms backpropagation area research manually tuning parameters fit set data give algorithm unfair advantage carefully tune backpropagation parameters data set adopted uniform parametersetting policy initial experimentation improved policies setting parameters result performance shavlik mooney towell theoretical analysis relative performance approaches concept learning warranted relevant result provide formal characterization classes concepts types algorithms probabilistically guaranteed learn high accuracy examples theoretical work focused classes learnable unlearnable polynomial time detailed analysis relative sample complexity practical algorithms finally paper focuses evaluating algorithms based training time predictive accuracy psychological plausibility interesting evaluation metric pazzani dyer found backpropagation adequately model human data learning simple conjunctive disjunctive concepts investigation needed relative ability symbolic neural learning algorithms model aspects human learning conclusion current controversy relative merits symbolic neural network approaches artificial intelligence symbolic connectionist learning systems address task inductively acquiring concepts classified examples comparative performance adequately investigated article performance symbolic learning system connectionist learning systems perceptron backpropagation compared real-world data sets data sets previous experiments symbolic learning research soybeans chess audiology heart-disease neural network research nettalk contributions reported experiments experimental results perceptron run significantly faster backpropagation learning classification examples perceptron heart-disease data perceptron nettalk data probability correctly classifying examples systems backpropagation performance heart-disease data set suggests outperforms data sets numerically-valued features conclusion supported experiments atlas contradicted results weiss kapouleas empirical evidence small amounts training data backpropagation occasionally learns accurate classifiers backpropagation slowness concern small data sets additional experiments sample data sets carefully corrupted suggest noisy data sets backpropagation slightly outperforms domain audiology robust noise examples incompletely backpropagation performs slightly approaches technique representing missing feature values neural networks proposed shown effective backpropagation equally sensitive reductions number features express examples surprisingly half features randomly dropped substantially impacting classification accuracy comparing symbolic neural learning claimed advantage neural approaches learning profitably distributed encodings empirical study nettalk data set suggests backpropagation advantage domain-specific distributed output encodings systems successfully distributed output encodings conclusion study beginnings understanding relative strengths weaknesses symbolic neural approaches machine learning hope results future learning algorithms evaluated acknowledgments comments suggestions editor ross quinlan anonymous reviewer gratefully acknowledged authors people supplying data sets bob stepp bob reinke soybean data ray bareiss bruce porter craig wier audiology data collected professor james jerger baylor college medicine robert detrano cleveland clinic foundation heart disease data set obtained california irvine repository machine learning data bases managed david aha rob holte peter clark alen shapiro chess data terry sejnowski nettalk data alan gove ran preliminary experiments assisted converting data sets common format elizabeth towell assisted analysis variance rita duran wan yik lee richard maclin contributed implementations condor system litzkow livny mutka runs unix jobs idle workstations provided large number cycles needed experiments equivalent suna cpu years provided condor research partially supported wisconsin graduate school texas austin department computer sciences artificial intelligence laboratory office naval research grant -jto shavlik nasaames grant ncc- mooney paper results extend supercede previously reported eleventh international conference artificial intelligence wisconsin technical report notes numbers input output bits domain soybeans chess audiology heart disease nettalk exception experiment training set section classifications examples randomly corrupted classifications test sets accuracies learning algorithms measured corrupted appendix table arithmetic means standard deviations results produced ten data sets domain section perceptron number epochs refers shavlik mooney towell number cycles category nettalk-full involves training set standard deviations reported statistically significantly test set accuracies reported pair-wise comparisons test set accuracies statistically significant table results experiment means standard deviations domain system soybeans perception backprop training testing correctness time sec epochs time sec training set test set t-test test set accuracy confidence backpropagation perception superior soybeans chess perception backprop t-test confidence backpropagation superior perception chess audiology perception backprop t-test confidence backpropagation superior perception audiology heart diseas perception backprop t-test confidence backpropagation superior perception superior perception heart disease nettalk-a perception backprop t-test confidence backpropagation superior perception nettalk-a nettalk-full perceptron 
backprop comparing symbolic neural learning ash dynamic node creation backpropagation networks technical report icssan diego california institute cognitive science atlas cole connor el-sharkawi marks muthusamy barnard performance comparisons backpropagation networks classification trees real-world applications advances neural information processing systems vol denver bareiss exemplar-based knowledge acquisition unified approach concept representation classification leaming boston academic press barnard cole neural-net training program based conjugate-gradient optimization technical report cse beaverton oregon graduate institute blum rivest training -node neural network np-complete proceedings workshop computational learning theory cambridge blunter ehrenfeucht haussler warmuth occam razor information processing letters breiman friedman olshen stone classification regression trees monterey wadsworth brooks buntine decision tree induction systems bayesian analysis kanal levitt lemmer eds uncertainty artificial intelligence vol amsterdam north-holland cheng fayyad irani qian improved decision trees generalized version proceeding international conference machine learning ann arbor detrano unpublished manuscript international application probability algorithm diagnosis coronary artery disease medical center long beach dietterich hild bakiri comparative study backpropagation english text-to-speech mapping proceedings seventh international conference machine learning austin duran concept learning incomplete datasets technical report austin texas department computer sciences fahlman faster learning variations back-propagation empirical study proceedings connections models summer school san mateo morgan kaufmann fisher knowledge acquisition incremental conceptual clustering thesis department information computer science california irvine technical report fisher mckusick empirical comparison back-propagation proceedings eleventh international joint conference artificial intelligence detroit fisher mckusick mooney shavlik towell processing issues comparison symbolic connectionist learning systems proceedings sixth international machine learning workshop ithaca fodor pylyshyn connectionism cognitive architecture critical analysis pinker mehler eds connections symbols cambridge mit press hanson pratt comparing biases minimal network construction back-propagation advances neural information processing systems vol denver hinton connectionist learning procedures artificial intelligence hinton sejnowski learning relearning boltzmann machines ruraelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume foundations cambridge mit press holte acker porter concept learning problem small disjuncts proceedings eleventh international joint conference artificial intelligence detroit honavar uhr network neuron-like units learns perceive generation reweighting links proceedings connectionist models summer school san mateo morgan kaufmann judd complexity loading shallow neural networks journal complexity kononenko bratko roskar experiments automatic learning medical diagnostic rules technical report ljubljana yugoslavia jozef stefan institute shavlik mooney towell kuchera francis computational analysis modem-day american english providence brown press langley editorial unified science machine learning machine learning cun denker solla optimal brain damage advances neural information processing systems vol denver litzkow livny mutka condor hunter idle workstations proceedings eighth international conference distributed computing systems mcclelland resource requirements standard programmable nets rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume foundations cambridge mit press mcclelland rumelhart explorations parallel distributed processing handbook models programs exercises cambridge mit press michalski theory methodology inductive learning artificial intelligence michalski chilausky learning told learning examples experimental comparison methods knowledge acquisition context developing expert system soybean disease diagnosis policy analysis information systems mingers empirical comparison pruning methods decision tree induction machine learning minsky papert perceptrons expanded edition cambridge mit press original edition published mitchell generalization search artificial intelligence mooney shavlik towell gove experimental comparison symbolic connectionist learning algorithms proceedings eleventh international joint conference artificial intelligence detroit muroga logic design switching theory york wiley rorke comparative study inductive learning systems chess endgame test problem technical report uiucdcs-f- urbana illinois department computer science pazzani dyer comparison concept identification human learning network learning generalized delta rule proceedings tenth international joint conference artificial intelligence milan italy quintan learning efficient classification procedures application chess end games michalski carbonell mitchell eds machine learning artificial intelligence approach vol palo alto tioga quinlan induction decision trees machine learning quinlan effect noise concept learning michalski carbonell mitchell eds machine learning artificial intelligence approach vol san mateo morgan kaufmann quinlan decision trees probabilistic classifiers proceedings fourth international machine learning workshop irvine quinlan simplifying decision trees international journal man-machine studies quinlan unknown attribute values induction proceedings sixth international machine learning workshop ithaca reinke knowledge acquisition refinement tools advise meta-expert system master thesis department computer science illinois urbana rendell cho seshu improving design similarity-based rule-learning systems international journal expert systems rosenblatt principles neurodynamics perceptrons theory brain mechanisms york spartan rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume foundations cambridge mit press schlimmer fisher case study incremental concept induction proceedings national conference artificial intelligence philadelphia comparing symbolic neural learning sejnowski rosenberg parallel networks learn pronounce english text complex systems shapiro structured induction expert systems reading addison wesley smolensky proper treatment connectionism behavioral brain sciences stepp conjunctive conceptual clustering methodology experimentation thesis department computer science illinois urbana tesauro sejnowski parallel network learns play backgammon artificial intelligence towell shavlik noordewier refinement approximately correct domain theories knowledge-based neural networks proceedings eighth national conference artificial intelligence boston utgoff perceptron trees case study hybrid concept representations proceedings national conference artificial intelligence paul utgoff incremental induction decision trees machine learning valiant theory learnable communications acm weiss galen tedepalli optimizing predictive diagnostic decision rules proceeding national conference artificial intelligence seattle weiss kapouleas empirical comparison pattern recognition neural nets machine learning classification methods proceedings eleventh international joint conference artificial intelligence detroit wirth catlett experiments costs benefits windowing proceedings international machine learning conference ann arbor 
advice transfer knowledge acquired reinforcement learning task lisa torrey trevor walker jude shavlik richard maclin wisconsin madison usa fltorrey twalker shavlikg wisc minnesota duluth usa rmaclin umn abstract present method transferring knowledge learned task related task problem solvers employ reinforcement learning acquire model task transform learned model advice task human teacher mapping task task guide knowledge transfer advice incorporated problem solver knowledge-based support vector regression method previously developed advice-taking approach problem solver discard transferred knowledge based subsequent experiences empirically demonstrate ectiveness approach games robocup soccer simulator keepaway breakaway results demonstrate problem solver learning play breakaway advice extracted keepaway outperforms problem solver learning bene advice introduction propose method transfer knowledge gained reinforcement learning task related task complex domains robocup soccer divided related learnable tasks transfer process knowledge acquired task improve learning related task skill keeping soccer ball opponents evade players defending goal making easier learn task goal scoring work present method performing transfer advice advice taking incorporate user guidance signi cantly improve performance complex domains previous work advice user observes learner performing task advice actions prefer situations contrast method transfer obtains action preferences automatically model learned previous task user mapping connects tasks allowing transfer advice applied task reasons advice accomplish knowledge transfer supplies learner prior knowledge relative merits appears proceedings sixteenth european conference machine learning ecml porto portugal actions situations skills apply user familiar learning algorithm guide knowledge transfer simply similarities tasks advice ned discarded learner contradicted experience transfer advice harmful long run user guidance imperfect robocup simulated soccer domain extract transfer advice task called keepaway apply task called breakaway keepaway game originally introduced stone sutton game call breakaway subtask shooting goals games set eld teams players actions controlling soccer ball games erent eld layouts erent objectives provide mapping advice points similarities exist algorithm produces transfer advice captures reinforcement learner knowledge keepaway give advice problem solver keepaway skills apply breakaway section describes robocup domain perspective section detail implementation incorporates advice introduce transfer advice algorithm demonstrate potential initial results reinforcement learning robocup reinforcement learning continual learning process agent navigates environment earn rewards environment state represented set features agent executes actions state change typically agent learns q-function estimates long-term sum rewards receive starting speci action current state agent policy procedure choosing actions action highest q-value current state taking action receiving reward agent updates estimate q-value improve q-function learning task m-on-n keepaway figure objective reinforcement learners called keepers ball hand-coded players called takers game ends opponent takes ball ball bounds keeper make action choice possession ball choose hold ball pass closest teammate pass furthest teammate movement options keepaway state representation designed stone sutton consists features capture simple geometric properties learner eld perspective distances players angles formed trios players describe relative distances learner absolute locations implementation learners share single function learned combination experiences receive reward time step team ball appears proceedings sixteenth european conference machine learning ecml porto portugal keepaway breakaway fig samples -onkeepaway breakaway games ball white circle held keeper left light circle dark border attacker opponents dark circles light borders takers left defender goalie m-on-n breakaway figure objective reinforcement learners called attackers score goal hand-coded defenders hand-coded goalie game ends succeed opponent takes ball ball bounds time limit seconds attacker possession ball learnable action choice move ball pass ball closest furthest teammate shoot ball goal limit movement choices forward center goal goal clockwise counterclockwise circle centered goal shoot action directs ball center side left side goal whichever blocked goalie breakaway state representation consists features measuring important distances angles similar keepaway features features include distances angles involving goal time left game learners share single model receive reward goal failed shot game endings method learners approximate q-function solving linear optimization problem stone sutton approach tile coding include non-linear features problem model express complex functions tile coding discretizes numeric feature overlapping tilings set discrete tiles tile represented boolean feature true numeric falls tile interval false process add boolean features state space numeric feature found stone sutton addition state space signi cantly improves learning robocup games trivial soccer simulator incorporates noise players sensors actions breakaway cult positive reward rarely received chance goalie easily block random shots learners breakaway actions choose larger state space navigate appears proceedings sixteenth european conference machine learning ecml porto portugal background support vector regression learners employ type called sarsa one-step look-ahead estimate q-values implementation support vector regression svr linear optimization method proposed mangasarian extended maclin compute model approximates q-function train learner batches recent model play games updates model training examples q-function approximation main structure learned model weight vector weight feature feature vector action weight vector set term expected q-value taking action state learners action scores highest probability sub-optimal exploratory action probability typically small number compute weight vector action subset training examples action place feature vectors rows data matrix previous model actual rewards received training steps compute q-value estimates place output vector optimal weight vector denotes vector omit simplicity practice prefer non-zero weights important features order model simple avoid tting training examples introduce slack variables inaccuracies penalty parameter trading inaccuracies complexity solution resulting minimization problem min jjwjj jbj cjjsjj denotes absolute denotes sum absolute values penalty set term solving problem produce weight vector action compromises accuracy simplicity mangasarian knowledge based kernel regression kbkr method advice form rule single action rule creates constraints problem solution addition constraints training data recently introduced extension kbkr called preference-kbkr advice pairs actions form read current state satis q-value preferred action exceed non-preferred action appears proceedings sixteenth european conference machine learning ecml porto portugal giving advice shooting moving ahead distance goal vector row column distance goal feature zeros vector set small positive number allowed inaccuracy training examples advice partially introduce slack variables penalty parameters trading impact advice solution impact training examples minimization problem addresses actions apply constraints relative values multiple pieces preference advice incorporated cplex commercial software program solve resulting linear program min jjwajj jbaj cjjsajj jjzijj action aawa piece advice bti transfer advice order transfer knowledge gained task related task automatically extract advice tells learner prefer actions situations based experience task transfer advice incorporated learning procedure task explained previous 
section figure summarizes process suppose learned keepaway taker passing nearest teammate action algorithm transfer knowledge breakaway generating advice defender passing nearest teammate action kind advice obvious transfer knowledge alternative translate actions features task actions features task apply function directly task hoping provide good starting point learning task erent reward structure estimates uninformative simply transferring q-function keepaway give breakaway learner inaccurate initial estimates appears proceedings sixteenth european conference machine learning ecml porto portugal learned functions task transferred functions task user-provided advice relating features actions task task advice task action preferred action learned functions task interactions task advice solving task optional fig transferring knowledge advice advice learning task transferring q-values method transfers partial policy covers regions feature space telling learner prefer actions regions give relative constraints q-values absolutely approach robust erences tasks reward structures input method requires human teacher mapping translates features actions task features actions task map keepaway features involving nearest taker breakaway features involving nearest defender keepaway action holdball breakaway action moveahead mapping algorithm evaluates state perspective task action situation transfer advice recommends taking action task table general form transfer advice algorithm table simple concrete note direct translation learned model advice represent models advice linear expressions features complications basic procedure feature logical analogue task cases user map feature constant feature takers -onkeepaway defenders -onbreakaway goalie behaves erently defender user set features describing distances takers maximum implying nonexistent defenders ect table basic algorithm create transfer advice set constant experiments table details learned model task mapping task task actions taska generate advice actions taska prefer task appears proceedings sixteenth european conference machine learning ecml porto portugal table simple demonstration extracting transfer advice actions task actions task learned model task set linear q-value expressions weights features translated advice task features task model advice format prefer user-provided mapping full advice expression translated expressions prefer learner actions feature describing player distance center keepaway eld user set feature average range constant mappings experiments reported handle feature task logical analogue task simply leave feature mapping create advice rule action analogue indicating analogue choice based experience task actions moveleft breakaway learned independently agent handle actions analogues simply leave action mapping added tile features tile encoding numeric features map tiles keepaway feature tiles breakaway feature automatically map tiles maximize amount breakaway feature range share method require mapped features identical ranges severely restrict mapping remapping capability learner apply knowledge multiple ways attacker keepaway skills evade defenders breakaway eld cleverness skills shoot suppose learner imagines teammate standing inside goal decision pass teammate corresponds decision shoot mapping features involving teammate features involving goal mapping prevent learner passing real teammate learner actions create sets advice rst mapping included remapping experiments situation-dependent mappings knowledge erently erent areas feature space map keepaway action breakaway action shoot learner close goal soccer players shoot appears proceedings sixteenth european conference machine learning ecml porto portugal short distances providing mapping applies learner goal erent applies learner goal situation-dependent mapping experiments expected keepaway passing skills prevent attackers shooting robocup transfer advice robocup tasks explore signi erences make transfer non-trivial problem keepaway learners make game long breakaway end game quickly scoring goal learners keepaway choose move learners breakaway keepaway takers move ball breakaway goalie erent numbers players similarities tasks features map directly actions identical conceptual level breakaway attackers play keepaway score designed default mappings shown table advantage similarities transfer -onkeepaway -onbreakaway breakaway features actions keepaway analogues tables keepaway features breakaway analogues mapped constants ranges remapping capability apply keepaway skills shooting remappings table advise learner imagine nearest teammate standing rst left side goal side pass teammate action advice recommend shooting breakaway feature describe distance goalie goal section constant place high-performing -onkeepaway model trained advice applied mappings transfer advice algorithm create advice -onbreakaway process produced advice items capturing keepaway skills original actions remapped pass actions form advice shows apply keepaway hold ball skill breakaway holdball passfar holdball passnear remap holdball passnear holdball passnear remap prefer moveahead passnear shoot essentially advice holding ball safer passing teammate goal moving forward ball safer passing shooting appears proceedings sixteenth european conference machine learning ecml porto portugal table default action feature mappings keepaway breakaway refer teammates distances learner learner keepers takers attackers keepaway breakaway passnear passnear holdball moveahead dist distance player distances max range min angle mid range min angle mid range distances eld center mid range table action feature remappings apply keepaway skills shooting remapping passnear shoot keepaway breakaway dist dist goal left min dist mid range min angle angle goal left goalie remapping passnear shoot keepaway breakaway dist dist goal min dist mid range min angle angle goal goalie empirical results linear program presented equation parameters rst set tuning keepaway games advice led set gamesplayed features exponentially increase progresses estimates nite sums accurate chose running tuning games transfer advice selecting decay initial values gamesplayed expect transfer advice valuable amount experience domain increases small number settings parameter choosing parameter values trained fresh set games report results nal set games figure shows results transfer experiments training runs transfer advice compared curves generated batch training games averaging erent runs data point shown smoothed previous games results rst games included averages rst point appears proceedings sixteenth european conference machine learning ecml porto portugal games played transfer advice advice games played fig performance function breakaway games played erent metrics learner transfer advice compared learner advice axis games played learning place rst games curve left shows average reinforcement game learners earn train quantity learners attempt maximize curve shows intuitive measure performance probability learners score goal function number games played results show transfer advice reinforcement learner modest advantage learning score goals advice slightly detrimental rst learners ned games began improve performance end led higher asymptotic result obtained qualitively similar results transferring -onbreakaway task probability scoring goal related work number researchers explored methods providing advice learning algorithms clouse utgo human observer step advise learner speci action lin replays teacher sequences bias learner teacher performance gordon subramanian accept advice form condition achieve goals genetic algorithms adjust respect data maclin shavlik developed if-then advice language incorporated rules neural network adjustment price boutilier designed method reinforcement learners 
imitate expert agents domain andre russell describe language creating learning agents policies appears proceedings sixteenth european conference machine learning ecml porto portugal constrained user commands laud dejong reinforcements shape learner kuhlmann developed rule-based advice system increases q-values xed amount recent work developed preference-kbkr method advice speci form action preferences current work ers previous advice-taking methods extracts advice model learned task employing user-designed advice related work deals knowledge transfer machine learning early research focuses learning simpler version task applying knowledge cult version task selfridge call directed training robotics singh addresses transfer knowledge sequential decision tasks agent track action subsequences tasks thrun mitchell study transfer problems lifelong learning framework related boolean classi cation tasks taylor stone investigated copying q-functions transfer keepaway games erent team sizes sherstov stone investigated action transfer transfer improve learning tasks large action spaces conclusions future work presented technique extracting knowledge gained task automatically transferring related task improve learning experiments demonstrate cult breakaway task robocup soccer learned ectively advice transferred related keepaway task key idea view models learned task source advice task represent learned models advice linear expressions features user match features actions task advice-taking systems robust imperfections advice receive user guidance approximate future work plan evaluate sensitivity algorithm errors omissions user mapping advice investigate ways automate process helping user design mapping begun adapt transfer advice process work non-linear models learning task arises domain human experts provide information task relates tasks learning algorithms exploit information extract knowledge tasks transfer advice shows potential effective intuitive increase human ability interact productively reinforcement learners interaction important scaling large problems appears proceedings sixteenth european conference machine learning ecml porto portugal acknowledgements research partially supported darpa grant united states naval research laboratory grant michael ferris assistance improving ciency linear programs breakaway code ftp ftp wisc machine-learning shavlikgroup robocup breakaway andre russell programmable reinforcement learning agents nips clouse utgo teaching method reinforcement learning proc icml gordon subramanian multistrategy learning scheme agent knowledge acquisition informatica kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer aaai workshop supervisory control learning adaptive systems laud dejong reinforcement learning shaping encouraging intended behaviors icml lin self-improving reactive agents based reinforcement learning planning teaching machine learning maclin shavlik creating advice-taking reinforcement learners machine learning maclin shavlik torrey walker knowledge-based support vector regression reinforcement learning ijcai workshop reasoning representation learning computer games maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression aaai mangasarian shavlik wild knowledge-based kernel approximation jmlr noda matsubara hiraki frank soccer server tool research multiagent systems applied arti cial intelligence price boutilier implicit imitation multiagent reinforcement learning icml selfridge sutton barto training tracking robotics ijcai sherstov stone improving action selection mdp knowledge transfer aaai singh transfer learning composing solutions elemental sequential tasks machine learning stone sutton scaling reinforcement learning robocup soccer icml sutton barto reinforcement learning introduction mit press cambridge taylor stone behavior transfer value-function-based reinforcement learning int joint conf autonomous agents multiagent sys thrun mitchell learning thing ijcai appears proceedings sixteenth european conference machine learning ecml porto portugal 
view learning statistical relational learning application mammography jesse davis elizabeth burnside dutra david page raghu ramakrishnan tor santos costa jude shavlik wisconsin madison west dayton madison usa jdavis wisc abstract statistical relational learning srl constructs probabilistic models relational databases key capability srl learning arcs bayes net sense connecting entries rows relational table tables srl approaches constrained existing database schema database applications users find profitable define alternative views database effect defining fields tables fields tables highly learning provide srl capability learning views introduction statistical relational learning srl focuses algorithms learning statistical models relational databases srl advances bayesian network learning related techniques handling domains multiple tables representing relationships rows table integrating data distinct databases srl techniques learn joint probability distributions fields relational database multiple tables constrained tables fields database modification human users relational databases find beneficial define alternative views database fields tables computed existing paper shows srl algorithms benefit ability define views accurate prediction important fields original database augment srl algorithms adding ability learn fields intensionally defined terms existing fields intensional background knowledge database terminology fields constitute learned view database inductive logic programming ilp learn rules intensionally define fields test approach specific application creating expert system mammography chose application number reasons important practical application sizable data access lucent centered disease tubular density amorphous dystrophic mass margins round punctate mass size mass mass stability milk calcium dermal popcorn fine linear eggshell pleomorphic rod-like skin lesion architectural distortion mass shape mass density breast density asymmetric density hrt age family figure expert bayes net expert developed system base evaluate work large proportion examples negative distribution skew found multi-relational applications data consists single table compare techniques standard propositional learning case sufficient view learning extend existing table fields clear applications approach yield additional tables view learning mammography offering breast cancer screening ever-increasing number women age represents great challenge costeffective delivery mammography screening depends consistent balance high sensitivity high specificity recent articles demonstrate subspecialist expert mammographers achieve balance perform significantly general radiologists general radiologists higher false positive rates biopsy rates diminishing positive predictive mammography fact specially trained mammographers detect breast cancer accurately longstanding shortage individuals expert system mammography potential general radiologist approach effectiveness subspecialty expert minimizing false negative false positive results bayesian networks bns probabilistic graphical models applied task breast cancer diagnosis mammography data bns produce diagnoses probabilities attached graphical nature comprehensible humans training figure shows structure bayesian network developed subspecialist expert mammographer variable node graph bayes net conditional probability table giving probability distribution values variable setting parents bayesian network figure achieves accuracies higher systems general radiologists perform mammograms commensurate performance radiologists specialize mammography figure shows main table fields omitted brevity large relational database mammography abnormalities data collected national mammography database nmd standard established american college radiology nmd designed standardize data collection mammography practices widely quality assurance figure presents hierarchy types learning task level level standard types bayesian network learning level simply learning parameters expert-supplied network structure level involves learning actual structure network addition parameters notice predict probability malignancy abnormality bayes net record abnormality data rows table relevant radiologists abnormalities mammogram previous mammograms mammogram abnormality size shape person previous mammogram characteristics incorporating data rows table existing bayesian network learning algorithms requires statistical relational learning srl techniques probabilistic relational models level figure shows state-of-the-art srl techniques illustrating relevant fields rows tables incorporated network aggregation size abnormality consideration aggregate field bayes net average size abnormalities found mammogram presently srl limited original view database original tables fields possibly aggregation utility aggregation simply existing fields insufficient accurate prediction malignancies level figure shows key capability introduced evaluated paper techniques rule learning learn view view includes features utilized bayes net defined simply aggregation existing features features defined learned rules capture hidden concepts central accurately predicting malignancy explicit database tables learned rule states change shape abnormality location earlier mammogram indicative malignancy increase average sizes abnormalities indicative malignancy note rules require rows table patient intensional background knowledge define concepts increases time rule captured standard aggregation existing fields view learning framework imagine variety approaches perform view learning closing section discusses number alternatives including performing view learning structure learning time search present work apply existing technology fashion obtain view learning capability relational database naturally simply represented subset first-order logic inductive logic programming ilp algorithms learn rules expressed logic relational data possibly background knowledge expressed logic program ilp systems operate searching space logical rules rules score measure fit data ilp learn rules predict abnormality malignant treat rule additional binary feature true body condition rule satisfied false run bayesian network structure learning algorithm allowing features addition original features simple rule covering positive examples negative examples abnormality mammogram malignant tissue asymmetric abnormality margins spiculated architectural distortion rule field view database feature bayesian network lines rule refer rows relational table abnormalities database rule encodes information current version bayesian network experiments purposes experiments conducted two-fold determine srl yields improvement compared propositional learning evaluate inductive logic programming ilp create features embody view database adds benefit current srl algorithms looked adding types relational attributes aggregate features horn-clause ilp rules aggregate features represent summaries abnormalities found mammogram patient performed spic var spic patient abnormality date mass shape mass size location benign malign level mal shape size parameter learning features node labels fields database data bayes net structure learn probabilities note probabilities needed mal shape mal size mal level mal shape size structure learning features data learn bayes net structure probabilities note structure size shape mal size mal aggregate learning features data background knowledge aggregation functions average mode max learn aggregate features bayes net structure features probabilities features rows tables level mal shape size avg size date view learning features data background knowledge aggregation functions intensionally-defined relations increase location learn features defined views equivalent rules sql queries bayes net structure probabilities level mal shape size avg size date shape change abnormality location increase average size abnormalities figure hierarchy learning types levels ordinary bayesian network learning algorithms level state-of-the-art srl techniques level paper series experiments aimed discovering moving hierarchy outlined figure improve performance learn structure original attributes performed expert structure corresponds level learning added aggregate features network finally 
created features ilp investigated adding features proposed ilp system aggregate network experimented number structure learning algorithms bayesian networks including bayes tree augmented bayes sparse candidate algorithm obtained results tan algorithm experiments focus discussion tan tan network attribute parent addition class variable tan model constructed polynomial time guarantee model maximizes log likelihood network structure dataset methodology dataset malignant abnormalities benign abnormalities evaluate compare approaches stratified -fold cross-validation randomly divided abnormalities roughly equal-sized sets approximately one-tenth malignant abnormalities one-tenth benign abnormalities evaluating structure learning aggregation folds training set performing aggregation binning discretize created features care examples train set determine cut bin widths performing view learning steps learning process part folds data learn ilp rules remaining folds learn bayes net structure parameters cross-validation relational database exists major methodological pitfall cases related multiple abnormalities single patient abnormalities related patient training set test set perform test cases expect perform cases patients avoid leakage information training set ensured abnormalities patient fold cross-validation pitfall learn rule predicts abnormality malignant based properties abnormalities mammograms predict status abnormality date based findings recorded dates present results experiment roc precision recall curves skewed class distribution large number benign cases prefer precision-recall curves roc curves show number false alarms unnecessary biopsies precision-recall curves remainder results precision percentage abnormalities classified malignant cancerous recall percentage malignant abnormalities correctly classified generate curves pooled results ten folds manner treat prediction generated model sorted estimates split points create graphs results relational database mammography data row abnormality mammogram fields relational table include shown bayesian network figure straightforward existing bayesian network structure learning algorithms learn possibly improved structure bayesian network compared performance learned networks expert defined structure shown figure estimated parameters expert structure dataset maximum likelihood estimates laplace correction figure shows roc curve experiments figure shows precision-recall curves figure shows area precision-recall curve expert network learned structure recalls application radiologists required perform level paired t-test compare areas curve fold found difference statistically significant level confidence radiologist selected numeric ordered features database computed aggregates features determined attributes suitable aggregation computed aggregates patient mammogram level true positive rate false positive rate level level level level figure roc curves structure learning level precision recall level level level level figure precision recall curves structure learning level patient level looked abnormalities specific patient mammogram level considered abnormalities present specific mammogram discretize averages divided range bins binary features predefined bin sizes features attempted equal numbers abnormalities bin aggregations functions maximum average aggregation introduced features interested reader paragraph presents details aggregation process step process construct aggregate features chose field aggregate selected aggregation function needed decide rows aggregate feature keys links follow slot chain prm terminology database links exist patient field access abnormalities patient providing aggregation patient level key combination patient mammogram date returns abnormalities patient specific mammogram providing aggregation mammogram level precision recall comparison levels learning level level level level figure precision recall curves level learning demonstrate process work computing aggregate feature patient database figure aggregate mass size field average aggregation function patient abnormalities mammogram mammogram calculate aggregate patient level average size abnormalities find aggregate mammogram level patient perform separate computations follow link yields abnormality average key mammogram simply follow link yields abnormalities average abnormalities figure shows database tested fields computed rule learning specifically ilp system aleph learn rules predictive malignancy thousand distinct rules learned fold rule covering malignant cases incorrectly covering benign cases order obtain varied set rules ran aleph positive fold served seed search avoid rule overfitting found authors breadth-first search rules minimal limit coverage seed generated tens thousands rules post processed rules greedy algorithm select scoring rule covers examples fold clauses selected based criteria needed multi-relational needed distinct needed cover significant number malignant cases resulting views added features database figure includes comparison levels learning observe significant improvements adding multi-relational features rules aggregates achieved performance aggregates higher recalls rules medium recalls ilp rules accurate features limited coverage figure area curve recalls level performs aggregates high recalls close ilp medium recalls paired t-test improvement level level significant area curve metric level level presents improvement level area curve metric confidence level levels correspond standard propositional learning levels incorporate relational information task relational information crucial improving preformance process generating views level radiologist potentially identified correlations attributes related work research srl advanced main lines methods graphical models represent relations frameworks extend logic handle probabilities line probabilistic relational models prms introduced friedman getoor koller pfeffer represent attempts learn structure graphical models incorporating relational information recently heckerman meek koller discussed extensions prms compared graphical models statistical learning algorithm probabilitistic logic representations sato cussens proposed general algorithm handle log linear models additionally muggleton provided learning algorithms stochastic logic programs structure logic program learned ilp techniques parameters learned algorithm scaled stochastic context-free grammars newer representations garnering arguably attention bayesian logic programs blps constraint logic programming bayes net constraints clp markov logic networks mlns markov logic networks similar approach nodes mlns ground instances literals rule arcs correspond rules major difference approach nodes rules patient mammogram spic pic mass size mass size patient abnormality date mass shape mass size location average average mal figure database aggregation mass size field work level detail approach makes straightforward combine logical rules features full advantage propositional learning algorithms present work builds previous work ilp feature construction work treats ilp-constructed rules boolean features re-represents feature vector feature-vector learner produce final classifier knowledge pompe kononenko apply bayes combine clauses work category srinivasan king rules extra features task predicting biological activities molecules atomand-bond structures popescul derive cluster relations combined original features structural regression vein relational decision trees aggregation provide extra features multi-relational setting close level setting knobbe proposed numeric aggregates combination logic-based feature construction single attributes perlich provost discuss approaches attribute construction aggregates multi-relational features authors propose hierarchy levels learning feature vectors independent attributes table multidimensional aggregation table aggregation tables techniques hierarchy applied perform view learning srl conclusions future work presented method statistical relational learning integrates learning attributes aggregates rules application shows benefits levels learning proposed level structure learning outperforms expert structure show multi-relational techniques achieve significant improvements single table domain consistent improvement obtained level aggregates views improvements makes sense include 
aggregates background knowledge rule generation alternatively extend rules aggregation operators proposed recent work vens found rule selection problem non-trivial greedy algorithm generates similar rules guaranteed maximize coverage approach problem optimization problem weighing coverage diversity accuracy approach ilp learn features existing table scratches surface potential view learning ambitious approach closely integrate structure learning view learning search performed move search space modify probabilistic model refine intensional definition field view learn intensional definition table concrete mammography learn rules defining binary predicate identifies similar abnormalities predicate represent many-to-many relationship abnormalities table required acknowledgments support research partially provided air force grant elizabeth burnside supported general electric research radiology academic fellowship dutra leave federal rio janeiro brazil tor santos costa leave porto portugal federal rio janeiro brazil rich maclin reading drafts paper referees insightful comments brown houn sickles kessler screening mammography community practice positive predictive abnormal findings yield follow-up diagnostic procedures ajr roentgenol burnside rubin shachter bayesian network screening mammography amia pages burnside rubin shachter bayesian network predict probability type breast cancer represented microcalcifications mammography medinfo santos costa page qazi cussens clp constraint logic programming probabilistic knowledge uaipages acapulco cussens parameter estimation stochastic logic programs machine learning ecklund shortage qualified breast imagers lead crisis diagn imaging nir friedman david geiger moises goldszmidt bayesian networks classifiers machine learning nir friedman getoor koller pfeffer learning probabilistic relational models proceedings international joint conference artificial intelligence stockholm sweden nir friedman nachman learning bayesian network structure massive datasets sparse candidate algorithm uaipages san francisco morgan kaufmann publishers dan geiger entropy-based learning algorithm bayesian conditional trees uaipages san mateo morgan kaufmann publishers heckerman meek koller probabilistic entity-relationship models prms plate models technical report msr-tr- microsoft research technical report microsoft research kahn roberts shaffer haddawy construction bayesian network mammographic diagnosis breast cancer comput biol med kersting raedt basic principles learning bayesian logic programs arno knobbe marc haas arno siebes propositionalisation aggregates pkdd pages muggleton inductive logic programming generation computing muggleton learning stochastic logic programs electronic transactions artificial intelligence jennifer neville david jensen lisa friedland michael hay learning relational probability trees kdd pages acm press claudia perlich foster provost aggregation-based feature invention relational concept classes kdd pages pompe kononenko naive bayesian classifier ilp-r raedt editor ilp pages alexandrin popescul lyle ungar steve lawrence david pennock statistical relational learning document mining icdm pages ramakrishnan gehrke database management systems mcgraw hill matt richardson pedro domingos markov logic networks http washington homes pedrod kbmn pdf sato statistical learning method logic programs distributional semantics sterling editor proceedings twelth international conference logic programming pages cambridge massachusetts mit press sickles wolverton dee performance parameters screening diagnostic mammography specialist general radiologists radiology srinivasan king feature construction inductive logic programming study quantitative predictions biological activity aided structural attributes ilp pages ashwin srinivasan aleph manual celine vens anneleen van assche hendrik blockeel zeroski order random forests complex aggregates ilp pages 

appears advances neural information processing systems vol mit press cambridge pictorial structures molecular modeling interpreting density maps frank dimaio jude shavlik george phillips department computer sciences department biochemistry wisconsin-madison wisconsin-madison dimaio shavlik wisc phillips biochem wisc abstract x-ray crystallography common protein structures elucidated time-consuming steps crystallographic process interpretation electron density map task involves finding patterns three-dimensional picture protein paper describes deft deformable template algorithm pictorial structures build flexible protein model protein amino-acid sequence matching pictorial structure density map automating density-map interpretation extensions pictorial structure matching algorithm automated interpretation deft tested set density maps ranging resolution producing rootmean-squared errors ranging introduction important question molecular biology structure protein knowledge protein unique conformation insight mechanisms protein acts algorithm exists accurately maps sequence structure forced wet laboratory methods elucidate structure proteins common method x-ray crystallography tedious process x-rays shot crystal purified protein producing pattern spots reflections processed yielding electron density map density map analogous threedimensional image protein final step x-ray crystallography referred interpreting map involves fitting complete molecular model position atom protein map interpretation typically performed crystallographer time-consuming manual process large research efforts put high-throughput structural genomics accelerating process important investigate speeding process x-ray crystallography automating time-consuming step interpreting density map amino-acid sequence protein advance giving complete topology protein intractably large conformational space protein hundreds amino acids thousands atoms makes automated map interpretation challenging groups attempted automatic interpretation varying success figure graphic illustrates density map quality resolutions resolutions depict alpha helix structure confounding problem sources error make automated interpretation extremely difficult primary source difficulty due crystal diffracting extent eliminating higher frequency components density map produces blurring effect evident density map blurring quantified resolution density map illustrated figure noise inherent data collection complicates interpretation minimal noise sufficiently good resolution automated density map interpretation essentially solved poorer quality maps interpretation difficult inaccurate automated approaches failed remainder paper describes deft deformable template computational framework building flexible three-dimensional model molecule locate patterns electron density map pictorial structures pictorial structures model classes objects single flexible template template represents object class collection parts linked graph structure edge defines relationship parts connects pictorial structure face include parts left eye eye edges connecting parts enforce constraint left eye adjacent eye dynamic programming matching algorithm felzenszwalb huttenlocher referred f-h matching algorithm pictorial structures quickly matched twodimensional image matching algorithm finds globally optimal position orientation part pictorial structure assuming conditional independence position part neighbors formally represent pictorial structure graph set parts edge connecting neighboring parts explicit dependency exists configurations parts part assigned configuration describing part position orientation image assume markov independence probability distribution part configurations conditionally independent part configuration configuration part neighbors graph assign edge deformation cost part mismatch cost functions negative log likelihoods part pair parts taking configuration pictorial structure model matching algorithm places model image maximum-likelihood finds configuration parts model image maximizing mexp mexp iiii lplipilp monotonicity exponentiation minimizes f-h matching algorithm places additional limitations pictorial structure object graph tree structured cyclic constraints allowed deformation cost function form jiijii lll jiij arbitrary functions norm euclidian distance figure interpreted density map figure shows arrangement atoms generated observed density figure construction pictorial structure model amino acid building flexible atomic model three-dimensional map large molecule topology proteins amino-acid sequence molecule task determine cartesian coordinates density map atom molecule figure shows sample interpreted density map deft finds coordinates atoms simultaneously building pictorial structure protein f-h matching optimally place model density map section describes deft deformation cost function matching cost function deft deformation cost related probability observing configuration molecule ideally function proportional inverse molecule potential function configurations lower potential energy observed nature potential complicated accurately approximated tree-structured pictorial structure graph solution relationships covalently bonded atoms deft constructs pictorial structure graph vertices correspond nonhydrogen atoms edges correspond covalent bonds joining atoms cost function edge defines maintain invariants interatomic distance bond angles allowing free rotation bond protein amino acid sequence model construction illustrated figure trivial part configuration defined parameters translational rotational euler angles cost function define connection type pictorial structure framework screw-joint shown figure screw-joint cost function mathematically terms directed version pictorial structure undirected graph graph constrained fast matching algorithm tree structure arbitrarily pick root node point edge root define screw joint terms parent child rotate child axis coincident vector child parent part model atom freely rotate local axis ideal geometry child parent parameters stored edge parameters define optimal translation parent child coordinate system parent turn defined z-axis corresponds axis connecting parent construct cost function define function maps parent configuration configuration parent ideal child parameters edge function defined jjjjjjiiiiii zyxzyx zyx atan atan zyxzyxzyx iiijjj rotation bond parameters world coordinates ijijij iii zyxzyx iii rotation matrix euler angles expressions define optimal orientation child coincident axis connects child parent f-h matching algorithm requires cost function form specifically norm screw-joint model sets deformation cost parent child distance child configuration ideal child configuration parent configuration equation simply identity function -norm weighted dimension atan atan zzzyyyxxxw xyzyxw llll jijiji translate ijji orient rotate jiji ijij equation cost rotating bond cost rotating axis cost translating deft screw-joint model sets rotate orient translate rotate orient translate deft match-cost function implementation based cowtan fffear algorithm algorithm quickly efficiently calculates squared distance weighted template density region density map learned template weight function fffear fourier convolution determine maximum likelihood weighted template generated region density density map non-hydrogen atom protein create target template neighborhood atom training set crystallographer-solved structures build separate template atom type -carbon sidechain carbon leucine backbone oxygen serine producing templates total part function fffearcomputed mismatch score part template positions orientations construct model parameters including optimal orientation edge template part learned training figure showing screw-joint connection parts model directed version mrf parent definition oriented local z-axis coincident ideal bond orientation ijijij zyx bond parameters learned deft model set structures learning orientation parameters fairly simple atom define canonic coordinates corresponds axis rotation child record distance orientation canonic coordinate frame average atoms type training set leucine -carbon determine average parameters avg avg avg converting averages spherical cartesian coordinates ideal orientation parameters similarly-defined canonic coordinate frame employed learning model templates case deft learning algorithm computes target weight templates based average 
inverse variance training set figure shows overview learning process implementation cowtan clipper library part model deft searches six-dimensional conformation space breaking dimension number discrete bins translational parameters sampled region unit cell rotational space uniformly sampled algorithm mitchell model enhancements initial testing pictorial-structure matching algorithm performs poorly density-map interpretation task added routines collision-detection routine improved template-matching routine deft pictorial-structure matching implementation enhancements applied general pictorial structure algorithm specific deft collision detection closer investigation revealed algorithm poor performance due distant chains colliding deft models covalent bonds matching algorithm returns structure non-bonded atoms impossibly close collisions problem deft initial implementation figure shows collision corrected algorithm candidate solution straightforward test spatial collisions simply test atoms structure impossibly physically close collision occurs candidate deft perturbs structure fffear target template map averaged bond geometry standard orientation alanine figure overview parameter-learning process atom type alanine rotate atom canonic orientation average atom type template average bond geometry optimal match longer returned approach works practice atoms aligned space probable conformation atoms belongs deft handles collisions assuming colliding branches correct collision occurs deft finds closest branch point colliding nodes root minimum subtree colliding nodes deft considers child root matching subtree rooted keeping remainder tree fixed change score perturbed branch recorded smallest score increase deft figure illustrates collision avoidance algorithm left collision predicted molecule darker color amino acid sidechain coincident backbone collision avoidance finds structure table describes collision-avoidance algorithm case colliding node due chain wrapping branches running root defined colliding node nearest top tree matched anew remainder structure fixed improved template matching original implementation deft learned template averaging atom types non-hydrogen atoms amino-acid tyrosine build single template producing tyrosine templates total inefficient requiring deft match redundant templates unsolved density map atoms flexible sidechains averaging blurs density contributions atoms bond target losing valuable information atom neighborhood deft improves template-matching algorithm modeling templates mixture gaussians generative model template modeled mixture basis templates basis template simply cluster templates cluster assignments learned iteratively algorithm iteration algorithm compute priori likelihood image generated cluster step probabilities update cluster means step convergence cluster weight fffear search target table deft collision handing routine illegal pictorial structure configuration return legal perturbation algorithm nodes illegally close node root smallest subtree nodes child optimal position subtree rooted fixing remainder tree score score score subtree rooted min arg min score replace subtree rooted imin return experimental studies tested deft set proteins provided phillips lab wisconsin set consists proteins resolution proteins reflections initial phases provided allowing build poor-quality density maps test algorithm poor-quality data down-sampled maps removing higher-resolution reflections recomputed density down-sampled maps physically identical maps natively constructed resolution structure solved crystallographers paper experiments conducted assumption mainchain atoms protein error factor assumption fair approaches exist mainchain tracing density maps deft simply walks mainchain placing atoms residue time residue independently split dataset training set residues test set residues protein training set training set built set templates matching fffear templates extended radius atom sampling sets templates built subsequently matched large set produced averaging training set templates atom type smaller set learned algorithm ran deft pictorial structure matching algorithm sets templates collision detection code placing individual atoms sidechain fairly quick taking hours -residue protein computing fffear match scores cpudemanding templates fffear takes cpu-hours compute match score location image total cpu-month match templates protein fortunately task trivially parallelized regularly computations computers simultaneously results tests summarized figure individual-atom templates collision detection code all-atom rms deviation varied resolution em-based clusters templates produced slight improvement work required templates matched image individual-atom templates finally promising collision detection leads significant error reduction interesting note individually improved templates collision avoidance improved search results bit worse collision detection research needed synergy enhancements investigation needed balancing number templates template size match cost function critically important part deft improvements profound impact error density map resolution evi base improved templates collision detection improved templates collision detection figu ies testset error strateg conclusions future work deft applied f-h pictorial structure matching algorithm task interpreting electron density maps process extended f-h algorithm key ways order model atoms rotating designed joint type screw joint developed extensions deal spatial collisions parts model implemented slightly-improved template construction routine enhancements applied pictorial-structure matching general specific task presented deft attempts bridge gap types model-fitting approaches interpreting electron density maps techniques good job placing individual atoms fail resolution hand fffear success finding rigid elements poor resolution maps unable locate highly flexible loops work extends resolution threshold individual atoms identified electron density maps deft flexible model combines weakly-matching image templates locate individual atoms maps individual atoms blurred approach investigated sidechain refinement structures poor resolution plan deft refinement phase complementing coarser method model configuration individual atom treat amino acid single part flexible template modeling rotations backbone current algorithm place individual atom optimization algorithm handles cycles pictorial structure graph handle collisions allowing edges non-bonded atoms recent work loopy belief propagation success optimality guarantee plan explore belief propagation pictorial-structure matching adding edges graph avoid collisions finally pictorial-structure framework deft built robust accuracy approach substantially improved implementation improvements allowing finer grid spacing larger fffear templates flexible molecular template potential produce atomic model map individual atoms visible power combining weakly matching image templates deft prove important high-throughput protein-structure determination acknowledgments work supported nlm grant nlm grant nih grant perrakis sixma wilson lamzin warp improvement extension crystallographic phases acta cryst levitt software routine automates fitting protein x-ray crystallographic electron density maps acta cryst ioerger holton christopher sacchettini textal pattern recognition system interpreting electron density maps proc ismb cowtan fast fourier feature recognition acta cryst felzenszwalb huttenlocher efficient matching pictorial structures proc cvpr mitchell uniform distributions rotations unpublished document greer three-dimensional pattern recognition mol biol sudderth mandel freeman willsky distributed occlusion reasoning tracking nonparametric belief propagation nips koller lerner angelov general algorithm approximate inference application hybrid bayes nets uai 
appears proceedings international conference inductive logic programming ilp porto portugal september learning ensembles first-order clauses recall-precision curves case study biomedical information extraction mark goadrich louis oliphant jude shavlik department biostatistics medical informatics department computer sciences wisconsin-madison usa abstract domains eld inductive logic programming ilp involve highly unbalanced data research focused information extraction task typically involves negative examples positive examples process nding facts unstructured text biomedical journals putting facts organized system focused learning recognize instances protein-localization relationship medline abstracts view problem machine-learning task positive negative extractions training corpus abstracts learn logical theory performs held-aside testing set common measure performance domains precision recall simply accuracy propose gleaner randomized search method collects good clauses broad spectrum points recall dimension recall-precision curves employs clauses thresholding method combine selected clauses compare gleaner ensembles standard aleph theories gleaner produces comparable testset results fraction training time needed ensembles introduction domains suitable inductive logic programming ilp roughly divided main groups group tasks inherent relational structure classic domain trains dataset goal discriminate types trains trains relational objects varying length types objects carried car realistic mutagenesis dataset goal classify chemical compound mutagenic relational nature atomic structure chemical ilp proven successful domains bringing inherently relational attributes hypothesis space group tasks examples addition relational structure relations examples domain learning friendship social networks classifying people determine structural relationships people based combination personal attributes attributes friends domain type learning suggest citations scienti correct citation combination data paper listed citations goal domains classify links objects objects research focused information extraction process nding facts unstructured text biomedical journals putting facts organized system focused learning multislot protein localization medline abstracts task identify links phrases correspond protein location protein cell relational data task multi-slot falls link-learning category link-learning tasks present number problems ilp system domains tend large number objects relations causing large explosion search space clauses rst approach sample objects bring space reasonable size moderate number objects brings problem large skew data negative examples suppose social network domain people friends people positive examples assuming friendship relationship necessarily symmetric negative examples include friendships negative examples skew information extraction domain typically unbalanced data small number phrases protein names learning relation entities protein location increases imbalance number positive examples subset crossproduct entities negative examples pairing dataset issues lead standard performance measure accuracy letting stand true positives false positives true negatives false negatives accuracy ned positive class small relative negative class trivial achieve high accuracy labeling test examples negative concentrate positive examples performance measures precision ned tptp recall ned tptp precision measure accurate predicting positive class recall measure total positives identify chose pursue machine-learning perspective set journal abstracts manually tagged protein-localization relationships goal learn theory extracts relations set abstracts performs unseen abstracts ve-fold cross validation approximately positive negative examples fold http ncbi nlm nih gov pubmed division examples uniform chose split data folds journal-abstract level sentences abstract fold number examples abstract variable ilp applied successfully information extraction biomedical domains link-learning tasks ilp ers advantages straight-forward incorporate domain knowledge expert advice produce logical clauses suitable analysis revision humans improve performance aleph mature ilp system learn rst-order clauses standard approach ilp learn clauses sequentially positive examples covered clause creating theory individual theory produce precision recall standard logical approach disjunction combine clauses theory evaluation create recallprecision curve illustrates trade-o measurements create recall-precision curve theory clauses require clauses satis varying obtain variety points recall-precision curve ilp systems traditionally designed produce recall-precision curves specially designed algorithms simply counting number clauses satis address goal ciently producing good recall-precision curves ilp propose gleaner algorithm gleaner randomized search method collects good clauses broad spectrum points recall dimension recall-precision curves employs clauses thresholding method combine selected clauses compare gleaner ensembles standard aleph theories gleaner produces comparable results fraction training time needed aleph ensembles smaller theories reduce classi cation time important consideration working large domains biomedical information extraction information extraction process scanning plain text les objects interest facts objects learning task ned information unstructured text documents extract relevant objects relationships main tasks named entity recognition ner multi-slot extractions ner identifying single type object individual corporation gene weapon successful rule-based approaches named-entity include rapier system learns clauses format pre extraction post boosted wrapper induction bwi method boosting weak rule-based classi ers extraction boundaries powerful extraction method bwi examined kauchak showing results high recall high precision wide variety tasks suggest smf smf mitochondrial membrane proteins uence pep-dependent protein import possibly step protein translocation protein location smf mitochondrial protein location smf mitochondrial fig sample sentence correct extractions multi-slot extraction builds objects found ner relationship items text examples parentchild relationship individuals ceo company interaction proteins cell multi-slot extraction typically harder objects relation identi semantic relationship objects recently biomedical journal articles major source interest community number reasons amount data enormous objects proteins genes standard naming conventions nite interest biomedical practitioners quickly relevant information biomedical journals highly domain-speci language figure previous machine-learning work biomedical multi-slot domain includes number erent approaches ray craven hidden markov model hmm modi include part speech tagging analyze method protein localization genetic disorder protein-protein interaction tasks datasets eliassi-rad shavlik implemented neural network primed domain-speci prior knowledge aitken foil perform ilp working closed ontology entities brunescu propose elcs bottom approach nding protein interactions rule templates sentences brunescu extended rapier bwi handle multi-slot extractions data labeling paper focus dataset learning location yeast proteins cell illustrated figure testbed ray craven data consist sentences abstracts found medline database relations original dataset labeling performed semi-automatically order avoid laborious task labeling human protein localizations gathered yeast protein database ypd sentences contained instances protein location pair marked positive computer program early exploration dataset found signi number false positives looked true positives apparently missed automated labeling algorithm labelings ambiguous nding parts positive protein localization human-judged semantics sentence involve localization addition labeling scheme data yeast proteins corpus listed ypd issues decided relabel dataset hand assisted ort soumya ray label positive examples manually performed protein location named-entity labeling relational labeling labeling standards groups task extract locations yeast proteins disagreement labelers tag protein location make training set precise expense recall protein labeling strove speci general labeled words directly refered protein gene molecule included gene names 
smf protein names fet full chemical names enzymes -cytochome reductase label sec sec mutant label isp delta rrp gene products defective give rise functioning protein molecule label protein families hsp adjective protein hsp dnak fusion proteins gene combined uorescent tag labeled proteins protein complexes antibodies open reading frames labeled positive protein examples proteins exist yeast labeled found species dataset dealt localization yeast proteins labeling location words direct list cellular locations listed introductory cellular biology text book including locations abbreviations cytoskeleton membrane lumen npc bud labeled location adjectives nucleoporin ribosomal determine relationship tagged proteins tagged locations classi cations clear ambiguous co-occurrence relationships directly implied text protein location yrb cytosol sentence yrb located cytosol classi clear relationships protein location implied stated protein location lip mitochondrial sentence lip mutants undergo high frequency mitochondrial dna deletions labeled ambiguous correct classi cation agreed labelers experiments clear category positive examples phrase pairings negative examples future goal improve manual-labeling interface background knowledge standard feature-vector machine learning setup ilp logical relations describe data algorithms attempt construct logical clauses based background structure separate positive negative sentence fragment text part speech phrase type haved named yfh localizes mitochondria fig sample sentence parse sundance sentence analyzer noun verb preposition phrase examples information extraction task construct background knowledge sentence structure statistical word frequency lexical properties biomedical dictionaries rst set relations sentence structure sundance sentence parser automatically derive parse tree sentences dataset part-of-speech words phrases tree tree attened degree nested phrases phrases sentence root words members phrase figure shows sentence parse word phrase sentence unique identi based ordering abstract create relations sentences phrases words based actual text document structure sentence child phrase previous word tree structure sequence words relations nounphrase article verb describe sentence structure include actual text sentence background knowledge predicate word string maps identi ers words addition words sentence stemmed porter stemmer stemmed version words group background relations frequency words appearing target phrases training set perfold basis prevent learning test set words body npc membrane times location phrases phrases general training set created predicates gradations times times general word frequency abstracts training set gradations calculated arguments protein location words frequently arguments create semantic classes consisting high frequency words semantic classes mark occurrences words training testing set source background knowledge derived lexical properties word alphanumeric words numbers alphabetic characters alphabetic words alphabetic characters lexical morphological features include singlechar hyphenated capitalized words classi novelword standard usr dict words dictionary unix sentence structure predicates phrase phrase phrase phrase speci word phrase word wordstring statistical word frequency predicates phrase word phrase argument phrase halfx word phrase argument partofspeech lexical properties predicates alphabetic word wordpos sentence sentence partofspeech biomedical dictionaries predicates phrase mesh term phrase term stemmedterm phrase term phrase term stemmedterm fig sample predicates information extraction task finally incorporate semantic knowledge biology medicine background relations medical subject headings mesh gene ontology online medical dictionary sentence structure simpli hierarchies level picked categories mesh protein peptide cellular structure cellular-localization category cellular-biology category online medical dictionary labeled phrases predicates words phrase match words category sentence structure predicates word phrase added allowing navigation parse tree phrases tagged rst phrase sentence likewise words length phrases calculated explicitly turned predicate length words phrases sentences phrases classi short medium long additional piece information predicate phrases true arguments distinct phrases lexical predicates augmented make applicable phrase level phrase alphabetic word phrase predicate phrase alphabetic word similarly phrases speci words marked phrase specific word lumen equivalent adding phrase child word string lumen predicates created pairs triplets words assert phrase word golgi labeled noun search step http nlm nih gov mesh meshhome html http geneontology http cancerweb ncl omd finally predicates added denote ordering phrases target arg target arg asserts protein phrase occurs location phrase similarly target arg target arg created adjacent target args true protein location phrases adjacent sentence identical target args noun phrase protein location count phrases target arguments list predicate categories sample predicates found figure ned predicates describing training examples unbalanced data filtering previously mentioned culties face domain large number examples sentence examine pair phrases positive examples positive negative ratio leading severely unbalanced data domain prior knowledge reduce number false positive examples observe positive relations noun phrases ratio limit size training data candidate extractions arguments noun phrases reduces positive negative ratio data necessarily track missed positive testing set non-noun phrase record false negatives recall-precision results reduce positive negative ratio randomly under-sample negatives retaining fourth training faster clause learning future work includes selecting close negative examples training randomly selecting aleph aleph top-down ilp covering algorithm developed oxford written completely prolog open source input aleph takes background information form predicates list modes declaring predicates chained designation predicate head predicate learned required lists positive negative examples head predicate high-level overview aleph generates clauses positive examples picking random seed saturated create bottom clause relation background knowledge reached bottom clause search space clauses aleph heuristically searches space clauses clause found time runs standard aleph combine learned clauses theory clauses learned cover positive training examples aleph exible ilp system wide variety learning parameters modi cation parameters utilized minimum accuracy place lower bound accuracy clauses learned system accuracy clause examples covered words precision minimum positives prevent aleph learning narrow clauses cover examples acceptable clause cover number positives clause length size clause constrained clause length limiting length explore wider breadth clauses prevent clauses speci search strategy aleph search good clauses type search parameter include standard search methods breadthrst search depthrst search iterative beam search iterative deepening heuristic methods requiring evaluation function evaluation function ways calculate node exploration common heuristic ilp coverage ned number positives covered clause minus number negatives similar heuristic compression coverage minus length clause working domains generate precision recall curves explored heuristic-search evaluation function precision recall measure precision recallprecision recall improve clause quality correct accuracy estimates clauses cover small number examples laplace estimate coverage tune set encourage clauses general added parameter aleph requiring recorded clause small positive coverage tuneset clauses unseen examples test set gleaner biomedical task link-learning task evaluate success methods precision recall order rapidly produce good recall-precision curves developed gleaner two-stage algorithm learn broad spectrum clauses 
combine thresholded disjunctive clause aimed maximizing precision choice recall algorithm summarized figure rst stage gleaner learns wide spectrum clauses aleph search clauses seed examples diversify search rst uniformly dividing recall dimension equal sized bins seed clauses random local-search method clauses generated compute recall clause determine bin clause falls create recall bins uniformly dividing range pick seed generate bottom clause random local search clauses generation clause find recall bin precision recall store bin find trainset recall clauses match examples recall bin find precision recall testset bin decision process fig gleaner algorithm bin tracks highest precision clause learned bin replaced precise clause found nding highest precision clause bin save clause product precision recall highest clauses falling recall bin end search process clauses collected seed seed examples total clauses assuming clause found falls bin seed perform random local search considered search methods rapid random restart rrr stochastic clause selection scs gsat walksat scs randomly picks clauses subsets bottom clause distribution clauses based length scs hard time nding high quality clauses biased select long clauses due heavytailed distribution clause lengths gsat selects initial clause random chooses add remove randomly selected literal clause evaluation function walksat modi gsat allowing percent bad moves rrr works similarly gsat walksat initial clause selection nes clauses adding predicates rst search restarting clause specied number evaluations gsat walksat occasionally make downhill moves search space rrr due interal workings aleph adding predicates clause cient removing found rrr takes time produces higher quality clauses methods gleaner search method remainder article stage takes place gathered clauses random search combine clauses single precision recall point bin choose clause collected bin poor generalization test set low-recall bins classify positive matches clauses collected bin obtain high precision recall drastically reduced alternatively classify positive matches clauses theory high recall low precision balance extremes classify examples positive covered large subset clauses hypothesis method produce theory recall bin construction higher precision clause require satisfy multiple clauses assuming gleaner combines clauses bin create large thresholded disjunctive clause form clauses cover order classify positive clause recall clauses bin cover full range recalls threshold bin training set bin starting incrementally lowering threshold increase recall stop lower increase distance recall clause desired recall evaluate disjunctive clause testset record precision recall end precision recall points bin span recall-precision curve ensembles ilp bagging popular ensemble approach machine learning multiple classi ers trained erent subsamples training data classi ers vote classi cation testset examples majority class selected output classi cation vote userdependent common schemes equal voting weighted tuneset accuracy voter main idea bagging produce diverse classi ers make mistakes erent regions input space votes combined prediction errors reduced bagging ilp previously investigated dutra demonstrate bagging helpful modest improvements accuracy straight-forward calculate con dence random seeds approach creating ensembles approach dutra showed essentially equivalent predictive accuracy bagging produces diversity learned models starting run underlying ilp system erent seed compare gleaner approach random seeds aleph experimental control call aleph times create theories sets clauses cover positive training examples negative create recall-precision curve theories simply classify positive theories classify positive varying produces family ensembles ensembles produces point recall-precision curve aleph involves large number parameters train test sets choose good set experimental control fig area recall-precision curve aleph ensembles varying number clauses compare gleaner system fair testset tune parameters compare erent evaluation functions judging clauses laplace essentially measures accuracy corrects small coverage coverage number positive examples covered minus number negatives covered precision recall harmonic precision recall commonly performance measure information extraction settings minimum accuracy learned clauses require clauses cover positive examples longer ten terms settings random sampling hypothesis space gleaner approach limit number clauses considered thousand limit number reductions million call counting predicate yap prolog obtained area recall-precision curve laplace evaluation function minimum clause accuracy setting average number clauses considered constructed theory approximately nding encountered reported dutra limit size theories figure plots area recallprecision curve aurpc function maximum number clauses learned theories running aleph normal completion parameters leads theories clauses average limit rst clauses aurpc drastically reason larger theories diversity smaller diversity key ensembles nice side-e ect limiting theory size runtime individual aleph executions substantially reduced section evaluate gleaner algorithm limit theory size ensemble aleph theories approach clauses figure testset aurpc essentially peaked section http ncc vsc yap yap html experiments vary size ensemble number theories number clauses theory order impact aurpc function amount time spent training considered parameters settings algorithm designs aleph create ensemble theories evaluated substantial number variants feel chosen settings provide satisfactory experiment control compare algorithm gleaner results experiments divided protein localization data folds equally divided journal-abstract level training set consisted folds fold held tuning testing current experiments tuning set minimally requiring clause learned training set cover positive examples tuning set evaluate performance algorithms recall-precision curves precisely area recall-precision curve aurpc gather single score algorithm auc traditionally analyze roc curves plot true positive rate versus false positive rate calculate aurpc rst standardize recall-precision curves cover full range recall values interpolate threshold points rst threshold point designate rfirst pfirst curve extended horizontally point pfirst randomly discard fraction extracted relations expect precision remaining examples setting determine recall ending point posneg found calling positive give closed curve extending recall dimension points recall-precision curve interpolate true positive false positive counts order calculate area create points tpa tpa tpb increasing false positives point fpb fpatpb tpa interpolation recall-precision curve erent roc curve roc interpolation linear connection points recall-precision space connection curved depending actual number positive negative examples covered point curve pronounced points recall precision curve constructed single point extended endpoints dataset positives negatives interpolating produce aurpc linear connection overestimate aurpc figure shows graphically sample clause found gleaner shown figure dataset important require protein phrase protein location rst word phrase phrase target arg target arg target phrases phrase marked location pos phrase alphanumeric wordpos sentence alphanumeric phrase halfx word arg arg verb phrase art protein phrase location phrase sentence variables clause positive extraction npl encodes nuclear protein rna recognition motif similarities family proteins involved rna metabolism protein location npl nuclear protein negative extraction false positive subcellular fractionation studies demonstrate amino acid vps peripherally 
cytoplasmic face late golgi vesicle compartment protein location amino acid vps cytoplasmic face fig sample clause recall precision testset alphanumeric words important clause sentence structure requiring protein phrase location phrase location phrase phrase sentence aleph-based method producing ensembles parameters vary number theories size ensemble number clauses ensemble produce ensemble points figure choose producing combinations fold parameters gleaner recall bins seed examples collect clauses total told rrr construct clauses restarting random clause generate aurpc data points gleaner choosing number seed examples intervals number candidate clauses generated seed results comparison found figure points averaged folds note graph logarithmic scale number clauses generated gleaner comparable aurpc numbers orders magnitude fewer clauses interesting note fig comparison aurpc gleaner aleph ensembles varying number clauses generated fig sample gleaner recall-precision curve fold gleaner curve consistent number clauses allowed ensemble method increases clauses considered topic future work devise version gleaner utilize additional candidate clauses figure show recall-precision curve produced gleaner candidate clauses seed seed examples fold comparison show one-point interpolation curve mentioned gleaner clauses theoretically produce higher precision individual rules recall long coverage positives greater coverage negatives practice clauses independent high-recall bins learned clauses identical overlap degrades performance conclusions future work multi-slot information extraction appealing challenge task ilp due large amount examples background knowledge substantial skew examples developed method called gleaner gathers wide spectrum clauses combines bins based recall clauses thresholding method aleph ensembles perform early stopping learning dozen rules aleph ensembles allotted limited amount time create multiple theories method gleaner results similar curves aleph ensembles outperforms ensembles allowed evaluate limited number clauses large heavily skewed datasets ilp research information-extraction task provide testbed ilp research aid ilp research dataset made website acknowledgements number approaches relating combination learned clauses produce con dence measure opposed combining multiple theories bagging gleaner propositionalization feature space examined lavrac propositional learner generates con dence measures similarly srinivasan investigated ilp feature construction tool propositional learners linear regression craven slatterly logical setup combined naive bayes classi ers generate recall-precision curves resulting theories plan compare within-theory ensemble methods multiple theory ensemble methods gleaner vein boosting ilp alternative method searching clauses learning combine single step recent work shown rankboost variant boosting directly optimizes area roc curve similar optimization area recall-precision curve achieved plan implement algorithm aleph comparison gleaner noticed learned clauses focused learning individual entities relation case creating logical clauses protein location clause relevant relation entities named-entity classi identify promising pieces relation rst reduce number examples produce high quality clauses due direct focus relation blaschke rind esh found success biomedical information extraction domain expert rules temkin gilder handcrafted context-free grammars similar ends step direction taking clauses domain expert learning revise advice similar work eliassi-rad shavlik finally datasets information extraction planning test method comparison genetic disorder protein interaction ray craven protein interaction dataset brunescu datasets gleaner include nuclear smuggling dataset tang social network dataset taskar citeseer citation dataset popescul acknowledgements dataset found ftp ftp wisc machine-learning shavlikgroup datasets ie-protein-location work supported national library medicine nlm grant nlm grant darpa eeld grant united states air force grant ines dutra vitor santos costa yap condor group condor assistance soumya ray marios skounakis labeling data david page aleph anonymous reviewers informative comments aitken learning information extraction rules inductive logic programming approach van harmelen editor proceedings european conference arti cial intelligence amsterdam ben taskar pieter abbeel koller label link prediction relational data ijcai workshop learning statistical models relational data blaschke hirschman valencia information extraction molecular biology brie ngs bioinformatics blaschke valencia bibliographic pointers biological data found automatically protein interactions case study comparative functional genomics bradley area roc curve evaluation machine learning algorithms pattern recognition breiman bagging predictors machine learning bunescu kate marcotte mooney ramani wong comparative experiments learning information extractors proteins interactions journal arti cial intelligence medicine cali mooney relational learning pattern-match rules information extraction working notes aaai spring symposium applying machine learning discourse processing pages menlo park aaai press cortes mohri auc optimization error rate minimization neural information processing systems nips craven slattery relational learning statistical predicate invention models hypertext machine learning castro dutra page costa shavlik empirical evaluation bagging inductive logic programming twelfth international conference inductive logic programming pages sydney australia dietterich machine-learning research current directions magazine eliassi-rad shavlik theory-re nement approach information extraction proceedings international conference machine learning freitag kushmerick boosted wrapper induction aaai iaai pages hoche wrobel relational learning constrained con dence-rated boosting international conference inductive logic programming strasbourg france guidelines protein tagging technical report georgetown kauchak smarr elkan sources success boosted wrapper induction journal machine learning research lavrac zelezny flach rsd relational subgroup discovery first-order feature construction proceedings international conference inductive logic programming ilp sydney australia manning schutze foundations statistical natural language processing mit press michalski larson inductive inference decision rules proceedings workshop pattern-directed inference systems popescul ungar lawrence pennock statistical relational learning document mining ieee international conference data mining icdmm porter algorithm stripping program ray craven representing sentence structure hidden markov models information extraction proceedings international joint conference arti cial intelligence ijcaie rilo sundance sentence analyzer http utah projects nlp rind esch tanabe weinstein hunter edgar extraction drugs genes relations biomedical literature proceedings paci symposium biocomputing shatkay feldman mining biomedical literature genomic era overview journal computational biology srinivasan aleph manual version http web comlab oucl research areas machlearn aleph srinivasan king feature construction inductive logic programming study quantitative predictions biological activity aided structural attributes muggleton editor proceedings international workshop inductive logic programming pages stockholm royal institute technology srinivasan muggleton sternberg king theories mutagenicity study first-order feature-based induction arti cial intelligence tang mooney melville scaling ilp large examples results link discovery counter-terrorism kdd workshop multi-relational data mining temkin gilder extraction protein interaction information unstructured text context-free grammar bioinformatics 
selection combination evaluation effective software sensors detecting abnormal computer usage jude shavlik computer sciences department wisconsin madison shavlik wisc mark shavlik shavlik technologies long lake road suite roseville mark shavlik shavlik abstract present empirically analyze machine-learning approach detecting intrusions individual computers winnowbased algorithm continually monitors user system behavior recording properties number bytes transferred seconds programs running load cpu hundreds measurements made analyzed data algorithm creates model represents computer range normal behavior parameters determine alarm raised due abnormal activity set percomputer basis based analysis training data major issue intrusion-detection systems low falsealarm rates empirical results suggest obtain high intrusion-detection rates low false-alarm rates day computer stealing cpu cycles report system measurements valuable terms detecting intrusions surprisingly large number measurements prove significantly categories subject descriptors security protection artificial intelligence learning general terms algorithms experimentation security keywords intrusion detection anomaly detection machine learning user modeling windows feature selection winnow algorithm introduction increasingly computerized networked world crucial develop defenses malicious activity information systems promising approach develop computer algorithms detect inappropriately intruding computer person intrusion detection difficult problem solve system performance adversely affected false positives minimized intrusions caught false negatives low current state art intrusion-detection systems good false positives high successful detection rare report approach made significant advances creating intrusion-detection system requires cpu cycles produces false alarms day detects intrusions quickly minutes intrusion-detection systems ids attack patterns adaptive software smart monitor learn system supposed work normal operation versus works misuse occurring address approach article specifically empirically determining sets fine-grained system measurements effective distinguishing usage assigned user computer misusage insiders organization created prototype anomaly-detection system creates statistical profiles normal usage computer running windows significant deviations normal behavior intrusion occurring probability specific computer receives mbytes sec evenings measured low monitoring program detects high transfer rate evening hours suggest intrusion occurring algorithm developed measures two-hundred windows properties creates features machine-learning training phase learns weight features order accurately characterize behavior user user set feature weights training features vote intrusion occurring weighted votes permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee kdd august seattle washington usa copyright acm intrusion compared evidence alarm raised section presents additional details ids algorithm glossed point ability create statistical models individual computer normal usage means computer unique characteristics serve protective role similar person antibodies distinguish cells invading organisms statistical-profile programs gather data normal operation computer learn distinguish behavior foreign behavior instance people notepad view small text files prefer wordpad leave computer unattended inappropriately access files individual differences people computer usage statistical-modeling program quickly recognize illegal access evaluate ability detect computer misuse collecting data multiple employees shavlik technologies creating user profiles analyzing training subsets data experimentally judging accuracy approach predicting data testing sets normal user computer intruder key hypothesis investigated creating statistical models user behavior accurately detect computer misuse focus algorithmic development methods produce low false-alarm rates major reason system administrators ignore ids systems produce false alarms empirical results suggest detect intrusions false alarm day user noted results based model insider intruder assumes insider user computer alter behavior explicitly mimic normal behavior training phase approach computationally intensive due parameter tuning parameter tuning central server evenings users work cpu load ids negligible ordinary operation requires cpu cycles standard personal computer approach robust fact users normal behavior constantly time approach detect abnormal behavior computers operating specialized http ftp servers similarly techniques monitor behavior autonomous intelligent software agents order detect rogue agents behavior consistent normal range agent behavior family tasks experiments reported involve computers humans normal everyday business tasks employ word user article reader mind approach applies equally monitoring servers autonomous intelligent agents needed apply approach scenario define set potentially distinctive properties measure write code measured properties periodically previous empirical studies investigated creating intrusion-detection systems monitoring properties computer systems idea back years prior work focused unix systems world computers run variant microsoft windows addition prior studies looked large collection system measurements warrender ghosh lane brodley unix system calls lee audit data tcp program lazarevic provide summary recent research application data mining network-based anomaly detection section describe algorithm developed analyzes windows properties measure creating profile normal usage user section presents discusses empirical studies evaluate strengths weaknesses algorithm stressing dimensions amount data training section lists windows properties end highest weights weighted-voting scheme section describes future follow-up research tasks section concludes article algorithm developed section describe algorithm developed key finding machine-learning algorithm called winnow weighted-majority type algorithm works core component ids algorithm operates taking weighted votes pool individual prediction methods continually adjusting weights order improve accuracy case individual predictors windows properties measure probability obtaining current comparing threshold individual measurement suggests intrusion occurring prob measured property property measure votes intrusion occurring weighted sum votes leads wrong prediction intrusion intrusion weights properties voted incorrectly halved exponentially quickly properties informative end small weights leading highly accurate ids winnow algorithm windows properties intrusion detection properties highest weights training viewed users surprisingly high number properties end high weights equation found slightly compare probabilities relative general computed experimental subjects prob user general public alarm sounded ratio constant rare events specific user rare events general make sense equations experiment sec table algorithm increased detection rate meeting target false-alarm day idea ratio focuses feature values rare user relative probability occurrence general population feature values rare user occur rarely general population produce low ratios feature values rare user rare general ratio distinguishes rare user computer users rare user rare general estimate prob feature general population simply pooling training data experimental subjects creating discrete probability distribution ten bins technique explained fielded system reasonable ids design requires pool users training tuning phases experimental setup involves measurements normal computer users ratio probabilities makes sense experiments defines rare user relative baseline computer users operating behavior intruders insiders working facility normal computer usage data analyze intruder rare general equation produce setting threshold presenting algorithm calls subroutine winnow algorithm discuss make features two-hundred windows properties measure technically features weighted voting features space limitations preclude describing properties measured appendix full project 
report lists briefly describes windows properties measure relate network activity file accesses cpu load programs running windows perfmon performance monitor program features measure tables article measurements derive additional measurements actual measured average previous values average previous values difference current previous difference current average difference current ave difference averages previous previous discovered experiments additional derived features play important role intrusion-detection rates significantly lower remainder article term feature refer combination measured windows property transformations words windows property measure produces features item list derived feature raw measurement include list completeness ids algorithm table main algorithm machine-learning approach creating ids typical divide learning process phases training data create model make winnow algorithm table explain data tuning set tune additional parameters ids finally evaluate learned ids works measuring performance testing data repeat process multiple users report average test-set performance experiments windows properties measure continuousvalued step table decide discretize measurement bins bins create discrete probability distribution values feature importantly discretization separately user accurately approximate user probability distribution bins experiment values number bins chose arbitrarily make sense number small reduce storage demands smooth measurements place bin occurs frequently choose cut points define remaining bins fitting sample measurements produced user standard probability distributions uniform gaussian erlang ranging erlang equivalent decaying exponential distribution increases distribution gaussian select probability distribution fits sample data lowest root-mean-squared error create bins uniform probability distribution uniformly divide interval minimum maximum bins remaining bins values minimum greater maximum values encountered future exceed gaussian probability distribution place lowest probability mass bin bin bin bin working highest leaves middle bin roughly standard deviation gaussian exponential probability distribution put half probability mass bin half remaining probability mass successive bin bin erlang probability distribution execute combination gaussian exponential depending investigate alternate design choices discretization process developed approach unchanged subsequent learning-algorithm development evaluation table creating maintaining ids user step initial training step collect measurements user place trainset step trainset choose good cut points user discretize continuous values text step select weights user measured features applying winnow algorithm table accompanying text trainset equal number archived sample measurements users discretize measurements users applying user cut points pretending users inappropriately computer step parameter tuning step measurements collected user users tuneset perform steps calculating false-alarm intrusion-detection rates conceptually independent runs combinations parameters tuned thresh mini thresh full step weighted features vote minialarms wgtedvotes wgtedvotes thresh mini raise mini-alarm steps table step fraction mini-alarms seconds thresh full raise alarm signaling intrusion occurring call wait seconds windows overlap step maximum false-alarm rate -hour day choose parameter settings produce highest intrusion-detection rate set sample users producing desired number false alarms user step continual operation step chosen settings thresh mini thresh full repeat steps testset make sense periodically retrain retune order adjust changing user behavior text features turned modeled gaussians exponential distribution common selection final point converting discrete probability distribution mentioned windows measurements vary orders magnitude bytes log values discretized features simply count training data feature fall bin producing probability distribution normalizing total number counts standard practice initialize bins count ensures estimate finite samples probability bin estimate prob feature measured mentioned earlier equations table variant winnow step initialize user weights measured feature wgt step training step wgtedvotes wgtedvotes step relative probability current measured feature add wgt wgtedvotes add wgt wgtedvotes relative probability current feature low evidence anomalous occurring experiments found worked performance robust values range worked step wgtedvotes wgtedvotes call current measurements anomalous step user produced current measurements considered anomalous false-alarm error made multiply features incorrectly voted raising alarm user produced current measurements considered anomalous intrusion missed multiply features incorrectly voted raising alarm false-alarm missedintrusion error occurred leave current weights unchanged turn discuss probabilities learn models distinguishing normal user computer intruder ideally training data user provided examples normal non-intrusion data sample data measured wide range intrusions user computer data problem plagues ids research general standard approach collect data users case simulate intrusions replaying user measurements user computer false alarm occurs user recent measurements viewed anomalous suggestive intrusion replayed computer detected intrusion occurs view user measurements anomalous evaluated feature discretization feature weighting notice discretization assuming operating computer figure abstractly illustrates define false alarms detected intrusions experimental setting figure false alarms detected intrusions mentioned table version littlestone winnow algorithm choose weights features measure algorithm simple impressive theoretical properties practical success real-world tasks large number features case task discussed algorithm sums weighted votes possibility intrusion occurring winning choice wrong features voted wrong choice weights halved perform winnow algorithm user case mixture examples half drawn user measured behavior intrusion examples half randomly drawn user experiment intrusion examples order raise alarm training phase step table set feature weights algorithm simply current weighted vote current weighted vote raise call mini alarm require mini alarms seconds order raise actual alarm words intrusion detector works steps table weighted vote current measurements thresh mini raise mini alarm fraction mini alarms sec thresh full predict intrusion section order good detection rates false alarms choose settings parameters per-user basis evaluating performance set tuning data step table significant advantage data-driven approach pre-select parameter values learning algorithm selects user personal set parameter values based performance parameters substantial sample tuning set data computationally demanding portion algorithm parameter-tuning phase depends parameter combinations considered tuning data combination evaluated fielded system make sense step central server evenings tasks measuring features computing weighted sums winnow adjust weights rapidly parameter tuning table algorithm requires desktop computer cpu cycles notice testing phase step table find execute winnow algorithm adjust weights features algorithm decides intrusion occurred false alarms user behavior switches intrusion-detection rates drastically drops hand continually adjusting weights means miss intrusion start learning behavior intruder weakness approach weakness statistics-based approaches intrusion detection general means empirical results reported section properly interpreted estimating probability detect intruder seconds activity subject future work empirically evaluate approach detect intruder successive seconds activity detect intruder seconds hand fact continually adjusting weights means legitimate user reauthenticates false alarm algorithm adapt change user behavior delicate balance adapting quickly legitimate user behavior reducing false alarms adapting quickly activity intruder thinking intruder behavior simply change behavior normal user computer missing actual intrusions simple fact life users behavior wide 
ranging changing time consistent user behavior accurately capture idiosyncrasies approach work experimental evaluation section reports experimental evaluation ids algorithm additional experiments reported detail shavlik shavlik results mentioned article methodology collected data employees shavlik technologies volunteered experimental subjects collected data weekdays experimental subjects training steps table train ids recognize differences behavior user replay user behavior false alarm alarm model user intrusion detected alarm model user users call users insiders view members small group co-workers remaining subjects total work days measurements serve simulated external intruders users computer-usage behavior training including computing denominator experimental subjects testing phase step table training tuning phases expects outsiders harder recognize intruders user computer behavior observed ids learning primary results discussion figure shows function table detection false-alarm rates scenario training lasts work days seconds tuning testing periods work days seconds train tune test sets temporally disjoint scenario involves five-week-long training process presented shavlik shavlik shorter training periods produce results good results averages insiders experimental subjects evaluated subjects insider intruders above-described outsider intruders resulting sets false-alarm detection rates averaged produce figure tuning phase table false-alarm rate step set extreme false-alarm rate produced tuning set due fact explicitly fit parameters tuning data false-alarm rate result testing rate expects fitting higher accuracies tuning data testing data due degrees freedom tuning phase arguably key issue machine learning central adaptive ids figure wide range window widths minutes false-alarm rates low eight-hour work day user intrusion-detection rates impressively high interestingly detection rate outsiders behavior training approximately insiders suggests learning algorithm good job learning characteristic user exploiting idiosyncratic differences user insiders based figure seconds reasonable setting fielded system subsequent experiments section noted sec figure completely features averages measurement seconds explained earlier article experiments examples user computer turned seconds replay window activity user user computer leakage user data back seconds fielded system seconds worth data user seconds user experimental setup support mixing user behavior fielded system sec simple solution average seconds seconds experiments expect impact change significant data point sec figure features involve seconds measurements point issue seconds measurements visited depth section percentage testset potentially confusing technical point clarified point eight-hour work day sixty-secondwide non-overlapping windows sixhundred-second-wide false alarm day sec corresponds false-alarm rate sec false-alarm rate produces falsealarm day average lower dotted line figure shows false-alarm rate produces false alarm day user figure increases actual number false alarms day decreases making call leads false alarms non-overlapping windows conversely increases intruder computer longer detected produce figure results table tuning step considered settings threshold mini threshold full combinations parameters experiment choices values number candidate parameter settings found restrict threshold mini cases figure sec sec table shows highest-weighted features end figure experiment weights averaged ten experimental subjects values create figure experimental subject setting figure false alarm detection rates window width seconds insider detection rate outsider detection rate false alarms false alarm day user normalize weights sum insuring configuration contributes equally remember weights changing table viewed representation snapshot appendix shavlik shavlik additional explanations features table features highest weights averaged experiments produced figure print jobs average previous values ranked print jobs average previous values system driver total bytes actual measured logon total actual measured print jobs actual measured lsass working set average previous values number semaphores average previous values calc elapsed time difference averages prev prev number semaphores actual measured lsass working set average previous values cmd handle count difference current average cmd handle count average previous values write bytes cache sec difference current average excel working set difference current average number semaphores average previous values cmd processor time difference averages prev prev lsass working set actual measured system driver total bytes average previous values cmd processor time difference current average cmd processor time difference current average system driver resident bytes actual measured excel handle count average previous values errors access permissions difference current average file write operations sec average previous values system driver resident bytes average previous values table measurements highest number occurrences top weights including ties experiments produced figure numbers parentheses percentages top appearances number semaphores logon total print jobs system driver total bytes cmd handle count system driver resident bytes excel handle count number mutexes errors access permissions files opened total tcp connections passive lsass working set lsass processor time system working set notepad processor time cmd working set packets sec datagrams received address errors excel working set msdev working set udp datagrams port sec winword working set file write operations sec bytes received sec bytes transmitted sec observe wide range features table relate network traffic measure file accesses refer programs relate load computer interesting notice features average values seconds important instantaneous values matter important change feature weakness table measured windows property important subjects high average weight table features play important roles produce table count measured property appears top weights including ties common training surprisingly half windows properties measure top list supports thesis monitor large number system properties order create behavioral model tailored individual computer user project final report displays longer additional lists highestweighted features including specific user derived calculations section regularly highly weighted features exception difference previous appears top weighted features noisy estimate smoothed difference current average difference -most factor additional results tables show features measurements windows property play important role figure illustrates performance table algorithm restrict features measurements window properties monitor y-axis test-set detection rate cases false-alarm rate meets goal user workday figure data case seconds days training data tuning testing experimental subject figure shows advantage features longer histories cost longer history data collected define feature histories back seconds minutes seconds intrusion feature values due solely intruder behavior appears limiting features seconds measurements good choice reported results average pool insiders outsiders interesting results individual experimental subjects table reports user detected intruding user computer cell row user column user probability detection user operates user computer seconds rightmost column detection rate outsiders operate insider computer detection rate -sec intrusions sound alarms expect individual penetration rates range results skewed cases attempted intrusions detected majority cells table fact report penetration rates detection rates table visually overwhelming cases user frequently detected operating user computer implication results table fielded system run 
experiments group users identify computer behavior sufficiently distinctive table algorithm effective protection number previous values figure detection rate function number previous values sec table percentage times user successfully intruded user machine sec columns range rows rightmost column reports rate successful intrusions set outsiders cells values left blank comparison bayes successful algorithm tasks bayes algorithm assumes features measurements case conditionally independent category estimates probability obtaining current set measurements categories intrusion versus normal behavior case applied bayes algorithm experimental setup evaluate table algorithm results obtain seconds detection rate average false alarms day user compares poorly table algorithm results identical scenario detection average false alarms day user fact started project bayes algorithm switched winnow-based approach realized bayes independence assumption severely violated create effective anomaly detector future work discuss extensions work reported fully discussed obvious extension obtain analyze data larger number users data collection server machines greatly beneficial data gathered actual intrusions simulating replaying user measurements user computer advantages data larger pool experimental subjects scaling issues addressed statistically justified confidence intervals results produced parameters tuned including hard-wired values current experiments apply winnow algorithm training phase step table remarkable accuracies seconds examples half called intrusion half consistently obtain numbers order missed intrusions false alarms starting features weighted equally winnow algorithm quickly pick characteristic user quickly adjust user behavior fact rapid adaptation curse previously discussed section intruder immediately detected normal user computer mini-alarms seconds sounding alarm calling recent measurements normal applying winnow measurements assumption normal user behavior mini-alarms occur intruders number mini-alarms produced exceed feel close fully exploiting power winnow algorithm intrusiondetection task tinkering algorithmic variations closer detection rates false alarms section winnow-based algorithm estimate probability current feature make simple yes-no call close estimated probability threshold extremely low probability impact threshold often-successful bayes algorithm actual probabilities calculations worthwhile ways combining weights winnow actual thresholded probabilities main algorithm table condition probabilities features measured lead informative probabilities performance simply prob file write operations sec valuable prob file write operations sec word recent cycles read similarly winnow algorithm select good pairs features alternatives computationally expensive domain expertise choose small subset combinations experiments article mix behavior normal user computer intruder case practice trivial combine sets windows measurements semantically meaningful simply add values feature cpu utilizations result thought devise plausible mix normal intruder behavior alternate approach run data-gathering software intrude computer simultaneously person results reported section tune parameters false alarms tuning data found testing data meet goal false alarm user day obtained test-set results week wanted obtain fewer false alarms techniques needed approach false alarms tuning set solution explored tune parameters false alarms increase stringency parameters require number mini-alarms needed tuning-set false alarms evaluation similar approaches needed collected windows event-log data set shavlik technologies employees decided data experiments data people intrude computer interesting event-log data generated approach simulating intruders result generation meaningful event-log entries failed logins type measurement promising monitor specific addresses involved traffic computer possibly interesting variables compute include number addresses visited seconds number time visited addresses seconds differences incoming outgoing addresses final future research topic extend approaches article local networks computers statistics behavior set computers monitored intrusion attempts anomalous computer highly anomalous behavior set machines conclusion approach creating effective intrusion-detection system ids continually gather analyze hundreds finegrained measurements windows hypothesis successfully tested properly automatically chosen set measurements provide fingerprint unique user serving accurately recognize abnormal usage computer provide insights system measurements play valuable roles creating statistical profiles users tables experiments high intrusion-detection rates low false-alarm rates stealing cpu cycles importance low false-alarm rates warnings ids disregarded specific key lessons learned valuable large number properties measure features play important role capturing idiosyncratic behavior user table continually reweight importance feature measured users behavior efficiently accomplished winnow algorithm features involve instantaneous measurements difference current measurement average seconds tune parameters per-user basis number mini alarms seconds needed trigger actual alarm tune parameters tuning datasets estimate future performance measuring detection falsealarm rates separate testing set performance data train tune learner unrealistically high estimates future performance tune false alarms variance detection rates users missed intrusions intrusions missed suggests users servers approach highly effective anomaly-based ids present expected play sole intrusion-detection role systems nicely complement ids patterns abuse misuse strategies arising anomaly-based approaches provide excellent opportunity detect internal details latest intrusion strategy fully understood acknowledgments employees shavlik technologies volunteered data gathered personal computers michael skroch encouraging undertake project michael fahland programming support data-collection process finally anonymous reviewers insightful comments research supported darpa insider threat active profiling itap program atias program agarwal joshi pnrule framework learning classifier models data mining case-study network intrusion detection proc siam intl conf data mining anderson computer security threat monitoring surveillance anderson company technical report fort washington darpa research development initiatives focused preventing detecting responding insider misuse critical defense information systems darpa workshop report ghosh schwartzbard schatz learning program behavior profiles intrusion detection usenix workshop intrusion detection network monitoring april lane brodley approaches online learning concept drift user identification computer security proc kdd lazarevic ertoz ozgur srivastava kumar comparative study anomaly detection schemes network intrusion detection proc siam conf data mining lee stolfo mok data mining framework building intrusion detection models proc ieee symp security privacy littlestone learning quickly irrelevant attributes abound machine learning lunt survey intrusion detection techniques computers security mitchell machine learning mcgraw-hill neumann challenges insider misuse sri computer science lab technical report shavlik shavlik final project report darpa insider threat active profiling itap program april warrender forrest pearlmutter detecting intrusions system calls ieee symposium security privacy 
framework set-oriented computation inductive logic programming application generalizing inverse entailment ector corrada bravo david page raghu ramakrishnan jude shavlik vitor santos costa department computer sciences department biostatistics medical informatics wisconsin-madison usa coppe sistemas ufrj brasil hcorrada raghu shavlik wisc page vitor biostat wisc abstract wepropose approach inductivelogic programming systematically exploits caching offers number advantages current systems avoids redundantcomputation amenable set-oriented generation evaluation hypotheses relational dbms technology easily applied ilp systems approach opens avenues probabilistically scoring rules search generation probabilistic rules benefits ilp framework propose scheme defining hypothesis search space inverse entailment multiple seeds introduction goal inductive logic programming ilp autonomously learn first-order logic programs model relational data current approach ilp limitations scalability computational efficiency recent efforts extend ideas relational database query optimization setting line present formulation ilp systematically exploits caching achieve greater efficiency flexibility present theoretical results characterize fundamental building blocks approach data structure extension operation hypotheses expose exploit opportunities caching results previous computation benefit avoiding redundant computation pervasive standard ilp search score paradigm extension operation formulated set-oriented computational strategy defined terms extended relational work supported air force grant darpa isto grant ford fellowship national academy sciences database operations facilitating relational ilp systems extensional representations hypotheses treated first-class objects statistics derivedfromthese objects easily maintained andcan define alternatives guide search process ilp probabilistic methods search directly statistics derived data structure representing hypotheses additionally statistics derived extensional representation hypotheses offer avenues learning class rules richer class horn clauses rules statements aggregates rules probabilistic statements statements missing values generated extensions scope paper investigate scheme restricting hypothesis search space inverse entailment based set multiple seed examples algorithm generalized inverse entailment offers flexibility robustness hypothesis space restriction including alternative seed-coverage measures study paper cost-based measures readily obtained hypothesis representation main contributions data representation extension-join operation section discussion potential benefits section set-oriented hypothesis generation framework proof soundness completeness respect inverse entailment subsumption single-seed case section generalization inverse entailment multiple-seeds case extension hypothesis generation framework case proof soundness completeness respect generalized coverage measure section mode-restricted languages ilp task consists learning logic programthat models dataset ground facts disjoint sets positive examples negative examples background knowledge form additional facts predicates defined horn clauses learned program set horn clauses added background knowledge entails positive facts entailing negative facts clause hypothesis learned program built predicates background knowledge assume functorfree horn clauses section define space hypotheses seek represent borrow concept user-specified modes constrain space allowable hypotheses aleph progol ilp systems definition mode defined n-ary background predicate list arguments bound list arguments free restrictions disjoint sets argument predicate bound free bound argument include case modes arguments constants results apply case definition moded literal adornment literal binding pattern mode background predicate literal mode defined qbf allowed moded literal argument bound input argument usual ilp nomenclature free output argument rest paper treat arguments literals implicit details required denote moded literals convenience define operations moded literals moded literal mode bound free vars pred set modes denote set allowable moded literals assume literal hypothesis moded literal leave adornment implicit needed set allowable moded literals mode-restricted set hypotheses defined recursively definition set allowable moded literalsm target predicate set hypotheses allowable mode-restricted language recursively defined true bound vars set variables vars hypothesis union variable sets literals vars vars uniontextni vars positive facts model instances target predicate background predicates modes adornments rbf qfb rbf qfb rbf qfb negationslash set definition set hypothesis capture representation wild representation seek represent hypotheses intuitively result operating hypothesis reused operating extension hypothesis instance measuring coverage hypothesis substitution found proving hypothesis covers bindings potentially make extension hypothesis cover representation reuse bindings measuring coverage extended hypothesis objective statistics hypothesis easy derive maintain representation hypotheses continuing coverage hypothesis easily recoverable data structure interested maintaining statistics measures coverage define data representation extension operation hypotheses meet goals definition hypothesis represented pair database table schema fid unique existing tables row identifier fid unique identifier parent row variable names appearing row binding makes cover fact set seeds schema serves share common subsets bindings hypotheses fid fields set seeds initial table built fid null seed represented row hypothesis qbf seed table shown fig built seed set facts table background predicate represented intensional extensional pair qbf shown fig fid null null fid fig initial table table pair qbf continuing facts base table background predicate represent hypothesis qbf rbf pair shown fig illustration avoid indirection store variable bindings seed row significant savings obtained storing shared bindings redundantly pair hypotheses extensions including share bindings variables appearing table chained tables bindings stored extensions refer bindings indirection eachnew table store copy table bindings unique row ids tables unambiguously reconstructed chained tables running reconstructed table shown fig fid null null fid fid fig table pair reconstructed version extension-join representation formulate hypothesis extension stylized join operation database tables operation takes input intensional extensional pairs hypothesis pair definition pred consists moded literal base table pred result operation hypothesis pair substitution extended moded literal make hypothesis cover seed retained table denote operation pred number steps significant equi-join input tables remaining steps bookkeeping set equi-join capture proper variable bindings extension extension-join defined algorithm algorithm extension-join operation input hypothesis pair extension pred output extended hypothesis pair x-join pred compute projection build join constraints result projection list execute join result projection result empty make table set output present details steps running create pair calculate rbf pred shown fig table pred facts compute projection project input hypothesis table columns bound arguments extension fid fields gather variable bindings fid columns required chaining rows result rows projected table shown fig extension rbf column needed bound argument rbf fid fid pred fig table projection bound arguments extension result equi-join pred projection identifier free argument columns principle extensions hypothesis require bound variables input hypothesis table result extension-joining extensions input hypothesis computed simultaneously single projection input hypothesis table step extension-join operation permits set-oriented optimizations kind build join constraints result projection list step finds common bindings input hypothesis extension input moded literal bindings expressed constraints equi-join operation result projected columns required chaining variables pair operands list join constraints form constructed variable column column base table current pred column pred bound argument variable 
assigned column rbf alist ofcolumn namesl constructedas fid columns base table join constraints list fid pred column pred involved constraint list execute join projection result previous step execute equi-join input tables operation defined relational algebra expression equi-join constraints list projection columns listed effect extending substitutions input table bindings input base predicate running result fid pred pred pred shown fig result empty build table step transforms result previous step conforms hypothesis schema chains rows rows making proper entries fid column table column names table derived moded literal unique generated row result final result shown fig definition extension-join algorithm hypothesis tables unique identifier row refer unique identifier parent row seed table row seed seed set row identifier eid uniquely seed row subsequent tables seed row identifier eid seed table fid links define selection operation denoted projection operation denoted row identifiers definition pred pred seed table built seed set reconstruction fid fields projection identifiers relational algebra projection operator eid selection rows involving seed eid row identifier seed initial table relational algebra selection operator property selectionon examples hypothesis table pushed selection original table seeds formalize lemma lemma true pred pred seed table built seed set pred pred selection operation definition proof eid unique row identifier seed table tuple snegationslash result joining tuples pred definition extension-join eid definition extension-join eid negationslash definition extension-join implies negationslash contradiction established eid conversely tuple negationslash result joining tuples pred definition extension-join implies definition extension-join eid definition extension-join implies eid shown eid snegationslash contradiction worth noting existing work addresses issues present data structures algorithms testing -subsumption store multiple substitutions compactly avoid backtracking finding satisfying substitutions compact representation make maintaining statistics type discuss difficult hand data structure storing multiple substitutions logan-h system reconstructed tables discuss store information redundantly techniques store coverage lists computed answers meet goals storing coverage lists technique tuple-id propagation compact storage fast retrieval statistics determine coverage measures hypothesis classification setting types statistics involving coverage probabilistic models easily derivable present formulation seeks balance goals caching availability general statistics benefits wild representation identify general areas representation offers advantages current search score paradigm ilp framework efficiency scalability flexibility framework easily adapts settings learning theories languages horn clauses desired discuss benefits caching benefits table bindings required determine coverageproperties hypothesis respect seed table bindings cached pair reused determine coverage properties extensions exploited context search-space restriction section set-oriented hypothesis extension extension-join operation carried efficiently relational database system defined terms relational operations ilp potentially carried disk-resident data set-oriented optimizations performed tuple level extension-join instance earlier optimization extension-join hypothesis set extensions modes share base table executed set-oriented fashion optimization spirit query packs presented blockeel alternative search methods cached table hypothesis pair maintain statistics defining hypothesis search-space exploring space paper present application statistics extensional tables restrict space allowable hypotheses stochastic search methods prior distributions space guide search parts space hypothesis space generated defined representation informative prior derived coverage statistics derivable cached tables similarly estimates property hypotheses guide search estimates coverage hypothesis parts hypothesis space explore representation estimates derived cached table resulting extension-join background predicates representative set seeds method representation estimate efficiently hypotheses evaluated struyf blockeel estimate prior selectivity literals decide efficient literal reordering hypothesis statistics derived representation provide good estimates selectivity literal language extensions framework permits learning rules languages sets horn clauses statistics cached table hypothesis train statistical model infers missing values instances similar datasets formulation clp language easily incorporated hypothesis framework alternatively statistics cached table make distributional statements variables hypothesis estimate distribution column target predicate determine correlation subsets columns background knowledge statistics derived cached tables statements type rich people tend live big houses made learned program extensions datalog language proposed add ability group constants calculate aggregates groups cached tables groups defined aggregates calculated fly learning process statements aggregates vens datalog extension negated literals clauses allowed requirement programs stratified respect negation assume setoriented bottom-up evaluation strategy system expect learn stratified programs negated literals remainder paper presents initial benefits wild representation avenues opens defining ilp task space allowable hypotheses defined background knowledge multiple facts target predicate initial application aleph progol systems restrict set hypotheses search space inverse entailment specific seed chosen generate set literals bottom clause finding facts background knowledge relevant chosen seed space hypotheses restricted include generalizations bottom clause hypotheses generated cover seed process seeks restrict search hypotheses seed defines space hypotheses respect seed presence noise data restricting search space based single seed potentially wasteful instance suppose positive examples mislabelled fact negative examples examples seeds restrict search space hypotheses cover negative examples hypothesis evaluation based coverage find possibly non-existent specific clause differentiates negative examples search parameters alleviate minimum positive coverage constraint principled method avoids phenomenon defining search space usefulness restriction imposed search space defined terms multiple seeds effect unfavorable choice seed mitigated true representative seeds set provide degree freedom space restriction defined terms multiple seeds show hypothesis generation strategy wild representation meets goals generalizing inverse entailment multiple seeds describe inverse entailment detail show wild representation define hypothesis space specifically define space inverse entailment single-seed case finally define generalization inverse entailment show defines class usefulness restrictions imposed hypothesis space inverse entailment inverse entailment constructs set literals defines allowable hypotheses search space practice construction partitioning approach partition constants appearing seed step iteration instances constants current partition found background predicate binding patterns defined modes ground literal instances added bottom clause constants appearing literals added partition argument free mode repeated constants added partition user-defined bound number iterations performed met finalize ground literals bottom clause variabilized set modes leave bound hypothesis length clarity easily implemented allowable hypotheses search space valid ordered subsets literals bottom clause literals extend hypothesis appears bottom clause variables appearing bound arguments hypothesis extended subset built bottom clause defined definition set allowable moded literals target predicate seed depth bound background knowledge bottom clause built seed define set hypotheses generated true bound vars remark stated previously hypotheses set cover seed muggleton proved inverse entailment complete -subsumption depth bound relaxed wild hypothesis generation process inverse entailment generalized set-oriented formulation single seed restrict search space set examples filter function determines candidate hypotheses included search space generalized version inverse entailment original aleph progol 
version benefits bottomup computation representation extension-join operation section propose algorithm generating hypotheses algorithm wild hypothesis generator input set allowable moded literals target predicate seed fact set depth bound background knowledgeb filter-function output set hypotheses generateh openset true built seed set output true openset empty choose remove openset foreach moded predicate valid extension compute pred true output allowable hypothesis depth add openset valid extensions set bound vars arguments marked bound assigned variable appearing hypothesis extended step depth defined muggleton easily obtained define true depth depth table columns variables appearing depth wild hypothesis generator formulate strategy enumerate set hypotheses generated inverse entailment single-seed case make seed set singleton set set filter function table input return true table empty denote emptiness-testing function empty prove proposition parameters generate complete sound set hypotheses cover single seed proposition soundness completeness wild generation single seed generateh empty hypothesis covers exists table proof proceeds induction number literals form true covers unique substitution maps variables constants result applying substitution literal construction step algorithm direction proposition true assume covers exists table show extension covers pair covers covers inductive hypothesis table covers exists substitution tuple pred specifically tuple pred pred due tuple pred definition extension-join tuple empty true implies desired prove pair covers proceed induction number literals true construction built constants appearing build substitution makes cover direction claim true assume covers show extension pred covers inductive hypothesis substitution tuple pred empty true tuple build substitution domain consisting variables appearing corresponds arguments free bind variables constants appearing arguments tuple bound subset domain build substitution result tuple pred finally due definition equi-join tuple pred implies covers desired corollary proposition remark corollary wild generates inverse entailment space single seed generateh empty definition exits table proof trivially proposition remark sets subset cover single seed generalized inverse entailment present scheme generalizing inverse entailment multiple seeds parameters wild generation algorithm restrict set allowable hypotheses specifically filter function generation hypotheses meet coverage criteria inverse entailment generalized sense criterion restriction inverse entailment hypotheses cover single seed class measures coverage hypotheses set seed examples class measures implemented filter function wild generation algorithm set seed examples denote subset seeds covered hypothesis definition set hypotheses generated inverse entailment single seed definition intuitively hypothesis generated bottom clause built seed equivalent stating due remark covers define filter functions propose evaluate paper subset define intersection int true claim proposition hypotheses generated wild hypothesis generator filter hypotheses generated bottom clause built turn seed resulting space intersection spaces defined set aleph bottom clauses support true threshold borrow concept frequent itemset relational pattern mining algorithms generalize coverage assumptions inverse entailment introduces extra parameter determine amount filtering apply claim proposition hypotheses generated filter cover seed examples notice equivalent int notice functions monotonic size seed subset return true entire set formalize definition properties whenproving ourmain resultin proposition definition proper filter function set seed examples filter function proper true superset true true table intensional extensional pair projection identifiers definition present main result generalized inverse entailment states wild generator produce hypotheses meet criteria imposed filter function generates hypotheses deem respect set seed examples proposition soundness completeness wild generation multiple seeds generateh set hypotheses generated wild generator seed set proper filter definition true exists table lemmas prove result stated proven present proof proposition denote generateh empty set hypotheses proposition set hypotheses generated wild hypothesis generator single seed emptiness-testing function empty denote set definition set hypotheses generated inverse entailment single seed recall corollary sets equal selection result lemma reasonabout pairs built multiple seeds terms single-seed space state lemma conditions projection seed row identifiers identifier show occurs pair single-seed space equivalently due proposition covers selection rows involving seed defined definition lemma pred pred seed table built seed set section eid unique row identifier seed table eid proof definition empty negationslash implies eid proceed induction number literals body construction direction lemma true assume pred pred seed table built seed set section eid implies pred eid show eid selection negationslash definition extension-join implies negationslash eid inductive hypothesis pair negationslash desired result set row identifiers examples seed set set identifiers subset examples seed set uniquely lemma shows conditions hypothesis pair set identifiers mapped set examples coveredby denote relationship mapping lead directly desired result proposition issues presented filter function addressing effects bulk proof proposition lemma pred pred seed table built seed set section eid unique row identifier seed table eid projection table definition set examples covered hypothesis definition proof definition proposition exists table implies lemma eid projection seed identifiers lemma eid implies proposition implies definition covers single seed proveproposition proceeding induction number literals hypothesis note hypothesis extension hypothesis set examples covered subset examples covered monotonicity proper filter function reason result applying set examples covered finally lemmas provide mapping set examples covered hypothesis examples present table resulting chain multiple extension-joins starting seed table proof pred pred seed table built seed set section true lemma true true proceed induction number literals body true construction direction proposition true assume true implies table true show table monotonicity true true inductive hypothesis exists pred pred table built seed set section true pred lemma implies true implies desired case strict intersection filter function proposition states hypotheses cover seed generated hand case support filter function hypotheses cover required number seeds generated future work experimentally evaluate effect alternative settings parameters exposed hypothesis space restriction framework observe effect accuracy learned hypothesis found spaces restricted generalized inverse entailment determining effect support thresholds accuracy important determining robust approach sampling effects compared inverse entailment test conjecture effect caused bad choice single seed fact mitigated proposed framework characterizing types datasets benefit approach enlightening conclusion presented framework ilp exploits caching avoids redundant computation framework built data structure hypothesis extension operation makes opportunities caching explicit presented structure defined extension operation terms relational database operations suggesting incorporate ilp relational database environment discussed current methods seek improve efficiency alternative search definition directly benefit framework presented addition variants search restriction strategy direct results framework discussed variant generalizes inverse entailment multiple seeds presented theoretical results offer foundation generalization finally framework enables learn theories languages sets horn clauses including theories make probabilistic statements statements aggregates negation directions mentioned holds 
potential significant improvement aspect ilp work paper step opens promising avenues future research dzeroski lavrac eds relational data mining springer-verlag york blockeel sebag scalability efficiency multi-relational data mining sigkdd explor newsl struyf blockeel query optimization inductive logic programming reordering literals ilp bockhorst ong foil-d efficiently scaling foil multi-relational data mining large datasets ilp blockeel dehaspe demoen janssens ramon vandecasteele improving efficiency inductive logic programming query packs artif intell res jair costa srinivasan camacho blockeel demoen janssens struyf vandecasteele laer query transformations improving efficiency ilp systems journal machine learning research cussens prior probabilities density estimation relational classification ilp zelezn srinivasan page monte carlo study randomised restarted search ilp ilp dimaio shavlik learning approximation inductive logic programming clause evaluation ilp vens assche blockeel dzeroski order random forests complex aggregates ilp costa page qazi cussens clp constraint logic programming probabilistic knowledge international conference uncertainty artificial intelligence srivasanan aleph manual source code http web comlab oucl research areas machlearn aleph aleph html muggleton inverse entailment progol generation comput ramakrishnan gehrke database management systems edn mcgraw-hill ferilli mauro basile esposito theta-subsumption resolution algorithm zhong ras tsumoto suzuki eds ismis volume lecture notes computer science springer mauro basile ferilli esposito fanizzi exhaustive matching procedure improvement learning efficiency maloberti sebag fast theta-subsumption constraint satisfaction algorithms machine learning arias khardon bottom-up ilp large refinement steps camacho king srinivasan eds ilp volume lecture notes computer science springer fonseca rocha camacho silva efficient data structures inductive logic programming yin han yang crossmine efficient classification multiple database relations icde ieee computer society ramakrishnan srivastava sudarshan efficient bottom-up evaluation logic programs dewilde vandewalle eds computer systems software engineering state-of-the-art kluwer academic publishers agrawal mannila srikant toivonen verkamo fast discovery association rules advances knowledge discovery data mining aaai mit press dehaspe toivonen discovery frequent datalog patterns data min knowl discov horv ath inductive logic programming international conference ilp szeged hungary september -october proceedings horv ath ilp volume lecture notes computer science springer 

applying theory revision design distributed databases fernanda bai marta mattoso jude shavlik gerson zaverucha department computer science coppe federal rio janeiro ufrj box rio janeiro brazil baiao marta gerson cos ufrj computer sciences department wisconsin-madison west dayton street madison usa shavlik wisc abstract work presents application theory revision design distributed databases automatically revise heuristic-based algorithm called analysis algorithm forte system analysis algorithm decides fragmentation technique class database prolog implementation provided initial domain theory fragmentation schemas previously performance obtained experimental results top object database benchmark provided set examples show effectiveness approach finding fragmentation schemas improved performance introduction distributed parallel processing database management systems efficient ways improving performance applications manipulate large volumes data accomplished removing irrelevant data accessed execution queries reducing data exchange sites main goals design distributed databases order improve performance applications important design information distribution properly distribution design involves making decisions fragmentation placement data sites computer network phase distribution design fragmentation phase focus work fragment class objects basic techniques horizontal vertical fragmentation combined applied ways define final fragmentation schema class fragmentation problem design distributed database np-hard problem number works literature addressing horizontal vertical class fragmentation technique designer decides horizontal fragmentation algorithm class vertical fragmentation algorithm class left assistance make decision previous work proposed set heuristics drive choice fragmentation technique applied class database schema heuristics implemented algorithm called analysis algorithm incorporated methodology includes analysis algorithm horizontal vertical class fragmentation algorithms adapted literature experimental results reported show applications executed times faster applying fragmentation schema resulted methodology compared alternative fragmentation schemas proposed works literature experimental results real applications continuously provide heuristics design distributed object databases ddodb incorporated analysis algorithm manually improve analysis algorithm experimental results required detailed analysis result manual modifications analysis algorithm formalization heuristics experiments incorporation analysis algorithm maintaining previous heuristics consistent proved increasingly difficult task work proposes theory revision design distributed databases trend showing automatically improves analysis algorithm forte system trend module framework handles class fragmentation problem design distributed databases defined approaches literature addressing ddodb problem addresses problem choosing adequate fragmentation technique applied class database schema works applying machine learning techniques solve database problems present approach inductive design deductive databases based database instances define intentional predicates relational bayesian networks estimate query selectivity query processor predict structure relational databases design distributed databases application theory revision approach paper organized section design distributed databases defined framework design distributed databases theory revision briefly reviewed section section show improve ddodb analysis algorithm forte system experimental results top benchmark presented section finally section presents conclusions future work framework design distributed databases section defines problem designing distributed database focusing object-oriented model presents framework propose class fragmentation phase distribution design design distributed databases distribution design database makes decisions fragmentation placement data sites computer network phase distribution design fragmentation phase process isolating fragments specific data accessed relevant applications run database object-oriented database data represented objects set objects sharing structure behavior define class classes related relationships database schema describes set classes relationships uml diagram representing database schema benchmark illustrated figure benchmark generic application top database design objects assembled composition parts notice composite part related atomic parts parts relationship atomic part part composite part baseassembly designobject type builddate document title text compositepart compshared compprivate documentation atomicpart parts rootpart connection type length isrootpart ispartof documents baseshared baseprivate fig benchmark database schema schema database distributed distribution design methodology capture set operations database quantitative information order define fragmentation schema applied database schema goal fragmentation phase operations captured decomposing application running database classified selection projection navigation operations definitions quantitative information needed includes cardinality class estimated size small medium large execution frequency operation fragmentation schema composed choice fragmentation technique definition set fragments class database schema basic fragmentation techniques applied class horizontal vertical fragmentation vertical fragmentation breaks class logical structure attributes methods distributes fragments horizontal fragmentation distributes class instances fragments horizontal fragment class subset class extension horizontal fragmentation subdivided primary derived horizontal fragmentation primary horizontal fragmentation basically optimizes selection projection operations derived horizontal fragmentation addresses relationships classes improves performance navigation operations apply vertical primary horizontal fragmentation techniques class simultaneously call hybrid fragmentation apply fragmentation techniques classes database schema call mixed fragmentation object oriented data model additional issues contribute increase difficulty class fragmentation turn complex problem previous work proposed set heuristics implemented algorithm called analysis algorithm examples heuristics proposed case selection operation class large cardinality class primary horizontal fragmentation case projection operation class large cardinality derived horizontally fragmented class vertical fragmentation algorithm capable handling conflicts fragmentation schema definition framework class fragmentation problem ddodb framework propose class fragmentation problem design distributed databases integrates modules ddodb heuristic module theory revision module trend ddodb branch-and-bound module figure improved analysis algorithm revised theory database application semantics operations quantitative info distribution designer optimal fragmentation schema optimal fragmentation schema examples ddodb branch bound module query processing cost function ddodb heuristic module good fragmentation schema analysis algorithm initial theory forte moduleforte trend module fragmentation schemas examples ood ood fig framework class fragmentation ddodb distribution designer input information database schema semantics classes relationships additional quantitative information estimated cardinality class applications projection selection navigation operations executed database information passed ddodb heuristic module ddodb heuristic module defines set heuristics design adequate fragmentation schema database application execution heuristic module algorithms analysis algorithm vertical fragmentation horizontal fragmentation follow set heuristics quickly output adequate fragmentation schema distribution designer previous results heuristic module presented set heuristics implemented ddodb heuristic module automatically improved executing theory revision process inductive logic programming ilp process called theory revision design distributed databases trend improvement process carried providing input parameters trend module prolog implementation analysis algorithm representing initial theory background knowledge fragmentation schemas previously performances representing set examples analysis algorithm automatically modified theory revision system called forte produce revised theory revised theory represent improved analysis algorithm output fragmentation schema improved performance revised analysis algorithm substitute original ddodb heuristic module pointed machine learning algorithms background knowledge combining inductive analytical mechanisms obtain benefits approaches generalization accuracy smaller number required training examples explanation capability additionally input information distribution designer passed module ddodb branch-and-bound module module represents alternative approach heuristic module obtains high execution cost fragmentation schema database application branchand-bound procedure searches optimal 
solution space potentially good fragmentation schemas application outputs result distribution designer algorithm bounds search fragmentation schema query processing cost function evaluation fragmentation schema hypotheses space cost function defined responsible estimating execution cost queries top distributed database resulting fragmentation schema generated heuristic module bound evaluations fragmentation schemas presenting higher estimated costs finally resulting fragmentation schema generated branch-andbound algorithm fragmentation schemas discarded search generate examples positive negative trend module incorporating branch-and-bound results ddodb heuristic module theory revision theory revision task problem finding minimal modification initial theory correctly classifies set training examples formally defined shown figure target concept set positive instances set negative instances hypothesis language initial theory expressed describing find revised theory expressed minimal modification correct instances fig theory revision task theory set function-free definite program clauses definite program clause clause form atomic formulae concept predicate theory examples training set instance instantiation necessarily ground concept instance concept cardinality cardinality connection large instance set facts gathers instances concept training set positive instance derivable theory augmented facts negative instances ddodb domain set facts define database schema definition classes cardinalities relationships specific type classes applications operations frequencies classifications accessed classes run database class atomicpart class compositepart cardinality atomicpart large cardinality compositepart small relationship rootpart relationshiptype rootpart relationshipaccess rootpart compositepart atomicpart operation classification projection accessedclasses atomicpart correctness theory defined set positive instances set negative instances theory correct instances holds revision process initial domain theory works performing set modifications order obtain correct revised theory modifications performed theory result applying revision operators make small syntactic correct revised theory obtained minimal modification initial theory achieved minimizing number operations performed requiring minimal modification initial theory assumed approximately correct revised theory semantically syntactically similar related works literature presented detailed comparison theory refinement systems literature concentrating theory revision systems general results theory-guided systems analysis included systems forte ptr author proposed framework classifying theory revision systems methodology evaluating algorithm identify location errors independently ability repair performance analysis forte system compared domains demonstrated searches larger space revised theories find accurate candidate ptr forte attempts repair revision points systems generates evaluates repair candidates forte system chosen perform improvement procedure ddodb algorithms forte order revision theories examples system automatically refining first-order horn-clause knowledge bases powerful representation language forte work domains involving relations ddodb domain forte theory revision system sense modifies incorrect knowledge applying identify repair strategy performs hill-climbing search hypothesis space applying revision operators specialization generalization initial domain theory attempt minimally modify order make consistent set training examples forte preserves initial theory revisions developed scored entire training set single instance forte direction revisions developed single instances details forte system found theory revision design distributed databases section proposes knowledge-based approach improving ddodb analysis algorithm theory revision goal applying knowledge-based approach automatically incorporate analysis algorithm required obtain fragmentation schemas improvements found additional experiments theory revision automatically reflect heuristics implicit results order apply forte system ddodb problem model represent relevant information ddodb domain adequate required forte basically included representing initial domain theory set examples initial domain theory trend approach analysis algorithm initial domain theory structure set rules presented figure complete prolog implementation analysis algorithm shown analysis phase analyze operation database database schema classes operations choose fragmentation method choose choose phf choose dhf fig structure set rules analysis algorithm forte assumes initial domain theory divided files fundamental domain theory fdt file predicates assumed correct initial theory revised thy file predicates subject revision process fundamental domain theory fdt file clause attributes relations examples defined dat file predicate explained clause object type database schema set applications objects attributes relations objects fixed represent information needed analysis algorithm revised forte responsible decomposing set examples create extensional definitions attributes relations objects accessed forte predicate illustrated figure fdt file predicates initial domain theory forte allowed revise illustrated figure predicate navigatesfromto figure defines operation navigates class class vice-versa path expression additionally create predicates isnotderivedfragmented due fact negated literals general logic programs addressed forte revision operators initial theory revised thy file predicates initial domain theory forte revise concepts analysis algorithm modified illustrated figure object types represent database schema classes relationships examples class class relationship relationship object types represent operations extracted applications examples operation operation attributes qualify object types examples attributes classes cardinality cardinality fragmentation fragmentation attribute relationships relationshiptype relationshiptype attributes operations frequency frequency classification classification relations object types examples relationshipaccess relationshipaccess operationaccess operationaccess navigates navigates predicates initial theory revised forte allowed revise isderivedfragmented fragmentation derivedfragmentation isnotderivedfragmented isderivedfragmented isverticallyfragmented fragmentation vertical isverticallyfragmented navigatesfromto operationaccess classpath member classpath member classpath navigates navigatesfromto operationaccess classpath member classpath member classpath navigates fig fundamental domain theory fdt classification navigation fdt navigatesfromto fdt relationshipaccess fdt relationship fdt relationshiptype fdt fdt isnotderivedfragmented fdt classification navigation fdt navigatesfromto fdt relationshipaccess fdt relationship fdt relationshiptype fdt fdt isnotderivedfragmented fdt classification navigation fdt navigatesfromto fdt relationshipaccess fdt relationship fdt relationshiptype fdt fdt isnotderivedfragmented ationmethod fdt classification selection fdt operationaccess fdt cardinality large fdt classification projection fdt operationaccess fdt cardinality large fdt isnotderivedfragmented fig initial theory revised intuitively clauses figure choose fragmentation technique derived horizontal primary horizontal vertical applied class database schema heuristics proposed hybrid fragmentation arises primary horizontal vertical fragmentations chosen clauses exclusive set examples essential information needed forte theory revision process set examples trend approach derived experimental results presented top page knowledge-based support vector machine classi ers glenn fung olvi mangasarian jude shavlik computer sciences department wisconsin madison gfung olvi shavlik wisc abstract prior knowledge form multiple polyhedral sets belonging categories introduced reformulation linear support vector machine classi resulting formulation leads linear program solved ciently real world examples dna sequencing breast cancer prognosis demonstrate ectiveness proposed method numerical results show improvement test set accuracy incorporation prior knowledge ordinary data-based linear support vector machine classi ers experiment shows linear classi based solely prior knowledge outperforms direct application prior knowledge rules classify data keywords nement prior knowledge support vector machines linear programming introduction support vector machines svms played major role classi cation problems unlike classi cation tools knowledge-based neural networks work incorporating prior knowledge support vector machines work present approach incorporating prior knowledge form polyhedral knowledge sets input space data knowledge sets simple cubes supposed belong categories data divided single knowledge set interpreted generalization training typically consists single point input space contrast knowledge sets consists region space powerful tool mathematical programming theorems alternative chapter embed prior data linear program ciently solved publicly solvers brie summarize contents paper section describe linear support vector machine classi give linear program describe prior knowledge form polyhedral knowledge sets belonging classes characterized section incorporate polyhedral sets linear programming formulation results knowledge-based support vector machine ksvm formulation formulation capable generating linear classi based real data prior knowledge section summary numerical results compare linear nonlinear classi ers incorporation prior knowledge section concludes paper page describe notation vectors column vectors transposed row vector prime scalar product vectors n-dimensional real space denoted vector sign function sign ned sign sign kxkp denotes p-norm notation signify real matrix matrix denote transpose denote i-th row vector real space arbitrary dimension denoted notation denote sum components vector zeros real space arbitrary dimension denoted identity matrix arbitrary dimension denoted separating plane respect point sets plane attempts separate halfspaces open halfspace points bounding plane set plane places closed halfspaces plane generates symbol denote logical abbreviation stands linear support vector machines prior knowledge problem depicted figure classifying points n-dimensional input space represented matrix membership point class speci diagonal matrix minus diagonal problem linear programming support vector machine linear kernel variant standard support vector machine linear program parameter min kwk denotes -norm ned introduction vector slack variables measuring empirical error characterize separating plane depicted figure problem linear program easily equivalent formulation min vector dimension economy notation rst formulation understanding computational implementation depicted figure normal bounding planes benchmark benchmark describes bound representative points object belonging oriented application sets constant object determines database management location relative origin classes strictly linearly separable error variable case shown figure plane bounds class points plane bounds class points aiw dii aiw dii plane midway bounding planes separating plane separates points belonging belonging completely approximately systems -norm term evaluate kwk performance centralized half environments reciprocal distance kwk measured performance results -norm top distance distributed databases bounding planes literature due security page commercial reasons represents choice figure maximizes fragmentation technique distance applied called class margin database maximizing schema margin positive enhances negative instances generalization generated capability support choices vector led good machine bad performance classes results linearly inseparable distributed database planes obtained bound total classes instances positive soft margin negative bound representation approximately error forte determined atomic nonnegative formula error variable positiveinstances aiw negativeinstances objects dii facts aiw positiveinstance negativeinstance dii list -norm positive error negative variable facts minimized concept parametrically learned weight objects resulting representation approximate application separating domain plane ddodb classi domain objects represented sign classes operations sign current application facts suppose facts prior fundamental information domain theory figure type shows points lying choosing vertical polyhedral fragmentation set determined class atomicpart linear inequalities benchmark application belong class analysis inequalities projection generalize operation simple box constraints figure atomicpart class designobject inequalities conclude implication baseassembly hold small compositepart knowledge small set lies side atomicpart medium bounding plane connection large accommodate case implication relationship satis componentsshared introduction slack error variables componentsprivate assuming implication holds rootpart equivalent parts solution statement turn implied statement solution operation simple backward projection implication facts suppose relationshipaccess contrary compshared baseassembly exists compositepart satisfying relationshipaccess obtain compprivate baseassembly contradiction compositepart relationshipaccess rootpart compositepart atomicpart relationshipaccess parts rst compositepart inequality atomicpart premultiplying relationshipaccess atomicpart fact connection natural relationshipaccess assumption atomicpart prior connection knowledge query set nonempty operationaccess forward atomicpart fig forte benchmark application implication true figure direct positive instance consequence nonhomogeneous term 
farkas atomicpart negative instances defined objects sets classes relationships operations application facts define existing relations application objects classes accessed relationship operations compose query classes accessed operation trend examples passed forte data file dat dat file examples forte learn defines execution parameters guide forte learning process complete description dat file benchmark experimental results section presents experimental results trend top benchmark showing effectiveness approach obtaining analysis algorithm produces fragmentation schema benchmark application due small amount examples overcome overfitting problem training applied k-fold cross validation approach split input data disjoint training test sets t-fold crossvalidation approach split training data disjoint training tuning sets revision algorithm monitors error respect tuning set revision keeping copy theory tuning set accuracy saved best-tuning-set-accuracy theory applied test set experimental methodology built forte random resampling adapted follow experiments executed run executed training set instances tuning set instances test set instances obtained revised theory final result runs besttuning-set-accuracy table shows results execution independent runs result refers revised theory proposed forte verified proposed revised ddodb theories identical represented final revised ddodb theory figure comparing definitions predicates figures verified revisions made forte rule addition rule added cardinality medium classification projection antecedent deletion antecedent fdt cardinality large removed rule fdt classification selection fdt operationaccess fdt cardinality large intuitively modifications show medium-sized classes subject vertical fragmentation case projection operation classes primary horizontal fragmentation independent size running versions analysis algorithm top benchmark application notice class atomicpart hybrid fragmentation primary horizontal vertical revision derived horizontal fragmentation illustrated table table summary forte execution output initial training accuracy initial test set accuracy final training accuracy final test set accuracy fdt classification navigation fdt navigatesfromto fdt relationshipaccess fdt relationship fdt relationshiptype fdt fdt isnotderivedfragmented fdt classification navigation fdt navigatesfromto fdt relationshipaccess fdt relationship fdt relationshiptype fdt fdt isnotderivedfragmented fdt classification navigation fdt navigatesfromto fdt relationshipaccess fdt relationship fdt relationshiptype fdt fdt isnotderivedfragmented fdt classification selection fdt operationaccess cardinality medium classification projection fdt classification projection fdt operationaccess fdt cardinality large fdt isnotderivedfragmented fig revised analysis algorithm table fragmentation techniques chosen versions analysis algorithm class initial version revised version baseassembly primary horizontal primary horizontal compositepart derived horizontal derived horizontal atomicpart derived horizontal hybrid connection derived horizontal derived horizontal compared costs resulting fragmentation schemas obtained initial revised versions analysis algorithm executing vertical horizontal fragmentation algorithms algorithms considered revision process costs calculated cost model assuming query optimizer choose efficient executing query choosing cost ve-pointer value-based join pointer-based join algorithms resulting costs illustrated figure cat initial analysis algorithm revised analysis algorithm cat fig comparing costs fragmentation schemas obtained initial revised analysis algorithm figure shows cost executing query benchmark application total cost benchmark application frequencies operation calculated cost cost cost cost cost cost produces costs versions analysis algorithm compared cost initialversion cost revisedversion results show effectiveness trend approach revising analysis algorithm obtaining version produced fragmentation schema reduced cost increased performance application conclusions heuristic algorithms address intractability class fragmentation problem design distributed database np-hard problem defined difficult improve manually defining incorporating heuristics experimental results maintaining previous consistent work presented knowledge-based approach automatically improving heuristic ddodb algorithm theory revision approach part framework handles class fragmentation problem design distributed databases proposed framework integrates modules ddodb heuristic module theory revision module called trend ddodb branch-and-bound module focus work apply trend automatically revise analysis algorithm heuristic module experimental results top benchmark presented examples revised algorithm led improvement system performance shows effectiveness approach finding fragmentation schema improved performance inductive logic programming future work include applying trend applications generation examples trend module branch-and-bound module address lack performance results top distributed databases literature intend enhance forte system deal negation failure extending ideas mentioned previous works group acknowledgements brazilian authors brazilian agencies cnpq faperj providing financial support work savio leandro aguiar helping implementation experimental methodology forte part work computer science department wisconsin madison usa authors fernanda bai gerson zaverucha leave ufrj bai methodology algorithms design distributed databases theory revision doctoral thesis computer science department coppe federal rio janeiro brazil technical report escoppe ufrj bai mattoso mixed fragmentation algorithm distributed object oriented databases proc int conf computing information icci winnipeg special issue journal computing information jci issn bai mattoso zaverucha inductive design distributed object oriented databases proc ifcis conf cooperative information systems coopis ieee press york usa bai mattoso zaverucha distribution design methodology object dbms submitted aug revised manuscript nov international journal distributed parallel theorem alternative theorem databases state equivalence key proposition knowledge-based approach proposition knowledge set classi cation set nonempty implication equivalent statement words set lies halfspace exists proof establish equivalence showing equivalence nonhomogeneous farkas theorem theorem equivalent solution solution alternative contradicts nonemptiness knowledgeset set solving contradiction ruled equivalent proposition play key role incorporating knowledge sets categories support vector classi formulation demonstrated section page margin figure linear svm separation points linear programming formulation linear svm separation points figure linear programming formulation incorporates knowledge sets halfspace halfspace depicted note substantial erence linear classi ers gures knowledge-based svm classi cation describe incorporate prior knowledge form polyhedral sets linear programming svm classi formulation assume knowledge sets sets belonging bix big sets belonging cix cig proposition relative bounding planes exist incorporate knowledge sets svm linear programming formulation classi adding conditions constraints min kwk linear programming formulation ensure knowledge sets bix big cix cig lie side bounding planes guarantee bounding planes exist precisely separate classes knowledge sets priori guarantee original points belonging sets linearly separable add error variables page slack error variable svm formulation attempt drive error variables modifying formulation min kwk nal knowledge-based linear programming formulation incorporates knowledge sets linear classi kluwer academic publishers bai mattoso zaverucha framework design distributed databases workshop distributed data structures wdas proceedings informatics series carleton scientific bellatreche simonet simonet vertical fragmentation distributed object database systems complex attributes methods proc int workshop database expert systems applications dexa ieee computer society zurich bellatreche karlapalem simonet algorithms support horizontal class partitioning object-oriented databases int journal distributed parallel databases kluwer academic publishers blockeel raedt inductive database design proc int symposium methodologies intelligent systems ismis blockeel raedt isidd interactive system inductive database design applied artificial intelligence brunk investigation knowledge intensive approaches concept learning theory refinement phd thesis california irvine usa brunk pazzani linguistically-based semantic bias theory revision proc int conf machine learning carey dewitt naughton benchmark proc acm sigmod washington chen implementation evaluation parallel query processing algorithms data partitioning heuristics object oriented databases int journal distributed parallel databases kluwer academic publishers ezeife barker comprehensive approach horizontal class fragmentation distributed object based system int journal distributed parallel databases kluwer academic publishers ezeife barker distributed object based design vertical fragmentation classes int journal distributed parallel databases kluwer academic publishers fogel zaverucha normal programs multiple predicate learning proc int conference inductive logic programming ilp madison july lnai springer verlag fung karlapalem object-oriented systems evaluation vertical class partitioning query processing object-oriented databases ieee transactions knowledge data engineering sep oct vol getoor friedman koller taskar probabilistic models relational structure proc int conf machine learning williamstown getoor taskar koller selectivity estimation probabilistic models proc acm sigmod santa barbara karlapalem navathe morsi issues distribution design object-oriented databases zsu eds distributed object management morgan kaufmann pub san francisco usa kohavi study cross-validation bootstrap accuracy estimation model selection proceedings ijcai koppel feldman segre bias-driven revision logical domain theories journal artificial intelligence research access foundation morgan kaufmann lima mattoso performance evaluation distribution 
oodbms case study proc int conf par dist computing systems pdcs isca-ieee dijon maier issues distributed object assembly zsu eds distributed object management morgan kaufmann publishers san francisco usa meyer mattoso parallel query processing shared-nothing object database server proc int meeting vector parallel processing vecpar porto portugal mitchell machine learning mcgraw-hill muggleton raedt inductive logic programming theory methods journal logic programming zsu valduriez principles distributed database systems jersey prentice-hall edition richards mooney refinement first-order horn-clause domain theories machine learning ruberg bai mattoso estimating costs path expression evaluation distributed object databases proceedings international conference database expert systems applications dexa lncs springer verlag savonnet terrasse tongnon fragtique methodology distributing object oriented databases proc int conf computing information icci winnipeg tavares victor mattoso parallel processing evaluation path expressions proc brazilian symposium databases sbc pessoa brazil wogulis approach repairing evaluating first-order theories multiple concepts negation thesis california irvine usa wrobel order theory refinement raedt advances inductive logic programming ios press 
weight empirical error term weight usual parameters chosen means tuning set extracted training set set linear program degenerates linear program ordinary linear svm set linear program generates linear svm strictly based knowledge sets speci training data paradigm situations training datasets easily expert knowledge doctors experience diagnosing diseases readily demonstrated breast cancer dataset section note -norm term kwk replaced half -norm squared kwk usual margin maximization term ordinary support vector machine classi ers linear program quadratic program typically takes longer time solve standard svms support vectors consist data points complement data points dropped problem changing separating plane knowledge-based linear programming formulation support vectors correspond data points rows matrix lagrange multipliers nonzero solving data points give answer solving entire matrix concept support vectors modi knowledge sets knowledge set represented matrix row matrices thought characterizing boundary plane knowledge set formulation rows wiped components variables optimal solution call complement components knowledge sets support constraints deleting constraints rows components alter solution knowledge-based linear program fact corroborated numerical tests carried deletion non-support constraints considered nement prior knowledge type nement prior knowledge occur separating plane intersects knowledge sets case plane added inequality knowledge set intersects illustrated demonstrate geometry incorporating knowledge sets synthetic points figure depicts ordinary linear separation linear svm formulation incorporate knowledge sets problem page belonging belonging solve linear program depict linear separation figure note substantial change generated linear separation incorporation knowledge sets 
note plane intersects knowledge set knowledge set ned numerical testing numerical tests detail carried dna promoter recognition dataset wisconsin prognostic breast cancer dataset wpbc ftp ftp wisc math-prog cpo-dataset machinelearn cancer wpbc brie summarize results rst dataset promoter recognition dataset domain dna sequence analysis promoter short dna sequence precedes gene sequence distinguished nonpromoter promoters important identifying starting locations genes long uncharacterized sequences dna prior knowledge dataset consists set prior rules matches examples training set rules serve classi capture signi information promoters incorporating classi results accurate classi prior rules converted straightforward manner knowledge sets methodology prior work tested algorithm dataset knowledge sets leave-one-out cross validation methodology entire training set elements repeatedly divided training set size test set size values ksvm svm obtained tuning procedure consisted varying square grid expressing prior knowledge form polyhedral sets applying ksvm obtained errors ksvm gave performance erent methods prior knowledge standard -norm support vector machine quinlan decision tree builder pebls nearest algorithm empirical method suggested biologist based collection lters promoter recognition neill method neural networks simple connected layer hidden units trained back-propagation ksvm svm results earlier report ksvm compared hybrid learning system maps problem speci prior knowledge represented propositional logic neural networks nes reformulated knowledge back propagation method knowledge based arti cial neural networks kbann kbann approach performed slightly algorithm obtained misclassi cations compared important note classi simpler linear classi sign neural network classi kbann considerably complex nonlinear classi note ksvm simpler implement kbann requires commonly linear programming solver addition ksvm linear support vector machine classi improves error ordinary linear -norm svm classi utilize prior knowledge sets dataset numerical tests wisconsin breast cancer prognosis dataset wpbc -month cuto predicting recurrence nonrecurrence disease prior knowledge utilized experiment consisted prognosis rules doctors depended features dataset tumor size feature diameter excised tumor page centimeters lymph node status refers number metastasized axillary lymph nodes feature rules recur nonrecur important note rules applied directly classify points training dataset correctly classify points remaining points classi rules rules applied classi classi cation accuracy rules doctors conjunction rules approach rules converted linear inequalities ksvm algorithm data linear program resulting linear classi -dimensional space ymph umor achieved accuracy ten-fold cross-validated test set correctness achieved standard svm data result remarkable knowledge-based formulation applied problems training data expert knowledge readily form knowledge sets fact makes method considerably erent previous hybrid methods kbann training examples needed order prior knowledge training data added knowledge-based formulation noticeable improvement obtained conclusion future directions proposed cient procedure incorporating prior knowledge form knowledge sets linear support vector machine classi combination dataset based solely knowledge sets promising approach handling prior knowledge worthy study ways handle simplify combinatorial nature incorporating prior knowledge linear inequalities class future applications problems training data easily expert knowledge readily form knowledge sets correspond solving knowledge based linear program typical type breast cancer prognosis knowledge sets generated linear classi good classi based data points incorporating prior knowledge powerful support vector machine classi ers concept support constraints discussed end section warrants study lead systematic simpli cation prior knowledge sets avenues research include knowledge sets characterized nonpolyhedral convex sets nonlinear kernels capable handling complex classi cation problems incorporation prior knowledge multiple instance learning lead improved classi ers eld acknowledgments research data mining institute report november supported nsf grants ccriri- cdaby afosr grant nlm grant microsoft auer learning multi-instance examples empirical evaluation theoretical approach pages bradley mangasarian feature selection concave minimization support vector machines shavlik editor machine learning proceedings fifteenth international conference icml pages san page francisco california morgan kaufmann ftp ftp wisc mathprog tech-reports cherkassky mulier learning data concepts theory methods john wiley sons york cost salzberg weighted nearest neighbor algorithm learning symbolic features machine learning dietterich lathrop lozano-perez solving multipleinstance problem axis-parallel rectangles arti cial intelligence fung mangasarian shavlik knowledge-based support vector machine classi ers technical report data mining institute computer sciences department wisconsin madison wisconsin november ftp ftp wisc pub dmi tech-reports girosi chan prior knowledge creation virtual examples rbf networks neural networks signal processing proceedings ieee-sp workshop pages york ieee signal processing society lee mangasarian wolberg survival-time classi cation breast cancer patients technical report data mining institute computer sciences department wisconsin madison wisconsin march computational optimization applications ftp ftp wisc pub dmi tech-reports mangasarian nonlinear programming siam philadelphia mangasarian arbitrary-norm separating plane operations research letters ftp ftp wisc math-prog tech-reports mangasarian generalized support vector machines smola bartlett sch olkopf schuurmans editors advances large margin classi ers pages cambridge mit press ftp ftp wisc math-prog tech-reports neill escherchia coli promoters concensus relates spacing class speci city repeat substructure dimensional organization journal biological chemistry quinlan induction decision trees volume rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland editors parallel distributed processing pages cambridge massachusetts mit press sch olkopf simard smola vapnik prior knowledge support vector kernels jordan kearns solla editors advances neural information processing systems pages cambridge mit press towell shavlik knowledge-based arti cial neural networks arti cial intelligence towell shavlik noordewier nement approximate domain theories knowledge-based arti cial neural networks proceedings eighth national conference arti cial intelligence aaaipages vapnik nature statistical learning theory springer york edition 
presented fourth international symposium recent advances intrusion detection raid evaluating software sensors actively profiling windows computer users jude shavlik computer sciences department wisconsin madison mark shavlik michael fahland shavlik technologies white bear lake jude mark mikef shavlik abstract report on-going intrusion-detection project empirically investigates usefulness stealing small amount cpu cycles main memory disk memory order continually gather analyze dozens fine-grained system measurements network traffic identity current programs executing user typing speed underlying scientific hypothesis properly chosen set measurements provide fingerprint unique user measurements serve distinguish computer misuse insiders introduction increasingly computerized networked world crucial develop defenses malicious insider activity information systems promising approach develop computer algorithms detect insiders inappropriately intruding computers intrusion detection difficult problem solve darpa system performance adversely effected false positives minimized intrusions caught false negatives low current state art intrusiondetection systems good false positives high successful detection rare intrusion-detection systems ids attack patterns adaptive software smart monitor learn system supposed work normal operation versus works misuse occurring lunt addressing approach specifically empirically determining sets fine-grained system measurements effective distinguishing usage assigned user computer misusage insiders darpa neumann organization building expertise windows computer security machine learning written extending prototype anomaly-detection system creates statistical profiles normal usage computer running windows significant deviations normal behavior intrusion occurring probability specific computer receives mbytes sec evenings measured raid low monitoring program detects high transfer rate evening hours suggest intrusion occurring ability create statistical models individual computer normal usage means computer unique characteristics serve protective role similar person antibodies distinguish cells invading organisms statistical-profile programs gather data normal operation computer learn distinguish behavior foreign behavior instance people heavily mouse interacting computer interface strongly prefer special keyboard characters control key leave computer unattended inappropriately access files individual differences people computer usage statistical-modeling program quickly recognize illegal access approach detect abnormal behavior computers operating http ftp servers project focuses computers humans addition determining informative real-time measurements indicative insider misuse secondary objective determine effective method combining multiple individual measurements single composite score indicative current threat insider misuse objective determine condition formal probabilistic sense individual measurements measured properties final objective empirical compare bayesian networks neural networks decision trees task measured properties learn accurate formal models identifying person computer ability detect insider misuse evaluated collecting data multiple users creating user profiles analyzing training subsets data experimentally judging accuracy applying profiles identify user produced sample set testing measurements empirically judge ability induced user profiles recognize specific user produced set measurements measuring tradeoff recognition false-positives rates experimental conditions key scientific hypothesis investigated creating statistical models user behavior accurately detect insider abuse focusing algorithmic development methods produce low false-alarm rates major reason system administrators ignore ids systems produce false alarms scientific questions addressed project include system user measurements informative recognizing computer running windows method combining multiple individual measurements single score indicative current insider threat expected true positive insider misuse detected false positive false alarms rates tradeoff curve true false positives shavlik shavlik fahland raid performance bayesian networks neural networks decision trees task learning predictive models collected statistics computational burden gathered statistics monitor insider misuse undertaking extensive empirical study scientifically answer questions real users ten employees shavlik technologies windows environment well-established experimental methodology machine-learning community measured system property developing algorithms choose measurements condition probability distribution measurement cpu cycles web browser function cpu load typing speed time day determine subsets measurements effective measurements make harder accurately find insider intrusion signal addition reducing number system properties measured lower cpu demands intrusion-detection system finally aim determine method combining fusing individual measurements composite score represents likelihood intrusion windows properties measure existing program collects real-time information windows sources performance monitor perfmon data event log monitoring user computer state information typing rates network traffic levels programs running specific system api invoked current commercial ids tools iss kane monitor packets log files combination provide information key ids system approach unique depend solely network traffic levels inaccurate busy times depend solely event logs time lag data-critical ids information audit logs data real-time detection system administrators turn event logging save performance disk space realizing severely hindering ids systems focus effectively utilizing rich sources information made advanced operating system windows topic prior experience context goecks windows perfmon utility number security items overlooked items accessed quickly categories looked include network performance disk activity process performance kernel usage event log monitoring key registry locations key system files login abnormalities suspect account addition invalid accesses key files registry entries monitored shavlik shavlik fahland raid window hooks facility intercept keystrokes monrose mouse events ids gather statistics typical usage patterns desktopcomputer user collect statistics things typing speed usage numeric keypad usage function cursor control keys speed mouse movement hypothesize keystroke mouse-usage information accurately recognize cases owner desktop computer secretary computer open area leaves lunch day securing computer items watched include running windows services common programs web browsers microsoft office editors program development tools lastly monitor calling kernel security functions list current user accounts calling functions highly suspicious activity common hackers part attacks preliminary results initial efforts focused data user keystrokes briefly report preliminary results future report precisely completely describe experimental studies set experiments ten experimental subjects employees shavlik technologies performing normal everyday activities implemented experimental approach works keystrokes user train separate statistical model user average users type keystrokes working day weeks activity keystrokes tune parameters explained separately user tuning data collected non-overlapping days create training data similarly testing data days separate training tuning data collected set keystrokes test set important proper experimental methodology separate data tune parameter estimate future accuracy tuning test set lead overestimates future accuracy person test set simulate person typing ten machines measure fraction times typing viewed coming owner machine intrusion flagged flagged intrusion person typing machine called false positive false alarm conversely flagged intrusion person typing person machine called true positive correct alarm goal high correct-alarm rate low false-alarm rate system administrators ignore intrusion-detection system generates false alarms shavlik shavlik fahland raid basic algorithm investigating works compute probability keystrokes including time keystrokes time key held released model learned machine normal user probability lower threshold mark keystroke marks keystrokes raise alarm algorithm automatically chooses 
settings person based optimizing accuracy tuning data set mentioned holding rate false alarms work day keystrokes average number keystrokes users typed day current accuracy results test data set function measuring accuracies testing data non-overlapping windows keystrokes order reduce correlation successive samples window width percentage intrusions detected false alarm rate work day encouraged recognize sizable fraction intrusions defined user typing user computer low false-alarm rate experiments type keystrokes types data collecting intruder lot damage keystrokes detecting large windows yesterday log files related work previous empirical studies investigated creating intrusion-detection systems monitoring properties computer systems idea back years anderson prior work focused unix systems world computers run variant microsoft windows addition prior studies looked large collection system measurements warrender ghosh lane brodley unix system calls lee audit data tcp program shavlik shavlik fahland raid summary recap basic plan project measure properties windows usage users times day save statistics labeled user produced time day run extensive empirical studies separate training tuning testing sets measured system property choose condition probability distribution give measurement cpu cycles ftp program function cpu load user typing speed time day determine subsets measurements effective measurements make harder accurately find insider intrusion signal addition reducing number system properties measured reduce cpu demands ids determine method combining fusing individual measurements composite score represents likelihood intrusion initial studies measurements users keystrokes including identity key pressed time successive key presses duration pressing releasing key measurements obtained encouraging results extending set windows measurements algorithms experimental studies acknowledgements project supported darpa award -cany opinions expressed article reflect authors necessarily government shavlik shavlik fahland raid anderson anderson computer security threat monitoring surveillance anderson company technical report fort washington darpa research development initiatives focused preventing detecting responding insider misuse critical defense information systems workshop report october http csl sri insider-misuse goecks goecks shavlik automatically labeling web pages based normal user actions proc intl conf intelligent user interfaces orleans http wisc shavlik abstracts goecks-iui doc abstract html gosh ghosh schwartzbard schatz learning program behavior profiles intrusion detection usenix workshop intrusion detection network monitoring april ftp ftp rstcorp pub papers usenix lane lane brodley approaches online learning concept drift user identification computer security intl conf knowledge discovery data mining york http mow ecn purdue brodley my-papers terran-kdd lee lee stolfo mok data mining framework building intrusion detection models proc ieee symp security privacy http columbia sal hpapers ieee lunt lunt survey intrusion detection techniques computers security monrose monrose rubin authentication keystroke dynamics annual conference computer communications security http avirubin keystroke neumann neumann challenges insider misuse sri computer science lab technical report http csl sri neumann pgn-misuse html warrender warrender forrest pearlmutter detecting intrusions system calls alternative data models ieee symp security privacy ftp ftp unm pub forrest oakland-with-cite pdf shavlik shavlik fahland raid 
learning approximation inductive logic programming clause evaluation frank dimaio jude shavlik computer sciences department wisconsin madison dayton madison dimaio shavlik wisc abstract challenge faced inductive logic programming ilp systems poor scalability problems large search spaces examples randomized search methods stochastic clause selection scs rapid random restarts rrr proven successful addressing weakness datasets hypothesis evaluation computationally expensive algorithms unreasonably long discover good solution attempt improve performance algorithms datasets learning approximation ilp hypothesis evaluation generate small set hypotheses uniformly sampled space candidate hypotheses evaluate set actual data hypotheses evaluation scores serve training data learning approximate hypothesis evaluator outline techniques make trained evaluation-function approximator order reduce computation required ilp hypothesis search test approximate clause evaluation algorithm popular ilp system aleph empirical results provided benchmark datasets show clause evaluation function accurately approximated introduction inductive logic programming ilp systems widely classification data mining information extraction tasks natural treatment relational data harnessing expressive power first-order logic makes working databases multiple relational tables ilp systems combine background domain knowledge categorized training data constructing set rules form first-order logic clauses formally training set positive examples negative examples background knowledge sets clauses first-order logic ilp goal find hypothesis set clauses first-order logic ehbehb background knowledge hypothesis deduce positive examples negative examples real world applications constraints typically relaxed allowing explain positive examples appears proceedings international conference inductive logic programming negative examples ilp systems successfully employed number varied domains including molecular biology engineering design natural language processing software analysis challenge ilp systems face scalability large datasets large hypothesis spaces define general framework learning function estimates goodness hypothesis actual data suggest number ways approximation employed application eliminates poor hypotheses wasting time evaluating approximate hypothesis evaluator guide generation promising candidate hypotheses application mines estimator function rules invent predicates remainder paper structured section background related work scaling ilp section describes construction hypothesis evaluation estimator section describes detail estimator function section shows results estimator learning benchmark datasets section presents future research directions ilp background related work algorithm underlying ilp systems basically treats hypothesis generation local search subsumption lattice subsumption lattice constructed based idea specificity clauses specificity refers implication clause specific clause general undecidable clause first-order logic implies ilp systems weaker notion plotkin -subsumption subsumption candidate clauses puts partial ordering clauses hypothesis space partial ordering lattice clauses built figure ilp implementations perform type local search lattice candidate hypotheses major distinction separating ilp implementations strategy exploring subsumption lattice algorithms fall main categories true pos pos pos pos pos pos figure illustrates subsumption lattice ilp implementations search lattice bounded true bottom clause ilp systems treat clause discovery local search moving lattice edges exceptions general-to-specific top-down specific-to-general bottom-up exploration subsumption lattice frameworks variety common local search strategies employed including breadth-first search depth-first search heuristic-guided hill-climbing variants uniform random sampling rapid random restarts genetic algorithms work general framework increasing speed ilp algorithm order candidate clauses evaluated challenge ilp systems face scale poorly large datasets srinivasan investigated performance ilp algorithms found running-time depends factors size subsumption lattice time required clause evaluation turns depends number examples background corpus factor size subsumption lattice depends number terms specific saturation saturation put lower bound subsumption lattice process performed single positive background knowledge saturation constructs specific fully-ground clause entails chosen constructed applying substitutions variables background knowledge ground terms clause called chosen bottom clause serves bottom element subsumption lattice figure ilp searches clauses considered ilp subsumption lattice subsume simple suppose background knowledge prolog notation ground atoms denoted initial lowercase letter variables denoted initial uppercase letter current positive begin saturation letting ground atoms imply positive apply consistent substitutions make substitutions notation atom variable atom substituted variable apply rule line background knowledge finally combining saturation positive size subsumption lattice directly related size ignore multiple variablizations single ground literal hypotheses terms size subsumption lattice bottom clause taking account multiple variablizations introduces additional factor exponential number constants bottom clause factor evaluation time clause complicated analyze srinivasan simplifies analysis assuming clause evaluated constant time evaluation clause entire training set occurs time set training examples exhaustive search subsumption lattice single clause takes worst-case running time datasets clause evaluation worse srinivasan work assumed deducing candidate hypothesis takes constant time recursive rule background fact deduction undecidable restricting simpler case function symbols considered datalog allowing recursive clauses evaluating candidate clause set ground background facts np-complete ilp datasets fall simpler function-free category evaluation time exponential number variables relates length expression words long hypothesis significantly longer test examples background knowledge shorter hypothesis large datasets precisely long hypotheses interesting result approaches scaling ilp focused factors reducing number clauses considered decreasing time spent clause evaluations reducing number clauses considered simplest techniques employ general search strategies iterative deepening beam search reduce number clauses subsumption lattice considered beam reduces worst-case running time extremely large datasets thousands hundred thousands prohibitively long approach reducing number clauses subsumption lattice considered successfully employed srinivasan random sampling strategy considers sampling clauses subsumption lattice chosen independent size subsumption lattice worst-case running time finding single clause srinivasan idea works domains sizable number sufficiently good solutions recent work zelezny coupled random clause generation method heuristic search idea rapid random restarts rrr explore subsumption lattice repeatedly generates random clauses short local search ckert kramer success stochastic search bottom-up rule learning outperforming gsat walksat ilp optimizations focus decreasing time spent clause evaluations term ilp running time improvements prolog clause evaluation function developed blockeel reordering candidate clauses reduce number redundant queries santos costa developed techniques intelligently reordering terms clauses reduce backtracking srinivasan developed set techniques working large number examples considers fraction examples learning process sebag rouveirol stochastic matching perform approximate inference polynomial opposed exponential time maloberti sebag provide alternative prolog sld resolution -subsumption treat -subsumption constraint satisfaction problem csp combination csp heuristics quickly perform -subsumption work distinct techniques describe method learning function estimates clause evaluation function ways reduce evaluation time clause quickly approximating goodness clause amount time independent number training examples couple zelezny rapid random restart method order bias restarts regions search space manner similar boyan moore stage algorithm escape local maxima heuristic search finally extract hypotheses perform predicate invention estimator learning clause evaluation function heuristic approaches exploring subsumption lattice make scoring function 
represent goodness hypothesis explaining training data hypothesis candidate clause first-order logic set categorized training examples eee eevalfna maps clause score training set scoring metric evalfn heevalfn multilayer feed-forward neural network section learn approximate scoring function eevalfna preliminary testing revealed machine learning algorithms bayes linear regression significantly accurate approximating clause evaluation function neural network single hidden layer capable approximating bounded continuous function arbitrarily small error online training algorithm detailed section train neural network neural network topology constructing clause evaluation function approximator method encoding clauses neural network inputs encoding based top-down lattice exploration number popular ilp implementations implementations positive chosen random training set chosen saturated building bottom clause recall bottom clause consists fully ground literals ilp system constructs candidate hypotheses choosing subset fully-ground literals variablizing replacing ground atoms variables manner replaces multiple instances single ground atom single variable approach multiple split variablizations single set fully-ground literals approaches differ select ground literals bottom clause neural-network inputs comprised set features derived candidate clause variablization saturating literal bottom clause input neural network input set literal bottom clause constructing clause set notice multiple sets literals bottom clause variablized yield clause means input representations single clause input representation specific literals chosen constructing candidate clause formally candidate clause chosen selecting subset literals most-specific bottom clause current treat clause vector xxx -dimensional space ngconstructiin chosen literal ground vector subset inputs neural network important aspect input vector candidate clause clause subsumption lattice unique input vector representation mapping work direction bit vector corresponds legal clause cases majority bit vectors correspond illegal clauses unbound input variables algorithms neural network search space bit vectors section aware additionally give predicate specific input network vector dimension corresponds predicate appearing construction based number times predicate candidate clause ground literals predicate finally set inputs neural network features extracted variablized clause features include length number literals nvars number distinct variables nshared vars number distinct variables appearing avg var freq average number times variable appears max var chain longest variable chain appearing clause max chain neural network consists fully-connected hidden layer output units output units correspond predicted positive negative coverage clause number examples deduced hypothesis predicted values scoring function computation predicted output eevalfna trivial commonly evaluation functions include coverage accuracy npp evaluate clause neural network converting vector notation equations forward-propagating trained neural network estimate calculating eevalfna figure presents network topology graphically online training neural network initial training makes srinivasan random uniform sampling user specifies burn-in length algorithm uniformly randomly selects clauses space legal clauses maximum clause length evaluate clauses training data creating input output pairs training uniform sampling generate pairs ensures neural network approximation accurate entire search space local search methods bias neural network approximation local region search space table overview algorithm initially train neural network methods present section approximation explore subsumption lattice continue evaluate clauses actual data figure overview showing neural network topology input vector construction notice vector constructed literals chosen fully-ground bottom clause candidate clause sets selected literals correspond candidate clause set chosen clause construction pos pos bottom selected lits clause nvars len predicted pos coverage predicted neg coverage short burn-in period concluded wasteful throw potential training data network approximation algorithm online learning algorithm make clause evaluations occur part ilp regular search improve accuracy approximation generate virtually unlimited number pairs network simply scoring clauses actual data online training algorithm shown table clause evaluated ilp generating pair training neural network online learning algorithm adds pair cache recently evaluated clauses cache typically stores recently evaluated clauses full elements cache randomly removed make room incoming elements regular intervals typically insertions neural network updated backpropagation entire cache fixed number epochs typically continually changing training set short training intervals small number hidden units typically prevent overtraining goal approximation learn approximation clause evaluation function entire subsumption lattice concerned high accuracy approximation high-scoring regions subsumption lattice ensure accuracy maintain cache clauses table neural network burn-in training online training algorithms burn-in training algorithm bottom clause set training examples size training set trainset size train neural network learn clause evaluation function evalfn early stopping avoid overtraining returning learned network online training algorithm called pair pos neg ilp generates algorithm cache recent best-scoring clauses regular interval arrivals updates arrivals algorithm updates trained network preset number epochs epochs update arrival overflows cache removes items random burnintraining burnin iopairs neuralnetwork minerror inf burnin rand clause built pos neg evaluate evalfn add pos neg iopairs split iopairs trainset tuneset max epochs foreach pos neg trainset run backprop pos neg error sse tuneset error minerror minerror error bestnn return bestnn onlinetrainingarrival pos neg full recent cache delete random recent cache insert pos neg recent cache score pos neg min cache insert pos neg sorted cache num arrivals num arrivals num arrivals arrivals updates num arrivals epochs arrival foreach recent cache run backprop foreach cache run backprop return cache typically size recent-clauses cache cache full lowest-scoring element removed make room incoming higher-scoring clauses neural network updated clauses best-scoring cache added training set update neural network clause evaluation approximation section describes methods clause approximator scale ilp larger datasets speed discovery high-scoring hypotheses methods approximately evaluating clauses search subsumption lattice evalfn surface defined neural network escape local maxima bias random restarts extracting hypotheses performing predicate invention approximator function rapidly exploring subsumption lattice clause approximator method piggyback local search method stochastic methods perform search usual manner expand node evaluating successor clauses complete set examples neural network compute approximate clause evaluation score eevalfna choose node expand depending search strategy approximate scores node approximately scored network score actual data cache future training expand node repeat process recall approximate evaluation takes running time running time required perform actual evaluation training data interestingly behavior technique varies bit depending search strategy employed branch-and-bound search method serves optimize order clauses evaluated coupled pruning significantly reduce total number real evaluations required search lets throw clauses don promising wasting time evaluating actual data clauses neural network predicts score poorly reach font open list evaluated actual data note break guaranteed optimality nix weigend developed technique neural network predict regression place error bar prediction technique approximately score clauses storing open list confidence bound simply predicted 
score favor evaluation clauses neural network accurately predict areas explored promising biasing random restarts favorable regions search space additionally surface defined trained neural network guide search function encoded neural network fixed weights defines smooth surface space network inputs employ neural-network designed surface stochastic search surface perform biased rapid random restarts referred biased-rrr randomly selecting literals perform stochastic gradient ascent neuralnetwork defined surface starting random clause perform stochastic gradient ascent surface endpoint random restart point begin evaluating clauses actual training examples guided restarts bias search regions search space issue arises neural network separate output units predicts positive coverage predicts negative coverage perform gradient ascent surface scoring function possibly nonlinear combination fortunately common scoring functions derive simple expression relating derivative scoring function derivative output units derivatives output unit respect input ixp ixn easily computed backpropagation variant backprop computes ijp werr ijn werr table summarizes expressions commonly scoring functions interesting variant approach network-defined surface escape local maxima performing standard ilp best-first search equivalent biased rapid random restart scoring function gradient ascent equation compression iii coverage lnp iii accuracy iii npx laplace iii npx entropy loglog npx iii lnln gini iii table table expresses gradient common scoring functions pievalfn terms gradients network output units predicted positive predicted negative coverage stochastic gradient ascent equations compute network-surface gradient scoring function equations denotes positive coverage denotes negative coverage denotes clause length random point starting point network-guided gradient ascent ending point previous period ilp standard search real data variation rapidly alternate periods ilp standard best-first search stochastic gradient ascent neural-network-defined surface variation illustrated figure idea intelligent rapid random restarts escape local maxima domain ilp boyan moore stage algorithm quadratic regression approximate search trajectories learn function mapping points feature space endpoint local search starting point approximation escape local maxima heuristic search algorithm ran time reported test-set accuracy solutions discovered local search extracting concepts function approximation finally extract concepts neural network craven shavlik developed method extract decision tree trained neural network running algorithm thresholded trained clause-evaluation approximator produce theory set clauses variablize score actual data set neural network fitting nonlinear surface scoring function find pairs triplets terms individually helpful lead highly accurate rule combined terms share variables connected single hidden unit highly-weighted edge possibly impact accuracy rule pair terms candidate terms invented predicate neural network approximation find predicates seeds invented predicates added background knowledge search remaining seeds subsumption lattices neural network approx evalfnpi actual scoring function evalfnpi space candidate clauses score evalfn figure graphic illustrates algorithm stochastic gradient ascent surface defined neural network escaping local minima ilp standard bestfirst search search alternates periods ilp best-first search stochastic gradient ascent network-defined surface difference variant biased-rrr search starting point stochastic gradient ascent biased-rrr begins period stochastic gradient ascent random point search space results discussion section presents results benchmark datasets show neural network capable learning approximation clause evaluation function network rapid-random-restart search bias restarts promising regions search space section benchmark dataset overview tested clause evaluation function approximation standard ilp benchmark datasets tasks included predicting mutagenic activity carcinogenic activity compounds predicting smuggling nuclear radioactive materials predicting metabolic activity proteins description datasets mutagenesis task concerned predicting mutagenicity compounds ilp learner provided background knowledge consisting chemical properties compounds general chemical knowledge form first-order logic relations dataset popular benchmark explores large search space carcinogenesis similar mutagenesis task inherently difficult problem task main concern predicting carcinogenic activity compounds potential carcinogenic compounds database problem consists labeled examples half carcinogenic nuclear smuggling dataset based reports russian nuclear materials smuggling interesting highly-relational nature relational tables task concerned predicting smuggling events linked dataset subset complete dataset examples split evenly positive negative examples protein metabolism task gene-function prediction task kdd cup challenge wisc dpage kddcup challenge involves learning protein functions sub-task concerned predicting proteins responsible metabolism subset complete dataset examples split evenly positives negatives learning clause evaluation function section details empirical evaluation neural network learning task goal ascertain neural network learn ilp clause evaluation function simplifying task experiments batch learning process online learning process outlined section ilp system aleph web comlab oucl research areas machlearn aleph aleph toc html generate sets randomly sampled clauses datasets positive examples construction bottom clause seed examples chosen randomly considered maximum clauselength nuclear smuggling task considered larger task clauses scored standard scoring metric variant aleph compression heuristic clause score exs pos total thclauselengcovered exs neg covered exs pos score unlike aleph compression include term denominator convert scores good range neural networks dividing total number positive examples comparison scores datasets dataset clauses scores train neural network machine learning package weka generated learning curves -fold cross-validation datasets neural network constructed hidden units learning rate fixed added early stopping weka avoid overtraining cross-validation fold set training set tuning set epochs neural network performed tuning set weka numeric feature normalization enabled numeric features learning curves datasets figure data curves show root-mean-squared rms error sets examples section explains curves graphs datasets hypothesis evaluation function eevalfna learned reasonable accuracy datasets data added training set neural network accurately learns evaluation function interesting note number examples required accurately learn approximator accuracy final classifier varies datasets absolute accuracy approximator varies datasets protein metabolism fully-trained network averages rms error mutagenesis results order magnitude worse promising worst performing approximator rms error assumed transfer knowledge seed examples learn neural network scratch saturated features employ independent selected saturation feature ground literals selected vector section instance-independent instance independent representation features shared generating rules seed examples rules bootstrap initial classifier based knowledge garnered previous rules looked contribution subset features datasets wanted instance-independent features contributed learning task weka construct learning curves dataset learning curves correspond training network instance-independent features instance-dependent features figure illustrates exception protein metabolism training instance-independent features produce accurate classifier training instance-dependent features complete set features datasets complete set features produce significantly accurate network approximator instance-dependent features suggests instance-independent features transfer learning seed seed approaches developed graphs illustrate capable learning clause evaluation function show degree function learned figure compares rms error network approximation rms error obtained random sampling training examples approximately score clauses alternate method computation reduction compare method determine number evaluations neural network worth number varies significantly datasets ranging sampling sampling 
fairly small benchmark datasets remains open question method compare sampling training examples larger problems larger hypothesis spaces datasets includes large problems arise biological sciences text extraction figure learning curves showing test-set accuracy domains comparing roles instance-dependent versus instance-independent features learning curves generated subset complete set features results compared case features train network mutagenesis training set size data instance-independent instance-dependent protein metabolism training set size nuclear smuggling training set size carcinogenesis training set size evaluation function approximator guide random search section details trained neural network bias random restarts rapid random restart search goal find best-scoring clause subsumption lattice clause evaluations results section concerned maximizing evaluation function training data assuming well-designed evaluation function corresponds good test-set performance implemented previous-described online learning algorithm aleph enable biased random restarts implemented stochastic gradient ascent algorithm gradient ascent implementation step considered flipping input bit flipping bit clause length maximum probability bit flip input evalfn xzxp exp flip formula determines softness gradient ascent results set times flip literal worst term simply flips sign gradient flipping bit move negative direction order test performance algorithm attempt find clause maximizes coverage scoring function defined number positive examples covered minus number negative examples covered stochastic gradient ascent bias rrr search restarts steps restart compare biased-rrr versus normal rrr parameters biased-rrr burn-in period consisted single random restart local moves report results datasets previous figure comparing rms error neural-network approximation obtained random sampling training examples approximate clauses error neural-network approximation varies widely cases sampling examples datasets sampling protein metabolism carcinogenesis mutagenesis nuclear smuggling sampling sampling sampling samplingneural netw ork section omitting mutagenesis quickly converges seeds found clause restart dataset explored subsumption lattices seed examples neural network consisted hidden units finally rapid random restart began endpoint previous local search finished fixed number random steps aleph search parameters left default figure shows results datasets graphs x-axis shows number clauses evaluated y-axis shows average coverage seeds found plots show datasets carcinogenesis protein metabolism biased-rrr found clause quicker traditional rrr task nuclear smuggling biased-rrr worse default implementation reasons unclear neural network learn evaluation function approximator domain conclusion future work demonstrated neural network clause evaluation tool improving runtime efficiency handling large search spaces ilp ilp confronted increasingly larger problems methods present grows treated network learning evaluation tasks computationally free operations true true running time neural network evaluation training independent figure performance biasedrrr search versus traditional rrr search x-axis shows number clauses evaluated y-axis displays average coverage clause found carcinogenesis protein metabolism biased-rrr performs nuclear smuggling outperformed carcinogenesis clause evaluations biased-rrr rrr protein metabolism clause evaluations nuclear smuggling clause evaluations number ilp examples dataset means examples ilp training set neural-network evaluation made virtually free strategy decrease runtime ilp systems large tasks pressing work remains implementing evaluating strategies taking advantage clause-evaluation approximator outlined sections accuracy lost approximating clause-evaluation function difficult determine affects solutions generated quickly evaluate clauses typical ilp search open question information extracted trained neural network botta characterized hypothesis space discovering critical region named phase transition critical region computational complexity inference increases clauses generated region tend poor generalization unseen test examples phase transition difficult region ilp algorithms algorithm performance specifically exploration finally discussed learning evaluation approximation leastsquared-error sense important ilp relative ranking candidate clauses approach caruana baluja rankprop algorithm alternative backprop concerned correctly predicting ranking output variables natural acknowledgements work supported national library medicine nlm grant darpa grant united states air force grant nlm grant authors condor team anonymous reviewers lavrac dzeroski inductive logic programming ellis horwood king muggleton sternberg predicting protein secondary structure inductive logic programming protein engineering srinivasan king muggleton sternberg predictive toxicology evaluation challenge proc intl joint conf artificial intelligence dolsak muggleton application ilp finite element mesh design proc intl workshop ilp zelle mooney learning semantic grammars constructive inductive logic programming proc natl conf artificial intelligence bratko grobelnik inductive learning applied program construction verification proc intl workshop inductive logic programming nienhuys-cheng wolf foundations inductive logic programming springer-verlag schmidt-schauss implication clauses undecidable theoretical computer science quinlan learning logical definitions relations machine learning muggleton feng efficient induction logic programs proc conf algorithmic learning theory muggleton inverse entailment progol generation computing srinivasan study probabilistic methods searching large spaces ilp tech report prg-tr- oxford univ computing lab zelezny srinivasan page lattice-search runtime distributions heavy-tailed proc intl conf inductive logic programming giordana saitta zini learning disjunctive concepts means genetic algorithms proc intl conf machine learning hanschke wurtz satisfiability smallest binary program info proc letters dantsin eiter gottlob voronkov complexity expressive power logic programming acm computing surveys ckert kramer stochastic local search k-term dnf learning proc intl conf machine learning blockeel dehasp demoen janssens ramon vandecasteele improving efficiency inductive logic programming query packs research santos costa srinivasan camacho blockeel demoen janssens struyf vandecasteele van laer query transformations improving efficiency ilp systems machine learning research srinivasan study sampling methods analysing large datasets ilp data mining knowledge discovery sebag rouveirol resource-bounded relational reasoning induction deduction stochastic matching machine learning maloberti sebag theta-subsumption constraint satisfaction perspective proc intl conf inductive logic programming boyan moore learning evaluation functions improve optimization local search machine learning research hornik stinchcombe white multilayer feedforward networks universal approximators neural networks nix weigend learning local error bars nonlinear regression advances neural information processing systems mit press craven shavlik extracting tree-structured representations trained networks advances neural information processing systems mit press king muggleton srinivasan sternberg structure-activity relationships derived machine learning pnas srinivasan king muggleton sternberg carcinogenesis predictions ilp proc intl workshop inductive logic programming witten frank data mining morgan kaufmann publishers goadrich oliphant shavlik learning ensembles first-order clauses recall-precision curves case study biomedical information extraction proc intl conf inductive logic programming botta giordana saitta sebag relational learning search critical region machine learning research caruana baluja future sort present advances neural information processing systems mit press 
appears proceedings international conference machine learning icml theory-re nement approach information extraction tina eliassi-rad eliassi wisc jude shavlik shavlik wisc computer sciences department wisconsin west dayton street madison usa abstract investigate applying theory nement task extracting information text theory nement partial domain knowledge incorrect supervised learner provided knowledge guides learner task learner discard knowledge training supervised learner knowledge-based neural network initially compiled prior knowledge information extraction task prior knowledge extraction slots speci task approach generate-and-test address task generation step produce candidate extractions intelligently searching space extractions test step trained network judge candidate output exceed system-selected threshold experiments cmu seminarannouncements yeast subcellularlocalization domains demonstrate approach introduction rapid growth on-line information created surge interest tools extract information on-line documents information extraction process pulling desired pieces information document building system requires large number annotated examples expert provide cient correct knowledge domain interest requirements make time-consuming cult build system annotated examples result tedious process reading training documents tagging extraction hand paper demonstrate theory-re nement approach towell shavlik build system theory nement strike balance needing large number labeled examples complete correct set domain knowledge system takes advantage intuition information retrieval inverse problems system set keywords asked rate relevance documents system set documents asked slots template explore essentially system address task system called wawa-ie basic wawa system previously shavlik eliassirad shavlik brie review user set instructions form if-then statements wawa-ie instructions describe system score bindings slots lled process call names slots lled variables binding variable synonym lling slot initial instructions compiled towell shavlik neural network called scorepage rates goodness document context variable bindings refer user-provided instructions advice emphasize system blindly follow user instructions nes based training examples user-provided advice typically leads higher accuracy fewer user-provided training examples wawa-ie generate-and-test approach extract information generate step user rst speci slots lled part-ofspeech tags parse structures wawa-ie genwawa-ie extraction subsystem larger system called wawa wisconsin adaptive web assistant erates large list candidates document test step wawa-ie scores candidate candidates produce scores greater system-de ned threshold returned extracted information critical component wawa-ie intelligent selector explained eliminates create exhaustive list candidate bindings key aspect wawa-ie advice language advice sentence conditional statement antecedent describes property page word appears verb page consequent speci estimate page relevancy domain interest mentioned relevance approach fact wawa-ie advice language variables understand wawa-ie variables assume extract speaker names collection seminar announcements give system good advice page phrase speaker firstname nnp lastname nnp score page highly leading question marks slots lled matches single word advice language user required part speech slot nnp denotes proper noun precondition rule matches phrases speaker joe smith figure illustrates extracting speaker names seminar announcement wawa-ie announcement fed candidate generator selector produces list speaker candidates entry candidates list bound variables advice output trained network real number represents con dence speaker candidate correct slot ller document wawa-ie system require user provide information wawa-ie set on-line documents information extracted extraction slots speaker names part-of-speech pos tags noun proper noun verb parse structures noun phrase verb phrase extraction slot variable speaker record holds multiple variables firstname lastname candidate generator selector seminar announcement don miss jane doe john smith talk doe smith talk turing tarpit building score jane doe speakerextractor speaker speaker candidates jane doe john smith smith turing generation step test step figure extraction speaker names wawa-ie set advice rules variables refer extraction slots set annotated examples training documents extraction slots marked user explicitly provide extraction slots pos tags separately advice extracted advice rules case studies extract names proteins subcellular locations yeast database ray craven advice rules task phrase proteinname nphrase vphrase locationname nphrase appears document score high variables proteinname locationname represent protein names subcellular structures nphrase trailing variables required parse structure variables nphrase refers noun phrase vphrase matches verb phrase precondition rule matches phrases ubc localizes endoplasmic reticulum training wawa-ie rst compiles user advice scorepage network wawa-ie call individual-slot candidate generator combination-slots candidate selector create training examples scorepage network candidate generation selection process training generate extractions trained network scores individual-slot candidate generator rst step wawa-ie takes training generate individual llers slot document fillers individual words phrases individual words collected brill tagger annotate word document pos slot collect word document pos tag matches tag assigned variable task advice cases variable phrase apply sentence analyzer called sundance rilo document sundance builds shallow parse tree segments sentences noun verb prepositional phrases collect phrases match parse structure extraction slot generate subphrases consecutive words sundance shallow parsing point typically lengthy lists candidate llers slot focus generating good combinations slots process combinatorially demanding section present evaluate heuristic method choosing good combinations combination-slots candidate selector wawa-ie methods creating complete assignments slots lists individual slot bindings exhaustive candidate selector exhaustively produce cross-product entries lists individual-slot candidates document tasks cient computational resources control evaluate options high-scoring simple random selection srs randomly select combination-slots candidates lists individual-slot candidates training combinations produce highest scores untrained scorepage network control compare intelligent selector combined bindings modi wsat modi version walksat algorithm selman figure describes algorithm document algorithm starts empty list combination-slots candidates extraction slot iteratively randomly selects item slot list individual-slot untrained network compiled initial advice training backpropagation labeled examples inputs max-tries max-alterations max-cands doc lists individual-slot candidate extractions doc output list combination-slots candidate extractions size max-cands algorithm max-tries randomly selected combination-slots candidate score doc add max-alterations randomly select slot change probability randomly select candidate add probability select candidate maximizes score doc add sort decreasing order score entries return top max-cands entries figure modi walksat algorithm candidates produces combination-slots candidate extraction candidate ller slot template score produced possibly untrained scorepage network high scale set variable bindings add combination list combination-slots candidates repeatedly randomly select slot template probability randomly select candidate selected slot add resulting combination-slots candidate list combination-slots candidates probability iterate candidates slot choose candidate produces highest network score document resulting combination-slots candidate added list combination-slots candidates training agent figure shows process building trained agent positive training examples provided domains rst generate negative training examples end candidate generator selector list negative training examples collected selector informative negative examples misses heuristic search selector scores training documents untrained scorepage network userprovided prior 
knowledge scored miss extractions highly true extractions negative examples collected train scorepage neural network negative examples provided positive examples training network recognize produce high output score correct extraction context document shavlik advantage global layout information documents interest training set initial advice individual-slot candidate generator combination-slots candidate selector slots pos tags parse structures lists candidate combination-slots extractions scorepage agent trained network agent knowledge-base figure building trained agent scorepage network outputs real number wawa-ie threshold output bindings scores threshold returned user extractions rest discarded note threshold manipulate performance agent threshold set high number agent miss lot correct llers slot low recall number correct llers extracts higher high precision recall van rijsbergen ratio number correct llers extracted total number llers correct extraction slots precision van rijsbergen ratio number correct llers extracted total number llers extracted commonly -measure combines precision recall formula precision recallprecision recall avoid tting scorepage network threshold output training divide training set disjoint sets sets train scorepage network set validation set rst stop training scorepage network speci cally cycle training examples times iteration training examples lists combination-slots candidates validation set evaluate -measure produced network settings threshold pick network produced highest -measure validation set nal trained network validation-set threshold processing subsequent test set examples testing trained agent figure depicts steps trained agent takes produce extractions slots pos tags parse structures test set unseen docs trained agent individual-slot candidate generator combination-slots candidate selector extractions lists candidate combination-slots extractions figure testing trained agent entry list combination-slots extraction candidates rst bind variables candidate values perform forward propagation trained scorepage network output score network test document based candidate bindings output network greater threshold ned validation step record bindings extraction bindings discarded discussion aspect wawa-ie exploitation relationship build agents treat extractions keywords turn judged context entire document theory nement advantage user prior knowledge perfectly correct wawa-ie learning system turn reduces labeled examples expensive task compiling users prior knowledge scorepage network good method nding informative negative training examples misses cost approach require user provide pos tags parse structures extraction slots assume brill tagger sundance perfect tag words parse sentences accuracy brill tagger annotates words document accuracy brill error rate propagates results accuracy estimates sundance remember subphrases phrases sundance produces approach computationally demanding due generate-and-test approach cpu cycles abundant experiment shows wawa-ie performs subset combinations slot llers experimental evaluation evaluate approach tasks rst involves extracting speaker location names collection seminar announcements task widely literature directly compare performance wawa-ie existing systems domain follow existing methodology independently extract speaker location names document assumed announcement announcement pair speakers locations return list speakers separate list locations combination slots selector task task involves extracting protein names locations cell collection abstracts biomedical articles yeast chose domain illustrates harder task rst domain domain llers extraction slots depend single abstract multiple proteins locations document single list protein location pairs extracted seminar announcements domain rst experiment compare wawa-ie information extraction systems cmu seminar announcements domain freitag systems srv freitag naive bayes freitag whisk soderland rapier cali rapier-wt cali naive bayes relational learning algorithms exploit prior knowledge freitag rst randomly divided documents seminar announcements domain ten splits randomly divided ten splits approximately training examples testing examples whisk results systems based data splits results whisk single trial documents training set documents testing set give wawa-ie advice rules backus naur form bnf aho notation speakers locations advice rules written speci cmu seminar announcements mind rules describe prior knowledge speaker location general seminar announcement half day write rules manually rules time domain create number negative training examples speaker location independently number positive examples choose negatives complete list possibilities collecting score highest untrained scorepage network remaining chosen randomly complete list tables show results system systems speaker location slots results reported averages ten splits precision recall -measure split determined optimal threshold found split validation set remember precision ratio number correct llers extracted total number llers extracted recall ratio number correct llers extracted total number llers correct extraction slots ideal system precision recall finally commonly -measure combines precision recall formula precision recallprecision recall speaker location seminar multiple forms announcement extraction considered correct long correct forms extracted speaker john doe smith words smith joe smith john doe smith smith smith document extractions considered correct method marking correct extractions systems compare approach complete list advice rules eliassi-rad table speaker slot results seminar announcements system wawa-ie trained agent srv rapier-wt rapier wawa-ie untrained agent naive bayes whisk table location slot results seminar announcements system wawa-ie trained agent rapier-wt srv rapier whisk rapier-w naive bayes wawa-ie untrained agent speaker location slots -measure system highest -measures high generate extraction candidates extract lot correct llers data set leads higher recall systems training reject candidates obtain reasonable precision systems higher precision depending user tradeo recall precision erent systems preferred testbed increase performance wawa-ie untrained agent wawa-ie trained agent shows system hard-wired perform domain training helped performance rst experiment train network nding speaker slots location slots section discuss experiment train network extract llers slots simultaneously yeast protein-localization domain experiment task extract protein names subcellular localization yeast protein-localization database produced ray craven call extraction template subcellular-localization relation created yeast database rst collecting target instances subcellular-localization relation yeast protein database ypd web site collected abstracts articles medline database national library medicine entries selected ypd ray craven methodology experiments domain tuple-level method measuring accuracy tuple instance subcellular-localization relation yeast data set training test instance individual sentence positive sentence labeled target tuples positive sentences tuples unique negative sentence labeled tuples negative sentences data set note sentence protein subcellular location considered negative wawa-ie advice rules bnf aho notation protein subcellular location advice rules written speci yeast data set mind rules describe prior knowledge instance subcellular-localization relation half day write rules manually rules time ray craven split yeast data set disjoint sets ran -fold cross-validation folds wawa-ie compare results rst investigate intelligently select good training examples reduce training time comparing test set -measure case negative examples figure illustrates erence -measure section modi walksat selector highscoring srs exhaustive candidate selector horizontal axis depicts percentage negative training examples learning process vertical axis depicts -measure trained ie-agent wawa-ie achieve good performance negative 
training candidates high-scoring srs selector worse modi walksat -measures wawa-ie trained agents outperform untrained agents approximately results shown demonstrates wawa-ie initial advice figure shows precision recall curves wawa-ie trained agent modi walksat selector negative examples wawa-ie trained agent selector negative training candidates -measure trained agent exhaustive selector trained agent wsat trained agent high-scoring srs figure -measure percentage negative training candidates erent selector algorithms negative training examples system ray craven precision recall trained agent wsat ray craven trained agent selector figure precision recall curves trained agent selector algorithm produces results computationally expensive cross-product entries lists individual-slot candidates trained agent modi walksat selector performs outperforming ray craven system results shown high-scoring srs selector performs similarly ray craven results illustrate theory nement justify intelligent candidate-selection algorithm reduce computational burden approach generate-and-test strategy wawa-ie modi walksat selector improve state art negative training candidates training finally recall variant walksat testing figure shows obtain good precision recall needing exhaustively score candidate related work unable system literature applies theory nement systems break groups rst group kind relational learning learn extraction patterns cali freitag soderland group learns parameters hidden markov models hmms hmms extract information bikel freitag mccallum leek ray craven seymore leek hmms extracting information biomedical text system lot initial knowledge build hmm model training data learn parameters hmm system knowledge authors statistical methods reduce lot training examples freitag mccallum hmms extract information text employ statistical technique called shrinkage problem cient labeled examples seymore hmms extract information on-line text problem cient training data data labeled purpose system similarly craven kumlien weakly labeled training data reduce labeled training examples advantage system utilize prior knowledge reduces large number labeled training examples depend initial knowledge correct easy users articulate domain-speci advice user-friendly interface provided converts advice speci wawa-ie advice language advantage system entire content document estimate correctness candidate extraction learn extraction slots documents advantage wawa-ie utilize untrained scorepage network produce informative negative training examples misses current future work catalog candidate-selector algorithms includes modi version gsat algorithm selman modi version random local search hill climber multiple random restarts running tests selector algorithms due space limitations discuss addition measuring performance function number positive training examples function number advice rules finally starting experiments domains webkb domain freitag incorporating candidate generation selection steps directly connectionist framework current scorepage network candidate extractions training process conclusion describe evaluate system theory nement perform information extraction wawaie neural network accepts advice variables rate candidate variable bindings content document extraction process rst generates large set candidate variable bindings slot selects subset combinations individual slot bindings heuristic search nally trained network judge bindings score higher system-computed threshold returned extracted information theory nement advantage prior knowledge domain interest produce informative training examples lead increase performance agent experiments cmu seminarannouncements domain wawa-ie achieved measures signi cantly higher previous approaches yeast protein-localization data set empirically show bene intelligent algorithm selecting candidates multiple slots provided additional evidence approach improves state art acknowledgements research supported part nlm grant nsf grant iriuw vilas trust andreae craven helpful comments aho sethi ullman compilers principles techniques tools addison wesley bikel schwartz weischedel algorithm learns mach learn brill advances rule-based part speech tagging proc aaai seattle cali relational learning techniques natural language information extraction doctoral dissertation texas austin craven kumlien constructing biological knowledge-bases extracting information text sources proc ismb heidelberg eliassi-rad building intelligent agents learn retrieve extract information doctoral dissertation dept wisconsin madison freitag machine learning information extraction informal domains doctoral diss cmu freitag mccallum information extraction hmms shrinkage proc aaai workshop machine learning information extraction leek information extraction hidden markov models master thesis ucsd national library medicine medline database http ncbi nlm nih gov pubmed ray craven representing sentence structure hidden markov models information extraction proc ijcai seattle rilo sundance sentence analyzer http utah projects nlp selman kautz cohen local search strategies satis ability testing dimacs series discrete mathematics theoretical seymore mccallum rosenfeld learning hidden markov model structure information extraction proc aaai workshop machine learning information extraction shavlik calcari eliassi-rad solock instructable adaptive interface discovering monitoring information world-wide web proc intell user interf redondo beach shavlik eliassi-rad intelligent agents web-based tasks advice-taking approach proc aaai workshop learning text categorization soderland learning information extraction rules semi-structured free text mach learn towell shavlik knowledge-based arti cial neural networks artif intell van rijsbergen information retrieval london buttersworths edition 
appears proceedings ieee conference computational systems bioinformatics csb self-tuning method one-chip snp identification michael molla jude shavlik thomas albert todd richmond steven smith wisconsin-madison nimblegen systems molla shavlik wisc talbert trichmond ssmith nimblegen abstract current methods interpreting oligonucleotidebased snp-detection microarrays snp chips based statistics require extensive parameter tuning extremely high-resolution images chip processed present method based simple data-classification technique called nearest-neighbors haploid organisms produces results comparable published results leading statistical methods requires parameter tuning interpret snp chips lower-resolution scanners type typically current microarray experiments algorithm present results snp-detection experiment independently applying algorithm identical sars snp chips correctly identify snps strain sars virus false positives experiments introduction date genomes hundreds organisms sequenced organisms consensus sequence deposited public database sequence matches individual genome sequenced individuals species differ slightly sequence identify differences completely sequence scratch genomes individuals species comparison costly generally impractical genetic variation individuals form single nucleotide polymorphisms snps altshuler cost-effective approach sequence scaffolding identify variations sequence individuals technique resequencing saiki method resequencing shown significant results utilizes oligonucleotide microarray technology hacia type resequencing chip consists complete tiling sequence chip probe -mer sequence base sequence mismatch probes representing snp position section detailed description method theory time snp present mismatch probe representing snp higher intensity signal probe matches sequence due unpredictability signal strength varying hybridization efficiency sources noise method typically results base positions identities incorrectly predicted words cases mismatch probe signal intensity sequence probe accurately separate true snps noisy false positives current approaches noise-reduction problem cutler require extensive parameter tuning involving analysis large amounts data tuning re-run time experimental conditions changed limitation current methods order single probe represented sufficient number pixels expensive high-resolution scanner present technique simple dataclassification technique differentiate potential snps chip noise unlike methods require high-resolution scanner requires tuning single chip analyzed haploid sars strain evaluate approach algorithm produces results similar published results snp identification appears proceedings ieee conference computational systems bioinformatics csb rate current statistical methods cutler method signal intensity probe chip data chip task definition task identify snps single nucleotide polymorphisms context oligonucleotidemicroarray-based dna resequencing nuwaysir singh-gasson type resequencing consists fully tiling making probes -mer sequence organism dna region interest probes mismatch probes generated base center position organism dna includes sequence probe reverse complement sequence mismatch probes sequences call group probes represent snps position quartet approach involve creation probes length organism dna sequence creating probes feasible large genomes human rat feasible viruses bacteria limited regions interest large genome summarize task interpreting resequencing chip data single resequencing chip representing complete genome organism regions interest genome identify positions sample sequence differ sequence positions real snps noise return positions confidence measure approach chip exposed sample probes resulting intensity call quartet set intensities term machine learning solution built technique machine learning examples highest intensities probe probe mismatch base call examples case conformers table appearing table illustration conform expect sequence mismatch probes highest intensity call quartet non-conformer non-conformers reflect actual snps dna organism results hybridization failures types noise represent actual snp sample note task separating conformers non-conformers trivial data-processing step separation non-conformers snps non-conformers arise noise data posit perform task accurately separating non-conformers snps noisy non-conformers applying called nearest-neighbor method mitchell method plots examples n-dimensional space dimensions features examples order interpret feature space examples nearest space classifications interpret question detail section feature space task defined intensities probes quartet traditional manner applying nearestneighbors method follow work manually label training set quartets true snps non-conformers arise one-base difference sample sequence sequence false snps non-conformers arise noise microarray experiment nearest-neighbors algorithm labeled examples needed categorize future non-conformers case approach feasible require laboriously collect training examples worse chip chemistry laboratory condition changed collect set training examples appears proceedings ieee conference computational systems bioinformatics csb underlying process generated noise changed apply nearest-neighbor approach needing human-labeled examples key idea examples involving bad microarray hybridizations tend group portions feature space examples good hybridizations separated noisy examples good examples identify snps simply finding examples highest-scoring base base sequence nature task specifically rely assumptions held true data looked including data experimental section article examples resulting proper probetarget hybridizations nearer feature space examples resulting hybridization failures majority non-conformers due noise data snps assumption false data set include data chips snps safely ignore snps areas feature space dense non-conformers snps rare snps involved successful hybrizations fall regions feature space surrounded conformers figure illustrates assumptions non-conformers falling areas feature space dense conforming examples predicted snps difference expected result lead called non-conformer due base difference sequence failed hybridization non-conformers surrounded non-conformers viewed noisy data addition likelihood area hybridization error roughly estimated density non-conformers area performing estimation non-conformers find approximate likelihood result hybridization error note approach makes labeled examples require human label examples snps labels conformer non-conformer distinction computed simply probe intensities group probes words task predict conformer distinction made simple calculation idea finding figure interpreting conformers neighbors feature space non-conformer feature space conformer bad data snp appears proceedings ieee conference computational systems bioinformatics csb nearest neighbors feature space separate nonconformers produced snps nonconformers resulting hybridization failures hypothesize surrounded conformers surrounded non-conformers algorithm table algorithm snp-detection microarrays k-nearest-neighbor algorithm involves plotting feature space examples finding examples nearest space categories neighbors determine prediction greater threshold neighbors conformers infer result failed hybridization non-conformer classify snp infer sample sequence match sequence base position explicitly classify nonsnp insufficient number neighbors conformers view noisy classify non-call conformer fraction conformers neighbors measure confidence prediction threshold definitions nearness feature space vary learning tasks choices importance case assumption previous section hold nearness feature space defined properly task feature space table dimensional space examples dimensions correspond intensities probes dimension identity base sequence defining nearness define inverse distance define distance probes infinite cases examples differ dimension defined one-norm distance examples distance examplej examplek kiji examplej examplek quartets featurei intensity ith intense probe addition feature space slight variations work unsuccessful variant features represent signal intensities identity base guess technique successful hybridization characteristics affinity probe sample vary slightly classifiers combine individual program program program input combined classifier nucleotides result identity base carries information typical patterns intensity variant sort probe signals intensity compared neighbors intensities nucleotide-by-nucleotide basis examples intensities probe middle compared probes middle etcetera table algorithm threshold experiments noted threshold 
find examples closest feature space nearest neighbors number nearest neighbors conformers threshold actual category conformer classify non-snp classify candidate snp classify non-call possibly bad data appears proceedings ieee conference computational systems bioinformatics csb table features describe quartets sequence resequencing probes probes bold quartet probe probe probe probe quartet probe probe probe probe quartet probe probe probe probe resulting intensities obtained exposing chip sample probe quartet highest-intensity probe quartet call quartet conformer note probe quartet highest intensity probe quartet call quartet non-conformer probe intensity feature set quartet produces features base sorted intensities note feature set information actual probe highest intensity category conformer non-conformer quartet highest intensity probe base base intensity intensity intensity intensity category conformer non-conformer conformer appears proceedings ieee conference computational systems bioinformatics csb method suffer fact average training examples order intensities test features sorted examples probe intensities similarly ordered close feature space considered nearest neighbors feature space densely populated problem case training examples support method circumstances alternative distance measures euclidean distance absolute-value approach chose called one-norm computationally efficient large number calls squaring function eliminated evaluation purposes evaluation compare algorithm simple alternative call baseline algorithm table baseline algorithm simply compares highest intensity probe highest ratio threshold algorithm assumes base represented highest intensity probe base sequence quartet non-conformer baseline algorithm calls candidate snp noted baseline algorithm state art snp-finding software discussed baseline algorithm simply basic straightforward interpretation results resequencing chip turning evaluating approach real genomic data discuss computational demands algorithm table implement clever data structures support fast determination nearest neighbors liu logarithmic number examples implemented data structures process data microarray matter minutes simple linear-time algorithm linear quadratic length dna sequence algorithm runtime time takes run wet lab phrase microarray experiment algorithm fast purposes takes approximately fifteen minutes process typical two-hundred-thousandprobe chip -gigahertz pentium processor megabytes ram longer typical statistical methods significant contributor time required preparation analysis chip order evaluate algorithm chose realistic task strain sars virus ruan completely sequenced standard capillary sequencing supplied sample strain sample differed sequence unknown degree task identify candidate snps strain predictions subsequently evaluated capillary sequencing wet laboratory methods wong sequence designed resequencing chip including forward reverse strands virus exposed chip sample algorithm predict snps chip results obtained combined forward reverse predictions snp position averaging predictions table baseline algorithm threshold maxintensity intensity highest intensity base secondintensity intensity highest intensity base maxintensity secondintensity threshold classify non-call actual category conformer classify non-snp classify candidate snp appears proceedings ieee conference computational systems bioinformatics csb materials methods preparation hybridization sars sample detailed description methods prepare analyze sars samples previously published wong briefly total rna extracted patient lung sputum fecal samples vero cultured cells inoculated sarscov rna rna reverse-transcribed doublestranded cdna tissue samples amplified nested-pcr strategy sample pcr-product fragments pooled equimolar ratio digested dnase invitrogen carlsbad end labeled biotin-n ddatp perkin elmer wellesley terminal deoxynucleotidyl transferase promega madison arrays synthesized previously nuwaysir singh-gasson resequencing arrays hybridized biotinylated dna overnight washed stained streptavidin conjugate amersham biosciences piscataway signal amplified secondary labeling dna biotinylated goat antistreptavidin vector laboratories burlingame data extraction analysis microarrays scanned resolution genepix scanner axon instruments union city image interpolated scaled size nih image software http rsb info nih gov nih-image feature microarray consists pixels pixel intensities extracted nimblescan software nimblegen systems madison results discussion algorithm performed task sequence positions represented quartets chip non-conformers algorithm identifies candidate snps subsequent laboratory experimentation performed identified actual snps identified algorithm actual snp non-conformers quartets highest-intensity probe sequence note general conformer snp algorithm call snps label quartet suspicious data sars strain conforming snp unable evaluate approach labeling snps non-calls conformers algorithm parameter settings categorizing nonconformers marked bad data candidate snps determined snps k-nearest neighbors baseline figure roc curve sars snp detection appears proceedings ieee conference computational systems bioinformatics csb order verify result generated identical snp chips exposed sample values threshold section discuss choose good values threshold results varied slightly algorithm found snps cases number false positives ranged algorithm largely self-tuning examples compared neighbors feature space classifications made properties neighbors opposed specific portions feature space pre-labeled clean noisy parameters threshold describe experiments investigate sensitivity algorithm settings parameters order choose values false positives result chose largest threshold allowed algorithm detect true snps results experiment figure fortunately approach overly sensitive chose hypothesize parameter setting work wide variety organisms strains figure presents impact varying threshold reports number true snps detected number false positives non-snps incorrectly called snps algorithm performance overly sensitive setting threshold anticipate single setting threshold work organisms strains hope threshold reset dataset remember approach classifies quartets non-calls neighbors predominantly non-conformers percentage quartets called snp non-snp typically call rate rate low procedure algorithm interprets small fraction data order increase call rate lower threshold chosen parameter settings achieve call rate identifying snps samples tested misclassifying small number non-snps figure impact y-axis reports number false positives noisy examples misclassified snps result largest threshold algorithm detect true snps appears proceedings ieee conference computational systems bioinformatics csb unable directly compare haploid snp calling accuracy current standard algorithm abacus cutler group conjunction affymetrix corp results comparable published cutler approach overhead due tuning require high-resolution scanning published results emphasis high-confidence snps cost low call rate cutler group reported accuracy good snps predicted human chromosome verified real report call rate chip approximately method geared high sensitivity snps change increasing threshold call rate drops make snp calls level false positives detect snps closely compare results species numbers suggest accuracy algorithm par cutler group related work approaches problem previously wang hirschhorn cutler successful date cutler group conjunction affymetrix parametric statistical techniques account distribution pixel intensities probe scanned signal pattern approach presents number limitations principal fact method sensitive chemistry scanner type chip layout order overcome problems extensive parameter tuning required involves analysis large amounts data re-run time chemistry light-gathering technology virtually experimental condition changed limitation order single probe represented sufficient number pixels high-resolution scanner threshold snps correctly called total non-snps incorrectly called snps figure impact threshold y-axis reports number snps found number false positives result threshold fixed appears proceedings ieee conference computational systems bioinformatics csb future current work efforts underway apply method genomes evaluation performance larger genomes varying 
degrees complexity snp density great interest lead refinement algorithm process applying method identification heterozygote snps snps alleles present sample statistical methods type snp identification exist cutler require comparison multiple individuals species require type high-performance hardware tuning previously mentioned method simply meansignal intensities intra-chip tuning homozygote haploid method mentioned decrease number probes needed analysis probes base position dna strand efficient standard methods process efficient handle large genomes approach employ resequencing chip base positions deemed high probability snps resulting chip analyzed manner yield good results snp-calling algorithm relies large number non-snps sample snps plan extend method work fewer non-snps plan experiment richer feature set intensities probes quartets representing bases genome position represented test conclusion identifying snps important task emerging field microarray technology provided tools identify snps straightforward snp chips presented alternative standard method interpretation snp chips empirical results sars strains encouraging prospects future snp detection method effective snp-detection ability additional strengths algorithm simplicity means calibration needed calibration single chip due nearest-neighbor approach classification training examples nearest-neighbor approach created simple-to-implement definitions conformers non-conformers table avoiding human laboriously label examples require high-resolution scanners acknowledgements edison liu christopher wong lance miller genome institute singapore sars samples research partially supported grants nih nlm altshuler pollara cowles van etten baldwin linton lander snp map human genome generated reduced representation shotgun sequencing nature cutler zwick carrasquillo yohn tobin kashuk mathews shah eichler warrington chakravarti highthroughput variation detection genotyping microarrays genome research hacia resequencing mutational analysis oligonucleotide microarrays nature genetics suppl hirschhorn sklar lindblad-toh lim ruiz-gutierrez bolk langhorst schaffner winchester lander sbe-tags array-based method efficient single-nucleotide polymorphism genotyping proc natl acad sci liu moore gray efficient exact k-nn nonparametric classification high dimensions proceedings neural information processing systems mitchell machine learning mcgrawhill york nuwaysir huang albert singh nuwaysir pitas richmond gorski berg ballin mccormick norton pollock sumwalt butcher porter molla hall blattner sussman wallace cerrina green gene expression analysis oligonucleotide arrays produced maskless photolithography genome research appears proceedings ieee conference computational systems bioinformatics csb ruan wei vega thoreau chia chiu lim zhang peng lin lee yee chee stanton long liu comparative full-length genome sequence analysis sars coronavirus isolates common mutations putative origins infection lancet saiki walsh levenson erlich genetic analysis amplified dna immobilized sequence-specific oligonucleotide probes proc natl acad sci usa singh-gasson green yue nelson blattner sussman cerrina maskless fabrication light-directed oligonucleotide microarrays digital micromirror array nature biotechnology wang fan siao berno young sapolsky ghandour preking winchester spencer kruglyak stein hsie topaloglou hubbell robinson mittmann morris shen kilburn rioux nusbaum rozen hudson lipshutz chee lander large-scale identification mapping genotyping single-nucleotide polymorphisms human genome science wong albert vega norton cutler richmond stanton liu miller tracking evolution sars coronavirus high-throughput high-density resequencing arrays genome research accepted 

probability density test result test result criterion distribution disease distribution disease average accuracies ensemble size average accuracies ensemble size average accuracies ensemble size seeds bagging average roc area ensemble size average roc area ensemble size average roc area ensemble size seeds bagging prob calledpos actuallypos prob calledpos actuallyneg prob calledpos actuallypos prob calledpos actuallyneg prob calledpos actuallypos prob calledpos actuallyneg seeds bagging 
network nnetwork combine network outputs ensemble output input network set-genc auto credit heart hep lung lymp magn prmt rbsm splice percent error magn rbsmprmtlymp splice set-genc lung hepheartcreditauto number tree nodes lungheart splicerbsmlympcredit prmtmagnhep set-genc auto features referenced 

increasing consensus accuracy dna fragment assemblies incorporating fluorescent trace representations carolyn allex allex wisc jude shavlik shavlik wisc schuyler baldwin schuy dnastar frederick blattner fred genetics wisc computer sciences department wisconsin madison west dayton madison tel dnastar south park madison tel genetics department wisconsin madison henry mall madison tel abstract present method determining consensus sequence dna fragment assemblies method trace-evidence directly incorporates aligned abi trace information consensus calculations previously representation trace-data classifications method extracts sums evidence representation determine consensus calls trace-evidence method results automatically produced consensus sequences accurate ambiguous produced standard majorityvoting methods additionally improvements achieved coverage required standard methods trace-evidence coverage error rates low coverage ten sequences introduction goal improve quality efficiency automatic dna sequencing determining sequence bases dna molecules important task sequencing process establish consensus sequence aligned fragments dna task referred consensus calling focus paper usual method accomplish consensus calling relies fragments representation sequences base calls alignment base-call sequences determined computer analysis fluorescent-dye intensity signal called trace data output automatic sequencers applied biosystems abi base calls align sequences majority-voting scheme commonly identify consensus aligned column bases shown earlier case study sequence trimming allex automatic sequencing processes improved incorporation descriptive fluorescent trace information method developed automatic consensus calling trace-evidence incorporates abi trace information trace-data classifications defined prior work allex representation key improvements method previous methods simplified representation trace data sequence base calls representation descriptive captures shape intensity visual characteristics traces trace-evidence method results automatically produced consensus sequences accurate ambiguous addition attains improvements fewer aligned sequences number aligned sequences coverage increase accuracy reduction coverage needed significant results higher accuracy trace-evidence consensus calling mitigates problem errors dna sequences error rate sequences genbank estimated lawrence solovyev dna translated protein results potential errors amino acids mutation single amino acid substantial adverse effects protein-related research change deletion insertion single base lead frame shifts inability identify open reading frames sequencing accuracy significantly dependent careful human examination editing consensus sequences fragment assemblies hand process proceedings international conference intelligent systems molecular biology halkidiki greece aaai press copyright american association artificial intelligence aaai rights reserved time-consuming expensive error-prone automatic assemblies improved consensus accuracies alleviate problems reducing needed coverage accurate sequencing trace-evidence means reduction sequencing costs fragment preparation represents substantial portion expense sequencing large sequencing projects typical produce coverage ensure accurate results show work equivalent accuracy achieved coverage sequences implemented trace-evidence consensus calling previously traceclass sequence trimming dnastar seqman fragment assembly program numerous software packages facilitate sequencing efforts gcg fragment assembly system dolz tigr assembler sutton staden package bonfield phred phrap washington sequencher gene codes corporation contrast systems approach ignores base calls derives consensus directly trace data remainder paper describes common previous method consensus calling detailed description trace-evidence method present empirical evidence effectiveness trace-evidence discussion future work conclusions complete paper readers unfamiliar dna sequencing appendix background make liberal figures illustrate points paper figures trace data displayed representations actual sequencing data supplied coli genome project wisconsin previous method majority common simple method calculate consensus counts number calls base aligned column staden majority base count fractional threshold total count base called unambiguously consensus called ambiguity combination refer method majority figure calculating consensus majority majority approach examines base calls underlying trace data prone errors majority requires minimum number sequences make unambiguous call column base calls total agreement methods directly analyze trace data avoid problems aagaagcactwaggatttggt aagaggcactaaggatttggt aagaagcacttaggatttggt aagaagcactaaggatttggt aagaagcacttaggatttggt consensus aligned fragments column majoritybase consensuscall figure majority consensus calls sequences aligned consensus computed majority threshold set consensus call column calls column calls call percentages threshold column calls resulting consensus call method trace-evidence improve quality automatic consensus calling emulating human analysis trace data ambiguous consensus call made assembly program human editors attempt resolve ambiguity visual examination fluorescent trace data figure describes resolving ambiguity hand watching editors working observed decisions make straight-forward believed program computer capture visual characteristics relevant human editors work information improve automatic consensus-calling overview method developed computing consensus directly incorporates abi trace-data information trace-data classifications developed earlier work allex figure review trace-data copyright american association artificial intelligence aaai rights reserved classifications method consensus calling sums evidence supplied base classification scores refer method trace-evidence consensus method trace-evidence method based idea scores trace-data classifications supplies amount evidence base assigned consensus high strong peak scores supply greatest amount evidence high medium peak scores supply greatest amount evidence high weak peak weak medium strong valley scores decreasing order fact high valley score counter-evidence base figure demonstrates evidence idea dnastar seqman figure human editing ambiguity highlighted column resolved human editors examine traces observe sequences good quality exhibit sharp well-defined peaks trace sequence poorer quality shows small discernible peak trace base miscalled human editors determine consensus determine consensus column aligned bases sum evidence based trace-data classification scores bases evidence base weighted quality trace data local area quality determined examining trace-data classification scores base highest evidence sum identified leader evidence sum leading evidence bases competitors evidence sums competing evidence threshold determines ignorable fraction competing evidence leading evidence leader competitors competing evidence greater threshold leader assigned consensus competing evidence bases surpasses threshold bases included determining ambiguous call definition valley peak negative curvature positive curvature sign change slope strong weak shoulder slope medium peak strong medium weak valley strong medium weak figure trace-data classifications trace-data classification representation shape intensity categorize trace data single base call classes criteria distinguish listed illustrated score assigned classes reflects amount strong medium weak peak valley characteristic exhibited data peak valley nearest base call identified scored gray lines show location base call calculations classifications sets trace data classification trace base called sets trace data shown scores trace combination strong-medium peak base-call location strong-medium valley distanced base-call location scores adjusted reflect distance peak valley base-call location intensity relative traces copyright american association artificial intelligence aaai rights reserved algorithmic details determine consensus column aligned bases types values calculated sequence column trace-data classification scores measure quality data quality trace data weight evidence supplied set classification scores credible higher quality trace data supplies evidence trace data lower quality gaps occur column quality measure decide consensus called gap sum quality measures sequences gap column compare sum quality measures sequences gap gap 
quality sum exceeds non-gap sum consensus called gap problem calling consensus gaps involved simply solved discuss discussion future work section figure evidence traces evidence found traces shaded region trace produce high strong peak score trace yield smaller score traces produce valley scores visual examination traces supports premise vast majority evidence base call counter-evidence base call steps consensus calculation single aligned column details calculations mentioned follow algorithm trace-evidence consensus algorithm single aligned column sequence find quality trace data small window centered column sum sequence gap column compare sum remaining sequences gap sum exceeds non-gap sum return gap determine scores classification scores sequence reduce vector values summarize evidence trace multiply produce vector adjusted data quality sum produce vector total evidence bases find highest evidence leading evidence base leader multiply leading evidence threshold compute maximum ignorable competing evidence compare leading evidence competing evidence competing evidence surpasses maximum ignorable return leader consensus call competitors surpass maximum determine return ambiguity trace-data classifications indicators trace data quality idea similar earlier establishing evidence base calls highly reliable made trace-data peaks sharp well-defined classify strong peaks base calls made trace data classify medium peaks reliable made weak peaks valleys increasingly reliable determine quality examine trace-data classification scores traces called bases sequence bases called ggtacg trace-data classifications traces calculated strong peak scores high data good quality higher scores quality data hand bases called low peak scores non-trivial valley scores base calls obvious copyright american association artificial intelligence aaai rights reserved data reliable lower quality figure compares relative quality trace data figure quality trace data top sequence trace called base exhibits sharp well-defined peak trace-data classifications show high strong peak scores contrast traces bottom sequence flattened overlapping strong peak scores generally nominal top sequence reliable high quality give evidence supplies weight consensus calculations calculations quality predefine constant weight vector classes imply better-quality data base higher values imply lower-quality data definition wsp wmp wwp wwv wmv wsv weight class wsp wmp wwp wwv wmv wsv weight vector quality data sequence column calculated base window size centered column interest calculate vector trace-data classification scores trace base called details calculation allex spi mpi wpi wvi mvi svi dot product w-transpose produces scalar quality measure base average measures produce quality score base center window work found yields good results definition quality measures peak classification scores sum definitions weight vector discussed discussion future work section figure calculation quality score center base total total figure quality score window size bases calculate quality score center base sets trace-data classification scores calculated traces base calls trace data call dot product set scores weight vector computed average quality score base center window copyright american association artificial intelligence aaai rights reserved weight vector similar manner summarize trace-data classification scores consensus computation multiplication weight vector ensures scores supplying evidence high scores weight supply evidence figure demonstrates idea sequence column vector summarizes evidence base base computed reflects amount evidence call base vector computed form matrix trace-data classification scores computing scores trace spa mpa wpa wva mva sva spc mpc wpc wvc mvc svc spg mpg wpg wvg mvg svg spt mpt wpt wvt mvt svt transpose matrix multiplication produces vector evidence values bases multiply quality local trace data produce evidence values adjusted quality data finally sum evidence base aligned column sum values produce total evidence base number sequences column ean ecn egn etn calculated consensus calling completed steps trace-evidence consensus algorithm determination consensus base call appears figure figure summarizing trace-data classification scores trace-data classification scores traces computed dotted weight vector result high trace trace exhibiting highest evidence values traces low summarized values provide evidence trace appropriately highest note calculation trace-data classification scores computed traces contrast calculation quality measure figure scores trace called base computed evidence trace supplies testing code testing consensus calling method incorporated experimental version dnastar seqman fragment assembly program apple macintosh powerpc seqman majority consensus calling method seqman superseded seqman powerful version incorporates trace analysis previous paper allex method fragment assemblies section coli compare correct calls majority traceevidence calls data correct calls assemblies supplied coli genome project wisconsin original assembly copyright american association artificial intelligence aaai rights reserved abi sequences ranged coverage sequences order generate abundance test cases varying amounts coverage developed applied minimization algorithm minimize coverage assembly sequence tca total figure trace-evidence consensus consensus base center column aligned sequences called sequence evidence base multiplied quality score products summed sequences evidence threshold called unambiguously competing evidence surpasses contrast majority method threshold make ambiguous call minimize coverage sequence fragments removed assembly coverage single column fall coverage coverage threshold idea minimize coverage algorithm simple pass assembly sequence determine lowest coverage low-coverage column sequence occurs remove sequence highest low-coverage provided low-coverage threshold sequence low-coverage shorter removed passes assembly repeated sequences removed violating coverage threshold restriction completion columns desired coverage due restriction algorithm summarized minimize coverage algorithm list sequences assembly list sequences considered removal sequence paired lowcoverage lcsi lcs lcsn empty remove sequences lowcoverage threshold remove shortest sequence highest low-coverage update low-coverage values figure steps execution minimize coverage algorithm repeatedly applied minimize coverage algorithm original assembly range coverage thresholds ten produced assemblies differing coverages abundance aligned columns coverage corresponded threshold testing minimized assemblies extracted statistics consensus calling columns corresponded coverage threshold assembly minimum coverage threshold compiled statistics columns coverage sequences exception statistics assembly desired coverage ten include columns coverage ten greater ten results tend remain constant high coverage table lists number consensus calls set results copyright american association artificial intelligence aaai rights reserved ctacatcttacatcacc atcggctacatcttac atcttacatcacc cggctacatcttacatcaccgt pass figure minimize coverage sequences aligned fragment assembly sequences bold provide optimal minimization threshold set sequences assembly column fewer sequences fewer original assembly addition sequence removed causing coverage fall minimum algorithm reduce coverage assembly completes passes outset sequences pass removes lists shorter sequences highest low-coverage sequences removed pass low-coverage threshold sequences end passes empty desired sequences remain assembly results report results compare correct consensus calls made trace-evidence majority coverage ten sequences threshold set seqman default majority trace-evidence graphs figure display number correct calls incorrect calls ambiguous calls methods results show significant improvement trace-evidence method lower coverages coverage 
trace-evidence leveling number incorrect calls large improvement majority method number correct ambiguous calls coverage number ambiguous calls fallen nominal values trace-evidence discussion future work observe striking examples utility traceevidence method base calls column systematically incorrect instances well-defined peak hidden high-intensity valley base incorrectly called high-intensity valley majority methods incorrectly call consensus base trace-evidence makes correct consensus call bases called incorrectly figure occurrence table number consensus calls test results coverage sequences ten number consensus calls included test results listed coverage number ofconsensus calls identified situations traceevidence make incorrect calls overwhelmingly problems involve gaps rarer cases difficulties low evidence sums poor-quality data briefly describe sources incorrect calls results reported incorrect calls coverages half coverages involve gaps column method determining gap inserted consensus consists simple comparison gap non-gap sums quality traces column insertion gap affects column occurs columns side determining gap call context examine data side base interest finding solution calling consensus gaps alignment virtually eliminate incorrect calls made trace-evidence method coverage copyright american association artificial intelligence aaai rights reserved trace-evidence majority calls coverage correct calls calls coverage incorrect calls calls coverage ambiguous calls figure test results results calls amount coverage graphed data point based consensus calls trace-evidence method produces correct calls fewer incorrect ambiguous calls low coverages instances incorrect calls extremely low evidence sums sums low maximum evidence indicative correct call solution label consensus ambiguous defer consensus determination human editors results reported paper solution low evidence calls counted ambiguous category circumvent low-evidence problem commercial version seqman consensus calling reverts majority maximum evidence ten number chosen works practice correct call majority call trace-evidence call figure trace-evidence majority consensus shaded column bases incorrectly called correctly threshold majority method incorrectly computes consensus traceevidence method detects evidence ample evidence calls correct consensus majority situation troublesome fourth sequence assembly case call conflicting base calls unquestioned hand-editing contrast trace-evidence correctly computes absence fourth sequence copyright american association artificial intelligence aaai rights reserved incorrect calls occur cases difficult majority trace-evidence regions poorer-quality trace data peaks overlapping ill-defined obstacle majority base calls incorrect regions trace-evidence difficulty lies relative locations trace peaks peak correct base call significantly offset base-call location result trace-data classifications computed peak detected low score due distance base-call location traces exhibit small distinct peak base-call location scored higher trace-evidence evidence small peak correct trace calls consensus incorrectly case illustrated figure correct call majority call trace-evidence call figure difficult consensus call sequences aligned correct call shaded column majority calls ambiguous consensus column includes conflicting base calls traceevidence method assigns negligible strong peak scores offset peaks traces high strong peak score trace sequence scores incorrectly sum adequate evidence insufficient evidence addition occurrence incorrect calls situations errors due weight vector chose weight vector calculations work chosen empirically listed table vectors conform restriction wsp wmp wwp wwv wmv wsv varying emphasis scores classes vectors table linear peaks parabolic peaks assign values valley classes found vectors greatly outperformed observation contradicts premise valley scores information explanation reconciles opposing observation premise simple functions sensitive make information valley scores approach finding weight vector statistical method multiple linear regression future compare vectors determined rigorous methods chose empirically table weight vectors functions generate weight vectors listed functions linear parabolic trigonometric cases linear peaks parabolic peaks values valley classes functions observed produce fragment assemblies description function linear linear peaks max parabolic parabolic peaks max trigonometric cos copyright american association artificial intelligence aaai rights reserved improving algorithmic approaches sequencing problems plan shift direction research solutions involve machine learning plan artificial neural networks trace-data classifications aid consensus calling basecalling recent years success developing neural network solutions problems molecular biology surged sampling includes protein-structure prediction rost sander dna sequence determination tibbetts bowling golden finding protein binding sites heumann lapedes stormo detection protein-coding regions uberbacher mural neural networks provide good solution biological problems involve intricate interactions strength neural networks lies ability learn recognize complex patterns conclusions goal work improve quality efficiency automatic fragment assemblies goal developed method consensus calling trace-evidence produces accurate consensus sequences reducing hand-editing decreasing amount coverage needed accomplished direct incorporation trace information automatic consensus calling trace-data classifications developed prior work contrast method accurate methods limited representation trace data base calls determine consensus acknowledgments research supported national research service award national institute general medical sciences national science foundation grant iri appendix sequencing background determine sequence bases large segment dna scientist produce small overlapping fragments segment sequence small fragments finally align overlapping regions small fragments establish sequence large segment large segment broken smaller fragments instruments sequence fragments kilo base length large segments generally longer applied biosystems abi modern sequencers determining sequence small fragments made fluorescent-dye labeling ansorge smith fragments dna labeled dyes base identifies base fragment time intensity figure fluorescent trace data intensities sets trace data vary time graphed peak intensity base call made basecalling program general base call corresponds trace highest intensity center graph instance intensity high base labeled prepare sequence fragment sets labeled subfragments produced ends identical end fragment ends vary sub-lengths fragment represented sets sub-fragments end gel electric current applied current linear migration dna gel sequencing machine scans records computer file intensities dyes subfragments migrate past end gel shorter fragments migrate faster longer intensities recorded sequence order original small fragment computer program processes file intensity data called trace data determine sequence bases original fragment process called basecalling simplest case time intensity dye high identifies base intensity high base unknown labeled no-call figure graphs sets trace data base calls copyright american association artificial intelligence aaai rights reserved table consensus calls consensus call bases gap ambiguities combinations adenine cytosine guanine thymine gap small fragments sequenced overlapping regions aligned fragment assembly sequence original segment interest consensus aligned small fragments problem determining consensus column fragment assembly called consensus calling aligned column total agreement base calls calls conflict decision made call consensus base gap indicating insertion sequences ambiguities combinations table lists consensus calls figure illustrates alignment fragments consensus sequence tgcmacgatctattggk-taag tgccacgatct tgcaacgatctattggt-taag tgcaacgatctattggt-taag cgatctattgggntaag consensus aligned fragments figure consensus calling fragments aligned overlapping regions top consensus computed fragments addition calls ambiguous calls gap call made allex baldwin shavlik blattner improving quality automatic dna sequence 
assembly fluorescent trace-data classifications states agarwal gaasterland hunter smith eds proceedings fourth international conference intelligent systems molecular biology louis aaai press ansorge sproat stegemann schwager non-radioactive automated method dna sequence determination journal biochemical biophysical methods bonfield smith staden dna sequence assembly program nucleic acids research dolz gcg fragment assembly programs methods molecular biology lawrence solovyev assignment position-specific error probability primary dna sequence data nucleic acids research heumann lapedes stormo neural networks determining protein specificity multiple alignment binding sites altman brutlag lathrop searls eds proceedings international conference intelligent systems molecular biology bethesda aaai press rost sander prediction protein secondary structure accuracy journal molecular biology smith sanders kaiser hughes dodd connell heiner kent hood fluorescence detection automated dna sequence analysis nature staden automation computer handling gel reading data produced shotgun method dna sequencing nucleic acids research sutton white adams kerlavage tigr assembler tool assembling large shotgun sequencing projects genome science technology tibbetts bowling golden iii neural networks automated basecalling gel-based dna sequencing ladders adams fields venter eds automated dna sequencing analysis san diego academic press uberbacher mural locating protein-coding regions human dna sequences multiple sensor-neural network approach proceedings national academy science usa copyright american association artificial intelligence aaai rights reserved 
improving quality automatic dna sequence assembly fluorescent trace-data classifications carolyn allex allex wisc jude shavlik shavlik wisc schuyler baldwin schuy dnastar frederick blattner fred genetics wisc computer sciences department wisconsin madison west dayton madison tel dnastar south park madison tel genetics department wisconsin madison henry mall madison tel abstract virtually large-scale sequencing projects automatic sequence-assembly programs aid determination dna sequences computer-generated assemblies require substantial hand-editing transform submissions genbank size sequencing projects increases essential improve quality automated assemblies timeconsuming hand-editing reduced current abi sequencing technology base calls made fluorescently-labeled dna fragments run gels present representation fluorescent trace data individual base calls representation fragment assembly improve quality assemblies demonstrate end-trimming sub-optimal data results significant improvement quality subsequent assemblies introduction fundamental goal human genome project determine sequence bases dna molecules late researchers making progress sequencing human dna model organisms maxam gilbert sanger methods evolved painstaking manual generation analysis data incorporation automated computerized techniques ansorge smith connell prober dear staden hunkapiller myers key computers analysis dna naturally represented discrete sequence bases sequence bases thought string alphabet symbols algorithms matching aligning strings well-studied computer science applied problems dna sequencing waterman kruskal critical application involves alignment overlapping sequences bases dna fragments process called sequence assembly sequences bases assembly determined examination fluorescent-dye intensity signal called trace data output automatic sequencers present descriptive representation trace data output applied biosystems abi automatic sequencers output representation abi trace data sequence discrete fluorescent-dye intensities information contained data enormous potential sequence assembly representation output sequencers makes direct data automatic assembly impossible representation makes trace-data information directly accessible automatic dna sequence-assembly programs substantiate belief present case study representation application trims sub-optimal data sequences assembly empirical results show inclusion trace-data information improves quality subsequent assemblies section paper presents background dna sequencing assembly readers unfamiliar representation trace data detailed presentation case study finally ideas future work conclusions complete paper background sequencing procedure consists selecting large segment dna producing overlapping fragments segment sequencing fragment finally appears proceedings fourth international conference intelligent systems molecular biology louis aaai press aligning overlapping areas fragments determine sequence original segment interest abi sequencer large segments dna long hundred kilobases fragments sequenced long work involves sequencing assembly individual fragments aspects procedure detail sequencing fragments basic idea fragment produce set complementary sub-fragments set complementary generated replication polymerase primer replication step deoxynucleotides dideoxynucleotides compete addition growing sequence deoxynucleotides permit elongation dideoxynucleotides terminate replication prober result set sub-fragments encompasses lengths initial primer figure sequence fragment dna set complementary sub-fragments sequencing quantities primer polymerase deoxynucleotides dye-labeled dideoxynucleotide terminators added copies fragment produce set complementary subfragments asterisks designate fluorescently-labeled dideoxynucleotide terminators dideoxynucleotide end sub-fragment labeled fluorescent dye dye labels bases sub-fragments length labeled dye methods labeling exist paper figure shows fragment set sub-fragments set labeled sub-fragments plate polyacrylamide gel electric current applied current migration sub-fragments gel smaller pieces dna migrate quickly larger sub-fragments separated size fluorescent labeling means determination fragment sequence ansorge smith abi sequencer reads intensity trace fluorescent dyes sub-fragments migrate past process called reading trace data produced called trace data set trace data fluorescent dyes set trace data composed discrete measurements points interpolated form continuous curve base calling sets trace data determine sequence bases fragment referred base calling sets trace data synchronized scanned base calling sequencer expects call base fairly regular intervals calls base intervals trace data perkin elmer ten trace-data points interval record points calls made base call trace figure sequence base calls sets trace data sequencer calls base highest trace values similar case calls gray lines base calls made sequencer calls bases order scans trace data calls made examining values trace data ideally trace values base substantially higher case base trace called trace values bases similar case sequencer makes no-call labels base goal obtain exact sequence bases complement fragment practice accuracy base calls made modern sequencers chen kelley sequence base calls trace data depicted figure sequence assembly fragments original dna segment interest sequenced proceed assembling fragments larger segments mccombie martingallardo myers rowen koop fragments overlap produce assembly aligning overlapping regions sequences computer assembly program approximate stringalignment algorithm find optimal alignment sequences base calls needleman wunsch martinez consensus base calls computed forms contiguous sequence dna contig staden figure illustrates idea figure overlapping fragments aligned determine sequence larger segment dna base sequence segment consensus aligned fragments assemble sequences correct base locations sequences align agree completely mccombie martin-gallardo consensus base call cases assigned ambiguity codes listed figure ambiguity call figure portrays multiple sequence alignment ambiguities sequence consensus ideal assembly data flawless sequences align form contig consensus base call fact rarely case difficulties inherent preparation sequencing fragments lead incorrect base calls quality trace data progressively worse end fragment incorrect calls no-calls region kelley perkin elmer figure base ambiguity codes assembly ambiguities resolved single contig formed sequence complete ready submitted genbank timeconsuming task performed human sequence-editors entails visual analysis assembly data rowen koop figure sequences aligned perfect agreement ambiguous base calls consensus sequence underlined trace-data representation commercial assembly programs dnastar seqman gene codes corp sequencher genetics computer group fragment assembly system sequence base calls tracedata information automatic assembly processes schroeder rosenberg edelman seqman sequencher provide graph representation trace data users visual examination human editors make extensive graphs assembly assist resolving ambiguous calls fine-tuning alignments merging contigs rowen koop size sequencing projects continually grows increasingly important reduce kinds costly manual operations mccombie martin-gallardo rowen koop claim manual processes reduced allowing explicit inclusion trace-data information automatic assembly process existing represention abi trace data discrete sequences fluorescent-dye intensities difficult incorporate developed algorithm transforms trace data visually-descriptive representation usable assembly programs trace-data output abi dna sequencer found data files abi analysis program sets data fragment dna fluorescent dyes trace data appears forms sequence raw intensities data processed trace peaks distinct uniform processed data produce graphs made users seqman sequencher studying graphs sequence editors pay attention relative intensities characteristic shapes trace data measure shapes relative intensities found graphs processed data describe representation capturing information make assembly program information editors representation interested classifying shape intensity local trace-data base call define local trace-data data midway previous call current call data midway current call call figure 
refer intervals data base tracedata set base trace-data composed ten data points representing intensities fluorescent dyes sections classification trace data refers single set trace data fluorescent-dye trace-data base figure base trace-data base trace-data trace data base extends midway previous call current call midway current call call overview trace-data representation define composed classes shapes assigned score broad categories defined divided classes broad categories base trace-data shapes test-set peaks accuracy valleys representation data number dna curves backprop ground-truth categorized cross-validation peak experiment data curves experiment categorized valley illustrated figure test-set peaks accuracy valleys representation number sharp nist backprop pronounced ground-truth cross-validation shoulder experiment smooth experiment curve sloping direction figure variety test-set shapes accuracy occur representation trace number data magellan trace backprop data takes ground-truth cross-validation number experiment characteristic experiment shapes sharp curves shoulder long smooth curves peak valley categories data divided classes strong medium weak curves assigned strong classes characterized sharp peaks dips assigned medium peaks valleys characterized occurrence shoulder curves curves weak classes smooth slope direction stereotypical class shapes sketched figure figure stereotypical shapes class curves gray lines base call locations distinction strong medium weak classes clear cases data assigned weighted combination class scores peak valley assigned score reflects amount strong medium weak character exhibited trace data single base peak valley peak valley base called point trace data assign scores peak valley closest location class scores weighted proximity base-call location peaks valleys closer base called higher score make comparisons sets trace data single base call situation classification scores adjusted reflect relative difference intensities heights peaks valleys higher peaks score higher lower peaks lower valleys score higher higher valleys algorithmic details data scanned strong peaks valleys medium peaks valleys finally found weak peak valley assumed step peak valley closest point base called scores assigned based proximity base-call location amount strong medium weak character exhibited examine data strong peaks valleys strong peak detected change negative positive slope likewise strong valley detected change positive negative slope slopes measured change intensity data point strong peak valley found checked amount strong medium character peaks start baseline intensity return baseline scored strong medium true valleys start maximum intensity return maximum intensity peaks valleys found step possess combination strong medium strengths calculate strong medium scores measure local size peaks valleys side peak valley find extremes slopes change directions changing positive negative vice-versa values locations determine fraction total height local area peak valley local extremes calculation center peak valley side scores strong medium classes computed strong peak score medium peak score strong valley score medium valley score peak valley location extreme left location extreme location strong peaks valleys found data scanned peaks valleys medium strength medium peak located slope remained data points significantly data points data points ensure true shoulder curve exists medium peak valley found amount medium weak character computed peaks valleys region slope score medium weak peaks valleys found step combination medium weak assign strengths determine fraction height local area shoulder finding locations side peak valley slope significantly locations peak valley slope-change locations calculation medium weak scores max max min medium peak score weak peak score medium valley score weak valley score peak valley location slope-change left location slope-change location computation medium class scores defined conflict computation assigning strong medium scores combined strong medium scores mutually exclusive combined medium weak scores finally data classified strong medium assignment steps weak peak valley assumed data increasingly-positive decreasingly-negative slopes define data assigned weak valley data slopes decreasingly-positive increasingly-negative scored weak peak partial weak medium scores assigned previous step class score adjusted computed reflect proximity peak valley location base called scores adjusted snew sold class score location peak valley location base call number base trace-data points peaks valleys closer base called higher scores existing representation graph representation representation figure sample base trace-data classification existing representation trace data sequence intensities base call graph representation trace data shown curve interpolated data points representation classification trace data based visual shape intensity trace data base called peak valley gray line location base called point peak valley detected points peak valley scores adjusted reflect distance peak valley point base called scores prior adjustment parenthesis representation trace data based visual shape intensity shown figure contrasted existing representation trace data sequence discrete intensity values representation valley medium strength weak strength detected left base-call location peak strong medium base call scores adjusted reflect peak valley base call location classification scores computed sets trace data base scores modified account relative intensity differences formula accomplishes pnew pold max vnew vold max strong medium weak peak score strong medium weak valley score base trace-data values higher peaks lower valleys higher scores class base trace-data classification representation assigned score desired single class assigned base tracedata selecting peak valley higher sum scores strong medium weak highest score set trace data assigned scores peak higher sum scores compared valley highest scoring class strong single class assignment strong peak anticipate fine-grained coarse classifications trace data summary classification base trace-data broad categories defined curvature include peak valley peak valley categories divided strong medium weak classes curves assigned strong classes characterized sharp peak dip curves assigned medium classes characterized shoulder curves assigned weak classes characterized smooth slope direction scores reflect amount strong medium weak character exhibited scores reflect proximity peaks valleys base-call location scores reflect relative intensity traces single class assigned choosing highest scoring class category higher sum scores base trace-data classification representation defined fragment assembly increase quality efficiency automatic processes demonstrate representation describe method successfully base trace-data classifications important pre-assembly step case study end-trimming quality trace data base calls decreases dramatically read gel progresses kelley good data peaks sharp well-defined scaled high perkin elmer figure shows set trace data progresses good useless accuracy data input dramatically affects results automatic assembly process data sufficient quality produce good assembly end-trimming common pre-processing step helps ensure good data assembly removes sub-optimal data ends sequences seto koop hood mccombie martin-gallardo rowen koop figure deterioration trace data trace data progressively worse gel read end-trimmming experiments dnastar seqman sequence-assembly software program sequences added time contigs sequences compared consensus existing contig acceptable alignment found contig sequence added contig created burks mccombie martingallardo dnastar bad data adds ambiguous incorrect base calls poisons consensus contig prevent subsequent sequences added contig mccombie martin-gallardo bad data spoil consensus 
prevent addition sequences results significant number ambiguities consensus resolved manually mccombie martin-gallardo existing methods existing methods end-trimming include absolute cutoffs n-trimming absolute cutoff method trims sequence data user-specified number bases abi data number bases based observation quality trace data generally deteriorates bases kelley information data base end sequence trimmed sequence trimming data base calls reasonable problem good data trimmed times poor data method n-trimming trims data exceeds allowed number no-calls windows sequence data dnastar seqman sequence-assembly program employs adaptation method end-clip seto koop hood seqman requires parameters sets window size bases specifies number allowed window sequence base calls scanned end window size found number window equal maximum number allowed data window end sequence trimmed figure set window size number allowed bases end sequence trimmed commonly trimming made contemporary abi sequencers sequencers tend make base call trace data erratic results inferior data remains untrimmed n-trimming method number no-calls correlated quality trace data base calls made advisable directly trace data determine quality information contained trace data make intelligent decision quality base calls location trimming trace-class trimming information trace data make end-trimming decisions examine base trace-data classifications defined earlier determine quality regions trace data method simply single class assignment base tracedata assigned class highest score category highest sum scores similar trimming algorithm scans data windows examining windows no-calls trimming sub-optimal base trace-data classifications scan window note class base trace-data base called location general trace data falls strong peak class considered optimal base calls made trace data classification accurate medium peaks trace data produce accurate base calls weak peaks valleys unreliable base calls perform trace-class trimming parameters set size window bases maximum number sub-optimal trace classes allowed window cutoff specifies classes considered sub-optimal cutoff adjusted trimming stringency changed correspondingly stringent cutoff classifications strong peaks sub-optimal cutoff identifying strong medium peaks suboptimal stringent sequence base trace-data classifications scanned end window size found number sub-optimal classifications window equal maximum number allowed data window end sequence trimmed sequence adding sequence assembly data thrown manual editing figure trace-class trimming sequence figure trace-class trimming window size ten maximum number sub-optimal data classifications allowed weak peaks valleys considered sub-optimal box encloses window end data fewer sub-optimal peaks shaded area sequence trimmed empirically evaluate trace-class trimming compare n-trimming optimizing parameters method set data testing parameters set data data sets data coli genome project lab wisconsin gathered assembly fragment coli data sets formed sequences set data assembly trimmed extensively bases locations remained sequence set added longer coli sequences genbank believed fall section coli genome sequences automatically assembled data contigs formed sequences align ideal data contigs ten sequences chosen inclusion data sets contigs genbank sequences removed full untrimmed length sequences reinstated set sequences contig formed separate data set called project independently assembled result projects evaluating trimming methods ten projects form training set optimize parameters ten sets form test set test quality subsequent assemblies optimized parameters training test sets chosen make number projects equal total number sequences similar evaluations estimated expected number contigs total contig length project project formed single contig cases expected number contigs greater regions contig bridged removed genbank sequences estimate expected total contig length simply length contigs extended complete untrimmed sequences data sets table table data sets number sequences actual number project number contigs contig length expected values project training set test set addition projects test set evaluated system unrelated set sequences segment human dna project reached completion number contigs contig length table describes set table human dna data set method results optimized parameters trace-class trimming method separately n-trimming n-trimming varied window size ten increments number allowed window trace-class trimming varied window size ten number sub-optimal peaks allowed trace class cutoffs strong peaks medium peaks weak peaks valleys considered sub-optimal project training set assembled combination parameters quality assemblies evaluated goal end-trimming produce better-quality automated assemblies dna fragments metrics measure quality assemblies number contigs general group sequences assemble small number contigs ultimate goal single contig metric number ambiguities consensus sequence fewer ambiguities means sequences align manual work needed measure total length contigs contigs long incorporating ambiguities measure number contigs contig length absolute deviation expected values number ambiguities average number ambiguous calls score set parameters normalize individually sum metrics data sets set parameters score parameter set normalized sums number contigs total length contigs number ambiguities metrics constants order importance metrics number contigs number ambiguities total length contigs set weight metrics scheme scored sorted parameter sets found general trace-class trimming assemblies resulted window size large bases cutoff defined strong medium peaks optimal number sub-optimal peaks allowed window size n-trimming assemblies resulted window size large bases number allowed small ten minimum scoring parameter sets trimming trace-class trimming chosen optimal parameter settings test set projects assembled top ten parameter settings n-trimming trace-class trimming settings human dna project assembled topscoring parameter sets baseline projects assembled trimming discussion measures evaluation number contigs total contig length number ambiguities trace-class trimming resulted assemblies quality produced n-trimming trimming figure graphs results ten test-set projects average test-set projects absolute deviation expected length contigs falls deviation expected number contigs number ambiguities falls assemblies n-trimming trace-class trimming decrease number ambiguities represents significant decrease amount hand editing assembled projects project number ambiguities resolved decrease bases n-trimming fewer trace-class trimming human dna project significant improvement assembly trace-class trimming assemblies n-trimming trimming table results human dna project trace-class trimming assembly produces contigs compared contigs trimming expected number results reduction number ambiguities assembly n-trimming key success trace-class trimming information contained trace data form base trace-data classifications classifications directly reflect morphology trace data good indicators accuracy base calls n-trimming method trace data examines sequence bases no-calls modern sequencers make base calls trace data erratic searching no-calls longer method assessing accuracy base calls future work trace-data representation attempt capturing visual qualities abi trace data success enriched make powerful relative intensities relative separations peaks identified important features patterns dna sequences golden torgersen tibbetts tibbetts bowling golden study merit incorporation features representation addition refinements made define peak sharpness intensity relative global scale avg absolute deviationfrom expectednumber contigs avg absolutedeviation expected contiglength bases avg trimming n-trimming trace-class trimming number contig ambiguities perkilobase figure 
results results graphed individually ten test-set projects average projects plan explore ways representation improve quality efficiency automated sequence-assembly end-trimming method works preprocessing step assembly pre-processing step base trace-data classifications base calling machine learning systems neural networks trained recognize patterns classification scores base calls system trained recognize patterns make base calls previously unseen data tibbetts bowling golden describe work method trace data input simple base-calling neural networks table human dna project test results trace-class trimming yields assembly expected number contigs compared n-trimming trace-class trimming assembly fewer ambiguities assembly n-trimming base trace-data classes incorporated actual assembly process seqman assembly program consensus computed scheme weight assigned base call sequence idea quality data higher weights result greater contribution consensus computation poorer quality data weights assigned uniformly trapezoidal rule dnastar lasergene user guide case sequence-specific information data weighted inappropriately quality end-trimming base trace-data classifications measure quality base calls sequence weights assigned base call reflect quality data local area call proposed method information specific sequence suggested rowen koop bonfield staden alternately base trace-data classifications assembly process compute consensus sequence encouraging results preliminary studies summing class scores evidence making consensus call post-assembly process significant amount time spent hand-editing machine learning techniques neural networks train system trace-data patterns editors automate significant portion manual process tibbetts bowling golden describe single-sequence automatic editing system neural network confirm calls suggest finally sophisticated analysis trace-data classifications provide information users assembly programs identify problematic areas trace data existence areas homopolymer regions gel compressions noisy data regions generally characterized trace data exhibits concurrent significant intensities peaks dye traces perkin elmer occurrence detected trace-data representation information gathered hand-editors automatic processes requiring assessment data quality conclusions quality efficiency automated dna assembly abi-generated sequences increased incorporation trace-data information process visually-oriented base trace-data classes describe provide representation trace data information makes incorporation shown trimming sub-optimal data assembly results assemblies base trace-data classifications trimming leads decrease number contigs reduction ambiguities closer approximation expected contig length refinements representation investigation acknowledgments mark craven ernest colantonio guy plunkett comments document research supported part national research service award national institute general medical sciences part small business innovation research grant department health human services ansorge sproat stegemann schwager non-radioactive automated method dna sequence determination journal biochemical biophysical methods burks engle forrest parsons soderlund stolorz stochastic optimization tools genomic sequence assembly adams fields venter eds automated dna sequencing analysis san diego academic press chen efficiency automated dna sequencing adams fields venter eds automated dna sequencing analysis san diego academic press connell fung heiner bridgham chakerian heron jones menchen mordan raff recknor smith springer woo hunkapiller automated dna sequence analysis biotechniques dear staden sequence assembly editing program efficient management large projects nucleic acids research dnastar lasergene user guide madison edelman personal communication golden iii torgersen tibbetts pattern recognition automated dna sequencing online signal conditioning feature extraction basecalling proceedings international conference intelligent systems molecular biology bethesda aaai press hunkapiller kaiser koop hood large-scale automated dna sequence determination science kelley automated dye-terminator dna sequencing adams fields venter eds automated dna sequencing analysis san diego academic press kruskal overview sequence comparison sankoff kruskal eds time warps string edits macromolecules theory practice sequence comparison reading addisonwesley publishing company martinez efficient method finding repeats molecular sequences nucleic acids research maxam gilbert method sequencing dna proceedings national academy science usa mccombie martin-gallardo largescale automated sequencing human chromosomal regions adams fields venter eds automated dna sequencing analysis san diego academic press myers advances sequence assembly adams fields venter eds automated dna sequencing analysis san diego academic press needleman wunsch general method applicable search similarities amino acid sequence proteins journal molecular biology perkin elmer dna sequencing chemistry guide foster city prober trainor dam hobbs robertson zagursky cocuzza jensen baumeister system rapid dna dideoxynucleotides science rosenberg personal communication rowen koop zen art largescale genomic sequencing automated dna sequencing analysis san diego academic press schroeder personal communication sanger nicklen coulson dna sequencing chain-terminating inhibitors proceedings national academy science usa seto koop hood experimentally derived data set constructed testing large-scale dna sequence assembly algorithms genomics smith sanders kaiser hughes dodd connell heiner kent hood fluorescence detection automated dna sequence analysis nature staden computer method storage manipulation dna gel reading data nucleic acids research tibbetts bowling golden iii neural networks automated base-calling gel-based dna sequencing ladders adams fields venter eds automated dna sequencing analysis san diego academic press waterman sequence alignments waterman mathematical methods dna sequences boca raton crc press 

appears proceedings international conference machine learning icml multiple levels learning diverse evidence sources uncover coordinately controlled genes mark craven craven biostat wisc david page page biostat wisc department biostatistics medical informatics wisconsin madison usa jude shavlik shavlik wisc joseph bockhorst joebock wisc department computer sciences wisconsin madison usa jeremy glasner jeremy genome wisc department genetics wisconsin madison usa abstract complete genomes numerous organisms determined key problem computational molecular biology uncovering relationships exist genes organism regulatory mechanisms control operation developing computational methods discovering regulatory mechanisms relationships end developed machine learning approach identifying sets genes coordinately controlled coli genome number factors make interesting application machine learning rich variety data types provide evidence task problem uncovering regulatory mechanisms decomposed multiple machine learning subtasks operating erent levels detail iii negative training examples features misleading predictiveness introduction complete genomic sequences organisms determined years genomes sequenced working draft human genome expected completed year signi challenge confronting biologists determine functions genes contained sequences aspect understanding function gene determine conditions active interactions genes end begun research project investigating computational approaches uncovering regulatory mechanisms interactions genes heavily studied organism coli key aspect approach learn predictive models existing data rst step machine learning methods induce models predicting operons informally operons consecutive sequences genes turned shut unit argue interesting machine learning application reasons problem uncovering regulatory mechanisms decomposed multiple related machine learning tasks rich variety data types provide evidence task including dna sequence data numeric gene-expression data hierarchically organized gene annotations negative examples target concept group features apparently predictive group misleading words values features unlabeled data reliable values labeled data issues detail paper revisit concluding section rst issue multiple related learning tasks discuss main task paper predicting sequences genes conmodel predicting operons model predicting terminators model predicting promoters regulatory networks model predicting models discovering regulatory signals figure multi-level learning approach discovering gene regulatory mechanisms tasks highest level represent motivating research task middle level represents main focus paper lowest level represent learning tasks address order make predictions middle level task arrows represent subtask relationships stitute operons task goal work intermediate step addressing problems inferring networks regulatory interactions discovering subclasses sequences involved controlling gene transcription hypothesize operon predictions improved rst identifying control sequences promoters terminators genome recognition sequences understood task address learning subtasks constructing predictive models sequences figure illustrates relationships learning tasks involved work predictions made model learned level passed learning components level input features idea decomposing learning task subtasks time buchanan shapiro applied recently robotic systems sullivan stone problem domain primary task predict operons coli genome approach developing applicable prokaryotic organisms genome coli sequenced wisconsin blattner consists single circular chromosome doublestranded dna chromosome strain coli data set base pairs thought string consisting characters four-letter alphabet gene sequence dna bases carries information required constructing protein proteins perform essential functions prokaryotes single-celled organism separate nuclei promoter terminator figure concept operon curved line represents part coli chromosome rectangular boxes represent genes operon sequence genes transcribed unit transcription controlled upstream sequence called promoter downstream sequence called terminator promoter enables molecule performing transcription bind dna terminator signals molecule detach dna gene transcribed direction determined strands located arrows gure direction transcription gene cells including structural support transport substances response chemical stimuli coli approximately genes located strands chromosome process proteins produced encoding genes referred gene expression individual genes expressed ering levels depending extent genes cell active environmental factors temperature presence nutrients multicellular organisms genes completely shut cells tissues rst step process gene expressed transcribed similar rna sequence expression gene regulated points signi regulatory controls exerted transcription process gene shut preventing transcribed organisms coli sets contiguous genes called operons transcribed coordinately words genes operon turned shut unit complete understanding regulatory mechanisms organisms coli requires sets genes constitute operons figure illustrates concept operon transcription process initiated molecule called rna polymerase binds dna rst gene operon rna polymerase binds special sequence called promoter moves dna template produce rna molecule rna polymerase past gene operon encounters special sequence called terminator signals release dna ceases transcription genes transcribed individually refer special cases singleton operons data learning model operons regulondb salgado include complete dna sequence genome beginning ending positions genes putative genes positions sequences promoters terminators functional annotation codes characterizing genes three-level -leaf hierarchy riley gene expression data characterizing activity levels genes putative genes experiments operons estimated blattner personal communication hundred undiscovered operons coli goal predict operons model learned data problem representation section describe features learning methods assess probability candidate operon length spacing features features relate size intergenic spacing operons non-operons operon size number genes candidate operon within-operon spacing maximum spacing dna base pairs genes candidate operon distances figure genes operon transcribed constraints inter-gene spacing feature ned singleton operons consisting gene distance neighboring genes distances preceding figure figure genes notice genes part candidate operon directionality neighboring genes boolean-valued features indicating directionality preceding genes individually match directionality genes candidate operon recall genes operon transcribed direction refer features collectively neighboring genes features functional annotation features genes operon typically act perform common function expect functions individual genes related functions genes coli three-level hierarchy riley levels hierarchy represent broad intermediate detailed functions path root leaf hierarchy root metabolism small molecules carbon energy metabolism anaerobic respiration measure functional distance genes terms deeply hierarchy match genes completely identical functions distance genes identical broad intermediate functions distance genes identical broad functions distance genes common function distance cases function unknown genes level split erence genes share broad function intermediate function unknown distance distance measure annotations pairs genes features measure functional homogeneity candidate operon collectively refer functional annotation features rst feature represents pairwise functional distance genes candidate operon functional distance gene preceding candidate genes candidate speci cally compute pairwise distances preceding gene genes operon feature analogous rst gene candidate operon transcription signal features discussed section operon special subsequences called promoters terminators upstream downstream subsequences act signals molecule performs transcription ectively telling begin transcription end transcription decide sequence genes represents operon upstream rst gene sequence promoter downstream terminator task recognizing promoters terminators easily accomplished examples types sequences cient conditions promoters terminators evidence operons rst method predictively identify approach learn interpolated markov models imms jelinek mercer 
characterize promoters terminators imms previously modeling biological sequences salzberg task erent interpolated markov model consists set markov models erent orders imm makes prediction case interpolating statistics represented models order imm trained recognize presence signal promoter terminator training set promoters consists sequences bases long promoter promoter sequences aligned common point obtain statistics likelihood base position promoter terminator data set similar consists sequences length imms represent probability dna bases position signal assess probability sequence promoter terminator calculate product imm estimated probability base sequence sjmodel imm base ith position sequence length sequence evaluating assess probability base position imm interpolates thorder markov model st-order markov model imm pri pri notation pri represents probability base ith position st-order model pri represents thorder model th-order model simply represents marginal probabilities base position st-order model represents conditional probabilities base previous base sequence pri pri pri pri sijsi simple scheme set values parameters represent amount weight give model interpolated positions set parameter set training data included occurrences base position set intuition trust st-order probability cient data estimate experiments set induced promoter terminator imms instances signals neighborhood candidate operon scanning promoter model bases immediately preceding rst gene candidate operon similarly scanning terminator model bases immediately gene candidate characterize candidate operon features promoter feature strongest predicted promoter upstream candidate terminator feature strongest predicted terminator downstream gene expression features recent microarray technology enables scientists measure activity level thousands genes experimental conditions wisconsin coli genome project begun generating data work presented paper data rst experiments measurements gene experiment intensity values representing relative amount rna produced gene experimental condition versus relative amount baseline condition refer sets measurements array channels array data compute sets features learning algorithm rst set features based ratios measurements set based raw measurements features based ratios associate gene vector length number experiments elements vector measured ratios gene genes operon coordinately controlled expect expression vectors genes correlated expression vectors randomly chosen genes rst set features classifying candidate operons based pairwise correlations expression vectors genes interest speci cally features characterize candidate operon correlation pair genes correlation rst gene previous gene sequence iii correlation gene gene sequence ratio correlation measure identify genes similar expression pro les expect stronger association genes operon genes transcribed absolute amount rna produced similar derive features raw measurements features treat channel separate experiment approach based model intensity result true underlying signal distributed random noise acj vector expression values jth experiment genes candidate operon operon likelihood observed measurements acj acjjo true acj random variable indicating candidate operon acj represents intensity ith gene candidate jth experiment acj represents density acj normal distribution parameters model represents true signal operon experiment represents standard deviation noise operon experiment approach estimates values jth experiment genes candidate parameter estimated training set operons tting linear function space data point space determined operon training set singletons assess candidate operon compute likelihood ratio experiments treat independent acjjo true acj acj marginal probability observed intensity values jth experiment modeled assuming distribution intensity values experiment exponential determine parameter exponential distribution maximum likelihood estimate approach calculate features candidate operon likelihood ratio genes likelihood ratio rst gene previous gene sequence iii likelihood ratio gene gene sequence collectively refer features section ratio-based features absolute features expression data features negative examples interesting aspect learning task set non-operons nature scienti inquiry biologists identied hundred operons coli focused attention identifying sequences genes constitute operons assemble set putative non-operons exploiting fact operons rarely overlap rule generate set negative examples enumerating sequence consecutive genes strand overlaps coincide operon generated non-operons true operons operons overlap cases probability small learning algorithms robust presence noisy data machine learning algorithms representation candidate operons presented preceding section employ number supervised learning algorithms induce model predicting candidate operon operon work date primarily naive bayes task section describe speci apply naive bayes empirical evaluation presented section evaluate predictive accuracy quinlan quinlan candidate operon consecutive sequence genes strand estimate probability candidate operon data naive bayes determine ojd djo dijo random variable indicating candidate actual operon represents data make determination ith feature heart task estimate likelihood data interest outcomes features provide numeric characterizations candidate operons represent conditional distribution feature class histogram approach rst step procedure choose cutpoints bins procedure selects bin boundaries bin training examples pooling positive negative examples bin examples class assume examples class fell bin smoothing method avoids zero-valued probabilities bayes rule produce estimated probability empirical evaluation run -fold cross-validation experiment data set consisting operons sequences genes thought operons questions answer level accuracy achieve approach accuracy naive bayes compare predictive individual features address rst questions compare naive bayes partitioning data cross validation give essentially feature representation naive bayes set parameters algorithm default settings treat naive bayes model classi posterior probability candidate operon greater classify operon table shows accuracy rates learners false positive true positive rates false positive rate ned true positive rate ned table classi ers present con gurations tend make erent types mistakes interesting question accuracy predictions vary raise lower threshold classify candidate operon figof non-operons test set reason subtle randomly distribute operons test sets operon assign overlapping negative examples test set examples overlap operon training set process ensures test-set overlaps training set nonoperons overlap multiple operons process leads non-operons appearing test set table predictive accuracy -fold crossvalidation experiment method accuracy rate rate bayes true positive rate false positive rate bayes figure roc curves naive bayes ure shows resulting roc curve vary threshold naive bayes model curve informative unlike accuracy numbers table depend class distribution property roc curves practice class distribution operons non-operons depending operon predictions associate erent costs erent types mistakes classier make figure shows roc curve generate curve varying misclassi cation costs running gure suggests predictive accuracy naive bayes perform experiments evaluate contribution individual features predictive accuracy probabilistic model rst experiment making predictions single feature small group closely related features collectively refer cases feature groups experiment learn models leave single group figure shows resulting roc curves models consist single feature groups figure shows roc curves models leave feature group figures illustrate interesting points features vary bit predictive accuracy terminator feature negligible predictive functional annotation neighboring genes expression data promoter features carry bit predictive 
intrue positive rate false positive rate features functional annotation neighboring genes expression data promoter operon size within-operon spacing terminator figure roc curves predictions made individual feature groups reader notice order listings key ects order curves true positive rate false positive rate features functional annotation neighboring genes expression data promoter operon size within-operon spacing terminator figure roc curves predictions made leaving individual feature groups curves stand lowest highest leaving neighboring genes functional annotation feature groups formation interestingly feature groups based erent types data individual feature groups predictive model result combining evidence variety sources figure illustrates model employs features overly reliant single feature leave-one-out curves close all-features curve discussion conclusions research date predicting operons broached issues lessons interest machine learning researchers biased features experiments showed predictive feature groups functional annotation features suspect predictive features high experiments suggest basic problem general genes operons genes genome heavily studied expect functional annotation values genes set operons reliable consistent values genes operons estimate extent bias correct issue features informative labeled examples general crops case promoter terminator features set promoters terminators concentrated set operons case ensure experiments biased represent presence promoters terminators directly learned models predict ensure operon-prediction experiments biased leaving promoter terminator training sets promoters terminators test-set operons general problem features informative instances training set real live test instances prevalent machine learning applications multiple machine learning subtasks lesson experiments addressing learning task decomposing multiple subtasks lesson illustrated fact promoter features predictive access features learned model predict cient conditions promoters naive bayes interesting result experiments predictive performances naive bayes comparable illustrated figure work serves additional data point understanding relative predictive accuracy machine learning approaches applications found naive bayes advantages application outputs probability candidate operon probabilities subsequent processing optimal partitioning genome operons craven naive bayes naturally handles situation singleton candidate operons fewer features within-operon spacing apply candidates absence negative training examples interesting aspect task absence negative training examples nature scienti inquiry biologists identi operons coli focused attention identifying sequences genes constitute operons generate set putative negative examples exploiting fact overlapping operons rare gene sequences overlap operons operons noise negative class labels exceptions general constraint addition conclusions pertinent machine learning research note lessons interest researchers applying learning methods computational biology domains multiple evidence sources experiments determining regulatory relationships genes combining evidence data sources single feature group predictive power learned model predictive feature groups represent diverse types data augmenting expression data related lesson experiments pertains relative gene expression data recent interest identifying sets related genes discovering regulatory relationships microarray expression data eisen brown friedman results suggest cautionary note obtain accurate operon predictions types data conjunction expression data fundamental challenge facing computational biology community determining functions genes newly determined genomes relationships genes argue task addressed employing wide array data sources evidence work presented represents promising rst step general approach argue increasingly challenging problems machine learning involve rich diverse sets data interrelated learning tasks application interesting case study machine learning researchers acknowledgements research supported nsf grant irinih grant univ wisconsin graduate school blattner kendziorski perna richmond helpful discussions anonymous reviewers detailed thoughtful reviews blattner complete genome sequence escherichia coli science brown grundy lin cristianini sugnet ares haussler support vector machine classi cation microarray gene expression data ucsc-crl- dept computer science california santa cruz craven page shavlik bockhorst glasner probabilistic learning approach whole-genome operon prediction proc international conf intelligent systems molecular biology san diego aaai press eisen spellman brown botstein cluster analysis display genome-wide expression patterns proc national academy sciences usa friedman linial nachman bayesian networks analyze expression data proc annual international conf computational molecular biology recomb tokyo japan buchanan learning intermediate concepts constructing hierarchical knowledge base proc international joint conf arti cial intelligence los angeles jelinek mercer interpolated estimation markov source parameters sparse data gelsema kanal eds pattern recognition practice north holland sullivan transferring learned knowledge lifelong learning mobile robot agent proc european workshop learning robots quinlan programs machine learning san mateo morgan kaufmann quinlan decision tree software http rulequest riley coli gene products physiological functions common ancestries curtiss eds escherichia coli salmonella american society microbiology edition salgado regulondb version transcriptional regulation operon organization escherichia coli nucleic acids research salzberg delcher kasif white microbial gene identi cation interpolated markov models nucleic acids research shapiro structured induction expert systems reading addison wesley stone layered learning multi-agent systems doctoral dissertation school computer science carnegie mellon cmu-cs- 

bellwether analysis predicting global aggregates local regions bee-chung chen raghu ramakrishnan jude shavlik pradeep tamma wisconsin madison usa yahoo research santa clara usa beechung shavlik pradeep wisc ramakris yahoo-inc abstract massive datasets commonplace wide range domains mining recognized challenging problem great potential motivated challenge effort concentrated developing scalable versions machine learning algorithms overlooked issue large datasets rarely labeled outputs learn predict due human labor required make key observation analysts queries define labels cases leads problem learning predict query-produced labels dataset entirety simply run query compute labels interesting scenarios predictive model trained data gathered significant incremental cost time challenge accurately predict query-labels projected completion datasets based cost-effective subsets call bellwethers introduction mining large datasets recognized challenging problem great potential effort concentrated developing scalable versions machine learning algorithms large datasets rarely labeled outputs learn predict due human labor typically required severely limits ability apply supervised learning techniques make key observation large class practically motivated problems conventional database queries tag cases attribute values predict mitigating labeling difficulty cases result aggregating database records company predict year worldwide profit item selling item worldwide year company exact profit company accurately predict annual worldwide profit features regional profit collected shorter time week sales smaller area sales wisconsin gained valuable business insight item historical data case information relating item dispersed sales records item create additional per-item features compute desired label worldwide annual profit item conventional olap-style queries fact create training datasets summarizing historical per-item sales region interest state month county week per-region training dataset train predictive data model item introduced collect sales data region aggregate create case item predictive model region estimate desired label worldwide annual profit question region base predictive model good region exists intuitively gathering sales data item region acceptable cost cost reflect real-world marketing expenses predictive model region high accuracy low variance call regions bellwethers problem considered paper identify bellwether regions paper make contributions introduce bellwether analysis framework apply predictive models massive datasets human labor labeling training examples formalize challenges raised framework showing richness problem opportunities future research develop efficient scalable algorithms find bellwether regions evaluate performance real-life datasets demonstrate bellwether analysis rest paper organized reviewing predictive models section introduce bellwether analysis section define basic bellwether analysis problem important variation finding item-centric bellwethers intuitively basic approach finds single region serve bellwether items recognizes regions items types items present scalable algorithm basic bellwether analysis section algorithms item-centric bellwethers sections section present detailed experimental evaluation real synthetic datasets measuring quality bellwethers found efficiency algorithms discuss related work conclude section background formally introducing bellwether analysis review basics predictive models data table attributes called features called target row table called predictive model learns relationship predicts based values called training set denote predictive model returns target target numeric called permission copy fee part material granted provided copies made distributed direct commercial advantage vldb copyright notice title date notice copying permission large data base endowment copy republish post servers redistribute lists requires fee special permission publisher acm vldb september seoul korea copyright vldb endowment acm regression model categorical called classification model decision trees support vector machines neural networks linear regression models examples predictive models quality predictive model measured error equivalently accuracy model expected discrepancy true target predicted classification models misclassification rate expectation making incorrect prediction commonly error measure regression models squared error mse root squared error rmse commonly mse expected squared difference true predicted rmse square root mse reality true distribution generally unknown error model computed estimated data commonly error estimates cross-validation error training-set error cross-validation error compute cross-validation error partition non-overlapping subsets examples train model test model obtain error crossvalidation error error values based distribution assumptions confidence interval crossvalidation error obtained based variance error values commonly training-set error estimate error model train model test obtain error called training-set error trainingset error overly optimistic simple models linear regression models training-set error approximate true error note overhead computing cross-validation error approximately times computing training-set error problem definition introduce motivating formally define problem bellwether analysis intuitively historical data find region week wisconsin small cost accurately predict target year worldwide sales item product based features week sales wisconsin item collected region problem significantly ordinary machine-learning problems features target values generated queries historical database motivating company predict year worldwide profit item selling item worldwide year company exact profit company accurately predict target features regional profit collected shorter time week smaller area focus wisconsin quickly adapt business strategy minimize loss maximize profit week wisconsin bellwether region goal find regions note denote region pair time interval location values find bellwether region company exploit historical sales database tables shown figure record ordertable represents transaction item identified itemid specific time location includes quantity profit earned transaction item information stored itemtable records category expense item advertisement information stored adtable size number colors advertisement identified adno foreign keys obtain item advertisement information transaction straightforward data-mining approach aggregate ordertable obtain target year worldwide profit historical item training set created associating features category rdexpense item called item-table features target train predictive model linear regression model training set model predict target item based features model accurate bellwether analysis needed item-table features sufficiently predictive accuracy model acceptable improve accuracy predictive model adding informative features note information provided ordertable adtable features predict target collecting features item incurs cost extreme sell item worldwide year worldwide profit prediction incurs high cost extreme pay item table information features goal bellwether analysis find cost-effective region features collected region improve accuracy model time interval location define region data acquisition figure shows dimension structures combination interval time dimension place location dimension candidate region regions levels based company experience cost collecting data region defined region loc features item generated queries database ordertable time location itemid adno quantity profit itemtable itemid category rdexpense adtable adno adsize numcolors country state dimension locationdimension time week week week year figure data schema motivating figure dimensions motivating circle regional profit sum profit loc ordertable total profit purchases item region loc circle regional max adsize max adsize loc ordertable adtable maximum size advertisement item region loc note denotes aggregation operator computes aggregate function attribute loc denotes selection operator selects 
records itemid time location equal loc denotes natural key-foreign-key join tables query-generated regional features regions find region small cost highly predictive model built features generated region data historical database find region model queries create training set region training represents item item-table features querygenerated regional features regional profit regional max adsize region item target item principle build predictive model region training set region evaluate accuracy region accuracy cost budget bellwether region item collect data bellwether region cost budget expect model features generated region accuracy regions collect data cost budget basic bellwether analysis generalizing motivating formally define basic bellwether analysis circle historical database historical data item identified motivating relational database ordertable itemtable adtable circle candidate region set candidate region defined combination dimension attribute values circle training item set item included training set denote subset items data region circle target generation query returns target year worldwide profit item denote table attributes target values training items denotes target attribute circle feature generation query returns feature vector item features generated aggregating item data region note itemtable features change regions region motivating returns category expense regional profit regional max adsize item region denote table attributes feature vectors items region set feature attribute names circle cost query returns cost collecting data item region cost selling item week putting transaction information historical database circle predictive model model linear regression model trained data region training set model circle error measure error measures error crossvalidation root squared error model circle coverage coverage fraction items data region intuitively region items model built region accurate accurate small subset items definition basic bellwether problem historical database candidate region set training item set target query feature query cost query error measure error find region criterion error coverage holds region called bellwether region item set model built data bellwether region called bellwether model note definition criterion instantiated ways possibilities circle constrained optimization criterion find error minimized subject coverage user-specified budget userspecified coverage threshold circle linear optimization criterion find error coverage minimized user-specified weights paper constrained optimization criterion regions satisfy constraints called feasible regions note computing error involves training model training set created queries data region section develop efficient algorithm basic bellwether problem item-centric bellwether-based prediction basic bellwether problem find single bellwether region entire set items minimizes error measure item model built bellwether region predict item target problems circle region found historical database generalize items over-fitting problem discussed machine learning literature circle subsets items behavior week wisconsin bellwether region laptops desktops bellwether region week maryland predict target item itemcentric approach define item-centric bellwether problem definition item-centric bellwether problem input basic bellwether problem item find region predicts user-specified budget region called bellwether region item note technically predict target item data itemcentric bellwether problem defined based time point future data put bellwether region predicts item evaluate predicts find bellwether region learn concept bellwether region properties historical items sections present methods finding bellwether regions items discussion extension problem definitions fairly general candidate region set regions form necessarily combinations dimension attribute values feature generation target generation cost queries arbitrary functions necessarily aggregate-select-join sql queries efficient algorithms paper apply special case mentioned fairly general point cases show richness problem opportunities future research circle combinatorial bellwether analysis previously defined bellwether candidates regions candidate combination regions equivalently candidate region set search space combinatorial bellwether analysis extremely flexible large opens possibility finding bellwether region combinations requires techniques efficiently search space circle multi-instance bellwether analysis previously defined feature generation query return feature vector item aggregated data region returns set feature vectors region item aggregation training consists set feature vectors target set setting non-standard similar multi-instance learning circle relational bellwether analysis relational predictive models make predictions based feature vectors historical relational database predict target item case returns relational database consisting data item region sub-database considered historical region point view circle automatic feature generation current formulation user feature generation queries number possibly queries huge desirable automatic feature generation framework note bellwether analysis similar feature selection goal select predictive features orthogonal problems fact models bellwether analysis employ feature selection techniques unique idea bellwether analysis collecting features region requires region-dependent cost finding cost-effective region important features cost free basic bellwether algorithm ideally unlimited resources long candidate region set finite iterate candidate regions region issue query create training set learn model training set finally pick costeffective region bellwether region reality computation expensive efficient algorithm interesting instance basic bellwether problem develop efficient algorithm olap-style basic bellwether search efficient computation structure space olap-style schemas regions aggregate queries historical database star schema fact table ordertable table itemtable adtable link natural key-foreign-key join special kinds attributes attribute item ids set dimension attributes time location dimension structure figure kinds dimension structures circle interval dimension values dimension intervals week values recorded time points week incremental intervals time time general defined kinds windows circle hierarchical dimension values dimension organized tree values recorded lowest leaf level state level tree combination dimension attribute values defines candidate region target generation query arbitrary target values change regions generated feature generation query consists set stylized aggregate-select-join sql queries element feature vector forms stylized queries table summarizes operators circle returns aggregate sum min max average attribute table item region sum profit ordertable returns total profit item region circle returns aggregate attribute table item region max adsize ordertable adtable returns largest size advertisement item region circle pifk foreign key links primary key returns aggregate attribute table item region projection ensures matching row considered aggregate matches multiple tuples sum adsize adno ordertable adtable returns total size advertisements item region individual advertisement counted cost query assume user cost table attributes cost describing cost finestgrained region cost larger region cost sum costs finest-grained regions finally note constrained optimization criterion efficient algorithm basic ideas algorithm prune regions cost coverage model built infeasible regions generate training sets feasible regions data cube computation table operators extended relational algebra select tuples region aggregate group group compute aggregate function omitted aggregates piz project set attributes duplicates natural key-foreign-key join tables techniques applied fact generation training sets feasible regions written single olap query optimized rewrite forms feature generation queries equivalent forms circle circle 
circle pifk pifk note assume aggregate operator performs cube operation dimension attributes returns relation aggregate region identified attribute item identified attribute selection condition rewritten forms remove selection query returns table feature region item joining tables obtain feature vectors items regions joining resulting table table target item associate feature vector target cost constraint coverage constraint expressed olap queries piz sum cost sum cost returns regions cost piz count count returns regions coverage count counts distinct values table single attribute set items interest constant putting feature vector item region pifk query generate training sets pifk piz sum cost sum cost piz count count efficiently evaluate query iceberg cube techniques find regions satisfying constraints apply techniques simultaneously computing multiple aggregate functions details omitted lack space generating training sets feasible regions simply build model feasible region evaluate error model region minimum-error model bellwether region bellwether tree item-centric bellwether-based prediction basic idea find bellwether region item identify subset historical items similar find bellwether region section introduce bellwether tree items characteristics recursively partitioned based item-table features section bellwether cube introduced predefined item hierarchies group similar items bellwether tree similar decision regression tree node tree splitting criterion rdexpense partitions set items parts leaf node unlike decision regression tree predicts target directly based items leaf find bellwether region set items leaf region serve bellwether region item leaf figure shows bellwether tree items rdexpense bellwether region bellwether region item rdexpense depends category item belongs features splitting criteria item-table features depend regions time predict target item redefine item set table itemtable figure attributes column item ids item-table features rdexpense category categorical numeric avoid confusion item-table features features generated regions call regional features rest section adapt standard regression tree construction algorithm build bellwether trees idea bellwether models subsets items branches split define good split recursively create child nodes splits recursive tree construction algorithm scalable extend rain forest algorithm make bellwether tree construction scalable main result provide sufficient statistic evaluating goodness splitting criterion plugged rain forest framework bellwether tree algorithm similar decision regression tree construction algorithm key component bellwether tree construction algorithm method choosing splits internal nodes intuitively pick criterion partitions set items parts bellwether region note reason bellwether regions subsets items reduce prediction error identify subsets items bellwether regions direct approach node find splitting criterion split find bellwether region child subset items expected prediction error reduced call amount error reduction goodness splitting criterion find splitting criterion node set items error denote error model built region set items trained splitting criteria feature pick circle categorical denote splitting criterion values splits child partitions denote pth child partition goodness splitting criterion goodness error error hrp bellwether region bellwether region denote number items circle numeric denote splitting criterion splitting point set splitting criteria rdexpense yesno category desktop laptop figure bellwether tree constraints sorted distinct values splits child partitions goodness goodness error error hrp bellwether regions number splitting points large points small number percentiles set splitting criteria node categorical feature numeric feature algorithm construct bellwether tree recursively split node choosing splitting criterion node termination condition holds simple termination condition stop number items node falls threshold algorithm shown figure avoid over-fitting tree constructed standard pruning techniques minimum description length principle applied omit details predict target item pass item tree leaf node based item-table features bellwether region item subset leaf node bellwether region item spend budget collect data bellwether region item put data database finally predict target input bellwether model trained leaf node bellwether tree algorithm note bellwether tree algorithm node splitting criterion child partition produced splitting criterion solve basic bellwether problem solving problem requires reading training sets feasible regions call dataset training sets feasible regions entire training data generally assume entire training data fits memory bellwether algorithm scan entire training data times number levels tree number splitting criteria considered node node partitions item set non-overlapping subsets items union training examples nodes level fact entire training data assume item table fits memory number transactions large number items modest reduce tremendous cost extend rainforest algorithm designed efficiently learn regular decision tree disk-resident dataset setting basic idea algorithm scan entire training data level tree scan collect sufficient statistics determining splitting criteria active nodes level active node met termination condition definition goodness splitting criterion sufficient statistic computing goodness splitting criterion error hrp child partition produced error hrp error model built bellwether region partition size partition note error computed process parent node definition bellwether region error hrp minr error create entry minimum error child partition obtain minimum error single scan entire training data algorithm presented figure note simplicity show algorithm rf-read assume sufficient statistics level fit memory techniques rf-hybrid proposed improve efficiency applied similar manner lemma bellwether tree algorithm produces tree produced bellwether tree algorithm scans entire training data times number levels tree bellwether cube similar bellwether tree bellwether cube finds bellwether regions subsets items unlike bellwether tree item subsets induced tree structure item subsets bellwether cube defined item hierarchies figure shows item hierarchies hardware low represents subset items belong hardware division low expenses bellwether region bellwether region item subset desktop bellwether cube systematically finds bellwether region cube subset items induced item hierarchies predict target item exploratory tool rollup drilldown analysis rest section define bellwether cubes show perform bellwether-based prediction conduct rollup drilldown analysis single-scan bellwether tree algorithm call splitnode root node item set prune resulting tree function splitnode node item set termination condition holds return foreach splitting criterion foreach child partition created find bellwether region evaluate goodness pick criterion 
maximum goodness create child nodes based foreach child node call splitnode bellwether tree algorithm foreach level tree foreach active node level splitting criterion foreach child partition set minerror scan entire training data foreach feasible region read training set region foreach active node level foreach splitting criterion foreach child partition created build model region error minerror minerror error size foreach active node level foreach splitting criterion compute goodness minerror size split based goodness figure bellwether tree algorithms bellwether cube construction algorithm presented improve efficiency transform model construction efficient data cube computation based frameworks developed main technical contribution show building weighted squares linear models 
computing training-set means squared errors models formulated algebraic function efficiently computed data cube computation techniques cube subsets items similar bellwether trees training item set extended item table itemtable figure attributes item hierarchy figure values recorded item table lowest leaf level hierarchy combination values nodes hierarchies defines cube subset items desktop hardware low denote cube subsets items levels category expense division expense range desktop represents subset items records desktop hardware represents subset items leaf hardware node figure shows levels induced hierarchies figure suggested levels organized lattice structure collection cube subsets combinations values nodes levels call cube subsets lowest level category expense level base subsets note difference item table fact table circle item table itemtable figure information items independent candidate bellwether regions information predict target item attributes item table create subsets items olap terminology item hierarchies figure fact dimensions item table avoid confusion call dimensions item table item hierarchies regions defined item hierarchies item subsets circle fact table ordertable figure information items candidate bellwether regions reserve term dimensions dimensions figure fact table term regions candidate bellwether regions defined combinations values dimensions bellwether cube algorithm inputs definition extended item table item hierarchies bellwether cube cube subset items bellwether region subset items reason collection called cube collection similar data cube difference data cube replaced aggregate number sum rdexpense items subset note identify bellwether region subset build models items model trained dataset generated region training represents distinct item size small models trained examples practice user size threshold bellwether cube includes cube subsets call subsets items significant subsets bellwether cube predict target item exploratory tool prediction item predict target item historical items similar find bellwether region item bellwether cube context items co-occur cube subset considered similar intuitively similarity decreases level cube subset higher higher extreme case cube subset items lowest similarity item identify cube subsets include item desktop cube subsets include desktop hardware desktop medium hardware medium desktop medium hardware bellwether regions subsets items candidate bellwether regions predicting item items subsets similar item similarities pick candidate bellwether region bellwether model model built bellwether region lowest upper confidence bound error defined confidence interval error user-specified parameter bellwether region item confidence bound lowest error picked bellwether region low error stable similar bellwether tree choosing cube subset items bellwether region spend budget collect data item region put data bellwether model built predict target taking input rollup drilldown analysis bellwether cube similar data cube cross-tabular user interface data cube supports rollup drilldown operations applied data cube cube subset items corresponds cell cross tabulation difference showing aggregate number count sum show bellwether region error bellwether model built region cell cross tabulation rollup drilldown operations change levels cube subsets presented current cross tabulation operations user explore space subsets items levels understand bellwether anyall division category item hierarchy category hardwaresoftware desktop laptop anyall range expense item hierarchy rdexpense mediumlow high category expense category range division expense category division range expense division range level hierarchy tree level hierarchy tree figure item hierarchies figure levels cube subsets behavior database omit details user interface note treatment applying data cube interface predictive models found algorithm unlimited resources enumerate cube subsets items cube subset apply basic bellwether search algorithm find bellwether region subset single-scan bellwether cube algorithm obvious algorithm inefficient cube subset items solve basic bellwether problem subset cube subsets nested carefully sharing computation improve efficiency large memory build bellwether cube single scan entire training data consists training sets feasible regions subsection describe single-scan technique discuss technique exploits shared computation subsection assume size main memory number significant cube subsets items main memory large simultaneously hold small amount size-fixed data significant cube subset number items modest number significant cube subsets items modest reasonable assume number significant cube subsets fit memory number significant cube subsets fit memory partition cube subsets parts part fits memory simplicity omit details denote bellwether region item subset definition error hrs minr error similar bellwether tree algorithm entry cube subset items memory scan entire training data find region minimum error algorithm shown figure note omit details select significant cube subsets items iceberg query computation techniques lemma single-scan bellwether cube algorithm outputs bellwether cube algorithm single scan entire training data optimizing repeated model construction describe techniques share computation building models nested subsets items note single-scan bellwether cube algorithm build model significant cube subset items cube subsets nested build models cube subsets hardware medium medium hardware included included review advantage nested-subset structure regular data cube computation compute aggregate function sum average cube subset aggregate function distributive algebraic aggregate values cube subsets computed efficiently subsets items partition aggregate function distributive function sum count examples distributive functions aggregate function algebraic function returns fixed-length tuple function distributive functions algebraic avg distributive function returns sum count sums sums counts separately divides total sum total count details observation fixing region error distributive algebraic function data cube computation techniques directly applied efficiently compute errors significant cube subsets items region note knowing error feasible region significant cube subset items sufficient build bellwether cube focus error computation model construction model construction embedded error computation observation error computed distributive algebraic aggregate functions replace foreach significant cube subset items build model line figure data cube query returns error significant cube subset omit details data cube computation question make error distributive algebraic aggregate function depends kind predictive model classification models chen developed scoring function decomposition technique test-set-based accuracy equivalently error computation treated computing distributive algebraic aggregate functions model distributively algebraically decomposable indecomposable models approximation technique called probability-based ensemble proposed make model distributively decomposable small accuracy drop techniques directly applied regression models chen developed technique efficiently build ordinary squares ols linear regression models nested cube subsets extend technique ways technique handle ols linear regression models weighted squares wls linear regression models statistics handle aggregated target values total sales technique directly transforms computation training-set squared errors wls linear models computation algebraic aggregate functions note linear models simple models training-set errors good estimates true errors models experimental results suggest behavior training-set errors similar cross-validation errors linear models review basics linear regression models details matrix row select significant cube subsets items iceberg cube 
query foreach significant cube subset items set minerror scan entire training data foreach feasible region foreach significant cube subset items build model error minerror minerror bellwetherregion output bellwetherregion figure single-scan bellwether cube algorithm training column represents feature jth feature note training examples feature values simplicity features assumed numeric column vector length target ith row positive diagonal matrix specifies weight weight ith linear regression model written model parameters error term ith p-element column vector denote model parameters ols linear model minimizes wls linear model minimizes note wls linear model reduces ols linear model wls linear model wls linear model wls denotes matrix transpose straightforward matrix derivation obtain weighted sum squared errors wls wls weighted squared error quantity divided number degrees freedom note number matrix vector sizes depend number examples sufficient obtain wls linear model training-set weighted squared error cube subset items subsets partition denote feature matrix target-value vector weight matrix sser denote weighted sum squared errors model trained region items theorem theorem sser algebraic aggregate function sser wkyk wkxk wkyk wkyk wkyk wkxk wkyk proof size returned fixed independent sizes straightforward matrix derivation wkyk wkxk wkyk returns sser boxshadowdwn experimental results section present experimental results show bellwether regions exist real datasets bellwether trees cubes improve prediction accuracy bellwether region found basic bellwether search demonstrate efficiency scalability proposed algorithms mail order dataset mail order dataset transaction records real catalog company year items transactions removing outliers items transactions selected analysis fact table transactions orders records order single item specific time specific location net profit include constant term regression model set earned order candidate region defined time interval months location midwest time location dimensions similar figure differences time dimension weeks month months months location dimension levels state division region cost region defined number months time interval number zip code areas location divided goal find cost-effective region features generated region predict total profit item entire time interval months features item profit number orders region information mailed catalogs ols linear regression model predictive model fold cross-validation root squared error error measure figure shows error bellwether model function budget understand bellwether model performs plot curves circle bel err curve shows error bellwether model model built data bellwether region point represents error bellwether model cost specific budget circle avg err curve shows average error point represents average error regions costs specific budget circle smp err curve shows performance random sampling approach randomly select collection regions cost collection budget note collection correspond olap-style region defined section candidates bellwether search result figure shows budget error bellwether model converges means increase performance model increasing budget bellwether region found months state maryland bellwether model performs random sampling approach significantly average case figure shows uniqueness bellwether regions point curve labelled represents fraction regions model error confidence interval error bellwether model specific budget high fraction means bellwether region found indistinguishable regions budget low fraction means bellwether unique region perform bellwether shown figure budget bellwether regions found unique figure shows plot error budget trainingset error cross-validation error note figure verifies simple models linear models training-set error approximates cross-validation error computed efficiently finally evaluate item-centric bellwether-based prediction mail order dataset result shown figure methods bellwether tree labelled tree bellwether cube labelled cube method simply bellwether region found basic bellwether search labelled basic -fold cross-validation prediction errors method budgets reported figure result shows budget bellwether cube bellwether tree improve accuracy basic bellwether search improvement significant reason difference subsets items dataset book store dataset show results real dataset clear bellwether region book store dataset sample large bookstore company stores spread united states dataset obtained transactional data books sold year states usa totaling transactions books thousand occurring books books transaction records stores spread cities states similar mail order dataset time location dimension attributes define candidate regions similarly ols linear regression models cross-validation errors determine bellwether regions result shown figure figure error bellwether model converge budget identify bellwether region high confidence shown figure large fraction regions indistinguishable returned basic search algorithm reason find clear bellwether regions unique bellwether region budget point dataset small note dataset transaction records stores small sample actual data warehouse prediction errors basic tree cube methods shown figure clear winner found simulation results understand bellwether trees cubes improve accuracy bellwether-based prediction conducted simulations schema mail order dataset synthetically generated data records item table items binary features target item generated decision tree number nodes based item-table features node decision tree randomly create tree nodes randomly choose bellwether region bellwether model leaf node tree bellwether region model leaf node generated based subset items falls leaf node determine target item item-table features find leaf node item belongs query regional features bellwether region leaf node regional features target generated linear regression model kxk degrees error model parameters bellwether region leaf node varying number nodes decision tree standard deviation error term create datasets degrees complexity noise setting number nodes noise level generate datasets average -fold crossvalidation errors datasets reported point figure figure shows prediction errors cube basic tree methods functions noise level complexity bellwether distribution learned set tree nodes cube tree consistently basic method noise large difference small explain cube tree significantly improve accuracy figure shows prediction errors methods functions complexity bellwether distribution number nodes tree generates dataset noise level set tree cube methods consistently basic method bellwether distribution complex accuracy improvement provided cube tree methods smaller algorithm efficiency scalability synthetic data evaluate efficiency scalability proposed algorithms show entire training data fit memory bellwether tree single scan bellwether cube algorithms significantly improve efficiency show algorithms scale linearly size entire training data experimental setting generate data item table fact table item table randomly generated items item hierarchies numeric attributes varying number nodes item hierarchies generate numbers cube subsets test performance bellwether cube algorithm varying number numeric attributes test bellwether tree algorithm numbers features fact table tree-structured hierarchical dimensions varying number nodes dimension hierarchies generate numbers regions generate transaction item region result region transactions size fact table total budget bel err avg err smp err budget act ion dis tin isa ble budget bel err avg err smp err budget basic ree cube figure 
basic bellwether analysis mail order dataset figure bellwether-based prediction mail order dataset number regions times target values generated based predefined bellwether regions small errors regional features randomly generated note synthetic data correspond real-world problems sufficient test efficiency scalability algorithms cost depends size fact table bellwether cubes cpu cost depends number significant item subsets bellwether trees cpu cost depends number splitting criteria considered captured number features item table ordinary squares linear regression models predictive models cost learning model depend concept learned synthetic data correspond real dataset computation cost roughly experiments assume entire training data training data feasible regions generated iceberg queries saved disk implement techniques iceberg queries focus performance bellwether tree cube algorithms denote bellwether tree algorithm tree bellwether tree algorithm tree bellwether cube algorithm cube single-scan bellwether cube algorithm singlescan cube single-scan cube data cube optimization optimized cube set maximum tree depth algorithms implemented java run linux machine pentium ghz cpu ram results compare algorithms algorithms designed disk-resident datasets note entire training set fits memory expect performance difference entire training set exceeds size memory performance algorithms worse algorithms designed disk-resident datasets fact case algorithm run memory compare algorithms simulation algorithms counterparts time training data region read data disk training data cached memory result shown figure request training data requires disk read single-scan cube optimized cube tree algorithms budget bel err avg err samp error budget fra cti dis tin isa ble budget singleregion cube ree figure bellwether analysis book store dataset noise cube basic tree number nodes cube basic tree figure error cube basic tree simulated data thousands examples optimized cube naive cube singlescan cube naive tree tree millions examples optimized cube singlescan cube millions examples tree figure scalability algorithms show scalability single-scan cube optimized cube figure tree figure algorithms scale linearly number examples entire training data optimized cube performance single-scan cube note tree takes time cubes tree scans entire training data level tree cubes scan entire training data techniques developed reduce cost tree cubes item hierarchies tree additional numeric item features finally investigated characteristics optimized cube tree respect number significant item subsets number item-table features optimized cube applied millions training examples tree applied million training examples figure shows optimized cube scales linearly number significant item subsets figure shows tree scales linearly number item-table features related work conclusion marketing analysts long understood locations good predictors larger populations peoria bellwether problem formulation terms cube-space regions olap data mining machine learning literature key idea introduce bellwether analysis olap queries generate target values class labels prediction tasks opens possibility developing prediction system expensive human labor data labeling paper demonstrated feasibility idea bellwether analysis shown developed efficient scalable algorithms future work apply bellwether analysis real-life datasets explore paradigm olap queries generate training sets include features targets kinds prediction tasks related work greiner papers discussed budgeted learning similar bellwether analysis goal problems find cost-effective make predictions examples aggregate queries exploit large historical data create features target values aggregate queries generate features common practice relational learning general create feature item -to-n relation requires aggregation friedman aggregate queries create features relational extension bayesian networks perlich introduced hierarchy relational concepts features generated aggregate queries research field find cost-effective terms real data collection cost make predictions considers queries generate target values turn large unlabeled dataset labeled predicting future target values based historical data generally discussed time-series analysis forecasting literature statistics aware problem formulation similar literature algorithmic aspect bellwether analysis evaluating iceberg queries create training set feasible regions constructing decision regression trees large datasets studied contribution make develop sufficient statistics bellwether trees existing algorithms scale construction bellwether trees interesting techniques applied bellwether analysis predictive models olap-style space considered based existing work developed technique efficiently compute squared errors weighted linear models beyer ramakrishnan bottom-up computation sparse iceberg cubes sigmod chen chen lin ramakrishnan prediction cubes vldb chen ramakrishnan barford chen yegneswaran composite subset measures vldb chen dong han wah wang multi-dimensional regression analysis time-series data streams vldb friedman getoor koller pfeffer learning probabilistic relational models ijcai gehrke ramakrishnan ganti rainforest framework fast decision tree construction large datasets vldb gray chaudhuri bosworth layman data cube relational aggregation operator generalizing group-by cross-tab totals data mining knowledge discovery hamilton time series analysis princeton press han pei dong wang efficient computation iceberg cubes complex measures sigmod harinarayan rajaraman ullman implementing data cubes efficiently sigmod kapoor greiner budgeted learning bounded active classifiers ecml mehta rissanen agrawal mdl-based decision tree pruning kdd mitchell machine learning mcgraw-hill perlich provost aggregation-based feature invention relational concept classes sigkdd quinlan programs machine learning morgan kaufmann quinlan rivest inferring decision trees minimum description length principle information computation ray craven supervised versus multiple instance learning empirical comparison icml seber lee linear regression analysis john wiley sons shao han xin mm-cubing computing iceberg cubes factorizing lattice space ssdbm zhou multi-instance learning survey technical report nanjing optimized cube significant subsets tree item table features figure characteristics optimized cube tree 

weight space cost output network outputs combiner input feature feature feature feature output output output hidden units classes classes output classes foroutput feature ifeature feature feature giving advice preferred actions reinforcement learners knowledge-based kernel regression richard macliny jude shavlikz lisa torreyz trevor walkerz edward wildz computer science departmenty computer sciences departmentz minnesota duluth wisconsin madison kirby drive west dayton street duluth madison rmaclin umn fshavlik ltorrey twalker wildtg wisc abstract present formulation providing advice reinforcement learner employs supportvector regression function approximator method extends recent advice-giving technique called knowledge-based kernel regression kbkr accepts advice single action reinforcement learner kbkr users set states action greater linear expression current state technique call preference kbkr pref-kbkr user provide advice natural manner recommending action preferred speci set states preferences essentially means users giving advice policies values natural humans present advice present motivation preference advice proof correctness extension kbkr addition show empirical results method make effective advice reinforcement-learning task based robocup simulator call breakaway work demonstrates signi potential advice-giving techniques addressing complex reinforcement learning problems demonstrating support-vector regression reinforcement learning introduction advice-taking methods proven effective number domains scaling reinforcement learning complex problems clouse utgoff lin gordon subramanian maclin shavlik andre russell kuhlmann promising recent method called knowledge-based kernel regression kbkr mangasarian shavlik wild successfully applied reinforcement-learning problems maclin advice kbkr takes form if-then rule dist goalcenter angle goalcenter goalie qshoot copyright american association arti cial intelligence aaai rights reserved rule states distance center goal equal angle player goal center goalie greater shooting greater selecting part rule dif cult human advisor knowledgeable values estimate actions environment natural approach human suggest action preferred understand values affect learning dist goalcenter angle goalcenter goalie prefer shoot pass present formulation based kbkr call preference kbkr pref-kbkr user type advice data support-vector regression section review support-vector regression kbkr technique present pref-kbkr algorithm present task based robocup simulator noda call breakaway demonstrate pref-kbkr testbed knowledge-based support-vector regression section present basics kbkr complete discussion mangasarian support-vector regression svr unknown function vector numeric features describing instance instance labeled describe soccer eld point view player expected taking action pass rst simple linear approximation vector weights features offset vectors column vectors transposed training set states consisting input vectors rows matrix vector desired values learn model nding solution problem appears proceedings twentieth national conference artificial intelligence aaai pittsburgh denotes vector omit clarity understanding scalar solutions problem ranked meet performance criterion minimum error respect values kernel approach weight vector replaced dual form converts aat generalize formulation replacing aat term kernel produce treat linear quadratic programming problem simply develop formulation error measure minimized producing solution formulation mangasarian min jbj cjjsjj xed parameters formulation vector measure error solution training penalize inaccuracies objective function minimized minimize weighted sum terms one-norm computes sum absolute values denotes absolute penalties penalize solution complex parameter trading inaccurate solution term complex solution terms resulting minimization problem presented linear program solver produces optimal set values work examine simpler linear regression formulation effective produce solutions easily understood user formulation written min jjwjj jbj cjjsjj empirical tests employ linear formulation linear models understandable scale large numbers training examples tile coding sutton barto produce non-linearity models reader note identical linear kernel aat cplex commercial software package solve linear program knowledge-based kernel regression kbkr piece advice domain knowledge represented notation read conditions hold output equal exceed linear combination inputs threshold term term user region input space advice applies row matrix values represents constraint advice user give advice rst rule introduction if-then rule matrix vector rows rst row column feature dist goalcenter row hold row column feature angle goalcenter goalie row values negated capture idea condition represents condition vector simply rule scalar advice attempting learn function action shoot advice format user reinforcement-learning task set states speci action high low mangasarian demonstrate advice implication approximated set equations solution employ slack variables equations partially met data advice imperfect formulation min jbj cjjsjj jjzjj slacks introduced formulas partially met piece advice xed positive parameters penalize slacks words slacks advice partially learner mangasarian tested method simple regression problems demonstrated resulting solution incorporate knowledge testing small feature spaces tested advice constraints input features svr preference kbkr order regression methods rst formulate problem regression problem previous research dietterich wang lagoudakis parr employed support-vector approaches reinforcement learning methods focused learning single state-evaluation function relied world simulator simulate effects actions strong assumption previous work maclin call slacks short variable eqs slack investigated support-vector approaches reinforcement learning dif culties applying kbkr order learn function needing world model formulated learning problem learning set regression models action environment employ supportvector regression methods learn functions individually technique effective number extensions nements kbkr method led signi gains performance signi drawback employing advice advisor determine function meet exceed set states dif cult note learn models action separately problem formulated solver set values action solve parameters simultaneously index set actions set problem min ajj jbaj cjjsajj action ata parameters model action values states action values estimates values action states set slacks action formulation appealing sense action functions solved simultaneously practice individual action optimization problems independent solved separately bit cient computationally standard kbkr employ approach formulation open possibility relative values pair actions simultaneously formulation represent form advice set conditions action preferable precise terms represent advice form read conditions hold preferred action exceed nonpreferred action type advice action marked preferred essence approach call preference kbkr pref-kbkr short writing models apx anx order incorporate nonlinear kernel prior knowledge formulation mangasarian assume accurately approximated linear combination rows bat apat anat kernelize equation obtaining tnk finally require kernels symmetric order incorporate optimization problem employ proposition literature proposition mangasarian shavlik wild proposition set ftjk nonempty xed implication equivalent system linear equalities solution substituting formulation advice proposition interested corollary corollary set ftjk nonempty xed implication equivalent system linear inequalities solution atp atn proof corollary directly substitutions concreteness prove corollary proof proposition mangasarian template implication equivalent system equations solution captures notion implication solution makes left-hand side equation true right-hand side false motzkin theorem alternative mangasarian insoluble equivalent system solution atp atn expression states non-negative non-zero contradict nonemptiness set ftjk assumed proposition ftjk solve equation obtain contradiction dividing equation rede ning obtain system note multiple pieces advice corollary indexing pieces advice set adding constraints prefkbkr formulation problem min ajj jbaj cjjsajj jjzijj action 
ata piece advice atp atn bti readability omit subscripts note added slack variables advice satis inexactly close section note experiments introduced nonlinearity tiling nonlinear kernel simple form advice wtp wtn analysis repeated obtain optimization problem incorporates advice min jjwajj jbaj cjjsajj jjzijj action aawa piece advice bti note linear combination rows formulation formulation linear kernel aat robocup soccer breakaway problem demonstrate effectiveness formulation experimented subtask developed based robocup soccer simulator noda call task breakaway similar spirit keepaway challenge problem stone sutton breakaway played end soccer eld objective attackers score goals defenders breakaway game ends defender takes ball goalie catches ball ball kicked figure sample breakaway game attackers represented light circles dark edges attempting score goalie goal shown black ball large white circle bounds goal scored game length reaches seconds simplicity rules player behavior goal off-sides rules figure sample snapshot breakaway game experiments con guration attackers defenders goalie simplify learning task attackers learn control ball attackers ball follow hard-wired strategy move intercept ball estimate reach faster attacker move good shooting position goal wait receive pass attacker possession ball learnable choice actions choose move ball pass ball teammate shoot ball goal limit movement actions choices forward center goal directly goal left circle centered goal shoot action directs ball center side left side goal whichever path blocked goalie note high-level actions consist multiple lowlevel commands simulation defenders goalie follow simple hard-wired strategy ball fastest intercept move block path ball nearest attacker prevent receiving pass goalie pick ball close moves block path ball center goal effective strategy attackers choose actions randomly score goalie games non-goalie defenders case attackers goalie set features describes world state perspective attacker ball features provide pair-wise distances players features distances attacker ball left center portions goal features distance attacker center goal feature angle formed players angles goalie attacker ball left center portions goal angle top-left corner eld center goal attacker ball time left game task made complex simulator incorporates noise players sensors addition player actions small amount error small probability player attempting pass ball misdirect send bounds attackers receive reward end game based game ends score goal reward ball bounds goalie steals time expires reward missed shots including goalie blocked lead rewards chose neutral order discourage shooting work keepaway stone sutton showed reinforcement learning successfully tile encoding state space feature discretized times set overlapping bins representation proved effective experiments task tiling experiments experimentation performed experiments breakaway learning algorithms pref-kbkr kbkr learner advice individual actions support-vector reinforcement learner advice attackers pooled experiences learn single shared model attackers model choose predicted action time exploitation randomly choose action exploration set discount rate games time-limited gave piece advice actions advice shoot appears introduction advise passing goal teammate closer goal goalie advise moving ahead goal closer teammate advise moving goal advise moving left angle topleft corner advise moving angle exceeds advise action lower constant determine computing training set experiments pref-kbkr advice similar advice kbkr preferences provided advice preferring shoot pass shown introduction pass preferred move ahead goal teammate closer goal goalie piece relative advice prefer move ahead shoot goal closer teammate goal provide preference advice move actions simply advice actions standard kbkr approach set values optimization problems examples scaling number examples penalizing average error training examples games played pref-kbkr kbkr advice figure average total reinforcement function number games played algorithms experiments total error varying number examples small number settings nonadvice approach main experimental control found worked kbkr-based algorithms simply chose values experimented settings initial values scale game penalty matching advice reduced function number games played incremental algorithms support-vector machines developed simply retrain models scratch games training examples action choose half examples uniformly earlier games half exponentially decaying probability age order recent experiences focusing recent data approaches training cycle recompute chosen training examples one-step sarsa learning sutton barto obtained recently learned models figure shows results experiments function number games played report average reward previous games curve average ten runs results show giving preferential advice advantageous breakaway leading statistically signi improvements performance kbkr advice approach cases unpaired t-tests results games played related work proposed mechanisms providing advice reinforcement learners clouse utgoff developed technique human observer step provide advice form single actions speci state lin replays teacher sequences bias learner teacher performance laud dejong developed method reinforcements shape learner methods providing mechanism advice-taking differs signi cantly pref-kbkr form usage advice researchers examined advice-taking methods form programming language procedural advice gordon subramanian developed method accepts advice form condition achieve goals operationalizes advice genetic algorithms adjust respect data work similar form advice signi cantly approach optimization linear programming means determining incorporate advice maclin shavlik present language providing advice reinforcement learner includes simple if-then rules complex rules involving recommended action sequences rules inserted neural network learns future experience kbkr support-vector machines unlike pref-kbkr maclin shavlik approach user preferences action andre russell describe language creating learning agents commands language assumed learners fully correct kbkr advice assumed constant rened slack variables conclusions future work presented algorithm called preference kbkr pref-kbkr human user present advice natural form reinforcement learner advice pref-kbkr takes form if-then rules situations advice applies preference action users present advice terms policies choice action perform current state giving advice terms values internal detail learning algorithm employ algorithm based knowledge-based kernel regression kbkr represent preference advice additional constraints support-vector regression task solved linear-program solver tested approach testbed recently developed based robocup soccer simulator addresses subtask soccer call breakaway empirically compare approaches support-vector solver receives advice kbkr solver receives advice desired values pref-kbkr technique results show giving advice advantageous advice preferred actions effective advice values future work plan investigate pref-kbkr wider set problems additional pieces advice intend test systems wider variety parameter settings important parameters effectiveness methods planned experiments kernels tiling features address scaling larger numbers advice larger numbers training examples advice-taking methods critical scaling reinforcement-learning methods large problems approaches making support-vector techniques crucial effort acknowledgements research partially supported darpa grant naval research laboratory grant code breakaway obtained ftp ftp wisc machinelearning shavlik-group robocup breakaway andre russell programmable reinforcement learning agents nips clouse utgoff teaching method reinforcement learning icml dietterich wang support vectors 
reinforcement learning ecml gordon subramanian multistrategy learning scheme agent knowledge acquisition informatica kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer aaai wshp supervisory control learning adaptive systems lagoudakis parr reinforcement learning classi cation leveraging modern classi ers icml laud dejong reinforcement learning shaping encouraging intended behaviors icml lin self-improving reactive agents based reinforcement learning planning teaching machine learning maclin shavlik creating advice-taking reinforcement learners machine learning maclin shavlik torrey walker knowledge-based support-vector regression reinforcement learning ijcai wshp reasoning representation learning computer games mangasarian shavlik wild knowledge-based kernel approximation journal machine learning research mangasarian nonlinear programming philadelphia siam noda matsubara hiraki frank soccer server tool research multiagent systems applied arti cial intelligence stone sutton scaling reinforcement learning robocup soccer icml sutton barto reinforcement learning introduction cambridge mit press 
networks combined testset error digit recognition networks combined testset error protein folding standard competitive 
appears proceedings international conference intelligent systems molecular biology ismba probabilistic learning approach whole-genome operon prediction mark cravenyz craven biostat wisc david pageyz page biostat wisc jude shavlikzy shavlik wisc joseph bockhorstzy joebock wisc jeremy glasner jeremy genome wisc ydept biostatistics medical informatics wisconsin medical sciences center avenue madison wisconsin zdept computer sciences wisconsin west dayton street madison wisconsin dept genetics henry mall wisconsin madison wisconsin abstract present computational approach predicting operons genomes prokaryotic organisms approach machine learning methods induce predictive models task rich variety data types including sequence data gene expression data functional annotations genes multiple learned models individually predict promoters terminators operons key part approach dynamic programming method predictions map putative gene genome probable operon evaluate approach data coli genome introduction availability complete genomic sequences microarray expression data calls computational methods uncovering regulatory apparatus cells begun research project wisconsin developing machine learning approaches predicting regulatory elements transcription-control signals operons regulatory relationships genes rich variety data sources including genomic sequence data expression data paper present approach predicting previously unknown operons prokaryotic organisms approach rst involves learning model estimate probability arbitrary sequence genes constitutes operon learned model component approach dynamic programming method assign copyright american association arti cial intelligence aaai rights reserved gene genome probable operon components predict operon map entire genome evaluate approach data coli genome research groups overbeek tamames recently addressed task predicting functionally coupled genes identifying clusters genes conserved erent genomes task erent interested sequences genes transcribed functionally related methods complementary based cross-genome information approach based information present single genome functional annotation genes inter-gene spacing predict operons blattner approach ers work features addition machine learning method determine weight feature predictions iii dynamic programming algorithm produce consistent operon map research groups dynamic programming consistent interpretation predictions made learned models snyder stormo mural uberbacher previous orts addressed task assembling predicted exons gene models contrast work formulates dynamic programming approach assigning genes operons notable aspect approach exploits rich variety data types provide evidence task predicting operons data types learned models include microarray expression data dna sequence features spatial regulatory networks model predicting models discovering regulatory signals model predicting operons model predicting terminators model predicting promoters figure multi-level learning approach discovering gene regulatory mechanisms tasks highest level represent motivating research task middle level represents main focus paper lowest level represent learning tasks address order make predictions middle level task arrows represent predictions features relative positions genes functional annotations genes experiments data types provide predictive task recent interest uncovering regulatory interactions genes microarray expression data eisen brown friedman approaches unlike method incorporated types data evidence compelling aspect approach involves multiple machine learning tasks operating erent levels detail stated previously main task predicting sequences genes constitute operons task represent goal work intermediate step attack problems inferring networks regulatory interactions discovering subclasses sequences involved controlling gene transcription hypothesize operon prediction task improved rst identifying control sequences promoters terminators genome recognition sequences understood task address learning subtasks constructing predictive models sequences figure illustrates relationships learning tasks motivating work subtasks address predictions made model learned level passed learning components higher level input features idea decomposing learning task simpler subtasks time buchanan shapiro applied recently robotics simulated robotic systems sullivan stone organization rest paper section describe detail task recognizing operons discuss data subsequent sections review elaborate recent work craven learning models score candidate operons rst sections describes machine learning approach describes problem representation present main contribution paper dynamic programming approach making whole-genome operon predictions learned operon models penultimate section presents detailed empirical evaluation approach nal section discussion conclusions problem domain primary task predict operons coli genome approach developing applicable prokaryotic organisms genome coli sequenced wisconsin blattner consists single circular chromosome double-stranded dna chromosome strain coli data set base pairs coli approximately genes located strands nition operon paper sequence genes conditions transcribed unit aspects nition important note genes transcribed individually included nition refer special cases singleton operons nition treats multiple operons cases rpsu-dnag-rpod coli multiple promoters terminators result erent subsequences larger gene sequence transcribed erent conditions distinct gene sequences transcribed unit operon figure illustrates concept operon transcription process initiated rna polymerase binds promoter rst gene operon rna polymerase moves dna template produce rna molecule rna polymerase past gene operon encounters special sequence called terminator signals release dna cease transcription data learning model operons regulondb salgado include complete dna sequence genome beginning ending positions genes putative genes positions sequences promoters terminators rho-dependent rho-independent functional annotation codes characterizing genes three-level -leaf hierarchy riley promoter terminator figure concept operon curved line represents part coli chromosome rectangular boxes represent genes operon sequence genes transcribed unit transcription controlled upstream sequence called promoter downstream sequence called terminator promoter enables molecule performing transcription bind dna terminator signals molecule detach dna gene transcribed direction determined strands located arrows gure direction transcription gene gene expression data characterizing activity levels genes putative genes experiments operons estimated blattner personal communication hundred undiscovered operons coli goal predict operons model learned data interesting aspect learning task set non-operons negative examples nature scienti inquiry hundred operons identi coli attention focused identifying sequences genes constitute operons assemble set putative non-operons exploiting fact operons rarely overlap rule generate set negative examples enumerating sequence consecutive genes strand overlaps coincide operon generated nonoperons true operons operons overlap cases probability small learning algorithms robust presence noisy data machine learning approach recent work craven applied machine learning algorithms task distinguishing operons non-operons section review application algorithm naive bayes task candidate operon consecutive sequence genes strand estimate probability candidate operon data bayes rule tells determine ojd djo random variable indicating candidate actual operon represents data make determination heart task estimate likelihood data interest outcomes naive bayes make assumption features independent class make approximation djo dijo ith feature features provide numeric characterizations candidate operons represent conditional distribution feature class histogram approach rst step procedure choose cutpoints bins procedure selects bin boundaries bin training examples pooling positive negative examples bin examples class assume examples class fell bin smoothing method avoids zero-valued probabilities bayes rule produce estimated probability kernel density estimates represent conditional distributions found predictive accuracy method histogram approach problem representation section 
describe features learning method assess probability candidate operon length spacing features features relate size intergenic spacing operons non-operons operon size number genes candidate operon within-operon spacing maximum spacing dna base pairs genes contained candidate operon distances figure genes operon transcribed constraints inter-gene spacing feature ned singleton operons operons consisting gene distance neighboring genes distances preceding figure figure genes notice genes part candidate operon directionality neighboring genes boolean-valued features indicating directionality preceding genes individually match directionality genes candidate operon recall genes operon transcribed direction refer features collectively neighboring genes features functional annotation features genes operon typically act perform common function expect functions individual genes related functions genes coli three-level hierarchy riley levels hierarchy represent broad intermediate detailed functions path root leaf hierarchy root metabolism small molecules carbon energy metabolism anaerobic respiration measure functional distance genes terms deeply hierarchy match genes completely identical functions distance genes identical broad intermediate functions distance genes identical broad functions distance genes common function distance cases function unknown genes level split erence genes share broad function intermediate function unknown distance distance measure annotations pairs genes features measure functional homogeneity candidate operon collectively refer functional annotation features rst feature represents pairwise functional distance genes candidate operon feature computed simply computing functional distance pair genes candidate operon computing pairwise distances functional distance gene preceding candidate genes candidate speci cally compute pairwise distances preceding gene genes operon feature analogous rst gene candidate operon transcription signal features type evidence operons transcription control signals promoters terminators decide sequence genes represents operon upstream rst gene sequence promoter downstream gene terminator task recognizing promoters terminators easily accomplished examples types sequences cient conditions promoters terminators evidence operons rst method predictively identify approach examples types signals learn statistical models predicting speci cally induce interpolated markov models imms jelinek mercer characterize promoters terminators imms previously modeling biological sequences salzberg task erent interpolated markov model consists set markov models erent orders imm makes prediction case interpolating statistics represented models erent order learn separate imms trained recognize type signal promoter rho-dependent terminator rho-independent terminator training set promoters consists sequences bases long promoter promoter sequences aligned common point rst base transcribed obtain statistics likelihood base position promoter terminator data set similar consists sequences length imms represent probability dna bases position signal assess probability sequence promoter terminator calculate product imm estimated probability base sequence sjmodel imm base ith position sequence length sequence evaluating assess probability base position imm interpolates th-order markov model st-order markov model imm pri pri notation pri represents probability base ith position st-order model pri represents th-order model th-order model simply represents marginal probabilities base position st-order model represents conditional probabilities base previous base sequence pri pri pri pri sijsi simple scheme set values parameters represent amount weight give model interpolated positions set parameter set training data included occurrences base position set intuition trust st-order probability cient data estimate experiments set induced promoter terminator imms instances signals neighborhood candidate operon scanning promoter model bases immediately preceding rst gene candidate operon similarly scanning terminator model bases immediately gene candidate scanning model move base time scanning process sequence predicted probabilities position model case terminators separate imms model rho-dependent terminators model rho-independent terminators probability position combine estimates models assuming sequence represent rho-dependent rho-independent terminator possibilities independent order convert sequences predicted probabilities features classify candidate operon track greatest predicted probability scan words characterize candidate operon features promoter feature strongest predicted promoter upstream candidate terminator feature strongest predicted terminator downstream gene expression features recent microarray technology nature genetics supplement enables simultaneous measurement messenger rna mrna levels thousands genes experimental conditions wisconsin coli genome project begun generating data work presented paper data rst experiments expression data cdna arrays measurements uorescence intensities gene experiment relative amount mrna experimental condition versus relative amount baseline condition refer sets measurements array channels array data compute sets features learning algorithm rst set features based ratios measurements set based raw measurements features based ratios associate gene vector length number experiments elements vector measured ratios gene cases element vector unde ned due measurement problems experiments genes operon coordinately controlled expect expression vectors genes correlated expression vectors randomly chosen genes rst set features classifying candidate operons based pairwise correlations expression vectors genes interest correlation metric similar previously analyzing gene expression data eisen correlation vector kth gene vector lth gene corr ned nedj represent jth vector values ratios jth experiment ned ned nedj computing correlation pair genes experiments genes ned ratios vector missing element speci position features based correlations characterize candidate operon correlation pair genes correlation rst gene previous gene sequence iii correlation gene gene sequence ratio correlation measure identify genes similar expression pro les expect stronger association genes operon genes transcribed absolute amount transcript produced similar derive features raw measurements uorescence intensities features treat channel separate experiment conditions baseline channels experiments merge data experiments normalizing total signal intensity array basic idea features compute raw intensity values candidate operon single experiment intensity values recorded genes candidate genes operon approach based model intensity result true underlying signal amount transcript present distributed random noise acj vector expression values jth experiment genes candidate operon operon likelihood observed measurements acj acjjo true acj random variable indicating candidate operon acj represents intensity ith gene candidate jth experiment acj represents density acj normal distribution parameters model represents true signal operon experiment represents standard deviation noise operon experiment approach estimates intensity values jth experiment genes candidate parameter estimated training set operons linear function space data point space determined operon training set singletons model accounts sources noise depends signal strength chen dougherty bittner marginal probability observed intensity values acj exp acj assume distribution intensity values genes experiment exponential parameter determined maximum likelihood estimate data experiment assess candidate operon compute likelihood ratio experiments treat independent acjjo true acj approach calculate features candidate operon likelihood ratio genes likelihood ratio rst gene previous gene sequence iii likelihood ratio gene gene sequence collectively refer features section ratio-based features absolute features expression data features constructing whole-genome operon map point discussion focused task classifying isolated operons primarily interested making operon predictions isolated candidates entire genomes section address 
task predicting operon maps operon map set predictions assigns gene genome operons recall nition operon sequence genes transcribed unit conditions discussion focus special case operon map assigns gene genome operon refer special case one-operon-per-gene map address map map map map figure alternative operon maps run genes top gure shows run genes run shown one-operon-pergene operon maps run map assigns gene hypothesized operon operons bars connecting genes map assigns genes operon map assigns gene operon special case reasons pretty good approximation true operon map organism words conditions genes part operon one-operon-per-gene map accurately predicted easier interpret general operon map developed cient algorithm nding optimal oneoperon-per-gene map assumptions figure illustrates concept one-operon-per-gene map remainder section describes algorithm determining optimal one-operon-per-gene map address issue evaluate map describe dynamic programming algorithm nding optimal map evaluating map discussed evaluate candidate operon turn attention matter evaluating candidate operon map competing operon maps score decide appears probable run genes consist sequence consecutive genes uninterrupted rna gene gene opposite strand iii operon assume operons include rna genes bridge genes opposite strand process run genes separately building operon map assumptions rst gene run rst gene operon gene run gene operon candidate operon map run genes conducting computational experiments require operon training set treated candidate operon evaluate determining probable map run mapjr rjmap map assuming uniform prior maps consideration simpli mapjr rjmap evaluate map probable run genes map assuming genes independent rjmap map gijoj represents ith gene run represents jth candidate operon map represents probability operon understand motivation formulation clustering problem centers xed number clusters mixture gaussians evaluate clustering probable data cluster centers typically assuming data points independent probability point clustering probability point clustering typically product probability cluster emitted point prior probability cluster task analogous partition run genes operons gene data point hypothesized operon map analogous cluster determine probability gene map gene explained operon simplify expression deterministic relationship candidate operons genes gene included candidate operon term gijoj operon map rest rjmap operon encompasses map taking account gene run expression simply length operon map making simpli cation taking logarithm rjmap score operon map score map log rjmap map jojj logpr jojj length terms genes operon probability candidate operon determined learned model recognizing operons earlier dynamic programming approach discussed score candidate operon map turn attention task nding optimal map run genes notion optimality one-operon-per-gene map scoring scheme run genes gli suppose score candidate operon run approach store scores two-dimensional array element array holds score operon length ends position run dynamic programming approach array determine optimal map element represents score map position partial map represented assign gene gii operon operons extend nal element array represents map assigns gene run operon extends run ned optimal one-operon-pergene map initialize array setting recurrence relation determine max order recover optimal map simply track obtained maximum element words track length operon map represented easy algorithm implemented pair nested loops outer loop ranges loop ranges alternatives determining time complexity approach length run processed theorem run genes algorithm returns optimal maximum scoring one-operon-pergene map run score optimal map proof prove stronger result algorithm nds optimal operon map ending operon length operon consisting genes run score map operon map end operon length algorithm returns map maximum set result implies theorem arbitrary run genes map denote operon map computed algorithm ends operon length genes jmap denote number operons map show induction jmap map optimal operon maps end operon length score map base case jmap consist genes exists operon map ending operon length map necessarily optimal score map simply desired inductive case jmap map consisting operon map inductive hypothesis map optimal map run consists rst genes map end operon length denote map map inductive hypothesis score map optimal run map map operon consisting nal genes score score map desired remains verify map optimal maps ending operon length suppose sake contradiction operon map ends operon consisting genes scores higher map score higher operon map begin higher scoring map score contradicting map optimal empirical evaluation section evaluate predictive accuracy approach run -fold cross-validation experiments data set consisting operons sequences genes thought operons questions answer experiments accuracy approach accuracy constructed operon non-operons test set reason subtle randomly distribute operons test sets operon assign overlapping negative examples test set examples overlap operon training set process ensures test-set overlaps training set nonoperons overlap multiple operons process leads non-operons appearing test set map compare accuracy obtained classifying isolated candidate operons predictive individual features address rst questions approaches classifying candidate operons rst learned naive bayes models classify candidate operons classi cations made independent approach learned models assign probability candidate operon dynamic programming approach construct genome-wide operon map based predicted probabilities step enforces global consistency predictions assigning gene operon ectively classi cations candidate operons baseline expected accuracy results randomly selecting operon maps uniform distribution maps determine expected computing candidate operon test set probability candidate predicted operon randomly chosen map map jcj jcj represents length operon candidate beginning run genes candidate end run genes probability tells frequently positive correctly classi negative incorrectly classi randomly selected map training test set split learn promoter terminator imms leaving model training sets promoters terminators test-set operons ensure operon predictions biased information closely linked test case promoter terminator case undiscovered operon table shows accuracy rates experiment false positive true positive rates false positive rate ned fpfp true positive rate ned tptp rst row shows results learned operon models treated classi ers row shows results dynamic programming method construct operon map map classify operon candidates net effect altering predictions constructing map correctly identify true operons hand predict false positives accuracy predictions made map slightly predictions made classi ers encouraging result table predictive accuracy learned models naive bayes operon map made predictions learned models randomly chosen operon maps method acc rate rate naive bayes operon map random map operon map added bene ensuring operon predictions consistent one-operon-per-gene assumption note task cult number nonoperon gene sequences overlap true operon generally quadratic length true operon opportunities inaccurate prediction reduce accuracy predictions true operons finally note accuracy criterion stringent classi cation considered wrong partial credit cases predict extent operon correctly perform experiments 
evaluate contribution individual features predictive accuracy models rst experiment making predictions single feature small group closely related features collectively refer cases feature groups experiment learn models leave single feature group table shows accuracy operon maps made models consist single feature groups table shows accuracy operon maps made models leave feature group tables include result table naive bayes models features tables illustrate interesting points features vary bit predictive accuracy individual feature groups predictive model result combining evidence variety sources table shows cases leaving single feature group model adversely ect predictive accuracy feature group impact left functional annotation group result disappointing suspect including feature biases results extent explanation concern functional annotation codes assigned genes manually people knowledge operons possibly information true operons leak test sets conjecture operons consistent complete functional annotation true unknown operons estimate extent bias correct table predictive accuracy operon maps made individual feature groups representation rate rate features functional annotation within-operon spacing expression data promoter terminator operon size neighboring genes table predictive accuracy operon maps made leaving individual feature groups representation rate rate leaving terminator promoter neighboring genes expression data operon size within-operon spacing functional annotation issue features informative labeled examples general crops case promoter terminator features case ensure experiments biased leaving promoter terminator training sets promoters terminators test-set operons sense predictive models roc receiver operating characteristic curves roc curve shows relationship true positive false positive rates vary threshold con dence predictions results rst row table determined treating naive bayes model classi posterior probability candidate operon greater classi operon roc curve shown upper left part figure hand shows accuracy model vary threshold posterior probability single roc curve learned models pooling test-set predictions models curve informative unlike accuracy numbers table depend prior probabilities classes misclassi cation costs property roc curves figure illustrates learned models considerable predictive model guessed randomly result roc line ned rate rate true positive rate false positive rate model features map features true positive rate false positive rate model features model functional annotation map functional annotation model functional annotation map functional annotation true positive rate false positive rate model features model neighboring genes map neighboring genes model neighboring genes map neighboring genes true positive rate false positive rate model features model expression data map expression data model expression data map expression data true positive rate false positive rate model features model promoter map promoter map model promoter map promoter true positive rate false positive rate model features model operon size map operon size model operon size map operon size true positive rate false positive rate model features model within-operon spacing map within-operon spacing model within-operon spacing map within-operon spacing true positive rate false positive rate model features model terminator map terminator model terminator map terminator figure roc plots roc curve upper left plot represents naive bayes model point gure represents predictive accuracy operon map produced dynamic programming algorithm plots gure illustrate predictive power individual feature groups plots shows roc curve predictions made leaving feature group roc curve predictions made feature group iii points operon maps constructed models additionally plot repeats roc curve upper left plot model features note curve represents accuracy naive bayes model classifying candidate operons independently obvious generate roc curves operon maps compare predictive accuracy operon maps roc curve naive bayes models plotting operon map single point space plot upper left part figure shows point lies roc curve naive bayes model result operon map results accuracy superior accuracy naive bayes predictions roc curves provide indication predictive individual features plots figure provide roc analysis individual feature groups plots shows roc curve predictions made leaving feature group roc curve predictions made feature group iii points operon maps constructed models additionally plot repeats roc curve upper left plot model features figure reinforces points made earlier individual feature group predictive models features cases leaving individual feature group weaken predictive accuracy process constructing operon map enforces consistency predictions boost predictive accuracy conclusions presented approach predicting operons prokaryotic genomes involves major components rst component involves learning naive bayes models rich variety data types sequence data expression data estimate probability sequence genes constitutes operon component dynamic programming method constructs operon map entire genome part assigning gene probable operon operon map consistent interpretation predictions made model empirical evaluation approach operons predicted fairly high accuracy combining evidence data sources operon maps produced dynamic programming method accurate individual predictions generated naive bayes models lesson experiments pertains relative gene expression data great interest identifying sets related genes discovering regulatory relationships expression data results suggest cautionary note obtain accurate operon predictions types data conjunction expression data main issues plan investigate future research additional sources evidence binding sites regulatory proteins plan incorporate models investigating alternative representations feature types including expression data terminators current terminator models provide predictive representation unable capture important information rna base pairing developing approach predicting terminators stochastic context free grammars represent base-pairing information gene expression data incorporating additional data predictions plan investigate methods handling data account relationships experiments generating data experiments closely related fourth plan begin addressing top-level learning tasks shown figure motivated work operon prediction task fundamental challenge facing computational biology community determining functions genes newly determined genomes relationships genes argue task addressed employing wide array data sources evidence work presented represents promising rst step general approach acknowledgments research supported nsf grant irinih grant wisconsin graduate school fred blattner christina kendziorski nicole perna craig richmond helpful discussions research blattner plunkett bloch perna burland riley collado-vides glasner rode mayhew gregor davis kirkpatrick goeden rose mau shao complete genome sequence escherichia coli science brown grundy lin cristianini sugnet ares haussler support vector machine classi cation microarray gene expression data dna sequence agtat translated protein sequence tyr gly pro technical report ucsc-crl- department computer science california santa cruz chen dougherty bittner ratio-based decisions quantitative analysis cdna microarray images journal biomedical optics craven page shavlik bockhorst glasner multiple levels learning diverse evidence sources uncover coordinately controlled genes proceedings seventeenth international conference machine learning stanford morgan kaufmann eisen spellman brown botstein cluster analysis display genomewide expression patterns proceedings national academy sciences usa nature genetics supplement chipping forecast volume january friedman linial nachman bayesian networks analyze expression data proceedings 
fourth annual international conference computational molecular biology recomb buchanan learning intermediate concepts constructing hierarchical knowledge base proceedings ninth international joint conference arti cial intelligence jelinek mercer interpolated estimation markov source parameters sparse data gelsema kanal eds pattern recognition practice north holland sullivan transferring learned knowledge lifelong learning mobile robot agent proceedings european workshop learning robots overbeek fonstein souza pusch maltsev gene clusters infer functional coupling proceedings national academy science usa riley coli gene products physiological functions common ancestries curtiss lin ingraham low magasanik neidhardt rezniko riley schaechter umbarger eds escherichia coli salmonella american society microbiology edition salgado santos-zavaleta gama-castro mill an-z arate blattner collado-vides regulondb version transcriptional regulation operon organization escherichia coli nucleic acids research salzberg delcher kasif white microbial gene identi cation interpolated markov models nucleic acids research shapiro structured induction expert systems reading addison wesley snyder stormo identi cation coding regions genomic dna sequences application dynamic programming neural networks nucleic acids research stone layered learning multi-agent systems dissertation school computer science carnegie mellon technical report cmucs- tamames casari ouzounis valencia conserved clusters functionally related genes bacterial genomes journal molecular evolution mural uberbacher constructing gene models accurately-predicted exons application dynamic programming computer applications biosciences 
test-set accuracy examples rejected neural networks nearest-neighbor classifiers cgg coding coding aaa ttt tttaaaaac aac generalization error training examples nucleotides codons decrease false negatives decrease false positives node node node node node node newnode existing node node generalization error training examples nucleotides codons 
generalization error training examples perceptron hus hus hus hus threshold abc abc output input output input output input output input networks resulting networks original crossed generalization error citre iterations data set data set data set data set advice behavior reinforcement action state learner environment observer 
sensor inputs actions hidden units advice current hidden units movenorthmoveeast state enemy west obstacle north moveeast movenorth state inputs outputs promoters splice junctions ribosome binding sites kbann topgen regent test set error networks considered splice junctions promotersr key topgen standard regent kbann test set error surrounded moveeastpusheast oktopusheast enemy inputs pusheast moveeast moveeast definition moveeast definition key agent enemy obstacle empty reward hiddenunits wall enemy obstacle empty sector wall enemy obstacle empty sector ring wall enemy obstacle empty sector ring actions sensor inputs reward reward reward action moveeast pusheast movenorth decrease false negatives decrease false positives node node node node node node newnode existing node node average testset reinforcement number training episodes average testset reinforcement advice advice episodes advice episodes advice episodes empty rule-set fidelity operations learning method search method knowledge based portion outputs inputs extra hidden units percent missing rules test set error kbann strawman topgen standard neural network domain theory percent missing rules topgen kbann strawman test set error 

number rule-set fidelity antecedents conjunctive antecedents m-of-n rules conjunctive rules m-of-n test set error rate splice junctions promoters terminators key kbann topgen strawman rule-set fidelity operations learning conjunctive rules learning m-of-n rules search conjunctive rules 

bias rule extraction bias rule extraction 
state current output hidden unit topology determined domain theory current state current input system boundary coil coil coilcoil helix strand coil input window output units hidden units input units primary structure predicted secondary structure 
training examples neural learning initial neural network trained neural network rules-tonetwork translation initial domain theory probability addition noise domain theory test set error rate drop antecedent fully-connected standard ann add antecedent 
training examples daid neural learning initial neural network trained neural network rules-tonetwork translation initial domain theory neural learning trained neural network refined symbolic knowledge symbolic knowledge initial initial neural network network rules rules network square testing error training instances step initial unit bias step bias bias steps numbertrue returns number true antecedentsnumbertrue steps dna precursor mrna mrna splicing protein folded protein iii 
exon intron exon kbann daid kbann std ann percent test set errors promoter domain kbann daid kbann std ann percent test set errors splice-junction domain network mofn subset test set error rate promoter domain training set testing set network mofn subset test set error rate splice-junction domain kbann daid kbann std ann relative training effort promoter domain kbann daid kbann std ann relative training effort splice-junction domain material non-insulating insulating ceramic paper styrofoam open-cell foam percent errors testing set number training examples kbann network randomly-weighted network 
promoter contact conformation minus minus dna sequence 

dna sequence translated protein reading frame translated protein reading frame tyr gly pro met glu leu 
cgg codon pos codon pos aaa aac aag tttttgttc word commonality frame words base content frame bias matrix coding words base position fractal dimension codon pos codon pos number run-length error run-length uberbacher-mural ann codons ann staden algorithm uberbacher-mural-staden ann submitted data mining generation challenges future directions kargupta joshi eds aaai mit press relational data mining inductive logic programming link discovery raymond mooneya prem melvillea lappoon rupert tanga jude shavlika castro dutraa david pagea tor santos costaa department computer sciences texas austin mooney melville rupert utexas department biostatistics medical informatics department computer sciences wisconsin madison shavlik dpage wisc dutra vitor biostat wisc abstract link discovery important task data mining counter-terrorism focus darpa evidence extraction link discovery eeld research program link discovery concerns identi cation complex relational patterns potentially threatening activities large amounts relational data data-mining methods assume data form feature-vector single relational table handle multi-relational data inductive logic programming form relational data mining discovers rules rst-order logic multi-relational data paper discusses application ilp learning patterns link discovery keywords ilp aleph mfoil counter-terrorism ensembles eeld introduction events september development information technology aid intelligence agencies efforts detect prevent terrorism important focus attention evidence extraction link discovery eeld program defense advanced research projects agency darpa research project attempts address issue establishment eeld program developing advanced software aiding detection terrorist activity pre-dates events program genesis preliminary darpa planning meeting held carnegie mellon opening center automated learning discovery june meeting discussed formation darpa research program focused knowledgediscovery data-mining kdd methods counter-terrorism scope program subsequently expanded focus related sub-tasks detecting potential terrorist activity numerous large information sources multiple formats evidence extraction task obtaining structured evidence data unstructured natural-language documents builds information extraction technology developed darpa earlier muc message understanding conference programs lehnert sundheim cowie lehnert current ace automated content extraction program national institute standards technology nist nist link discovery task identifying complex multi-relational patterns potentially threatening activities large amounts relational data input data input data existing relational databases finally pattern learning concerns automated discovery relational patterns potentially threatening activities patterns learned improve accuracy current eeld program focused sub-topics started summer incorporated information awareness iao darpa data patterns eeld include representations people organizations objects actions types relations data represented large graph entities connected variety relations areas link analysis social network analysis sociology criminology intelligence jensen goldberg wasserman faust sparrow study networks graph-theoretic representations data mining pattern learning counter terrorism requires handling multi-relational graph-theoretic data current data-mining methods assume data single relational table consists tuples items market-basket analysis type data easily handled machine learning techniques assume propositional feature vector attribute representation examples witten frank relational data mining rdm zeroski lavra hand concerns mining data multiple relational tables richly connected style data needed link discovery pattern learning link discovery requires relational data mining widely studied methods inducing relational patterns inductive logic programming ilp muggleton lavrac dzeroski ilp concerns induction horn-clause rules rstorder logic logic programs data rst-order logic paper discusses on-going work applying ilp link discovery part eeld project inductive logic programming ilp ilp study learning methods data rules represented rstorder predicate logic predicate logic quanti variables relations represent concepts expressible examples feature vectors relational database easily translated rst-order logic source data ilp wrobel rules written prolog syntax conclusion appears rst uncle relation uncle brother parent uncle husband sister parent goal inductive logic programming ilp infer rules sort database background facts logical nitions relations muggleton lavrac dzeroski ilp system learn rules uncle target predicate set positive negative examples uncle relationships set facts relations parent brother sister husband background predicates members extended family uncle tom frank uncle bob john uncle tom cindy uncle bob tom parent bob frank parent cindy frank parent alice john parent tom john brother tom cindy sister cindy tom husband tom alice husband bob cindy alternatively rules logically brother sister relations supplied relationships inferred complete set facts basic predicates parent spouse gender if-then rules rst-order logic formally referred horn clauses formal nition ilp problem background knowledge set horn clauses positive examples set horn clauses typically ground literals negative examples set horn clauses typically ground literals find hypothesis set horn clauses completeness consistency variety algorithms ilp problem developed zeroski lavra applied variety important data-mining problems zeroski relational data mining remains under-appreciated topic larger kdd community recent textbooks data mining han kamber witten frank hand mannila smyth mention topic important topic generation data mining systems critical link discovery applications counter-terrorism initial work ilp link discovery tested ilp algorithms eeld datasets current eeld datasets pertain domains nuclear smuggling contract killing contract-killing domain divided natural real world data manually collected extracted news sources synthetic arti cial data generated simulators section presents experimental results natural smuggling contractkilling data section presents initial results synthetic contractkilling data experiments natural data nuclear-smuggling data nuclear-smuggling dataset consists reports russian nuclear materials smuggling mckay woessner roule chronology nuclear radioactive smuggling incidents basis analysis patterns smuggling russian nuclear materials information chronology based open-source reporting primarily world news connection wnc lexis-nexis articles obtained sources translated italian german russian research chronology grew began chronology rst appeared appendix paper williams woessner williams woessner williams woessner continually evolving chronology published separate papers journal part recent events section woessner woessner part evidence extraction link discovery eeld project coverage chronology extended march chronology grew incidents incident descriptions chronology entry descriptions incident incidents chronology extensively cross-referenced data presented chronology incidents relational database format format objects rows tables attributes differing types columns tables values matter input source information user objects types denoted pre xes consist entity objects consist location material organization person source weapon event objects consist generic event link objects expressing links entities events consisting represented table table links entities events nuclear-smuggling data event person organization location weapon material event person organization location weapon material actual database experiments relational tables number tuples relational table vary elements ilp system learn events incident related order construct larger knowledge structures recognized threats ilp system positive training examples links events assume events unrelated compose set negative examples stipulate related commutative speci ilp system experiments related true related proven vice-versa set examples consists positive examples distinct negative examples randomly drawn full set negative examples linking problem nuclear-smuggling data challenging heavily relational learning problem large number relations traditional ilp applications require small number relations natural contract-killing data dataset contract killings rst compiled hayon cook cook hayon response research russian organized 
crime encountered frequent tantalizing contract killings contract-killing reports provided photograph criminal scene russia comparable assessment linked trends victims relationship victims relationship victims perpetrators dataset contract killings continually expanded cook hayon funding darpa eeld program veridian systems division vsd williams database captured chronology incidents incident chronology received description information drawn sources typically news article occasionally nuclear-smuggling dataset information chronology based opensource reporting foreign broadcast information service fbis joint research service jprs journals subsequently fbis on-line cut-down on-line version world news connection wnc services lexis-nexis main information sources additional materials worldwide web consulted feasible helpful search exhaustive limited time resources involved data organized relational tables format nuclear-smuggling data previous section dataset experiments relational tables number tuples relational table varies element ilp learner task characterize rival versus obstacle threat events obstacle threat examples pooled category producing two-category learning task rival obstacle threat treated motives dataset motivation learning task recognize patterns activity underlying motives turn contributes recognizing threats number positive examples dataset number negative examples ilp results natural data aleph ilp system aleph srinivasan learn rules natural datasets default aleph simple greedy set covering procedure constructs complete consistent hypothesis clause time search single clause aleph selects rst uncovered positive seed saturates performs admissible search space clauses subsume saturation subject user-speci clause length bound details aleph experiments dutra page santos costa ensembles ensembles aim improving accuracy combining predictions multiple classi ers order obtain single classi investigate employing ensemble classi ers classi logical theory generated aleph methods presented ensemble generation dietterich paper concentrate popular method frequently create accurate ensemble individual components bagging breiman bagging works training classi random sample training set bagging important advantage effective unstable learning algorithms breiman small variations parameters huge variations learned theories case ilp advantage implemented parallel trivially details bagging approach ilp experimental methodology found dutra page santos costa experimental results based vefold cross-validation times train examples test learned remaining addition test set task identifying linked events nuclear-smuggling dataset aleph produces average testset accuracy improvement baseline case majority class guessing events linked produces average accuracy bagging sets rules increases accuracy rule good accuracy found system shown figure rule covers positive examples negative examples rule smuggling events related event involves person involved event event involves material appears linked event person eventa personc relationb relationb descriptiond event person eventf personc relationb relationb descriptiond material location materialg evente event material eventf materialg figure nuclear-smuggling data sample learned rule event words person event involved event material event person played role description events symbols arguments relevant rule figure illustrates connections events material people involved solid lines direct connections shown literals body clause dotted line corresponds concept learned describes connection events person material event event eevent inferred figure pictorial representation learned rule task identifying motive contract-killing data set difcult aleph accuracy compared baseline accuracy utilization ensembles improves accuracy time rule figure shows kind logical clause ilp system found dataset rule covers positive examples single negative rule event killing rival follow chain events connects event event event event event event relates organizations events kind relation relationc events chain subsets incident experiments synthetic data synthetic data contract killing provenient simulators bayesian network simulator extraction transport task-based simulator team lead powers bayesian network simulator generates data based probabilistic model developed information extraction transport incorporated iet simulator outputs case les complete unadulterated descriptions murder case rivalkilling eventa event event eventb eventa relationc eventdescriptiond event event eventb evente relationc eventdescriptiond event event evente eventf eventdescriptiond eventf figure natural contract-killing data sample learned rule case les ltered observability facts accessible investigator eliminated make task realistic data corrupted misidentifying role players incorrectly reporting group memberships ltered corrupted data form evidence les representation evidence les facts event represented binary predicates isa murder murder hire perpetrator murder killer victim murder murder victim devicetypeused murder pistol czech task-based simulator team lead powers exible mechanism creating synthetic datasets eeld program includes pattern speci cation language knowledge base case generation representation evidence generation corruption answer key representations bayesian simulator relied bayesian network core task-based simulator tasks task methods method probability selected preconditions satis simulator powerful functionality ltering corrupting data important represent situations actual data expected low observability shown represent simulation output binary predicates report situation uid starting date uid information source type uid police organization meeting taking place uid date event uid social participants uid uid social participants uid uid ite illocutionary force uid inform notice simulation results include meta-data speci event reported advantage data current experiments ilp results 
synthetic bn-based data synthetic bn-based contract killing dataset consists murder events murder event labeled murder hire rst-degree second-degree murder murder hire events rst-degree second-degree murder events task learn classi correctly classify unlabeled event categories task variation mfoil lavrac dzeroski learn binary classi discriminate events murder hire events aleph mfoil learns clause time greedy covering constrained general-to-speci search learn individual rules mfoil learn classi ers identify rst-degree second-degree murders binary classi ers combined form three-way classi task event classi positive classi event labeled category classi classi classi event positive select category commonly represented training data ran -fold cross-validation dataset murder events measured precision recall classi categories precision recall category ned results summarized table observe recall seconddegree murders precision recall results system learns precise classi second-degree murders consequence lower recall adjust parameters system compromise precision higher recall computed accuracy classi ned percentage events correctly classi categories compare majority-class classi classi events frequently represented category experiments accuracy majority-class classi classi cation accuracy system majority-class classi figure shows sample rules system learns rst rule murder event involves member criminal organization crime motivated economic gains murder hire rule murder result event performed love rst-degree murder premeditated murders rule murder result theft motivated rivalry performed public property second-degree murder sample rules show system classifying events produces rules meaningful interpretable humans ilp results synthetic data synthetic contract killing dataset consists runs task-based simulator parameters set low observability large simutable results synthetic contract-killing data murder hire degree degree precision recall murder hire group member maleficiary events crime motive economic degree murder events performed loves degree murder events event occurs location type publicproperty crime motive rival occurrent subevent type stealing 
generic figure synthetic contract-killing data sample learned rules lations interested learning detect instances murder hire instance victim perpretator contractor positive examples obtained simulator output hard interesting cases non-murder hires negative examples initial experiments performed positive-only learning simulator output includes large number relations relations simulation generates background knowledge consisting facts number positive examples varies ranging simulation experiments performed aleph -fold cross-validation aleph implementation muggleton positive data learning algorithm muggleton explicit negative examples mentioned fold corresponds independent run simulator simulator generated constants renamed uniquely folds avoid duplicate names folds figure shows sample rule aleph system learned notice rules obtained low observability rule detects murder hire perpetrator commited crime contractor met bank account receive transfer money rule covers positive examples average accuracy folds rules aleph generated focused money transactions involving perpretator contractor murder hire victima contractorb perpetratorc perpetrator crimed perpetratorc social participants meetinge contractorb social participants meetinge personf account holder accountg personf generic moneytransferh accountg generic moneytransferi accountg figure task-based simulator data sample learned rule current future research under-studied issue relational data mining scaling algorithms large databases research ilp rdm conducted machine learning arti cial intelligence communities database systems communities insuf cient research systems issues involved performing rdm commercial relational-database systems scaling algorithms extremely large datasets main memory integrating ideas systems work data mining deductive databases ramamohanarao harland critical addressing issues related scaling working ciently learning complex relational concepts large amounts data stochastic sampling methods major shortcoming ilp computational demand results large hypothesis spaces searched intelligently sampling large spaces provide excellent performance time srinivasan zelezny srinivasan page developing algorithms learn robust probabilistic relational concepts represented stochastic logic programs muggleton variants enrich expressiveness robustness learned concepts alternative stochastic logic programs working learning clauses constraint logic programming language constraints bayesian networks page costa page cussens approach plan investigate approximate prior knowledge induce accurate comprehensible relational concepts fewer training examples richards mooney prior knowledge greatly reduce burden users express easy aspects task hand collect small number training examples extend prior knowledge finally plan active learning ilp systems select effective training examples interactively learning relational concepts muggleton intelligently choosing examples users label extraction accuracy obtained fewer examples greatly reducing burden users ilp systems related work widely studied ilp approach relational data mining participants eeld program taking alternative rdm approaches pattern learning link discovery section brie reviews approaches graph-based relational learning relational data mining methods based learning structural patterns graphs subdue cook holder cook holder discovers highly repetitive subgraphs labeled graph minimum description length mdl principle subdue discover interesting substructures graphical data classify cluster graphs discovered patterns match data subdue employ inexact graph-matching procedure based graph edit-distance subdue successfully applied number important rdm problems molecular biology geology program analysis applied discover patterns link discovery part eeld project details http ailab uta eeld relational data easily represented labeled graphs graph-based rdm methods subdue natural approach probabilistic relational models probabilistic relational models prm koller pfeffer extension bayesian networks handling relational data methods learning bayesian networks extended produce algorithms inducing prm data friedman prm nice property integrating advantages logical probabilistic approaches knowledge representation reasoning combine representational expressivity rst-order logic uncertain reasoning abilities bayesian networks prm applied number interesting problems molecular biology web-page classi cation analysis movie data applied pattern learning link discovery part eeld project relational feature construction approach learning relational data rst atten propositionalize data constructing features capture relational information applying standard learning algorithm resulting feature vectors kramer lavra flach proximity neville jensen system constructs features categorizing entities based categories properties entities related interactive classi cation procedure dynamically update inferences objects based earlier inferences related objects proximity successfully applied company movie data applied pattern learning link discovery part eeld project conclusions link discovery important problem automatically detecting potential threatening activity large heterogeneous data sources darpa eeld program government research project exploring link discovery important problem development counter-terrorism technology learning linkdiscovery patterns potentially threatening activity dif cult data mining problem requires discovering relational patterns large amounts complex relational data existing data-mining methods assume data single relational table link discovery relational data mining techniques inductive logic programming needed problems molecular biology srinivasan natural-language understanding zelle mooney web page classi cation craven information extraction califf mooney freitag areas require mining multi-relational data relational data mining requires exploring larger space patterns performing complex inference pattern matching current rdm methods scalable large databases relational data mining major research topics development generation data mining systems area counter-terrorism acknowledgments research sponsored defense advanced research projects agency managed rome laboratory contract views conclusions contained document authors interpreted necessarily representing cial policies expressed implied defense advanced research projects agency rome laboratory united states government tor santos costa castro dutra leave coppe sistemas federal rio janeiro partially supported cnpq hans chalupksy group isi andr valente gave support task-based simulator biomedical computing group support staff condor team computer sciences department wisconsin madison invaluable condor ashwin srinivasan aleph system breiman breiman bagging predictors machine learning breiman breiman stacked regressions machine learning califf mooney califf mooney relational learning pattern-match rules information extraction proceedings national conference arti cial intelligence cook holder cook holder substructure discovery minimum description length background knowledge journal arti cial intelligence research cook holder cook holder graph-based data mining ieee intelligent systems cook hayon cook hayon chronology russian killings transnational organized crime costa page cussens costa page cussens clp constraint logic programming bayesian network constraints unpublished technical note cowie lehnert cowie lehnert information extraction communications acm craven craven dipasquo freitag mccallum mitchell nigam slattery learning construct knowledge bases world wide web arti cial intelligence dietterich dietterich machine-learning research current directions magazine dutra page santos costa dutra page santos costa empirical evaluation bagging inductive logic programming proceedings international conference inductive logic programming lecture notes arti cial intelligence springer-verlag zeroski lavra zeroski lavra introduction inductive logic programming zeroski lavra eds relational data mining berlin springer verlag zeroski lavra zeroski lavra eds relational data mining berlin springer verlag zeroski zeroski relational data mining applications overview zeroski lavra eds relational data mining berlin springer verlag extraction transport extraction transport bayesian netwrok simulator internal report freitag freitag information extraction html application general learning approach proceedings national 
conference arti cial intelligence madison aaai press mit press friedman friedman getoor koller pfeffer learning probabilistic relational models proceedings international joint conference arti cial intelligence han kamber han kamber data mining concepts techniques san francisco morgan kauffmann publishers hand mannila smyth hand mannila smyth principles data mining cambridge mit press jensen goldberg jensen goldberg eds aaai fall symposium arti cial intelligence link analysis menlo park aaai press koller pfeffer koller pfeffer probabilistic frame-based systems proceedings national conference arti cial intelligence madison aaai press mit press kramer lavra flach kramer lavra flach propositionalization approaches relational data mining zeroski lavra eds relational data mining berlin springer verlag lavrac dzeroski lavrac dzeroski inductive logic programming techniques applications ellis horwood lehnert sundheim lehnert sundheim performance evaluation text-analysis technologies magazine mckay woessner roule mckay woessner roule evidence extraction link discovery eeld seedling project database schema description version technical report veridian systems division muggleton muggleton bryant page sternberg combining active learning inductive logic programming close loop machine learning colton proceedings aisb symposium scienti creativity informal proceedings muggleton muggleton inductive logic programming york academic press muggleton muggleton learning positive data machine learning journal accepted subject revision muggleton muggleton stochastic logic programs journal logic programming neville jensen neville jensen iterative classi cation relational data papers aaaiworkshop learning statistical models relational data austin aaai press mit press nist nist ace automatic content extraction http nist gov speech tests ace page page ilp lloyd dahl furbach kerber lau palamidessi pereira sagiv stuckey eds proceedings computational logic springer verlag ramamohanarao harland ramamohanarao harland introduction deductive database languages systems vldb journal richards mooney richards mooney automated nement rst-order horn-clause domain theories machine learning sparrow sparrow application network analysis criminal intelligence assessment prospects social networks srinivasan srinivasan muggleton sternberg king theories mutagenicity study rst-order feature-based induction arti cial intelligence srinivasan srinivasan study sampling methods analysing large datasets ilp data mining knowledge discovery srinivasan srinivasan aleph manual team lead powers team lead powers taskbased simulator version july internal report information extraction transport wasserman faust wasserman faust social network analysis methods applications cambridge cambridge press williams woessner williams woessner nuclear material traf cking interim assessment transnational organized crime williams woessner williams woessner nuclear material traf cking interim assessment ridgway viewpoints technical report ridgway center pittsburgh williams williams patterns indicators warnings link analysis contract killings dataset technical report veridian systems division witten frank witten frank data mining practical machine learning tools techniques java implementations san francisco morgan kaufmann woessner woessner chronology nuclear smuggling incidents july -may transnational organized crime woessner woessner chronology radioactive nuclear materials smuggling incidents july -june transnational organized crime wrobel wrobel inductive logic programming knowledge discovery databases zeroski lavra eds relational data mining berlin springer verlag zelezny srinivasan page zelezny srinivasan page lattice-search runtime distributions heavy-tailed proceedings international conference inductive logic programming springer verlag zelle mooney zelle mooney learning parse database queries inductive logic programming proceedings national conference arti cial intelligence 
agtatdna sequence translated protein sequence tyr gly pro coding coding aaa ttt tttaaaaac aac generalization error training examples nucleotides codons generalization error training examples nucleotides codons relational skill transfer advice taking lisa torrey ltorrey wisc jude shavlik shavlik wisc trevor walker twalker wisc wisconsin madison usa richard maclin rmaclin umn minnesota duluth usa abstract describe reinforcement learning system transfers relational skills previously learned source task related target task system inductive logic programming analyze experience source task transfers rules actions target-task learner accepts rules advice-taking algorithm system accepts humanprovided advice guide application transferred skills provide instruction skills target task introduction machine learning tasks addressed independently implicit assumption task relation tasks domains reinforcement learning assumption false tasks domain tend similarities view similarities shared skills goal transfer shared skills source task order speed learning similar target task suppose soccer player learned source task ball opponents passing teammates target task suppose ball opponents order shoot goals player started passing skills source task master target task quickly human observer give tips aspects problem shoot goal present system interacting agents manner called advice induction appearing icmlworkshop structural knowledge transfer machine learning june pittsburgh longer version group working paper instruction system constructs relational transfer advice inductive logic programming analyze experience source task learn skills rst-order logic user add supplementary user advice skills target task learner receives advice follow ignore reinforcement learning robocup demonstrate approach robocup simulated soccer domain figure task m-on-n keepaway objective reinforcement learners called keepers ball hand-coded players called takers learners receive reward time step team ball original keepaway task stone sutton keeper ball choose hold pass teammate version called mobile keepaway move inwards outwards clockwise counterclockwise respect eld center developed version expect realistic movement generalize robocup games keepaway state representation keepers ordered distance learner takers features include distances angles players -onkeepaway distbetween player anglede nedby keeper closesttaker note simplicity denote variable types names task m-on-n breakaway torrey objective reinforcement learners called attackers score goal hand-coded defenders hand-coded goalie learners receive reward goal reward attacker ball choose move ahead left respect goal pass teammate shoot goal section relational skill transfer advice taking -onkeepaway -onbreakaway figure snapshots keepaway breakaway games breakaway state representation attackers ordered distance learner defenders features include distances angles players goal sections -onbreakaway distbetween player anglede nedby goalpart goalie tasks approach stone sutton features discretized intervals called tiles boolean feature tile distbetween takes bewteen units sarsa algorithms approximates q-function incorporates advice linear optimization method called kbkr maclin player code developed amsterdam trilearn code base transferring skills propose transfer method assume tasks similarly structured q-functions transfers relational skills analyzing games played source task games collections state-action pairs action viewed classication state pairs training examples learn classify states keepaway games learn concept states passing teammate good action user identi skills transferred mapping relates objects source task target task optionally advice guide transferred skills provide skills information performs transfer automatically existing game traces source task system learns skill concepts translates advice target task applies transfer advice user advice learning target task table summarizes algorithm high-level pseudocode figure illustrates transfer part algorithm ilp mapping state distbetween distbetween distbetween action pass outcome caught training examples pass teammate distbetween teammate distbetween skill concept distbetween distbetween prefer pass advice figure showing transfers skills advice item conjunction conditions constraint applied met prefer pass move actions advice ned disagrees learner experience advice-taking algorithm maclin protection imperfect transfer anticipate task arises domain data task exists target task breakaway assume objective maximize probability scoring goal playing number training games learning skills inductive logic programming ilp learn skill concepts rst-order rules generalize rule pass teammate capture essential elements passing skill rules passing individual teammates prolog-based aleph software package srinivasan conducts random search hypothesis space selects rule nds highest score generalization familiar metric produce datasets search selects positive negative examples source task games positive state meet conditions skill performed desired outcome occurred expected q-value recent q-function minimum score minqpos table algorithm game traces source task skills transferred object mapping tasks user advice optional skill collect training examples learn rules aleph select rule highest score translate rule transfer advice learn target task advice relational skill transfer advice taking action pass teammate outcome caught teammate pass teammate good pass teammate action good pass teammate bad positive pass teammate negative pass teammate reject figure selects training examples ratiopos times expected q-values actions negative action performed highest q-value minimum score minq neg expected q-value skill learned rationeg times highest maximum score maxqneg standard ratio values ratiopos rationeg parameters set positive negative examples figure illustrates sorting process robocup mapping skills produce advice task system translates source-task objects target-task objects based user-provided mapping learning mapping automatically interesting topic addressed instantiates skills pass teammate speci rules pass finally propositionalizes conditions variables rule -onbreakaway condition distbetween attacker ectively disjunction conditions distance distance disjunctions part advice language represent recall feature range divided boolean tiles feature falls interval system express disjunction requiring tiles active distbetween distbetween tile boundaries exist target task adds tile boundaries feature space advice source task expressed knowing target-task feature space learning source task user advice part mapping task users optionally provide additional advice advice guide application transferred skills passing skills transferred keepaway breakaway make distinction passing goal goal objective score goals players prefer passing goal user provide guidance instructing system add condition pass teammate skill distbetween teammate goal distbetween goal users provide advice skills transferred source task users provide simple rules shoot actions breakaway user advice perform transfer natural powerful users interact transfer process empirical results present results transferring skill pass teammate -onmobile keepaway -onbreakaway learned skill prolog notation pass teammate distbetween teammate anglede nedby teammate closesttaker distbetween taker distbetween player skill produces item transfer advice teammate encourage passing goal assume user adds extra constraint section finally assume user approximations skills breakaway distbetween goalleft anglede nedby goalleft goalie prefer shoot goalleft actions distbetween goalright anglede nedby goalright goalie prefer shoot goalright actions distbetween goalcenter prefer moveahead shoot actions compare learning breakaway advice evaluate ects analyze individual contributions transfer advice user advice compare learning advice type separately curve figure average independent runs data point smoothed games transfer advice include constraint passing forward scoring probability relational skill transfer advice taking probability goal games played advice transfer advice user advice advice 
figure learning curves -onbreakaway combinations transfer user advice higher con dence level based unpaired t-tests games played full set advice scoring probable con dence level point full system performs signi cantly user transfer advice con dence level transfer user guidance produce synergy performing sum parts related work approach builds methods providing advice agents driessens dzeroski human guidance create partial initial q-function relational kuhlmann propose rulebased advice increase q-values xed amount aspect work extracting explanatory rules complex functions sun studies rule learning neural-network based reinforcement learners fung investigate extracting rules support vector machines address knowledge transfer singh studies transfer knowledge sequential decision tasks taylor stone copy initial q-functions transfer keepaway games erent sizes torrey introduce transfer keepaway breakaway advice constructed q-function conclusions future work reinforcement learners bene signi cantly user-guided transfer skills previous task presented system transfers structured skills learning rst-order rules agent behavior allowing user guidance system assume similar reward structure source target tasks robustness imperfect transfer advicetaking experimental results demonstrate effectiveness approach complex domain challenge encountered transfer learning erences action sets reward structures source target task make cult transfer shared actions changing game objective adding action meaning shared skill addressed problem user guidance human domain knowledge apply transferred skills prefer minimize user role transfer underlying issue separation general speci knowledge transfer learning transfer general aspects skills domain ltering task-speci aspects ilp learn general rst-order skill concepts step goal future step learning skills multiple games domain lead general rules transfer acknowledgements research partially supported darpa grant united states naval research laboratory grant driessens dzeroski integrating experimentation guidance relational reinforcement learning proc icml fung sandilya rao rule extraction linear support vector machines proc kdd kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice aaai workshop supervisory control learning adaptive systems maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression proc aaai singh transfer learning composing solutions elemental sequential tasks machine learning srinivasan aleph manual stone sutton scaling reinforcement learning robocup soccer proc icml sun knowledge extraction reinforcement learning learning paradigms soft computing taylor stone behavior transfer value-functionbased reinforcement learning proc aamas torrey walker shavlik maclin advice transfer knowledge acquired reinforcement learning task proc ecml 
generalization error training examples nucleotides perceptron nucleotides hus codons perceptron generalization error citre iterations data set data set data set data set decrease false negatives decrease false positives node node node node node node newnode existing node node empty knowledge based portion outputs inputs extra hidden units 
percent missing rules test set error kbann strawman topgen standard neural network domain theory percent missing rules topgen kbann strawman test set error test set error rate splice junctions promoters terminators key kbann topgen strawman 
learning extract genic interactions gleaner mark goadrich richm wisc louis oliphant oliphant wisc jude shavlik shavlik wisc department biostatistics medical informatics department computer sciences wisconsin-madison avenue madison usa abstract explore application gleaner inductive logic programming approach learning highly-skewed domains learning language logic biomedical information-extraction challenge task create describe large number background knowledge predicates suited task gleaner outperforms standard aleph theories respect recall additional linguistic background knowledge improves recall introduction information extraction process scanning unstructured text objects interest facts objects recently biomedical journal articles major source interest community number reasons amount data enormous objects proteins genes standard naming conventions interest biomedical practitioners quickly relevant information blaschke shatkay feldman eliassi-rad shavlik ray craven bunescu framed machine learning task information unstructured text documents extract relevant objects relationships inductive logic programming ilp well-suited biomedical domains ilp offers advantages straight-forward incorporate domain knowledge expert advice produces logical clauses suitable analysis revision humans improve performance appearing proceedings learning language logic workshop lll bonn germany copyright author owner article report data-preparation techniques results applying gleaner goadrich learning language logic biomedical information extraction task learning genic interactions gleaner two-stage ilp algorithm learns broad spectrum clauses combines thresholded disjunctive clause aimed maximizing precision choice recall compare results standard aleph srinivasan recall precision discuss areas open future research data preparation dataset article learning language logic challenge task goal learn recognize interaction english sentences protein agents gene targets bacillus subtilis sentences training set contained direct agent target gere stimulates cotd transcription indirect gere binds site promoters cotx relation gere cotx mediated phrase promoters organizers call subtasks co-reference co-reference chose learn separately rst learning relationships co-reference learning relationships co-reference training data consist sentences found medline database relations co-reference relations co-reference subtask trainset tuneset threshold making testset predictions slightly erent tasks found bene examples outweighed dividing training sets subfolds http genome jouy inra texte lllchallenge http ncbi nlm nih gov pubmed learning extract genic interactions gleaner filtering positive examples dataset consisting word word pairings labeled challenge-task committee negative examples left participants negative examples per-sentence basis rst nding words participate positive relationship pairings words labeled positives negatives training tuning produced co-reference negative examples co-reference negative examples testset provided unlabeled contained sentences task co-reference task co-reference unlike training data testset contained sentences relations testset created examples pairing protein gene names found provided dictionary produced total testset examples subsequent experiments reduced examples removing testset examples agent target relation identical happened trainset ultimately positive negative test examples co-reference task positive negative test examples co-reference task background knowledge prepare data learning inductive logic programming constructed variety background knowledge sentence structure statistical word frequencies lexical properties biomedical dictionaries examples table rst set relations sentence structure brill tagger retrained genia dataset kim predict part speech word employ shallow parser created burr settles conditional random fields erty trained standard corpus sang derive parse tree nested phrases sentences dataset phrases sentence root words members phrase figure shows sample sentence parse divided level phrases word phrase sentence unique identi based ordering abstract sen create relations sentences phrases words based actual text document structure sentence child figure sample sentence parse noun verb preposition noun phrase verb phrase prepositional phrase phrase previous word tree structure sequence words predicates nounphrase article verb describe part speech structure include actual text sentence background knowledge predicate word string maps identi ers words addition words sentence stemmed porter stemmer porter stemmed version words general sentence-structure predicates word phrase added allowing navigation parse tree phrases tagged rst phrase sentence likewise words length phrases calculated explicitly turned predicate length words phrases sentences phrases classi short medium long additional piece information predicate phrases true arguments distinct phrases group background relations frequency words appearing target phrases training set frequently occurring words indicators underlying semantic class helpful identifying correct phrases testset created boolean predicates ratios times times times general word frequency sentences training set formula determine words matched ratios wordjwi target phrase wordjwi target phrase words depend bind protein times protein phrases phrases general co-reference training set gradations calculated target arguments protein gene automatically create semantic classes consisting high frequency words semantic classes mark occurrences words training testing set source background knowledge delearning extract genic interactions gleaner table translation sample sentence ykud transcribed sigk rna polymerase sporulation prolog sentence abstract pubmed predicates created listed background prolog predicates created knowledge sentence sentence sen structure phrase sen phrase sen word sen word sen word sen phrase child sen sen word sen sen word string sen ykud target arg target arg sen part speech segment sen segment sen sen sen prep sen medical ontologies phrase mesh term sen rna lexical properties phrase alphanumeric word sen phrase specific word sen transcribed phrase originally leading cap sen word frequency phrase arg word sen rived lexical properties word alphanumericwords numbers alphabetic characters sigma spo alphabetic words alphabetic characters lexical morphological features include singlechar hyphenated membranebound capitalized rna words classi novelword sporulation standard usr dict words dictionary unix lexical predicates augmented make applicable phrase level general predicates created pairs triplets words assert phrase word bind tagged verb step search hypothesis space fourth source incorporate semantic knowledge biology medicine background relations medical subject headings mesh sentence structure simpli hierarchy level phrases labeled predicate phrase mesh term words phrase match words mesh http nlm nih gov mesh meshhome html additionally predicates added denote ordering phrases target arg target arg asserts protein phrase occurs gene phrase similarly target arg target arg created identical target args true protein gene phrases phrase phrase sigmab dependent katx expression adjacent target args adjacent phrases gene protein count phrases target arguments ned predicates describing training examples enriched data background knowledge provided challenge task organizers processed corpus link parser temperly tool automatically constructing syntactic parse tree rened output create type additional information word assigned root word called lemma instance word lemma type information syntactic relations words included appositive complement modi negation learning extract genic interactions gleaner table pseudo-code gleaner algorithm initialize bins create recall bins bin bin bin uniformly divide recall range populate bins clauses generated pick seed bottom clause rapid random restart clauses generation clause find recall binr trainset precision 
recall replace binr determine bin threshold binj find highest precision theory trainset recall clauses match examples recall binj evaluate testset find precision recall testset bin decision process object subject relations sentence grammar predicted parts speech word relationship total relations sentence ykud transcribed sigk rna polymerase sporulation link parser reports noun yukd subject verb transcribed polymerase complements transcribed rna sigk modi ers polymerase chose ignore lemma information previously incorporated stem word focused syntactic information predicates compare inclusion versus exclusion enriched background information results gleaner gleaner goadrich randomized search method collects good clauses broad spectrum points recall dimension recallprecision curves employs clauses thresholding method combine sets selected clauses pseudo-code algorithm appears table gleaner aleph srinivasan underlying engine generating clauses input aleph takes figure sample run gleaner seed bins showing considered clause small circle chosen clause bin large circle repeated seeds gather clauses assuming clause found falls bin seed background information form intensional extensional predicates list modes declaring predicates chained designation predicate head predicate learned high-level overview aleph sequentially generates clauses positive examples picking random seed saturated create bottom clause relation background knowledge connected relations xed number steps bottom clause determines search space clauses aleph heuristically searches space clauses clause found time runs clauses learned cover positive training examples learned clauses combined form theory experiments compare gleaner standard aleph theories initialization rst stage gleaner learns wide spectrum clauses illustrated figure search clauses random seed examples encourage diversity experiments recall dimension uniformly divided equal sized bins seed clauses rapid random restart zelezn clauses generated compute recall clause determine bin clause falls bin track clause appearing bin current seed heuristic function precision recall determine clause end search process clauses collected seed seed examples total clauses assuming clause found falls learning extract genic interactions gleaner bin seed stage modi slighly goadrich takes place clauses gathered random search gleaner combines clauses bin create large thresholded disjunctive clause bin form clauses cover order classify positive theories generate recall-precision curves exploring values tuneset starting incrementally lowering threshold increase recall curves overlap recall precision results choose theory created highest points combined curve tuneset irrespective bin generated points end recall-precision points bin span recall-precision curve unique aspect gleaner point recall-precision curve generated separate theory usual setup create curve hypothesis transformed ranking examples nding erent thresholds classi cation separate-theory method related roc convex hull created separate classi ers fawcett separate theories strength gleaner approach theory point curves hindered mistakes previous points theory totally independent end-user gleaner choose preferred operating point recall-precision curve algorithm generate testset classi cations closest bin desired recall results found threshold results dimensions vary training methods learning data co-references data co-references including provided linguistic information enriched basic data tables show results gleaner testset data combinations restriction word agent target relation sample clause learned aleph found table clause focused common property agents targets agents nouns internal capital challenge-task submission test examples non-identical restriction resulted small increase precision results table results gleaner aleph theory baseline all-positive prediction lll challenge task coreference alg enriched recall precision gleaner aleph aleph pos table results gleaner aleph theory baseline all-positive prediction lll challenge task coreference alg enriched recall precision gleaner aleph aleph pos letters complements nouns complement verbs targets noun phrases negatively correlated words training set chose preferred operating point choosing bin highest measure tuning set bin basic dataset co-reference enriched dataset co-reference bin dataset co-reference enriched data similar recall points achieved marked decrease precision coreference dataset plan explore enriched data link parser temperly future work informationextraction datasets show comparison gleaner algorithms examine results single aleph theory learned training set combination restrict clause learned minlearning extract genic interactions gleaner table sample clause recall precision co-reference training set agent target complement complement pass word parent phrase internal cap word word parent phrase arg halfx word arg isa segment target arg target arg variable agent target sentence variables clause figure recall-precision curves gleaner aleph dataset co-reference imum precision cover minimum positives training set maximum clauses clause theory basic data aleph improves precision recall lower results gleaner notice large drop precision recall clauses clauses attribute tting compare algorithm calling positive guarantees recall notice gleaner increase precision baseline datasets figure shows recall-precision curves gleaner recall-precision points aleph theories dataset co-reference figure shows results dataset co-reference gleaner span recall-precision dimension stellar results figure recall-precision curves gleaner aleph dataset co-reference co-reference dataset gleaner distinguishing agent target genic interaction predicted predicted genic interaction keeping precision lower low results fact sentences genes proteins relationships included training sets made half testing set lack negative sentences training sets hampered ability distinguish good bad sentences learning clauses size lll challenge task small comparison previous work goadrich creating possibility tting ected enriched linguistic predicates statistical predicates focused irrelevant words speci gene protein words sigma gere collecting labeled learning extract genic interactions gleaner data biomedical information extraction expensive bene worth cost conclusions paper explored inductive logic programming approaches biomedical information extraction aleph learns high-precision clauses cover training set gleaner learns clauses wide spectrum recall points combines create broad thresholded theories developed large number background knowledge predicates capture structure semantics biomedical text evaluated algorithms learning language logic challenge task work remaining combination ilp biomedical information extraction logical structure sentence parses biological semantic class information readily included ilp approach genic-interaction dataset interesting agent entity target entity closed set crossover worth noting erence training set testing set respect negative examples plan explore issues arose dataset perform cross-validation experiments test statistical signi cance results include negative sentences training set acknowledgements gratefully acknowledge funding usa nlm grant usa nlm grant usa darpa grant usa air force grant burr settles parsing tagging sentences blaschke hirschman valencia information extraction molecular biology briefings bioinformatics brill transformation-based error-driven learning natural language processing case study part speech tagging computational linguistics bunescu kate marcotte mooney ramani wong comparative experiments learning information extractors proteins interactions journal arti cial intelligence medicine eliassi-rad shavlik theoryre nement approach information extraction proceedings international conference machine learning fawcett roc graphs notes practical considerations researchers technical report labs hpl- goadrich oliphant shavlik learning ensembles first-order clauses recall-precision curves case study biomedical information extraction proceedings international conference 
inductive logic programming ilp porto portugal kim ohta teteisi tsujii genia corpus semantically annotated corpus bio-textmining bioinformatics erty mccallum pereira conditional random elds probabilistic models segmenting labeling sequence data proc international conf machine learning morgan kaufmann san francisco porter algorithm stripping program ray craven representing sentence structure hidden markov models information extraction proceedings international joint conference arti cial intelligence ijcai sang transforming chunker parser linguistics netherlands shatkay feldman mining biomedical literature genomic era overview journal computational biology srinivasan aleph manual version http web comlab oucl research areas machlearn aleph temperly sleator erty introduction link grammar parser http link wisc link zelezn srinivasan page latticesearch runtime distributions heavy-tailed proceedings international conference inductive logic programming springer verlag 
machine learning research group working paper refining algorithms knowledge-based neural networks improving chou-fasman algorithm protein folding richard maclin jude shavlik computer sciences dept wisconsin dayton madison maclin wisc abstract describe method machine learning refine algorithms represented generalized finite-state automata knowledge automaton translated artificial neural network refined applying backpropagation set examples technique translating automaton network extends kbann algorithm system translates set propositional non-recursive rules neural network topology weights neural network set kbann network represents knowledge rules present extended system fskbann augments kbann algorithm handle finite-state automata employ fskbann refine chou-fasman algorithm method predicting globular proteins fold chou-fasman algorithm elegantly formalized non-recursive rules concisely finite-state automaton empirical evidence shows refined algorithm fskbann produces statistically significantly accurate original chou-fasman algorithm neural network trained standard approach provide extensive statistics type errors approaches makes discuss definitions solution quality protein-folding problem introduction addressing unsolved problem normal approach start researching problem approaches direction takes builds earlier work strategy generally computerized empirical learning systems systems learn concepts empirical learning systems acquire concept scratch ignoring knowledge exists problem ignoring existing knowledge dangerous resulting learned concept important factors identified earlier work learning system unable solve problem headstart prior knowledge give approach present empirical learning system advantage prior knowledge version report chapter computational learning theory natural learning systems hanson drastal rivest editors mit press problem systematic learning system takes initial knowledge attempting learn concept scratch refines initial knowledge address problem work extends kbann system knowledge-based artificial neural networks towell extended system refine chou-fasman algorithm chou predicting folded structure globular proteins difficult problem molecular biology kbann knowledge represented simple propositional-calculus rules referred domain theory create initial neural network represents knowledge contained rules network starting point learning randomlyconfigured initial network technique proven effective complex problems gene recognition noordewier towell original domain theory good solving problem main limitation kbann technique translate rules propositional variables non-recursive paper describes addition kbann extends simple type recursion extend kbann include simple form recursion representing state information neural network state network represents context problem problem cross room state variables represent things light room rules introduced solve problem account state problem rules turn light considered state problem light style problem solving problem solved step series actions leading state leads goal state turning light navigating couch moving cindy toy fire engine network similarly attempts determine resulting state current state input approach recursive state calculated step previous state network extended kbann system called finite-state kbann fskbann translates domain theories state information represented generalized finite-state automata hopcroft kbann fskbann translates domain theory neural network refines network backpropagation rumelhart set examples choice neural networks empirical learning system build made couple reasons basic reason networks provide general mechanism representing concepts neural network proper number hidden units hidden layers learn type concept hornik reason neural networks generally deal noisy incorrect data shavlik limitations neural networks basic problem selecting topology network determining topology neural network problem dependent experimentation weiss long costly process problem deciding input network choice greatly affect neural network features provided network unable learn concept provided network unable determine features critical problems handled extent kbann approach rules domain theory determine initial topology network network variables focus initially chose problem protein secondary-structure prediction defined section number reasons secondary-structure prediction problem open problem increasingly critical human genome project watson continues produce data problem attractive exist number algorithms proposed biological researchers attempt solve problem limited success chou-fasman algorithm chou focus work best-known widely-used algorithms field algorithm represented original kbann system represented fskbann secondary-structure problem interest number researchers applied standard neural networks problem holley qian work combines approaches achieve accurate result neural networks chou-fasman algorithm table illustrates broad overview task table overview approach protein-folding problem algorithm predicting secondary structure chou-fasman algorithm chou sample proteins secondary structure brookhaven x-ray crystallography database bernstein refine initial algorithm sample proteins predict secondary structure proteins paper presents empirically analyzes fskbann algorithm problem solving domains prior knowledge exists section presents basic kbann algorithm discusses extended algorithm handle state information section description protein-folding problem including discussion number approaches number experiments performed test utility fskbann approach problem fourth section describes enhancements approach exploring conclude review work related fields specifically work attempts solve protein-structure problem network methods state information algorithms refine domain theories kbann algorithm kbann algorithm towell translates domain theory represented simple rules promising initial neural network rules define network topology initialize weights network section outlines standard kbann simple algorithm works description extension standard algorithm domain theories represented finite-state automata fsas standard kbann kbann translates domain theories represented propositional non-recursive rules overview algorithm appears table algorithm takes input set rules specification propositions input output units kbann produces network proposition unit highly active proposition true inactive proposition false process assigning propositions units step creates initial network kbann adds extra links low weights network step step kbann trains network backpropagation refining original domain theory table overview kbann algorithm step translate domain theory neural network step add extra links input units network step train network backpropagation step setting initial topology weights rules domain theory determine topology network kbann represents rule separate unit antecedent rule kbann adds connection network unit representing antecedent unit representing rule consequent weight connection equal constant generally positive antecedents negated antecedents bias unit number positive antecedents rule translated resulting unit active positive antecedents negative antecedents translating rule figure kbann creates unit marked figure connections input units weights bias unit set units figure rules marked rule number figure links weight solid lines links weight dashed figure kbann process set sample rules neural network representing rules network adding extra links units figure representing rules figure units represent propositions represented unit correspond rule proposition consequent rule unit representing rule directly corresponds proposition rule imply proposition proposition handled slightly differently case implicit disjunction kbann generates unit rule generates unit represent disjunction units representing rules connected unit representing consequent connections weight bias unit representing consequent set figure unit representing active units representing rules active step adding extra connections initial topology initial weights set kbann adds lowly-weighted connections network connections network add antecedents rules learning phase basic algorithm connecting units connect unit level unit level connection kbann determines level unit recursively unit level 
higher level highest unit connected figure assuming input units level units labeled level unit proposition level kbann connects level-one units level-zero units level-two units levelone units exception made output units output units connected output units resulting network appears figure low-weight connections shown thin solid lines step refining domain theory setting network kbann trains network set examples backpropagation rumelhart neural network learning algorithm domain theory serves bias giving network good starting point backpropagation search weight space evaluation kbann kbann proven effective promoter problem towell splice-junction problem noordewier tasks molecular biology kbann achieves promoter problem error rate compared error rate neural networks non-learning solution splice-junction problem kbann standard neural networks error rate large numbers examples small numbers training examples kbann makes half number errors standard neural networks finite-state kbann limitation kbann domain theories expressed propositional non-recursive rules makes impossible easily represent domain theories chou-fasman algorithm predicts protein secondary structure address limitation extending kbann represent finite-state automata difficulties representing fsa neural network representing state automaton capturing notion scanning input string fskbann addresses issues maintaining past state fskbann neural-network structure similar networks introduced elman jordan represent state network takes input previous state input values determines state state previous-state input step copying-back process acts recurrent links fixed weights output units representing current state input units representing previous state chose representation fully-recurrent neural network implementation simplicity major difference networks fully-recurrent networks weight adjustment occurs network links output units inputs scanning input fskbann captures notion scanning series input values successively determining current state activating network input sejnowski input combined previously-determined state input network network determines resulting state concept input generalized networks fsa generally examines single input time requiring single input fskbann takes vector values representing current input input represented input propositions input amino acid neighbors protein mapping fsas neural networks final addition needed standard kbann algorithm handle fsas step transforms fsa set rules involving state fskbann translates transition arcs fsa rule antecedents rule original state predicate arc labeled consequent rule resulting state arc labeled state state fsa figure fskbann translates arc rule figure figure displays rules derived fsa figure fskbann maps rules network input units input propositions previous state output network represents resulting state network resulting figure rules shown figure fskbann concludes adding extra links network training network algorithmic details important consideration training neural networks state information output previous state possibilities teacher correct output actual output produced network approach advantages drawbacks network output output incorrect training step network learn wrong transition teacher output avoids problem network dependent receiving correct output input problem domains trained network achieve high accuracy case actual output advantage network learn state information place faith correctness information experiments protein-folding problem section ciu ciw aiv bix figure fskbann process sample finite-state automaton rules derived fsa initial fskbann translation fsa approach training actual output expect achieve perfect accuracy predictions preliminary testing teacher output led substantially worse results additions kbann extend larger class problems enriching vocabulary user express domain theories section describe fskbann represent refine chou-fasman algorithm predicting protein secondary structure protein-folding problem globular proteins long strings amino acids hundred elements long average amino acids represented capital letters string amino acids making protein constitutes primary structure protein protein forms folds three-dimensional shape spatial location amino acids structure tertiary structure protein tertiary structure important form protein strongly influences function predicting protein secondary structure present means exist determine tertiary structure protein include x-ray crystallography magnetic resonance processes costly time consuming alternative solution predict secondary structure protein approximation tertiary structure secondary structure protein description local structure amino acid prevalent system determining secondary structure divides protein types structures -helix regions -sheet regions random coils regions figure diagram tertiary structure protein shape divided regions secondary structure table shows sample mapping protein primary secondary structures basic task defined primary structure protein produce secondary-structure assignment amino acids protein section figure richardson figure ribbon drawing three-dimensional structure protein richardson areas resembling springs -helix structures flat arrows represent -sheets remaining regions random coils experiments focus learning perform task table primary secondary structures sample protein primary amino acids secondary local structures predicting secondary structure primary structure part protein-folding problem methods attempt predict super-secondary structures descriptions combinations secondary structures chothia problem combine information secondary structure primary structure possibly super-secondary structure determine tertiary structure cohen complete review protein-folding problem fasman approaches predicting secondary structure based biological information number algorithms proposed biological literature predicting protein secondary structure generally algorithms focus predicting secondary structure amino acid local information information string amino acids immediately protein algorithm focus chou fasman chou algorithm similar -helix -sheet prediction discuss -helix prediction figure overview algorithm step process find -helix nucleation sites nucleation sites amino acids part -helix structures conformation probabilities rules reported chou fasman sites algorithm extends structure forward backward protein long probability part -helix structure remains high -helix -sheet regions predicted chou-fasman algorithm compares relative probabilities regions resolve predictions overlap table results chou-fasman related non-learning algorithms note data sets test algorithms amino acids proteins part coil structures accuracy achieved trivially predicting coil important point biological researchers algorithms account local information achieve limited accuracy wilson generally believed accuracy accuracy limited structure affected portion protein protein due three-dimensional folding back proximity limited success results led researchers means creating prediction algorithms including neuralnetwork approaches primary structure predict nucleation sites step step extend regions step resolve overlaps figure steps chou-fasman algorithm table accuracies non-learning structure-prediction algorithms method accuracy comments chou-fasman original algorithm chou data qian chou-fasman reworked version chou nishikawa chou-fasman reworked version chou data qian lim lim nishikawa robson robson nishikawa garnier robson garnier data qian neural-network approaches researchers attempted neural networks solve protein-folding problem holley qian neural networks efforts input window amino acids consisting central amino acid predicted number amino acids sequence amino acid window position represented input units amino acids end network output predicts structure central amino acid includes unit type secondary structure holley karplus output units -helix -sheet predicting coil active networks include number hidden units single layer input output units number varied studies figure diagram type network networks trained set proteins secondary structure tested separate set proteins table presents results studies approach reason differences 
results studies sets proteins testing results disappointing strategy combine neural network approach knowledge chou-fasman algorithm produce network solution approach input units input windowprimary structure output units hidden units helix sheet coil predicted secondary structure figure general neural network architecture holley karplus qian sejnowski table neural network results secondary-structure prediction task number ofmethod accuracy hidden units window size holley karplus holley qian sejnowski qian representing chou-fasman algorithm finite-state automaton primary problem representing chou-fasman algorithm set rules capturing notion extending region neural network state information -helix region extended current window position network determine nucleation site predicted left window steps left window steps left window require lot replication rules rules recognize nucleation site copied window position nucleation site window solved problem representing chou-fasman algorithm fsa figure algorithm state helix remain break-helix true notion transition figure automata complex transition set rules dependent input window current state table displays samples types rules derived fsa full set rules appendix rules involve state information rules prove propositions rules rule involve state prove proposition break-helix rule break-helix defined terms propositions helix-break helix-break propositions defined propositions appendix rule results state refer previous state type rule occurs transition state occur matter previous state rule matter previous state init-helix true resulting state helix rule transition made state automaton transition state helix state coil break-helix true propositions refers window position helix-break means proposition helix-break true amino acid window position central amino acid init helix init sheet init sheet init helix break helix break sheet continue helix continue sheet helix sheet coil figure finite-state automaton represent chou-fasman algorithm table sample rules chou-fasman finite-state automaton break-helix helix-break helix-break helixi init-helix coili helixibreak-helix figure diagram type network represent chou-fasman domain theory important aspect approach fsa extends structure single direction scans primary structure extend structure directions network scans primary structure directions averages predictions experimental study performed number experiments protein structure-prediction problem evaluate fskbann study demonstrates fskbann small statisticallysignificant gain accuracy standard artificial neural networks anns non-learning version chou-fasman algorithm copy output input scanning direction predicted secondary structure init helixcont sheet init sheetcont helix helix helix sheet sheet input window hidden units figure general neural-network architecture represent chou-fasman algorithm section describes data experiments training method present analyze number experiments experiments evaluate effectiveness fskbann approach versus standard anns chou-fasman algorithm state information anns performance test-set proteins function number training examples conclude in-depth empirical analysis strengths weaknesses methods experimental details performed experiments data set qian sejnowski qian data set consists proteins total amino acids average length amino acids protein amino acids part coil structures part -helix structures part -sheet structures divided proteins randomly training test sets two-thirds proteins one-third proteins original proteins backpropagation rumelhart train neural networks network approaches fskbann standard anns patience fahlman criterion stopping learning training set patience criterion states training continue error rate decreased number training cycles study set number epochs patience minimum accuracy training set noisiness data set prevents close perfect learning training set training divided proteins portions training set tuning set lang employ training set train network tuning set estimate generalization network epoch system trains network amino acids training set assesses accuracy tuning set retain set weights achieving highest accuracy tuning set set weights measure test set accuracy system randomly chooses representative tuning set considers tuning set representative percentages type structure tuning set roughly approximate percentages training proteins percentages -helix -sheet random coil structures tuning set close percentages present training proteins system testing set comparing percentages chooses tuning set randomly picking set proteins evaluating representativeness repeating process tuning set chosen meets criterion empirical testing reported found tuning set size proteins achieved results fskbann anns important style training reported qian sejnowski tested network periodically retained network achieved highest accuracy test set experiment-running system trains fskbann networks protein scanning forward reverse protein averages predictions amino acid simulates extending secondary structure directions system trained anns ways scanning directions scanning direction qian report results anns scanning direction results slightly superior results scanning directions anns number hidden units fskbann networks represent domain theory hidden units suggested qian sejnowski reasons wished hold network size constant comparing fskbann standard anns qian sejnowski found minor differences accuracy number hidden units ranged final item note system post-processing network output suggested holley amino acid system predicts secondary structure choosing output unit highest activation system eliminates short sequences short sequences sequences -helix -sheet predictions amino acids length short sequences general occur real proteins kabsch prediction sequences replaced coil results analysis table results averaged test sets statistics reported percent accuracy percent accuracy secondary structure correlation coefficients secondary structure correlation coefficient mathews defined formula tps fps tps fns tns fps tns fns tpstns fpsfns formula secondary structure coefficient calculated tps tns fps fns number true positives true negatives false positives false negatives structure correlation coefficient good evaluating effectiveness prediction classes secondary structure separately resulting gain accuracy fskbann anns non-learning choufasman method statistically significant level confidence t-test table comparison fskbann ann non-learning chou-fasman algorithm accuracy correlation coefficients method total helix sheet coil helix sheet coil chou-fasman ann fskbann apparent gain accuracy fskbann ann networks appears fairly small percentage points number misleading correlation coefficients give accurate picture show fskbann -helix coil prediction -sheet prediction reason ann solution fairly accuracy predicts large number coil structures largest class predictions gain accuracy fskbann chou-fasman algorithm fairly large exhibits gain correlation coefficients interesting note fskbann chou-fasman solutions produce approximately accuracy -sheets correlation coefficient demonstrate chou-fasman algorithm achieves accuracy predicting larger number -sheets evaluate usefulness domain theory number training instances decreases estimate collecting proteins performed series tests divided training sets subsets contained proteins contained contained fourth training proteins process produced training sets training sets train fskbann ann networks figure results tests fskbann shows gain accuracy training set size statistically significant level confidence results figure demonstrate couple interesting trends fskbann networks matter large training set shape curve number training proteins test set correctness chou-fasman standard ann finite-state kbann figure percent correctness test proteins function training-set size accuracy continue increase proteins training anomaly curve gain accuracy training proteins large expect number training instances small domain knowledge big advantage problem small training set obtain random sets proteins 
indicative population individual proteins generally reflect distribution secondary structures population proteins large numbers -helix regions -sheets large numbers -sheet regions -helices learn predict skewed population network produce poor solution mitigated proteins introduced causing training population closely match population fskbann networks standard anns ways topology determined domain theory copy output network back state network step evaluate utility feature trained anns similar qian sejnowski work added networks state information output copied back part input step table compares results tests results standard anns results show state information increase accuracy network prediction table effect state variables standard neural-network approach accuracy correlation coefficients method total helix sheet coil helix sheet coil ann ann state finally analyze detailed performance approaches gathered number additional statistics fskbann ann chou-fasman solutions statistics analyze results terms regions region consecutive sequence amino acids secondary structure regions measure accuracy obtained comparing prediction amino acid adequately capture notion abstract reinforcement learning methods difficulty scaling large complex problems approach proven effective scaling make advice provided human extend recent advice-giving technique called knowledge-based kernel regression kbkr evaluate approach keepaway subtask robocup soccer simulator present empirical results show approach make effective advice work demonstrates potential advice-giving techniques kbkr offers insight design decisions involved employing support-vector regression introduction reinforcement learning techniques q-learning sarsa sutton barto effective learning techniques difficulty scaling challenging large-scale problems method addressing complexity problems incorporate advice provided human teacher approach advice proven effective number domains lin gordon subramanian maclin shavlik andre russell kuhlmann recently mangasarian introduced method called knowledge-based kernel regression kbkr kernel method incorporate advice form simple if-then rules support vector method learning regression real-valued problems technique proved effective simple regression problems tested article extend general kbkr approach test complex game robocup soccer simulator noda keepaway stone sutton applying kbkr approach keepaway found make number adjustments extensions kbkr method representation problem adjustments extensions proved critical effectively learning keepaway task section paper present basic kbkr method section present robocup simulator keepaway discuss difficulties game section discuss support-vector regressor q-learning present reformulation kbkr method issues address order good results present results experiments approach keepaway final sections discuss related research future directions conclusions knowledge-based support vector regression section present basics knowledge-based kernel regression mangasarian support vector regression linear regression problem involves find set weights offset learn function form transpose vector vector numeric features describing instance values describing soccer field player point view instance labeled taking action holdball denotes vector clarity omit understanding scalar set observed input vectors set states observed learning set values current estimates states find solution equation set states row input vector column feature set expected values input vector exact solution infeasible equation generalized solutions problem ranked meet performance criterion minimum error respect values kernel approach weight vector replaced dual form converts formulation generalized replacing term kernel produce article simply linear models understandable scale knowledge-based support-vector regression reinforcement learning richard maclin jude shavlik trevor walker lisa torrey minnesota duluth wisconsin madison computer science department computer sciences department kirby duluth dayton madison rmaclin umn shavlik torrey twalker wisc appears proceedings ijcai workshop reasoning representation learning computer games edinburgh scotland large numbers training examples tile coding shown produce non-linearity models reader note identical simply linear kernel linear programming method learn model simply expression minimized producing solution common formulation linear regression models call refer formulation slack variables solution inaccurate training examples penalize inaccuracies objective function minimized minimize weighted sum slack terms absolute weights term one-norm computes sum absolute values penalty weights penalizes solution complex parameter trading inaccurate solution terms complex solution weights resulting minimization problem presented linear program solver produces optimal set values knowledge-based kernel regression kbkr piece advice domain knowledge represented notation read conditions hold output equal exceed linear combination inputs threshold term term user region input space advice applies row matrix values represents constraint advice user give rule distancea distanceb distancea distanceb if-then rule matrix single row column feature distancea column distanceb entry features vector scalar general rows values set linear constraints treated conjunction define polyhedral region input space right-hand side advice applies vector scalar define linear combination input features predicted match exceed rule user advises left-hand side condition holds greater distancea distanceb captured creating vector coefficients features distancea distanceb setting advice format user reinforcement-learning task define set states specific action high low discuss numerically represent high mangasarian prove advice implication equivalent set equations solution converted non-kernel form softening leads optimization problem case linear models slack terms advice approximately satisfied parameters penalize slacks words slacks advice partially learner mangasarian tested method simple regression problems demonstrated resulting solution incorporate knowledge testing small feature spaces tested advice constraints input features article apply methodology complex learning problem based robocup simulator robocup soccer game keepaway experimented game keepaway simulated robocup soccer stone sutton game goal keepers ball takers long receiving reward time step hold ball keepers learn takers follow hand-coded policy figure keepaway involving keepers takers simplify learning task stone sutton chose learning occur keeper holds ball player ball nearest keeper pursues ball perform hand-coded moves open pass keeper holding ball keepers perform open move learnable action choice hold ball pass keeper note passing requires multiple actions simulation orienting body performing multiple steps kicking lowlevel actions managed simulator addressed stone sutton experiments policy takers simple takers pursue ball takers pursue ball cover keeper work employ feature representation stone sutton measure values define state world perspective keeper ball features record geometric properties pair-wise distances players angles formed sets players task made complex simulator incorporates noise information describing state addition actions agents noise chance keeper passing ball keeper misdirect ball possibly sending sybawsts scbw min zhwubz sybawsts zscbw zus min bounds takers score keeper team measured terms long hold ball stone sutton demonstrated task learned employed sarsa learning replacing eligibility traces cmac function approximator tile encoding state space feature discretized times set overlapping bins divide feature ranges overlapping bins width bin covering values representation proved effective experiments kbkr keepaway order regression formulate problem regression problem represent realvalued function set learned models action input model state model makes prediction action one-step sarsa estimate incremental training algorithms developed support vector machines employ batch training save series states actions reinforcements experienced set games stop train models resulting models chunk games create training examples data current model compute values state one-step sarsa learning estimates accurate obtained states encountered batch approach effective leads potential problem game continues data accumulates eventually sets constraints intractably large commercial solvers addition learner controls experiences older data valuable newer data mechanism choose training examples taking stochastic sample data set limit number training examples set limit experimented examples limit discard examples uniformly randomly selected half limit discard age probability select unselected age raised power age set produce data set maximum size initial experiments keepaway employed kernel-based regression directly numeric features stone sutton tile encoding experiments found gaussian linear kernels applied features performed slightly random policy results stay seconds compare figure results stone sutton tile coding led substantially improved performance technique experiments provide learning algorithms raw numeric features 
binary features result separately tiling features create binary features raw feature numeric features methods explore wide range features numeric features easily expressed advice critical adjustment found add kbkr approach append additional constraints constraints defined matrix vector approach added feature mentioned advice constraints form min feature feature max feature distancec ranges add constraint distancec addresses severe limitation original kbkr method original kbkr approach advice unmodified slack variables implies right-hand side true cases matter values features advise distancea output place restrictions features values kbkr algorithm include secondary structure biologists view cohen biologists knowing number regions approximate order regions important knowing structure amino acid lies predictions figure prediction misses completely -helix region errors prediction slightly skewed -helix region ends errors appears answer statistics gathered assess solution predicting -helix regions table -sheet regions table results table table give picture strengths weakness approach table shows fskbann solution overlaps slightly fewer actual -helix regions anns overlaps tend longer hand fskbann networks overpredict fewer regions anns predict fewer -helix regions primary structure secondary structure prediction prediction figure predictions secondary structure table region-oriented statistics -helix prediction occurrence description chou fasman actual helixpredicted predicted helix region overlap actual helix region percentage time helixactual average length actual helix region number regions helixpredicted average length predicted helix region number regions predicted helix actual helix actual helix regionis overlapped predicted helix region length overlap percentage time kbann ann correspond actual -helix region table fskbann ann networks accurately predict occurrence regions chou-fasman algorithm table demonstrates fskbann predictions overlap higher percentage actual -sheet regions chou-fasman algorithm anns accuracy -sheet predictions approximately fskbann chou-fasman method length overlap chou-fasman method longer fskbann cost predicting longer regions ann networks extremely poorly predicting overlapping actual -sheet regions fskbann networks anns overpredicting -sheets chou-fasman method results fskbann solution significantly ann solution predicting -sheet regions sacrifice accuracy predicting -helix regions finally results fskbann solution job avoiding problem overpredicting coil regions ann solution results suggest work developing methods evaluating solution quality solutions accurately predict classes favored solutions predicting largest class table region-oriented statistics -sheet prediction occurrence description chou fasman predicted sheet regiondoes overlap actual sheet region percentage timeactual predicted sheet actual sheet region overlapped predicted sheet region length overlap percentage time predicted actual average length predicted sheet region number regions predicted sheet sheet average length actual sheet region number regions actual sheet ann kbann sheet future work number extensions fskbann hopes increasing gain predictive accuracy extensions structure-prediction problem broken classes architecture training style neural network additions domain theory producing structure-prediction network studying problem translating trained network back human-readable form finally interested applying fskbann problems including biological recognizing splice-junctions finding reading frames problems natural language understanding neural network architectures studies work field holley qian suggest number method training networks observation noted holley karplus holley activity units helix sheet prediction correlate prediction correct table trend apparent networks table reports accuracy predictions output units helix sheet coil threshold threshold time units activation accuracy predictions table predictive accuracy coverage function output-unit activation threshold accuracy predicted observation suggests change prediction method predicting protein structure scan predict strongest activated areas feed predictions back network scan advantage helix sheet regions consist multiple amino acids predicting adjacent figure outlines process structure prediction work question information fed back network predictions chosen figure illustrates general structure type network plan feed information back difference networks earlier figures input window position represents amino primary structure initial prediction step step step final prediction figure predicting structure filling structures information subsequent predictions acid primary sequence current estimate probability types secondary structure initially values priori probabilities category network predicts set probabilities positions protein algorithm evaluate vector probabilities predictions threshold predictions scan type network system advantage information predictions made network trained similar fskbann networks scanning protein making prediction amino acid trained recurrent technique idea originally suggested qian sejnowski work qian neural networks call cascaded networks idea translating chou-fasman algorithm algorithm largely separate parts finding nucleation sites extending sequences figure sketches scheme networks fskbann network constructed domain theory nucleation sites predict probability position part structure neural network probabilities primary-structure information knowledge extending structures incorporate knowledge past predictions scanning protein information needed rules extend structures network exhibit effects helix sheet coil probability secondary structure input window amino acid probability secondary structure hidden units units figure network includes current prediction probabilities part input predicted secondary structure helix sheet coil input window features linear model feature hypothetically extremes violate part advice legal ranges features including booleans result tiling limit range input space kbkr satisfy advice reason automatically generate constraints binary features constrained true false advice numeric feature assume advice distancea add constraints capture information impacts tiles tiles checking distancea checking distancea add constraints tile false advice tile true tile feature bins tile match constraint imagine distancea divided tiles add constraints indicating false constraint distancea true constraint equation tiles true distancea distancea figure sample keepaway game keepers light gray black outlines takers black gray outlines ball held keeper upper game continues takers holds ball time steps sec ball bounds white lines distancea denotes feature tile distancea covering range cases tile partially lines constraint included advice distancea covered tiles advice included distancea add constraints tiles false add constraint tiles true equation shown cases occur count advice lining tiling scheme conjoin constraints tiles original constraint numeric feature safe include tiles span original advice constraint numeric feature needed extend mechanism advice kbkr order apply original kbkr work output values constrained linear combination constants produced user advice action high low mechanism convert terms numbers simply define high low decided training data numbers specifically term averageq advice computed examples training set term advice language makes easier advice states action units typical state briefly mentioned earlier linear programs penalize term models initial experiments found term led underfitting training data recall training data action model collection states applied training progresses training examples states executing action good idea action high states term learned models account high values model applied state action bad choice predicted high instance imagine training set examples approximately examples constant model optimal fit training set lead good performance deployed environment address weakness include training set states action low partly keeping early examples training set addition address weakness highly penalizing non-zero terms linear programs clarity explicitly show scaling factor term earlier linear programs hypothesis penalization encourage learner weighted feature values model function objective function penalizes weights models weights model set high values captured essential states high generalizing future states improved empirical evidence strongly penalizing supports hypothesis general carefully choose training examples non-incremental learner experimental results performed experiments keepaway task approach incorporating advice learner kbkr method experimental control support-vector regressor advice words section main algorithm experimental control modified explained section measure results terms length time keepers hold ball results show average learner employing advice outperform learner advice methodology experiments performed versus keepaway keepers takers takers employed fixed policy section keepers learning agents pooled experience learn single model shared keepers reinforcement signals learners receive step game game ends takers control ball ball bounds discount rate set stone sutton action-selection process policy performed exploitation action chose action highest time randomly chose action exploration remaining time stone sutton report average total reinforcement learners average time keepers held ball previous games set values examples scaling number examples penalizing average error training examples total error varying number examples number weights fixed penalty due data mismatch grow number training examples increases small number settings non-advice approach experimental control found worked hold advice pass advice figure pieces advice involve suggestion hold ball nearest taker pass ball taker closing teammate takers large passing lane kbkr approach simply chose values experimented settings point results graphs averaged ten runs results reported function number games played games length amount experience differs result mitigated provide state action pairs learners discussed advice employed pieces advice advice based advice kuhlman rule suggests keeper ball hold nearest taker fig left advice applies suggests holding exceed average holding meter closest taker meters advice notation distancenearesttaker hold averageq distancenearesttaker piece advice pass ball figure advice tests taker closing teammate takers passing lane degrees teammate advice applies suggests passing nearest teammate exceeds average seconds degree degrees seconds angles larger degrees results discussion fig presents results experiments results show learner advice obtains gains performance due advice retains sizable advantage performance large number training examples figure advice-taking based amino acid probability secondary structure copy output input hidden units probability secondary structure helix sheet coil hidden units amino acid figure networks cascaded prediction network calculates probability structure amino acid network combines predictions network primary structure predict secondary structure discussed qian sejnowski connecting areas secondary structure separated anomalous prediction leaving single anomalous predictions final idea redefine task learning fsa focusing predicting state step view problem predicting locations state transitions occur predicting structure amino acid goal find places amino acids structure type figure sample network approach transition network predicts middle amino acids network output units representing transitions type structure type structure cases structure change amino acids network predict transition open problem approach deciding combine predicted transitions consistent secondary structure additions structure-prediction domain theory altering network architecture number domain theory problem representation kbann domain theory give network good set initial weights search starts location weight space predicted transition transition point input window hidden units figure network predict finite-state transitions augmenting chou-fasman domain theory information increase accuracy solution training start location basic augmenting domain theory include information domain theories gorii goriii algorithms garnier lim method lim advantage adding approaches strengths prediction chou-fasman weak advantage add domain theories interact backpropagation training incorporate pieces information jacobs hunter hunter suggests input representation amino acids meaningful neural network approaches problem input encoding bit amino acids amino acid bit hunter suggests -bit representation bits encode properties 
amino acid meaningful biologists shared amino acids encoding unique amino acids information lost network find solution focus amino acids properties amino acids translation trained networks symbolic terms basic problem kbann approach extracting information human-readable form trained network towell shavlik approached problem kbann system towell problem examined researchers address rule extraction augmented networks fskbann include state information extract learned fsas problems finally question fskbann domains couple biological problems natural type approach splice-junction problem noordewier states intron exon reading-frame problem staden states reading frames interest evaluating approach problems fields natural language elman task involve learning recognize simple sentences involving simple regular grammar information grammar missing incorrect related research work shares similarities research areas algorithms predicting protein structure neural networks state information systems refine domain theories methods predicting protein secondary structure number algorithms proposed predicting protein secondary structure loosely divided biological knowledge non-learning methods learning mechanism non-learning methods widely approaches biological literature fasman predicting protein secondary structure chou-fasman chou prevelige robson garnier robson lim lim algorithms chou-fasman relies information likelihood type amino acid part secondary structure information summed local areas protein determine likelihood area part structure robson robson gorii goriii garnier solutions based information theory approaches neural networks base prediction window information central amino acid position central amino acid window position robson algorithm determines relevance amino acid predicting type secondary structure computerized versions chou-fasman robson techniques implemented tested qian sejnowski test data exhibit accuracy table lim method lim account long-range interactions stereochemical theory globular proteins secondary structure solutions garnier prevelige include theories fourth type secondary structure -turns classified coils main advantage fskbann algorithms fskbann biological information mechanism learning learning methods number investigators learning algorithms sample folded proteins predict secondary structure proteins holley karplus holley qian sejnowski qian simple hidden-layer neural networks predict secondary structure studies focus varying hidden unit size window size achieving results shown table parameters report test set accuracies qian sejnowski cascaded architecture produces percentage point improvement accuracy single network results stolorz stolorz perceptron architecture evaluate error function mutual information produces percentage point gain accuracy standard perceptron interesting thing stolorz measure improves helix sheet prediction expense coil prediction desirable effect coil making training data overpredicted neural-network techniques zhang machine learning zhang method combines information statistical technique memory-based reasoning algorithm neural network divide training set halves components trained half training set training half training set learn combine results components neural network results report training set size proteins zhang major difference learning approaches fskbann fskbann incorporates domain knowledge fskbann differs neural networks studies state information unlike approaches methods representing state information neural networks researchers proposed neural-network architectures incorporating information state idea retaining state context training patterns occurs work aimed solving natural language problems cleeremans elman mozer porat servan-schreiber john approaches mechanism preserving past activations units input sequence jordan jordan elman elman introduced recurrent network topology fskbann networks set hidden units called context units preserve state network time step previous context units copied back input system networks possibility keeping multiple past contexts input system difference networks elman jordan context units output units hidden units output network state idea type network introduced jordan represent finite-state automaton discussed cleeremans cleeremans state type network perfectly learn recognize grammar derived finite-state automaton major difference research cleeremans focus initial domain theory expressed finite-state automaton attempting learn scratch methods refining domain theories number efforts aimed refining approximate domain theories ourston mooney system ourston domain theory focus corrections inductive learning system performs system constructs explanation proof possibly incomplete instances concept inductive learner examines missing portions errors concept explanation suggest domain theory make explanations correct flann dietterich flann examine method fix domain theory general system takes examples concept refined produced explanations domain theory explanations passed inductive learning system finds maximally-specific shared explanation structure research differs efforts fskbann refines kbkr produce significant improvements reinforcement learner unpaired t-test performance games played research demonstrated advice previously section advantages support-vector based regression method make promising approach results directly comparable stone sutton implemented robocup players behavior ball differs slightly tile features differently stone sutton learning curves start seconds game end games results initially similar keepers games longer possibly due takers implemented stone sutton method good proxy focus relative impact advice non-advice solution related work number researchers explored methods providing advice reinforcement learners include methods replaying teaching sequences lin extracting information watching teacher play game price boutilier advice create reinforcement signals shape performance learner laud dejong methods similar goal shortening learning time differ significantly kind advice provided human work closely related work present includes techniques developed incorporate advice form textual instructions programming language constructs gordon subramanian developed method advice form condition achieve goals adjusts advice genetic algorithms work similar form advice significantly approach optimization linear programming incorporate advice previous work maclin shavlik developed language providing advice included simple if-then rules complex rules involving multiple steps rules incorporated neural network learned future observations earlier work hidden units added neural network represent advice article piece advice represents constraints acceptable solution andre russell developed language creating agents language user partial knowledge task programming constructs create solver includes choice points user specifies actions learner acquires policy choose possibilities work differs assume advice correct kuhlmann advice form rules states action good bad advice matched predicted action state increased fixed amount work differs work advice constraints values simply adding learner make advice advice represented data games played advice advice figure results standard support vector linear regression versus learner 
receives start learning advice text tested kuhlmann method no-advice algorithm found improve performance area related research work employ support vector regression methods agents dietterich wang lagoudakis parr explored methods support vector methods perform main limitation approaches methods assume model environment learned model simulation q-value inference approach traditional model-free approach state-transition function conclusions future directions presented evaluated approach applying mangasarian knowledge-based kernel regression kbkr technique tasks work investigated strengths weaknesses kbkr developed adjustments extensions technique successfully applied complex task experiments demonstrate resulting technique employing advice shows promise reducing amount experience learning complex tasks making easier scale larger problems key findings contributions paper demonstrated challenging game keepaway variant kbkr approach successfully deployed reinforcement-learning setting demonstrated batch support-vector regression learn challenging reinforcement-learning environment needing model impact actions environment found order advice effectively kbkr algorithm legal ranges input features advice absolutely discarded slack variables model includes input feature mentioned advice part advice guaranteed met current world state matches part augment advice numeric features making explicit constraints binary features results tiling numeric features found critical optimization penalize size weights solution sizable penalty term term solution learner simply predict work incremental support vector machines chose learn models batch fashion complex problem large set states quickly results constraints efficiently solved linear-programming system develop method selecting subset information train models found tile coding unable learn keepaway advantage tile domain theories recursive rules assume domain theory overly general vanlehn sierra system vanlehn extends domain theory expressed grammar system constructs maximal partial parse set training examples determine gaps holes parse examples sierra inductive learning algorithm fill gap grammar examples correctly parsed sierra depends receiving good sequence training examples assumes set training examples presented shares gaps set presented gaps fskbann neural network methodology refine domain theory vulnerable poor sequences data conclusions present evaluate fskbann system broadens kbann approach richer expressive vocabulary fskbann mechanism translating domain theories represented generalized finite-state automata neural networks networks trained techniques backpropagation refine initial domain knowledge extension kbann domain theories include knowledge state significantly enhances power kbann rules expressed domain theory account current problem-solving context state solution tested fskbann refining chou-fasman algorithm predicting protein secondary structure fskbann-refined algorithm proved accurate standard neural network approaches problem non-learning version choufasman algorithm fskbann solution proved effective considered terms class secondary structure fskbann shows gains predictive power classes anns non-learning chou-fasman algorithm finally work demonstrates criteria evaluating solutions protein-structure problem direct straightforward measure predicted structures match actual structures misleading definition solution quality possibly based edit distance cohen favor solutions predicting classes secondary structure success fskbann secondary-structure problem prove tool addressing problems include state information work improving neural-network refinement process extraction symbolic knowledge trained network acknowledgements research partially supported national science foundation grant iriand office naval research grant -jthe authors terrence sejnowski providing data testing geoffrey towell mark craven lorien pratt charles squires gary lewandowski valuable discussion bernstein bernstein koeyzle williams meyer brice rodgers kennard shimanouchi tatsumi protein data bank computer-based archival file macromolecular structures journal molecular biology chothia chothia principles determine structure proteins annual review biochemistry chou chou fasman prediction secondary structure proteins amino acid sequence advanced enzymology cleeremans cleeremans servan-schreiber mcclelland finite state automata simple recurrent networks neural computation cohen cohen kuntz tertiary structure prediction prediction protein structure principles protein conformation fasman plenum press york cohen cohen presnell cohen langridge proposal feature-based scoring protein secondary structure predictions proceedings aaaiworkshop artificial intelligence approaches classification pattern recognition molecular biology anaheim july elman elman finding structure time cognitive science fahlman fahlman lebiere cascade-correlation learning architecture neural information processing systems touretzky fasman fasman development prediction protein structure prediction protein structure principles protein conformation fasman plenum press york flann flann dietterich study explanation-based methods inductive learning machine learning rule 
learning searching adapted nets proceedings ninth national conference artificial intelligence anaheim july garnier garnier robson gor method predicting secondary structures proteins prediction protein structure principles protein conformation fasman plenum press york holley holley karplus protein structure prediction neural network proceedings national academy sciences usa hopcroft hopcroft ullman introduction automata theory languages computation addison wesley reading hornik hornik stinchcombe white multilayer feedforward networks universal approximators neural networks hunter hunter representing amino acids bitstrings proceedings aaaiworkshop artificial intelligence approaches classification pattern recognition molecular biology anaheim july jacobs jacobs jordan nowlan hinton adaptive mixtures local experts neural computation jordan jordan serial order parallel distributed processing approach technical report california institute cognitive science san diego kabsch kabsch sander dictionary protein secondary structure pattern recognition hydrogenbonded geometric features biopolymers lang lang waibel hinton time-delay neural network architecture isolated word recognition neural networks lim lim algorithms prediction -helical -structural regions globular proteins journal molecular biology mathews mathews comparison predicted observed secondary structure phage lysozyme biochimca biophysica acta mozer mozer bachrach slug connectionist architecture inferring structure finite-state environments machine neural learning trained neural network refined symbolic knowledge symbolic knowledge initial initial neural network network rules rules network coding non-linear kernels non-linearity tile coding sufficed finally looked simple ways extend mechanism advice kbkr found helpful refer dynamic properties advice average method giving advice natural manner future research directions include testing reformulated version kbkr additional complex tasks addition complex features advice language multi-step plans additional constraints optimization problem directly including bellman constraints optimization formulation ability give advice form world states action action combination support-vector techniques advice taking promising approach problems acknowledgements research supported darpa ipto grant naval research grant andre russell andre russell programmable reinforcement learning agents nips dietterich wang dietterich wang support vectors reinforcement learning ecml gordon subramanian gordon subramanian multistrategy learning scheme agent knowledge acquisition informatica kuhlmann kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer aaai workshop supervisory control learning adaptive systems lagoudakis parr lagoudakis parr reinforcement learning classification leveraging modern classifiers icml laud dejong laud dejong reinforcement learning shaping encouraging intended behaviors icml lin l-j lin self-improving reactive agents based reinforcement learning planning teaching machine learning maclin shavlik maclin shavlik incorporating advice agents learn reinforcements aaai mangasarian mangasarian shavlik wild knowledge-based kernel approximation journal machine learning research noda noda matsubara hiraki frank soccer server tool research multiagent systems applied artificial intelligence price boutilier price boutilier implicit imitation multiagent reinforcement learning icml stone sutton stone sutton scaling reinforcement learning robocup soccer icml sutton barto sutton barto reinforcement learning introduction mit press 
learning nishikawa nishikawa assessment secondary-structure prediction proteins comparison computerized chou-fasman method biochimica biophysica acta noordewier noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences advances neural information processing systems vol morgan kaufmann denver ourston ourston mooney changing rules comprehensive approach theory refinement proceedings eighth national conference artificial intelligence boston july porat porat feldman learning automata ordered examples machine learning prevelige prevelige fasman chou-fasman prediction secondary structure proteins choufasman-prevelige algorithm prediction protein structure principles protein conformation fasman plenum press york qian qian sejnowski predicting secondary structure globular proteins neural network models journal molecular biology richardson richardson richardson principles patterns protein conformation prediction protein structure principles protein conformation fasman plenum press york robson robson suzuki conformational properties amino acid residues globular proteins journal molecular biology rumelhart rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition volume foundations rumelhart mcclelland eds mit press cambridge sejnowski sejnowski rosenberg parallel networks learn pronounce english text complex systems servan-schreiber servan-schreiber cleeremans mcclelland graded state machines representation temporal contingencies simple recurrent networks machine learning shavlik shavlik mooney towell symbolic neural learning algorithms experimental comparison machine learning john john mcclelland learning applying contextual constraints sentence comprehension artificial intelligence staden staden finding protein coding sequences genomic sequences methods enzymology stolorz stolorz lapedes xia predicting protein secondary structure neural net statistical methods technical report la-ur- theoretical division los alamos national laboratory los alamos towell towell shavlik noordewier refinement approximate domain theories knowledge-base neural networks proceedings eighth national conference artificial intelligence boston july towell towell shavlik interpretation artificial neural networks mapping knowledge-based neural networks rules working paper computer sciences department wisconsin madison vanlehn vanlehn learning subprocedure lesson artificial intelligence watson watson human genome project past present future science weiss weiss kapouleas empirical comparison pattern recognition neural nets machine learning classification methods proceedings eleventh international joint conference artificial intelligence detroit august wilson wilson haft getzoff tainer lerner brenner identical short peptide sequences unrelated proteins conformations testing ground theories immune recognition proceeding national academy sciences usa zhang zhang exploration protein structures representation prediction thesis department computer science brandeis waltham zhang zhang personal communication appendix chou-fasman domain theory chou-fasman algorithm chou involves activities recognizing nucleation sites extending sites resolving overlapping predictions appendix provide details steps describe representation algorithm collection rules recognize nucleation sites chou fasman assign conformation values amino acids conformation values represent amino acid part helix sheet structure higher values group amino acids classes similar conformation classes helix formers highindifferent indifferent breakers sheet formers indifferent breakers table defines values types breakers formers table rules represent chou-fasman algorithm true amino acid positions secondary structure algorithm predicting predicts -helix nucleation site consecutive set amino acids helix formers helix breakers helix high-indifferent amino acids count helix rule determine location nucleation site simply adds helix-former helix-breaker values window amino acids wide totals greater predicts helix nucleation site proposition init-helix rules nucleation -sheets similar -helix nucleation window amino acids wide sheet nucleation site predicted sheet formers sheet breakers step algorithm resolving overlaps reason numbers table making formers breakers boolean properties chou fasman suggest conformation values regions compared resolve overlaps networks weighting links amino acids numbers table combination alanines produce higher activation aij input unit input bias output -infinity bias infinity input output activation init-helix unit combination phenylalanines chou-fasman algorithm continues predict -helix long predicate conthelix true rules define cont-helix terms helix-breaking rules helix continues long break region encountered -helix break region occurs helix-breaker amino acid immediately helix-breaker helix-indifferent amino acid helix broken encountering amino acid proline process extending -sheet structures works similarly algorithm predicts coil default table breaker values amino acids helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-former helix-breaker helix-breaker helix-breaker helix-breaker helix-breaker sheet-former sheet-former sheet-former sheet-former sheet-former sheet-former sheet-former sheet-former sheet-former sheet-former sheet-former sheet-breaker sheet-breaker sheet-breaker sheet-breaker sheet-breaker sheet-breaker sheet-breaker produced values tables reported chou fasman chou normalized values formers dividing conformation conformation weakest helix alanine helix conformation alanine conformation weakest helix phenylalanine breaker values work similarly calculate breaker multiplicative inverse conformation directly values chou fasman reasons wanted smaller values decrease number times strong helix-formers add similarly sheets breaker conformation values tend numbers stronger breakers close wanted breaker larger stronger breaker inverse breaker conformation restricting result exceed table chou-fasman algorithm expressed inference rules rules recognizing nucleation sites init-helix position helix amino acid position position helix breaker amino acid position init-sheet position sheet amino acid position position sheet breaker amino acid position rules pairs amino acids terminate helix structures helix-break helix-break helix-indiff break-helix helix-break helix-break break-helix helix-break helix-indiff rules pairs amino acids terminate sheet structures sheet-break sheet-break sheet-indiff break-sheet sheet-break sheet-break break-sheet sheet-break sheet-indiff rules continuing structures cont-helix break-helix cont-sheet break-sheet rules predicting helix nucleation propagating state helixi init-helix helixi helixicont-helix rules predicting sheet nucleation propagating state sheeti init-sheet sheeti sheeticont-sheet rules predicting coil default coili helixi break-helix coili sheeti break-sheet coili coili 
input units output unit hidden units direction forward propagation bias bias bpwt age low high med bias step initial unit bias step bias bias steps numbertrue returns number true antecedentsnumbertrue steps point location number sequence feat act processing node main data structures active partition produced feature category category category feature values categories active examples vector pos partition neg partition pos part xor act active examples feature number junction move south junction move south relational reinforcement learning sampling space first-order conjunctive features trevor walker twalker wisc jude shavlik shavlik wisc richard maclin rmaclin umn computer sciences department wisconsin-madison avenue madison usa abstract propose method reinforcement learning domains relational first-order features approach rapidly sample large space features selecting good subset basis q-function q-function created regression model combines collection first-order features single prediction control effect random predictions ensemble approach predictions generating multiple q-function models combining results models single prediction experiments technique interesting reinforcement learning problem keep-away subtask robocup suggest method learn effectively predict q-values challenging reinforcement-learning task introduction reinforcement-learning tasks involve domains inherently relational involving arbitrary numbers interacting objects range number blocks classical blocks-world problem number players robotic soccer match number interacting proteins regulatory network cell promising approach reinforcement learning domains represent situations arbitrarily sized set predicate-calculus facts engineering world description traditional fixed-length feature vector investigating learning algorithms approximate real-valued q-functions reinforcement learning world-state descriptions expressed predicate calculus appearing proceedings icml workshop relational reinforcement learning banff canada previously investigated approach dzeroski raedt blockeel accomplishing task relational regression trees kramer blockeel raedt approach partitions space state descriptions discrete regions assigns constant region future states falling region q-value estimated constant proposed approach built instance-based learning driessens ramon project investigating alternate approach stochastically sampling extremely large space conjunctions predicate-calculus facts color blue color red block blue block red block approach rapidly sample large space compound features selecting good subset basis q-function q-function weighted combination conjunctive features choose weights optimization methods perform regularized linear regression shanno rocke regularized kernel regression vapnik golowich smola gaussian kernel section present approach rapidly creating relational features creating q-function first-order features section present test domain keep-away subtask robocup stone sutton section presents initial results demonstrating proposed approach learn function present conclusions future directions work approach table shows algorithm generating q-function ensemble predictors stochastically generated first-order-logic features approach based srinivasan method stochastically generating random predicates search space srinivasan method defines set predicates created length presents simple method instantiating random member set predicates approach generate ensemble predictors q-value action predictor made weighted set randomly generated first-order logic features table algorithm creating q-function weighted ensemble predictors predictor made weighted set randomly generated order logic features parameters algorithm include number members ensemble number features predictor maximum number predicates feature minimum number differences feature truth values low high percentages examples covered accept feature training set state action q-value triples q-function learned distinct action create ensemble predictors repeating times select features predictor repeating times stochastically create feature consisting conjuncts predicates technique introduced srinivasan reject current feature return step feature covers small percentage large percentage states training set feature true training cases true training cases reject current feature return step feature significantly previous features predictor determined requiring feature compared features collected truth states training set apply feature states vector truth values element state feature feature vector differ previous feature vectors places accept current feature add current predictor build model input features regression method regularized linear regression kernel regression gaussian kernel predict q-value action state calculate q-value predictors ensemble action average middle predictions ten predictions predict average approach avoid outliers easily skew averaged prediction resulting predicted action compared action highest q-value chosen exploitation exploring reinforcement learners choose suboptimal actions hypothesis compared relational tree regression rapidly create q-functions accurate randomly sampling space predicate-calculus conjuncts previously shown classification tasks srinivasan weighted combinations complex features generated relational features network mofn subset test set error rate promoter domain training set testing set network mofn subset test set error rate splice-junction domain popescul advantage approach easy convert semi-incremental version model created step incrementally adjusted gradient descent training arrives periodically complete batch non-incremental learning run performed error predictions exceeds threshold alternatively batch learning continually parallel process runs background note algorithm table varied number ways step features generated local search based feature matches data effective feature approach similar explored zelezny srinivasan page explore methods building regression model input features expect family methods based algorithm produce fast effective learning methods task keep-away evaluate approach keep-away subtask simulated robocup soccer stone sutton task goal keepers ball takers long receiving reward time step ball keepers learn takers follow hand-coded policy figure keep-away game snapshot involving keepers takers simplify learning task learning occurs specific situation keeper holds ball player ball nearest keeper pursues ball perform moves open pass keeper holding ball keepers perform open move figure sample keep-away game progress keepers light gray black outlines takers black gray outlines ball held keeper upper game continues takers holds ball time steps ball bounds white lines learnable action choice hold ball pass keepers note passing steps require multiple actions simulation orienting player body performing multiple steps actual kicking low-level actions managed simulator addressed experiments policy takers fairly simple takers pursue ball takers pursue ball cover keepers keep-away task explored reinforcement-learning research stone sutton kuhlmann stone mooney shavlik interesting question intend explore relational approach make easy keeper behavior scale simpler problems keepers takers game complex game keepers takers testing goal initial testing demonstrate function accurately learned method discussed previous section focus predicting expected values set handcoded players playing keeper taker keep-away experiments demonstrate method learns predictive model accurate predictor values methodology generate training data evaluation record states actions values single agent call learner simulate exploration initial reinforcement learning learner random policy plays game smart keepers policy smart keepers discussed takers playing game follow policy discussed section learner receives input state predicates table provide background knowledge set derived predicates based facts state shown table hand-coded policy smart keepers attempts choose action perform rules taker fixed distance hold ball calculate widest passing lane keepers pass widest passing lane passing lanes width randomly choose action perform table state set predicates capturing player world keeper player keeper taker player taker dist dist game step distance player player step game pair players takers keepers dist ball dist game step distance player ball player controls ball player controls ball player position xpos game step position player field player position ypos game step position player field player dist top dist game step distance player top border player dist bottom dist game step distance player bottom border player dist left dist game step distance player left border player dist dist game step distance player border player angle 
ang game step angle players point view player viewing world pair players player view describes table set derived features describe state derived features derived base facts table teammate dist dist game step true distance player fixed dist calculate similar predicate test distance greater fixed dist thing opponents teammate dist range mind maxd game step true distance range mind maxd calculate similar predicate opponents teammate dist dist game step determines teammate closer dist calculate predicate teammate farther dist predicates check thing opponents calculate predicates test conditions true teammates opponents players dist greater dist game step player dist players border dist border dist game step true border dist calculate similar predicate true border greater distance border dist dist game step true border dist calculate similar predicate greater similar predicates check borders meet distance requirements angle btwn teammates ang game step true angle teammates angle ang calculate similar predicate greater check predicates opponents pair players angle btwn tmtes range mina maxa game step true angle teammates range mina maxa calculate similar predicate greater check predicates opponents pair players distances chosen angles degrees field size define passing lane width base isosceles triangle apex position passing player midpoint base position receiver takers contained area triangle widest passing lane lane widest base random learner agent smart agents standard takers generated hundred games training tuning testing sets explained game record state predicates table focus predicting log monte carlo estimate q-value focus logarithm noticed distribution log values closely approximates normal distribution order measure effect amounts data training divided training data groups games performed training games games work focus learning pass actions hold action passes interesting cases hold action generally change state situation player simply ball average passes game games training examples parameters determined part learning employ tuning-set methodology collected results additional games tune parameters estimate predictive accuracy experiments divide additional data groups games run group games tuning set games testing set run group games tuning set testing set note tuning sets means total size training sets experimentation training set tuning set experiments arbitrarily set number features number members ensemble maximum number predicates feature minimum number differences feature previously accepted feature low high percentage examples feature cover tuning set choose method combining first-order features prediction order select weights combine features predictor regularized kernel regression method mangasarian musicant method regression problem phrased support vector problem main parameters method penalize support-vector slack variables larger higher cost mis-predicting training points term determine leeway allowed precisely fitting point term global parameter specifies tube leeway learned solution training points fall tube penalized expression optimized problem phrased kernel problem investigate kernels producing final model number games training set ver baseline method experiments linear kernel gaussian kernel gaussian kernel parameter term indicating width gaussian investigated values parameter experiments kernels linear kernel gaussian kernels values kernels built models values values values suggested mangasarian musicant chose values ad-hoc manner combination parameter values kernels leads combinations tuning set games select model test set games test generalization testing measure closely learned model approximate actual values measured sample games determine interested error test set relates prediction reason calculate absolute errors normalize dividing average measurement predicting predicting item values predict times average error percentage difference obtained compared average predicted experiments calculate average percentage difference plot function amount training data baseline comparison compare simple baseline algorithm predicts action average training data action results figure shows results initial experiments simple baseline algorithm proposed method show small steady decrease error size training set grows takes minute average select set relational features prolog learn regularized regression model matlab future directions directions related approach plan pursue future plan pursue methods increase accuracy q-function estimates accomplish plan developing enhanced feature-selection algorithm figure normalized error results training proposed q-function learning method baseline method text results shown training set sizes training games tuning enhanced feature selection accomplished ways features generate non-redundant cover training examples improved generating features performing local greedy search features continually improve accuracy q-function estimate improved accuracy q-function estimate plan perform traditional q-learning relational algorithms plan test learning speed algorithm function number games played rewards received hope relational method outperforms traditional nonrelational methods terms training time received rewards size task number players team increases establishing viability q-learning determine algorithm scales complex problems due relational nature algorithm expect scale readily larger keep-away field sizes increased number keepers takers retraining hope move large fields smaller retraining plan empirically compare propositional learners approaches relational reinforcement learning decision trees instancebased methods test bed conclusions presented method reinforcement learning domains constructs first-order features describe state environment approach advantage learning scaled larger problems generalization occur tasks structural similarities approach involves rapidly sampling large space first-order features selecting good subset basis function q-function created producing regression model combines collection firstorder features single q-value prediction prevent extreme predictions resulting possibility generating skewed sets random features generate ensemble models combine predictions resulting models taking median prediction middle models experimenting technique interesting reinforcement learning problem keep-away subtask robocup shows small gain accuracy predicting values compared simple model indicating proposed method lead effectively reinforcement learning acknowledgments research partially supported darpa grant blockeel raedt top-down induction first-order logical decision trees artificial intelligence driessens ramon relational instancebased regression relational reinforcement learning proceedings international conference machine learning icml washington dzeroski raedt blockeel relational reinforcement learning proceedings international conference machine learning icml madison kramer structural regression trees proceedings national conference artificial intelligence aaai portland oregon kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer proceedings aaai workshop supervisory control learning adaptive systems san jose mangasarian musicant large scale kernel regression linear programming machine learning popescul unger lawrence pennock statistical relational learning document mining proceedings international conference data mining melbourne shanno rocke numerical methods robust regression linear models siam sci stat comput srinivasan study probabilistic methods searching large spaces ilp technical report prg-tr- oxford univ computing lab stone sutton scaling reinforcement learning robocup soccer proceedings international conference machine learning williams massachusetts vapnik golowich smola support vector method function approximation regression estimation signal processing mozer jordan petsche eds advances neural informationprocessing systems pages zelezny srinivasan page latticesearch runtime distributions heavy-tailed 
proceedings international conference inductive logic programming ilp sydney australia 

intelligent web agents learn retrieve extract information tina eliassi-rad jude shavlik center applied scientific computing lawrence livermore national laboratory box livermore usa eliassi llnl gov computer sciences department wisconsin-madison west dayton street madison usa shavlik wisc abstract describe systems machine learning methods retrieve extract textual information web present wisconsin adaptive web assistant wawa constructs web agent accepting user preferences form instructions adapting agent behavior encounters information approach enables wawa rapidly build instructable self-adaptive web agents information retrieval information extraction tasks wawa neural networks provide adaptive capabilities agents user-provided instructions compiled neural networks modified training examples users create training examples rating pages wawa retrieves importantly system techniques reinforcement learning internally create examples users provide additional instruction life agent empirical results domains show advantages approach keywords instructable adaptive software agents web mining machine learning neural networks information retrieval information extraction introduction rapid growth information world wide web boosted interest machine learning techniques solve problems retrieving extracting textual information web information-retrieval work author computer sciences department wisconsin-madison research supported part nlm grant nsf grant iriand vilas trust learners attempt model user preferences return web documents matching interests information-extraction learners attempt find patterns fill user-defined template questionnaire correct pieces information discuss noted learners chapter learners machine learning techniques ranging bayesian classifier syskill webert theory-refinement reinforcement learning wawa-ir breath investigated approaches learners basically falls categories systems hidden markov models systems relational learners iii systems theory-refinement techniques wawa-ie system wawa short wisconsin adaptive web assistant interacts user web build intelligent agent retrieving extracting information sub-systems information retrieval sub-system called wawa-ir information extraction sub-system called wawa-ie wawa-ir general search-engine agent trained produce specialized personalized agents wawa-ie general extractor system creates specialized agents extract pieces information documents domain interest wawa builds agents based ideas theory-refinement community machine learning user-provided domain knowledge compiled knowledge based neural networks prior knowledge refined training examples theory refinement find appealing middle ground nonadaptive agent programming languages systems solely learn user preferences training examples hand utilizing user prior knowledge enables wawa agents perform initially hand wawa agents learners rely user prior knowledge correct chapter organized present fundamental operations wawa agents section wawa information-retrieval system learners discussed section section present wawa information-extraction system notable systems section describes future directions finally section summarizes material chapter core wawa agents section briefly review fundamental operations wawa agent wawa-ir wawa-ie agents learning ability makes wawa agents arguably intelligent adapt behavior due users instructions feedback environments knowledge base wawa agent centered basic functions scorelink scorepage figure highly accurate functions standard heuristic search lead effective retrieval text documents best-scoring links traversed highest-scoring pages collected link score page score fig central functions wawa agents score web pages hyperlinks users tailor agent behavior providing advice functions advice compiled knowledge based neural networks implementing functions scorelink scorepage functions guide agent wandering web judge pages encountered subsequent reinforcements web encountering dead links ratings retrieved pages user wishes provide refine linkand page-scoring functions wawa agent scorepage network supervised learner learns user-provided training examples advice wawa agent scorelink network reinforcement learner network automatically creates training examples user-provided training examples advice design scorelink network important advantage producing self-tuning agents training examples created agent user-provided instructions mapped scorepage scorelink networks web-based language called advice expression advice language instruction basic form preconditions actions preconditions refer aspects contents structure web pages actions goodness page link preconditions met wawa extracts input features html plain-text web pages input features constitute primitives advice language combined logical numerical operators create complicated advice constructs table lists wawa extracted input features features anywhereonpage word anywhereintitle word word input return true word page inside title page wawa captures large number features sliding fixedsee eliassi-rad full description wawa input features size window page word time wawa defines features representing page respect current center sliding window isnthwordintitle word feature true word word left page title wawa bags words size sliding window capture instructions green bay packers show page input features related words positions page wawa agent input vector includes features length page date page created modified page server provide information sliding window inside emphasized html text number words title url words mentioned advice present title url table sample extracted input features anywhereonpage word anywhereintitle word isnthwordintitle word centerwordinwindow word numberofwordsintitle insideemphasizedtext timepagewaslastmodified key feature wawa advice language ability capture abstract concepts names variables understand variables wawa assume system create home-page finder give system good advice title page phrase firstname lastname home page show page leading question marks variables bound receiving request find specific person home page variables advice applied task finding home pages number people advice compiled scorepage scorelink networks variant kbann algorithm mapping process analogous compiling traditional program machine code system compiles advice rules intermediate language expressed neural networks important advantage machine code automatically refined based feedback provided user typically sliding window words web apply backpropagation algorithm learn training set illustrate mapping advice rule variables illustrates powerful phrase construct wawa advice language suppose advice rule phrase professor firstname lastname page show page advice compilation wawa maps phrase centering sliding window figure phrase sequence words maps positions input units sliding window variable firstname center sliding window score page bias true word left inwindow professor true word inwindow bound lastname true word centerinwindow bound firstname fig mapping advice scorepage network variables input units bound network units turned match bindings words sliding window bindings firstname joe lastname smith input unit true word centerinwindow bound firstname true set current word center window joe wawa connects referenced input units newly created sigmoidal hidden unit weights bias threshold hidden unit set required predicates true order weighted sum inputs exceed bias produce activation hidden unit additional zero-weighted links added hidden unit subsequent learning standard kbann finally wawa links hidden unit output unit weight determined rule action mapping advice rules variables process variable-binding step phrase sequence consecutive words retrieving information web information retrieval systems input set documents corpus query consisting bunch keywords keyphrases ultimate goal system return documents relevant query achieve goal learners attempt model user preferences return web documents matching interests section describes design creating specialized personalized intelligent agents retrieving information web experimental study system recently 
developed learners wawa-ir wawa system general search-engine agent training specialized personalized table high-level description wawa-ir table wawa information retrieval algorithm saved disk previous session create scorelink scorepage neural networks reading user initial advice start adding user-provided urls search queue initialize search queue urls query user chosen set web search engine sites execute concurrent processes independent process search queue empty maximum number urls visited urltovisit pop search queue fetch urltovisit evaluate urltovisit scorepage network score high insert urltovisit sorted list pages found score urltovisit improve predictions scorelink network evaluate hyperlinks urltovisit scorelink network score links session insert urls sorted search queue fit max-length bound independent process user additional advice insert neural network independent process user rates fetched page utilize rating create training scorepage neural network basic operation wawa-ir heuristic search scorelink network acting heuristic function solely finding goal node collect pages scorepage rates highest user choose seed queue pages fetch ways set starting urls provide simple query wawa-ir converts query urls query urls user-chosen subset selectable search engine sites altavista excite google hotbot infoseek lycos teoma yahoo ways train wawa-ir neural networks systemgenerated training examples advice user iii user-generated training examples fetching page wawa-ir predicts retrieving predicted based text surrounding hyperlink global information referring page title url fetching analyzing actual text wawa-ir re-estimates differences estimates score constitute error backpropagation improve scorelink neural network addition system-internal method automatically creating training examples user improve scorepage scorelink neural networks ways user provide additional advice observing agent behavior invoke thoughts good additional instructions wawa-ir agent accept advice augment neural networks time simply adds network additional hidden units represent compiled advice technique effectiveness demonstrated tasks providing additional hints rapidly drastically improve performance wawa-ir agent provided advice relevant maclin shavlik showed algorithm robust advice incrementally bad advice agent quickly learn ignore tedious user rate pages mechanism providing training examples backpropagation user unable articulate agent misscoring pages links standard methodology previously investigated researchers pazzani conjecture improvement wawa-ir neural networks scorepage result users providing advice personal experience easy simple advice require large number labeled examples order learn purely inductively words advice rule typically covers large number labeled examples rule file found avoid showing page cover pages phrase file found type training performed pages constitute initial search queue wawa-ir experimental study evaluate wawa system built home-page finder agent wawa advice language chose task building home-page finder existing system named ahoy valuable benchmark ahoy technique called dynamic sifting filters output web indices generates guesses urls promising candidates found home-page finder wrote simple interface layered top wawa-ir advice language variables wrote general advice rules related home-page finding slight variants middle names initials specializing wawa-ir task creating initial general advice day spent parts days tinkering advice examples training set describe step allowed manually refine advice process expect typical future users wawa-ir run experiments evaluate wawa-ir randomly selected people aha list machine learning case-based reasoning cbr researchers aic nrl navy mil aha people html table lists performance wawa-ir home-page finder results ahoy hotbot versions hotbot version performs engine specialized search people aha page queries hotbot version provide search engine general-purpose disjunctive query person required word variants person query wawa-ir initially sends search engines altavista excite infoseek lycos yahoo experiments pages hotbot returns assume people results returned search engine reporting percentage test set home-pages found report average ordinal position rank page found wawa-ir ahoy hotbot return sorted lists table empirical results wawa-ir ahoy hotbot system found rank page found wawa-ir advice rules ahoy hotbot person search hotbot general search results provide strong evidence version wawa-ir specialized home-page finder adding simple advice produces home-page complete list advice rules appears eliassi-rad eliassi-rad shavlik full description methodology results finder proprietary people-finder created hotbot ahoy difference percentage home-pages found wawa-ir hotbot experiment statistically significant confidence level difference wawa-ir ahoy statistically significant confidence level results illustrate build effective agent web-based task quickly experiments performed time google exist publicly compare system google ran experiments home page finder table compares performances wawa-ir home page finder seeded google results google run wawa-ir experiment seeded google search engines altavista excite infoseek lycos teoma experiments trained wawa-ir agent reinforcement learning supervised learning home-page finding advice rules table empirical results wawa-ir agents google system found rank variance page found wawa-ir google google wawa-ir google wawa-ir seeded google slightly improve google performance finding pages test set wawa-ir seeded google find home pages google aggregate search engines accurate google google appears good finding home pages due pagerank scoring function globally ranks web page based location web graph structure page content wawa-ir experiments seeded google advantage lower rank variance google attribute difference wawa-ir learning ability bump home pages top list finally set experiments show wawa-ir personalize search engines reorganizing results return searching nearby pages score high learners wawa syskill webert webwatcher web agents machine learning techniques bayesian classifier reinforcement learning tfidf hybrid learn interesting web pages hyperlinks unlike wawa systems unable accept refine advice simple provide lead learning rating manually visiting web pages drummond created system assists users browsing software libraries system learns unobtrusively observing users actions letizia system similar drummond lookahead search current location user web browser compared wawa drummond system letizia disadvantage advantage advice user webfoot system similar wawa html page-layout information divide web page segments text wawa segments create expressive advice language extract input features neural networks webfoot hand utilizes segments extract information web pages unlike wawa webfoot learns supervised learning cora domain-specific search engine computer science research papers wawa reinforcement-learning techniques efficiently spider web cora reinforcement learner trained off-line set documents hyperlinks enables q-function learned dynamic programming reward function state transition function wawa training hand on-line wawa temporal-difference methods evaluate reward hyperlink addition wawa reinforcement-learner automatically generates training examples accept refine user advice cora reinforcementlearner unable perform actions classify text cora naive bayes combination algorithm statistical technique shrinkage unlike wawa cora text classifier learns training examples accept refine advice extracting information web information extraction process pulling desired pieces information document author article building learners requires large number annotated examples expert provide sufficient correct knowledge domain interest requirements make time-consuming difficult build system similar case wawa 
theory-refinement mechanism build system wawa-ie theory refinement strike balance needing large number labeled examples complete correct set domain knowledge section describes wawa-ie experimental results wawa-ie recently developed learners wawa-ie wawa-ie takes advantage intuition inverse problems illustrate intuition assume access effective home-page finder takes input person annotated examples result tedious process reading training documents tagging extraction hand returns home page inverse system system takes home pages returns names people pages belong generate-and-test approach information extraction utilize essentially system address task generate step user specifies slots filled part-of-speech tags parse structures wawa-ie generates large list candidate extractions document entry list candidate extractions complete set slot fillers user-defined extraction template test step wawa-ie scores entry list candidate extractions candidates produce scores greater system-defined threshold returned extracted information critical component wawa-ie intelligent selector eliminates create exhaustive list candidate bindings step wawa-ie takes training generate fillers individual slot document fillers individual words phrases individual words collected brill tagger annotates word document part-of-speech tag slot collect word document pos tag matches tag assigned variable task advice cases variable phrase apply sentence analyzer called sundance document collect phrases match parse structure extraction slot generate subphrases consecutive words sundance performs crude shallow parsing point typically lengthy lists candidate fillers slot focus generating good combinations fill slots process combinatorially demanding reduce computational complexity wawa-ie methods called selectors creating complete assignments slots lists individual slot bindings wawa-ie selectors range suboptimal cheap simple random sampling individual list optimal expensive exhaustively producing combinations individual slot fillers heuristically inclined selectors wawa-ie modified walksat algorithm modified gsat algorithm hill-climbing algorithm random restarts statistically-oriented selector modified walksat algorithm build list combination-slots candidate extractions document randomly selecting item extraction slot list individual-slot candidates produces combinationslots candidate extraction candidate filler slot template score produced scorepage network high scale set variable bindings add combination list combination-slots candidates repeatedly randomly select slot template probability randomly select candidate selected slot add resulting combination-slots candidate list combination-slots candidates probability iterate candidates slot add candidate produces highest network score document list combinationslots candidates build wawa-ie agent user information set on-line documents information extracted extraction slots speaker names part-of-speech pos tags noun verb parse structures noun phrase verb phrase extraction slot set advice rules variables refer extraction slots set annotated examples training documents extraction slots marked hand case studies extract names proteins subcellular locations yeast database ray craven advice rules task phrase proteinname nphrase vphrase locationname nphrase appears document score high variables proteinname locationname represent protein names subcellular structures nphrase trailing variables required parse structure variables nphrase refers noun phrase vphrase matches verb phrase precondition rule matches phrases ubc localizes endoplasmic reticulum figure shows process building trained agent positive training examples provided domains generate negative training examples end run training documents candidate generator selector step heuristic candidate selector scores training extraction untrained scorepage network untrained network compiled initial advice training backpropagation labeled examples effect untrained scorepage network generated list informative negative examples misses due fact user-provided prior knowledge rates miss training extractions highly true extractions negative examples collected train scorepage neural network negative examples provided positive examples training network recognize produce high output score correct extraction context document advantage global layout information documents interest figure depicts steps trained agent takes produce extractions entry list combination-slots extraction candidates bind variables candidate values perform forward propagation trained scorepage network output score network test document based candidate bindings output network eliassi-rad complete details selectors user explicitly provide extraction slots pos tags separately advice extracted advice rules greater system-defined threshold record bindings extraction bindings discarded trained agent extractions prior knowledge training set list candidate extractions candidate generator selector slots syntactical info test set unseen docs trained network agent knowledge-base agent scorepage candidate generator selector list candidate extractions slots syntactical info fig trained agent sting trained agent articles single abstract doc tuple-le yeast wawa-ie experimental study valuate wawa-ie task involving cations cell yeast protei craven domain collectio yeast study fillers ultiple proteins ument single list protein locatio craven methodology experi vel method measuring accuracy otein location wawa-ie advice rules bnf bcellular location advice rules ata set mind half manually refine rules time information measures precisio recision ratio number umber fillers extracted recall training wawa-ie computes task-speci corepage network analyzing results tuning eliassi-rad experimental details traction n-localizati produced traction slots pairs ments thi tuple notation written write reca rrect fillers ratio fic threshold examples dom rotein nam domai buildin stracts biomedical epend locations tracted domain instance protein specifics ese rules -measure tracted total number correct output ains fillers extracted total number fillers correct extraction slots ideal system precision recall commonly -measure combines precision recall formula precision recall precision recall -measure versatile precision recall explaining relative performance systems takes account inherent tradeoff exists precision recall ray craven split yeast data set disjoint sets ran -fold cross-validation experiments folds wawa-ie compare results figure illustrates difference test-set measure modified walksat selector exhaustive candidate selector negative exampls horizontal axis depicts percentage negative training examples learning process vertical axis depicts -measure trained ie-agent test set wawa-ie achieve good performance negative training candidates demonstrates intelligently select good training examples reduce training time negative training candidates trained selector trained wsat -measure fig -measure test set percentage negative training candidates selector algorithms -measures wawa-ie trained agents outperform untrained agents approximately results shown demonstrates wawa-ie refine initial advice figure shows precision recall curves wawa-ie trained agent modified walksat selector negative examples wawa-ie trained agent selector negative training examples system ray craven precision recall trained agent wsat ray craven trained agent selector fig precision recall curves trained agent selector algorithm produces results computationally expensive cross-product entries lists individual-slot candidates trained agent modified walksat selector performs outperforming ray craven system results illustrate theory refinement justify intelligent candidate-selection algorithm reduce computational burden approach computationally demanding generate-and-test strategy wawa-ie modified walksat selector improve state art negative training candidates training finally recall variant walksat testing figure shows obtain good precision recall needing exhaustively score candidate learners find system literature applies theory refinement problem feldman system takes set approximate rules training examples incrementally revise inaccuracies initial rules revision algorithm heuristics find place type revision 
performed unlike wawa-ie advice rules rules provide advice refine existing rules system manipulates rules directly wawa-ie compiles rules neural networks standard neural training refine rules finally approach suggest revisions human user wawa-ie approach make revisions automatically systems break groups group kind relational learning learn extraction patterns group learns parameters hidden markov models hmms hmms extract information recently freitag kushmerick combined wrapper induction techniques adaboost algorithm create extraction system named bwi short boosted wrapper induction system out-performed relational learners competitive systems hmms wawa-ie leek hmms extracting information biomedical text system lot initial knowledge build hmm model training data learn parameters hmm system refine knowledge authors statistical methods reduce lot training examples freitag mccallum hmms extract information text employ statistical technique called shrinkage problem sufficient labeled examples seymore hmms extract information on-line text problem sufficient training data data labeled purpose system similarly craven kumlien weakly labeled training data reduce labeled training examples advantage system utilize prior knowledge reduces large number labeled training examples depend initial knowledge correct easy users articulate domain-specific advice user-friendly interface provided converts advice specifics wawa advice language advantage system entire content document estimate correctness candidate extraction learn extraction slots documents advantage wawa-ie utilize untrained scorepage network produce informative negative training examples misses future directions order understand people instructable web agent wawa improve advice language build personalized easily customized intelligent web agents embed wawa major existing web browser minimizing interface features users learn order interact system related develop methods wawa automatically infer plausible training examples observing users normal browsers domains incorporate candidate generation selection steps directly connectionist framework current scorepage network find candidate extractions training process finally interesting area research explore theory-refinement techniques supervised learning algorithms support vector machines hmms relational learners conclusion argue promising create intelligent agents involve user ability direct programming provide approximately correct instructions sort agent ability accept automatically create training examples due largely unstructured nature size web hybrid approach appealing solely based non-adaptive agent programming languages users rate mark desired extractions large number web pages wawa utilizes user knowledge task hand build agents retrieve extract information important characteristics wawa agents ability receive instructions refine knowledge-bases learning instructions provided user perfectly correct ability receive user advice continually iii ability create informative training examples present evaluate wawa information-retrieval system appealing approach creating personalized information-finding agents web central aspect design machine learner core users create specialized agents articulating interests advice language wawa-ir compiles instructions neural networks allowing subsequent refinement system creates training examples reinforcement learning supervised training user rate information wawa-ir agent finds process continuous learning makes wawa-ir agents adaptive home-page finder case study demonstrates build effective agent webbased task quickly describe evaluate system theory refinement perform information extraction wawa information-extraction system neural network accepts advice variables rate candidate variable bindings content document extraction process generates large set candidate variable bindings slot selects subset slot bindings heuristic search finally trained network judge bindings score higher system-computed threshold returned extracted information theory refinement advantage prior knowledge domain interest produce informative training examples lead increase performance agent experiments yeast protein-localization domain illustrates compete state-ofthe-systems empirically show benefits intelligent algorithms selecting candidates multiple slots briefly reviewed approaches tasks based machine learning techniques systems including demonstrate promise machine learning make sense vast resourses world-wide web aho sethi ullman compilers principles techniques tools addison wesley bikel schwartz weischedel algorithm learns machine learning special issue natural language learning brill advances rule-based part speech tagging proc aaaiconference brin page anatomy large-scale hypertextual web search engine computer networks isdn systems califf relational learning techniques natural language information extraction thesis department computer sciences texas austin craven kumlien constructing biological knowledge-bases extracting information text sources proc ismbcristianini shawe-taylor introduction support vector machines kernel-based learning methods cambridge press dempster laird rubin maximum likelihood incomplete data algorithm journal royal statistical society drummond ionescu holte learning agent assists browsing software libraries technical report tr- ottawa ottawa canada eliassi-rad building intelligent agents learn retrieve extract information thesis computer sciences department wisconsin madison eliassi-rad shavlik system building intelligent agents learn retrieve extract information appears international journal user modeling user-adapted interaction special issue user modeling intelligent agents eliassi-rad shavlik theory-refinement approach information extraction proc icmlconference feldman liberzon rosenfeld schler stoppi framework explicit bias revision approximate information extraction rules proc kddconference freitag machine learning information extraction informal domains thesis computer science department carnegie mellon pittsburgh freitag mccallum information extraction hmms shrinkage workshop notes aaaiconference machine learning information extraction freitag kushmerick boosted wrapper induction proc aaaiconference goecks shavlik learning users interests unobtrusively observing normal behavior proc iuijoachims freitag mitchell webwatcher tour guide world wide web proc ijcaiconference kushmerick wrapper induction efficiency expressiveness artificial intelligence leek information extraction hidden markov models masters thesis department computer science engineering california san diego lieberman letzia agent assists web browsing proc ijcaiconference mccallum rosenfeld mitchell improving text classification shrinkage hierarchy classes proc icmlconference mccallum nigam comparison event models naive bayes text classification workshop notes aaaiconference learning text categorization mccallum nigam rennie seymore building domainspecific search engines machine learning techniques aaaispring symposium stanford maclin shavlik creating advice-taking reinforcement learners machine learning mitchell machine learning mcgraw-hill national library medicine medline database http ncbi nlm nih gov pubmed ourston mooney theory refinement combining analytical empirical methods artificial intelligence pazzani kibler utility knowledge inductive learning machine learning pazzani muramatsu billsus syskill webert identifying interesting web sites proc aaaiconference ray craven representing sentence structure hidden markov models information extraction proc ijcaiconference rennie mccallum reinforcement learning spider web efficiently proc icmlconference riloff sundance sentence analyzer http utah projects nlp rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition vol mit press russell norvig artificial intelligence modern approach prentice hall schapire singer improved boosting algorithms confidence-rated predictions proc coltconference selman kautz cohen local search strategies satisfiability testing dimacs series discrete mathematics theoretical seymore mccallum rosenfeld learning hidden markov model structure information extraction workshop notes aaaiconference machine learning information extraction shakes langheinrich etzioni dynamic sifting case stury homepage domain proc wwwconference shavlik eliassi-rad intelligent agents web-based tasks advice-taking approach workshop notes aaaiconference learning text categorization madison shavlik calcari eliassi-rad solock instructable 
adaptive interface discovering monitoring information worldwide web proc iuiconference soderland learning extract text-based information world wide web proc kddconference soderland learning information extraction rules semistructured free text machine learning special issue natural language learning sutton barto reinforcement learning mit press towell shavlik knowledge-based artificial neural networks artificial intelligence van rijsbergen information retrieval buttersworths edition yang evaluation statistical approaches text categorization journal information retrieval 

appears aaaiworkshop supervisory control learning adaptive systems guiding reinforcement learner natural language advice initial results robocup soccer gregory kuhlmann peter stone raymond mooney department computer sciences texas austin fkuhlmann pstone mooneyg utexas jude shavlik department computer sciences wisconsin madison shavlik wisc abstract describe current efforts creating reinforcement learner learns reinforcements provided environment human-generated advice research involves complementary components mapping advice expressed english formal advice language advice expressed formal notation reinforcement learner subtask challenging robocup simulated soccer task noda testbed introduction reinforcement learning common create adaptive systems learn act complex dynamic environments sutton barto learner repeatedly senses world chooses action perform occasionally receives feedback environment learner improve performance employing effective create intelligent robots software agents writing programs hand addition requires human intervention supervised machine learning requires large sets labeled training examples describe current efforts creating reinforcement learner learns reinforcements provided environment human-written suggestions fact goal suggestions provided ordinary english typically feedback reinforcement learner simply numeric representation rewards punishments feedback human teacher learner provide researchers designed successful methods feedback include high-level advice expressed humans natural level abstraction statements formal language noelle cottrell maclin shavlik siegelmann eliassi-rad shavlik agent employs machine learning advice perfectly accurate fully precise completely speci approaches rl-agent human partner provide advice time based agent current behavior advice suggest action immedicopyright american association arti cial intelligence aaai rights reserved ately clouse utgoff explicit conditions action henceforth case paper good advice rapidly improve agent performance agent recover quickly poor advice maclin natural language convenient ordinary users advice methods learn map natural formal language training examples desired transformation automate construction nl-advice interpreter learning methods system automatically adapt language speci users mooney research group developing successful methods learning translate formal semantic representations zelle mooney thompson mooney tang mooney mapping instructions expressed ordinary english formal representation aim alleviate sizable burden explicitly supplying formal representations advice remainder article describe recent research involves complementary components mapping advice expressed english formal advice language advice expressed formal notation reinforcement learner subtask challenging robocup simulated soccer task noda testbed describing technical approaches present subtask robocup soccer keepaway task robocup soccer keepaway subproblem robocup simulated soccer introduced stone sutton stone sutton team keepers maintain possession ball limited region opposing team takers attempts gain possession takers possession ball leaves region episode ends players reset episode keepers possession ball parameters task include size region number keepers number takers figure shows screen shot episode keepers takers called short playing region flash les illustrating task http utexas austinvilla sim keepaway boundary keepers takers ball figure screen shot middle keepaway episode region flash les illustrating task http utexas austinvilla sim keepaway agents robocup simulator noda receive visual perceptions msec indicating relative distance angle visible objects world ball agents execute primitive parameterized action turn angle dash power kick power angle msec agents sense act asynchronously random noise injected sensations actions individual agents controlled separate processes inter-agent communication permitted simulator enforces communication bandwidth range constraints full details robocup simulator presented server manual chen work focus exclusively training keepers incorporating domain knowledge learners choose simulator set primitive actions higher-level actions constructed set basic skills implemented cmunitedteam stone riley veloso keepers freedom decide action possession ball keeper possession hold ball pass teammates keepers possession ball required select receive option player soonest ball remaining players open pass incorporate domain knowledge providing keepers rotationally-invariant state features computed world state keepers set state variables computed based positions keepers takers ordered increasing distance center playing region dist distance ang angle vertex keepers takers state variables dist dist dist dist dist dist dist dist dist min dist dist min dist dist min ang ang min ang ang purposes behavior takers hard-wired simple takers soonest ball remaining takers block open passing lanes obvious performance measure task average episode duration keepers attempt maximize takers minimize end keepers constant positive reward time step episode persists full details task learning scenario stone sutton natural language interface allowing human teachers provide advice natural language instruct learning agent master complex formal advice language approach parser automatically translates natural-language instructions underlying formal language domain statements formal language uence action policy learned agent robocup coach competition teams compete provide effective instructions coachable team simulated soccer domain coaching information provided formal language called clang coach language chen constructing english translations clang statements produced teams robocup coach competition produced corpus training testing natural-language interface sample annotated statements corpus player ball pass ball player bowner pass pass goalie bowner dont pass players ball shoot pass players bowner shoot dont pass suf ciently restricted task robocup coaching parsing natural-language sentences formal representations manageable task current nlp technology jurafsky martin developing parser labor-intensive software-engineering project methods learn map natural language formal language input output pairs signi cantly automate dif cult development process previously developed methods learning translate natural-language sentences formal semantic representations developed integrated systems chill zelle mooney learns parser mapping natural-language sentences directly logical form wolfie thompson mooney learns lexicon word meanings required parser adapting chill wolfie learn map english clang exploring approaches semantic parsing rst approach pattern-based transformation rules map phrases natural language directly clang approach rst statistical parser charniak produce syntactic parse tree pattern-based transformation rules map subtrees parse clang expressions approach parser integrates syntactic semantic analysis pairing production syntactic grammar compositional semantic function produces semantic form phrase semantic forms subphrases norvig lack space elaborate rst approaches simplest method clang formal grammar nes language production rules action pass unum set condition bowner unum set unum set non-terminal symbol set uniform numbers translator transformation rules map natural-language phrase patterns subtrees parse tree clang representation transformation rules repeatedly applied construct parse tree clang representation bottom-up non-terminal symbols clang grammar provide convenient intermediate representations write general effective transformation rules sentence player ball player pass player transformation rule player ball condition bowner fng rewrites sentence condition bowner player pass player pass player action pass fng rewrites sentence condition bowner player action pass player action directive fng action rewrites result condition bowner directive pass finally condition directive rule condition directive produces nal clang result rule bowner pass developed learning system called silt automatically induces transformation rules manually annotated sentences 
kate current fold cross-validation experiments clang corpus sentences training independent testing demonstrate learned parser produces completely correct translations sentences correct translations sentences giving advice keeper previously stone sutton achieved successful learning results keepaway domain reinforcement learning algorithm called episodic smdp sarsa sutton barto represented state work linear tile-coding function approximation cmacs albus reinforcement learning methods work function approximator takes keeper set state variables input produces values actions hold pass teammate free variables function approximator feature weights adjusted action approximates expected episode duration learner select action values determine keeper action keeper typically selecting action highest occasionally reinforcement learners exploratory actions actions highest order incorporate advice add component learner called advice unit function approximator advice unit generates values actions current world state output advice unit learned determined advice supplied user stated previously system advice rules represented standardized coach language clang clang rules consist conditions rule triggered directives action player conditions composed high-level predicates world state ball player clang includes ability refer basic skills pass dribble shoot input advice unit set features world state suf cient evaluate clang conditions initially action values set time step action chosen conditions rules supplied advice unit matched current world state rule res lhs satis action increased decreased constant amount depending directive advises action values generated advice unit added generated function approximator presented learning algorithm fullsoccer state actionvalues advice unit state variables cmac function approximator keeper sum figure pictorial summary complete advice integration scheme nice things advice incorporation method require function approximator advice unit set features describe world time advice adjusted learner adjusting function approximator advised action agent effectively unlearn advice experiments advice giving conducted series preliminary experiments measure impact advice quality learned policies speed policies converge experiment add single piece advice beginning learning trial advice applies member keeper team remains active duration experiment sample pieces advice created pieces sample advice test system advice extensively engineered maximize performance simply collection rst thoughts made observing players training recognizing shortcomings rst piece advice hold advice states player possession ball choose hold ball pass opponents observed early learning keepers tend pass reasonable advise pass takers threat figure illustrates advice keepers represented darklylled circles takers represented lightlylled circles give sense advice represented syntax clang clang rule hold advice definerule hold-advice direc ppos opp arc hold rule describes circular region shown dashed figure centered keepers radius figure hold advice opponents mthen hold states opponents region keeper advised perform hold action piece advice call quadrant advice figure play region divided quadrants player advised pass teammate teammate quadrant quadrant opponents advice aims encourage players pass teammates defended figure quadrant advice pass teammate quadrant opponents lane advice figure instructs players pass teammate passing lane teammate open passing lane ned isosceles triangular region apex position player ball base midpoint position intended pass recipient lane open opponents inside region purpose advice encourage passes succeed figure lane advice pass teammate opponents passing lane nal piece advice experiments edge advice differs previous advice advises action shown figure rule nes edge regions sides playing eld regions wide player advised pass teammate players edge region goal advice discourage passes edges play region high probability bounds due noise simulator figure edge advice pass edges play region empirical results advice piece advice tested ran learning trials starting random initial state learner function approximator results compared learning trials advice learning trial measure average episode duration time episodes averaged -episode sliding window plot learning curves trial graph give sense variance results shown figures experiments keepers played takers eld default robocup simulator players objects view cone shown previously learning method paper work condition kuhlmann stone work simpli problem giving players simpli cation ensures conditions advice unit accurately evaluated episode duration seconds training time hours hold advice advice episode duration seconds episode duration seconds episode duration seconds episode duration seconds figure learning curves comparing hold advice advice clear figure hold advice helpful learning advice consistently outperforms learning surprising players learn faster result similarly results quadrant advice shown figure demonstrate advice speeding learning helps learners perform episode duration seconds training time hours quadrant advice advice episode duration seconds episode duration seconds episode duration seconds episode duration seconds figure learning curves comparing quadrant advice advice episode duration seconds training time hours lane advice advice episode duration seconds episode duration seconds episode duration seconds episode duration seconds figure learning curves comparing lane advice advice figure shows lane advice helpful performance improvement dramatic previous cases finally figure learners edge advice consistently bene cial appears learning trial keepers bene advice additional experiments establishing kinds advice bene cial isolation started exploring possibility combining advice ran experiments pieces advice active time typically learners performed advice results good learned advice activated individually explanation result situations pieces advice recommend actions triggered time effectively cancel additional work needed fully understand resolve dif cult issue plan continue explore ways combine advice achieve desired additive effect reported work maclin conclusion future work allowing humans provide high-level advice software assistants valuable improve dialog humans software metaphor giving commands computers episode duration seconds training time hours edge advice advice episode duration seconds episode duration seconds episode duration seconds episode duration seconds figure learning curves comparing edge advice advice giving advice accept adapt discard advice advice-taking systems potential radically change interact robots software agents empirically investigated idea giving advice adaptive agent learns advice effectively show simple intuitive advice substantially improve state-of-the-art reinforcement learner challenging dynamic task pieces advice shown improve performance robocup keepaway task plan continue extending work advisable reinforcement learning cover complete simulated robocup task investigating additional ways mapping english statements formal advice alternate approaches advice reinforcement learners developing multiple approaches automatically learning translate natural-language semantic representations evaluate assembled english clang corpus extending advice cmac-based learner modifying weights cmac directly changing learner exploration function give higher consideration advised actions investigating knowledge-based support vector machines fung mangasarian shavlik instructable agents relational learning methods dzeroski raedt driessens acknowledgements ruifang rohit kate yuk wah wong contributing work natural-language understanding research supported darpa grant nsf career award iisreferences albus brains behavior robotics peterborough byte books charniak maximum-entropy-inspired parser proceedings meeting north american association computational linguistics chen 
foroughi heintz kapetanakis kostiadis kummeneje noda obst riley steffens wang yin users manual robocup soccer server manual soccer server version http sourceforge net projects sserver clouse utgoff teaching method reinforcement learning proceedings ninth international conference machine learning dzeroski raedt driessens relational reinforcement learning machine learning eliassi-rad shavlik system building intelligent agents learn retrieve extract information international journal user modeling user-adapted interaction special issue user modeling intelligent agents fung mangasarian shavlik knowledge-based support vector machine classi ers advances neural information processing systems mit press jurafsky martin speech language processing introduction natural language processing computational linguistics speech recognition upper saddle river prentice hall kate wong mooney learning transformation rules semantic parsing review http utexas users html kuhlmann stone progress learning keepaway polani browning bonarini yoshida eds robocuprobot soccer world cup vii berlin springer verlag maclin shavlik incorporating advice agents learn reinforcements proceedings twelfth national conference arti cial intelligence maclin learning instruction experience methods incorporating procedural domain theories knowledge-based neural networks dissertation computer sciences department wisconsin madison noda matsubara hiraki frank soccer server tool research multiagent systems applied arti cial intelligence noelle cottrell instructable connectionist systems sun bookman eds computational architectures integrating neural symbolic processes boston kluwer academic norvig paradigms arti cial intelligence programming case studies common lisp san mateo morgan kaufmann siegelmann neural programming language proceedings twelfth national conference arti cial intelligence stone sutton scaling reinforcement learning robocup soccer proceedings eighteenth international conference machine learning morgan kaufmann san francisco stone riley veloso cmunitedchampion simulator team veloso pagello kitano eds robocuprobot soccer world cup iii berlin springer sutton barto reinforcement learning introduction cambridge mit press tang mooney multiple clause constructors inductive logic programming semantic parsing proceedings european conference machine learning thompson mooney acquiring wordmeaning mappings natural language interfaces journal arti cial intelligence research zelle mooney learning parse database queries inductive logic programming proceedings thirteenth national conference arti cial intelligence 

detection rate based consensus volcanos false alarm rate based consensus volcanos anns jartool-plannett jartool-gauss scientist labeling scientist labeling scientist labeling scientist labeling detection rate based consensus volcanos false alarm rate based consensus volcanos anns jartool-plannett reps hus anns pcc -highres hus anns pcc -medres hus anns pcc -lowres hus anns pcc -medres-petal hus anns pcc -allres-petal hus anns fft-feats hus anns all-but-fft hus anns all-feats hus scientist labeling scientist labeling scientist labeling scientist labeling detection rate based consensus volcanos false alarm rate based consensus volcanos anns all-but-fft hus ann all-but-fft hus ann all-but-fft hus ann all-but-fft hus ann all-but-fft hus scientist labeling scientist labeling scientist labeling scientist labeling detection rate based consensus volcanos false alarm rate based consensus volcanos anns pcc -medres-petal hus anns pcc -medres hus scientist labeling scientist labeling scientist labeling scientist labeling detection rate based consensus volcanos false alarm rate based consensus volcanos anns pcc -medres-petal hus jartool-gauss scientist labeling scientist labeling scientist labeling scientist labeling appears working notes icml workshop machine learning bioinformatics august pictorial structures identify proteins x-ray crystallographic electron density maps frank dimaio dimaio wisc jude shavlik shavlik wisc george phillips phillips biochem wisc dept computer sciences dept biostatistics medical informatics dept biochemistry wisconsin madison abstract time-consuming steps determining protein structure x-ray crystallography interpretation electron density map viewed computer-vision problem density map simply three-dimensional image protein due intractably large space conformations protein adopt building protein model match density map extremely difficult paper describes pictorial structures build flexible protein model protein amino acid sequence pictorial structure representing object collection parts connected pairwise deformable springs model parameters learned training data efficient algorithm match model density map probable arrangement protein atoms found reasonable running time test algorithm tasks amino-acid sidechain-refinement task location protein backbone approximately algorithm places remaining atoms region density accurately placing atoms actual location determined crystallographer task classification task algorithm predict type amino acid contained unknown region density task algorithm accurate discriminating amino acids introduction background fundamental problem molecular biology involves determination protein shape protein folding protein sequence determines shape issue important knowing protein structure great insight mechanisms protein involved knowledge mechanisms needed disease treatment drug development additionally knowing structure protein biologists step closer holy grail direct mapping sequence structure current state structural biology algorithm exists accurately maps sequence structure forced wet laboratory methods elucidate structure proteins common technique today determining structure proteins x-ray crystallography complex experimental technique molecular-scale visualization process time-consuming requires number steps protein structure determined large research effort recently put high-throughput structural genomics speeding tedious process increasingly important work speed process x-ray crystallography automating interpretation electron density map taking three-dimensional image protein finding atomic coordinates process x-ray crystallography determine protein structure complex protein produced significant quantities purified crystallized crystal x-ray beam diffraction pattern x-rays crystal collected processed producing protein electron density map map three-dimensional snapshot protein map details actual structure protein large unwieldy format useless make data usable biologists core information protein atomic coordinates extracted final step crystallographic process interpreting map building molecular model protein crystals prepared map interpretation time-consuming step subsequent analysis proteins interpretation performed manually number attempts made automated density map interpretation amino-acid sequence protein advance complete topology protein searching main difficulty interpretation large number conformations protein adopt similarity features map atom appears gaussian blob distinction atoms elements fact protein conformation involve free rotation single bond leads extremely large conformation space makes building model protein match density map extremely difficult describe computational framework building flexible model protein protein sequence electron density map overview algorithm shown table pictorial structures representation object collection parts linked pairwise deformable spring-like connections connection defines relationship parts connects building atomic model connections correspond covalent bonds relationships define maintain bond invariants interatomic distance bond angles allowing bond variable torsion angle freely vary model model arbitrary-sized protein fragments recent dynamic programming-based matching algorithm felzenszwalb huttenlocher referred felz-hut pictorial structures quickly matched two-dimensional image matching algorithm finds globally optimal position orientation part pictorial structure making simplifying assumptions independence parts connections simple face pictorial structure shown figure pictorial structure model parts pairwise connections form graph evg vvv set parts edge connects neighboring parts amino-acid sequence protein seq electron density map densitymap predict atomic coordinates protein density map algorithm build pictorial structure sequence seq bestmatch run felz-hut algo find match densitymap illegal structure bestmatch bestmatch run soft-max algo heuristically find non-optimal match return bestmatch table high level outline algorithm figure sample pictorial structure tures locating face image collection parts includes pic shown dotted lines statistical constraints pairs parts explicit dependency configuration parts configu vration part ition bability lobally probable configuration part image assuming independence parts matching function ignoring occlusion itch consists part pos orientation felz -hut matching algorithm treats graph markov random field mrf pro part configuration conditionally independent part model configuration part neighbors graph edge deformation cost probabilistically negative log likelihood configuration pair parts model similarly edge matching cost negative log likelihood probability part configuration image matching algorithm finds markov random field assumption independence edges algorithm simply aims find configuration parts model image minimize lplipilp jiij lld match notice monotonicity equivalent minimizing evv jiij lldl match felz-hut matching algorithm places additional limitations topology markov random field form deformation cost function mrf defined pictorial structure tree structured cyclic constraints allowed deformation cost function form felzhut quickly computes minimization step algorithm distance transform space llll ttd jiji jiijij preceding arbitrary functions norm distance metric conditions atching algorithm find globally optimal structures faces general scenes waterfalls configuration parts respect numb configurations considered num two-dimensional images pictorial mountains cars bodies felzenszwalb huttenlocher paper discusses broad classes connections flexible revolute joints prismatic joints provide felz-hut een recognizing linear tim ber objects locate definitions classes connection functions building general-purpose molecule recognition framework construct class connection screw-joint unlike previous work type connection relates objects three-dimensional space screw-joint based rotation covalent bond free rotation arou single axis building lexible atomic model vision three-dimensional image large molecule topology amino-acid sequence molecule find molecular model prior probability configuration molecule ideally deformation cost function interactions build pictorial structure model part model low-cost rotation bonds steeply penalizing rotation translation order introduce broad class cted version mrf mrf constrained fast matching algorithm tree task basically problem computer protein atom 
appears proceedings ijcaiworkshop machine learning information filtering automatically labeling web pages based normal user actions jeremy goecks jude shavlik computer sciences dept wisconsin dayton street madison computer science dept wisconsin dayton street madison jeremy goecks undergraduate computer science major wisconsin jude shavlik partially supported nsf grant iriabstract agents attempting learn user interests cost obtaining labeled training instances prohibitive user directly label training instance users present approach circumvents human-labeled pages learn surrogate tasks desired output easily measured number hyperlinks clicked page amount scrolling performed assumption outputs highly correlate user interests words unobtrusively observing user behavior learn functions agent silently observe user browser behavior day training examples learn functions gather middle night pages interest user previous work focused learning user profile passively observing hyperlinks clicked passed extend approach measuring user mouse scrolling activity addition user browsing activity present empirical results demonstrate agent accurately predict easily measured aspects browser introduction research devoted task developing intelligent agents learn user interests profile find information world wide web based profiles joachims pazzani web page hyperlink user agent task predict user interest level page hyperlink agent predicts user interested page agent retrieve page viewing user central question topic learning user profiles concerns learning algorithm obtains training examples expecting user manually rate pages desirable previous work liberman mladenic investigated employing passively recorded user navigation behavior analyzing hyperlinks clicked hyperlinks passed patterns user behavior surrogate measures predicting user interest page extend work unobtrusively recording user navigation behavior additional user actions scrolling utilize combination measurement surrogate user interest page hypothesize tested measurements correlate user true interests collect automatically generated training examples user normal web browser approach collection large training sets burdening user demonstrate paper feasible learn promising surrogate measurements illustrate agent generate labeled instances normal user actions imagine agent watching user interested financial information internet bookstores user navigates news article stock prices future prospects amazon barnes noble scroll page reading article click related links finishing article observing user performed large number actions page agent label page positive instance user interests conversely navigating article compares book selection book reviews amazon barnes noble user read article scrolling click related links page observing number actions performed user agent label web page negative user interests direct input user agent generate labeled instances normal user actions obtain labeled training examples costly user discuss effective learning profile user section discuss agent obtains training examples generalizes examples sections discuss methodology results crossvalidation study agent sections discuss extensions research relation previous research finally present conclusions agent architecture section discuss agent architecture browser agent microsoft internet explorer made choice internet explorer opposed browser internet client sdk microsoft common object model provide hooks program observe record measure variety user actions step user visits web page record html contents page normal actions performed user page specifically html text page input instance normal user actions output training recording html text page straightforward operation normal user actions recorded agent user actions correlate user interest page include number hyperlinks clicked user amount scrolling user performed user bookmarked page fact agent record number hyperlinks clicked user page agent directly utilize actions reasons technology limitations prevent agent obtaining accurate measure amount scrolling user bookmarking page action highly correlated user interest appears page proceedings rare event national science foundation workshop measuring generation data action mining nov baltimore sufficient relational agent data mining reliably label inductive web logic page programming link interesting discovery uninteresting raymond mooney addition prem melville recording lappoon number rupert tang links department clicked computer sciences user agent texas records austin level activity mooney melville normal rupert actions utexas give jude agent additional shavlik insight castro level dutra user david interest page tor page santos costa action department serves biostatistics surrogate measure amount scrolling performed user page similarly action serves surrogate measure mouse activity page surrogate measure amount scrolling performed user number command-state page command-state change internal event event model occurs user resizes window utilizes edit menu scrolls vast majority command-state result user scrolling command-state serve reliable surrogate amount user-scrolling activity similarly status-bar-text serve surrogate mouse activity status-bar-text occur user moves mouse hyperlink url hyperlink displayed status bar navigates menu description command pointed mouse displayed status bar status bar text correlate mouse activity simplicity refer surrogates action measures refer agent measure user scrolling mouse activity user navigates page agents records text html text number hyperlinks user clicked amount user scrolling activity amount user mouse activity creates input constitute outputs training examples user returns page previously visited recently logical agent add user actions recorded return visit user actions recorded previous visit makes sense user navigates back-and-forth pages single start page user interested start page creating training page time recording user actions visit capture user true interest page situation agent sums actions user page visited finite period time active time instance finite period time user actions page summed agent active time twenty minutes future work address means determine optimal active time user user returns page active time instance page created actions instance active time employing concept instance active time agent attempts capture user true interest page user performs actions page visit user revisits page times performs actions page visit step build labeled training instances information recorded step train instances representation information collected agent step method learning employed agent highly interdependent discuss agent input vector representation learning algorithm step agent fundamental goal html text web page predict amount normal user actions page words learning algorithm agent successfully predict number hyperlinks user click amount scrolling action user protein determine cartesian coordinates center atom method describe attempts find atomic coordinates building pictorial structure protein felz-hut matching algorithm find probable location atoms protein focus efforts deformation cost molecule inverse atomic potential function molecule configurations lower potential energy probable atomic potential function complicated expression takes account bonded non-bonded interactions expression roughly approximated sum pairwise potentials impossible manner maintains tree-structured mrf fast matching algorithm requires alternatively create simplified atomic model ignoring non-bonde corresponds atom protein connection corresponds covalent bond knowing protein sequence advance makes construction model trivial matching model configuration assign part consists parameters translational rotational rotational parameters euler angles ref rotation z-axis rotation x-axis rotation z-axis simplified model connection screw-joint rotating screw screw joint rotation single axis order simplify cost function specification axis rotation z-axis matching algorithm considers orientations part limit model requires parts model rotated canonic orientation axis rotation z-axis atomic model 
defining relationship parts involves dire structure arbitrary root chosen directed graph mrf constructed edge points tree root node edge concerns relationship parent child deformation cost edge defined terms medical informatics parameters stored department computer sciences wisconsin madison shavlik dpage wisc dutra vitor biostat wisc abstract link discovery important task data mining counter-terrorism focus darpa evidence extraction link discovery eeld research program link discovery concerns identi cation complex relational patterns potentially threatening activities large amounts relational data data-mining methods assume data form feature-vector single relational table handle multi-relational data inductive logic programming form relational data mining discovers rules rst-order logic multi-relational data paper discusses application ilp learning patterns link discovery introduction events september development information technology aid intelligence agencies efforts detect prevent terrorism important focus attention evidence extraction link discovery eeld program defense advanced research projects agency darpa research project attempts address issue establishment eeld program developing advanced software aiding detection terrorist activity predates events program genesis preliminary darpa planning meeting held carnegie mellon opening center automated learning discovery june meeting discussed formation darpa research program focused knowledge-discovery data-mining kdd methods counterterrorism scope program subsequently expanded focus related sub-tasks detecting potential terrorist activity numerous large information sources multiple formats evidence extraction task obtaining structured evidence data unstructured natural-language documents builds information extraction technology developed darpa earlier muc message understanding conference programs current ace automated content extraction program national institute standards technology nist link discovery task identifying complex multi-relational patterns potentially threatening activities large amounts relational data input data input data existing relational databases finally pattern learning concerns automated discovery relational patterns potentially threatening activities patterns learned improve accuracy current eeld program focused sub-topics started summer incorporated information awareness iao darpa data patterns eeld include representations people organizations objects actions types relations data represented large graph entities connected variety relations areas link analysis social network analysis sociology criminology intelligence study networks graph-theoretic representations data mining pattern learning counter terrorism requires handling multi-relational graph-theoretic data current data-mining methods assume data single relational table consists tuples items market-basket analysis type data easily handled machine learning techniques assume propositional feature vector attribute representation examples relational data mining rdm hand concerns mining data multiple relational tables richly connected style data needed link discovery pattern learning link discovery requires relational data mining widely studied methods inducing relational patterns inductive logic programming ilp ilp concerns induction horn-clause rules rst-order logic logic programs data rst-order logic paper discusses on-going work applying ilp link discovery part eeld project inductive logic programming ilp ilp study learning methods data rules represented rst-order predicate logic predicate logic quanti variables relations represent concepts expressible examples feature vectors relational database easily translated rst-order logic source data ilp rules written prolog syntax conclusion appears rst uncle relation uncle brother parent uncle husband sister parent goal inductive logic programming ilp infer rules sort database background facts logical nitions relations ilp system learn rules uncle target predicate set positive negative examples uncle relationships set facts relations parent brother sister husband background predicates members extended family uncle tom frank uncle bob john uncle tom cindy uncle bob tom parent bob frank parent cindy frank parent alice john parent tom john brother tom cindy sister cindy tom husband tom alice husband bob cindy alternatively rules logically brother sister relations supplied relationships inferred complete set facts basic predicates parent spouse gender if-then rules rst-order logic formally referred horn clauses formal nition ilp problem background knowledge set horn clauses positive examples set horn clauses typically ground literals negative examples set horn clauses typically ground literals find hypothesis set horn clauses perform amount mouse action user agent employs fully connected three-layer neural network learning algorithm choice initially neural network made largely convenience evaluating learning algorithms task discuss topology neural network base representation web page bag-ofwords representation salton representation compatible neural networks empirically proven successful information retrieval lang pazzani salton bag-of-words representation simply encodes frequency keywords page html ignores word order explain obtain set key words phrases frequency array serves input network frequencies normalized respect expected values tfidf salton performing normalization agent agent enhanced version basic bag-ofwords representation equip agent ability handle key phrases simply phrases agent opposed single words agent handle phrases words agent simply treats key phrase long word handles keywords key phrases manner number input units network number key words phrases agent make majority html markup tags distinguish usage context word keyword found title document weight keyword found body document similarly keywords displayed smaller fonts slightly weight 
keywords displayed larger fonts employ types discounting enhance bag-of-words representation type discounting derives law decreasing returns purposes law keyword valuable time keyword found page weight time keyword found page weight time word found weight refer type discounting occurrence discounting occurrence discounting continues times word found page lastly discounting method discount words position page relative top page refer discounting position discounting hypothesize keyword valuable top page versus keyword bottom page enhancing bagof-words representation key phrases html tags occurrence discounting position discounting increases richness input representation web page considerably finally address issue acquiring keywords user numerous means mind analyzing user homepage examining text web pages visited user heuristic function information gain pazzani choose keywords pages text focus research independent means choose keywords simplicity experiments agent simply provided list keywords productionquality agent require sophisticated method choosing keywords network output units corresponds user action output units network output unit hyperlinks clicked represents fraction page hyperlinks clicked means user clicked current page hyperlinks outputs units scrolling activity mouse activity represent counts above-described events scaled number hidden units defined equation input units output units finally train network standard backpropagation algorithm trained agent predict based text page number hyperlinks page user click amount scrolling amount mouse activity user agent obtains training instances neural network silently observing user browsing discussed agent utilizes instances learn profile user experimental methodology section present cross-validation experiment measures agent accuracy predicting chosen user actions determine accurately agent predict normal user actions performed fold cross-validation experiment fold early stopping training fold training data tuning set training complete agent restores network performed tuning set record agent accuracy fold test set root-mean-square method measure accuracy agent network turn data study diverse environment obtaining representative sample web pages trivial short discussion type data experiment relevant stated provide list keywords agent keywords study related machine learning included words phrases machine learning neural network q-learning quinlan total key words phrases collect data browsed agent observed collected data browsing attempted simulate individual interested reading topics research projects machine learning browsing performed questions issues mind total pages visited pages related machine learning remaining page hyperlinks clicked scrolling-related events mouse-related events due current technological limitations agent obtain instances pages frames serverside scripts data collected academic websites tend technologies effort made visit completeness broad range pages perform number navigation actions including consistency forward back variety buttons algorithms clicking dead ilp links problem links download developed files applied browsing faq variety important ftp sites datamining problems information argue relational data mining remains under-appreciated data topic collected larger experiment kdd community representative real-world recent data textbooks sample data obtained mining mention ordinary user topic results presented section important topic representative generation data agent mining performance systems wide class critical users link experimental discovery results applications section present counter-terrorism initial discuss work results ilp link cross-validation discovery study tested ilp section algorithms table summarizes eeld datasets results recall current eeld average datasets values pertain domains measurements nuclear smuggling contract killing contract-killing table domain results cross-validation divided study natural real section world data agent manually collected collected total extracted news hundred sources instances synthetic arti ten cial train data test generated folds simulator fold section agent presents set experimental results data tuning natural set smuggling early contract-killing stopping data agent trains section presents train train initial instances results minus tune synthetic instances set contract-killing data epochs experiments chooses natural network state data performs nuclear-smuggling data tune set nuclear-smuggling measures dataset accuracy consists reports test russian instances nuclear action materials predicted root smuggling square chronology nuclear radioactive smuggling error incidents test basis sets hyperlinks analysis patterns clicked smuggling scrolling russian nuclear materials activity mouse information chronology activity based data open-source table reporting shows primarily world error news predicting connection mouse wnc activity lexis-nexis larger error articles obtained output units sources results translated italian user german mouse activity russian page research correlate chronology grew began chronology rst appeared user appendix interest paper page williams hypothesize woessner agent means detecting continually mouse evolving activity chronology published refined separate papers error observed journal experiment part recent events data-collection error section part indicative evidence hard extraction property link predict discovery research eeld ongoing project effort coverage design chronology accurate extended means detecting march mouse activity lastly chronology grew important remember incidents incident instances created descriptions agent chronology perfect entry indirect descriptions interpretations incident user incidents actions chronology accurate extensively directly cross-referenced user data input presented chronology claim incidents overcome relational database fact format format collect objects examples rows user tables burdened related attributes work differing discussed types earlier previous columns work focused tables employing values user navigation behavior matter surrogate input measuring source user interest information web user page personal objects web watcher types mladenic letizia denoted pre liberman xes examples agents utilize consist observed user browsing patterns entity recommend objects hyperlinks follow consist case location personal web material watcher organization pages person view source case weapon letizia extended event objects research recording user mouse consist actions generic user scrolling event actions link objects amount information expressing links continues expand entities events increasing consisting users represented assistance finding information table interest actual database experiments research agents relational tables number mentioned tuples continue relational grow table vary active recent topic machine learning elements reducing ilp system labeled learn examples events incident recent papers related order techniques construct augment larger labeled knowledge training data structures unlabeled data recognized threats mccallum ilp nigam system miller positive uyar training examples links events assume events unrelated compose set negative examples stipulate related commutative speci ilp system experiments related true related proven vice-versa set examples consists positive examples negative examples linking problem nuclear-smuggling data challenging heavily relational learning problem large number relations traditional ilp applications require small number relations natural contract-killing data dataset contract killings rst compiled hayon cook response research russian organized crime encountered frequent tantalizing contract killings contract-killing reports provided photograph criminal scene russia comparable assessment linked trends victims relationship victims relationship victims perpetrators dataset contract killings continually expanded cook hayon funding darpa eeld program veridian systems division vsd database captured chronology incidents incident chronology received description information drawn sources typically news article occasionally nuclear-smuggling dataset information chronology based open-source reporting foreign broadcast information service fbis joint research service jprs journals subsequently fbis on-line cut-down on-line version 
world news connection wnc services lexis-nexis main information sources additional materials worldwide web consulted feasible helpful search exhaustive limited time resources involved data organized relational tables format nuclear-smuggling data previous section dataset experiments relational tables number tuples relational table varies element ilp learner task characterize rival versus obstacle threat events edge ijijij zyx parameters define optimal translation parent child coordinate system parent model learns parameters tec discussed z-axis axis free rotation rotate child axis coincident parent coordinate frame figure presents graphically point construct edge deformation hnique ter ost function extending work felzenszwalb huttenlocher joints three-dimensional space define optimal relationship configuration configuration cost function edge distance vector conformational space optimal configuration distance measure -norm weighted dimension atan atan wll rotate ijji zzzyyyxxxw xyzyxw jijiji translate ijji orient preceding rotation bond parameters world coordinates rotation matrix euler angles zyx ijijij zyx ijijij zyxzyx iii iii iii important things note previous expre stssion fir complicated expressions terms trigonometric relationships defining optimal orientation nfiguration cost model match function constructed match function probability part image child pointing parent bond equation rotate cost rotating bond orient cost rotating axis translate cost translating screw-joint model set weights rotate low orient transla high high cost configuration ovements bond rotations definitions follow straightforwardly hat finitio location image empirical evidence suggests setting cost placing part euclidean distance local neighborhood image template learn template model-training process figure showing screw-joint connection parts ideal model directed version mrf parent definition oriented local z-axis coincident bond orientation ijijij zyx bond parameters learned algorithm construct model parameters model including optimal orientation edge template part learned training model structure learning orientation parameters fairly simple rotate atom canonic orientation orientation parent bond direction template averaging avoid averaging features require child bond pointed direction x-y plane child atom don worry rotation root child rotated direction child exists direction child atom record distance orientation canonic orientation average atoms type training set average lysine determine average parameters avg avg avg converting averages spherical cartesian coordinates precisely ideal orientation parameters similar rotation canonic orientation employed learning model templates case algorithm simply samples neighborhood atom averages points atoms type training set important note supervised learning algorithm train data similar resolution test data figure shows overview learning process part model matching algorithm searches six-dimensional conformation space zyx breaking dimension number discrete bins translational parameters allowed range region unit cell rotational parameters range issue arose implementation involved matching amino acids rings amino-acid topologies include cycles presents problem fast matching algorithm requires acyclic graph problem rings treated rigid objects true proline ring variable pucker approximation close application disallowing rotation ring bonds ignore bond ring letting split ring freely flop difficult issue arose involved collisions occasionally matching algorithm return structure overlapping atoms atoms close structure possibly occurred physically structure corresponds global optimal configuration parts simplified model clear handle case opted explore space non-optimal solutions soft maximums felz-hut dynamic program child locations taking maximal scoring minimal cost location location scores probability denominator sums choices parameter controls softness maximum likelihood non-optimal solution algorithm repeats soft-maximum process softer softer maximum legal structure found legal structure correct structure means physically additionally soft maxima leads quadratic running time original algorithm linear runtime pruning candidate locations impossible actual performance degradation acceptable experimental studies built pictorial-structure models amino acids alanine valine tyrosine lysine learned parameters single resolution protein training set single resolution protein served test set proteins structures elucidated crystallographers allowing compare results correct mapping simple experiment locating single amino acid diameter sphere density chose spheres completely contained entire amino acid searching avoid possibly correctly matching wrong region density map assumed sphere alpha carbon backbone atoms exist sphere considered grid atom centered fairly crude refinement techniques fine tune coarsely solved structure considered bins bins bins running time varies based size amino acid ghz pentium seconds find globally optimal location soft maximums seconds additional non-optimal solution task looked region density type amino acid contained closely place atoms actual atom locations ran pictorial-structure matching algorithm instance amino acids models compared atoms distance predicted actual distance figure shows accuracy atoms curve figure figure overview parameter-learning process atom type case alanine rotate atom canonic orientation parent bond located direction nigam child reducing direction x-y human plane labeling average main focus atom project type approach average template surrogate average bond measures geometry web browsers operating systems easily obtain burden averaged user template averaged perspective bond geometry back canonic orientation alanine mids work learning apprentices mitchell learning apprentices typically defined learning observing normal placement human accuracy angstroms software systems aimed avoid oms figure explicit amino training acid modes placement accuracy on-going plot future shows work number fraction y-axis questions atoms arisen initial work agent discuss extension agent discuss important enhancement agent agent accurately learns user profile observing user browsing briefly summarize ongoing work embedding neural network larger system fully autonomous agent build profile user interest employ profile heuristic search retrieve pages interest user agent greatly benefit improved methods detecting user actions user actions detected agent provide good first-order approximation user activity web page imagine sensitive methods recording user activity instance asserted command-statechange activity good approximation amount scrolling user page imagine precise measure scrolling records exact amount scrolling user similarly status-bartext change good approximation user mouse activity imagine precise ways measuring mouse activity today internet technology including client-side page scripts sensitive instruments user activity advanced record information promising approach time intercept user actions hooks operating system intercept scrolling activity mouse activity win api windows future ideally embed agent inside web browser application opposed agent running separate application expect embedding agent browser facilitate considerable improvement agent activity detection mechanisms fortunately trend browsers microsoft provide increasingly access programs actions users perform plan perform experiments agent human subjects subject agent collect number instances methods discussed paper collecting instances agent learn user profile subject instances utilize profile heuristic search pages interest subject subject rate subject assign number pages returned agent interest experiment agent employs learned user profile practical task agent results judged concrete standard lastly quality instances generated agent future research address agent performance generated examples relative performance examples constructed user-labeled pages approach requires effort user ratio hundred automatically generated examples user-labeled acceptable assume train agent user-labeled pages performs level consistently happy agent collect examples order reach performance level conclusions presented evaluated design agent employs standard neural network learn user interests key aspect system unobtrusively measures normal actions performed user page measurements surrogate desirable burdensome collect measurements user interest level explicitly labeling interest level pages users implicitly label web pages actions perform cross-validation experiment suggests agent learn predict high degree accuracy surrogate measurements user interest 
investigated joachims joachims freitag mitchell webwatcher tour guide world wide web ijcaipp lang lang newsweeder learning filter news icmlpp lieberman liberman letizia agent assists web browsing ijcaipp mccallum nigam mccallum nigam employing pool-based active learning text classification icmlpp miller uyar miller uyar mixture experts classifier learning based labelled unlabelled data advances neural information processing nipspp mitchell mitchell mahadevan steinberg leap learning apprentice vlsi design ijcaipp mladenic mladenic personal webwatcher implementation design technical report ijs-dpdepartment intelligent systems stefan institute october nigam nigam mccallum thrun mitchell learning classify text labeled unlabelled documents aaaipp pazzani pazzani muramatsu billsus syskill webert identifying interesting web sites aaaipp salton salton developments automatic text retrieval science 
obstacle threat examples pooled category producing two-category learning task rival obstacle threat treated motives dataset motivation learning task recognize patterns activity underlying motives turn contributes recognizing threats number positive examples dataset number negative examples ilp results aleph ilp system aleph experiments involving natural synthetic data default aleph simple greedy set covering table links entities events nuclear smuggling data event person organization location weapon material event person organization location weapon material procedure constructs complete consistent hypothesis clause time search single clause aleph selects rst uncovered positive seed saturates performs admissible search space clauses subsume saturation subject user-speci clause length bound details aleph experiments ensembles ensembles aim improving accuracy combining predictions multiple classi ers order obtain single classi investigate employing ensemble classi ers classi logical theory generated aleph methods presented ensemble generation paper concentrate popular method frequently create accurate ensemble individual components bagging bagging works training classi random sample training set bagging important advantage effective unstable learning algorithms small variations parameters huge variations learned theories case ilp advantage implemented parallel trivially details bagging approach ilp experimental methodology found experimental results based ve-fold cross-validation times train examples test learned remaining addition test set task identifying linked events nuclear-smuggling dataset aleph produces average testset accuracy improvement baseline case majority class guessing events linked produces average accuracy bagging sets rules increases accuracy rule good accuracy found system shown figure rule covers positive examples negative examples rule smuggling events related involve peoplec people connected person event person-person motive description dates symbols arguments relevant rule task identifying motive contract-killing data set dif cult aleph accuracy compared baseline accuracy bagging improves accuracy time rule figure shows kind logical clause ilp system found dataset rule covers positive examples single negative rule event killing rival follow chain events connects event event event event event event relates organizations events kind relation relationc events chain subsets incident experiments synthetic data synthetic contract-killing data synthetic data contract killing generated bayesian network simulator based probabilistic model developed information extraction transport incorporated iet simulator outputs case les complete unadulterated descriptions murder case case les ltered observability facts accessible investigator eliminated make task realistic data corrupted misidentifying role players incorrectly reporting group memberships ltered corrupted data form evidence les evidence les facts event represented predicates isa murder murderforhire perpetrator murder killer victim murder murdervictim devicetypeused murder pistolczech linked eventa eventd event person eventa personc relationb relationb person person personc eventf motiveg startdateh enddatei datedescriptionj event person eventd persone relationb relationb person person persone eventf motiveg startdateh enddatei datedescriptionj figure nuclear smuggling data sample learned rule rivalkilling eventa event event eventb eventa relationc eventdescriptiond event event eventb evente relationc eventdescriptiond event event evente eventf eventdescriptiond eventf figure natural contract killing data sample learned rule synthetic contract killing dataset consists murder events murder event labeled murder hire rst-degree second-degree murder murder hire events rst-degree second-degree murder events task learn classi correctly classify unlabeled event categories ilp results task variation mfoil learn binary classi discriminate events murder hire events aleph mfoil learns clause time greedy covering constrained general-to-speci search learn individual rules mfoil learn classi ers identify rst-degree second-degree murders binary classi ers combined form three-way classier task event classi positive classi event labeled category classi classi classi event positive select category commonly represented training data ran -fold cross-validation dataset murder events measured precision recall classi categories precision recall category ned results summarized table observe recall second-degree murders precision recall results system learns precise classi second-degree murders consequence lower recall adjust parameters system compromise precision higher recall computed accuracy classi ned percentage events correctly classi categories compare majorityclass classi classi events frequently represented category experiments accuracy majority-class classi classi cation accuracy system majority-class classi figure shows sample rules system learns rst rule murder event involves member criminal organization crime motivated economic gains murder hire rule murder result event performed love rst-degree murder premeditated murders rule murder result theft motivated rivalry performed public property seconddegree murder sample rules show system classifying events produces rules meaningful interpretable humans table results synthetic contract killing data murder degree degree hire precision recall current future research under-studied issue relational data mining scaling algorithms large databases research murderforhire groupmembermaleficiary subevents crimemotive economic firstdegreemurder subevents performedby loves seconddegreemurder subevents publicproperty crimemotive rival occurrentsubeventtype stealing generic figure synthetic contract killing data sample learned rules ilp rdm conducted machine learning arti cial intelligence communities database systems communities insuf cient research systems issues involved performing rdm commercial relational-database systems scaling algorithms extremely large datasets main memory integrating ideas systems work data mining deductive databases critical addressing issues related scaling working efciently learning complex relational concepts large amounts data stochastic sampling methods major shortcoming ilp computational demand results large hypothesis spaces searched intelligently sampling large spaces provide excellent performance time developing algorithms learn robust probabilistic relational concepts represented stochastic logic programs variants enrich expressiveness robustness learned concepts alternative stochastic logic programs working learning clauses constraint logic programming language constraints bayesian networks approach plan investigate approximate prior knowledge induce accurate comprehensible relational concepts fewer training examples prior knowledge greatly reduce burden users express easy aspects task hand collect small number training examples extend prior knowledge finally plan active learning ilp systems select effective training examples interactively learning relational concepts intelligently choosing examples users label extraction accuracy obtained fewer examples greatly reducing burden users ilp systems related work widely studied ilp approach relational data mining participants eeld program taking alternative rdm approaches pattern learning link discovery section brie reviews approaches graph based relational learning relational data mining methods based learning structural patterns graphs subdue discovers highly repetitive subgraphs labeled graph minimum description length mdl principle subdue discover interesting substructures graphical data classify cluster graphs discovered patterns match data subdue employ inexact graphmatching procedure based graph edit-distance subdue successfully applied number important rdm 
problems molecular biology geology program analysis applied discover patterns link discovery part eeld project http ailab uta eeld relational data easily represented labeled graphs graph-based rdm methods subdue natural approach probabilistic relational models probabilistic relational models prm extension bayesian networks handling relational data methods learning bayesian networks extended produce algorithms inducing prm data prm nice property integrating advantages logical probabilistic approaches knowledge representation reasoning combine representational expressivity rst-order logic uncertain reasoning abilities bayesian networks prm applied number interesting problems molecular biology web-page classi cation analysis movie data applied pattern learning link discovery part eeld project relational feature construction approach learning relational data rst atten propositionalize data constructing features capture relational information applying standard learning algorithm resulting feature vectors proximity system constructs features categorizing entities based categories properties entities related interactive classi cation procedure dynamically update inferences objects based earlier inferences related objects proximity successfully applied company movie data applied pattern learning link discovery part eeld project conclusions link discovery important problem automatically detecting potential threatening activity large heterogeneous data sources darpa eeld program government research project exploring link discovery important problem development counterterrorism technology learning link-discovery patterns potentially threatening activity dif cult data mining problem requires discovering relational patterns large amounts complex relational data existing data-mining methods assume data single relational table link discovery relational data mining techniques inductive logic programming needed problems molecular biology natural-language understanding web page classi cation information extraction areas require mining multi-relational data relational data mining requires exploring larger space patterns performing complex inference pattern matching current rdm methods scalable large databases relational data mining major research topics development generation data mining systems area counter-terrorism acknowledgments work supported darpa eeld grant tor santos costa castro dutra leave coppe sistemas federal rio janeiro partially supported cnpq biomedical group support staff condor team computer sciences department invaluable condor ashwin srinivasan aleph system breiman bagging predictors machine learning breiman stacked regressions machine learning califf mooney relational learning pattern-match rules information extraction proceedings sixteenth national conference arti cial intelligence aaaipages orlando july cook holder substructure discovery minimum description length background knowledge journal arti cial intelligence research cook holder graph-based data mining ieee intelligent systems cook hayon chronology russian killings transnational organized crime costa page cussens clp constraint logic programming bayesian network constraints unpublished technical note cowie lehnert information extraction communications association computing machinery craven dipasquo freitag mccallum mitchell nigam slattery learning construct knowledge bases world wide web arti cial accuracy x-axis shows fraction atoms accuracy plot shows atoms result good meaningful comparison methods pictorial-structure model artificial discrimination task spheres density provide matching algorithm amino acid type contained algorithm matched pictorial structures image normalized score returned type highest-scoring model table shows confusion matrix prediction task table shows algorithm good job discrimination task scoring accuracy amino acids vary wildly good data predictive accuracy task artificial algorithm designed place molecule region predict contents region finally figures show algorithm output good figure bad figure matches cases match pictorial structure matching algorithm found poor quality density map poor cases correctness crystallographer solution uncertain related work number attempts made past decades automate interpretation electron density maps successful arp warp proprietary technique involves placing moving atoms density map randomly sufficiently good job explaining density observed monte carlo approach multiple times averaging output results good method actual ala lys tyr val ala lys predicted tyr val table confusion matrix showing accuracy predictive benchmark tyrosine valine lysine lysine lysine tyrosine valine figure examples good matches structures shown lighter shade algorithm-determined structures shown darker shade light dark clouds show contours density note actual structure shows additional atoms pictorial structure alanine figure samples poor matches cases density map poor quality bringing question crytallographer interpretation successfully number times past fairly significant drawback density map fairly high resolution attempts made maid textal maid computer approaches interpretation human finding major secondary structures alpha helices beta sheets filling remaining regions textal hand takes pattern-recognition approach solving problem set fifteen rotation-invariant features resolutions determine type amino acid contained region features statistical deviation skew contained densities amino acid type database lookup attempts place atoms density map technique shown good results moderate resolution data provided locations alpha carbon advance techniques unlike approach make hierarchy routines order construct protein model finally fourth attempt fffear fast fourier transforms find secondary structures poor-quality density 
maps find alpha helices maps poor resolution beta sheets located maps resolutions unlike algorithm fffear constrained rigid templates approaches constructs flexible atomic model order interpret electron density map conclusions future work pictorial structures powerful tool building flexible molecular model fast matching algorithm placing models region unknown density paper extends work felzenszwalb huttenlocher extending pictorial-structure framework dimensions framework build flexible atomic model amount information assume precise sphere entire amino acid knowledge approximate alpha-carbon locations makes method impractical automated map interpretation matching algorithm efficient scale entire protein require pictorial structure thousands parts plan current approach refinement phase complements coarser method model configuration individual atom current method coarser model treat amino acid single rigid feature concern rotations backbone model place large pieces protein density map coarser grid approximate amino-acid locations alpha-carbon locations current finer-grained algorithm place individual atom room current method improvement match function algorithms fffear success finding rigid templates poor-quality maps modular nature match function pictorial-structure matching algorithm makes taking advantage powerful matching algorithm easy improvements prove important step development accurate automated density map interpretation tool acknowledgements work supported nlm grant nlm grant nih grant zolnai project management system structural functional proteomics sesame journal structural functional genomics lamzin wilson automated refinement protein models acta crystallographica perrakis sixma wilson lamzin warp improvement extension crystallographic phases weighted averaging multiple refined dummy atomic models acta crystallographica levitt software routine automates fitting protein x-ray crystallographic electron density maps acta crystallographica ioerger holton christopher sacchettini textal pattern recognition system interpreting electron density maps intl conf intelligent systems molecular biology cowtan modified phased translation functions application molecular-fragment location acta crystallographica cromer mann compton scattering factors spherically symmetric free atoms chemical physics fischler elschlager representation matching pictorial structures ieee transactions computers felzenszwalb huttenlocher efficient matching pictorial structures ieee conference computer vision pattern recognition burl leung perona probabilistic approach object recognition local photometry global geometry european conference computer vision lipson grimson sinha configuration based scene classification image indexing ieee conference computer vision pattern recognition 
intelligence dietterich machine-learning research current directions magazine dutra page costa shavlik empirical evaluation bagging inductive logic programming proceedings international conference inductive logic programming springer-verlag september zeroski relational data mining applications overview zeroski lavra editors relational data mining springer verlag berlin zeroski lavra introduction inductive logic programming zeroski lavra editors relational data mining springer verlag berlin zeroski lavra editors relational data mining springer verlag berlin freitag information extraction html application general learning approach proceedings fifteenth national conference arti cial intelligence aaaipages madison july aaai press mit press friedman getoor koller pfeffer learning probabilistic relational models proceedings sixteenth international joint conference arti cial intelligence ijcaistockholm sweden han kamber data mining concepts techniques morgan kauffmann publishers san francisco hand mannila smyth principles data mining mit press cambridge jensen goldberg editors aaai fall symposium arti cial intelligence link analysis menlo park aaai press koller pfeffer probabilistic frame-based systems proceedings fifteenth national conference articial intelligence aaaipages madison july aaai press mit press kramer lavra flach propositionalization approaches relational data mining zeroski lavra editors relational data mining springer verlag berlin lavrac dzeroski inductive logic programming techniques applications ellis horwood lehnert sundheim performance evaluation text-analysis technologies magazine mckay woessner roule evidence extraction link discovery eeld seedling project database schema description version technical report veridian systems division august muggleton stochastic logic programs journal logic programming muggleton bryant page sternberg combining active learning inductive logic programming close loop machine learning colton editor proceedings aisb symposium scienti creativity informal proceedings muggleton editor inductive logic programming academic press york neville jensen iterative classi cation relational data papers aaaiworkshop learning statistical models relational data austin aaai press mit press nist ace automatic content extraction http nist gov speech tests ace page ilp lloyd dahl furbach kerber lau palamidessi pereira sagiv stuckey editors proceedings computational logic pages springer verlag ramamohanarao harland introduction deductive database languages systems vldb journal richards mooney automated nement rst-order horn-clause domain theories machine learning sparrow application network analysis criminal intelligence assessment prospects social networks srinivasan study sampling methods analysing large datasets ilp data mining knowledge discovery srinivasan aleph manual http web comlab oucl research areas machlearn aleph aleph toc html srinivasan muggleton sternberg king theories mutagenicity study rst-order feature-based induction arti cial intelligence wasserman faust social network analysis methods applications cambridge press cambridge williams patterns indicators warnings link analysis contract killings dataset technical report veridian systems division january williams woessner nuclear material traf cking interim assessment transnational organized crime williams woessner nuclear material traf cking interim assessment ridgway viewpoints technical report ridgway center pittsburgh february witten frank data mining practical machine learning tools techniques java implementations morgan kaufmann san francisco woessner chronology nuclear smuggling incidents july -may transnational organized crime woessner chronology radioactive nuclear materials smuggling incidents july -june transnational organized crime wrobel inductive logic programming knowledge discovery databases zeroski lavra editors relational data mining springer verlag berlin zelezny srinivasan page lattice-search runtime distributions heavy-tailed twelfth international conference inductive logic programming springer verlag july zelle mooney learning parse database queries inductive logic programming proceedings thirteenth national conference arti cial intelligence aaaipages portland aug 
appears proceedings icdm workshop foundations directions data mining november speeding relational data mining learning estimate candidate hypothesis scores frank dimaio jude shavlik computer sciences department wisconsin madison dayton madison dimaio shavlik wisc abstract motivation multi-relational data mining knowledge discovery relational databases multiple related tables difficulty relational data mining faces managing intractably large hypothesis spaces attempt overcome difficulty sampling hypothesis space generate small set hypotheses uniformly sampled space candidate hypotheses evaluate set actual data hypotheses evaluation scores serve training data learning approximate hypothesis evaluator approximate evaluation quickly rate potential hypotheses needing score actual data test approximate clause evaluation algorithm popular inductive logic programming ilp system aleph neural network approximate hypothesis-evaluation function trained neural network replaces aleph hypothesis evaluation actual data scoring potential rules time independent number examples approximate evaluator heuristic search escape local maxima test neural network ability learning hypothesis-evaluation function benchmark ilp domains neural network accurately approximate hypothesis-evaluation function introduction background data mining techniques assume data exists form easily converted set fixed-length feature vectors converted fixed-size array real numbers integers nominal attributes multi-relational datasets conversion inelegant scales poorly conversely inductive logic programming ilp natively handles multi-relational data ilp natural treatment multi-relational datasets avoids problems converting examples feature vectors advantage rules full expressive power first-order logic making rich human-readable hypotheses ilp systems proven successful constructing set accurate rules datasets relations systems successfully employed number varied domains including molecular biology engineering design natural language processing software analysis ilp systems combine background domain knowledge categorized training data constructing set rules hypotheses first-order logic formally training set positive examples negative examples background knowledge set clauses first-order logic ilp goal finding hypothesis set clauses first-order logic ehbehb background knowledge hypothesis deduce positive examples negative examples real world applications constraints relaxed allowing explain positive examples negative examples algorithm underlying ilp systems basically searches clause subsumption lattice evaluating candidate clauses training data search begins initial candidate clause considers hypothesis generation local search problem subsumption lattice starting point search type local search depends implementation ilp system subsumption lattice constructed based idea specificity clauses specificity refers implication clause specific clause general undecidable clause first-order logic implies ilp systems weaker notion plotkin subsumption subsumption implies implication implication imply subsumption subsumption candidate clauses puts partial ordering clauses hypothesis space partial ordering lattice clauses built ilp implementations perform type local search lattice candidate hypotheses ilp implementations standard greedy covering algorithm completing local search subsumption lattice rule evaluated accepted positive examples covered explained rule removed dataset process repeated positive covered major distinction separating ilp implementations strategy exploring subsumption lattice algorithms fall main categories exceptions general-to-specific top-down specific-to-general bottom-up enumeration subsumption lattice framework variety common local search strategies employed including breadth-first search depth-first search heuristic-guided hill-climbing variants uniform random sampling rapid random restarts work general framework increasing speed ilp algorithm order candidate clauses evaluated complaint levied ilp systems scale poorly large datasets srinivasan investigated performance ilp algorithms general found worst-case running-time depends size subsumption lattice time required clause evaluation factor search space size depends maximum allowed clause length number terms saturation idea saturation number ilp systems put bound size subsumption lattice saturation involves choosing positive training set background knowledge saturation constructs specific fullyground clause entails chosen constructed applying substitutions variables ground terms clause called chosen bottom clause serves bottom element subsumption lattice ilp searches clauses considered ilp subsumption lattice subsume imply saturated simple suppose background knowledge prolog notation ground atoms denoted initial lowercase letter variables denoted initial uppercase letter current positive begin saturation letting ground atoms imply positive apply consistent substitutions make substitutions notation atom variable atom substituted variable apply rule line background knowledge finally combining saturation positive returning matter runtime complexity maximum clauselength bottom clause worst case size subsumption lattice ilp algorithm search factor affecting ilp performance evaluation time clause aspect complicated analyze srinivasan simplifies analysis assuming clause evaluated constant time evaluation clause entire training set occurs time set training examples exhaustive search subsumption lattice single clause takes worst-case running time important note situation bit worse running time makes srinivasan work assumed deduction goal clause set background relations takes constant amount time recursive rule background fact deduction undecidable restricting simpler case function symbols considered datalog allowing recursive clauses evaluating candidate clause set ground background facts np-complete ilp datasets fall simpler functionfree category evaluation time exponential number variables relates length expression difficult problems encountered longer hypotheses required order cover positive examples resulting execution time worse improvements ilp focused finding search strategy reducing fraction search space explored hill-climbing explore search space reduces worst-case running time end number heuristic functions guide ilp searches attempts proven successful srinivasan employs random sampling strategy considers sampling clauses subsumption lattice chosen clause found top clauses subsumption lattice maximum length interestingly independent size subsumption lattice much-improved worst-case running time generating single clause srinivasan idea based ordinal optimization works domains sizable number sufficiently good solutions technique needle-in-the-haystack problems ilp optimizations focus decreasing time spent clause evaluations term ilp running time improvements prolog clause evaluation function developed blockeel reordering candidate clauses reduce number redundant queries santos costa developed techniques intelligently reordering terms clauses reduce backtracking srinivasan developed set techniques working large number examples considers fraction examples learning process predicted score space clauses figure graphical representation function learned neural network exploring surface escape local maxima surface defined clause evaluation function work closely related group effort spent reducing time clause evaluations learning function estimates clause evaluation function quickly approximate goodness clause amount time independent number training examples make multilayer feed-forward neural network approximating ilp scoring function evalfn evalfn candidate clause set categorized training examples mapping clause score training set scoring metric evalfn score represents goodness hypothesis explaining training data evalfn train neural network approximates sufficient accuracy section specifics network topology training process search performed usual manner evaluating candidate clauses complete set examples neural network compute approximate clause evaluation score proceed algorithm clause scored actual training data notice method clause evaluation running time evalfn evalfn running time required perform actual evaluation training data additionally surface defined trained neural network guide search function encoded neural network fixed weights defines smooth surface space network inputs figure presents surface 
graphically smoothing nature approximation neural network order escape local maxima heuristic search subsumption lattice idea function approximation intelligently guide local search domain ilp boyan moore quadratic regression approximate function mapping points feature space endpoint trajectory local search starting point approximation escape local maxima heuristic search algorithm ran time reported test-set accuracy solutions discovered local search important note empirically paper concern learning approximation investigated speed local searches subsumption lattice intelligently escape local maxima searching hypothesis space evalfn evalfn learning clause evaluation function step building clause evaluation function approximator construction neural network requires choosing network inputs network topology base network construction top-down lattice exploration number popular ilp implementations implementations positive chosen random training set chosen saturated building bottom clause note prolog notation means recall bottom clause consists fully ground literals ilp system constructs candidate hypotheses choosing subset fully-ground literals variablizing replacing ground atoms variables manner replaces multiple instances single ground atom single variable approaches differ select ground literals bottom clause figure illustrates process neural-network inputs comprised set features derived candidate clause variablization saturating literal bottom clause input neural network input set literal bottom clause constructing clause set notice multiple sets literals bottom clause variablized yield clause set input units specific literals construct candidate clause formally candidate clause chosen selecting subset literals most-specific bottom clause current treat clause vector xxx -dimensional space bottom clause fully ground literals cyclic node node node arc arc arc path path path path path path path path path selected subset literals node arc path candidate clause cyclic node arc path saturation pick subset literals variablization background knowledge path arc path arc path node arc node arc node arc positive cyclic figure overview process ilp algorithms construct clauses bottom clause constructed applying substitutions ground terms variables subset literals bottom clause chosen variablization literals converted candidate hypothesis variablization replaces multiple instances single atom single variable prolog notation clauses means ground atoms denoted initial lowercase letter variables denoted initial uppercase letter ngconstructi chosen literal ground vector part inputs neural network important aspect input vector candidate clause clause aleph hypothesis space unique input vector representation additionally give functor function specific input network vector dimension corresponds functor appearing construction based number times functor candidate clause ground literals functor finally set inputs neural network features extracted variablized clause features include length number literals nvars number distinct variables nshared vars number distinct variables appearing avg var freq average number times variable appears max var chain longest variable chain appearing clause maximum chain length neural network consists fully-connected hidden layer single output unit output corresponds predicted output evalfn evaluate clause neural network converting vector notation forwardpropagating neural network trained approximate figure details network topology evalfn training neural network table algorithm train neural network training set size number neuralnetworktraining evalfn burnin iopairs neuralnetwork minerror inf trainset size random clause subsumption lattice evaluate evalfn add iopairs split iopairs trainset tuneset num epochs foreach score trainset run backpropogation algorithm score error score tuneset error minerror mineror error bestnn return bestnn table neural network training algorithm bottom clause set training examples heuristic function evalfn number training clauses trainset size train neural network learn clause evaluation function evalfn early stopping avoid overtraining return network learned clauses uniformly randomly selected space legal clauses maximum clause length evaluate randomly selected clauses training data creating input output pairs training neural network training process makes srinivasan random uniform sampling uniform sampling generate pairs ensures neural network approximation accurate entire search space local search methods bias neural network approximation local region search space speeding searching ilp trained neural network training complete neural network approximate clause scores ilp search requires clause evaluation solution finally found score clause actual data determine coverage solution search located empirical evidence suggests problem treated adaptive sampling problem burn-in evaluating clause network small percentage candidate clauses bypass neural network compute clauses actual scores clause bypasses network computed score forms pair train neural network storing subset recently-scored pairs time clause bypasses neural network epoch training occurs stored pairs avoid overtraining network specific examples time pairs removed set stored pairs strategies explore favorable regions search space search progresses adaptive sampling learn greatest accuracy higher-scoring regions search space additionally continue improve accuracy neural network search progresses assuming time required evaluate clause greater time required approximate clause neural network factor speedup training examples assumption valid implementing testing usage approximator topic future work trained neural network escape maxima local search developing search strategy directly trained neural network determining candidate clauses trained neural network defines surface network input space network sigmoidal units smoothing effect actual evaluation function advantage smoothing neural-network approximation escape local minima hill-climbing search technique suggestive approach boyan moore stage algorithm approach envision alternating iterations hill-climbing actual evaluation function hill-climbing neuralnetwork approximation neural network smoothing tend guide search global maximum hypothesis space evalfn results discussion section details empirical evaluation neural network learning task goal ascertain neural network learn ilp clause evaluation function simplifying task experiments batch learning process incremental learning process outlined section datasets tested neural network learning standard ilp benchmarks tasks included predicting mutagenic activity carcinogenic activity compounds predicting smuggling nuclear radioactive materials predicting metabolic activity proteins description datasets mutagenesis task concerned predicting mutagenicity compounds ilp learner provided background knowledge consisting chemical properties compounds general chemical knowledge form first-order logic relations dataset popular benchmark explores large search space carcinogenesis similar mutagenesis task inherently difficult problem task main concern predicting carcinogenic activity compounds potential carcinogenic compounds database problem consists labeled examples half carcinogenic nuclear smuggling dataset based reports russian nuclear materials smuggling interesting highly-relational nature relational tables task concerned predicting smuggling events linked dataset subset complete dataset examples split evenly positive negative examples protein metabolism task gene function prediction task kdd cup challenge wisc dpage kddcup challenge involves learning protein functions sub-task concerned predicting proteins responsible metabolism subset complete dataset examples split evenly positives negatives learning clause evaluation function ilp system aleph web comlab oucl research areas machlearn aleph aleph toc html generate sets randomly sampled clauses datasets positive examples construction bottom clause seed examples chosen randomly considered maximum clauselength nuclear smuggling task considered larger task clauses 
scored variant aleph compression heuristic clause score examples positive total thclauseleng covered exs negative covered exs positive score unlike aleph compression include term denominator convert scores good range neural networks dividing total number positive examples comparison scores datasets dataset clauses scores train neural network machine learning package weka generated learning curves -fold cross-validation datasets neural network constructed hidden units learning rate fixed pos pos bottom clause selected literals candidate clause nvars len predicted output figure ove view showing neural network topology input vector construction notice vector constructed literals chosen fully-ground bottom clause candidate clause sets selected literals correspond candidate clause set chosen clause construction momentum set added early stopping weka avoid overtraining cross-validation fold set training set tuning set epochs neural network performed tuning set weka numeric feature normalization enabled numeric input features learning curves datasets figure data curves show rootmean-squared rms error sets examples section explains curves graphs discussion results figure illustrates datasets hypothesis evaluation function learned reasonable accuracy datasets data added training set neural network accurately learns evaluation function interesting note number examples required accurately learn approximator accuracy final classifier varies datasets evalfn absolute accuracy approximator varies datasets protein metabolism fully-trained network averages rms error mutagenesis results order magnitude worse promising worst performing approximator rms error instance-independent instance-dependent features concern learning clause evaluation function rule-by-rule basis learning neural network saturated features employ independent selected saturation feature ground literals selected vector section instanceindependent instance independent representation features shared generating rules seed examples rules bootstrap initial classifier based knowledge generated learning previous rules looked contribution subset features datasets wanted instance-independent features contributed learning task methodology section weka construct additional learning curves datasets learning curves correspond mutagenesis training set size rro data instance-independent instance-dependent protein etabolism training set size nuclear smuggling training set size rro carcinogenesis training set size rro figure learning curves showing test set accuracy domains plots show root-meansquared rms distance predicted actual score improved examples added training set plots show learning instance-dependent features instanceindependent features training network instance-independent features instance-dependent features exception protein metabolism training instance-independent features produce accurate classifier training instancedependent features complete set features datasets complete set features produce significantly accurate network approximator instance-dependent features suggests instance-independent features transfer learning seed seed approaches developed conclusion future work neural network clause evaluation powerful tool improving runtime efficiency handling large search spaces ilp ilp confronted increasingly larger problems methods present paper grows treated network learning evaluation tasks computationally free operations true true running time neural network evaluation training independent number ilp examples dataset means examples ilp training set neural-network evaluation made virtually free strategy decrease running time ilp systems large tasks pressing work remains implementing evaluating strategies taking advantage clause-evaluation approximator outlined sections accuracy lost approximating clause-evaluation function difficult determine affect quality solutions generated quickly evaluate clauses typical ilp search open question network approximation chose hypothesis space search direction future work approach idea boyan moore stage algorithm recent work ilp search function focused gsat rapid random restarts rrr explore subsumption lattice paper learn approximate clause evaluation function boyan moore concentrate learning gsat trajectories starting locations subsumption lattice boyan moore showed trajectory approximation function provide smoothing effect search space neural-network evaluation approximator prove beneficial conclusion experiments suggest ilp clause-evaluation function approximated neural network clear ilp problems larger larger datasets encountered strategies increasingly important acknowledgements work supported nlm grant darpa eeld grant nlm grant lavrac dzeroski inductive logic programming techniques applications ellis horwood nienhuys-cheng wolf foundations inductive logic programming springer-verlag schmidt-schauss implication clauses undecidable theoretical computer science quinlan learning logical definitions relations machine learning muggleton feng efficient induction logic programs proc conf algorithmic learning theory muggleton inverse entailment progol generation computing srinivasan study probabilistic methods searching large spaces ilp tech report prg-tr- oxford univ computing lab zelezny srinivasan page latticesearch runtime distributions heavy-tailed proc intl conference ilp hanschke wurtz satisfiability smallest binary program info proc letters dantsin eiter gottlob voronkov complexity expressive power logic programming acm computing surveys sreenivas vakili ordinal optimization deds discrete event dynamic systems theory applications blockeel improving efficiency inductive logic programming query packs research santos costa query transformations improving efficiency ilp systems machine learning research srinivasan study sampling methods analysing large datasets ilp data mining knowledge discovery boyan moore learning evaluation functions improve optimization local search machine learning research king muggleton srinivasan sternberg structure-activity relationships derived machine learning pnas dolsak muggleton application ilp finite element mesh design proc intl workshop inductive logic programming mckay woessner roule evidence extraction link discovery seedling project database schema description veridian technical report witten frank data mining morgan kaufmann publishers 


appears proceedings darpa workshop case-based reasoning morgan-kaufmann finding genes case-based reasoning presence noisy case boundaries jude shavlik computer sciences department wisconsin madison usa shavlik wisc abstract effectively previous cases requires reasoner match fashion current problem large library stored cases largely unaddressed task case-based reasoning process parsing continuous input discrete cases parsing accurately relevant previous cases found advantages case-based problem solving lost parsing data cases complicated input data noisy paper presents approach applying case-based paradigm presence noisy case boundaries approach fully implemented applied domain molecular biology specifically successful case-based approach gene finding empirical study demonstrates method robust high error rates system conjunction human genome project wisconsin department genetics sequencing dna bacterium coli introduction problem solve initial step case-based reasoner takes recall relevant cases memory current situation completely identical previous sort partial matching needed involves selecting cues index memory kolodner partial matching presupposes case-based reasoner well-defined current situation boundaries current case cleanly defined real-world continuous problem accurately parsing experience discrete cases extremely challenging largely unaddressed task exception redmond made complicated sensors measuring world noisy good idea constitutes boundaries case noise easily blur signals paper presents method performing case-based reasoning presence noisy case boundaries task domain molecular biology successfully technique find genes noisy dna sequences sections provide introduction molecular biology describe algorithm note genes subsequences lengthy string due nature genetic code types noise string current partial-matching algorithms gene finding fail research partially supported office naval research grant -jnational science foundation grant iriand department energy grant de-fg gene finding task highly amenable solutions rapidly-growing computer databases information discrete opposed involving partial differential equations forms computational science domain-specific knowledge heuristic basic approaches investigated related communities development algorithms perform similarity matches genes casebased reasoning lipman myers paper machine learning learn general characteristics genes lapedes noordewier stormo towell creating grammars recognize genes searls searls remainder article presents approach falls category introduction molecular biology paper describes application molecular biology introduction genes proteins appears section sufficient detail included non-biologist understand rest paper information found textbooks watson dna linear sequence alphabet letters called nucleotide base human dna estimated nucleotides common intestinal bacterium coli bases dna molecule involves complementary sequences organized double helix sequence paired similarly paired pairing forms basis cell replication discovery watson crick revolutionized biology watson replication addressed paper reader dna single linear sequence complement reverse strand easily calculated needed important aspect dna purposes paper subsequences encode proteins subsequences called genes proteins work horses cell enzymes proteins cell membranes proteins linear sequences -character alphabet letters called amino acid process called translation gene read protein produced consecutive three-letter string gene encodes amino acids mapping nucleotide triplets single amino acids called genetic code three-letter string called codon called stop codons cell stop translating dna table hypothetical gene shows protein encode bars group bases codons short real gene genes hundred nucleotides long translation process involves grouping nucleotides threes reading frame extreme importance reading frames nucleotide intermediate step called transcription dna copied similar molecule called rna rna translated proteins level detail understand paper reader assume dna directly maps protein distinct three-letter dna strings genetic code redundant item codon return topic reading frames play major role paper determine case boundaries human genome project alberts massive world-wide research project goal determining sequence human dna locating genes genomes scientifically important species sought genetics project wisconsin sequencing dna common bacterium coli research reported paper result collaboration project headed prof blattner genetics department blattner group chopped coli dna roughly pieces pieces called contig contiguous sequence computationally analyzing contig sequenced blattner laboratory absolute start codon makes gene finding non-trivial task towell discussion applying machine learning techniques find signal start gene remainder paper describes case-based approach gene finding works presence noisy dna sequences sequencing estimated error rate alberts wrong nucleotide recorded disastrously extra nucleotide inserted existing missed explained insertions deletions greatly affect translation process due triplet nature genetic code finding genes noisy data project map sequence organism genome interpretation final sequence undertaking great magnitude inherent potential errors recorded sequence complicates analyses wisconsin coli sequencing project producing large amounts anonymous dna computationally analyzing data closely-related main goals research correct sequencing errors noting inconsistencies biological data locate identify regions sequence encode proteins heretofore unknown undertaking complementary approaches involves performing robust similarity matches protein sequences paper involves detecting dna segments general characteristics genes towell long-term aim assign function regions anonymous dna produced wisconsin sequencing project primary concern respect error correction locate frameshift errors mistaken insertion deletion nucleotide gene shift improper reading table hypothetical gene translation protein gene agc atg caa tag protein stop frame due triplet nature genetic code errors disastrous occur inside putative gene translation process frame remainder predicted protein bears resemblance correct protein partial matching fail computational methods paper presents designed locate genes robust presence frameshift errors important reader understand frameshift errors greatly affect protein predicted translating gene process translating string bits alphabetic characters ascii code bit dropped inserted resulting translation bear resemblance correct text international databases store biological sequence data notably genbank bilofsky protein information resource pir george databases store cases complete genes genbank complete proteins pir number researchers developed case-based algorithms partially match dna subsequences amino-acid sequences databases lipman myers methods suffer extremely sensitive frameshift errors previous approaches excellent job matching presence substitution errors mistaking fact primary strengths find homologous matches homologous protein species similar terms amino-acid sequence biological functionality due process evolution homologous proteins abound locating major importance genes sequenced case-by-case basis frameshift errors frequent primary biological task find similar matches proteins previous algorithms designed problem successful advent massive sequencing projects noisy dna unknown functionality unknown codon boundaries rapidly produced conditions case-matching algorithm needed case-based gene finding algorithm developed case-based algorithm gene finding robust presence frameshift errors find-it algorithm builds blast similarity-search program myers blast efficiently produces approximate matches matches extend frameshift errors find-it method coherently combines partial matches protein reading frames overcoming missing extra nucleotides sequenced dna table describes algorithm sequence dna algorithm collects openreading frames orfs sequence complement algorithm translates orfs proteins variation blast match protein pir database convert proteins directly matching dna genbank entries partial matches amino acids biologically well-defined blast pam matrix dayhoff define 
similarity amino acids due genetic code degeneracy codon-usage patterns species vastly dna sequences lead similar protein sequences finally matches genbank easily effected translating genes database proteins orf dna successive stop codons orfs genes sequencing errors genes lie orf table find-it algorithm matching proteins dna sequences presence frameshift errors sequence dna collect open-reading frames orfs reading frames dna complementary strand total frames minimal length required considered orf sequencing errors introduced false stop codons stretch dna discarded translate orf amino-acid sequence apply blast myers produce partial matches gaps protein information resource pir database note dna sequence frames translated matched collect matches pir protein encountered piecing matches consistent coverings protein text details score combined protein matches sort report matches applying table algorithm summary translating sequenced dna proteins matching pir protein database requires address problem frameshift errors advantage partial matches defined biologically feature blast protein match entirety reports matching subsequences match terminate protein due frameshift error case remainder protein match orf examples follow match returned blast maps portion dna sequence segment protein figure schematically shows matches numbers matches reading frame note matches provide consistent covering protein match inconsistent note combining matches extra nucleotide dna identified marked define means consistent collection matches protein call covering assume match maps dna segment protein segment match maps dna segment protein segment matches belong consistent covering constraints hold left-to-right order dna protein sequence dna strand forward reverse matches intersect sequence dna protein intersect locations extra missing nucleotides dna sequence small discrepancies dna sequence blast matches protein figure combining blast matches overcome frameshift errors distance dna locations approximately times distance protein locations amount dna end match beginning roughly corresponds number amino acids matches constraint relaxed find-it applied eukaryotic dna advanced species genes necessarily contiguous pieces dna approach elegantly extends recognition exons intervening sequences current experience find-it generally consistent coverings protein set matches dna contig partially matches thousand proteins successful coverings matches order combinations matches constraints consistent covering greatly prune number considered addition require half protein sequence matched protein coverage collection matches determined log time proteins discarded performing inefficient step finding consistent coverings program spends considerable amount time searching consistent covers major open issues understand computational complexity task devise efficient algorithms section discusses topic sample results coli contigs applied algorithm coli contigs sequenced matches coli proteins found strong matches proteins species found strong match defined half protein sequence matched homologous proteins match stretch dna find-it retains strongest-matching matches accounted contig dna contig coli proteins encountered homologous matches detected contig covered current research increase numbers figure matches contig combined protein coverings arrow-headed segment represents match produced blast tested find-it dna sequence completely-sequenced bacteriophage called proteins pir found matches nonproteins homologies noted pir annotation previouslyunknown homologies substantial biological interest multiple matches detect sequencing errors section presents actual composite matches produced find-it figure involves matching coli protein figure match human protein matches illustrate sequencing errors detected show coli homologous genes located figure consistent matches contig protein paecs phosphoserine phosphatase escherichia coli ec-number protein length protein coverage forward strand dna mpni frame mpni protein mpni kiphlrtpgllvmdmd staiqiecideiaklagta staiqiecideiaklagt staiqiecideiaklagtg dna tgr ngem frame gem vaevteramrgeldftaslr rvatlkgada-i protein agtgem dna nilq frame nilq protein nilq dna agft frame gft ffaeylrdk protein ggft figure sample match coli gene contig figure covering entire protein sequence phosphoserine phosphatase involves blast matches match begins dna reports frame appears protein sequences matching score reported blast represents sum pam matrix scores aligned amino acids top line match translated dna bottom protein sequence numbers end lines represent nucleotide positions entire dna contig amino-acid positions protein middle line presents alignment letters represent identical matches represent positive-scoring partial matches periods represent matches score blanks represent negative-scoring matches dashes represent gaps introduced blast matching process reason lower-case letters appears finally braces seed match blast algorithm myers details note addition characterizing portion contig covering suggests frameshift errors figure frame transitions careful inspection boundaries successive matches leads prediction missing extra nucleotides sequencing errors ambiguities located noting discrepancies protein sequence translated dna finally gaps nucleotide insertions deletions length multiple hypotheses checked reviewing original sequencing gels genetics laboratory sequencing errors database pir genbank errors corrected illustrate error-correction process top matches figure protein elements match repeated matches end match stronger discard matches due insertion gap match deleted matches lower case figure amino acids dropped match beginning dna sequence note nucleotide match nucleotide item codon item sequencing process missed nucleotide inspection genetic code map codons amino acids shows nucleotide inserted position shift match reading frame accordance match reading frame similar analyses applied matches figure allowing gaps permits sequences sqll sqmll match alignment amino acids protein insignificant function evolution amino acids disappear effect matching gaps accommodates phenomenon protein acetyl-coa acyltransferase precursor human ec-number protein length protein coverage forward strand dna tvnr lcgssmqalhdaa frame di---cv -i-ar tvnr protein tvnr qcssglqavasia rmimtg--d-aqaclvggvehmg g--dac---gve ggirngsydigmac---gvesms dna mglt frame gisre faa-a p--t protein mgit --de irp paf tag sdgaaa gl-p avvg img kagl neafa --ek n-l ghplgc gar tlln mcig qgiatvfe vfe mgaaavfe figure sample match non-e coli gene contig experiment evaluating find-it noise sensitivity section experimental evaluation gene-finding method noise sensitivity algorithm studied experimental method gene length extracted genbank database gene replication protein bacteriophage amounts noise added twenty-five experimental runs applied find-it counted times found initial gene investigated simple noise models composite model replacement probability nucleotide replaced deletion probability nucleotide deleted insertion probability nucleotide inserted nucleotide combination probability occurs nucleotide possibilities equally noise models simplistic due nature sequencing process insertions deletions occur runs nucleotide aaaaa models sufficient present purposes figure results experiment experiment noise models find-it unaffected noise rate exceeds recall estimated error rate sequencing find-it approach robustly find genes sequences biological laboratories producing current research issues improving extending find-it method current activities include improving algorithm efficiency tuning parameters adding ability locate good dense matches cover sizable portion protein prove finding matches protein domains present implementation locating matches unacceptably expensive computationally studying computational complexity covering problem defined previous section devising efficient possibly heuristic algorithms problem constructing consistent covers defined modeled combinatorial optimization problem 
maximum weight matching interval graph restricted version problem weights solved log steps golumbic 
masuda preliminary work wisconsin computer science graduate student meidanis shown log algorithm general case algorithm shows promise tool rapidly finding consistent coverings plan implement decide edgeweighting scheme leads covers similarity matches proteins alternative methods recognizing genes investigating complementary approaches involve neural-network learning identify regions dna general global statistics genes locate stretches dna replacements deletions insertions equally chances finding original gene amount noise added figure find-it chances finding gene function sequence noise results averaged experimental runs domain portion protein possesses stand-alone function signal biological activity promoter regions bind protein initiates transcription translation towell research lines approach includes gribskov staden approach investigated lapedes noordewier stormo towell conclusion presented case-based approach gene finding robust presence errors input data case libraries errors frameshift errors greatly complicate task determining boundaries cases due triplet nature genetic code research addresses important general question makes case specifically parse noisy world discrete cases matching case library current case improperly delimited partial matching previous cases fail algorithm addresses problem producing multiple partial matches cases combining subset consistent leads error detection correction general idea robust case matching promises applicable domains involving continuous data speech recognition vision successfully applying find-it method support human genome project wisconsin genetics laboratory found previously unknown coli genes acknowledgments discussions fred blattner donna daniels guy plunkett eric bach debby joseph prasoon tiwari mick noordewier joao meidanis greatly improved research special fred blattner director coli sequencing project providing sample contigs pointing problem matching dna frame-shift sequencing errors alberts alberts mapping sequencing human genome national academy press washington bilofsky bilofsky burks genbank genetic sequence data bank nucleic acids research dayhoff dayhoff atlas protein sequence structure national biomedical research foundation wash george george barker hunt protein identification resource nucleic acids research golumbic golumbic hammer stability circular arc graphs journal algorithms gribskov gribskov devereux burgess codon preference plot graphical analysis protein coding sequences prediction gene expression nucleic acids research kolodner kolodner retrieval organizational strategies conceptual memory lawrence erlbaum assoc hillsdale lapedes lapedes barnes burks farber sirotkin application neural networks machine learning algorithms dna sequence analysis computers dna sfi studies sciences complexity vii addison-wesley lipman lipman pearson rapid sensitive protein similarity searches science masuda masuda nakajima optimal algorithm finding maximum independent set circular arc graph society industrial applied mathematics journal computing myers myers miller altschul gish lipman basic local alignment search tool journal molecular biology noordewier noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences ieee conf neural information processing systems denver redmond redmond learning experience creating cases examples proc case-based reasoning workshop pensacola beach searls searls representing genetic information formal grammars proc nat conf paul aug searls searls investigating linguistics dna definite clause grammars proc north american conf logic programming staden staden finding protein coding regions genomic sequences methods enzymology vol doolittle academic press york stormo stormo schneider gold ehrenfeucht perceptron algorithm distinguish translational initiation sites nucleic acids research towell towell shavlik noordewier refinement approximate domain theories knowledge-based artificial neural networks proc nat conf boston july watson watson crick molecular structure nucleic acids structure deoxyribose nucleic acid nature watson watson hopkins roberts steitz weiner molecular biology gene benjamin-cummings menlo park 
appears machine learning proceedings eighth international workshop birnbaum collins eds morgan kaufmann san mateo constructive induction knowledge-based neural networks geoffrey towell mark craven jude shavlik department computer sciences wisconsin madison wisconsin towell craven shavlik wisc abstract artificial neural networks proven successful general method inductive learning examples viewed terms constructive induction describe method knowledgebased neural network kind created kbann algorithm basis system constructive induction training extract types rules network modified versions rules initially provided knowledgebased neural network rules describe newly constructed features experiments show extracted rules accurate classifying examples trained network rules extracted introduction artificial neural networks anns proven powerful general technique machine learning host empirical comparisons anns effective generalizing training testing examples common symbolic machine learning algorithms atlas fisher shavlik weiss generalization aptitude results ability anns train hidden units form intermediate representations words success anns results ability hidden units loci constructive induction anns tend black boxes training intermediate representations easily comprehended humans result features constructed anns passed related problems paper describes work aimed making constructed features ann human inspection understanding method begins kbann algorithm towell supply knowledge-based neural network knn intermediate information derive order correctly solve problem kbann reduces scope problem solved simplifies task understanding features discovered training knn trained set classified examples standard neural learning methods rumelhart finally nofm algorithm extracts rules trained knns understand feature representations arise training initial knowledge provided knn adapted present results knns tool constructive induction molecular genetics problem recognizing splice junctions kbann algorithm kbann algorithm knowledge base domainspecific inference rules domain theory form prolog-like clauses determine topology initial weights knn domain theory complete correct support approximately correct reasoning kbann translates domain theory knn units links correspond parts domain theory detailed explanation procedure kbann translate rules ann found towell figure translation domain theory knn kbann artificial domain theory figure defines membership category figure represents hierarchical structure rules solid dotted lines represent prohibitory dependencies figure represents knn results translation domain theory neural network units figure introduced knn handle disjunction rule set unit knn corresponds consequent antecedent domain theory bias true returns number true antecedents true true figure nofm algorithm thick lines figure represent heavily-weighted links knn correspond dependencies domain theory thin lines represent links added network refinement domain theory rule extraction section briefly presents nofm method algorithm extracting rules trained knns towell method makes assumptions networks training knn significantly shift meaning units making assumption methods attach labels extracted rules correspond consequents symbolic knowledge knn based enhancing comprehensibility rules units trained knn activation making assumption non-input unit trained knn treated step function boolean rule observation trained knns suggests assumptions valid nofm method searches antecedents form antecedents true nofm suggested experiments neural networks good learning n-of-m concepts fisher observation rule sets extracted methods similar saito subsets compactly expressed n-of-m concepts nofm method finds groups links approximately weight treats group weights nofm algorithm figure shows unit trained knn heavilyweighted antecedents lowly-weighted antecedents shown rules extracted nofm similar figure simplify rules removing weights thresholds simplification extracted rules easier comprehend networks primary advantage nofm algorithm exhaustively search combinations links find rules result nofm runs polynomial time respect number incoming links unit methods saito require exponential time nofm require alterations structure networks extraction sestito rules derived nofm lack simplicity straightforward prolog-like rules extracted methods splice-junction problem splice junctions points dna sequence superfluous dna removed process protein creation problem addressed recognize boundaries exons parts dna sequence retained splicing introns parts dna sequence spliced sequence dna figure illustrates splicing occurs process protein creation problem consists subtasks recognizing exon intron boundaries referred recognizing intron exon boundaries referred dna mrna intron exon key precursor mrna figure organization genes higher organisms approach problem dataset initially noordewier dataset examples remaining consists nucleotide-long dna sequence categorized type boundary center sequence domain theory classifies examples correctly derived biological literature watson abstracted view knn resulting theory depicted figure previously reported experiments noordewier domain theory include intermediate terms learn concept splice-junction knn requires hidden units addition domain theory achieve high level accuracy extra hidden units enable network constructively induce requisite terms captured domain theory sections discuss problems arise incorporating hidden units domain theory knn problems addressed pyr rich dna sequence stop stop figure initial splice-junction knn shaded units represent definitional rules domain theory incoming weights bias change training units adding units constructive induction performance neural network dependent topology kolen shavlik organize topology network order learn task open question domain knowledge problem network topology kbann directly addresses issue eliminates search network-topology space adding extra hidden units capture features expressed domain theory reopens problem topology determination specifically number connectivity added hidden units determined guiding principle determine number connectivity added hidden units make decisions topology simplify problem interpreting trained knns idea guided topology determinations analogy vision scanning dna sequences analogy suggests recognition cones honavar scanning dna sequence individual hidden units connected input units representing short contiguous subsequence dna strand analogy recognition cones supported biology dna suggests localized regions dna sequence key recognizing important biological signals analogy partially resolves number connectivity issues raised added units experiments reported cone unit covered sequence nucleotides long cone size slightly longer dna features splice-junction domain theory cones overlap ends sequence important information found input unit covered cones networks cones shorter length architectures stoppyr rich cone units figure splice-junction network cone units varied amount overlap cones variations significant effect network performance cone network connected directly output units intermediate layer hidden units figure schematic representation splice junction network cones added simplifies interpretation multiple output units encode features shared cone supporting decision connect cones units output unit dietterich suggestion sharing hidden units important contributor classification abilities anns dietterich input output weights biases cone units initialized random near-zero values near-zero values parameters reflect fact unlike units domain theory roles cone units completely unknown training interpreting added hidden units problems arise apply nofm interpretation algorithm knns include cone units units knns labeled result initial correspondence parts domain theory cone units similar labels biological significance feature measured cone unit determined region dna sequence measured unit serves consequent extracted rules problem rules extracted cone units unnecessary double negatives occurs biases cone units links units initialized near-zero values 
result signs links biases cone units training meaningful relation arbitrary respect concepts units result simply applying nofm cone units rules unnecessary double negatives alleviate problem cones negative biases inverted prior interpretation signs bias input output links reversed biases receivers links reversed units adjusted reflect changed signal problem related nofm algorithm approximates network unit linear threshold unit ltu previously discussed nofm method assumes units knn tend activations condition activation bifurcation true nofm accurately approximate unit ltu activation bifurcation holds true training knn units domain theory hold true cone units initially configured approximate ltus occasionally coax cone units approximating ltus progressively steepening logistic activation function units training issues addressed nofm method interpret rules encoded added hidden units making constructive inductions knns human review experimental results utility knns method constructive induction measured independent axes ability rules retain accuracy knn extracted comprehensibility extracted rules figure addresses issue plotting accuracy extracted rules versus accuracy networks rules derived results figure subset chosen randomly examples testing repeated -fold crossvalidation weiss extracted rules perform networks training examples error rates rules slightly networks testing examples suggests extracted rules capture meaningful regularities training set avoiding spurious correlations account performance networks training sets addition figure presents results knns cone units addition cone units improved accuracy network extracted rules aspects comprehensibility constructive inductions individual rules extracted network understandable sets rules derived training slightly modified table presents representative set rules extracted cone units nofm trials ten-fold cross-validation test information relevant questions comprehensibility individual rules largest rules anten-fold cross-validation good approach issue training set examples common training set network extracted rules percent errors train set added units test set added units train set added units test set added units figure error rates training testing examples tecedents recall number antecedents comprehensibility story antecedent weighted weights shown clarity rules rarely antecedents weights rules extracted single training episode comprehensible table typical rules extracted cone units consequent antecedents notation signifies cone connected output unit subsequence starting nucleotides ending splice-junction antecedents upper-case times single -fold cross-validation test antecedents shown uppercase extracted rules consistent fourteen twenty antecedents table trials discussion measures rules extracted cone units successful rules short consistent comprehensible extracted rules accurate classifying testing examples network derived improvement accuracy result extracted rules picking important generalizations made network ignoring small correlations network perform training set rules constructed cone units identify biologically-significant structures working detailed assessments interestingness constructed rules important note rules extracted cone units sufficient recognition boundaries constructed rules lend additional evidence support predicates initial domain theory supports suggestion made previously knns suited constructive induction presence knowledge knn considerably reduces problems resolved feature construction plans enhancing utility knns performing constructive induction primarily aimed improving methods extraction rules trained knns working rule-extraction algorithm operates training knn allowing link weights freely arbitrary values algorithm periodically rounds link weight member predetermined small set values undergoing transitions interpretable set rules black-boxish knn back interpretable set rules rounding algorithm preserve comprehensibility knowledge base training conclusions method neural networks perform constructive induction kbann system form knowledge-based neural network significantly reduced difficulty learning interpreting intermediate representations formed hidden units technique deriving rules trained knn express information captured network manner easily comprehended humans make interpretation technique specialized computer vision concept recognition cones apply dna sequence analysis recognition cones constrained inductions hidden units domain theory facilitating interpretability constructed rules efforts promote interpretabilityof constructed rules successful trained network constructed rules identified significant aspects domain studied additionally derived rules easily comprehensible averaging fewer antecedents rules extracted trained network constructed rules play integral part accurate sets testing examples networks extracted acknowledgements work partially supported office naval research grant -jand national science foundation grant iriand department energy grant de-fg atlas atlas cole connor el-sharkawi marks muthusamy barnard performance comparisons backpropagation networks classification trees real-world applications advances neural information processing systems volume pages denver dietterich dietterich hild bakiri comparative study backpropagation english textto-speech mapping proceedings seventh international conference machine learning pages austin fisher fisher mckusick empirical comparison back-propagation proceedings eleventh international joint conference artificial intelligence pages detroit honavar honavar uhr network neuron-like units learns perceive generation reweighting links hinton sejnowski touretzky editors proceedings connectionist models summer school pages morgan kaufmann san mateo kolen kolen pollack backpropagation sensitive initial conditions advances neural information processing systems volume denver morgan kaufmann noordewier noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences advances neural information processingsystems volume denver morgan kaufmann rumelhart rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland editors parallel distributed processing explorations microstructure cognition volume foundations pages mit press cambridge saito saito nakano medical diagnostic expert system based pdp model proceedings ieee international conference neural networks volume pages sestito sestito dillon multi-layered neural networks learning symbolic knowledge proceedings australian artificial intelligence conference perth australia shavlik shavlik mooney towell symbolic neural net learning algorithms empirical comparison machine learning towell towell shavlik craven interpretation artificial neural networks mapping knowledge-based neural networks rules technical report computer sciences department wisconsin madison towell towell shavlik noordewier refinement approximately correct domain theories knowledge-based neural networks proceedings eighth national conference artificial intelligence pages boston watson watson hopkins roberts steitz weiner molecular biology gene pages weiss weiss kapouleas empirical comparison pattern recognition neural nets machine learning classification methods proceedingsof eleventh international joint conferenceon artificial intelligence pages detroit weiss weiss kulikowski computer systems learn morgan kaufmann san mateo 
appears machine learning proceedings eighth international workshop birnbaum collins eds morgan kaufmann san mateo refining domain theories expressed finite-state automata richard maclin jude shavlik computer science dept wisconsin madison maclin wisc abstract kbann system neural networks refine domain theories domain knowledge kbann expressed nonrecursive propositional rules extend kbann domain theories expressed finite-state automata apply finite-state kbann task predicting proteins fold producing small statistically significant gain accuracy standard neural network approach non-learning algorithm biological literature method shows promise solving real-world problems terms statedependent decisions introduction research artificial neural networks anns recently largely preexisting knowledge task hand recent effort kbann system addresses problem domain knowledge select promising configuration neural network approach domain knowledge determine topology network biases network start good set weights approach effective complex domains gene recognition initial domain theory correct paper discuss kbann domain theory vocabulary extended application problem protein folding demonstrates promise approach present kbann algorithm limited types domain theories represented domain theories written simple non-recursive propositional rules limitation means domain theories complex standard kbann algorithm research extends kbann include domain theories represented finite-state automata result kbann applied complex problems including requiring state-dependent knowledge sample task examine predicting folding globular proteins number reasons chose domain real-world problem present simple solution exists produces highly accurate predictions number attempts apply standard neural network methods comparison purposes finally number domain theories notably chou-fasman method naturally expressed simple propositional rules expressed finite-state automaton section presents kbann method extended finite-state automata experiment involving protein folding paper finishes short discussion conclusions finite-state domain theories section reviews basic kbann algorithm presents method extending finite-state automata show examples standard finite-state kbann standard kbann standard kbann takes domain theory expressed simple rules translates network initial weights pair rules figure figure set propositional rules kbann translation rules kbann translates rules network shown figure positive links solid lines negative links dashed lines binary input units hidden units correspond rules active active active active active unit active hidden unit active domain theory translated network unit connected unconnected units lower level small weight link connections system learn antecedents rules part original domain theory resulting network trained backpropagation backpropagation refines domain theory correctly classify training examples covered finite-state kbann finite-state automaton fsa represented set nodes arcs node state arc transition state fsa beginning start state scans series input values input automaton transitions state examining arcs present state arcs labeled input problems representing fsa kbann represent concepts scanning series input values keeping track state solution problem scan input nettalk system input window values central input obtained advancing window place solve problem current state system input network step suggested jordan elman step window advances state step copied back state copied-back state makes translation fsa simple extension kbann method arc fsa viewed rule antecedents source state input consequent destination state representation fsa neural network similar cleeremanns finite-state kbann translating fsa automaton figure recognizing numbers divisible fsa translated set rules current state input antecedents resulting state consequent finite-state kbann translates fsa figure rules examples shown figure applying kbann rules produces network shown figure network hidden units act and-gates output units or-gates note domain theory figure correct imperfect domain theory transition rules incorrect missing transitions finite-state kbann sequences correct errors finite-state kbann generalized notion input window input including present input number input values present input fsa recognize numbers divisible present state current input state present state current input state present state current input state rules derived state state state input input state state state kbann translation rules figure translation process fsa case study domain protein folding section defines problem predicting protein folding examines standard neural network approaches problem discussion chou-fasman domain theory approach problem report experiments performed evaluate approach protein folding proteins long strings amino acids twenty amino acids represented capital letters amino acids protein primary structure protein protein forms folds threedimensional shape location amino acids shape tertiary structure protein tertiary structure important shape protein determines function present means exist predict tertiary structure protein x-ray crystallography magnetic resonance processes costly time consuming alternative solution determine secondary structure proteins approximation tertiary structure secondary structure description local structure amino acid prevalent description secondary structure divides protein types structures -helix regions -sheet regions random coils regions sample mapping protein primary secondary structure shown figure common method solve problem biologists chou-fasman method shown prediction accuracy sample protein sequences primary vyrnnfksa secondary ccccc figure primary secondary structures sample protein standard neural network approaches predicting secondary structure researchers attempted neural networks solve problem neural networks efforts input window amino acids consisting central amino acid predicted number amino acids sequence window predict secondary structure central amino acid output network unit type secondary structure networks include number hidden units single layer input output units number varied studies general picture type network shown figure output values determined resulting prediction obtained amino acid unit highest activation chosen exceptions made short sequences short sequences sequences amino acids predict helix sheet amino acids length short sequences general occur real proteins predictions replaced coil test set accuracies reported efforts chou-fasman domain theory standard approach biological literature predicting secondary structure chou-fasman method chou-fasman method similar helix sheet prediction helix prediction step find helix nucleation sites nucleation sites small regions amino acids part -helix structures rules reported choufasman small region structure extended forward backward protein long likelihood -helix remains high helix sheet regions predicted relative likelihoods regions compared resolve overlaps input units input windowprimary structure scanning direction output units hidden units helix sheet coil helix sheet coil predicted secondary structure figure general architecture networks general picture finite-state automaton represent domain theory appears figure note notion transition automata complex transition represented set rules applied input window current state determine transition initial state fsa coil sheethelix coil init helix init sheet init sheet init helix break helix break sheet continue helix continue sheet figure fsa represent chou-fasman domain theory sample chou-fasman rules shown figure note rules refer state rules subrules transition rules rule assumption rules incomplete incorrect inconsistent input finite-state kbann takes incorrect transition rules refines correct transition chosen amino acid amino acid helix-former amino acid terminate helix true center amino acid helix amino acids 
helix formers conformation initiate helix present state coil initiate helix true state helix figure rules chou-fasman domain theory recognizing nucleation site depends information input window implemented regular kbann extending sequence hand requires finite-state capability system extending sequence depends recognized nucleation site previous input window sequences extended directions chou-fasman method system scans protein presented scan proceeds forward protein scan backward results combined helices sheets extended directions scans performed results averaged general picture type network translate domain theory shown figure experimental study performed experiments finite-state kbann determine standard anns chou-fasman method predicting secondary structure testing set proteins qian sejnowski data set consists proteins proteins randomly divided training test sets two-thirds proteins one-third proteins original proteins process repeated ten times training sets proteins divided training sets contained proteins contained contained fourth training proteins original chou-fasman domain theory suggests threshold nishikawa suggested nucleation sites chosen higher threshold output units hidden units input units input windowprimary structure copy output input helix sheet coil helix sheet coil scanning direction predicted secondary structure figure general structure networks represent chou-fasman domain theory network trained epochs training set percent correctness recorded standard ann number hidden units networks kbann method results averaged runs training set size shown figure reason accuracy reported standard ann lower reported qian sejnowski recorded highest accuracy achieved system epochs results report accuracy end epochs empirical results figure finite-state kbann shows small increase accuracy standard ann statistical analysis differences prediction accuracy t-test show differences small statistically significant level confidence largest training sets proteins accuracy finite-state kbann standard ann results significant level discussion results presented show improvement standard neural network solutions chou-fasman method gain point fairly small detailed inspection errors made neural nets chou-fasman method show similarities differences approaches poorly cases helix sheet predicted fairly finding approximate location helix -sheet structures determining structures extend neural network techniques predicting coil helix sheet prediction surprising secondary structure data set coil helix sheet roughly equally split remaining instances kbann number training proteins percent correctness chou-fasman standard ann finite-state kbann figure average accuracy test set function training set size solution job predicting helix sheet standard ann solution accounting advantage standard ann method surprising choufasman domain theory focuses predicting helix sheet coil predicted default present approach achieve high accuracy number extensions approach considered initial representation chou-fasman domain theory raise accuracy kbann dependent quality initial domain theory combining chou-fasman techniques lim robson algorithms produce results conclusions finite-state kbann method powerful extension standard kbann technique finite-state kbann represents domain theories expressed finite-state automata state automata remembered neural network copying state back input network step scanning input string fsa handled network scan input activating current input current state determine output state finite-state kbann derives set rules fsa translated neural network state transitions refined set training examples correct training examples allowing domain theories include state-dependent rules kbann extended cover additional realworld applications application finite-state kbann protein structure-prediction problem demonstrates method produce improvement standard neural network methods existing non-learning algorithms chou-fasman work generating domain theories improving network refinement process acknowledgments research partially supported national science foundation grant iriand office naval research grant -jthe authors terrence sejnowski providing protein data testing chou fasman prediction secondary structure proteins amino acid sequence advan enzymol cleeremans servan-schreiber mcclelland finite state automata simple recurrent networks neural computation elman finding structure time cog sci holley karplus protein structure prediction neural network proc nat acad sci jordan serial order parallel distributed processing approach ucal inst cog sci san diego kapsch sander biopolymers lim algorithms prediction -helical structural regions globular proteins mol bio nishikawa assessment secondary-structure prediction proteins comparison computerized chou-fasman method biochim biophys acta noordewier towell shavlik training knowledge-based neural networks recognize genes dna sequences advances neural information processing systems vol morgan kaufmann denver qian sejnowski predicting secondary structure globular proteins neural network models mol bio robson suzuki conformational properties amino acid residues globular proteins mol bio rumelhart hinton williams learning internal representations error propagation parallel distributed processing explorations microstructure cognition volume foundations rumelhart mcclelland mit press cambridge sejnowski rosenberg parallel networks learn pronounce english text complex systems towell shavlik noordewier refinement approximate domain theories knowledge-base neural networks aaai 

output input output input output input output input networks resulting networks original crossed promoters splice junctions ribosome binding sites kbann topgen regent test set error networks considered splice junctions promotersr key topgen standard regent kbann test set error 
learning students improve intelligent tutoring system eric gutstein computer sciences department wisconsin-madison dayton madison gutstein wisc abstract sift self-improving fractions tutor intelligent tutoring system learns interactions students tutors learning module takes transcripts tutoring sessions input analyzes results work modifies knowledge bases domainspecific ways creating tutorial rules extending domain knowledge produce transcripts sift tutors student-simulations interact tutor solve problems sift generates rule hypothesis possibly explain inappropriate wrong actions hypotheses ultimately valid initially assumes rules equal correctness probability continues tutor probabilistic conflict-resolution rule-selection algorithm feedback rule applications modify probabilities dempster-shafer theory evidence initial results show sift learns rules eventually converges correct minimal rule sequences learns type student learning patterns intuitively plausible terms human tutors keywords learning environment intelligent tutoring systems evaluation simulation introduction central question address work intelligent tutoring system learn interactions students improve tutoring time generally machine learn environmental interaction approaches questions exist area self-improving systems built dillenbourg dillenbourg goodyear kimble shea machine learning researchers approached question ways including learning discovery langley lenat learning experimentation carbonell gil mitchell utgoff banjeri shen feedback-driven learning connectionist learning rumelhart mcclelland addressed work correcting imperfect domain theories mitchell keller kedar-cabelli rajamoney rajamoney dejong approach question rule-based examine results tutoring criteria judge effectiveness alter tutoring knowledge learn tutorial rules extend domain knowledge modifications future tutoring modifications made sufficiently general apply potentially future tutoring situations effective systems specialization operators complement order discriminate enlarged rule set apply system continue refine knowledge finely honing rule selection based feedback tutors eventually converge rule sequences rules work situations students paper describe sift self-improving fractions tutor based framework sift implemented production system combines scaled-down tutor learning module improves tutoring practice information flow shown figure sift knowledge bases learning mechanisms rules domain knowledge derived analysis actual transcripts expert tutoring children fractions interviews tutor curriculum experts knowledge math teacher years gutstein additional difficulties designing effective user interface students sift productionsystem student-simulations refer term student paper expert tutor nancy mack mathematics educator faculty northern illinois studied children learn fractions understanding dissertation research provided transcripts mack teaching area fractions students interact tutor solve mis-solve problems occasionally learn produce transcripts tutor learning module evaluation purposes describe results section modify input tutorial utterance student response tutorial rules content knowldge transcript record tutorial utterance student response tutor students learning module input input record record figure sift information flow students tutor produce transcripts input learning module learner subsequently modifies tutorial rules content knowledge learning sift sift learns experience interacting environment environment considered students specifically learns analyzing transcripts sessions created tutor students inputs learning module transcripts tutor rules underlying content domain knowledge encoded semantic net rational task analysis education terminology results previous learning learning goal produce knowledge modifications improve tutoring tutoring complex task recently understood fine-grained level fully implement successful glades lewis mcarthur stasz zmuidzinas mcarthur stasz zmuidzinas ohlsson putnam shavelson webb stasz mcarthur findings researchers tutoring knowledge-intensive task demanding domain expertise attempt improve depend heavily domain knowledge research teaching addition tutoring supports contention corno snow shulman examples sift relies knowledge creates tutorial rules alter actions tutor extends content knowledge intelligent tutoring system improve general evaluating empirically effectiveness knowledge-base modifications sift conflict-resolution algorithm choose tutorial rule execute enabled algorithm includes mechanisms primary choice method based rule probabilities updated sift learning based success failure rule application criteria success failure rule application domain-dependent issue updating probabilities general method modifying knowledge bases describe fully table abstract description sift learning module inputs learning module recording interactions goal-directed rule-based agent probabilistic conflict-resolution algorithm external world agent knowledge base consisting rules controlling actions semantic net domain knowledge learning system actions analyze record evaluate individual sets actions respect contribute detract agent goals modify agent knowledge bases domain-dependent ways create rules extend agent semantic net empirically evaluate prior knowledge modifications domain-independent update rule probabilities dempster-shafer reasoning gordon shortliffe table abstract description sift learning module sift learns rules variety situations general precondition sift learn judge tutor inappropriate tutorial action situation learns primary list instances learns evaluation opportunistic digressions digression tutor decides subject taught switches topics sift evaluates digressions successful determines tutor departure future lewis discuss depth digressions obvious mack transcripts student response tutor expectation-set expectation-set set incorrect variableized answers sift stores chunk problem-type student responses tutor believes understands underlying misconception instance suppose tutor common error adding numerator denominator adding fractions expectation-set student answer problem premise tutor students learn understands student error learning sift extends expectation-set answer similar model-tracing anderson evaluation rule sequences tutoring goal achieved implicit tutoring goal students solve problem correctly make progress solutions break sub-categories tutor attempts student generalize concept procedure table inductive leap large early student sequence problems helps generalize gutstein shavlik problem sequence varies dimensions student unable handle mack transcripts student correctly solved problem type integer-minus-fraction problem mack gave type mixed-minus-mixed form operands problem integer mixed number fraction mixed number added complexity insufficient scaffolding cog apprenticeship ref student handle tutor presents problem symbolic terms student informal intuitive knowledge concepts sufficiently developed recognized problem mathematics education hiebert carpenter student insufficiently learned prerequisite knowledge tutor presents compound problem type operator student broken component parts describe learning mechanisms fully gutstein detail sift learns transcript compound problem discuss results learning tests type problem table transcript portion mack tutoring compound problem derived preconditions sift rules actions takes learns call problem type problem mack student encoded sift divide parts parts case mack answer sift reviews transcript notices problem times increasingly explicit hints learning rules fire rules depends partly sift current student-model tutor rates student knowledge prerequisite chunks partly sequence taught chunks shown figure rule corresponds hypothesis student failed answer problem correctly sift generates hypotheses student insufficiently learned prerequisite taught longest ago implying memory lapse decaying factors student insufficiently learned prerequisite taught recently implying insufficiently encoded student insufficiently learned lowest-rated prerequisite chunk student needed problem decomposed order understand solve hypothesis reasonable general depend student subject matter taught absence information determines unequivocally correct unhypothesized reason tutor actions appeared anomalous reasons fact sift discuss results section problem-decomposition hypothesis sift creates rules rules based compound problem case rule creates problem sift number hints increases final time problem hint number threshold allowable hints triggers learning rules claim 
exhaustive hypothesis set mathematics educations describe set scenario lend easily buggy diagnosis burton brown write production rules mal-rules produce student errors stage explain content interaction depend language situation context prior knowledge factors addition intent buggy diagnosis provide remediation framework burton brown claim inferences confirmed experts mathematics learning event add additional hypotheses sift simple due production system knowledge representation tutor suppose pizza cut pieces question parts-how big part pieces altogether mack answer student tutor pizza re-ask problem focuses part clarify--ie hint student fourth tutor fourths re-ask increases hint level--actually half answer student pause tutor asked student wanted manipulatives sized pieces piece pizza decomposes original problem asks part student tutor pieces answer part original problem question part original problem student pause fourths pieces fourths put fourths tutor asked student pizza cut pieces pizza inductive step--asks student begin generalizing solution student fifths pieces tutor asked cut pieces continues student generalize inductive leap student eighths pizza pieces eighths table tutoring interaction tutorial goal solving problem angle bracketed statements comments mack operator compound problem division applied operands rule preconditions rule gave compound problem ensures rule enabled time original rule similar situation problem type divide-units pizza cut pieces big piece sift takes problem simulates working memory contents ideal student solve preconditions rule created produces problem operator operands result problem correctly solved operand original problem problem type multinverse piece pizza fourth pizza pieces pizza altogether sift simulates working memory solving preconditions final rule key sibling means related child-of pre-requisite preceeds add-unlikenot-rel-prime compounddiv-then-mult add-unlikerel-prime add-likedenom-frs add-unlikeone-is-factor mult-fractby-inverse divide-units figure portion task analysis sift tutors problem starting add-like-denom-frs sift add-unlike-one-is-factor created rule original compound problem immediately preceding problems considered building-block chunks compound student correctly solved building-block chunk information encoded part task analysis table summarizes rule created preconditions actions rule preconditions original rule created problem simulates solving problemcontents lhs simulates solving problemcontents lhs problem-types problemas prerequisites action operator operands produce problem problemtake operator problems solution operand produce problemgive original problem table summary rules created problem rules created hypotheses listed rules added sift tutorial knowledge base rules created hypothesis enabled time rules make conflict-set tutorial rules enabled isomorphic situation table lists rules original rule conflict-set t-rulean original rule shifts chunks student solves problems type decomposing-rulethe rules learned problem-decomposition hypothesis lowest-rated-chunktable original rule newly created rules making conflict-set learning instance human tutor prefer hypothesis reason knowledge sift initially assumes equal probabilities hypothesis rule situation sift empirical verification rule isomorphic situation feedback alter rule probabilities dempster-shafer theory evidence gordon shortliffe dempster-shafer theory mathematically-based generalization bayesian probability theory purpose proposed mycin incrementally narrow hypothesis set time gathering evidence altering hypotheses probabilities sift rule corresponds hypothesis rule firing evidence hypothesis correctness evidence gathering process sift accumulation results rule firings hypothesis set represented enabled rules table note includes original rule sift alters rule probabilities dempster-shafer updating rule rule correctly fires student solves problem rule produced sift increases probability lowers rules scenario turns hypotheses reteaching recently taught chunk reteaching lowest rated prerequisite chunk create identical rules sift create identical rules rules created hypotheses implemented mycin longer time authors proposed application conflict-set reverse takes place rule fires incorrectly rule probability hypothesis set reaches converges sift stops altering probabilities set important aspect dempster-shafer theory confidence expressed non-singleton hypotheses set tutor student insufficiently prerequisite initially non-singleton set hypotheses probability important sift rule selection process shown figure rule fires incorrectly sift increases probabilities rules hypothesis set aggregate differentiating results real world human tutor appropriately assign changing probabilities based specific situation figure sample hypothesis space probabilities assigned non-singleton subsets term represent set hypotheses addition dempster-shafer theory defines plausibility probability hypothesis set amount certainty assigned hypothesis negation doubt ideas assigning probabilities non-singleton hypothesis-sets plausibility affect sift tutor chooses rule fire sift generates random number maps linear sequence hypothesis sets shown figure random number falls singleton subset sift fires rule falls rule chooses rule maximum plausibility note rule maximum probability random number generated sift fire plausible rule case rule probability higher figure hypothesis space figure presented linearly random number sift fire rule plausible rules raw score higher results evaluate self-improving test learning learn test learning measure learning human students learn sift usable people student-simulations evaluate evaluate sift tutor set students record results sift learn transcripts re-tutor isomorphic set students probabilistically varied original measure effectiveness define evaluation methods discuss learning results sift tutoring chunks shown figure sift learns sessions end students unsuccessfully solving compound-dividethen-mult problem students unable solve problem type sift hints hint threshold learning module takes transcripts creates tutoring rules table sift continues tutor rules originals gradually converges correct rules dempster-shafer probability updating test categorized students groups sift tutors students randomly choosing student group group students correctly solve problem tutored similar mack tutoring table divide-units problem immediately multinverse problem immediately problem divide-units mult-inverse problems derived problem group students solve problem receive sufficient practice student-simulations seventeen rules rules solve problems correctly incorrectly student rules applicable problem type assigned probabilities sum prerequisite chunks matter students groups solve compound problem correctly probability sift learns give extra practice divide-and-conquering assistance initially groups solve compound problems wrong reason learns sift tutors giving explicit hints student problem wrong give set number hints initially give extra practice prerequisite chunk break problem parts learn things sift tutors student solve problem learns rules table adds rule base continues teaching shown figure paths reach compound-dividethen-mult starting add-like-denom-frs sift learns sets rules depending path approaches depending type student tutors rule set enabled situations main distinction rule sets conditions applicability group students sift converges sequences tutoring average students trials cases sift converged incorrect rules happened probability students correctly solving compound problem presented divide-units mult-fraction-by-inverse problem probabilities greater rule sequences sift converged group students group results complicated sift extra practice chunk interplay rules tutoring student group learning sift tutor switches problem types student answers consecutive problems row type t-rulementioned table tutor providing extra practice prerequisite chunk problems row switching leaves room tutor explore problem combinations final chunk path reached figure sift takes longer time converge correct rule sequence average number students converge students group students trials table summarizes results chunks necessarily actual prerequisite chunks purposes evaluation experimented 
probabilities ranging student average number students taught convergence number trials group group table learning results tutoring problem sift tutors groups students learning test sift learned rule sequences group students lowest-rated-rulet-rule- t-rulelowest-rated-rule- lowest-rated-rulet-rule- t-ruleis original rule sift converged slowly sequence students tutored probability incorrectly solving compound problem receiving extra practice preceding chunks point sift gave student preceding problems student failed correctly solve compound problem caused additional rules learned delayed convergence based situations occurring time sift treats rules equally learn experience rules applicable rules caused sift learn ultimately inappropriate rules slowing convergence intuitively makes sense people human tutor student things norm tutor learn worked majority sift learns students probability correctly solving compound problem occur convergence faster sift learns rules natural assumes conformity found people variance students rules expected learn account sift learns rules tutoring groups students starting ten initial rules rules similar varying student model preconditions learns rules early tutoring corresponds human tutor learn lot rules tutoring procedures early differing applicability conditions refine practice settle minimal sufficient set figure shows graph number rules learned ten students tutored sift converges chooses rule sequence found sift selects problem give student stored list eliminating complicated problem generation rules reason start small set tutoring rules empirically valid based students teaching sift subsequently starts tutoring additional students learning patterns learns rules account eventually settles proper rule sequences additional students learning rules converges sift ultimately correct number students taught number rules learned ten students taught figure average rate sift learns rules teaching problems conclusion expert systems including fixed knowledge bases inherently limited human experts learn perform craft learning forms reflective analysis results work important theories tutoring developed foundations future researchers acknowledge expertise knowledge involved tutoring mcarthur important integrate machine learning theories educational build teaching systems improve age based knowledge actions expert human tutor machine learning techniques methods sift represents attempt bridge fields anderson cognitive psychology intelligent tutoring proceedings cognitive science society conference boulder carbonell gil learning experimentation proceedings international machine learning workshop irvine california irvine corno snow adapting teaching individual differences learners wittrock handbook research teaching york macmillan press dillenbourg pragmatic approach student modeling principles architecture proto-teg proceedings european seminar lemans france dillenbourg self-improving tutoring systems international journal educational research dillenbourg goodyear reflective tutoring systems self-representation selfimprovement bierman breuker sandberg eds proceedings international conference education amsterdam gordon shortliffe dempster-shafer theory evidence buchanan shortliffe eds rule-based expert systems mycin experiments stanford heuristic programming project reading addison-wesley gutstein expert tutor knowledge design self-improving intelligent tutoring system frasson gauthier mccalla eds lecture notes computer science proceedings international conference intelligent tutoring systems berlin germany springer-verlag gutstein sift self-improving fractions tutor doctoral dissertation preparation madison wisconsin-madison gutstein shavlik choosing problem evoke students strategies working notes aaai spring symposium knowledge-based environments learning teaching stanford stanford hiebert carpenter learning teaching understanding grouws handbook research mathematics teaching learning york mcmillan kimble self-improving tutor symbolic integration sleeman brown eds intelligent tutoring systems london academic press langley data-driven discovery physical laws cognitive science lenat ubiquity discovery artificial intelligence lewis mcarthur stasz zmuidzinas discovery-based tutoring mathematics working notes aaai spring symposium knowledge-based environments learning teaching stanford stanford mack learning fractions understanding unpublished doctoral dissertation madison wisconsin mack learning fractions understanding building informal knowledge journal research mathematics education mcarthur stasz zmuidzinas tutoring techniques algebra cognition instruction mitchell keller kedar-cabelli explanation-based generalization unifying view machine learning mitchell utgoff banjeri learning experimentation acquiring refining problemsolving heuristics michalski carbonell mitchell eds machine learning artificial intelligence approach palo alto morgan kaufmann shea self-improving quadratics tutor sleeman brown eds intelligent tutoring systems london academic press ohlsson principles intelligent tutoring instructional science putnam teachers thoughts actions live simulated tutoring addition unpublished doctoral dissertation stanford stanford putnam structuring adjusting content students study live simulated tutoring addition american educational research journal rajamoney dejong classification detection handling imperfect theory problems proceedings tenth ijcai conference milan italy rajamoney explanation-based theory revision approach problems incomplete incorrect theories unpublished doctoral dissertation champaign illinois rumelhart mcclelland pdp research group parallel distributed processing exploring microstructure cognition cambridge mit press shen learning environment based percepts actions unpublished doctoral dissertation pittsburgh carnegie-mellon shavelson webb stasz mcarthur teaching mathematical problem solving insights teachers tutors charles silver eds teaching assessing mathematical problem solving reston erlbaum associates nctm shulman paradigms research programs study teaching contemporary perspective wittrock handbook research teaching york macmillan press 
location direction footprint highest queue true stop footprint pushed time footprint pushed time location direction stop stop location direction stop continue 
introduction artificial intelligence mid exam october room stats closed book sheet notes calculator allowed write answers pages show work feel question fully state assumptions make order solve problem backs sheets scratch work write pages exam make exam problems ten pages student problem score max score total problem decision trees points imagine recognize good art features written program measure numeric properties piece art code determine most-used primary color red blue yellow set training examples appears color result good bad good bad good method discretize continuous features divide bins low high complete reformulated table briefly explain work table color result explanation reformulation good bad good bad good score information gain calculation assign features show work back previous sheet needed feature chosen root decision tree built break ties favor color show interior node algorithm add decision tree show work secod interior node completely separate training data stop adding node label leaf nodes decision tree created assuming tuning set pruned tree pruning algorithm produce round pruning justify answer color result good bad bad problem search points search space start node satisfy goal test arcs labeled cost traversing estimated cost goal reported inside nodes search strategies goal state reached list order states popped open list equal nodes removed open alphabetical order uniform cost goal state reached states popped open iterative deepening goal state reached states popped open function goal state reached states popped open beam beam width function goal state reached states popped open goal state reached states popped open problem representation logic points convert english sentences first-order predicate calculus fopc named predicates functions constants feel sentence ambiguous clarify meaning representing logic write answers space english sentence mary tall bill dogs tiny picasso paintings valuable houses sue house large problem reasoning logic points wff valid justify answer provide justify formal interpretation makes wff true formally show don deduce additional wff number wff justification problem miscellaneous short answers points briefly describe concepts explain significance write answers phrases heuristic functions occam razor quantifiers a-b pruning problem game playing points game turn move players choose weighted coins flip coin heads time tails heads player make move tails make move problem needn move means coin heads time tails heads players make move tails make move assume computer turn play game tree values leaf nodes results calls sbe higher scores computer explain move computer make hint expected-value calculations parts btat btat btat btat assume randomness players simply choose moves apply minimax algorithm tree explain move computer make part assume computer turn play assuming leaf nodes visited left-to-right identify unnecessary call sbe randomness case explain answer btat btat btat btat 
page knowledge-based support vector machine classi ers glenn fung olvi mangasarian jude shavlik computer sciences department wisconsin madison gfung olvi shavlik wisc abstract prior knowledge form multiple polyhedral sets belonging categories introduced reformulation linear support vector machine classi resulting formulation leads linear program solved ciently real world examples dna sequencing breast cancer prognosis demonstrate ectiveness proposed method numerical results show improvement test set accuracy incorporation prior knowledge ordinary data-based linear support vector machine classi ers experiment shows linear classi based solely prior knowledge outperforms direct application prior knowledge rules classify data keywords nement prior knowledge support vector machines linear programming introduction support vector machines svms played major role classi cation problems unlike classi cation tools knowledge-based neural networks work incorporating prior knowledge support vector machines work present approach incorporating prior knowledge form polyhedral knowledge sets input space data knowledge sets simple cubes supposed belong categories data divided single knowledge set interpreted generalization training typically consists single point input space contrast knowledge sets consists region space powerful tool mathematical programming theorems alternative chapter embed prior data linear program ciently solved publicly solvers brie summarize contents paper section describe linear support vector machine classi give linear program describe prior knowledge form polyhedral knowledge sets belonging classes characterized section incorporate polyhedral sets linear programming formulation results knowledge-based support vector machine ksvm formulation formulation capable generating linear classi based real data prior knowledge section summary numerical results compare linear nonlinear classi ers incorporation prior knowledge section concludes paper page describe notation vectors column vectors transposed row vector prime scalar product vectors n-dimensional real space denoted vector sign function sign ned sign sign kxkp denotes p-norm notation signify real matrix matrix denote transpose denote i-th row vector real space arbitrary dimension denoted notation denote sum components vector zeros real space arbitrary dimension denoted identity matrix arbitrary dimension denoted separating plane respect point sets plane attempts separate halfspaces open halfspace points bounding plane set plane places closed halfspaces plane generates symbol denote logical abbreviation stands linear support vector machines prior knowledge problem depicted figure classifying points n-dimensional input space represented matrix membership point class speci diagonal matrix minus diagonal problem linear programming support vector machine linear kernel variant standard support vector machine linear program parameter min kwk denotes -norm ned introduction vector slack variables measuring empirical error characterize separating plane depicted figure problem linear program easily equivalent formulation min vector dimension economy notation rst formulation understanding computational implementation depicted figure normal bounding planes bound points belonging sets constant determines location relative origin classes strictly linearly separable error variable case shown figure plane bounds class points plane bounds class points aiw dii aiw dii plane midway bounding planes separating plane separates points belonging belonging completely approximately -norm term kwk half reciprocal distance kwk measured -norm distance bounding planes page figure maximizes distance called margin maximizing margin enhances generalization capability support vector machine classes linearly inseparable planes bound classes soft margin bound approximately error determined nonnegative error variable aiw dii aiw dii -norm error variable minimized parametrically weight resulting approximate separating plane classi sign sign suppose prior information type points lying polyhedral set determined linear inequalities belong class inequalities generalize simple box constraints figure inequalities conclude implication hold knowledge set lies side bounding plane accommodate case implication satis introduction slack error variables assuming implication holds equivalent solution statement turn implied statement solution simple backward implication suppose contrary exists satisfying obtain contradiction rst inequality premultiplying fact natural assumption prior knowledge set nonempty forward implication true direct consequence nonhomogeneous farkas theorem alternative theorem state equivalence key proposition knowledge-based approach proposition knowledge set classi cation set nonempty implication equivalent statement words set lies halfspace exists proof establish equivalence showing equivalence nonhomogeneous farkas theorem theorem equivalent solution solution alternative contradicts nonemptiness knowledgeset set solving contradiction ruled equivalent proposition play key role incorporating knowledge sets categories support vector classi formulation demonstrated section page margin figure linear svm separation points linear programming formulation linear svm separation points figure linear programming formulation incorporates knowledge sets halfspace halfspace depicted note substantial erence linear classi ers gures knowledge-based svm classi cation describe incorporate prior knowledge form polyhedral sets linear programming svm classi formulation assume knowledge sets sets belonging bix big sets belonging cix cig proposition relative bounding planes exist incorporate knowledge sets svm linear programming formulation classi adding conditions constraints min kwk linear programming formulation ensure knowledge sets bix big cix cig lie side bounding planes guarantee bounding planes exist precisely separate classes knowledge sets priori guarantee original points belonging sets linearly separable add error variables page slack error variable svm formulation attempt drive error variables modifying formulation min kwk nal knowledge-based linear programming formulation incorporates knowledge sets linear classi weight empirical error term weight usual parameters chosen means tuning set extracted training set set linear program degenerates linear program ordinary linear svm set linear program generates linear svm strictly based knowledge sets speci training data paradigm situations training datasets easily expert knowledge doctors experience diagnosing diseases readily demonstrated breast cancer dataset section note -norm term kwk replaced half -norm squared kwk usual margin maximization term ordinary support vector machine classi ers linear program quadratic program typically takes longer time solve standard svms support vectors consist data points complement data points dropped problem changing separating plane knowledge-based linear programming formulation support vectors correspond data points rows matrix lagrange multipliers nonzero solving data points give answer solving entire matrix concept support vectors modi knowledge sets knowledge set represented matrix row matrices thought characterizing boundary plane knowledge set formulation rows wiped components variables optimal solution call complement components knowledge sets support constraints deleting constraints rows components alter solution knowledge-based linear program fact corroborated numerical tests carried deletion non-support constraints considered nement prior knowledge type nement prior knowledge occur separating plane intersects knowledge sets case plane added inequality knowledge set intersects illustrated demonstrate geometry incorporating knowledge sets synthetic points figure depicts ordinary linear separation linear svm formulation incorporate knowledge sets problem page belonging belonging solve linear program depict linear separation figure note substantial change generated linear separation incorporation knowledge sets 
note plane intersects knowledge set knowledge set ned numerical testing numerical tests detail carried dna promoter recognition dataset wisconsin prognostic breast cancer dataset wpbc ftp ftp wisc math-prog cpo-dataset machinelearn cancer wpbc brie summarize results rst dataset promoter recognition dataset domain dna sequence analysis promoter short dna sequence precedes gene sequence distinguished nonpromoter promoters important identifying starting locations genes long uncharacterized sequences dna prior knowledge dataset consists set prior rules matches examples training set rules serve classi capture signi information promoters incorporating classi results accurate classi prior rules converted straightforward manner knowledge sets methodology prior work tested algorithm dataset knowledge sets leave-one-out cross validation methodology entire training set elements repeatedly divided training set size test set size values ksvm svm obtained tuning procedure consisted varying square grid expressing prior knowledge form polyhedral sets applying ksvm obtained errors ksvm gave performance erent methods prior knowledge standard -norm support vector machine quinlan decision tree builder pebls nearest algorithm empirical method suggested biologist based collection lters promoter recognition neill method neural networks simple connected layer hidden units trained back-propagation ksvm svm results earlier report ksvm compared hybrid learning system maps problem speci prior knowledge represented propositional logic neural networks nes reformulated knowledge back propagation method knowledge based arti cial neural networks kbann kbann approach performed slightly algorithm obtained misclassi cations compared important note classi simpler linear classi sign neural network classi kbann considerably complex nonlinear classi note ksvm simpler implement kbann requires commonly linear programming solver addition ksvm linear support vector machine classi improves error ordinary linear -norm svm classi utilize prior knowledge sets dataset numerical tests wisconsin breast cancer prognosis dataset wpbc -month cuto predicting recurrence nonrecurrence disease prior knowledge utilized experiment consisted prognosis rules doctors depended features dataset tumor size feature diameter excised tumor page centimeters lymph node status refers number metastasized axillary lymph nodes feature rules recur nonrecur important note rules applied directly classify points training dataset correctly classify points remaining points classi rules rules applied classi classi cation accuracy rules doctors conjunction rules approach rules converted linear inequalities ksvm algorithm data linear program resulting linear classi -dimensional space ymph umor achieved accuracy ten-fold cross-validated test set correctness achieved standard svm data result remarkable knowledge-based formulation applied problems training data expert knowledge readily form knowledge sets fact makes method considerably erent previous hybrid methods kbann training examples needed order prior knowledge training data added knowledge-based formulation noticeable improvement obtained conclusion future directions proposed cient procedure incorporating prior knowledge form knowledge sets linear support vector machine classi combination dataset based solely knowledge sets promising approach handling prior knowledge worthy study ways handle simplify combinatorial nature incorporating prior knowledge linear inequalities class future applications problems training data easily expert knowledge readily form knowledge sets correspond solving knowledge based linear program typical type breast cancer prognosis knowledge sets generated linear classi good classi based data points incorporating prior knowledge powerful support vector machine classi ers concept support constraints discussed end section warrants study lead systematic simpli cation prior knowledge sets avenues research include knowledge sets characterized nonpolyhedral convex sets nonlinear kernels capable handling complex classi cation problems incorporation prior knowledge multiple instance learning lead improved classi ers eld acknowledgments research data mining institute report november supported nsf grants ccriri- cdaby afosr grant nlm grant microsoft auer learning multi-instance examples empirical evaluation theoretical approach pages bradley mangasarian feature selection concave minimization support vector machines shavlik editor machine learning proceedings fifteenth international conference icml pages san page francisco california morgan kaufmann ftp ftp wisc mathprog tech-reports cherkassky mulier learning data concepts theory methods john wiley sons york cost salzberg weighted nearest neighbor algorithm learning symbolic features machine learning dietterich lathrop lozano-perez solving multipleinstance problem axis-parallel rectangles arti cial intelligence fung mangasarian shavlik knowledge-based support vector machine classi ers technical report data mining institute computer sciences department wisconsin madison wisconsin november ftp ftp wisc pub dmi tech-reports girosi chan prior knowledge creation virtual examples rbf networks neural networks signal processing proceedings ieee-sp workshop pages york ieee signal processing society lee mangasarian wolberg survival-time classi cation breast cancer patients technical report data mining institute computer sciences department wisconsin madison wisconsin march computational optimization applications ftp ftp wisc pub dmi tech-reports mangasarian nonlinear programming siam philadelphia mangasarian arbitrary-norm separating plane operations research letters ftp ftp wisc math-prog tech-reports mangasarian generalized support vector machines smola bartlett sch olkopf schuurmans editors advances large margin classi ers pages cambridge mit press ftp ftp wisc math-prog tech-reports neill escherchia coli promoters concensus relates spacing class speci city repeat substructure dimensional organization journal biological chemistry quinlan induction decision trees volume rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland editors parallel distributed processing pages cambridge massachusetts mit press sch olkopf simard smola vapnik prior knowledge support vector kernels jordan kearns solla editors advances neural information processing systems pages cambridge mit press towell shavlik knowledge-based arti cial neural networks arti cial intelligence towell shavlik noordewier nement approximate domain theories knowledge-based arti cial neural networks proceedings eighth national conference arti cial intelligence aaaipages vapnik nature statistical learning theory springer york edition 
introduction artificial intelligence final exam room noland closed book sheets notes calculator allowed write answers pages show work feel question fully state assumptions make order solve problem backs sheets scratch work write pages exam make exam problems pages student problem score max score total problem first-order predicate calculus points convert english sentences first-order predicate calculus named predicates functions constants feel sentence ambiguous clarify meaning representing logic taller mary red blocks weigh blue billg person exists richer give situation calculus borrowing book doesn change owns situation calculus problem production systems points production-system rules listed variables leading terms constants conflict-resolution strategy choose rule preconditions extra constraint rule variable bindings predicate procedurally defined production system negation-by-failure assert retract retract print success assert initial contents working memory fill table showing cycles production system operation cycle matching rules chosen rule iii list net working memory result cycles problem bayesian case-based reasoning points imagine recognize bad widgets produced factory measure numeric properties widget property discretized low normal high randomly grab widgets assembly line extensively test good obtaining results result good bad good bad good explain data bayes rule determine widget good bad show work explain assumptions simplifications make case-based reasoning system address task handle test problem neural networks points -of-n encoding show perceptron configured address widget task problem initialize weights threshold show apply delta rule training examples problem show state perceptron step function activation function good bad set learning rate adjust threshold training assuming weights threshold training set problem draw weight space weight input node output node problem genetic algorithms points fitness function fitness a-e boolean-valued parameters compute fitness members initial population compute probability member population selected fitness-proportional reproduction process fitness reproduction probability assuming members population selected reproduction cross-over point show resulting children problem miscellaneous short answers points briefly describe concepts heuristic functions hidden markov models hmms linear separability problem markov models points assume user user equally share computer write program determines person computer choose create first-order markov model characterizes user typing behavior decide group keystrokes classes estimated transition probabilities producing graphs users start state logging imagine current user logs immediately types iou current user show explain calculations letterdigit user letterdigit user problem gradient descent error function points derive weight-change rule perceptron error function activation function output unit identity function output equals incoming weighted sum threshold involved error output 
wisconsin machine learning research group working paper september improving efficiency belief propagation large highly connected graphs frank dimaio jude shavlik computer sciences dept wisconsin madison madison dimaio shavlik wisc abstract describe part-based object-recognition framework specialized mining complex objects detailed images objects modeled collection parts pairwise potential function algorithm key component efficient inference algorithm based belief propagation finds optimal layout parts input image belief propagation message passing method approximate inference graphical models suited task large objects parts intractable present aggbp message aggregation scheme groups messages approximated single message producing message update analogous mean-field methods objects consisting parts reduce cpu time memory requirements apply aggbp real-world synthetic tasks framework recognize protein fragments three-dimensional images scaling task average-sized proteins infeasible enhancements synthetic object generator test algorithm ability locate wide variety part-based objects experiments show improvements result minimal loss accuracy cases produce accurate solution standard introduction based models recognizing generic objects images models represent physical objects graph collection vertices parts connected edges enforcing pairwise constraints inference algorithm determines probable location part model image previous work considered simple objects parts two-dimensional image data present part-based object recognition algorithm specialized objects hundreds parts detailed three-dimensional images rich three-dimensional data commonly arises biological datasets recent advancements biological imaging techniques fmri scans produce detailed images brain confocal microscopy constructs high-quality images tissues ray crystallography yields electron density map three-dimensional image macromolecule threedimensional data objects comprised parts connected complex topology detailed biological imagery easier acquire techniques accurately interpret images needed vascular biologist automatically locate blood vessels kidney section crystallographer trace piece rna electron density map rich two-dimensional data detailed satellite imagery complex objects interpreted current methods effectively mine complex objects algorithm includes efficient message-passing inference algorithm based belief propagation message-passing algorithms extremely powerful tool inference graphical models probabilistic models defined graph belief propagation sum-product algorithm message-passing method computing marginal distributions tree-structured graphs graphs arbitrary topologies optimality guaranteed empirically loopy extremely accurate approximations exact inference methods intractable large highly-connected graphs large input images loopy offer efficiency near-fully connected graphs hundreds thousands vertices approximations messages compute marginal distributions reasonable amount time describe aggbp aggregate technique approximating groups messages single message composite message turns similar message update mean-field methods illustrate types graphs aggbp reduce running time near-fully connected graph nodes additionally provide method dealing continuously-valued variables efficient require accurate initialization recently extension nonparametric belief propagation nbp introduced nbp represents variables continuous nongaussian distributions mixture gaussians introduce efficient variant alternately represents probability distributions continuous three-dimensional space set fourier-series coefficients describe efficientmessagepassingandupdatealgorithmsinthisframe- work finally test approximation techniques real-world synthetic data testbed realworld computer-vision task identifying protein fragments three-dimensional images interpreting protein images important step determining protein structures x-ray crystallography aggbp lets scale interpretation large proteins large images testbed synthetic object generator test aggbp performance locating wide variety objects part topologies modeling objects describe class objects graphical model graphical models bayesian networks markov fields represent joint probability distribution set variables function defined graph pairwise undirected graphical model pairwise markov field represents joint distribution product potential functions defined edge vertex graph represent object graph vertices correspond parts object edges correspond constraints pairs parts formally graphg consists set nodes connected edges node graph hidden random variablexs graph conditioned set observation variables object recognition position part vertex observation potential edge structural potential represent full joint probability productdisplay productdisplay applications paper included concernedwithfindingthemaximummarginalassignment labelsxs maximize joint probability describe object graphical model provide pieces data part graph node observation potential edge structural potential graph describing object potential functions learned set previously solved problem instances object recognition part graph fully connected edges identical weak diffuse potentials sparse subset graph skeleton connects highly correlated variables illustration graphical model recognizing people images figure model sparsely connected skeleton connects highly correlated nodes head body connected skeletal structure position head highly correlated position body pairs nodes left leg left arm connected skeletal structure labels completely conditionally independent weak correlation parts occupy location space constraint implicitly modeled chain connects skeletal structure edge occupancy edges serve ensure parts inthemodeldonotoverlapin dspace forexample modeling hand occupancy edges required ensure fingers occupy space potential edges typically diffuse non-zero small neighborhood origin node local coordinates part observation potential based application simple classifier location space returns probability part location individual part potential functions template matching color matching edge detection method observation potentials accurate belief propagation infer true location combined power weak detectors illustrated person-detector structural potentials broken types skeletal potentials sequential potentials model relationship parts connected object skeleton occupancy potentials model weakly correlated relationship pairs nodes skeletal potentials arbitrary form learned set allowable object configure sample graphical model recognizing person image thicker dark edges illustrate highly-correlated skeleton model parser thinner language light fuzzy edges logic interface weakly-correlated advice occupancy mapper edges rule ensure network ratle parts advice occupy reformulatedadvice observer agent behavior translator space formations function position orientation object occupancy potentials form step function hard collision model sigmoidal function soft collision model occupancy potentials depend position connected objects nonzero connected objects sufficiently inference locating objects image object graphical model inference attempts find most-probable location object parts image object graph fully connected number loops exact inference methods work tree-based methods intractable exhaustive methods forced rely approximate inference methods object-recognition framework belief propagation message-passing approximate inference algorithm belief propagation belief propagation based pearl polytree algorithm computes marginal probability location part passing series local messages marginal probability refers joint probability variable summed summationdisplay summationdisplay summationdisplay summationdisplay marginal distribution important information distribution variable algorithm belief propagation input observational potentials structural potentials output approximation marginal summationdisplay summationdisplay summationdisplay summationdisplay initialize accumulator messages converged foreach parts foreach partt iftnegationslash sand updated mnt integraltextx bntmn dxt end mnt end end end full joint distribution requiring explicitly compute possibly intractable full joint distribution pseudocode appears algorithm iteration sages passes convolution product neighbors clarity message dependence dropped mnt integraldisplay productdisplay dxt sdenotes neighbors oftin graph excluding typically messages normalized probabilities sum unity koller assign order nodes update belief node sequentially alternating forward backward passes ordering iteration product incoming messages node observation potential bns productdisplay mnu tree-structured graphs graphs cycles algorithm exact tasks limitation overly restrictive graphs arbitrary topologies guarantees convergence algorithm convergence correct solution empirical results show loopy produces good estimates practice papers explored circumstancesunder loopy convergence 
optimality guaranteed weiss shown category graphical models single loops optimality guaranteed recent work shown existence fixed-points loopy unique optimal heskes developed sufficient conditions uniqueness fixed-points characterized fixedpoints loopy explored message approximation loopy exact message computation intractable stochastic approximation messages messagesimplification havebeeninvestigated additionally dealing continuous-valued variables sort approximation simplifying assumptions made ihler explored consequences approximating messages placing bounds accumulated message errors progresses recent paper investigates special case labels continuously valued ideas particle filtering authors weighted-gaussian probability density estimates set weights wis set gaussian centers covariance matrix estimate belief bns summationdisplayw message computation implemented efficient gibbs sampling routine gibbs sampler approximates product gaussian mixtures components anm-component mixture sampling compute message products convolution operation forward sampling employed inference algorithm applied vision tasks isard makes similar technique sampling routine specialized mixture-of-gaussian edge potentials sections provide techniques scale belief propagation part-based object recognition techniques message aggregation handle large highly connected graphs arise mining complex images include alternate representation continuously-valued beliefs potentials efficient message computation products scaling belief propagation belief propagation originally intended small sparsely connected graphs large highly-connected graphs number messages quickly overwhelming make tractable types graphs propose aggbp approximates subset outgoing messages single node single message replacing message computations message aggregation undirected graphical models object recognition pairs nodes skeleton edges highly correlated messages edges high information content important compute messages edges coarse approximations field methods introduce error graphs majority edges occupancy edges enforce constraint parts occupy space potential functions edges weak uniform messages edges carry information edges make approximations full message update overkill formally message update equation alternately written explicit dependence message dropped clarity mnt integraldisplay bnt dxt denominator term serves avoid double-counting feedback making method exact tree-structured graphs loopy graphs feedback graph loops unavoidable messages occupancy edges denominator carries information aggbp drops loss accuracy update equation mean-field theory update mnt integraldisplay bnt dxt assumingthatthestructural potential identical occupancy edges occupancy messages outgoing single node identical remainder section refer approximate messages asmt assuming identical aggbp reduces number occupancy messages computed model withn parts updating belief part requires multiplying incoming occupancy messages times incoming skeletal message n-part model reduce complexity utilize fact node receives broadcast message nodes graph neighbors skeleton graph senda figure message aggregation approximates outgoing messages node single message nonadjacent nodes caching aggregate message products results significant runtime savings ing aggregate messages central accumulator acc nproductdisplay accumulator efficiently update node belief sending single message product occupancy messages figure illustrates aggbp graph chain chain compute product incoming messages accumulator acc acc acc numerators message updates skeletal messages denominators approximated occupancy messages aggbp reduces runtime memory requirements model parts storage benefit appealing space part large storing messages space-prohibitive section closer application case algorithm aggregate belief propagation algorithm initialize accumulatoracc messagesmto converged foreach parts acc acc acc foreach partt skeleton neighbor updated mnt integraltextx bntmn dxt end mnt mnt end end compute composite message mns integraltextx bns update accumulator acc acc mns end end algorithm pseudocode overview aggbp notice argumentsxs andxt dropped clarity progress node node computing outgoing message single node compute significantly fewer composite messages key difference algorithm loop skeleton neighbor graphs part-based object recognition loop rarely entered requiring significantly fewer message calculations finally occupancy edges potential function parts model size aggbp advantage approximation additional approximation error case aggbp simply computes broadcast message part average potential function outgoing mnt integraldisplay summationtextn bnt dxt section explores aggbp handles varying occupancy potentials belief representation section describes approach belief propagation continuous-valued labels based particle filtering cases models insufficient general particle filtering-based methods accurate initialization gaussian accurate inference section describe alternative belief representation fourier-series probability density estimate represent probabilities messages particle-filtering-based methods tend concentrate high-probability space approach accurately represents probability distribution entire space random variable efficient message passingandmessagecomputationmakethisrepresentationideal large highly-connected graphs formally represent marginal distributions bns set -dimensional fourier coefficients upper-frequency limit bns ksummationdisplay pii xssquaresmallsolidk density estimate message convolution recall computingmt requires integrating product edge potential observation potential incoming message product producttextms allxt computation difficult general fourier-based density estimates edge potential represented function difference labels connected nodes convolution mnt parenleftbig productdisplay tparenrightbig easily computed product fourier coefficients fbracketleftbigmnt bracketrightbig fbracketleftbig bracketrightbig parenrightbig bracketrightbig object recognition occlusion potentials represented manner potential depends difference labels computational shortcut originally proposed felzenswalb belief propagation low-level vision computing message products efficient running time high-frequency limit density estimate message products shown computing current belief bns node requires taking product incoming messages multiplying observation potential fourier coefficients messages compute multiplication real space bns productdisplayf message convolution operation fairly efficient transform inverse transform runs time log experiments section compare standard full belief propagation algorithm aggbp real-world andsyntheticdatasets cating protein fragments images objects consist chain parts amino acids synthetic dataset algorithm performance recognizing objects complex part topologies protein fragment identification application object recognition arises x-ray crystallography approach building graphical model protein order identify three-dimensional image three-dimensional images electron density maps produced determining protein structures x-ray crystallography interpreting map illustrated figure final step x-ray crystallography interpretation produces cartesian coordinates atom protein difficult timeconsuming interpret electron-density maps make weeks months crystallographer time find atom protein alternatively backbone trace focuses predicting location keycarbon atom alpha carbon contained amino acid aggbp automatically determine backbone structure electron density map protein sequence task overview protein constructed linear chain amino acids naturally-occurring amino acids consists constant four-atom motif backbone variable sidechain figure illustrates protein structure highlighting backbone sidechains protein recognition difficult reasons electrondensity maps experimentally determined noisy additionally protein chain extremely flexible proteins typically tightly coiled amino acids distant linear chain close three-dimensional space protein chain typically figure overview electron density map interpretation amino acid sequence proteinand adensitymap thecrystallographer sgoalistofind atoms alternatively backbone trace location key carbon atom called amino acid amino end n-terminus carboxyl end c-terminus peptide bond sidechains backbone alpha carbon amino acid residue figure proteins constructed amino acids condensing polypeptide chain chain amino acids illustrated hundreds thousands amino acids density map image copy protein figure shows encoding protein markov field model node represents amino-acid protein label amino-acid consists terms cartesian coordinatesxs amino acid internal parameters internal parameters alternate parameterization rotational parameters bend angle formed consecutive probability distributions cartesian space make fourier-series based 
parameterization outlined section internal parametersqs modeled constant-width gaussians conditioned cartesian coordinates recent paper paper authors describes protein-specific structural observation potential functionsarelearned ala gly lys leu figure protein part graph probability conformation product observation potential sequential potential dark lines occupancy potential light lines rithms map interpretation previous work present results shown describe message approximations aggregation describing contributions briefly review potential functions electron density map interpretation introduced prior article experiments paper node potential function computed matching learned set small-protein-fragment templates electron density map details potential function scope paper edge potential functions basic types sequential edges connect adjacent amino acids potential seqst ensures adjacent amino acids proper distance proper orientation respect proper distance orientation learned set previouslysolved protein structures occupancy edges connect pairs amino acids potential occst ensures amino acids occupy space graph fully connected messages protein fragment length maliz cpu time maliz emor cpu time aggbp cpu time memory aggbp memory figure comparison memory cpu time usage approximate-bp standard continuous three-dimensional probability distributions entire unit cell storage run-time requirements considerable aggbp message aggregation approximation outlined section shortcuts inference medium-sized proteins computationally intractable results section details experiments locating protein fragments electron density maps maps provided crystallographer george phillips uw-madison convoluted maps gaussian simulate poorquality resolution density map resolution automated interpretation methods fail maps previously solved crystallographer giving true solution compare predictions protein provided sequence constructed markov field based sequence exact-bp toaggbp markov field model exact-bp unable scale entire protein amino acids testset compare methods protein fragments advice behavior reinforcement action state learner environment observer amino acids cpu time iteration memory usage techniques illustrated figure actual running-time memory usage dependant size density map wenormalizethesevalues sothatexact-bp stimeand memory usage amino-acids average-sized protein values sec increase fragment length machine began paging figure include time swapping disk larger fragments issue fragment lengths searched fragments length proteins electron-density maps total target fragments fragments chosen roughly corresponded beginning middle end protein chain ran aggbp convergence iterations iteration single forward backward pass protein map reduced electron density map small neighborhood fragment searched fragment reduced density map more-realistic model searching complete protein results experiment figure plot metrics rms deviation log-likelihood maximum-marginal interpretation predicted backbone trace average kl-divergence marginal distributions function iteration plots show solutions found methods differ terms error versus true trace produce equally accurate traces interestingly figure shows log-likelihood maximum-marginal interpretation metric aggbp produces solution aggbp approximation avoids overfitting data figure shows rms error function proteinfragment length surprisingly methods perform slightly worse searching longer fragments predicted structure fairly accurate quality maps rms error finally scatterplot log-likelihoods fragments represented point illustrated figure figure points diagonal correspond fragments aggbp produced morelikely interpretation fragment aggbp produces solution greater log-likelihood standard difference statistically significant two-tailed paired test synthetic object recognition protein fragment identification testbed shows cpu memory savings achievable algorithm limited part topology skeletal structure linear chain part constant distance section construct synthetic object generator builds part graphs varying branching factors object sizes object softness explore approximation performance part-finder accuracies iteration aggbp iteration c-alpha rms tion true aggbp aggbp protein fragment length c-alpha rms iteration ggbp kl-div genc true true aggbp aggbp true lik eliho figure comparison approximation standard algorithm progresses rms deviation average kl-divergence predicted marginals log-likelihood maximum-marginal interpretation additionally shows rms error function protein fragment size iteration object generator developed synthetic object generator understand aggbp works range tasks locating objects composed interconnected parts object generator lets vary graph topology individual part parameters figure generator constructs objects predefined number parts arranged tree-structured skeleton branching factor skeleton randomly assembled parts pairs nodes connected skeleton connected edges enforcing occupancy potentials ensure parts occupy three-dimensional space part model radiusri rand softness structural potential functions derived pairs parts directly connected skeleton maintain distance equal sum radii pairs occupy space sum radii closer distance softness increases softness parameter part pairs slightly closer sum radii low probability specifically softness parameter replaces occupancy potential step function sigmoid non-zero softness probability distribution distance partsiandj radiiri andrj softnesssi andsj pij exp parenleftbig parenrightbig testbed generator generates observation potentials obs probability distribution part location space generated type pattern matcher image algorithm generator assumes classifier location space returns score shown figure scores drawn distributions true location part score part aggbp log-likelihood lik eliho -mers -mers -mers -mers -mers -mers figure scatterplot showing target fragments log likelihood aggbp trace versus standard trace points diagonal correspond fragments aggbp returned solution vary radii increase branching factor spatial overlap figure illustration graph generator drawn distribution location score drawn distribution simplification assume distributions fixed-width gaussians means varying difference means results classifiers varying accuracy wegenerateeach part observation potential drawing scores random distribution assuming distributions scores converted probabilities bayes rule specific width corresponds single part classifier accuracy remainder section report area precision-recall curve auprc positive score distribution negative score distribution score figure observation potentials generated drawing scores distributions parameter directly related part-classifier accuracy number positive negative examples induces grid corresponds auprc results generator vary parameters model default values shown parentheses branching-factor average branching factor skeleton graph default softness part softness default radius standard deviation radii graph default difference means positive score distribution negative score distribution report area classifier precision-recall curve default area graph average part radius fixed grid point model constructed parts object recognition framework search optimal layout parts generated object observation potentials previous section standard belief propagation aggbp compared results assumed part parameters radius softness learned algorithm aggbp standard ran convergence iterations standard occasionally converge cases highest-likelihood solution iteration parameter setting randomlygeneratedpart graphs results experiment figure standard ground truth aggbp truth aggbp solution standard branching factor rms true true aggbp aggbp softness rms standard-deviation radius rms classifier auprc rms figure comparison approximation standard synthetic object generator holding parameters fixed vary skeleton branching factor part softness radius standard deviation classifier auprc report rms error algorithm aggbp standard ground truth runtime memory usage identical previous experiment varied parameters graph branching factor classifier auprc figures solutions returned methods comparable accuracy solutions equally close ground truth figure shows algorithm performs radii model parts varied performance algorithms similar standard deviation part radii increased times radius larger variations handled clustering objects multiple groups based radius approximating messages group interesting result figure object softness varied 
increasing object softness objects move closer allowed low probability non-zero softness aggbp finds accurate solution standard reason unclear due feedback introduced softness dampened approximation running non-zero softness standard fails converge giving support idea approximation dampening feedback loops log likelihood plots shown synthetic experiments theseexperiments show method valid wide variety model parameters part topologies large majority synthetic experiments aggbp produced interpretation good standard small fraction time conclusions future work describe part-based object recognition framework suited mining detailed image data introduce aggbp message approximation aggregation scheme makes belief propagation tractable large highly connected graphs message-approximation similar mean-field methods reduce number message computations single node fully connected graphs object-recognition framework reduce running time memory requirements object parts additionally describe efficient probability representation based fourier series experiments vision task arising x-ray crystallography shows improvements produce solutions good standard synthetic tests show aggbp accurate variety object types part topologies producing solution good standard unclear aggbp produce more-accurate results standard approximatemessage computation ignores term serves avoid feedback makes method exact tree-structured graphs ingraphswithloops suchfeedbackisunavoidable loops graph types edge potentials ignoring term produces moreaccurate approximation dampening feedback loops inherent loopy belief propagation investigation needed future dynamic approach message aggregation protein backbone-tracing task aggbp approximation error highest edges connecting amino acids nearby space accurately predict amino acids close space iterates precisely compute messages pairs nodes approximately compute messages edges results aggbp illustrate techniques automatic interpretation complex image data shortcuts introduce drastically increase size problems tractable real synthetic dataset produces accurate results significant cpu storage savings standard algorithm appears powerful tool mining large images acknowledgements work supported nlm grant nlm grant coughlan ferreira finding deformable shapes loopy belief propagation proc eccv dimaio shavlik phillips probabilistic approach protein backbone tracing electron density maps proc ismb doucet godsill andrieu sequential monte carlo sampling methods bayesian filtering statistics computing felzenszwalb huttenlocher efficient matching pictorial structures proc cvpr felzenszwalb huttenlocher efficient belief propagation early vision proc cvpr frey graphical models machine learning digital communication mit press heskes uniqueness loopy belief propagation fixed points neural comp ihler sudderth freeman willsky gaussian mixtures proc nips isard pampas real valued graphical models computer vision proc cvpr jordan ghahramani jaakkola saul introduction variational methods graphical models machine learning koller lerner angelov general algorithm approximate inference application hybrid bayes nets proc uai kullback leibler information sufficiency annals mathematical statistics mackay neal good codes based sparse matrices cryptography coding ima conference murphy weiss jordan loopy belief propagation approximate inference empirical study proc uai pearl probabilistic reasoning intelligent systems morgan kaufman san mateo rhodes crystallography made crystal clear academic press silverman density estimation statistics data analysis chapman hall sudderth ihler freeman willsky nonparametric belief propagation proc cvpr sudderth mandel freeman willsky visual hand tracking nonparametric belief propagation mit lids technical report tatikondaandm jordan loopybeliefpropagation gibbs measures proc uai weiss interpreting images propagating bayesian beliefs proc nips weiss neuralcomp weiss freeman correctness belief propagation gaussian graphical models arbitrary topology neural comp yedidia freeman weiss constructing free-energy approximations generalized belief propagation algorithms ieee trans information theory 
journal machine learning research submitted revised published knowledge-based kernel approximation olvi mangasarian olvi wisc jude shavlik shavlik wisc edward wild wildt wisc computer sciences department wisconsin west dayton street madison usa editor john shawe-taylor abstract prior knowledge form linear inequalities satisfied multiple polyhedral sets incorporated function approximation generated linear combination linear nonlinear kernels addition approximation satisfy conventional conditions exact inexact function values points determining approximation leads linear programming formulation nonlinear kernels mapping prior polyhedral knowledge input space defined kernels prior knowledge translates nonlinear inequalities original input space number computational examples including real world breast cancer prognosis dataset shown prior knowledge significantly improve function approximation keywords function approximation regression prior knowledge support vector machines linear programming introduction support vector machines svms play major role classification problems vapnik cherkassky mulier mangasarian recently prior knowledge incorporated svm classifiers improve classification task handle problems conventional data sch olkopf fung svms extensively regression drucker smola sch olkopf evgeniou mangasarian musicant prior knowledge properties function approximated incorporated svm function approximation svm classifier fung work introduce prior knowledge form linear inequalities satisfied function polyhedral regions input space linear kernels similar regions feature space nonlinear kernels inequalities unlike point-wise inequalities general convex constraints treated approximation theory mangasarian schumaker micchelli utreras deutsch inequalities satisfied specific polyhedral sets prior knowledge treated extensive approximation theory literature outline contents paper section define prior knowledge formulation linear kernel approximation input space problem leads linear olvi enemies enemies enemies enemies dist enemies dist enemies dist enemies dist enemies dist rewards dist rewards dist mangasarian jude shavlik edward wild mangasarian shavlik wild programming formulation space section approximate function linear combination nonlinear kernel functions explicitly map polyhedral prior knowledge input space defined kernel functions leads linear programming formulation space section demonstrate utility results number synthetic approximation problems real world breast cancer prognosis dataset show prior knowledge improve approximation section concludes paper summary extensions applications present work describe notation vectors column vectors transposed row vector prime prime scalar product vectors n-dimensional real space denoted xprimey bardblxbardbl denotes -norm notation signify real matrix matrix aprime denote transpose denote row j-th column vector real space arbitrary dimension denoted notation eprimey denote sum components vector zeros real space arbitrary dimension denoted kernel maps column vectors xprime real number xprime aprime row vector aprime matrix make assumptions kernels symmetry xprime prime yprime assume make mercer positive semidefiniteness condition vapnik sch olkopf smola base natural logarithm denoted frequently kernel nonlinear classification gaussian kernel vapnik cherkassky mulier mangasarian jth element bardblaiprime jbardbl positive constant approximate equality denoted abbreviation stands subject symbol denotes logical denotes logical prior knowledge linear kernel approximation begin linear kernel model show introduce prior knowledge approximation unknown function approximate exact function values dataset points denoted matrix point exact inexact denoted real number approximate linear nonlinear function matrix unknown linear parameters simple linear approximation wprimex unknown weight vector constant determined minimizing error criterion leads linear combination rows aprime similar dual representation linear support vector machine weight mangasarian sch olkopf smola aaprime knowledge-based kernel approximation immediately suggests general idea replacing linear kernel aaprime arbitrary nonlinear kernel aprime leads approximation nonlinear linear aprime measure error componentwise vector defined aprime drive error minimizing -norm error -norm complexity reduction stabilization leads constrained optimization problem positive parameter determines relative weight exact data fitting complexity reduction min bardbl bardbl cbardblsbardbl aprime represented linear program min eprimea ceprimes aprime note -norm formulation employed leads linear programming formulation regard kernel aprime positive semidefinite case kernel-induced norm lead quadratic program quadratic program difficult solve linear program nonconvex np-hard problem case kernel employed positive semidefinite introduce prior knowledge linear kernel suppose function represented satisfies condition points necessarily training set lying nonempty polyhedral set determined linear inequalities function linear approximation wprimex dominate linear function hprimex user-provided fixed implication wprimex hprimex equivalently terms aprime primeax hprimex implication added constraints linear program make equivalence relationship converts implication set linear constraints appended linear program similar technique fung proposition incorporate prior knowledge linear classifiers mangasarian shavlik wild proposition prior knowledge equivalence set nonempty fixed implication equivalent system linear inequalities solution bprimeu aprime dprimeu proof implication equivalent system solution primea hprime motzkin theorem alternative mangasarian theorem equivalent system inequalities solution bprimeu aprime dprimeu negationslash contradict nonemptiness knowledge set solve obtain contradiction uprime xprimebprimeu dprimeu dprimeu dividing redefining obtain square adding constraints linear programming formulation linear kernel aprime aaprime obtain desired linear program incorporates prior knowledge implication approximation problem min eprimea ceprimes aaprime aprime bprimeu dprimeu note linear programming formulation linear kernel approximation approximation wprimex primeax unknown function prior knowledge linear input data problem restrictive turn principal concern work incorporation prior knowledge nonlinear kernel approximation knowledge-based nonlinear kernel approximation part paper incorporate prior knowledge nonlinear kernel linear programming formulation prior knowledge implication begin linear prior knowledge implication linear combination rows aprimet implication baprimet primeaaprimet hprimeaprimet knowledge-based kernel approximation fixed assumption restrictive problems sufficiently large number training data points vector input space represented linear combination training data points kernelize matrix products implication implication aprime primek aprime hprimeaprimet note kernels appearing satisfy mercer positive semidefiniteness condition kernel linear kernel renders left side implication note nonlinear kernel implication nonlinear input space data linear implication variable mapped polyhedral implication nonlinear input space data assuming simplicity kernel symmetric aprime prime bprime directly proposition equivalence relation holds implication proposition nonlinear kernel prior knowledge equivalence set aprime nonempty implication equivalent system linear inequalities solution bprime aprime dprimeu append constraints equivalent nonlinear kernel implication linear programming formulation linear program approximating function prior knowledge nonlinear kernel min eprimea ceprimes aprime bprime aprime dprimeu prior knowledge implication satisfiable balance influence prior knowledge fitting conventional data points introduce error variables constraints linear program error variables driven modified objective function min eprimea ceprimes eprimez aprime bprime aprime dprimeu positive parameters final linear program single prior knowledge implication implication sets constraints repeated implication sake simplicity omit details values parameters chosen balance fitting conventional numerical mangasarian shavlik wild data versus prior knowledge choose parameters set tuning set data points choose parameters give fit tuning set note kernels appearing possibly distinct kernels positive semidefinite fact kernel bprime linear kernel abprime numerical experiments noticeable change gaussian kernel turn numerical experiments numerical experiments focus paper theoretical order illustrate power proposed formulation tested 
algorithm synthetic examples real world prior knowledge synthetic examples based sinc function extensively kernel approximation testing vapnik baudat anouar synthetic two-dimensional hyperboloid results significant improvement due prior knowledge parameters synthetic examples selected combination exhaustive search simple variation nelder-mead simplex algorithm nelder mead reflection average error criterion chosen parameter values captions relevant figures one-dimensional sinc function one-dimensional sinc function sinc sinpixpix data sinc function includes approximate function values points intervals endpoints approximate local minima sinc function approximate function values sinc perturbed true values standard deviation addition values values knowledge-based kernel approximation figure one-dimensional sinc function sinc sinpixpix dashed curve gaussian kernel approximation prior knowledge based points shown diamonds solid diamonds depict support points nonlinear gaussian kernel generating approximation sinc rows negationslash solution nonlinear gaussian kernel approximation xprime aprime approximation average error grid points interval parameter values figure one-dimensional sinc function sinc sinpixpix dashed curve gaussian kernel approximation prior knowledge based points shown diamonds figure solid diamonds depict support points nonlinear gaussian kernel generating approximation sinc prior knowledge consists implication sin implemented replacing nonlinear kernel approximation approximation average error grid points interval times error figure parameter values mangasarian shavlik wild figure exact product sinc function sinpix pix sinpix pix figure gaussian kernel approximation product sinc function sinpix pix sinpix pix based exact function values incorrect function values prior knowledge approximation average error grid points set parameter values figure gaussian kernel approximation product sinc function based function values figure prior knowledge consisting sin approximation average error grid points set times error figure parameters knowledge-based kernel approximation actual limit sinc function values intended misleading approximation figure depicts sinc dashed curve approximation prior knowledge solid curve based points shown diamonds solid diamonds depict support points rows negationslash solution nonlinear gaussian kernel approximation xprime aprime approximation figure average error error computed averaging grid equally spaced points interval figure depicts sinc dashed curve approximation prior knowledge solid curve based points shown figure solid diamond points support points rows negationslash solution nonlinear gaussian kernel approximation approximation figure average error computed grid equally spaced points prior knowledge approximate one-dimensional sinc function sin sin minimum sinc knowledge interval prior knowledge implemented replacing nonlinear kernel approximation implication aprime aprime primek aprime sin two-dimensional sinc function two-dimensional sinc function sinc sinc sinpix pix sinpix pix data two-dimensional sinc function includes points region region excludes largest bump function centered values exact function values values similar previous dimensional sinc actual limit function values values intended mislead approximation figure depicts two-dimensional sinc function figure depicts approximation sinc sinc prior knowledge surface based points approximation figure average error computed averaging grid equally spaced points figure depicts approximation sinc sinc prior knowledge surface based points approximation figure average error computed averaging equally spaced points mangasarian shavlik wild figure exact hyperboloid function figure gaussian kernel approximation hyperboloid function based exact function values line prior knowledge approximation average error points set parameter values figure gaussian kernel approximation hyperboloid function based function values figure prior knowledge consisting implications approximation average error points set times error figure parameter values knowledge-based kernel approximation prior knowledge consists implication sin sin equal minimum sinc sinc knowledge set prior knowledge implemented replacing nonlinear kernel approximation implication two-dimensional hyperboloid function two-dimensional hyperboloid function two-dimensional hyperboloid function data consists points line values points actual function values figure depicts two-dimensional hyperboloid function figure depicts approximation hyperboloid function prior knowledge surface based points approximation figure average error computed grid equally spaced points figure depicts approximation hyperboloid function nonlinear surface based points prior knowledge approximation figure average error computed grid equally spaced points prior knowledge consists implications implications implemented replacing nonlinear kernel approximation implication regions knowledge cones negative implications analogous explain implication justified basis knowledge cone sufficiently large intended capture coarsely global shape succeeds generating accurate approximation function predicting lymph node metastasis conclude numerical results potentially application knowledge-based approximation breast cancer prognosis mangasarian wolberg lee important prognostic indicator breast cancer recurrence number metastasized lymph nodes patient armpit determine number patient undergo optional surgery addition removal breast tumor predicted number metastasized lymph nodes sufficiently small oncological surgeon decide perform additional surgery approximate number metastasized mangasarian shavlik wild lymph nodes function thirty cytological features histological feature cytological features obtained fine needle aspirate diagnostic procedure histological feature obtained surgery proposed knowledge-based approximation improve determination function predicts number metastasized lymph nodes polyhedral regions past training data existence substantial number metastasized lymph nodes regions presence metastasis knowledge applied obtain accurate lymph node function based numerical function approximation performed preliminary experiments wisconsin prognostic breast cancer wpbc data murphy aha experiments reduced predicted number metastasized lymph nodes based cytological features cell texture worst cell smoothness worst cell area histological feature tumor size tumor size obvious histological feature include cytological features selected breast cancer diagnosis mangasarian approximating function note online version wpbc data entries lymph node information removed experiments removing entries left examples dataset simulate procedure expert obtaining prior knowledge past data procedure random dataset analyze past data inspecting past data choose background knowledge denote texture worst smoothness worst area tumor size prior knowledge based typical oncological surgeon advice larger values variables result metastasized lymph nodes constants chosen taking average values entries past data metastasized lymph node ten-fold cross validation compare average absolute error approximation prior knowledge approximation prior knowledge equation data past data generate constants parameters gaussian kernel chosen nelder-mead algorithm tuning set training data fold average absolute error function approximation prior knowledge average absolute error prior knowledge reduction function data ten-fold cross validation experiments approximation accurate results adding prior knowledge improve function approximation substantially sophisticated prior knowledge based detailed analysis data consultation domain experts reduce error close section potential application reinforcement learning task sutton barto goal predict taking action state domain function approximated cartesian product set states set actions plan keep-away subtask soccer game developed stone sutton state description includes measurements distance opposing players distance soccer ball distances edges field actions include knowledge-based kernel approximation holding ball attempting pass teammate demonstrated providing prior knowledge improve choice actions significantly kuhlmann maclin shavlik advice prior knowledge successfully domain simple advice opponent meters holding ball good idea approach approximate 
function function states actions advice stated implication assuming opponents denotes distance opponent distance opponent action holding ball predicted constant hoped advice generating improved function based current description state soccer game conclusion outlook presented knowledge-based formulation nonlinear kernel svm approximation approximation obtained linear programming formulation nonlinear symmetric kernel positive semidefiniteness mercer condition assumed issues sampling knowledge sets order generate function values matrix vector situations conventional data points constitute interesting topic future research additional future work includes refinement prior knowledge applications medical problems computer vision microarray gene classification efficacy drug treatment prior knowledge acknowledgments grateful colleagues rich maclin dave musicant constructive comments research data mining institute report october supported nsf grants ccrand iriby nlm grant darpa isto grant phs grant microsoft baudat anouar kernel-based methods function approximation international joint conference neural networks pages washington cherkassky mulier learning data concepts theory methods john wiley sons york deutsch approximation product spaces springer-verlag berlin drucker burges kaufman smola vapnik support vector regression machines mozer jordan petsche editors advances neural information processing systems pages cambridge mit press mangasarian shavlik wild evgeniou pontil poggio regularization networks support vector machines smola bartlett sch olkopf schuurmans editors advances large margin classifiers pages cambridge mit press fung mangasarian shavlik knowledge-based nonlinear kernel classifiers technical report data mining institute computer sciences department wisconsin madison wisconsin march ftp ftp wisc pub dmi tech-reports conference learning theory colt workshop kernel machines washington august proceedings edited warmuth sch olkopf springer verlag berlin fung mangasarian shavlik knowledge-based support vector machine classifiers suzanna becker sebastian thrun klaus obermayer editors advances neural information processing systems pages mit press cambridge october ftp ftp wisc pub dmi tech-reports kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer proceedings aaai workshop supervisory control learning adaptive systems san jose lee mangasarian wolberg survival-time classification breast cancer patients technical report data mining institute computer sciences department wisconsin madison wisconsin march ftp ftp wisc pub dmi tech-reports computational optimization applications maclin shavlik creating advice-taking reinforcement learners machine learning mangasarian nonlinear programming siam philadelphia mangasarian generalized support vector machines smola bartlett sch olkopf schuurmans editors advances large margin classifiers pages cambridge mit press ftp ftp wisc math-prog tech-reports mangasarian data mining support vector machines july http ftp wisc math-prog talks ifip ppt mangasarian musicant large scale kernel regression linear programming machine learning ftp ftp wisc pub dmi tech-reports mangasarian schumaker splines optimal control schoenberg editor approximations special emphasis splines pages york academic press mangasarian schumaker discrete splines mathematical programming siam journal control mangasarian street wolberg breast cancer diagnosis prognosis linear programming operations research july-august knowledge-based kernel approximation micchelli utreras smoothing interpolation convex subset hilbert space siam journal statistical computing murphy aha uci machine learning repository ics uci mlearn mlrepository html nelder mead simplex method function minimization computer journal sch olkopf simard smola vapnik prior knowledge support vector kernels jordan kearns solla editors advances neural information processing systems pages cambridge mit press sch olkopf smola learning kernels mit press cambridge smola sch olkopf kernel-based method pattern recognition regression approximation operator inversion algorithmica stone sutton scaling reinforcement learning robocup soccer proceedings eighteenth international conference machine learning icml williams sutton barto reinforcement learning introduction mit press cambridge vapnik nature statistical learning theory springer york edition vapnik golowich smola support vector method function approximation regression estimation signal processing neural information processing systems volume pages cambridge mit press wolberg street heisey mangasarian computerized breast cancer diagnosis prognosis fine-needle aspirates archives surgery 
sensor inputs actions hidden units advice current hidden units movenorthmoveeast state enemy west obstacle north moveeast movenorth state inputs outputs surrounded moveeastpusheast oktopusheast enemy inputs pusheast moveeast definition definitionsurrounded surrounded agent enemy obstacle empty key reward hidden units wall enemy obstacle empty sector wall enemy obstacle empty sector ring wall enemy obstacle empty sector wall enemy obstacle empty sector ring actions sensor inputs reward reward reward reward action moveeast pusheast movenorth number training episodes average cumulative testset reinforcement simplemoves number training episodes elimenemies number training episodes average cumulative testset reinforcement nonlocalmoves number training episodes surrounded advice advice episodes advice episodes advice episodes 
case accuracy estimation comparing induction algorithms foster provost bell atlantic science tech westchester avenue white plains foster basit tom fawcett bell atlantic science tech westchester avenue white plains fawcett basit ron kohavi silicon graphics shoreline blvd mountain view ronnyk sgi abstract analyze critically classi ccation accuracy compare classi cers natural data sets providing investigation roc analysis standard machine learning algorithms standard benchmark data sets results raise concernsaboutthe accuracyfor comparing classi cers drawinto question conclusions drawn studies presentation describe demonstrate proper roc analysis comparative studies machine learning research argue methodology preferable making practical choices drawing scienti conclusions introduction substantial research devoted development analysis algorithms building classi cers part researchinvolves comparing induction algorithms common methodology evaluations perform statistical comparisons accuracies learned classi cers suites benchmark data sets purpose question statistical tests dietterich salzberg question accuracy estimation primary scienti methodologies celd important scienti community cast critical eye reasonable justi ccations comparing accuracies natural data sets require empirical veri ccation argue form roc analysis proper methodology provide veri ccation provide analysis classi cer performanceusing standard machine learning algorithms standard benchmark data sets results raise concerns accuracy practical comparisons drawing scienti conclusions predictive performance concern contribution paper two-fold analyze critically common assumption machine learning research provideinsightsintoits applicability anddiscuss implications process describe superior methodology evaluation induction algorithms natural data sets roc analysis machine learning research applied principled mannergearedto speci conclusions machine learning researchers liketodraw hope work makes signi ccant progress goal justifying accuracy comparisons induction problems intentin applying machine learning algorithms build existing data model classi cer classify previously unseen examples welimit predictive performance whichisclearly intent accuracy-based machine learning studies issues comprehensibility computational performance assume true distribution examples classi cer applied advance make informed choice performance estimated data berent methodologies arriving estimations kohavi dietterich commonly performance metric classi ccation accuracy care comparisons accuracies benchmark data sets theoretically universe induction algorithms algorithm superior induction problems wolpert scha ber tacit reason comparing classi cers natural data sets data sets represent problems systems face real world superior performance benchmarks translate superior performance real-world tasks end celd amassed admirable collection data sets wide variety classi cer applications merz murphy countless research results havebeenpublished based comparisons classi cer accuracy benchmark data sets argue comparing accuracies benchmark data sets classi cer performance real-world tasks accuracy maximization goal manyofthe real-world tasks natural data sets classi ccation accuracy assumes equal misclassi ccationcosts false positive false negative errors assumption problematic real-world problems type classi ccation error expensive fact documented primarily celds statistics medical diagnosis pattern recognition decision theory machine learning fraud detection cost missing case fraud berent cost false alarm fawcett provost accuracy maximization assumes class distribution class priors target environment benchmark data sets existing distribution natural distribution strati ced iris data set instances class splice junction data set dna donor sites acceptor sites nonboundary sites natural class distribution skewed dna codes human genes saitta neri knowledge target class distribution claim maximizing accuracy problem data set drawn accuracy maximization accuracy estimates compare induction algorithms data sets believetobethetwo candidate justi ccations classi cer highest accuracy mayvery classi cer minimizes cost classi cer tradeo true positive predictions false positives tuned learned model produces probability estimates combined prior probabilities cost estimates decisionanalytic classi ccations model high classi ccation accuracy produces good probability estimates havelow cost target scenario induction algorithm produces highest accuracy classi cers produce minimum-cost classi cers training berently breiman suggest altering class distribution bective building cost-sensitive decision trees work cost-sensitive classi ccation turney criticize wisconsin machine practice learning comparing group machine working learning paper advice-based transfer reinforcement learning algorithms lisa based torrey jude accuracy shavlik trevor walker ecient richard point maclin computer accuracy science department metric wisconsin madison real-world performance usa measured computer science department analyze minnesota duluth candidate usa justi abstract ccations report founded overview work justifications reasonable transfer crst reinforcement discuss learning commonly cited advice-taking special mechanisms case goal transfer justi learning ccation arguing speed makes learning target untenable task assumptions transferring knowledge present results related previously empirical learned study source task leads methods conclude designed justi ccations robustly questionable positive transfer speed define learning negative problem transfer principle slow two-class problem designed reproportion human teachers cstratify provide classes simple based guidance target increases bene transfered knowledge methods push boundaries current work area perform transfer complex dissimilar tasks challenging robocup simulated soccer domain introduction reinforcement learning tasks addressed independently implicit assumption task relation tasks domains related tasks learning task scratch agents domains knowledge learned previous tasks speed learning task goal transfer learning domain simulated soccer robocup suppose agent learned game keeping ball opponents passing teammates suppose game learn score goals opponents games similarities agent bene knowledge rst game learning case refer rst game source task game target task transfer challenging reasons source target tasks features actions rewards means knowledge transferable actions source exist target transferable knowledge helpful action exists leads rewards source target uncertainty compounded fact agents actions unintended ways shoot action pass teammate goal attempt score goal straightforward transfer method nal solution source task initial solution target task perform standard wisconsin machine learning group working paper reinforcement learning build solution incrementally refer approach model reuse method speed learning considerably task solutions similar slow learning moderately erent agent similar solutions learning target task prefer robust transfer method advice taking paradigm achieve goal advice set instructions task solution advice passing soccer opponent nearest teammate open pass nearest teammate advice-taking algorithms advice ned express source task knowledge advice leads positive transfer quickly ned leads negative transfer advice source task refer transfer advice transfer methods essentially erent ways transfer advice source task advice express direct human guidance case refer user advice transfer methods user advice guide transfer process natural powerful users interact learning system background section provide background information reinforcement learning advice taking rely material discuss transfer methods describe motivating domain transfer robocup simulated soccer costs experiments class present distribution domain reinforcement maximizing learning accuracy advice transformed reinforcement data corresponds learning minimizing agent costs navigates target environment data breiman earn rewards environment strategyis state impracticablefor typically conducting empirical set researchbased features action agent takes receives reward observes state q-learning common form agent builds q-function estimate long-term taking action state agent policy typically action highest q-value current state occasional exploratory actions taking action receiving reward agent updates q-value estimates current state implementation agents build q-functions sarsa procedures exp age age game wisconsin machine learning group working paper number games learner trained game older games smaller learning rate initial half-life games form q-function action weighted linear sum state features updating q-functions action agents play batches full games time batch solve linear optimization problem weights minimize modelsize datamis modelsize sum absolute values feature weights datamis disagreement learned function outputs training examples numeric parameter speci relative importance minimizing disagreement data versus nding simple model incorporate advice algorithm adding term optimization problem minimizes modelsize datamis advicemis advicemis disagreement learned function outputs advice constraints numeric parameter speci relative importance minimizing disagreement advice versus minimizing original quantity time decays advice fades learner gains experience longer requires guidance correspondingly increases advice set soft constraints task solution depending agrees training examples learner follow advice follow approximately ning ignore altogether algorithm based knowledge-based kernel regression section technical detail kbkr substantial body related work benchmark advice taking approach erent considered advice taking expert imitation examples lin replays teacher sequences bias learner teacher performance sammut imitation human experts train ight simulation program form advice puts direct constraints policies agents learn andre russell describe language giving policy constraints learning agents kuhlmann propose rule-based advice system increases q-values xed amount method ers user action preferences internal details desired q-values advice beginning learning process driessens dzeroski human guidance create partial initial q-function relational advice speci step clouse utgo human observer step advise learner speci action method ers advice present learning process impact decreases time wisconsin machine learning group working paper methods advice rules gordon subramanian accept advice form condition achieve goals genetic algorithms adjust respect data maclin shavlik develop if-then advice language incorporate rules neural network adjustment method ers incorporate rules optimization problem neural networks genetic algorithms knowledge-based kernel regression data sets section explain transformation kbkr valid advice-taking algorithm two-class problems detail note settings approximated bectively multiclass transfer problems experiments open information question technical read costs order data understand sets transfer noted learning concepts applied explains researchers precisely bradley advice catlett applied provost system fawcett mentioned assigning costs precisely batch games virtually impossible agent kbkr generally training examples build q-function class model distribution form natural data set function ctrue action target class weighted distribution linear sum state uncertainties features claim means agent transform nding cost-minimization optimal problems weight vector accuracy-maximization problems weight cases feature target feature conditions vector action virtually impossible weight vector impossible set term real-world domains expected q-value ctrue taking target action costs class state distribution change time learners time place action place scores situation highest situation fawcett probability provost sub-optimal exploratory ability action transform cost probability minimization accuracy transfer maximization experiments justify initial limiting decayed comparisons exponentially classi ccation half-life accuracy episodes class compute distribution weight vector action comparisons based classi subset ccation training accuracy examples action indicative place broader notion feature vectors cbetter rows performance data roc matrix analysis dominating size models investigate examples algorithm generates begin high-accuracy discard classi games cers randomly generally probability discarding produces game low-cost increases classi cers exponentially target age cost scenario game target current cost model class actual distribution rewards information received order examples conclude compute q-value classi estimates cer place higher accuracy output vector classi cer optimal weight show vector performs reasonable assumptions limit denotes investigation vector two-class problems omit analysis simplicity straightforward evaluation transfer framework experiments wechoose receiver matrix operating exploration characteristic examples roc analysis regular classic exploitation methodology examples signal detection bad theory moves common forgotten medical diagnosis recently begun exploitation examples generally exploration rate swets provost create fawcett exploration brie examples review randomly choosing exploitation basics steps roc analysis model roc space score denotes -policy actions coordinate system steps visualizing practice classi cer prefer performance roc non-zero space weights typically true positive important rate features order plotted model axis simple false avoid positive tting rate training plotted examples axis introduce eachclassi slack ceris variables representedby pointinroc inaccuracies space wisconsin machine learning pair group models working paper produce continuous examples output penalty parameter estimate trading posterior probability inaccuracies instance complexity class solution membership resulting statistics vary minimization problem min threshold output jjwjj varied jbj cjjsjj extremes threshold denotes absolute denotes sum absolute values cning classi penalty cer set resulting term curve called solving roc curve problem illustrates produce error weight tradeo vector action model roc compromises curves accuracy describe predictive simplicity behavior experiments classi cer independent class decaying distributions exponentially error costs half-life decouple classi episodes ccation mangasarian performance factors original formulation purposes called knowledge crucial based notion kernel regression model kbkr dominates advice rocspace meaning form roc rule curves single beneath action equal rule creates dominating constraints model problem model solution addition figure constraints good training data models recently introduced cost extension class kbkr distributions called preference-kbkr dominating model advice exists pairs considered actions form cbest model terms predictive performance dominating read model current exist state satis figure q-value models preferred action represented exceed target scenarios non-preferred action cases exist scenarios giving model advice maximizes shooting accuracy moving single-number ahead metric distance haveminimum goal cost figure shows vector test-set roc curves row uci domains column study distance goal note feature cbumpiness zeros roc vector curves figure largest set domains small positive bumpy roc number curves allowed bumpiness typical inaccuracy induction studies training examples roc curves advice generated hold-out test partially set accuracy introduce estimates slack based variables single penalty hold-out set parameters roc curves trading misleading howmuch impact observed advice variation solution due impact training ftest training partition examples thusit isdi ecult experiments expected behavior decaying learned exponentially models half-life conduct roc episodes analysis minimization cross-validation problem addresses bradley produced actions roc curves -fold apply cross constraints validation relative values similarly multiple bumpy pieces bradley generated preference advice curves incorporated technique pooling pooling ith points making makes raw roc advise curve taking 
action stating preferred actions cplex commercial software program solve resulting linear program wisconsin machine learning group working paper min jjwajj jbaj cjjsajj jjzijj action aawa piece advice bti note method called kernel regression kernel found non-kernelized version worked developed version kbkr called extenkbkr incorporates advice designed higher volume advice transfer experiments require version produce large amounts advice refer reader maclin details method robocup motivating domain transfer robocup simulated soccer robocup project goal producing robotic soccer teams compete human level software simulator research purposes stone sutton introduced robocup domain challenging large continuous state space nondeterministic action ects full game soccer complex researchers developed smaller games robocup domain figure inherently multi-agent games standard simpli cation agent possession soccer ball learning time model built combined data agents keepaway breakaway movedownfield fig snapshots robocup soccer tasks wisconsin machine learning group working paper rst robocup task m-on-n keepaway objective reinforcement learners called keepers ball hand-coded players called takers keeper ball choose hold pass teammate keepers ball follow hand-coded strategy receive passes game ends opponent takes ball ball bounds learners receive reward time step team ball keepaway state representation based designed stone sutton features listed table keepers ordered distance learner takers note present features predicates rst-order logic variables capitalized typed player keeper constants uncapitalized simplicity types variable names leaving implied terms player player keeper keeper fully relational reinforcement learning predicates grounded propositional features learning rst transfer methods nal transfer method features rst-order representation show version robocup task m-on-n movedown eld objective reinforcement learners called attackers move line opposing team side eld maintaining possession ball attacker ball choose pass teammate move ahead left respect opponent goal attackers ball follow hand-coded strategy receive passes game ends cross line opponent takes ball ball bounds time limit seconds learners receive symmetrical positive negative rewards horizontal movement forward backward movedown eld state representation presented torrey features listed table attackers ordered distance learner defenders robocup task m-on-n breakaway objective attackers score goal hand-coded defenders handcoded goalie attacker ball choose pass teammate move ahead left respect opponent goal shoot left center part goal attackers ball follow hand-coded strategy receive passes game ends score goal opponent takes ball ball bounds time limit seconds learners receive reward score goal reward breakaway state representation presented torrey features listed table attackers ordered distance learner non-goalie defenders system discretizes feature tasks intervals called tiles boolean feature tile denoted distbetween takes wisconsin machine learning group working paper table robocup task feature spaces keepaway features distbetween player distbetween keeper mindisttaker keeper anglede nedby keeper minangletaker keeper distbetween player eldcenter movedown eld features distbetween player distbetween attacker mindistdefender attacker anglede nedby attacker minangledefender attacker disttorightedge attacker timeleft breakaway features distbetween player distbetween attacker mindistdefender attacker anglede nedby attacker minangledefender attacker distbetween attacker goalpart distbetween attacker goalie anglede nedby attacker goalie anglede nedby goalpart goalie anglede nedby toprightcorner goalcenter timeleft wisconsin machine learning group working paper units stone sutton found tiling important timely learning robocup linear q-function model ability represent complex nonlinear functions robocup games substantial erences features actions rewards goal goalie shoot actions exist breakaway tasks move actions exist keepaway tasks rewards keepaway movedown eld occur incremental progress breakaway reward sparse erences solutions tasks erent knowledge transferable share features actions pass action cult tasks speeding learning transfer desirable transfer reinforcement learning goal transfer learning speed learning target task transferring knowledge related source task design method transfer reinforcement learning answer questions knowledge transfer source task extract knowledge source task apply knowledge target task put methods context discuss range answers questions mention relevant related work types knowledge source task straightforward type knowledge learned source task actual solution source task q-functions type model learned taylor taylor stone transfer q-functions gorski laird transfer functions type knowledge policy created model choice action state information consists action preferences explicit values actions transfer policies torrey discuss section skill piece policy circumstances speci action skills express general information full policy transfer skills torrey discuss section domains actions action expensive source task provide information actions important sherstov stone transfer type knowledge multi-step actions options source task provide insight action sequences make wisconsin machine learning group working paper options perkins precup soni singh transfer type knowledge explicit implicit markov decision process mdp describes task transfer information states transitions mdp asadi ferguson mahadevan walsh describe approaches vein methods extracting knowledge source task types knowledge mentioned extracted source task general ways analyzing underlying model q-function observing agent successful behavior source task analyzing underlying model simple copying model directly taylor slightly erent approach torrey source-task model highest-value target-task action observing successful behavior source task learners imitate expert agents human sammut electronic price boutilier observed games torrey learning rules skills methods applying knowledge target task method applying source-task knowledge target task mapping tasks required mapping shows tasks related matching features actions standard practice assume human information recent work liu stone analogical structure mapping acquire automatically alternative mentioned relational reinforcement learning rrl make mapping unnecessary domain formulated rrl general applies source target task translation required driessens stracuzzi asgharbeygi describe ways apply rrl transfer learning assuming mapping exists translate source-task knowledge target-task terms ways apply knowledge mentioned model averaged discussed byswets pickett pooling assumes 
ith points curves estimatingthe samepointinrocspace isdoubtful bradley method generating curves study importanttohave good approximation expected roc curve generate results -fold cross-validationusing adi berent methodology called averaging averaging procedure recommended byswets bradley acknowledges fact germane study problematic false positives true positives bagged adult false positives true positives bagged satimage figure raw un-averaged roc curves uci database domains pickett assumes normalctted roc curves binormal roc space weaverage roc curves manner k-fold cross-validation roc curve folds treated function linear interpolations points roc space multiple points maximum chosen averaged roc curve function plot averaged roc curves sample points regularly spaced fp-axis compute con cdence intervals common assumption binomial distribution standard methods produce dominating models state precisely basic hypothesis investigated standard learning algorithms produce dominating models standard benchmark data sets hypothesis true generally conclude algorithm higher accuracy generally target costs priors note classi ccation performance line segment connecting tworoc points achieved randomly selecting classi ccations weighted interpolation proportion classi cers cning endpoints conclusion problems accuracy comparisons select non-dominating classi cer indistinguishable pointofcomparison maybemuchworse hypothesis true conclusion havetorely berent justi ccation provide experimental study hypothesis designed uci repository chose ten datasets contained instances accuracy decision trees roc curves ecult read high accuracies domain induced classi cers minority class road wechose class grass selected inducers mlc kohavi decision tree learner naivebayes discretization k-nearest neighbor severalk values ibk bagged-mc breiman similar quinlan probabilistic predictions made laplace correction leaves discretizes data based entropy minimization dougherty builds naive-bayesmodel domingos pazzani ibk votes closest neighbors neighbor votes weight equal distance test instance averaged roc curves shown figures vehicle ten domains absolute dominator general runs performed data sets crossvalidation folds dominating classi cers cases close adult waveformin cases curve dominates area roc space dominated refute hypothesis algorithms produce true positive false positive bag-mc vehicle true positive false positive bag-mc waveformtrue positive false positive bag-mc dna true positive false positive bag-mc adult figure smoothed roc curves uci database domains statistically signi ccantly dominating classi cers draws question claims calgorithma algorithm based accuracy comparison order draw suchaconclusion absence target costs class distributions roc curve algorithm signi ccant dominator algorithm obvious implications machine learning research practical situations weaker claim cient algorithm good choice good algorithm accuracies signi ccantly berent clear type conclusion justi ced domains curves statistically indistinguishable dominators area space signi cantly dominated practical situations typically comparisons made wealth classi cers classi cers compared general pairwise comparisons algorithms cases model pair otherin berent regionsof rocspace draws question single number metrics practical algorithm comparison metrics based precise target cost class distribution information standard methods coerced yield dominating roc curves justi ccation accuracy compare algorithms subtly berent crst speci cally itallowsfor possibility coercingalgorithms produce berentbehaviors berent scenarios cost-sensitive learning accuracy comparisons justi ced arguing domain algorithm higher accuracy algorithm lower cost reasonable costs class distributions con crming refuting justi ccation completely scope paper coerce algorithms berent environmental conditions open question straightforward method stratifying samples evaluated satisfactorily argue roc framework outlined minor modi ccation evaluate question ber discon crming evidence algorithms produce berent models berent cost class distributions roc methodology stated adequate evaluate performance algorithm individual model characterizeanalgorithm sperformanceforrocanalysis producing composite curve set generated models pooling convex hull roc curves produced set models detail provost fawcett wecannow form hypothesis potential justi ccation standard learning algorithms produce dominating roc curves standardbenchmark data sets con crming hypothesis important step justifying common practice ignoring target costs class distributions class cer comparisons natural data con crming evidence hand discon crming evidence results presented naive bayes robust respect costs produce roc curve target costs class distribution shown decision trees surprisingly robust probability estimates generated laplace estimate bradford result holds generally results previous section discon crm presenthypothesis bradley results provide discon crming evidence speci ccally studied real-world medical data sets uci repository sources bradley plotted roc curves classi cer learning algorithms consisting neural nets decision trees statistical techniques bradley composite roc curves formed training models berently berent cost distributions previously criticized design study purpose answering question results replicated current methodology theywould make strong statement data sets dominating classi cer implies domain exist disjoint sets conditions berent induction algorithms preferable recommendations limitations designing comparative studies researchers clear conclusions draw results argued comparisons algorithms based accuracy unsatisfactory dominating classi cer presenting case accuracy goals show precise comparisons made target cost class distributions dominator conclusions qualiced single number metric make strong conclusions domain-speci information ranges costs class distributions classi cer dominates problems cost-sensitive classi ccation learning skewed class distributions analyzed precisely knowledge target conditions precise concise robust speci ccation classi cer performance made detail byprovost fawcett slopes lines tangentto roc convex hull determine ranges costs class distributions classi cers minimize cost speci target conditions slope cost ratio times reciprocal class ratio ten domains optimal classi cers berent target conditions table road domain figure table naivebayes classi cer target conditions slope bagged-mc slopes greater table locally dominating classi cers ten uci domains domain slope range dominator domain slope range dominator adult pima bagged-mc bagged-mc breast bagged-mc cancer bagged-mc bagged-mc satimage crx bagged-mc bagged-mc bagged-mc german bagged-mc bagged-mc waveform bagged-mc road grass bagged-mc bagged-mc dna vehicle bagged-mc bagged-mc perform equally admit elegant single-number comparison research practice summary dominating classi cer exist cost class distribution information unavailable strong statement classi cer superiority made make precise statements superiority speci regions roc space knowisthatfew false positive errors tolerated maybeable cnd algorithm superior cfar left edge roc space limited investigation classes bect conclusions results negative recommending analytical framework wenotethat extending work multiple dimensions interesting open problem finally completely satis ced method generating con cdence intervals present intervals neymanpearson observer egan whichwants maximize appropriateness questionable evaluating minimum expected cost set costs contours roc space lines slope area future work fundamental drawback methodology conclusions wehave bered debate justi ccation accuracy estimation primary metric comparing algorithms benchmark data sets elucidated top candidates justi ccation haveshown realistic cost class distributions precisely supported experimental evidence draw conclusions work justi ccations accuracy compare classi cers questionable proper roc analysis applied comparative studies machine learning research roc analysis simple comparing single-number metric additional power delivers worth bort situations roc 
analysis strong general conclusions made positive negative situations strong general conclusions made roc analysis precise analysis conducted roc analysis machine learning research applied principled manner geared speci conclusions machine learning researchers liketodraw hope work makes signi ccant progress goal acknowledgements discussed justi ccations accuracy-based comparisons roc analysis applied classi cer learning rob holte provided helpful comments draft paper bradford kunz kohavi brunk brodley pruning decision trees reuse nal model source task initial model target task performing normal singh mehta taylor examples approach method follow source task policies exploration steps normal target task random exploration approach referred policy reuse madden howley fernandez veloso examples approach technique speed normal reward shaping designer task deliberately constructs rewards wisconsin machine learning group working paper helps learner solution konidaris barto apply idea transfer source-task knowledge shape rewards approach focus advice taking explained advice viewed set soft constraints task solution transfer methods torrey torrey construct advice source task advice-taking algorithm learn target task transfer methods work produced advice-based transfer methods mentioned rst called policy transfer encourage agents apply source-task policy speci q-functions target task called skill transfer give advice skills learned observation source task sections provide results experiments transfer methods include experiments model reuse baseline approach model reuse model reuse baseline transfer method nal source-task model initial target-task model experiments model q-function action weighted sum features assume user mapping shows propositional 
features actions correspond tasks mapping translate source-task model model target task actions tasks one-to-one correspondences action source exist target simply ignore q-function action ignore hold action transfer keepaway breakaway action exists target exist source initialize q-function action weights applies shoot actions transfer keepaway breakaway source-task action mapped action target task mapping separate set feature mappings single pass action -onbreakaway mapped pass actions -onbreakaway mapping player source mapped target mapped target features tasks partially overlapping feature source exist target mapped numeric typical average feature replace feature constant translated q-function applies distancetocenter feature transfer keepaway breakaway feature exists target exist source simply give weight translated q-function feature exist wisconsin machine learning group working paper tasks erent ranges mapping feature values scaled shifted recall tiles representing segments numeric feature automatically map tile source task tile target task ratio overlap area tile intervals non-overlap area highest translated model perform model reuse system agent play target task model batches games learns solving optimization problem section batch produce q-functions note system relearns q-functions completely batch changing incrementally play translated model rst generate training examples model uence persist learning model persist found add additional examples rst batches learning create additional examples extra exploration examples created kbkr section start pseudo-examples rst learning decay number linearly batches games model reuse results figures display target-task learning curves transfer experiments method curve average runs standard reinforcement learning curves transfer model reuse source tasks transfer curve average transfer runs erent source runs total runs results include misclassi ccation costs proceedings ecmlpages bradley area roc curve evaluation machine learning algorithms pattern recognition breiman friedman olshen stone classi ccation regression trees wadsworth international group breiman bagging predictors machine learning catlett tailoring rulesets misclassi ccatioin costs proceedings conference statistics pages dietterich approximate statistical tests comparing supervised classi ccation learning algorithms neural computation domingos pazzani independence conditions optimality simple bayesian classi cer machine learning dougherty kohavi sahami supervised unsupervised discretization continuous features prieditis russell eds proceedings icmlpages morgan kaufmann egan signal detection theory roc analysis series cognitition perception academic press york fawcett provost adaptive fraud detection data mining knowledge discovery http fwww croftj net fawcett dmkdps kohavi sommer celd dougherty data mining mlc machine learning library international journal arti ccial intelligence tools http fwww sgi ftechnology fmlc kohavi study cross-validation bootstrap accuracy estimation model selection mellish proceedings ijcaipages morgan kaufmann http frobotics stanford ronnyk merz murphy uci repository machine learning databases http fwww ics uci mlearn fmlrepository html provost fawcett analysis visualization classi cer performance comparison imprecise class cost distributions proceedings kddpages aaai press provost fawcett robust classi ccation systems imprecise environments proceedings aaaiaaai press http fwww croftj net fawcett papers faaai -dist quinlan programs machine learning morgan kaufmann san mateo california saitta neri learning creal world machine learning salzberg comparing classi cers pitfalls avoidand recommendedapproach data mining knowledge discovery scha ber conservation law generalization performance icmlpages morgan kaufmann swets andr pickett evaluation diagnostic systems methods signal detection theory york academic press swets measuring accuracy diagnostic systems science turney cost sensitive learning bibliography http fai iit nrc bibliographies fcost-sensitive html wolpert relationship pac statistical physics framework bayesian framework framework ind wolpert mathematics generalization addison wesley true positive false positive bag-mc breast cancer true positive false positive bag-mc crx true positive false positive bag-mc german true positive false positive bag-mc pima true positive false positive bag-mc roadgrass true positive false positive bag-mc satimage figure smoothed roc curves uci database domains cont 
source target variance variance high smooth y-value data point averaging y-values data points mappings experiments listed appendix section task measure performance plot number training games breakaway probability agents score goal game movedown eld average total reward acquired game keepaway average length game gure curve source task target task team size decreased refer close transfer tasks closely related close transfer results subtasks remaining curves figure erent source target tasks equivalent team sizes refer distant transfer tasks distantly related distant transfer concentrate transfer easier tasks harder tasks keepaway breakaway movedown eld breakaway distant transfer model reuse produces performance decrease takes half learning curve recover expected model reuse produces negative transfer tasks closely related wisconsin machine learning group working paper probability goal training games standard fig probability scoring goal training -onbreakaway standard model reuse -onbreakaway -onmovedown eld -onkeepaway average total reward training games standard fig average total reward training -onmovedown eld standard model reuse -onmovedown eld average game length sec training games standard fig average game length training -onkeepaway standard model reuse -onkeepaway wisconsin machine learning group working paper table statistical results model reuse experiments breakaway movedown eld keepaway comparing area curve standard reinforcement learning srl experiment conclusion p-value con dence interval higher con dence srl higher con dence srl higher con dence srl equivalent srl equivalent close transfer model reuse produces performance increase standard quickly falls level standard players begin learning model directly happen incremental learning system taylor sudden system model reuse mixed results close transfer compare learning curves quantitatively randomization test calculate p-value areas curves p-value probability t-statistic measured areas high transfer learning curves inherently erent standard curves low means con dent curves higher area fact signi cantly higher estimate magnitude erence calculate bootstrapped con dence interval performance measures derived work paul cohen group darpa transfer learning program table summarizes statistical results model reuse experiments experiment p-value con dence level curves higher area con dence interval average transfer curve area minus average standard curve area statistical results show model reuse reliably increase total area learning curve close transfer graphs show initial bene distant transfer model reuse signi cantly decreases total area advice-based transfer methods present sections robust policy transfer advice policy transfer advice taking transfer method advise source-task policy speci q-functions target task reusing q-functions directly method transfers policy created q-functions advises target-task learner actions wisconsin machine learning group working paper table simple constructing policy-transfer advice actions task actions task learned model task set linear q-value expressions weights features translated advice task features source task model advice format prefer pair actions user-provided mapping full advice expression prefer translated model pair actions source task leaves learner free determine actual q-values actions give advice agent explained advice taking robust mechanism transfer nish describing method explain build transfer advice model reuse assume user mapping propositional features actions tasks strategies dealing partially overlapping feature action sets continue shifting scaling multiple mappings applying mapping state target task evaluate q-values actions state translated model compare pairs target-task actions source-task analogues advise higher-scoring action preferred lowerscoring action table simple concrete process set approximately target-task q-value range policy transfer results figures display results policy-transfer experiments represent close distant transfer scenarios previous section performed experimental methodology mappings model-reuse experiments listed appendix section graphs show unlike model reuse policy transfer small positive impact close distant transfer scenarios performed statistical analysis previous section results table cases policy transfer curves signi cantly higher area standard curves impact policy transfer small practically speaking robust model reuse wisconsin machine learning group working paper probability goal training games standard fig probability scoring goal training -onbreakaway standard policy transfer -onbreakaway -onmovedown eld -onkeepaway average total reward training games standard fig average total reward training -onmovedown eld standard policy transfer -onmovedown eld average game length sec training games standard fig average game length training -onkeepaway standard policy transfer -onkeepaway wisconsin machine learning group working paper table statistical results policy transfer experiments breakaway movedown eld keepaway comparing area curve standard reinforcement learning srl experiment conclusion p-value con dence interval higher con dence srl equivalent higher con dence higher con dence higher con dence policy transfer produces large amount complex advice variant kbkr called extenkbkr handles high advice volumes model reuse method relies lowlevel task-speci q-functions perform transfer transfer method section attempts transfer higher-level knowledge leads larger performance gains skill transfer advice main objective transfer learning determine source-task knowledge general transferable speci non-transferable q-functions combine types knowledge methods model reuse policy transfer q-functions separate skill transfer method designed capture general knowledge source task lter speci knowledge transferring entire policy method transfers skills source target tasks common 
q-functions describe skills method inductive logic programming ilp learn skill concepts generalize games played source task learn rst-order rules general propositional rules variables rule pass teammate capture essential elements passing skill rules passing speci teammates expect common skill elements transfer tasks figure shows skill transfer process context transfer keepaway breakaway keepaway games provide training examples concept states passing teammate good action ilp algorithm learns rule representing pass skill finally mapping applied produce transfer advice breakaway note represent states actions rst-order predicates method learning source target tasks features actions propositional lift rst-order logic temporarily wisconsin machine learning group working paper ilp mapping state distbetween distbetween distbetween action pass outcome caught training examples pass teammate distbetween teammate distbetween skill concept distbetween distbetween prefer pass advice fig showing transfer skills transfer process rst-order representation matching feature action predicates identical domain map assume user mapping logical objects source target tasks keepaway maps breakaway problem partially overlapping feature sets simple solution transfer method ilp algorithm learning advice rules limit search space subset feature predicates feature predicates exist target task advice rules forces algorithm skill nitions relevant target task learning skills ilp algorithms searching space rules prolog-based aleph software package conduct random heuristic search hypothesis space skill transfer method selects rule nds highest score generalization familiar metric put weight rule precision rule recall produce datasets search skill-transfer method examines states games source task selects positive negative examples found states training examples unambiguously positive negative left datasets states detected q-values figure summarizes process robocup good positive conditions met skill performed desired outcome occurs expected q-value recent q-function percentile training set times predicted q-values actions purpose conditions remove ambiguous examples actions good actions good types good negative examples conditions describe type action performed action q-value wisconsin machine learning group working paper action pass teammate outcome caught teammate pass teammate good pass teammate action good pass teammate bad positive pass teammate negative pass teammate reject fig showing select training examples percentile training set q-value skill learned times q-value percentile training set conditions remove similarly ambiguous examples type good negative includes states skill learned desired outcome occur make search space nite replace continuous features distances angles nite sets discrete features rule figure boolean constraint distbetween derived continuous distance feature skill transfer method nds thresholds highest information gain intervals thresholds constraints rules constraints rule found parameters produce reasonable running times robocup adjusted appropriately domains converting skills transfer advice convert skill concept transfer advice apply object mapping propositionalize rule propositionalizing kbkr advice-taking algorithm works propositional advice automated process preserves meaning rst-order rules losing information technical details involved instantiate skills pass teammate target task -onbreakaway produce rules pass pass deal conditions rule body variables rule condition distbetween attacker ectively disjunction conditions distance distance interval disjunctions part wisconsin machine learning group working paper advice language tile features represent recall feature range divided boolean tiles feature falls interval disjunction satis tiles active -onbreakaway distbetween distbetween exact tile boundaries exist target task add tile boundaries feature space transfer advice expressed target-task feature space unknown time source task learned multiple conditions rule refer variable distbetween attacker anglede nedby attacker closestdefender variable attacker represents object clauses system propositionalize clauses separately denes boolean background-knowledge predicate puts simultaneous constraints features newfeature attacker closestdefender dist distbetween attacker ang anglede nedby attacker closestdefender dist ang expresses entire condition feature -onbreak- newfeature newfeature add boolean features considered multi-dimensional tiles target task skill transfer enhance feature space target task advice item produced skill prefer skill actions shared source target task policy transfer set preference amount approximately target-task q-value range user advice compared policy transfer skill transfer produces small number simple interpretable rules introduces possibility user input transfer process users understand transfer advice add specializing rules writing rules non-transferred skills target task skill transfer method optional user advice passing skills transferred keepaway breakaway make distinction passing goal goal objective score goals players prefer passing goal user provide guidance instructing system add condition pass teammate skill wisconsin machine learning group working paper distbetween goal distbetween teammate goal alternatively expert user make system ability features target task advantage approach formally ning feature tiled user rst write nition prolog goaldistance teammate distteammate distbetween teammate goal dista distbetween goal dista distteammate user instruct system add pass teammate rule goaldistance teammate actions transfer scenario target task shoot moveahead users write simple rules approximate skills distbetween goalpart anglede nedby goalpart goalie prefer shoot goalpart actions distbetween goalcenter prefer moveahead moveaway shoot actions user advice natural powerful users facilitate transfer providing mapping skill transfer results figures display results skill-transfer experiments represent close-transfer distant-transfer scenarios previous sections performed experimental methodology mappings user advice experiments listed appendix sections examples learned skill concepts section graphs show skill transfer large positive impact close-transfer distant-transfer scenarios performed statistical analysis previous sections results table cases skill transfer curves signi cantly higher area standard curves comparison con dence intervals table con rms impact larger policy transfer skill transfer robustness policy transfer produce larger performance gains preferred transfer method wisconsin machine learning group working paper probability goal training games standard fig probability scoring goal training -onbreakaway standard skill transfer -onbreakaway -onmovedown eld -onkeepaway average total reward training games standard fig average total reward training -onmovedown eld standard skill transfer -onmovedown eld average game length sec training games standard fig average game length training -onkeepaway standard skill transfer -onkeepaway wisconsin machine learning group working paper table statistical results skill transfer experiments breakaway movedown eld keepaway comparing area curve standard reinforcement learning srl experiment conclusion p-value con dence interval higher con dence higher con dence higher con dence higher con dence srl equivalent analysis skill transfer section discuss additional experiments test boundaries skill transfer impact factors ectiveness skill transfer quality learning source task quality user guidance performed transfer paying attention sourcetask learning curve interesting types source-task learning curves worse transfer choice source runs choose choose expect produce target-task performance figure plots average area curve target task skill transfer area curve source task transfer performed order plot data skill-transfer experiments scale normalize areas group fall correlation coe cient small correlation source-task target-task area 
helpful choose source runs higher area curve impact large factor quality user guidance ects skill transfer simply reasonable non-optimized user advice skill-transfer experiments investigate results produced reasonable variants user easily figure shows learning curves skill transfer keepaway breakaway variants user advice variant ambitious encouraging players shoot variant cautious giving stricter conditions shooting variants listed appendix section figure shows learning curve skill transfer movedown eld breakaway user advice skill transfer produces signi cantly higher area curve standard gain smaller addition user advice produces signi gain results user advice ect performance skill transfer reasonable variants robustness wisconsin machine learning group working paper normalized source task aoc fig plot showing target-task performance correlates source-task performance probability goal training games standard original variant variant fig probability scoring goal training -onbreakaway skill transfer -onkeepaway variants original user advice probability goal training games standard original user advice fig probability scoring goal training -onbreakaway skill transfer -onmovedown eld original user advice wisconsin machine learning group working paper means users worry providing perfect advice order skill-transfer method work approximate user advice signi cantly improve performance future work combine multiple transfer methods achieve higher performance policy transfer skill transfer combined scenario model reuse combined close-transfer scenarios plan extend ilp methods learning multiple-step relational plans single-step rules produce temporally extended actions options macros applied target task advice approach relational plans capture information source task single rules direction work developing advice-taking methods relational reinforcement learning rrl rrl developed dzeroski asgharbeygi transfer advice applied directly rst-order form conclusions reinforcement learners bene signi cantly knowledge transferred previous task advice-taking transfer methods ilp-based skill-transfer method lead robust transfer challenging dissimilar tasks rst-order logic relational information helps separate general speci information source task advice protection negative transfer ects skill transfer experiments demonstrate simple user guidance naturally incorporated advice-based transfer methods resulting transfer acknowledgements research partially supported darpa grant united states naval research laboratory grant public distribution robocup players server code http biostat wisc ml-group robocup andre russell programmable reinforcement learning agents nips asadi papudesi huber learning skill representation hierarchies ective control knowledge transfer icml workshop structural knowledge transfer machine learning wisconsin machine learning group working paper asgharbeygi stracuzzi langley relational temporal erence learning icml clouse utgo teaching method reinforcement learning icml cohen personal communication driessens dzeroski integrating experimentation guidance relational reinforcement learning icml driessens ramon croonenborghs transfer learning reinforcement learning goal policy parametrization icml workshop structural knowledge transfer machine learning dzeroski raedt blockeel relational reinforcement learning icml falkenhainer forbus gentner structure-mapping engine algorithm examples arti cial intelligence ferguson mahadevan proto-transfer learning markov decision processes spectral methods icml workshop structural knowledge transfer machine learning fernandez veloso policy reuse transfer learning tasks erent state action spaces icml workshop structural knowledge transfer machine learning fernandez veloso probabilistic policy reuse reinforcement learning agent aamas gordon subramanian multistrategy learning scheme agent knowledge acquisition informatica gorski laird experiments transfer multiple learning mechanisms icml workshop structural knowledge transfer machine learning konidaris barto autonomous shaping knowledge transfer reinforcement learning icml kuhlmann stone mooney shavlik guiding reinforcement learner natural language advice initial results robocup soccer aaai workshop supervisory control learning adaptive systems lin self-improving reactive agents based reinforcement learning planning teaching machine learning liu stone value-function-based transfer reinforcement learning structure mapping aaai maclin shavlik creating advice-taking reinforcement learners machine learning maclin shavlik torrey walker knowledge-based support vector regression reinforcement learning ijcai workshop reasoning representation learning computer games maclin shavlik torrey walker wild giving advice preferred actions reinforcement learners knowledge-based kernel regression aaai maclin shavlik walker torrey simple ective method incorporating advice kernel methods aaai madden howley transfer experience reinforcement learning environments progressive culty review pages mangasarian shavlik wild knowledge-based kernel approximation jmlr pages wisconsin machine learning group working paper mehta natarajan tadepalli fern transfer variable-reward hierarchical reinforcement learning nips workshop transfer learning muggleton raedt inductive logic programming theory methods journal logic programming pages noda matsubara hiraki frank soccer server tool research multiagent systems applied arti cial intelligence perkins precup options knowledge transfer reinforcement learning technical report um-cs- price boutilier implicit imitation multiagent reinforcement learning icml sammut hurst kedzier michie learning icml sherstov stone action-space knowledge transfer mdp formalism suboptimality bounds algorithms colt singh transfer learning composing solutions elemental sequential tasks machine learning soni singh homomorphisms transfer options continuous reinforcement learning domains aaai srinivasan aleph manual stone sutton scaling reinforcement learning robocup soccer icml stracuzzi asgharbeygi transfer knowledge structures relational temporal erence learning icml workshop structural knowledge transfer machine learning sutton learning predict methods temporal erences machine learning pages sutton barto reinforcement learning introduction mit press taylor stone behavior transfer value-function-based reinforcement learning aamas taylor stone liu functions rl-based behavior transfer comparative study aaai torrey shavlik walker maclin relational skill transfer advice taking ecml torrey shavlik walker maclin relational skill transfer advice taking icml workshop structural knowledge transfer machine learning torrey walker shavlik maclin advice transfer knowledge acquired reinforcement learning task ecml walsh littman transferring state abstractions mdps icml workshop structural knowledge transfer machine learning watkins learning delayed rewards technical report phd thesis cambridge psychology dept wisconsin machine learning group working paper appendix appendix speci mappings user advice skill concepts learned mappings model reuse policy transfer mappings model reuse policy transfer match propositional features actions source target -onbreakaway -onbreakaway multiple mappings knowledge pass action transfers pass pass close transfer feature action sets similar straightforward map mappings shown wisconsin machine learning research group working paper building genome expression models microarray expression data text michael molla wisconsin-madison ave madison peter andrae victoria wellington jude shavlik wisconsin-madison microarray expression data generated gigabyte world undoubted exponential increases annotated genomic data rapidly pouring public databases goal develop automated ways combining sources information produce insight operation cells conditions approach machine-learning techniques identify characteristics genes upregulated down-regulated microarray experiment seek models accurate easy interpret paper explores effectiveness algorithms task pfoil standard machine-learning rule-building algorithm gorb rulebuilding algorithm devised permutation test evaluate statistical significance learned models paper reports experiments actual coli microarray data discussing strengths weaknesses algorithms demonstrating trade-offs accuracy comprehensibility introduction rna medium organism genes produce specific proteins building blocks life understanding organism regulates production specific rna sequences crucial understanding mechanism organism functions expression level gene measure amount rna produced gene time microarrays quickly inexpensively measure expression levels thousands genes simultaneously microarrays employ fluorescently labeled fragments rna bind locations microarray surface scanning laser measures intensity fluorescence point surface levels expression specific rnas inferred intensity values measured laser microarrays measure expression levels specific physical event genes changed expression levels response event genes expression levels increased up-regulated levels decreased downregulated wisconsin machine learning research group working paper development microarrays large collections experimental data led automated methods assist interpretation microarray-based biomedical experiments present method creating partial interpretations microarray experiments combine expression-level data textual information individual genes interpretations consist models characterize genes expression levels upor downregulated goal models assist human scientist understanding results experiment approach machine learning create models accurate comprehensible order make comprehensible models expressed terms english words text descriptions individual genes descriptions curated swissprot protein database bairoch apweiler annotations proteins text protein generated gene description gene models consist sets words descriptions characterize up-regulated down-regulated genes note text descriptions genes generate interpretations microarray experiments experiment genes up-regulated down-regulated text description gene experiments basic task numeric rna-expression levels gene gene array conditions event antibiotic treatment gene microarray swissprot text describing protein produced gene produce model accurately characterizes genes up-regulated down-regulated response event article models sets disjunctive if-then rules form word word gene annotation word word present gene up-regulated shorthand remainder paper present part rules focus up-regulated group arbitrary choice list rule word word word word rules disjunctive rules match gene annotation model characterizes gene up-regulated rule matches model characterizes gene down-regulated work related recent attempts machine learning predict gene-regulation levels brown dudoit xing focus goal predict gene-regulation levels automatically generate human-readable characterizations upor down-regulated genes scientists generate hypotheses explain experiments wisconsin machine learning research group working paper investigate rule-building algorithm design standard successful algorithm machine-learning literature evaluating satisfies desiderata accuracy comprehensibility standard approach compare pfoil rule learner based propositional logic current set experiments gene up-regulated ratio rnaafter rnabefore gene expression ratio greater ratio down-regulated commonly discard ambiguous genes expression ratio train learners data set up-regulated down-regulated genes attempt model ambiguous genes previous paper molla experiments pitted successful standard machine-learning algorithms task interpreting gene chip expression data text annotating genes algorithms pfoil mooney bayes mitchell paper introduce algorithm gorb propose evaluation method machine learning algorithms previous paper crossvalidation dominant technique machine-learning evaluation paper argue statistical method called permutation test metric characterization tasks cross-validation good measure predictive accuracy table table feature action mappings -onbreakaway -onbreakaway model reuse policy transfer obvious parts shown bold text -onbreakaway -onbreakaway distbetween distbetween distbetween goalie distbetween goalie distbetween goalie distbetween goalie anglede nedby goalie anglede nedby goalie distbetween goalleft distbetween goalleft distbetween goalright distbetween goalright distbetween goalcenter distbetween goalcenter anglede nedby goalleft goalie anglede nedby goalleft goalie anglede nedby goalright goalie anglede nedby goalright goalie anglede nedby goalcenter goalie anglede nedby goalcenter goalie anglede nedby toprightcorner goalcenter anglede nedby toprightcorner goalcenter timeleft timeleft moveahead moveahead moveaway moveaway moveright moveright moveleft moveleft shoot goalleft shoot goalleft shoot goalright shoot goalright shoot goalcenter shoot goalcenter pass pass distbetween distbetween distbetween goalie distbetween goalie distbetween goalie distbetween goalie anglede nedby goalie anglede nedby goalie distbetween goalleft distbetween goalleft distbetween goalright distbetween goalright distbetween goalcenter 
distbetween goalcenter anglede nedby goalleft goalie anglede nedby goalleft goalie anglede nedby goalright goalie anglede nedby goalright goalie anglede nedby goalcenter goalie anglede nedby goalcenter goalie anglede nedby toprightcorner goalcenter anglede nedby toprightcorner goalcenter timeleft timeleft pass pass wisconsin machine learning group working paper table feature action mappings -onmovedown eld -onmovedown eld model reuse policy transfer obvious parts shown bold text -onmovedown eld -onmovedown eld distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween mindistdefender distbetween mindistdefender distbetween mindistdefender distbetween mindistdefender anglede nedby minangledefender anglede nedby minangledefender anglede nedby minangledefender anglede nedby minangledefender disttorightedge disttorightedge disttorightedge disttorightedge disttorightedge disttorightedge timeleft timeleft moveahead moveahead moveaway moveaway moveright moveright moveleft moveleft pass pass pass pass distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween mindistdefender distbetween mindistdefender distbetween mindistdefender distbetween mindistdefender anglede nedby minangledefender anglede nedby minangledefender anglede nedby minangledefender anglede nedby minangledefender disttorightedge disttorightedge disttorightedge disttorightedge disttorightedge disttorightedge timeleft timeleft pass pass mappings -onmovedown eld -onmovedown eld -onkeepaway -onkeepaway similar nature default map furthest teammate source furthest teammate target secondary mapping cover furthest teammate mappings shown tables -onmovedown eld -onbreakaway single mapping relates players one-to-one distant transfer features actions left mapping chose leave move actions felt transfer scenario mapping shown table mapping -onkeepaway -onbreakaway fewer features actions common features keepaway mapped constants breakaway chose constants thought typical values features mapping shown table wisconsin machine learning group working paper table feature action mappings -onkeepaway -onkeep- model reuse policy transfer obvious parts shown bold text -onkeepaway -onkeepaway distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween mindistkeeper distbetween mindistkeeper distbetween mindistkeeper distbetween mindistkeeper anglede nedby minanglekeeper anglede nedby minanglekeeper anglede nedby minanglekeeper anglede nedby minanglekeeper distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter holdball holdball pass pass pass pass distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween mindistkeeper distbetween mindistkeeper distbetween mindistkeeper distbetween mindistkeeper anglede nedby minanglekeeper anglede nedby minanglekeeper anglede nedby minanglekeeper anglede nedby minanglekeeper distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter pass pass table feature action mapping -onmovedown eld -onbreak- model reuse policy transfer obvious parts shown bold text -onmovedown eld -onbreakaway distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween mindistdefender permutation test distbetween applied mindistdefender learning distbetween mindistdefender distbetween mindistdefender anglede nedby minangledefender anglede nedby minangledefender anglede nedby minangledefender anglede nedby minangledefender disttorightedge distbetween goalcenter disttorightedge distbetween goalcenter disttorightedge distbetween goalcenter timeleft timeleft moveahead moveahead pass pass pass pass wisconsin machine learning group working paper table feature action mapping -onkeepaway -onbreak- model reuse policy transfer obvious parts shown bold text -onkeepaway -onbreakaway distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween distbetween mindisttaker distbetween mindistdefender distbetween mindisttaker distbetween mindistdefender anglede nedby minangletaker anglede nedby minangledefender anglede nedby minangletaker anglede nedby minangledefender distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter distbetween eldcenter pass pass pass pass mappings skill transfer mappings skill transfer match objects source target tasks robocup objects players goal parts single mappings objects rules variables cover players -onbreakaway -onbreakaway mapping straightforward goalie goalie goalleft goalleft goalright goalright goalcenter goalcenter -onmovedown eld -onmovedown eld similar indices furthest players change mindistdefender mindistdefender mindistdefender mindistdefender minangledefender minangledefender minangledefender minangledefender -onkeepaway -onkeepaway indices change wisconsin machine learning group working paper mindisttaker mindisttaker mindisttaker mindisttaker minangletaker minangletaker minangletaker minangletaker -onmovedown eld -onbreakaway defender mindistdefender mindistdefender mindistdefender mindistdefender minangledefender minangledefender minangledefender minangledefender -onkeepaway -onbreakaway defender mindisttaker mindistdefender mindisttaker mindistdefender minangletaker minangledefender minangletaker minangledefender user advice skill transfer subset user advice skill transfer experiments distbetween goalright anglede nedby goalright goalie prefer shoot goalright actions distbetween goalleft anglede nedby goalleft goalie prefer shoot goalleft actions distbetween goalcenter prefer moveahead moveaway shoot add pass teammate goaldistance teammate wisconsin machine learning group working paper scenario add user advice covers skills transferred transferring keepaway breakaway shoot parts movedown eld breakaway variants keepaway breakaway advice section shown variant encourages attempted shots user choose assumption attempts lead success distbetween goalright anglede nedby goalright goalie prefer shoot goalright actions distbetween goalleft anglede nedby goalleft goalie prefer shoot goalleft actions distbetween goalcenter prefer moveahead moveaway shoot add pass teammate goaldistance teammate variant opposite stricter conditions shooting user choose assumption fewer safer attempts lead success distbetween goalright anglede nedby goalright goalie prefer shoot goalright actions distbetween goalleft anglede nedby goalleft goalie prefer shoot goalleft actions distbetween goalcenter prefer moveahead moveaway shoot add pass teammate goaldistance teammate sample skill concepts learned skill transfer examples skills algorithm learned skill transfer -onbreakaway rule shoot shoot goalpart distbetween goalcenter anglede nedby goalpart goalie distbetween oppositepart goalpart anglede nedby oppositepart goalpart goalie anglede nedby goalcenter goalie wisconsin machine learning group working paper rule requires large open shot angle minimum distance goal angle constraints restrict goalie position small area -onmovedown eld rule pass pass teammate distbetween teammate distbetween teammate anglede nedby teammate minangledefender teammate tasks disttorightedge teammate model distbetween characterization opponent data rule speci accurate acceptable predictor range desired distance output receiving method teammate measure minimum desired pass property angle accuracy requires record accuracy teammate close model classifying nish line examples eld entire data set opponent close contrast intercept previous work recorded -onkeepaway accuracy rule models pass held-out pass test teammate set ensure distbetween teammate accuracy measurement eldcenter distbetween statistically valid teammate mindisttaker teammate test anglede set nedby teammate paper repeatedly minangletaker randomly permute labels examples train learner record accuracy resulting model time model real data significant accuracy model significantly accuracy models permuted data sets section explains permutation test desired property human comprehensibility mentioned comprehensibility reason express rules terms english words actual comprehensibility model difficult measure crude approximation counting number distinct swissprot words appearing model section presents machine-learning algorithms investigate experiments section explains experimental methodology section presents discusses experimental results obtained data blattner coli laboratory wisconsin section describes related work final section describes planned follow-up research wisconsin machine learning research group working paper summarizes lessons learned work create tool englishtext protein annotations assist interpretation microarray experiments algorithm descriptions section describes algorithms pfoil mooney gorb general-purpose one-step-look-ahead rule builder algorithms input collection training instances case genes labeled belonging classes call vector booleanvalued features feature corresponds word present absent text description gene algorithms produce model categorize gene basis feature values words describing pfoil pfoil mooney propositional version foil quinlan rulebuilding algorithm incrementally builds rules characterize instances class data set foil builds rules first-order logic language rules conjunctions features case english words negation logical variables recursive interpreted first-order reasoning engine prolog pfoil simpler propositional language builds rules conjunctions features pfoil rules interpreted straightforwardly rule covers instance feature rule true instance domain rule specifies words present gene annotation pfoil builds set rules constructing rule time constructs rule adding feature negation time current rule step chooses feature maximizes performance rule foilgain measure stops adding rule rule covers positive instances remaining features positive foilgain rule complete algorithm removes positive instances covered rule data set builds rule positive examples covered learned rule foilgain measure improvement obtained adding feature rule trade-off coverage rule number positive instances class covered rule increase precision rule fraction instances covered rule positive foilgain rule log log wisconsin machine learning research group working paper number positive negative instances covered rule number positive negative instances covered feature added rule originally mooney pfoil prune rule-set pfoil constructing rules covered positive instances data set noise result large set rules specific instances address problem extended pfoil include rule-pruning stage lines pruning foil pruning stage algorithm repeatedly removes single feature rules choosing feature removal results highest accuracy remaining rule set features removed rule rule removed rule set halting pruning accuracy peaks experiments continue pruning rule set empty order explore trade-off comprehensibility accuracy gorb gorb algorithm table similar pfoil terms input output searches identical hypothesis space rules differs searches space pfoil gorb explores hypothesis space adding feature time ever-expanding disjunction conjunctive rules difference building rules sequentially rule time gorb considers adding feature existing rule starting rule illustrated table letter representing feature 
step current rule set illustrated difference information-gain-based heuristic decide feature add gorb computes accuracy result addition feature time-consuming method directly seeks improve accuracy desired property version pfoil included pruning phase works identically pfoil pruning stage repeatedly removing single feature rules choosing feature removal results highest accuracy remaining rule set worth noting phase pfoil gorb essentially gorb hypothesis space search reverse searching space features addition results accuracy pruning algorithm searches space features included model removal results accuracy wisconsin machine learning research group working paper table rule construction gorb start rule-set prevaccuracy curaccuracy curaccuracy prevaccuracy prevaccuracy curaccuracy curaccuracy feature measure accuracy rule-set data rule added consisting current feature feature up-regulated accuracy curaccuracy set curaccuracy rule rule-set measure accuracy rule set data current feature added current rule rule up-regulated rule feature up-regulated accuracy curaccuracy set curaccuracy repeat process negation current feature curaccuracy prevaccuracy add feature generated curaccuracy rule rule-set existing curaccuracy measured return rule-set permutation test property pfoil gorb guaranteed find model data set relationship descriptions labels space disjunctions conjunctive rules large describe set instances labels worst case algorithms generate distinct rule instance data sets pathological case genes identical annotation up-regulated down-regulated table hypothesis space search pfoil gorb step step step step step step pfoil rule rule rule rule rule rule rule rule rule cde teammate anglede nedby otherteammate minangletaker otherteammate rule speci minimum pass angle open distance receiving teammate requires teammate close center eld maximum pass angle alternate teammate parts rules unexpected make sense hindsight shoot rule speci minimum distance goal maximum distance large shot angles reasonable distances shows advantages advice learned transfer user advice 
rule cde rule gorb rule rule rule rule rule ace rule ace rule rule rule rule rule rule rule rule wisconsin machine learning research group working paper instances features algorithms find compact models exploit possibly random associations instances labels fact model produced algorithms evidence model represents meaningful relationship descriptions labels show model significant show algorithm produced model quality result random associations standard showing significance model classification prediction task test accuracy held-out test set model result chance associations training set perform poorly test set approach prediction tasks accuracy model test set good estimate accuracy future instances important prediction task characterization task future instances predict training data complete data set measuring accuracy test set measure limited size data set means holding data training set reduce quality models learned permutation test measuring significance characterization task require holding data unrelated prediction permutation test good statistical test determine significance comparing results algorithm real data set results algorithm permutations real data set meaningful relationships lost model real data set models permuted data sets considered significant model real data set models permuted data sets model taking advantage semantically meaningful relationships data set considered significant application quality model accuracy data set size model permutation test compares accuracy model real data set accuracy models size produced data sets obtaining randomly permuting labels gene upor downregulated real data set model real data set considered significant accuracy higher accuracies models permuted data sets important permutation test permutations considered obtain statistically valid result number permutations required depends data found permutations adequate application obtain measure significance determining probability random permuted data set result model good real model obtaining accurate measure require permutations pursued approach wisconsin machine learning research group working paper experimental methodology data microarray experiments performed blattner coli sequencing laboratory wisconsin current computational experiments methods experiments measure expression approximately forty-two hundred genes coli conditions conditions include heat cold shock antibiotics periods time order measure change expression due condition compare expression levels measured replicate microarrays measured standard conditions definition experiment includes average up-regulated genes down-regulated genes unregulated genes construct text description genes words text fields swissprot database include comment fields exception database cdb mass spectrometry cms topics description field organism classification field keyword field title fields exception titles complete sequence coli genome implemented algorithms programming language experimental results discussion table shows rulesets pfoil gorb typical run antibiotic testbeds features added rule sets pruned features shows format models similar content case models generated algorithms share word surprising characterizing experiment word biosynthesis reflects underrepresentation basic biosynthetic genes up-regulated symport interesting rules comprise rule predicting upregulation gorb model coded designation enzyme commission organization classifies enzymes function code nucleotidyltransferases enzymes move nucleotides symport mechanism molecular transport up-regulation provide hint types processes involved coli antibiotic shock response figures show results permutation tests algorithms experimental data sets results step pruning algorithms data set data points plotted baseline http genome wisc details experiments data material non-insulating insulating ceramic paper styrofoam open-cell foam wisconsin machine learning research group working paper accuracy experiment accuracy achieved learner chose frequent category up-regulated real data permuted data pfoil make larger models accuracy pfoil complete model real data comparable accuracy gorb models similar size compared gorb substantially accurate gorb complete rule set words accurate describing data set time pfoil rule set pruned rules accuracy dropped striking fact algorithms perform real data permuted data gorb margin wider remains range comprehensibility words crucial importance explained earlier means gorb making models rely real patterns data patterns removed gorb performs worse figure shows average amount performance learning real data beats performance permuted data rule sizes values figure represent gaps permuted data labels line real data labels line figures averaged experimental conditions numbers important measure extent learner performance depends real regularities data opposed absolute accuracy sources overfitting gorb pfoil respect fact models size smaller pfoil perform permuted data real data presently explanation behavior occurred random chance table sample small rule-sets built experimental cold shock data pfoil disjunctive rules rule rule transport membrane rule hypothetical rule hypothetical gene rule oxidoreductase gorb disjunctive rules rule ribosomal rule factor control activation rule similarity structural rule mutation rule sos rule rnabinding rule similar wisconsin machine learning research group working paper figure pfoil heat shock model size num ber features permuted data labels real data labels baseline figure gorb heat shock odel size num ber feature permuted data labels real data labels baseline figure pfoil acid shock ode size num ber features permuted data labels real data labels baseline figure gorb acid shock model size number features permuted data labels real data labels baseline figure pfoil cold shock odel size num ber features permuted data labels real data labels baseline figure pfoil antibiotic shock model size number features permuted data labels real data labels baseline figure gorb antibiotic shock odel size num ber features permuted data labels real data labels baseline figure 
gorb cold shock model size num feature permuted data labels real data labels baseline wisconsin machine learning research group working paper discussion structure gorb rule set table typical generated gorb rule starts high frequency word case protein give high coverage low accuracy rest rule features negations frequently occurring words specialize rule improve accuracy rules specialized rules consisting words cover smaller number instances covered rule features rules generally low frequency words rules high accuracy pruning process pfoil generate rules consisting features rules typically low accuracy created pruning specializing features rules consequence pfoil pruning process results bumpy accuracy curve large dips accuracy rule pruned single feature rise accuracy rule removed hand gorb pruning curves smoother gorb constructs small rules smoothness desirable makes smoother tradeoff accuracy comprehensibility practical benefit tool based approach gorb rule sets accurate significant pfoil rule sets range rule set sizes difference heavily pruned models striking gorb appears characterizing data allowed words models suggests job pfoil identifying real regularities data accurate gorb strategy suffer large number rules relevant context greater rule set single infrequently occurring word descriptive set genes covered rules remedy algorithm biased rules similar length individual rules high precision moderate coverage measure comprehensibility methods good short models tend good longer clear mentioned earlier length crude measure ready attainment final answer comprehensibility involve human testers related work wisconsin machine learning research group working paper great deal research text mining involves biomedical tasks hearst lindi system searches medical literature text relating subject problem make logical connections form hypothesis benefits approach researchers advance expression data textual descriptions genes represented expression data system makes automated pass discovering interesting pubgene tool jennssen interprets gene-expression data based textual data big difference pubgene compiles clusters text ahead time match expression data already-made cluster tool designed expression data define models masys text genes explain experiments cluster expression values experiments text explain clusters text directly learning process explain single experiments future directions conclusions presented approach aiding interpretation microarray experiments based machine learning swissprot text representation microarray genes argue important properties computational tool produce models accurate readable empirically compared algorithm design widely successful algorithm pfoil coli microarray experiment evaluating approaches respect desiderata evaluated significance models permutation test shown algorithms find characterizations data reasonable accuracy algorithm job data set algorithm produces models greater significance pfoil models current limitation approach relies text sequence data plan explore methods make sequence data text annotating genes enhancement increase textual data abstracts journal articles logical step sequences words common phrases individual words criterion important stability learned models change experiment repeated slightly conditions plan apply method takes advantage replicate data previously devised method measuring stability molla future direction move method simply two-fold change expression definition upand down-regulation wisconsin machine learning research group working paper intend apply method gauging fold-change newton redo experiments presented acknowledgments jeremy glasner fred blattner coli lab wisconsin-madison providing data helping understand research funded grants nlm nih nih bairoch apweiler swiss-prot protein sequence database supplement trembl nucl acids res brown lin cristianini sugnet furey ares haussler knowledge-based analysis microarray gene expression data support vector machines proc natl acad science dudoit yang callow speed statistical methods identifying differentially expressed genes replicated cdna microarray experiments ucberkeley statistics dept good resampling methods practical guide data analysis edition birkhduser boston hearst untangling text data mining proc annual meetg assoc ling jenssen t-k greid komorowski hovig literature network human genes high-throughput gene-expression analysis nature genetics manning sch tze foundations statistical natural language processing mit press masys keyword hierarchies interpret gene expression patterns bioinformatics mitchell machine learning mcgraw-hill mladeni grobelnik feature selection unbalanced class distribution naive bayes proceedings international conference machine learning icmlmorgan kaufmann publishers san francisco molla andreae glasner blattner shavlik interpreting microarray expression data text annotating genes proceedings conf computational biology genome informatics mooney encouraging experimental results learning cnf machine learning newton kendziorski richmond blattner tsui differential variability expression ratios improving statistical inference gene expression microarray data journal computational biology porter algorithm suffix stripping program quinlan learning logical descriptions relations machine learning xing jordan karp feature selection high-dimensional genomic microarray data proceedings international conference machine learning morgan kaufmann 
feature price activation low-price med-price high-price bias size bus bread box car bus house bias true bias chapter generative discriminative classifiers naive bayes logistic regression machine learning copyright tom mitchell rights reserved draft september distribute author permission rough draft chapter intended inclusion edition textbook machine learning mitchell mcgraw hill educational purposes duplicate repost internet online copies materials related book visit web site cmu tom mlbook html send suggestions improvements suggested exercises tom mitchell cmu learning classifiers based bayes rule relationship supervised learning function approximation problems bayesian reasoning begin design learning algorithms based bayes rule supervised learning problem approximate unknown target function equivalently yjx begin assume boolean-valued random variable vector boolean attributes words xni boolean random variable denoting ith attribute applying bayes rule yijx represented copyright tom mitchell yijx xkjy xkjy represents mth summation denominator legal values random variable learn yjx training data estimate xjy estimates bayes rule determine yjx instance note notation consistently upper case symbols refer random variables including vector non-vector variables vector subscripts refer random variable feature lower case symbols refer values random variables refer random variable taking jth abbreviate omitting variable names abbreviating jjy jjyk write refer expected superscripts index training examples refers random variable jth training denote indicator function logical argument true dfxg operator denote number elements set satisfy property hat estimates estimated unbiased learning bayes classifiers impractical train bayes classifier estimating xjy reasonable training data required obtain reliable estimates distributions assume training examples generated drawing instances random unknown underlying distribution allowing teacher label hundred independently drawn training examples suffice obtain maximum likelihood estimate percent correct boolean variable accurately estimating xjy typically requires examples number parameters estimate boolean vector boolean attributes case estimate set parameters xijy index takes values vector values takes values estimate approximately parameters calculate exact number required chapter edition machine learning copyright tom mitchell parameters note fixed sum values compute independent parameters values estimate total parameters corresponds distinct parameters distinct instances instance space obtain reliable estimates parameters observe distinct instances multiple times unrealistic practical learning domains naive bayes algorithm intractable sample complexity learning bayesian classifiers ways reduce complexity naive bayes classifier making conditional independence assumption dramatically reduces number parameters estimated modeling xjy original conditional independence definition random variables conditionally independent probability distribution governing independent xijy xijz boolean random variables describe current weather rain hunder lightning assert hunder independent rain lightning lightning hunder lightning additional information hunder provided rain clear dependence hunder rain general conditional dependence lightning derivation naive bayes algorithm naive bayes algorithm classification algorithm based bayes rule assumes attributes conditionally independent assumption dramatically simplifies representation xjy problem estimating training data case case xjy copyright tom mitchell line general property probabilities line directly definition conditional independence generally attributes conditionally independent xnjy xijy notice boolean variables parameters define xikjy dramatic reduction compared parameters needed characterize xjy make conditional independence assumption derive naive bayes algorithm assuming general discrete-valued variable attributes discrete realvalued attributes goal train classifier output probability distribution values instance classify expression probability kth bayes rule ykjx xnjy xnjy sum values assuming conditionally independent equation rewrite ykjx xijy xijy equation fundamental equation naive bayes classifier instance xnew xni equation shows calculate probability observed attribute values xnew distributions xijy estimated training data interested probable naive bayes classification rule argmaxy xijy xijy simplifies denominator depend argmaxy xijy copyright tom mitchell naive bayes discrete-valued inputs summarize precisely define naive bayes learning algorithm describing parameters estimated estimate input attributes discrete values discrete variable taking values learning task estimate sets parameters jjy input attribute values values note njk parameters note independent satisfy pair values addition estimate parameters define prior probability pik note parameters independent estimate parameters maximum likelihood estimates based calculating relative frequencies events data bayesian map estimates augmenting observed data prior distributions values parameters maximum likelihood estimates set training examples jjy dfxi ykg dfy ykg dfxg operator returns number elements set satisfy property danger maximum likelihood estimate result estimates data happen training examples satisfying condition numerator avoid common smoothed estimate effectively adds number additional hallucinated examples assumes hallucinated examples spread evenly values smoothed estimate jjy dfxi ykg dfy ykg number distinct values determines strength smoothing number hallucinated examples expression corresponds map estimate assume dirichlet prior distribution parameters equal-valued parameters set approach called laplace smoothing maximum likelihood estimates pik pik dfy ykgjdj copyright tom mitchell jdj denotes number elements training set alternatively obtain smoothed estimate equivalently map estimate based dirichlet prior pik parameters assuming equal priors pik expression pik dfy ykg ljdj number distinct values determines strength prior assumptions relative observed data naive bayes continuous inputs case continuous inputs continue equations basis designing naive bayes classifier continuous choose represent distributions xijy common approach assume discrete distribution continuous gaussian defined standard deviation specific order train naive bayes classifier estimate standard deviation gaussians xijy attribute note parameters estimated independently estimate priors pik model summarizes gaussian naive bayes classifier assumes data generated mixture class-conditional dependent class variable gaussians naive bayes assumption introduces additional constraint attribute values independent mixture components problem settings additional information introduce additional assumptions restrict number parameters complexity estimating reason noise observed common source assume identical attribute class homework exercise issue maximum likelihood estimates mle maximum posteriori map estimates parameters maximum likelihood estimator copyright tom mitchell superscript refers jth training note role select training examples maximum likelihood estimator maximum likelihood estimator biased minimum variance unbiased estimator mvue logistic regression logistic regression approach learning functions form yjx case discrete-valued xni vector discrete continuous variables section primarily case boolean variable order simplify notation final subsection extend treatment case takes finite number discrete values logistic regression assumes parametric form distribution yjx directly estimates parameters training data parametric model assumed logistic regression case boolean exp wixi exp wixi exp wixi notice equation directly equation sum probabilities equal highly convenient property form yjx leads simple linear expression classification classify generally assign maximizes ykjx put assign label condition holds substituting equations exp wixi copyright tom mitchell exp figure form logistic function logistic regression yjx assumed follow form taking natural log sides linear classification rule assigns label satisfies wixi assigns interestingly parametric form yjx logistic regression precisely form implied assumptions gaussian naive bayes classifier view logistic regression closely related alternative gnb produce results cases 
form yjx gaussian naive bayes classifier derive form yjx entailed assumptions gaussian naive bayes gnb classifier showing precisely form logistic regression summarized equations gnb based modeling assumptions boolean governed bernoulli distribution parameter xni continuous random variable xijy gaussian distribution form conditionally independent note assuming standard deviations vary attribute attribute depend copyright tom mitchell derive parametric form yjx set gnb assumptions general bayes rule write xjy xjy xjy dividing numerator denominator numerator yields xjy xjy equivalently exp xjy xjy conditional independence assumption write exp xijy xijy exp pipi xijy xijy note final step expresses terms binomial parameter summation denominator equation assumption xijy gaussian expand term xijy ijy exp exp lnexp copyright tom mitchell note expression linear weighted sum substituting expression back equation exp pipi equivalently exp wixi weights pipi exp wixi exp wixi estimating parameters logistic regression subsection proves yjx expressed parametric form equations gaussian naive bayes assumptions detailed weights terms parameters estimated gnb classifier describe alternative method estimating weights interested alternative reasons form yjx assumed logistic regression holds problem settings gnb problem detailed section general method estimating broad range cases cases suspect gnb assumptions perfectly satisfied case estimate parameters directly data intermediate step estimating gnb parameters forces adopt stringent modeling assumptions reasonable approach training logistic regression choose parameter values maximize conditional data likelihood conditional data likelihood probability observed values training data conditioned values choose parameters satisfy argmaxw ljxl wni vector parameters estimated denotes observed lth training denotes observed copyright tom mitchell lth training expression argmax conditional data likelihood include conditional emphasize expression function attempting maximize equivalently work log conditional likelihood argmaxw lnp ljxl conditional data log likelihood denote written lnp jxl lnp jxl note utilizing fact values terms expression non-zero derivation consistent common usage section flip assignment boolean variable assign exp wixi exp wixi exp wixi case reexpress log conditional likelihood lnp jxl lnp jxl jxl jxl lnp jxl wixli exp wixli xli denotes lth training note superscript related log likelihood function closed form solution maximizing respect common approach gradient ascent work gradient vector partial derivatives ith component vector gradient form jxl ljxl logistic regression prediction equations weights accommodate weight assume illusory expression derivative intuitive interpretation term inside parentheses simply prediction error difference copyright tom mitchell observed predicted probability note jxl prefer jxl makes jxl equal error term multiplied xli accounts magnitude wixli term making prediction formula derivative standard gradient ascent optimize weights beginning initial weights repeatedly update weights direction gradient changing ith weight xli jxl small constant determines step size conditional log likelihood concave function gradient ascent procedure converge global maximum gradient ascent greater detail chapter mitchell cases computational efficiency important common variant gradient ascent called conjugate gradient ascent converges quickly regularization logistic regression overfitting training data problem arise logistic regression data high dimensional training data sparse approach reducing overfitting regularization create modified penalized log likelihood function penalizes large values approach penalized log likelihood function argmaxw lnp ljxl jjwjj adds penalty proportional magnitude constant determines strength penalty term penalty term interpreted result imposing normal prior variance related note normal lnp yield term proportional jjwjj penalized log likelihood function easy rederive gradient descent rule derivative penalized log likelihood function similar earlier derivative additional penalty term jxl modified gradient descent rule xli jxl cases prior knowledge values specific derive similar penalty term normal prior non-zero copyright tom mitchell logistic regression functions discrete values considered logistic regression learn yjx case boolean variable generally discrete values ykg form ykjx ykjx exp wkixi exp jixi ykjx exp jixi denotes weight jth class input easy earlier expressions case boolean equations special case expressions note form expression ykjx assures ykjx primary difference expressions boolean takes values construct linear expressions capture distributions values distribution final kth simply minus probabilities values case gradient descent rule regularization xli jjxl lth training equal note earlier learning rule equation special case learning rule case quantity inside parentheses viewed error term estimated conditional probability jjxl perfectly matches observed relationship naive bayes classifiers logistic regression summarize logistic regression directly estimates parameters yjx naive bayes directly estimates parameters xjy call discriminative classifier generative classifier showed assumptions variant gaussian naive bayes classifier imply parametric form yjx logistic regression showed parameters logistic regression copyright tom mitchell expressed terms gaussian naive bayes parameters fact gnb assumptions hold asymptotically number training examples grows infinity gnb logistic regression converge identical classifiers algorithms differ interesting ways gnb modeling assumptions hold logistic regression gnb typically learn classifier functions case asymptotic number training examples approach infinity classification accuracy logistic regression asymptotic accuracy gnb logistic regression consistent naive bayes assumption input features conditionally independent rigidly tied assumption naive bayes data disobeys assumption conditional likelihood maximization algorithm logistic regression adjust parameters maximize fit conditional likelihood data resulting parameters inconsistent naive bayes parameter estimates gnb logistic regression converge asymptotic accuracies rates jordan show gnb parameter estimates converge asymptotic values order logn examples dimension contrast logistic regression parameter estimates converge slowly requiring order examples authors show data sets logistic regression outperforms gnb training examples gnb outperforms logistic regression training data scarce main points chapter include bayes rule basis designing learning algorithms function approximators learn target function equivalently yjx training data learn estimates xjy examples classified estimated probability distributions bayes rule type classifier called generative classifier view distribution xjy describing generate random instances conditioned target attribute learning bayes classifiers typically requires unrealistic number training examples jxj training examples instance space form prior assumption made naive bayes classifier assumes attributes describing conditionally independent assumption dramatically reduces number parameters copyright tom mitchell estimated learn classifier naive bayes widely learning algorithm discrete continuous vector discrete-valued attributes naive bayes learning algorithms viewed linear classifiers naive bayes classifier corresponds hyperplane decision surface statement holds gaussian naive bayes classifiers variance feature assumed independent class logistic regression function approximation algorithm training data directly estimate yjx contrast naive bayes sense logistic regression referred discriminative classifier view distribution yjx directly discriminating target instance logistic regression linear classifier linear classifiers produced logistic regression gaussian naive bayes identical limit number training examples approaches infinity provided naive bayes assumptions hold assumptions hold naive bayes bias perform accurately logistic regression 
limit put naive bayes learning algorithm greater bias lower variance logistic regression bias actual data naive bayes preferred logistic regression preferred view function approximation learning algorithms statistical estimators functions conditional distributions yjx estimate yjx sample training data statistical estimators characterize learning algorithms bias expected variance samples training data reading wasserman describes reweighted squares method logistic regression jordan provide theoretical experimental comparison naive bayes classifier logistic regression exercises beginning chapter remarked hundred training examples suffice obtain estimate percent correct describe conditions confidence interval estimate copyright tom mitchell learning function boolean boolean variable continuous variable state parameters estimated define naive bayes classifier case give formula computing yjx terms parameters feature values section showed boolean xni vector continuous variables assumptions gaussian naive bayes classifier imply yjx logistic function parameters exp wixi exp wixi exp wixi case boolean xni vector boolean variables prove case yjx form logistic regression discriminative counterpart naive bayes generative classifier boolean features hints simple notation boolean variables parameter define xijy define case similarly denote notice notation represent xijy xijy xii note term equal exponent similarly term equal exponent based suggestion sandra zilles question asks relationship map hypothesis bayes optimal hypothesis hypothesis space defined set instances hypotheses equal prior probabilities suppose arbitrary set training data calculate posterior probabilities based choose map hypothesis calculate bayes optimal hypothesis suppose find bayes optimal classifier equal generally case bayes optimal hypothesis corresponds averaging hypotheses create hypothesis equal bayes copyright tom mitchell optimal classifier respect classifies instance bayes optimal classifier create hypothesis space train training data map hypothesis bayes optimal classifier respect equivalent hint answer depends priors assign hypotheses give constraints priors assure answers acknowledgements receiving helpful comments earlier drafts chapter nathaniel fairfield vineet kumar andrew mccallum anand prahlad wei wang geoff webb sandra zilles mitchell machine learning mcgraw hill jordan discriminative generative classifiers comparison logistic regression naive bayes neural information processing systems jordan wasserman statistics springer-verlag 
bias link weights link weights kbann cobweb standard-backprop pebls perceptron nearest-neighbor relative information score -fold leave lpp ilvgeda arabadtrp tyrt rrnx rrne rrng rrnab rrng rrnab rrndex rrnd leu trna recaampc aroh rpoabiob struvrbp rpljthr spot subb-espc galp tufbglns trpp lexa rnapori-l tnaa deop fol pori-r hisjalas lacp maltbioa uvrb arac uvrbp deop malefg malkrpob trpr laci 
support vector kernel machines nello cristianini biowulf technologies nello support-vector net http support-vector net tutorial html icml support-vector net history svms introduced coltby boser guyon vapnik greatly developed initially popularized nips community important active field machine learning research special issues machine learning journal journal machine learning research kernel machines large class learning algorithms svms instance support-vector net history annual workshop nips centralized website kernel-machines textbook support-vector net large diverse community machine learning optimization statistics neural networks functional analysis successful applications fields bioinformatics text handwriting recognition fast expanding field support-vector net preliminaries task class algorithms detect exploit complex patterns data clustering classifying ranking cleaning data typical problems represent complex patterns exclude spurious unstable patterns overfitting computational problem statistical problem support-vector net informal reasoning class kernel methods implicitly defines class patterns introducing end thenif condition infer remember elseconclusion ifconclusion infer remember ifconclusion elseconclusion condition notion similarity data similarity documents length topic language choice similarity choice relevant features support-vector net formal reasoning kernel methods exploit information products data items standard algorithms rewritten require products data inputs kernel functions products feature space potentially complex kernel features data support-vector net case product vectors hyperplane xzi support-vector net overview tutorial introduce basic concepts extended kernel perceptron derive support vector machines kernel based algorithms properties limitations kernels kernel alignment optimizing kernel alignment support-vector net parts overview linear learning machines llm kernel induced feature spaces generalization theory optimization theory support vector machines svm support-vector net modularity kernel-based learning algorithm composed modules general purpose learning machine problem specific kernel function k-b algorithm fitted kernel kernels constructed modular great software engineering analysis important concept support-vector net -linear learning machines simplest case classification decision function hyperplane input space perceptron algorithm rosenblatt analyze perceptron algorithm svms kernel methods general support-vector net basic notation input space output space hypothesis real-valued training set test error dot product support-vector net perceptron linear separation input space sign support-vector net perceptron algorithm update rule ignoring threshold support-vector net observations solution linear combination training points informative points mistake driven coefficient point combination reflects difficulty support-vector net observations mistake bound coefficients non-negative rewrite algorithm alternative representation support-vector net dual representation decision function re-written important concept support-vector net dual representation update rule rewritten note dual representation data appears inside dot products support-vector net duality property svms duality feature support vector machines svms linear learning machines represented dual fashion data dot products decision function training algorithm support-vector net limitations llms linear classifiers deal non-linearly separable data noisy data formulation deals vectorial data support-vector net non-linear classifiers solution creating net simple linear classifiers neurons neural network problems local minima parameters heuristics needed train solution map data richer feature space including non-linear features linear classifier support-vector net learning feature space map data feature space linearly separable support-vector net problems feature space working high dimensional feature spaces solves problem expressing complex functions computational problem working large vectors generalization theory problem curse dimensionality support-vector net implicit mapping feature space introduce kernels solve computational problem working dimensions make infinite dimensions efficiently time space advantages practical conceptual support-vector net kernel-induced feature spaces dual representation data points inside dot products dimensionality space necessarily important map support-vector net kernels function returns dot product images arguments function verify kernel important concept support-vector net kernels llms feature space simply rewriting dual representation replacing dot products kernels support-vector net kernel matrix aka gram matrix important concept support-vector net kernel matrix central structure kernel machines information bottleneck information learning algorithm fuses information data kernel interesting properties support-vector net mercer theorem kernel matrix symmetric positive definite symmetric positive definite matrix regarded kernel matrix product matrix space support-vector net formally mercer theorem semi positive definite symmetric function kernel exists mapping write pos support-vector net mercer theorem eigenvalues expansion mercer kernels eigenfunctions act features support-vector net examples kernels simple examples kernels support-vector net polynomial kernels support-vector net polynomial kernels support-vector net spirals separated hyperplane feature space gaussian kernels support-vector net making kernels set kernels closed operations kernels kernel kernel kernel make complex kernels simple modularity important concept support-vector net property svms svms linear learning machines dual representation operate kernel induced feature space linear function feature space implicitely defined support-vector net kernels general structures haussler watkins kernels sets sequences trees applied text categorization bioinformatics support-vector net bad kernel kernel kernel matrix diagonal points orthogonal clusters structure support-vector net free kernel mapping space irrelevant features kernel matrix diagonal prior knowledge target choose good kernel important concept support-vector net kernel-based algorithms note algorithms kernels llms clustering pca dual representation optimization problems representer theorem support-vector net support-vector net generalization problem curse dimensionality easy overfit high dimensional spaces regularities found training set accidental found test set svm problem ill posed finding hyperplane separates data hyperplanes exist principled choose hyperplane topic support-vector net generalization problem methods exist choose good hyperplane inductive principles bayes statistical learning theory pac mdl focus simple case motivated statistical learning theory give basic svm support-vector net statistical computational learning theory generalization bounds risk overfitting setting assumption data standard bounds theory give upper lower bound proportional dimension dimension llms proportional dimension space huge support-vector net assumptions definitions distribution input space train test points drawn randomly training error fraction points misclassifed test error probability misclassify point dimension size largest subset shattered dichotomy implemented support-vector net bounds number dimensions typically hyperplane choose support-vector net margin based bounds xfy min note compression bounds exist online bounds support-vector net margin based bounds worst case bound holds lucky margin large bound applied generalization achieved hyperplane maximal margin margin large kernel chosen important concept support-vector net maximal margin classifier minimize risk overfitting choosing maximal margin hyperplane feature space feature svms maximize margin svms control capacity increasing margin reducing number degrees freedom dimension free capacity control support-vector net kinds margin functional geometric margin funct geom min min support-vector net kinds margin support-vector net max margin minimal norm fix functional margin geometric margin equal maximize margin minimizing norm support-vector net max margin minimal norm distance convex hulls support-vector net primal problem minimize subject important step support-vector net optimization theory problem finding maximal margin hyperplane constrained optimization quadratic programming lagrange theory kuhn-tucker theory lagrangian support-vector net primal dual differentiate substitute 
support-vector net dual problem maximize subject duality kernels important step support-vector net convexity quadratic optimization problem convex local minima effect mercer conditions solvable polynomial time convexity fundamental property svms important concept support-vector net kuhn-tucker theorem properties solution duality kernels kkt conditions sparseness points nearest hyperplane margin positive weight called support vectors support-vector net kkt conditions imply sparseness sparseness fundamental property svms support-vector net properties svms summary duality kernels margin convexity sparseness support-vector net dealing noise case non-separable data feature space margin distribution optimized support-vector net soft-margin classifier minimize subject support-vector net slack variables support-vector net soft margin-dual repeat repeataction untilcondition thenaction end whencondition thenaction repeataction state state repeataction lagrangian box constraints diagonal support-vector net regression case regression properties retained introducing epsilon-insensitive loss support-vector net regression -tube support-vector net implementation techniques maximizing quadratic function subject linear equality constraint inequalities support-vector net simple approximation initially complex pachages stochastic gradient ascent sequentially update weight time excellent approximation cases support-vector net full solution smo update weights simultaneously realizes gradient descent leaving linear constraint platt online versions exist li-long gentile support-vector net kernelized algorithms adatron nearest neighbour fisher discriminant bayes classifier ridge regression work past years designing kernel based algorithms work designing good kernels algorithm support-vector net combining kernels advantageous combine kernels features leads overfitting kernel methods kernel combination based principles alignment support-vector net kernel alignment notion similarity kernels alignment similarity gram matrices important concept support-vector net interpretations measure clustering data correlation coefficient oracles basic idea ultimate kernel labels vector target relevant feature support-vector net ideal kernel support-vector net combining kernels alignment increased combining kernels aligned target aligned support-vector net spectral machines approximately maximize alignment set labels kernel solving problem approximated principal eigenvector thresholded courant-hilbert theorem yky arg max support-vector net courant-hilbert theorem symmetric positive definite principal eigenvalue eigenvector characterized max vav support-vector net optimizing kernel alignment adapt kernel labels vice versa case model selection method case clustering transduction method support-vector net applications svms bioinformatics machine vision text categorization handwritten character recognition time series analysis support-vector net text kernels joachims bag words latent semantic kernels icml string matching kernels kermit project support-vector net bioinformatics gene expression protein sequences phylogenetic information promoters support-vector net conclusions replacement neural networks general rich class pattern recognition methods zzz vxssruw yhfwru qhw kernel machines website kernel-machines neurocolt 
thenaction end whilecondition whileaction whilecondition state state whileaction thenaction whileaction tree closerthantree disttotree disttotree bias tall match trees leafy match trees leafy match tall match tall match trees leafy match tall match trees leafy match trees tall leafy tall match trees leafy match trees leafy match tall match tall match trees leafy match tall match trees leafy match trees tall leafy infer notlarge small medium end small medium notlarge small medium end remember notlarge small medium notlarge addition addition action action condition term memory condition term memory intermediate term intermediate term condition atgoal moveforward end atgoal moveforward goalisnorthandeast movenorth moveeast end goalisnorthandeast movenorth moveeast applybrake atgoal end atgoal applybrake end multiaction action action action action action action outstate instate outstate instate action multicondition action surrounded moveeastpusheast oktopusheast enemy inputs outs pusheast moveeast american association artificial intelligence electronic document retrieved american association artificial intelligence burgess drive menlo park california info aaai http aaai membership information consult web page material copyrighted material reproduced form electronic mechanical means including photocopying recording information storage retrieval permission writing aaai machine-learning research making great progress directions article summarizes directions discusses current open problems directions improvement classification accuracy learning ensembles classifiers methods scaling supervised learning algorithms reinforcement learning learning complex stochastic models years explosion machine-learning research explosion separate research communities symbolic machine learning computational learning theory neural networks statistics pattern recognition discovered begun work machine-learning techniques applied kinds problem including knowledge discovery databases language processing robot control combinatorial optimization traditional problems speech recognition face recognition handwriting recognition medical data analysis game playing article selected topics machine learning lot recent activity purpose article describe results areas broader audience sketch open research problems topic areas ensembles classifiers methods scaling supervised learning algorithms reinforcement learning learning complex stochastic models reader cautioned article comprehensive review topics goal provide representative sample research areas areas papers describe relevant work apologize authors work unable include article ensembles classifiers topic concerns methods improving accuracy supervised learning begin introducing notation supervised learning learning program training examples form unknown function values typically vectors form components discrete real valued height weight color age called features notation refer jth feature situations drop subscript implied context values typically drawn discrete set classes case classification real line case regression article focus primarily classification training examples corrupted random noise set training examples learning algorithm outputs classifier classifier hypothesis true function values predicts values denote classifiers ensemble classifiers set classifiers individual decisions combined typically weighted unweighted voting classify examples active areas research supervised learning study methods constructing good ensembles classifiers main discovery ensembles accurate individual classifiers make ensemble accurate component classifiers individual classifiers disagree hansen salamon imagine ensemble classifiers case clasarticles winter machine-learning research current directions thomas dietterich copyright american association artificial intelligence rights reserved ular algorithms begin reviewing general techniques subsampling training examples method manipulates training examples generate multiple hypotheses learning algorithm run times time subset training examples technique works unstable learning algorithms algorithms output classifier undergoes major response small training data decision tree neural network rule-learning algorithms unstable linear-regression nearest-neighbor linear-threshold algorithms generally stable straightforward manipulating training set called bagging run bagging presents learning algorithm training set consists sample training examples drawn randomly replacement original training set items training set called bootstrap replicate original training set technique called bootstrap aggregation breiman bootstrap replicate average percent original training set training examples appearing multiple times training-set sampling method construct training sets leaving disjoint subsets training data training set divided randomly disjoint subsets overlapping training sets constructed dropping subsets procedure construct training sets tenfold cross-validation ensembles constructed called crossvalidated committees parmanto munro doyle method manipulating training set illustrated adaboost algorithm developed freund schapire shown figure bagging adaboost manipulates training examples generate multiple hypotheses adaboost maintains probability distribution training examples iteration draws training set size sampling replacement probability distribution learning algorithm applied produce classifier error rate classifier training examples weighted computed adjust probability distribution training examples figure note probability distribution obtained normalizing set weights training examples sifiers identical wrong wrong errors made classifiers uncorrelated wrong correct majority vote correctly classifies precisely error rates hypotheses equal errors independent probability majority vote wrong area binomial distribution hypotheses wrong figure shows area simulated ensemble hypotheses error rate area curve hypotheses simultaneously wrong error rate individual hypotheses individual hypotheses make uncorrelated errors rates exceeding error rate voted ensemble increases result voting key successful ensemble methods construct individual classifiers error rates errors uncorrelated methods constructing ensembles methods constructing ensembles developed methods general applied learning algorithm methods specific particfigure probability hypotheses make error assuming hypothesis error rate makes errors independently hypotheses articles magazine probability number classifiers error effect change weights place weight training examples misclassified weight examples correctly classified subsequent iterations adaboost constructs progressively difficult learning problems final classifier constructed weighted vote individual classifiers classifier weighted accuracy distribution trained line adaboost algorithm figure base learning algorithm learn called probability distribution learning algorithm learn probability distribution directly procedure generally results quinlan developed version decision tree learning program works weighted training sample experiments showed worked extremely imagine versions back propagation scaled computed output error training weight errors important training examples larger gradient-descent steps errors unimportant low-weight examples algorithm probability distribution directly training sample constructed drawing random sample replacement proportion probabilities procedure makes adaboost stochastic experiments shown effective figure compares performance adaboost random sampling point plotted test domains irvine repository machine-learning databases merz murphy points lie line error rate adaboost error rate figure compares performance bagging bagging produces sizable reductions error rate problems finally figure compares bagging boosting underlying algorithm results show techniques comparable boosting appears advantage bagging manipulating input features general technique generating multiple classifiers manipulate set input features learning algorithm project identify volcanoes venus cherkauer trained ensemble neural networks networks based subsets input features network sizes input-feature subsets selected hand group features based image-processing operations principal component analysis fast fourier transform resulting ensemble classifier match performance human experts identifying volcanoes articles winter input set ofm labeled examples labels learn learning algorithm constant initialize initialize weights compute normalized weights learn call learn normalized weights calculate error goto compute weights end output argmax vextenddouble log vextenddouble figure adaboost algorithm formula true error rate error rate adaboost figure comparison adaboost applied point represents test domains points lying diagonal line exhibit lower error adaboost based data freund schapire hypotheses constructed input features highly redundant manipulating output targets general technique constructing good ensemble classifiers manipulate values learning algorithm dietterich bakiri describe technique called error-correcting output coding ecoc suppose number classes large learning problems constructed randomly partitioning classes subsets input data relabeled original classes set derived label original classes set derived label relabeled data learning algorithm constructs classifier repeating process times generating subsets obtain ensemble classifiers data point classify answer classify class receives vote class receives vote classifiers voted class highest number votes selected prediction ensemble equivalent thinking method class encoded bit code word bit l-th learned classifier attempts predict 
bit code words classifiers applied classify point predictions combined l-bit string choose class code word closest hamming distance l-bit output string methods designing good error-correcting codes applied choose code words equivalently subsets dietterich bakiri report technique improves performance back-propagation algorithms variety difficult classification problems recently schapire showed adaboost combined error-correcting output coding yield excellent ensemble-classification method calls adaboost performance method superior ecoc method bagging essentially complex algorithm called adaboost main advantage adaboost implementation simplicity work learning algorithm solving two-class problems ricci aha applied method combines error-correcting output coding feature selection learning classifitumer ghosh applied similar technique sonar data set input features found deleting input features hurt performance individual classifiers voted ensemble perform technique works articles magazine error rate error rate bagged figure comparison bagging applied point represents test domains points lying diagonal line exhibit lower error bagging based data freund schapire bagging voted classifiers error rate bagging error rate adaboost figure comparison bagging applied adaboost applied point represents test domains points lying diagonal line exhibit lower error boosting bagging based data freund schapire apply feature-selection techniques choose features learning classifier obtained improvements tasks approach injecting randomness generalpurpose method generating ensembles classifiers inject randomness learning algorithm back-propagation algorithm training neural networks initial weights network set randomly algorithm applied training examples initial weights resulting classifier kolen pollack method common generating ensembles neural networks manipulating training set effective study parmanto munro doyle compared technique bagging tenfold cross-validated committees found cross-validated committees worked bagging multiple random initial weights synthetic data set medical diagnosis data sets decision tree algorithm easy inject randomness kwok carter key decision choose feature test internal node decision tree internal node applies criterion information-gain ratio rank feature tests chooses top-ranked feature-value test discrete-valued features values decision tree splits data subsets depending chosen feature real-valued features decision tree splits data subsets depending chosen feature chosen threshold dietterich kong implemented variant chooses randomly equal probability top tests table compares single run ensembles classifiers constructed bagging injecting randomness results show injecting randomness obtains performance domains notice injected randomness obtains perfect test-set performance letterrecognition task ali pazzani injected randomness foil algorithm learning prologstyle rules foil works ranks conditions add rule information-gain criterion ali pazzani computed candidate conditions scored percent topranked candidate applied weighted random-choice algorithm choose compared ensembles classifiers single run foil found statistically significant improvements tasks statistically significant loss performance task obtained similar results -fold cross-validation construct training sets raviv intrator combine bootstrap sampling training data injecting noise input features learning algorithm train member ensemble neural networks draw training examples replacement original training data values training perturbed adding gaussian noise input features report large improvements synthetic benchmark task medical diagnosis task method closely related techniques injecting randomness markov chain monte carlo mcmc method applied neural networks mackay neal decision trees chipman george mcculloch basic idea mcmc method related methods construct markov process generates infinite sequence hypotheses bayesian setting goal generate task test-set size -fold bootstrap -fold random vowel soybean part speech nettalk letter recognition difference significant -fold random difference significant articles winter table results domains error rate boldface parison bagging found method gave excellent results real-world domains abu-mostafa caruana describe technique training neural network auxiliary tasks main task key idea add output units network role predict values auxiliary tasks include prediction error auxiliary tasks error criterion back propagation seeks minimize auxiliary output units connected hidden units primary task output auxiliary outputs influence behavior network primary task parmanto show diverse classifiers learned training primary task auxiliary tasks good source auxiliary tasks network attempt predict input features addition primary output task apply method medical diagnosis problems recently munro parmanto developed approach auxiliary output determined dynamically competition set networks network primary output secondary auxiliary output training network training computes primary secondary output predictions network secondary output prediction highest winner target remaining networks target secondary output networks target primary output effect encourage networks experts predicting secondary output regions input space primary secondary output share hidden layer errors primary output decorrelated show method substantially outperforms ensemble ordinary networks trained initial random weights trained synthetic classification task addition methods training ensembles neural networks methods specific decision trees buntine developed algorithm learning option trees decision trees internal node alternative splits producing decision tree classify decision trees evaluated resulting classifications voted kohavi kunz describe option tree algorithm compare performance bagged trees hypothesis probability training sample computed usual normalized product likelihood prior probability apply mcmc define set operators convert neural network operator adjust weights network decision tree operator interchange parent child node tree replace node mcmc process works maintaining current hypothesis step selects operator applies obtain computes likelihood resulting classifier training data decides discard back technical conditions prove process kind eventually converges stationary probability distribution sampled proportion posterior probabilities practice difficult stationary distribution reached standard approach run markov process long period discarding generated classifiers collect set classifiers markov process classifiers combined weighted vote posterior probabilities algorithm-specific methods generating ensembles addition generalpurpose methods generating diverse ensembles classifiers techniques applied back-propagation algorithm training neural networks rosen trains neural networks simultaneously forces networks diverse adding correlation penalty error function back propagation minimizes specifically training rosen track correlations predictions network applies back propagation minimize error function sum usual squared output prediction error term measures correlations networks reports substantial improvements simple synthetic tasks opitz shavlik similar approach kind genetic algorithm search good population neural network classifiers iteration apply genetic operators current ensemble generate network topologies networks trained error function combines usual squared output prediction error multiplicative term incorporates diversity classifiers training networks prune population retain networks criterion considers accuracy diversity comarticles magazine show option trees generally match performance bagging produce understandable result completes review methods generating ensembles single learning algorithm generate ensemble combining classifiers constructed learning algorithms learning algorithms based principles produce diverse classifiers classifiers perform worse guarantee diversity classifiers learning algorithms combined checked cross-validation accuracy diversity form weighted combination approach shown effective applications zhang mesirov waltz methods combining classifiers trained ensemble classifiers combine individual classification decisions methods explored subdivided unweighted vote weighted vote gating networks simplest approach unweighted vote bagging ecoc methods intelligent voting schemes experience forecasting 
literature simple unweighted voting robust clemen refinement simple majority vote classifier produce class-probability estimates simple classification decision class-probability estimate data point probability true class combine class probabilities hypotheses class probability ensemble predicted class class highest class probability weighted voting methods developed ensembles regression problems perrone cooper hashem apply squares regression find weights maximize accuracy ensemble training data show weight applied inversely proportional variance estimates difficulty applying linear squares hypotheses correlated highly describe methods choosing correlated subsets ensemble combining linear squares classification problems weights obtained measuring accuracy individual classifier training data holdout data set constructing weights proportional accuracies ali pazzani describe method call likelihood combination apply naive bayes algorithm naive bayes classifier learn weights classifiers adaboost weight classifier computed accuracy measured weighted training distribution learn bayesian approach weighted vote compute posterior probability method requires definition prior distribution multiplied likelihood estimate posterior probability ali pazzani experiment method earlier work bayesian voting decision trees performed buntine approach combining classifiers learn gating network gating function takes input produces output weights applied compute weighted vote classifiers jordan jacobs learn gating networks form words dot product parameter vector input feature vector output weight so-called softmax individual learning algorithm risk overfitting training data learning gating function addition learning individual classifiers discuss hierarchical mixture-of-experts method developed jordan jacobs learns gating network simultaneously fourth approach combining classifiers called stacking works suppose learning algorithms set training examples usual apply algorithms training data produce hypotheses goal stacking learn good combining classifier final classification computed wolpert proposed scheme learning form leave-one-out cross-validation articles winter classifier learning algorithms large hypothesis spaces eliminating hypotheses misclassify training examples hypotheses remaining hypotheses equally accurate respect training data reasons preferring hypotheses preferring simpler hypotheses hypotheses higher prior probability nonetheless typically plausible hypotheses collection surviving hypotheses easily construct ensemble classifiers combine methods earlier ensembles learning algorithms solve difficult search problems pose problem finding smallest decision tree consistent set training examples np-hard hyafil rivest practical decision tree algorithms search heuristics guide greedy search small decision trees similarly finding weights smallest neural network consistent training examples np-hard blum rivest neural network algorithms local search methods gradient descent find locally optimal weights network consequence imperfect search algorithms combination training examples prior knowledge preferences simple hypotheses bayesian priors determines unique hypothesis find typically find hypothesis complex lower posterior probability run search algorithms slightly training sample injected noise techniques earlier find suboptimal hypothesis ensembles compensating imperfect search algorithms ensembles hypothesis space true function include equally good approximations taking weighted combinations approximations represent classifiers lie understand point visualize decision boundaries constructed learning algorithms decision boundary surface examples lie side surface assigned class examples lie side surface decision boundaries constructed decision tree learnbe classifier constructed algorithm applied training examples words algorithm applied training data times leaving training time apply classifier obtain predicted class procedure data set level-two examples features classes predicted classifiers form apply learning algorithm level-two data learn breiman applied approach combining forms linear regression good results ensembles work gave basic intuition ensembles improve performance uncorrelated errors made individual classifiers removed voting deeper question lurking find ensembles classifiers make uncorrelated errors question shouldn find single classifier performs ensemble reasons good ensembles constructed difficult impossible find single classifier performs ensemble understand reasons nature machine-learning algorithms machine-learning algorithms work searching space hypotheses accurate hypothesis hypothesis approximates unknown function important aspects hypothesis space size possibility good approximations hypothesis space large large amount training data constrain search good approximations training rules makes plausible hypotheses misclassify two-class problem ideally training eliminate half hypotheses require log examples select unique classifier ensembles training data provide sufficient information choosing single gave basic intuition ensembles improve performance uncorrelated errors made individual classifiers removed voting deeper question lurking find ensembles classifiers make uncorrelated errors articles magazine ing algorithms line segments generally hyperplane segments parallel coordinate axes true boundary classes diagonal line decision tree algorithms approximate diagonal staircase axis-parallel segments figure bootstrap training samples weighted samples created adaboost shift locations staircase approximation voting approximations construct approximations diagonal decision boundary interestingly improved staircase approximations equivalent complex decision trees trees large include hypothesis space space large training data ensembles provide overcoming representational inadequacies hypothesis space open problems ensembles ensembles established method obtaining highly accurate classifiers combining accurate questions construct ensembles issues understand decisions made ensembles faced learning problem approach constructing applying ensemble classifiers principle single ensemble method single learning algorithm methods uniformly methods situations experimental studies shown adaboost methods constructing ensembles decision trees schapire compares adaboost adaboost bagging error-correcting output coding shows adaboost methods generally superior quinlan shown domains noisy training data adaboost perform badly places high weight incorrectly labeled training examples constructs bad classifiers kong dietterich showed combining bagging error-correcting output coding improved performance methods suggests combinations ensemble methods explored dietterich kong showed error-correcting output coding work highly local algorithms nearest-neighbor methods systematic studies methods constructing ensembles neural networks rule-learning systems types classifier work remains area ensembles provide accurate classifiers problems limit practical application problem ensembles require large amounts memory store large amounts computation apply earlier mentioned ensemble decision trees attains perfect performance letter-recognition benchmark task decision trees require megabytes storage makes impractical present-day computers important line research find ways converting ensembles redundant representations deletion highly correlated members ensemble representational transformations difficulty ensemble classifiers ensemble insight makes decisions single decision tree interpreted human users ensemble voted decision trees difficult understand methods found obtaining explanations locally ensembles work question craven trepan algorithm craven shavlik scaling machinelearning algorithms major research area explored techniques scaling learning algorithms apply problems millions training examples thousands features hundreds classes large machinefigure decision boundaries figure shows true diagonal decision boundary staircase approximations kind created decision tree algorithms figure shows voted decision boundary approximation diagonal boundary articles winter class class class class threshold standard approach sort training examples current node values feature make sequential pass sorted examples choose threshold standard depth-first algorithms data sorted candidate feature node tree expensive shafer agrawal mehta describe sprint method training data broken separate disk file attribute sorted attribute feature disk file records form index training feature training class choose splitting threshold feature simple matter make serial scan disk 
file construct class histograms potential splitting threshold chosen disk file partitioned logically files examples values equal examples values greater files written disk hash table built main memory index training left child nodes newly created split disk files attributes read split file left child file child index looked hash table determine child belongs sprint parallelized easily applied data sets million examples mehta agrawal rissanen developed closely related algorithm sliq makes main memory scales millions training examples slightly faster sprint sprint sliq scale approximately linearly number training examples number features cost performing initial sorting data approach large data sets advantage ensembles decision trees chan stolfo training data randomly partitioned disjoint subsets separate decision tree grown subset parallel trees vote make classification decisions accuracy individual decision trees accuracy single tree grown data accuracy ensemble accuracy single tree parallel processors achieve speedup time required construct decision trees learning problems beginning arise database-mining applications millions transactions day desirable machine-learning algorithms analyze large data sets hours computer time area large learning problems arise information retrieval full-text databases world wide web information retrieval word document treated input feature training thousands features finally applications speech recognition object recognition character recognition chinese japanese present situations hundreds thousands classes discriminated learning large training sets decision tree algorithms extended handle large data sets ways approach based intelligently sampling subsets training data tree grown describe process works review decision tree algorithms operate decision trees constructed starting entire training set empty tree test chosen root tree training data partitioned disjoint subsets depending outcome test algorithm applied recursively disjoint subsets algorithm terminates training examples subset data belong class point leaf node created labeled class process choosing test root tree root subtree involves analyzing training data choosing feature predictor output class large redundant data set make choice based sample data musick catlett russell presented algorithm dynamically chooses sample based difficult decision node algorithm typically behaves small sample root tree progressively enlarging sample tree grows technique reduce time required grow tree reducing accuracy tree approach based developing clever data structures avoid store training data random-access memory hardest step decision tree algorithms find tests real-valued features tests form large machinelearning problems beginning arise databasemining applications millions transactions day desirable machinelearning algorithms analyze large data sets hours computer time articles magazine fourth approach problem choosing splits real-valued input features discretize values features feature training employee income measured dollars tens thousands distinct values running time decision-tree learning algorithms linear number distinct values feature problem solved grouping income small number ranges greater ranges chosen resulting decision trees accurate simple fast algorithms developed choosing good discretization points kohavi sahami fayyad irani catlett effective rule-learning algorithm called ripper developed william cohen based earlier algorithm irep developed furnkranz widmer ripper constructs rules form test test test test form discrete features real-valued features rule cover training satisfies tests left-hand side rule training set size run time ripper scales log major improvement rule-learning program rules quinlan scales figure shows pseudocode ripper ripper works building initial set rules optimizing set rules times parameter typically set describe ripper case classes examples class referred positive examples examples class referred negative examples ripper easily extended handle larger numbers classes build set rules ripper constructs rule time learning rule divides training data growing set two-thirds data pruning set remaining one-third iteratively adds tests rule rule covers negative examples tests selected information-gain heuristic developed quinlan foil system rule grown immediately pruned deleting tests reverse order test test test find pruned rule maximizes quantity number positive pruning examples covered pruned rule number negative pruning examples covered pruned rule rule grown pruned ripper adds rule set discards training examples covered rule description-length criterion decide stop adding rules description length set rules number bits needed represent rules number bits needed identify training examples exceptions rules minimum description-length criteria kind applied successfully ruleand tree-learning algorithms quinlan rivest ripper stops adding rules description length rule set bits larger description length observed point considers rules reverse order deletes rule reduce total description length rule set optimize set rules ripper considers deleting rule turn regrowing articles winter figure ripper algorithm cohen procedure buildruleset positive examples negative examples ruleset descriptionlength ruleset grow prune rule split growpos growneg prunepos pruneneg rule growrule growpos growneg rule prunerule rule prunepos pruneneg add rule ruleset descriptionlength ruleset prune rule set exit rule ruleset considered reverse order descriptionlength ruleset frg dlthen delete ruleset descriptionlength ruleset end end return ruleset end descriptionlength ruleset delete examples covered rule end end buildruleset procedure optimizeruleset ruleset rule ruleset delete ruleset upos examples covered ruleset uneg examples covered ruleset split upos uneg growpos growneg prunepos pruneneg reprule growrule growpos growneg reprule prunerule reprule prunepos pruneneg revrule growrule growpos growneg revrule prunerule revrule prunepos pruneneg choose reprule revrule add ruleset end end optimizeruleset procedure ripper ruleset buildruleset repeat times ruleset optimizeruleset ruleset return ruleset end ripper discrete feature mutual information weight computed proportion training examples class probability feature takes real-valued features sums integrals approximated good approximation apply discretization algorithm advocated fayyad irani convert real-valued feature discretevalued feature apply previous formula wettschereck dietterich obtained good results nearest-neighbor algorithms mutual information weighting problem mutual information weighting treats feature independently features predictive power apparent combination features mutual information assigns weight difficult class learning problems involves learning parity functions random irrelevant features parity function binary features equal odd number features equal suppose define learning problem relevant features irrelevant random binary features class parity relevant features mutual information weights features approximately previous formula algorithm overcomes problem successful preprocessing algorithms date relief-f algorithm kononenko extension earlier algorithm called relief kira rendell basic idea algorithms draw examples random compute nearest neighbors adjust set feature weights give weight features discriminate neighbors classes specifically randomly chosen training training examples nearest euclidean distance class class goal relief set weight input feature words weight maximized high probability taking feature low probability taking feature relief-f computes reliable estimate probability difference computing nearest neighbors class wpxx pxx jjj wpycxv cpx log repruning candidate replacement rules grown pruned candidate grown starting empty rule candidate grown starting current rule candidates selected description-length heuristic added rule 
set cohen compared ripper rules data sets found ripper matched beat rules problems rule sets finds smaller constructed rules implementation ripper research cohen research att wcohen ripperd html learning features learning problems hundreds thousands potential features describing input object popular learning algorithms back propagation scale features statistical point view examples irrelevant noisy input features provide information easy learning algorithms confused noisy features construct poor classifiers practical applications wise carefully choose features provide learning algorithm research machine learning sought automate selection weighting features algorithms developed purpose excellent review written wettschereck aha mohri comprehensive review statistical literature feature selection found miller discuss significant methods main approaches pursued approach perform initial analysis training data select subset features feed learning algorithm approach subsets features learning algorithm estimate performance algorithm features subsets perform approach integrate selection weighting features directly learning algorithm discuss examples approach selecting weighting features preprocessing simple preprocessing technique compute mutual information called information gain input feature class mutual information random variables average reduction uncertainty variable articles magazine figure describes relief-f algorithm algorithm feature values defined kononenko simec robnik-sikonja shown relief-f effective detecting relevant features features highly dependent features -parity problem mentioned previously irrelevant random features relief-f correctly separate relevant features irrelevant training examples kononenko computes nearest neighbors class experiments sets number sample points equal number training examples large data sets good results obtained smaller samples kononenko experimented integrating relief-f decision tree learning algorithm called assistant-r show assistant-r perform original assistant program mutual information choose features domains highly dependent features essentially performance domains independent features selecting weighting features testing learning algorithm john kohavi pfleger describe computationally expensive method call wrapper method selecting input features idea generate sets features run learning algorithm features evaluate resulting classifiers -fold cross-validation single holdout set -fold cross-validation training data subdivided randomly disjoint equal-sized sets learning algorithm applied times time training set subsets resulting classifier tested one-tenth data held performance classifiers respective holdout sets averaged provide estimate performance learning algorithm trained features kohavi john explored step-wise selection algorithms start set features empty set considered adding deleting single feature feature set evaluated -fold cross-validation change made set real valued features discrete features considered method practical data sets small numbers features fast learning algorithms gave excellent results california irvine benchmarks moore lee describe efficient approach feature selection combines leave-one-out cross-validation loocv nearest-neighbor algorithm leave-one-out cross-validation training temporarily deleted training data nearest-neighbor learning algorithm applied predict class total number classification errors leave-one-out cross-validated estimate error rate learning algorithm moore lee loocv error compare sets features goal finding set relevant features minimizes loocv error combine clever ideas achieve idea called racing suppose sets relevant features repeatedly choose training random temporarily delete training set apply nearest-neighbor rule classify features set features set count number classification errors set process training examples leave-one-out fashion error rate feature set larger error rate conclude high confidence feature set terminate race paper moore lee apply bayesian statistics make termination decision idea based schemas represent set relevant features figure relief-f algorithm articles winter procedure relief-f number random examples draw number neighbors compute features fraction training examples belonging class randomly select instance hit set examples nearest class set examples nearest end feature hit xtj xij xtj xij end end return end relief-f works learning algorithms integrating feature weighting learning algorithm discuss methods integrate feature selection directly learning algorithm shown work experimentally method called winnow works extremely problems thousands potentially relevant input features algorithm called variable-kernel similarity metric vsm method lowe vsm form gaussian radial basis function method classify data point defines multivariate gaussian probability distribution centered standard deviation training data set votes class amount class highest vote assigned class data point key effectiveness vsm learns weighted distance metric measuring distance data point training point vsm adjusts size gaussian distribution depending local density training examples neighborhood detail vsm controlled set learned feature weights kernel radius parameter number neighbors classify data point vsm computes weighted distances nearest neighbors computes kernel width average distance nearest neighbors finally computes probability point belongs class quantity vote training vsm guesses class highest probability vsm learn values feature weights kernel radius performing gradient-descent search minimize loocv accuracy vsm classifier lowe shows compute gradient loocv error respect weights parameter starting exp dwxx jtj bit vector position means feature relevant means irrelevant schema vector position means feature selected randomly relevant percent time moore lee race pairs schemas training randomly selected temporarily deleted training set nearest-neighbor algorithm applied classify schemas raced classify schema features features selected features selected probability suppose illustration features moore lee begin conducting simultaneous pairwise races races races races races races races terminated schemas found opponent iteration single-bit refinements winning schema raced suppose schema winner race iteration involves pairwise races races races races races process continues removed winning schema moore hidden lee units found action method moveeast pusheast misses movenorth previous important action relevant moveeastnoaction features pusheast features agenta highly enemya dependent obstaclea emptya rare keya cases fooda races sensor inputsa long time actionsa conclude sector problem wall appears emptyfood enemy obstacle occluded algorithm sector slow nea wall replace emptyfood enemy obstacle contrast occluded algorithm quick replace moore lee investigated algorithm called schemata terminates race evaluations favor race statistics choose feature relevant median number training examples evaluated schemata percent number evaluations required greedy forward selection percent number evaluations required greedy backward elimination achieves levels accuracy important direction future research compare accuracy speed schemata relief-f articles magazine initial values parameters vsm computes nearest neighbors training computes gradient performs search direction gradient minimize loocv error set nearest neighbors fixed search direction gradient called line search efficient algorithms press weights changing line search set nearest neighbors gradient recomputed error minimized direction gradient nearest neighbors recomputed gradient computed line search performed lowe applied conjugate gradient algorithm select search direction reports generally line searches required minimize loocv error resulting classifiers gave excellent results challenging benchmark tasks feature weighting algorithm discuss winnow algorithm developed littlestone winnow linear threshold algorithm two-class problems binary valued input features classifies class class winnow online algorithm accepts examples time updates weights pseudocode algorithm shown figure winnow initializes weights accepts applies threshold rule compute predicted class predicted class correct winnow predicted class wrong winnow updates weights weights low feature number greater called promotion parameter weights high feature decreases weight setting number called 
demotion parameter fact winnow leaves weights unchanged predicted class correct puzzling people turns critical successful behavior theoretically experimentally explanation adaboost strategy focuses winnow attention mistakes explanation helps prevent overfitting littlestone theoretical analysis winnow introduced worst-case mistake bound method idea assume true function belongs set classifiers derive bound maximum number mistakes winnow makes adversary allowed choose order training examples presented winnow littlestone proves result theorem disjunction input features winnow learn make log mistakes theorem shows convergence time winnow linear number relevant features logarithmic total number features similar results hold values parameters permits winnow learn functions features interesting classes boolean functions winnow exponential update algorithm weights relevant features grow exponentially weights irrelevant features shrink exponentially general results computational learning theory developed similar exponential update algorithms applications common property algorithms excel number relevant features small compared total number features winnow applied experimental learning problems blum describes application calendar scheduling task task calendar system description proposed meeting including list invitees properties predict start time day week location duration meeting articles winter procedure winnow promotion parameter demotion parameter threshold initialize training vextenddouble ifz ifz end end end end winnow figure winnow algorithm ple winnow task decide occurrence words correct incorrect based context golding roth kinds boolean input feature kind context words context word feature true word cloudy appears words target word kind collocation features test string words part-of-speech tags immediately adjacent target word sequence target verb collocation feature checks target word immediately word word potentially verb dictionary lookup based one-million-word brown corpus kucera francis golding roth defined potentially relevant features golding roth applied winnow varying compared accuracy previous method modified naive bayesian algorithm figure shows results sets frequently confused words important advantage winnow addition speed ability operate online adapt rapidly target function advantage shown important calendar scheduling problem scheduling types meeting shift semesters change summer break blum experiments showed winnow respond quickly comparison enable decision tree algorithm respond decide training examples deleted task difficult kinds meeting change change semesters meetings stay decision recent training examples means examples rare meetings scheduling change lost hurts performance decision tree approach contrast winnow revises weights features features describing rare meetings retain weights weights features modified rapidly summary scaling learning algorithms concludes review methods scaling learning algorithms apply large problems techniques problems million training examples solved reasonable amounts input features values blum defined boolean features values pairs input features input features event-type position-of-attendees define separate boolean feature legal combination event-type meeting position-of-attendees grad-student total boolean input features applied winnow modified winnow prune set weights small winnow found boolean features prediction accuracy previous classifier task greedy forward-selection algorithm select relevant features decision tree learning algorithm golding roth describe application winnow context-sensitive spelling correction task identifying spelling errors legal word substituted late substituted random house dictionary flexner lists sets commonly confused words golding roth developed separate winnow classifier listed sets examarticles magazine figure comparison percentage correct classifications modified bayesian method winnow sets frequently confused words trained percent brown corpus tested remaining percent points lying line correspond cases winnow accurate winnow accuracy modified bayes accuracy computer time clear current stock ideas permit solution problems billions training examples important open problem gather practical experience large problems understand properties determine algorithms fail recurring theme subsamples training data make critical intermediate decisions choice relevant features theme development efficient online algorithms winnow anytime algorithms produce answer long permitted run longer run result produce important open topic problem handling thousands output classes section ensembles classifiers methods case error-correcting output coding adaboost methods scale number classes error-correcting output coding tested problems classes tests large problems thousands classes performed reinforcement learning previous sections discussed problems supervised learning examples section addresses problems sequential decision making control heading reinforcement learning work reinforcement learning dates back earliest days arthur samuel developed famous checkers program recently important advances practice theory reinforcement learning famous work gerry tesauro td-gammon program learned play backgammon computer program human players interesting applications work zhang dietterich job-shop scheduling crites barto real-time scheduling passenger elevators kaelbling littman moore published excellent survey reinforcement learning mahadevan kaelbling report recent national science foundation sponsored workshop subject books barto sutton bertsekas tsitskilis describe newly developed reinforcement learning algorithms theory summarize developments introduction dynamic programming important insight past years reinforcement learning analyzed form online approximate dynamic programming barto bradtke singh introduce insight notation robot interacting external environment time environment state robot set actions robot executes action environment move state convenient desired behavior robot define reward function specifies real-valued reward transition assign positive reward actions reach desired goal location assign negative reward undesirable actions colliding walls people robots reward states defined long-term goal robot defined function rewards receives commonly criterion cumulative discounted reward discount factor controls relative importance short-term long-term rewards procedure rule choosing action state called policy robot formalized function goal reinforcement learning algorithms compute optimal policy denoted maximizes cumulative discounted reward researchers dynamic programming bellman found convenient define real-valued function called function policy function expected cumulative discounted reward received starting state executing policy defined recursively formula probability state current state action policy reward function transition probability function compute function solving system linear equations equation form equation state system equations articles winter backups performed convenient order random requirement small sum expected discounted rewards converges simple backups performed converges state function compute policy guaranteed good policy computed performing one-step lookahead search choosing action backedup largest argmax words action compute expected backed-up define action maximum backed-up alternately computing policy updating equation converge optimal policy optimal function note plug equation policy unchanged optimal policy defines algorithm policy iteration shown figure easy show policy iteration converges fixed number iterations iteration expensive requires computing current policy alternative policy iteration work directly function bellman proved function optimal policy unique fixed point bellman equation max words perform one-step lookahead policy-improvement step policy iteration remembering action update estimate state called bellman backup expensive simple backup action resulting state performing bellman backups state converge optimal function called value-iteration algorithm summarized figure iteration difficult policy iteration backup expensive bellman backup simple backup function long time converge optimal policy converged long function converges hybrid algorithm combines aspects 
solved standard methods gaussian elimination gauss-seidel iteration solved iteratively converting equation assignment statement assignment statement called simple backup viewed taking current estimated backing compute revised estimate shown figure state perform action resulting states probabilities rewards reaching states estimated values states assuming estimated compute policy simple figure simple backup arc labeled probability making transition resulting states labeled reward figure policy-iteration algorithm articles magazine procedure policyiteration arbitrary initial policy repeat unchanged perform simple backups compute state equation update state equation end end policyiteration iteration policy iteration called modified policy iteration algorithm essentially policy iteration fixed number simple backups performed state iteration estimated function current policy completely converge correct function fairly mild conditions shown algorithm converge optimal policy algorithms policy iteration iteration modified policy iteration require performing backups state running time scales number states fact iteration run infinite amount time converging policy iteration requires time problems states compute policy iteration small problems difficulty problems interest state space states renders algorithms infeasible bellman termed curse dimensionality number states running time increases exponentially number dimensions state space drawback algorithms require complete model system transition probabilities reward function applications model unavailable robot interacting unknown environment model easily converted transition probability matrix elevator control problem studied crites barto software simulator current state proposed action generate state transition probability distribution distribution explicitly represented direct dynamic programming algorithm problem constructing explicit probability transition matrix reward function called curse modeling problems severe curse dimensionality reinforcement learning algorithms provide overcoming curses reinforcement learning algorithms introduced key innovations stochastic approximation backups valuefunction approximation model-free learning discuss innovations context algorithm developed sutton temporal difference learning begin describing simplified version sutton algorithm called method computing approximate simple backups online suppose state follow current policy taking action interacting real external environment simulator environment makes probabilistic transition state produces reward algorithm observes state reward updates function learning rate parameter typical values technically values shrink time converge basic idea visit times apply action times sampling time effect performed simple backup sampling probability distribution direct computational access strategy compute policy explicit model environment serves model sutton developed introduced idea storing separate state suppose represent function neural network differentiable function approximator form vector adjustable weights representation directly assign state adjust weights closer desired defining error function half squared difference current estimated state backed-up called temporal difference error goal rsas pipi articles winter procedure valueiteration arbitrary initial function repeat unchanged states state perform bellman backup equation end state compute optimal policy equation end valueiteration figure value-iteration algorithm implemented maintaining current gradient vector change full algorithm shown figure readers familiar momentum method stabilizing back propagation note eligibility trace mechanism similar momentum method previous weight remembered eligibility trace mechanism previous gradient vectors remembered future temporal differences determine step size previous gradients theoretical studies behavior general results obtained tsitsiklis van roy analyze case function approximator linear combination fixed arbitrary orthogonal basis functions analyze approximate notion distance functions needed tsitsiklis van roy define measure probability policy visits state measure approximation represented linear combination basis functions tsitsiklis van roy prove converge function quantity left-hand side error function learned true function policy numerator right-hand side inherent dpi fsw ttwttt wtiti modify reduce temporal difference error differentiating treating occurrence adjustable obtain learning rule gradient respect weights takes step size direction decreasing gradient scaled size temporal difference error smoothly parameterized function approximation circumvent curse dimensionality provided accurately approximate true function small number parameters sutton introduced wonderful idea algorithm eligibility trace suppose visited sequence states updating sutton suggested update values preceding states key idea dynamic programming propagate information expected rewards backward state space sutton proposed remember gradient t-i t-i t-i state t-i visited update t-i t-i taking step direction smaller step direction smaller step direction step size decreased factor learning rule called eligibility state figure shows eligibility sequence states bar graph infinite sum fsw fsw wtiti figure sequence states eligibility state shown vertical bar articles magazine ssssssss approximation error resulting linear combination fixed-basis functions denominator error results fact approximation errors state propagated backward earlier states large denominator approaches error denominator small produce large errors result hold essential backups performed performed current policy condition observed fail converge algorithm computing fixed policy direct access transition probabilities reward function algorithm limited utility perform policy-improvement step equation implementing policy-iteration algorithm policy improvement requires access model generate states probabilities applications domains model game-playing settings backgammon easy compute set moves state probabilities successor states combined policy improvement learn approximately optimal policy backgammon tesauro famous td-gammon system td-gammon neural network representation state state backgammon game vector features encode locations pieces board values shown dice td-gammon begins randomly initialized neural network plays series games step makes full one-step lookahead search applies neural network evaluate resulting states makes move highest backed-up short applies equation compute action perform form local policy improvement making move observes resulting state applies rule update function effect td-gammon executing form modified policy iteration alternates step policy evaluation update step policy improvement computation move make guarantee algorithm converge optimal policy easy construct examples strategy performing action based current approximation function leads local minimum navigation problem shown figure large mountain separating start state goal network roads passes north mountain similar shorter network passes south suppose initial policy takes north side eventually reach goal updating function suppose north side appears shorter south side estimates values states south side high future trials continue north roads southern route called problem exploration heart problem find optimal policy prove offpolicy action leads expected results worse actions optimal policy situation essential explore southern path determine worse northern path strategy taking action appears optimal based current function called pure exploitation strategy figure shows pure exploitation strategy find optimal policy online reinforcement learning algorithms balance exploitation exploration fortunately backgammon random dice rolls inject randomness game td-gammon investigates moves game experimentally performance td-gammon outstanding plays computer program good world players results versions program separate matches figure algorithm computing policy articles winter procedure initialize initialize randomly starting state converged action observe resulting state reward rwf fig vextenddouble vextenddouble end return deflnes end resource constraints actions search space identify earliest constraint violation repair moving tasks time lengthening 
schedule search terminates violation-free schedule found zhang dietterich reformulated jobshop scheduling problem reinforcement learning problem optimal policy chooses sequence repairs produces shortest schedule reward function small cost repair action final reward inversely proportional final length schedule zhang dietterich applied feedforward neural network represent function actions domain deterministic deliberate exploration needed system makes random exploratory move probability gradually decreased learning learning system finds schedules substantially shorter previous method expenditure computer time approach converting combinatorial optimization problems reinforcement learning problems applicable important industrial domains applications show simulator task solve reinforcement learning problem large search spaces domains involving interaction hard-to-model real-world environments robot navigation factory automation method needed approaches explored approach learn predictive model environment interacting interaction environment training supervised learning form env env environment current state action resulting state standard supervised learning algorithms applied learn model combined learn optimal policy human players shown table situations moves chosen td-gammon adopted expert humans similar strategy applied zhang dietterich problem job-shop scheduling job-shop scheduling set tasks scheduled avoid resource conflicts task requires resources duration task prerequisite tasks completed executed optimal schedule completes tasks minimum amount time satisfies resource prerequisite constraints zweben daun deale developed repair-based search space task state complete schedule tasks assigned start times starting state critical path schedule task scheduled early subject prerequisite constraints ignoring figure networks roads mountain exploration initial policy northern roads discover southern roads provide shorter route goal articles magazine start program training games opponents results td-gammon robertie davis magriel pts games ppg td-gammon goulding woolsey snellings russell sylvester pts games ppg td-gammon robertie pts games ppg ppg points game table summary performance td-gammon world players results expressed net points won lost points won game tesauro approach model-free algorithm called q-learning developed watkins watkins watkins dayan subject section model-free reinforcement learning q-learning q-learning online approximation iteration key q-learning replace function action-value function quantity expected cumulative discounted reward performing action state pursuing current policy state maximum values state max write version bellman equation max role function illustrated figure contrasted function left part figure bellman backup updates values states result actions part figure analogous backup works taking values backing compute updated function learned algorithm exploits insight online sampling transition probabilities additional idea online sampling actions specifically suppose visited state performed action observed resulting state reward update function max suppose time visit state choose action uniformly random effect approximate full bellman backup equation expected cumulative discounted reward executing action maximum values random choice ensures learn values action state online updates ensure experience resulting states proportion probabilities general choose actions perform long nonzero probability performing action state reasonable strategy choose action estimated largest time choose random action small probability watkins proves long action performed state infinitely times function converges optimal function probability learned convert policy conversion presented insurmountable difficulty trivial representation optimal policy computed argmax crites barto applied q-learning problem controlling elevators story office building peak trafarticles winter figure comparison function function black nodes represent situations agent chosen action white nodes states agent chosen action ing problem assigning radio channels cellular telephone traffic singh bertsekas showed q-learning find policy sophisticated complex published methods open problems reinforcement learning important problems remain unsolved reinforcement learning reflects relative youth field discuss problems multilayer sigmoidal neural networks value-function approximation worked reason networks suited reinforcement learning tend forget episodes good bad retrained episodes frequently make small gradient-descent steps makes learning slow early stages important open problem clarify properties ideal value-function approximator possess develop function approximators properties initial research suggests value-function approximators local averagers compute state interpolating values previously visited states gordon key problem develop reinforcement methods hierarchical problem solving large search spaces distance goal branching factor big search method work large search spaces hierarchical fic people press elevator call buttons floors building call elevator floor inside elevator press destination buttons request elevator stop floors elevator control decisions decide stopping floor direction decide approaching floor stop floor skip crites barto applied rules make decision reinforcement learning problem learn stop skip floors goal controller minimize square time passengers wait elevator arrive pressing call button crites barto team q-learners elevator cars q-function represented neural network input features sigmoidal hidden units linear output units represent stop skip reward negative squared wait time previous action employed form random exploration exploratory actions chosen higher estimated values figure compares performance learned policy heuristic algorithms including nonproprietary algorithms left-hand graph shows squared wait time right-hand graph shows percentage passengers wait seconds elevator learned policy performs methods interesting application q-learnfigure comparison learned elevator policy published heuristic policies articles magazine sector dlb huff lqf huff fim esa esa squared wait time policy sector dlb huff lqf huff fim esa esa percent waits seconds policy approximately hierarchical structure exploited reduce cost search studies ideas hierarchical reinforcement learning dayan hinton kaelbling singh key problem develop intelligent exploration methods weak exploration methods rely random biased random choice actions expected scale large complex spaces property successful applications shown previously backgammon job-shop scheduling random search reaches goal state receives reward domains success contingent long sequence successful choices random search low probability receiving reward intelligent search methods means-ends analysis integrated reinforcement learning systems integrated learning architectures soar laird newell rosenbloom prodigy minton fourth problem optimizing cumulative discounted reward problems system operate continuously goal maximize average reward unit time algorithms criterion complex behaved methods put forward recently mahadevan tadepalli schwartz difficult problem existing reinforcement learning algorithms assume entire state environment visible time step assumption true applications robot navigation factory control sensors provide partial information environment algorithms solution hidden-state reinforcement learning problems developed littman cassandra kaelbling mccallum parr russell cassandra kaelbling littman exact solution appears difficult challenge find approximate methods scale large hidden-state applications substantial open problems reinforcement learning methods applied wide range industrial problems traditional dynamic programming methods infeasible researchers area optimistic reinforcement learning algorithms solve problems resisted solution machine-learning methods past general problem choosing actions optimize expected utility problem faced general intelligent agents reinforcement learning approach attacking problems learning stochastic models final topic discuss area learning stochastic models traditionally researchers machine learning sought general-purpose learning algorithms decision tree rule neural network nearest-neighbor algorithms efficiently search 
large flexible space classifiers good fit training data algorithms general major drawback practical problem extensive prior knowledge difficult incorporate prior knowledge general algorithms secondary problem classifiers constructed general learning algorithms difficult interpret internal structure correspondence real-world process generating training data past years tremendous interest knowledgebased approach based stochastic modeling stochastic model describes real-world process observed data generated terms generative stochastic model causal model emphasize perspective stochastic model typically represented probabilistic network graph structure captures probabilistic dependencies independencies set random variables node graph probability distribution individual distributions joint distribution observed data computed solve learning problem programmer designs structure graph chooses forms probability distributions yielding stochastic model free parameters parameters node-probability distributions training sample learning algorithms applied determine values free parameters fitting model data stochastic model learned probabilistic inference carried support tasks classification diagnosis prediction details probabilistic networks recent textbooks jensen castillo gutierrez hadi probabilistic networks figure probabilistic netarticles winter graph age affects insulin diabetes node age insulin conditionally independent diabetes generally values parents node independent nodes graph descendants formally parents parents parents descendent addition structure network probability distribution represented standard approach discretize variables small number values represent probability distribution table suppose discretized age values preg values mass values kilograms probability distributions nodes represented probability tables shown table learning task fill probability values tables table requires independent parameters values sum table requires parameters table requires parameters row sum similar tables required nodes network set training examples learning problem easy solve probability computed directly training data cell computed proportion patients sample pregnancy parameter fraction training examples age preg mass technically maximum-likelihood estimates probabilities difficulty arise cells tables examples resulting probability estimates uncertain solution smooth probabilities adjacent cells table require similar ensemble approach generate ensemble fitted stochastic models section ensembles classifiers process learning stochastic model consists steps choosing graph structure form probability distribution node graph fitting parameters probability distributions training data work diagnose diabetes variables abbreviations age age patient preg number pregnancies mass body mass insulin blood-insulin level glucose-tolerance test glucose bloodglucose level glucose-tolerance test diabetes true patient diabetes medical diagnosis setting variables observed computer estimate probability patient diabetes estimate probability diabetes variable true network corresponds decomposition joint-probability distribution variables node network corresponds probability distribution form node parents parents node nodes arcs pointing node words network correct representation relationships variables factor joint probability distribution product smaller distributions structure network arcs absent viewed conditional independencies variables conditionally independent figure probabilistic network diabetes diagnosis articles magazine diabetes insulin glucose age preg mass current applications steps performed user step performed learning algorithm section briefly discuss methods automating step learning graph structure learn model apply predict patients diabetes case observe values variables model diabetes node goal compute probability node seek distribution precompute distribution offline observing data resulting conditional probability table immense approach wait values variables observed compute single row class probability table inference problem studied intensively general elegant algorithm junction-tree algorithm developed jensen lauritzen olesen addition efficient online algorithms discovered ambrosio worst case algorithms require exponential time probabilistic network sparsely connected running time reasonable naive bayes classifier simple approach stochastic modeling classification problems so-called naive bayes classifier duda hart approach training examples assumed produced probabilistic network shown figure class variable features model environment generates choosing stochastically class generate class chosen features describing generated independently individual distributions applications values features discretized feature takes small number discrete values probability distributions represented tables diabetes network learned directly training data counting fraction examples age articles winter table probability tables age preg mass nodes figure learning algorithm fill actual probability values based observed training data preg age preg petitive superior domingos pazzani showed algorithm robust violations assumption features generated independently naive unsupervised learning important application stochastic models problems unsupervised learning unsupervised learning collection examples goal construct model examples generated examples belong collection classes determine properties classes called clustering data classes algorithms developed apply measure distance examples group nearby examples stochastic modeling approach unsupervised learning works essentially supervised learning begin defining structure model probabilistic network commonly model naive network naive bayes classifier shown figure network structure problem learning parameters network difficult data values class variable simple case problem fitting stochastic models hidden variables variables values observed training data algorithms developed fitting networks hidden variables naive bayes classification algorithm basic goal article compute maximum-likelihood estimates parameters network refer vector weights words find maximizes observed training sample typically formulated equivalent problem maximizing log likelihood log assumption training generated independently equivalent maximizing log i-th training briefly sketch algorithms gradient descent expectation maximization gibbs sampling gradient descent bayes networks russell describe method computing gradient log likelihood logp focus node network representing variable parents class feature simple form network easy derive classification rule application bayes rule suppose classes decision rule classify class equivalently bayes rule write dividing equation cancel normalizing denominator obtain quantity product individual probabilities decision rule classify class fact naive bayes model barely deserves model applications performs surprisingly figure compares performance naive bayes classifier benchmark tasks domingos pazzani results show domains naive bayes performs badly typically compx articles magazine figure probabilistic network naive bayes classifier entry conditional probability table russell show conditional probability numerator computed algorithm inference probabilistic networks including junction-tree algorithm mentioned earlier formula gradient parameters network computed respect training sample apply standard gradient-descent algorithms fixed step-size methods conjugate gradient algorithm search maximumlikelihood set weights subtlety ensure weights lie sum appropriately suffices constrain renormalize step gradient descent russell tested algorithm wide variety probabilistic network structures expectation maximization algorithm algorithm expectation-maximization algorithm dempster laird rubin applied probabilistic networks node-probability distributions belong exponential family distributions includes binomial multinomial exponential poisson normal distributions conditional probability tables multinomial distributions algorithm applied case article algorithm iterative algorithm starts initial incrementally modifies increase likelihood observed data understand imagine training augmented include parameters describing values hidden variables concreteness simple case figure class variable hidden assume takes values augment training redundant case generally observed augmented expected values sufficient statistics describing probability distribution hidden variables algorithm alternates steps converges e-step current compute augmentations m-step augmented data set compute maximum-likelihood estimates assumption log probability distribution values hidden variables correctly augmented training case figure e-step applies bayes 
theorem quantities right-hand side computed current quantity product feature current estimated probability generating class m-step naive bayes classification estimate weights augmented training examples estimate sum augmented divide sample size estimate conditional probability feature examples class training sum augmented values divide total effect treat augmented training member class probability member class probability well-known application algorithm unsupervised clustering autoclass program cheeseman addition discrete variables kind discussing autoclass handle continpx jixiij articles winter accuracy naive bayes accuracy figure comparison naive bayesian classifier data sets network setting repeat loop times compute values probabilistic network figure compute current guesses observed values sample distribution flip biased coin probability heads compute values parameter represents probability generating class suppose prior distribution uniform distribution values number training examples assigned class posterior probability special form beta distribution parameters algorithms drawing samples distribution similarly parameter represents probability j-th feature drawn class assuming uniform priors variable beta distribution parameters number training examples class sample distribution thing parameters class record parameter vector set gibbs sampler converge stationary distribution perform number iterations recording values parameter vector procedure ensemble probabilistic networks applied classify data points close resemblance gibbs sampling algorithm algorithms training examples augmented information hidden variables information describes probability distribution hidden variables gibbs sampling information consists random values drawn probability distribution parameters recomputed maximum-likelihood estimates based current values hidden variables gibbs sampling parameter values sampled posterior distribution current values hidden variables viewed maximum-likelihood approximation map approximation gibbs sampling potential problem arise gibbs sampling caused symmetries stochastic model case suppose true uous variables computing maximum-likelihood estimates parameters adopts prior probability distribution parameter values computes maximum posteriori probability map values easily accomplished minor modification interesting applications autoclass problem analyzing infrared spectra stars autoclass discovered class star discovery subsequently accepted astronomers cheeseman gibbs sampling final algorithm discuss monte carlo technique called gibbs sampling geman geman gibbs sampling method generating random samples joint-probability distribution sampling directly joint distribution difficult suppose conditional distribution variable terms gibbs sampler works start set arbitrary values random variables sample random variable distribution repeat long mild conditions empirical distribution generated points converge joint distribution learning probabilistic networks suppose full bayesian approach learning unknown parameters network cases compute posterior probability unknown parameters data denote vector unknown parameters full bayesian approach treat random variable prior probability distribution training examples compute difficult hidden classes gibbs sampler generate samples distribution simply ignoring values obtain samples values constitute ensemble learned values network parameters classify data point apply values predict class vote voting equal weight values higher posterior probability sample apply gibbs sampler unsupervised learning problem begin choosing random initial values parameters articles magazine underlying hidden classes class class model generate examples class class force model easily represent examples class represent examples class permitted run long fact gibbs sampler explore possibilities sample weight vectors includes weight vectors alternatives combine weight vectors bad results practice problem arise general steps remove symmetries model neal gibbs sampling general method applied situations algorithm generality gibbs sampling made construct general-purpose programming environment called bugs learning stochastic models gilks thomas spiegelhalter environment user specifies graph structure stochastic model form probability distribution node prior distributions parameter system develops gibbs sampling algorithm fitting model training data bugs downloaded mrc-bsu cam bugs sophisticated stochastic models section presents examples sophisticated stochastic models developed hierarchical mixture experts hme model hidden markov model hmm dynamic probabilistic network dpn naive model figure models applied wide number problems performing kind detailed modeling causal connections performed diabetes figure hierarchical mixture experts hme model jordan jacobs intended supervised learning situations believes training data generated mixture separate experts speech-recognition system face task distinguishing spoken words bee tree gate mate task naturally decomposes hard subproblems distinguishing bee tree distinguishing gate mate figure shows probabilistic network simple mixture-of-experts model case model training generated generating data points choosing expert stochastically depending choosing class depending values things note model direction causality reversed naive bayes network figure assume features observed model probability distribution bottom node graph make strong assumptions probability distribution extremely complex probability classes function combination features expert development general model jordan jacobs assume probability distribution simple form figure shows model kind neural network classifier random variable specifies expert input features fed experts gating network output expert probability distribution classes output gate probability distribution experts model analytic form index varies experts output gating network expert probability distribution classes output expert jordan jacobs investigated networks individual experts gating network simple forms twoclass problem expert form transpose vector parameters vector input feature values usual logistic sigmoid function exp gating network earlier ensembles classifiers gating values computed words dot product weight vector input features output soft-max values values positive sum multinomial logit model statistics problem learning parameters zeu articles winter states variation hmm equivalent stochastic finite-state automaton fsa time step fsa makes probabilistic state transition generates output letter common variation hmms include absorbing state halting state values state variable hmm makes transition state terminates string generated permitting hmms model strings variable length hmms applied widely speech recognition alphabet letters consists frames speech signal rabiner word language modeled hmm spoken word speech-recognition system computes likelihood word hmms generated spoken word recognizer predicts word similar analysis applied level sentences concatenating word-level hmms goal find sentence generated speech signal learn hmm set training examples provided string sequence states generated string hidden algorithm applied context hmms baum-welch forward-backward algorithm e-step algorithm augments training statistics describing hypothesized string states generated m-step parameters probability distributions reestimated augmented training data stolcke omohundro developed algorithms learning structure parameters hmms training examples applied techniques problems speech recognition incorporated algorithm speaker-independent speechrecognition system dynamic probabilistic network hmm number values state variable point time large number parameters state-transition probability distribution intractably large solution represent causal structure state stochastic model mobile robot steerable television camera images observed camera output alphabet hmm hidden state consist location robot room direction camera pointing suppose locations camera direca mixture-of-experts model similar problem unsupervised learning hidden variable class training expert responsible generating training knew expert generated training fit parameters directly training data naive bayes algorithm jordan jacobs applied gradient-descent algorithm solve learning problem choice sigmoid soft-max functions experts gates algorithm efficient 
implementation sequence weighted squares problems simple one-level hierarchy experts shown figure extended deeper hierarchies figure shows three-level hierarchy fitting algorithms apply general case hidden markov model figure shows probabilistic network hidden markov model hmm rabiner hmm generates examples strings length alphabet letters string letters stands observable generate string hmm begins generating initial state probability distribution state generates letter string distribution generates state generates letter applications transition probability distribution pairs adjacent state variables similarly output emission probability distribution articles magazine figure probabilistic model describing mixture experts gateexpert expert figure mixture-of-experts model viewed specialized neural network tions hmm hidden state single variable values suppose robot separate commands change location steer camera time step chooses perform actions makes sense represent hidden state separate state variables robot location camera direction figure shows resulting stochastic model variously called dpn kanazawa koller russell dynamic belief network dean kanazawa factorial hmm ghamramani jordan inference learning dpns computationally challenging approach applying algorithm gibbs sampling sound number step training episodes computing average cumulative augmented testset training reinforcement examples simplemoves difficult ghahramani jordan kanazawa koller number training russell episodes describe elimenemies algorithms perform approximate e-steps recent review number active training research episodes area average cumulative found testset smyth reinforcement heckerman nonlocalmoves jordan application-specific stochastic models major motivation number training stochastic episodes modeling surrounded approach advice machine advice learning episodes communicate advice background knowledge episodes advice learning algorithm episodes general models discussed achieve goal extent direction applications machine learning pursuing approach give flavor kinds model developed describe recently published papers revow williams hinton developed stochastic model handwritten digit recognition shown figure generate digit model randomly choose digits determines home locations points control uniform spline denoted figure spline specifies shape digit step randomly perturbs control points gaussian distribution produce control points generate handwritten digit denoted figure generate six-degree-of-freedom affine transformation models translation rotation skewing randomly chosen affine transformation control points deterministically lay sequence beads spline curve beads act circular gaussian ink generators ink pixels image parameter specifies standard deviation gaussian ink generators beads distance spline number beads varies function point made choices independent image pixels model generates inked pixels diagram box lower left corner generate pixel generates boolean variable pixel noise pixel pixel digit noise pixel location chosen uniformly random inked digit pixel beads chosen random variable bead index pixel inked symmetric gaussian probability distribution parameters model include probability noise pixels standard deviaarticles winter figure hierarchical mixture-of-experts model figure probabilistic network hidden markov model digit affine transform knots beads bead index location noise digit pixel bead probability digit model generated image practice revow maximimumlikelihood approach compute values control points noise-digit choices maximize likelihood generating observed image algorithm applied compute figure shows fitting model typical image notice fitting initialized large ink generators capture inked pixels begins converge decreased schedule beads positioned spline current fitting resumed task performed approximately times image learning goal learning learn digit home locations control points variance perturbing control points addition parameters controlling affine transformation learned parameters assumed classes solve learning problem revow apply e-step training examples augmented maximumlikelihood locations control points computed nested earlier m-step average home location control point average computed model digit trained fitting training examples digit running time dominated cost classification algorithm revow report results competitive best-known methods task recognizing digits mail zip codes achieve performance postprocessing steps analyze digit model fits image considered learning mixtures digit models cases significantly ways writing digit digit central horizontal bar advantage stochastic modeling approach learning fast computationally quick statistically parameters digit model coordinates control point affine transformation parameters advantage digit images preprocessed remove slants scale standard size preprocessing significant source errors require extra implementation effort related advantage stotion gaussian ink generators variance gaussian perturb control points parameters controlling affine transformation home locations control points perform tasks model classification learning classification image learned spline models goal find spline model generated image computing probability spline model digit generated image computation difficult locations inked pixels hypothesized class digit principle locations control points values choices inked pixel noise pixel digit pixel combination generated observed image probability integrate parameters compute articles magazine figure simple dynamic probabilistic network independent state variables robot location camera direction figure probabilistic network generating digit choosing pixels ink chastic models fit single digit context digits precise segmentation required prior classification primary disadvantage classification slower perform search fit digit model learning structure stochastic models methods discussed point learn parameters stochastic model structure important research question structure learned data recent research made major progress developing algorithms problem algorithms developed chow liu learning network structure form directed tree algorithm constructs complete undirected graph nodes variables edges labeled mutual information variables algorithm finds maximum weighted spanning tree graph chooses root node arbitrarily orders arcs point root nice feature algorithm fast runs polynomial time cooper herskovits developed algorithm called learning structure stochastic model variables observed training data adopt maximum posteriori map approach bayesian framework key contribution paper derive formula posterior probability network formula updated incrementally made network formula implemented simple greedy search algorithm finding approximately map network structure algorithm requires user provide ordering variables adding arcs variables earlier ordering variables ordering approach significantly constrains search ensures learned network remain acyclic begins network arcs variables independent evaluates posterior probability adding single edge makes highest-ranking addition greedy algorithm continued single change improves posterior probability heckerman geiger chickering describe modification method computing posterior probabilities local search algorithm improvement method requires prior probability distribution form prior network parameters equivalent sample size controls data required override prior penalty arc prior network local search algorithm considers one-step network arc addition deletion reversal retains change increases posterior probability network obtain results comparable interesting found important role prior network provide good starting point local search bias objective function guiding search friedman goldszmidt developed network learning algorithm called tree-augmented naive bayes tan specifically supervised learning tan algorithm starts naive bayes network kind shown articles winter figure stages fitting models image revow reproduced permission image displayed top row row shows model fitted bottom row attempts fit model light circles bottom rows image artificially thinned circles visible spirtes glymour scheines summary stochastic models completes review methods learning stochastic models good survey articles topic buntine heckerman area stochastic modeling active journals conferences devoted exclusively neural network applications presenting papers stochastic modeling research community gathering experience developing improved algorithms 
fitting reasoning stochastic models stochastic models work intractable challenge find general-purpose tractable approximation algorithms reasoning elegant expressive stochastic models concluding remarks survey choose areas omit briefly mention active areas central topic machine learning control overfitting developments area researchers explored penalty functions resampling techniques including crossvalidation preventing overfitting understanding overfitting process obtained statistical concepts bias variance authors developed bias-variance decompositions classification problems active topic study algorithms learning relations expressed horn-clause programs area inductive logic programming algorithms theoretical results developed area finally papers addressed practical problems arise applications visualization learned knowledge methods extracting understandable rules neural networks algorithms identifying noise outliers data algorithms learning easy-to-understand classifiers exciting developments past years relevant literature machine learning growing rapidly areas computer science apply machine-learning methods attack problems expect flow interesting problems practical solutions continue exciting time working machine learning figure considers adding arcs improve posterior probability network apply modification chow liu algorithm learn tree structure arcs connecting variables figure shows network learned tam diabetes diagnosis problem compared figure directions arcs wrong nonetheless network accurate classifications figure compares performance tan benchmark problems plot shows tan outperforms domains important papers topic structure learning probabilistic networks important verma pearl spirtes meek spiegelhalter articles magazine preg age insulin dpf mass glucose diabetes figure probabilistic network constructed tan algorithm diabetes diagnosis task accuracy tan accuracy figure comparison tan benchmark tasks points diagonal line correspond cases tan gave accurate results abu-mostafa learning hints neural networks journal complexity ali pazzani error reduction learning multiple descriptions machine learning barto bradtke singh learning act real-time dynamic programming artificial intelligence barto sutton introduction reinforcement learning cambridge mass mit press bellman dynamic programming princeton princeton press bertsekas tsitsiklis neuro-dynamic programming belmont mass athena scientific blum rivest training -node neural network np-complete extended abstract proceedings workshop computational learning theory san francisco calif morgan kaufmann blum empirical support winnow weighted-majority algorithms results calendar scheduling domain machine learning breiman bagging predictors machine learning breiman stacked regressions machine learning buntine guide literature learning probabilistic networks data ieee transactions knowledge data engineering buntine operations learning graphical models journal artificial intelligence research buntine theory learning classification rules thesis school computing science technology caruana algorithms applications multitask learning proceedings thirteenth international conference machine learning saitta san francisco calif morgan kaufmann cassandra kaelbling littman acting optimally partially observable stochastic domains proceedings twelfth national conference artificial intelligence menlo park calif american association artificial intelligence castillo gutierrez hadi expert systems probabilistic network models york springer-verlag catlett changing continuous attributes ordered discrete attributes proceedings european working session learning kodratoff berlin springer-verlag chan stolfo learning arbiter combiner trees partitioned data scaling machine learning proceedings international conference knowledge discovery data mining menlo park calif aaai press cheeseman kelly taylor freeman stutz bayesian classification proceedings seventh national conference artificial intelligence menlo park calif american association artificial intelligence cherkauer human expert-level performance scientific image analysis task system combined artificial neural networks working notes aaai workshop integrating multiple learned models chan fit imlm chipman george mcculloch bayesian cart department statistics chicago gsbrem uchicago papers cart chow liu approximating discrete probability distributions dependence trees ieee transactions information theory clemen combining forecasts review annotated bibliography international journal forecasting cohen fast effective rule induction proceedings twelfth international conference machine learning san francisco calif morgan kaufmann cooper herskovits bayesian method induction probabilistic networks data machine learning craven shavlik extracting tree-structured representations trained networks advances neural information processing systems eds touretzky mozer hasselmo cambridge mass mit press crites barto improving elevator performance reinforcement learning advances neural information processing systems san francisco calif morgan kaufmann ambrosio incremental probabilistic inference ninth annual conference uncertainty eds heckerman mamdani san francisco calif morgan kaufmann dayan hinton feudal reinforcement learning advances neural information processing systems san francisco calif morgan kaufmann dean kanazawa model reasoning persistence causation computational intelligence dempster laird rubin maximum likelihood incomplete data algorithm journal royal statistical society dietterich bakiri solving multiclass learning problems error-correcting output codes journal artificial intelligence research dietterich kong machine learning bias statistical bias statistical variance decision tree algorithms department computer science oregon state ftp orst pub tgd papers tr-bias articles winter bayesian networks msr-tr- advanced technology division microsoft research redmond washington heckerman geiger chickering learning bayesian networks combination knowledge statistical data machine learning hyafil rivest constructing optimal binary decision trees np-complete information processing letters jensen lauritzen olesen bayesian updating recursive graphical models local computations computational statistical quarterly jensen introduction bayesian networks york springer john kohavi pfleger irrelevant features subset selection problem proceedings eleventh international conference machine learning san francisco calif morgan kaufmann jordan jacobs hierarchical mixtures experts algorithm neural computation kaelbling hierarchical reinforcement learning preliminary results proceedings tenth international conference machine learning san francisco calif morgan kaufmann kaelbling littman moore reinforcement learning survey journal artificial intelligence research kanazawa koller russell stochastic simulation algorithms dynamic probabilistic networks proceedings eleventh conference uncertainty artificial intelligence san francisco calif morgan kaufmann kira rendell practical approach feature selection proceedings ninth international conference machine learning san francisco calif morgan kaufmann kohavi kunz option decision trees majority votes proceedings fourteenth international conference machine learning san francisco calif morgan kaufmann kohavi sahami error-based entropy-based discretizing continuous features proceedings international conference knowledge discovery data mining san francisco calif morgan kaufmann kolen pollack back propagation sensitive initial conditions advances neural information processing systems san francisco calif morgan kaufmann kong dietterich error-correcting output coding corrects bias variance twelfth international conference machine learning eds prieditis russell san francisco calif morgan kaufmann kononenko estimating attributes analysis extensions relief proceedings european conference machine learning amsterdam springer verlag domingos pazzani independence conditions optimality simple bayesian classifier proceedings thirteenth international conference machine learning saitta san francisco calif morgan kaufmann duda hart pattern classification scene analysis york wiley fayyad irani multi-interval discretization continuous-valued attributes classification learning proceedings thirteenth international joint conference artificial intelligence menlo park calif international joint conferences artificial intelligence flexner random house unabridged dictionary york random house freund schapire decision-theoretic generalization on-line learning application boosting proceedings european conference computational learning theory berlin springer-verlag freund schapire experiments boosting algorithm proceedings thirteenth international conference machine learning saitta san francisco calif morgan kaufmann friedman goldszmidt building classifiers bayesian networks proceedings thirteenth national conference 
artificial intelligence menlo park calif american association artificial intelligence furnkranz widmer incremental reduced error pruning proceedings eleventh international conference machine learning san francisco calif morgan kaufmann geman geman stochastic relaxation gibbs distributions bayesian restoration images ieee transactions pattern analysis machine intelligence ghahramani jordan factorial hidden markov models advances neural information processing systems eds touretzky mozer hasselmo cambridge mass mit press gilks thomas spiegelhalter language program complex bayesian modelling statistician golding roth applying winnow context-sensitive spelling correction proceedings thirteenth international conference machine learning saitta san francisco calif morgan kaufmann gordon stable function approximation dynamic programming proceedings twelfth international conference machine learning san francisco calif morgan kaufmann hansen salamon neural network ensembles ieee transactions pattern analysis machine intelligence hashem optimal linear combinations neural networks thesis school industrial engineering purdue heckerman tutorial learning articles magazine kononenko simec robnik-sikonja overcoming myopic inductive learning algorithms relief-f applied intelligence forthcoming kucera francis computational analysis present-day american english providence brown press kwok carter multiple decision trees uncertainty artificial intelligence eds schachter levitt kannal lemmer amsterdam elsevier science laird newell rosenbloom soar architecture general intelligence artificial intelligence littlestone learning quickly irrelevant attributes abound linear-threshold algorithm machine learning littman cassandra kaelbling learning policies partially observable environments scaling proceedings twelfth international conference machine learning san francisco calif morgan kaufmann lowe similarity metric learning variable-kernel classifier neural computation mackay practical bayesian framework backpropagation networks neural computation mahadevan average reward reinforcement learning foundations algorithms empirical results machine learning mahadevan kaelbling national science foundation workshop reinforcement learning magazine mccallum instance-based utile distinctions reinforcement learning hidden state proceedings twelfth international conference machine learning san francisco calif morgan kaufmann mehta agrawal rissanen sliq fast scalable classifier data mining lecture notes computer science york springerverlag merz murphy uci repository machine learning databases ics uci mlearn mlrepository html miller subset selection regression york chapman hall minton carbonell knoblock kuokka etzioni gil explanationbased learning problem-solving perspective artificial intelligence moore lee efficient algorithms minimizing cross-validation error proceedings eleventh international conference machine learning eds cohen hirsh san francisco calif morgan kaufmann munro parmanto competition networks improves committee performance advances neural information processing systems cambridge mass mit press musick catlett russell decisiontheoretic subsampling induction large databases proceedings tenth international conference machine learning utgoff san francisco calif morgan kaufmann neal probabilistic inference markov chain monte carlo methods crg-tr- department computer science toronto tadepalli auto-exploratory average reward reinforcement learning proceedings thirteenth national conference artificial intelligence menlo park calif american association artificial intelligence opitz shavlik generating accurate diverse members neural-network ensemble advances neural information processing systems eds touretzky mozer hesselmo cambridge mass mit press parmanto munro doyle improving committee diagnosis resampling techniques advances neural information processing systems eds touretzky mozer hesselmo cambridge mass mit press parmanto munro doyle doria aldrighetti marino mitchel fung neural network classifier hepatoma detection proceedings world congress neural networks hillsdale lawrence erlbaum parr russell approximating optimal policies partially observable stochastic domains proceedings fourteenth international joint conference artificial intelligence menlo park calif international joint conferences artificial intelligence perrone cooper networks disagree ensemble methods hybrid neural networks neural networks speech image processing mammone york chapman hall press flannery teukolsky verrerling numerical recipes art scientific computing cambridge cambridge press quinlan bagging boosting proceedings thirteenth national conference artificial intelligence menlo park calif american association artificial intelligence quinlan programs empirical learning san francisco calif morgan kaufmann quinlan learning logical definitions relations machine learning quinlan rivest inferring decision trees minimum description length principle information computation rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee washington ieee computer society raviv intrator bootstrapping noise effective regularization technique connection science articles winter sutton learning predict methods temporal differences machine learning tesauro temporal difference learning td-gammon communications acm tesauro practical issues temporal difference learning machine learning tsitsiklis van roy analysis temporal-difference learning function approximation lids-plaboratory information decision systems massachusetts institute technology tumer ghosh error correlation error reduction ensemble classifiers connection science verma pearl equivalence synthesis causal models proceedings sixth conference uncertainty artificial intelligence san francisco calif morgan kaufmann watkins learning delayed rewards thesis king college oxford watkins dayan technical note q-learning machine learning wettschereck aha mohri review empirical evaluation feature weighting methods class lazy learning algorithms artificial intelligence review wettschereck dietterich experimental comparison nearest-neighbor nearest-hyperrectangle algorithms machine learning wolpert stacked generalization neural networks zhang dietterich reinforcement learning approach job-shop scheduling proceedings twelfth international joint conference artificial intelligence menlo park calif international joint conferences artificial intelligence zhang mesirov waltz hybrid system protein secondary structure prediction journal molecular biology zweben daun deale scheduling rescheduling iterative repair intelligent scheduling eds zweben fox san francisco calif morgan kaufmann thomas dietterich stanford faculty member computer science oregon state held position senior scientist arris pharmaceutical corporation applied machinelearning methods drug design problems editor jude shavlik readings machine learning executive editor journal machine learning e-mail address tgd orst revow williams hinton generative models handwritten digit recognition ieee transactions pattern analysis machine intelligence ricci aha extending local learners error-correcting output codes naval center applied research artificial intelligence washington rosen ensemble learning decorrelated neural networks connection science russell binder koller kanazawa local learning probabilistic networks hidden variables proceedings fourteenth international joint conference artificial intelligence menlo park calif international joint conferences artificial intelligence samuel studies machine learning game checkers ibm journal research development schapire output codes boost multiclass learning problems proceedings fourteenth international conference machine learning san francisco calif morgan kaufmann schwartz reinforcement learning method maximizing undiscounted rewards proceedings tenth international conference machine learning utgoff san francisco calif morgan kaufmann shafer agrawal mehta sprint scalable parallel classifier data mining proceedings twenty-second vldb conference san francisco calif morgan kaufmann singh transfer learning composing solutions elemental sequential tasks machine learning singh bertsekas reinforcement learning dynamic channel allocation cellular telephone systems advances neural information processing systems cambridge mass mit press smyth heckerman jordan probabilistic independence networks hidden markov probability models neural computation spiegelhalter dawid lauritzen cowell bayesian analysis expert systems statistical science spirtes glymour scheines causation prediction search york springer-verlag online hss cmu html departments philosophy tetrad book book html spirtes meek learning bayesian networks discrete variables data proceedings international conference knowledge discovery data mining san francisco calif morgan kaufmann stolcke 
omohundro best-first model merging hidden markov model induction tr- international computer science institute berkeley california articles magazine 
number training episodes average cumulative testset reinforcement advice simplemoves advice notsimplemoves oppositesimplemoves midvale blvd nakoma home work ave park beltline ball wall goal team players team players unguarded guarded front guarded front left guarded front attacking player ball defending players number training games net testset games won baselinewith advice input characters statesa statesa 
