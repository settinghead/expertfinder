computer systems research past future butler lampson people inventing ideas computer systems nea rly decades driven moore law spectacula rly successful virtual memory packet networks objects relational databases graphical user interfaces examples promising ideas rked capabilities formal methods distributed computing persist ent objects fate doubt parallel computing risc ftware reuse important invention decade world wide web made computer systems researchers light experienc talk topics exciting work years computer systems research butler lampson microsoft computer revolution begun outline context history motivation challenges context moore law performance decade room temperature single electron memory cell size square moore law predicts moore law qualitative processors chip years reconfigurable logic tiny processors ram chip chip network bandwidth sec long distance latency change computer science engineering science describe explain engineering build engineer dime fool dollar today engineering things time virtual memory address spaces packet nets objects subtypes rdb sql transactions bitmaps guis web algorithms history worked capabilities fancy type systems functional programming formal methods software engineering rpc web distributed computing persistent objects security history worked parallelism risc garbage collection interfaces specifications reuse works unix filters big things browser flaky ole failure systems research didn invent web simple idea wasteful fast flaky doesn work denial doesn scale computers simulation communication people storage communication time control physical real time mobile applications simulation models real world clothing cities communication people information fingertips telepresence home control robots mems motherhood challenges correctness scaling parallelism reuse trustworthiness ease challenges programming concurrency processors chip matching biological concurrency cycles declarative sql spreadsheets successes intelligence data models class hierarchy knowledge rep uncertainty real world input speech vision adapting environment challenges systems systems work meeting specs adapting changing environment evolving run made unreliable components growing practical limit credible simulations analysis writing good specs testing performance understanding doesn matter challenges information personal memex remember person hears sees telepresence simulate memex collect world libraries retrieve person challenges physical world hearing speaking people real lighting occlusion noise recognizing speech objects real time organizing systems unreliable changing components scale molecules cells insects cities common sense requires good models reality conclusions engineers understand moore law aim mass markets computers learn build systems work 
dependability analysis virtual memory systems lakshmi bairavasundaram andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison west dayton madison wilaksh dusseau remzi wisc abstract recent research shown modern hard disks complex failure modes conform failstop operation disks exhibit partial failures block access errors block corruption commodity operating systems required deal failures commodity hard disks failure-prone important operating system component exposed disk failures virtual memory system paper examine failure handling policies virtual memory systems classes partial disk errors type context aware fault injection explore internal code paths experiments find failure handling policies current virtual memory systems simplistic inconsistent absent fault injection technique identifies bugs failure handling code systems study identifies reasons poor failure handling design failure-aware virtual memory system introduction modern commodity operating systems assume disk drives work perfectly fail perfectly modern hard disks perfect operate fail-stop fashion exhibit complex partial failures set blocks inaccessible data stored blocks silently corrupted complex errors study worse errors expected diminish disk technology improves increase disk drive complexity increased low-cost unreliable ata disks incidence errors increase commodity operating systems equipped deal partial disk failures highend systems typically employed mechanisms deal disk faults techniques checksumming disk scrubbing commodity operating systems explicitly failure handling mechanisms policies recent work explored failure handling policies commodity file systems paper explore failure handling capabilities virtual memory systems integral part modern operating system file systems significant user disk storage type context aware fault injection techniques elicit failure handling policies virtual memory systems operating systems linux freebsd perform preliminary study windows virtual memory system characterize policies systems based internal robustness iron taxonomy proposed earlier work find virtual memory systems wellequipped deal partial failures file systems studied earlier virtual memory systems policies illogically inconsistent failure handling routines bugs cases failure handling policy simplistic cases absent disregard partial disk failures leads problems ranging loss physical memory abstraction data corruption system security violations paper organized section background partial disk failures virtual memory systems iron taxonomy section describes fault injection analysis methodology section presents experimental results analyzes failure handling approaches systems section discusses related work section concludes background section discusses partial failures commodity hard disks background virtual memory systems finally presents taxonomy failure handling policies paper characterize failure handling virtual memory systems partial disk failures section presents partial failures disk subsystem discusses suitable failure model disks layers storage stack contribute partial failures exhibited disk subsystem range classic problems media errors due bit rot head crashes errors bus controllers recent problems arising buggy firmware code additionally shown device drivers bugs rest operating system entire range sources disk failures documented recent paper proposes failure model disks called fail-partial failure model adopt model injecting disk errors study study makes aspects fail-partial failure model types errors partial disk failures errors blocks read written error code returned disk block corruption contents disk block read operating system altered error code returned disk transience errors disk failures permanent sticky temporary transient case transient failure errors operation performed failure model incorporate specific frequencies error types data frequency partial disk failures scarce drive manufacturers loathe provide information schwarz estimate partial disk errors occur times absolute disk failures recent experiments gray ingen sata disk drives uncovered uncorrectable read errors point view operating system -month experiment period partial disk failures occur dealt question arises component deal failures instance argue disk mirroring deal errors belief operating system components rely mirrored disks provide dependable computing environment component-specific policies optimizations employed simple mirroring operating system component disks include failure handling policy virtual memory systems virtual memory system disk storage provide applications address space larger physical memory helps system execute multiple processes large address spaces simultaneously disk area virtual memory system called swap space virtual memory system swap space store memory pages expected typically systems tend remove pages accessed recently accessed frequently memory store disk called page-out page stored disk accessed brought back physical memory called page-in page-out pagein process transparent applications performance effects virtual memory system responsible handling disk errors maintaining illusion page physical memory virtual memory systems make file systems situations directly on-disk space swap space maintained file file system virtual memory systems applications memory-map file data mmap system call file portion file memory mapped applications operate file data memory locations user code pages memory-mapped executable file program executed situations involving file system virtual memory system depends file system recover propagate disk errors subsections outline features virtual memory systems linux freebsd failure handling policies studied paper features windows virtual memory system discussed evaluation section linux linux virtual memory system largely derived previous linux versions performs swapping user-mode pages user-mode pages data stack code pages form user process order virtual memory system simple pages belong kernel paged simplification highly restrictive kernel pages occupy small portion main memory page replacement algorithm similar algorithm paged-out pages accessed space created pages read disk system issues reads advance read-ahead based application accesses improve performance swap area separate disk partition file file system swap header information swap area number blocks list faulty blocks level technique comment detection assumes disk works errorcode check return codes lower levels assumes lower level detect errors sanity check data structures consistency require extra space block redundancy redundancy blocks detect corruption end-to-end table levels iron detection taxonomy table describes levels detection iron taxonomy levels developed earlier paper level technique comment recovery assumes disk works propagate propagate error informs user record record operation succeed prevents dependent actions proceeding stop stop activity crash prevent writes limit amount damage guess return guess block contents wrong failure hidden retry retry read write handles failures transient repair repair data structs lose data remap remaps block file locale assumes disk informs system failures redundancy block replication forms enables recovery loss corruption table levels iron recovery taxonomy table describes levels recovery iron taxonomy levels developed earlier level record added paper freebsd freebsd open source operating system derived bsd unix design virtual memory system freebsd originally based mach virtual memory system considerable updates years freebsd virtual memory system allocates pages requested free list pages maintains sufficient free pages paging frequently inactive pages freebsd virtual memory system paging entire processes implies 
addition user-mode pages kernel thread stacks processes paged page tables freed system extreme memory pressure unlike linux freebsd virtual memory system perform extra read-ahead issue separate block read commands read blocks part read command block needed linux freebsd swap area disk partition file freebsd swap area data structures linux swap header failure handling policy taxonomy paper extend iron taxonomy proposed earlier paper taxonomy originally designed describe failure handling policies file systems find applicable virtual memory systems iron taxonomy consists axis detection recovery table table describe levels detection recovery addition levels proposed previously include recovery level record level system records operation succeed level recovery prevents system performing action assumes successful completion operation write error detected system recovers record free dirty memory page assuming successfully written disk avoiding data loss extend iron taxonomy adding axis prevention prevention axis encompasses techniques avoid loss due partial disk failures table describes levels prevention system special prevention techniques case system assumes disk works errors dealt occur remember basic prevention strategy remember specific block bad system bad experience block strategy prevent future data loss reboot phenomenon observed long time systems fail faults cured systems rebooted reinitialized systems rid effects transient bugs accumulated time fact failure prevention strategy periodically rebooting subsystems rebooting strategy virtual memory systems range disabling enabling swap area periodically re-initializing drivers disk controllers loadbalance prevention technique attempts reduce wear data blocks balancing load level technique comment prevention assumes disk works remember remembers disk errors prevents usage blocks errors reboot periodically re-initializes system avoid bugs due excess state loadbalance balances read write load disk blocks attempts reduce effects wear blocks scan performs read write checks bogus data detects possibly sticky block errors table levels iron prevention taxonomy table describes levels prevention technique wearleveling file systems flash drives jffs scan final prevention technique scanning disk bad blocks performing accesses bogus data technique raid systems weed potential bad blocks process called disk scrubbing technique classified eager detection technique earlier feel employed prevention technique virtual memory systems scan swap area periodically disk idle time freeblock scheduling avoid disk blocks found bad scan methodology section describe fault injection analysis methodology fault injection framework consists components benchmark injector benchmark layer sets system exposure disk faults layer consists types user processes coordinator managing benchmarking fault injection victims allocate large memory region sleep read memory region aggressors allocate large memory regions force victims pages swap area file system disk errors injected victims pages paged disk read back victims error injection performed injector layer interposes virtual memory system hard disk specifically injector built pseudo-device driver linux geom layer freebsd upper filter driver windows injector located device drivers drivers significant source errors virtual memory system equipped handle errors policies virtual memory system observed isolation method types errors injected injector read errors write errors corruption errors case read write errors error code returned virtual memory system ensure valid data memory read failed error code technique needed virtual memory system ignore error code returned case valid data respective memory page system work fine corruption errors block contents altered block experiments case corruption detected perform detailed analysis corrupting field data structure field-specific values separate experiments perform type context aware fault injection injecting disk errors specific disk blocks specific times data type user-level private data segment user data error injected disk block holds private user data page type-aware context basic function performed virtual memory system interface offered virtual memory system applications context swapoff system call error injected disk block swapoff progress context-aware table presents data types errors injected virtual memory systems table presents contexts error injection performed types contexts explored dependent system study order perform type-aware error injection injector detect type blocks read written detection accomplished variety ways benchmark layer communicates type information data pages injector benchmark allocates data pages initializes pages specific values conveys values injector cases injector block contents determine block type method employed determine type location block linux swap header located block failure handling policy system identified manual observation results error injection specifically sources information injector injector logs operations detail enabling determine failure handling policies instance virtual memory system performing retries read write repeated disk block number remapping disk write repeated disk block memory page benchmark benchmark records return values signals received helps determining error propagated benchmark checks reports validity data read back helps checking block type description detection virtual memory system swap header describes swap space location linux user data page private user data segment content linux freebsd user stack page user stack segment content linux freebsd shared shared memory page processes content linux freebsd mmapped memory-mapped file data content linux freebsd user code page user code segment location linux freebsd kernel stack page kernel thread stack user process kernel information freebsd table data types table describes types blocks failed detection method applicable virtual memory system type order detect kernel thread stack pages made simple modification freebsd kernel obtain memory addresses pages context workload virtual memory system actions swapon makes swap space swapping read swap header initialize in-core structures swapoff removes swap space page-in valid blocks free swap space pagetouch page accessed victim read page disk readahead workload induces readahead reading nearby pages perform read-ahead reading blocks disk madvise victim issues madvise madv willneed page-in blocks hint future reads specifiedinhint pageout aggressors create memory pressure causing page-out write inactive memory pages disk umount file system unmounted write dirty mmaped file data complete process scheduled complete page-out page-in essential data structures process table contexts table shows workload contexts experiments actions performed virtual memory system context data corruption system messages operating system emit error messages system message log techniques determine failure handling policies adopted virtual memory systems combinations data type context error type techniques primarily determine detection recovery policies discuss experiments determine prevention policies section analysis section present results experiments virtual memory systems analyze failure handling approach systems finally discuss experience fault injection techniques experimental results performed detailed analysis linux freebsd virtual memory systems preliminary analysis windows virtual memory system focus detection recovery techniques linux freebsd discuss prevention techniques systems finally evaluate windows present scenarios combinations data 
type context error type linux freebsd experiments involving swap space performed separate disk partition swap space windows experiments involving memory-mapped files user code pages ext file system linux unix file system ufs freebsd observed failure handling policy experiments involving file system combination policies virtual memory system file system linux tables present results fault injection linux virtual memory system detection find read errors detected errorcode checking return codes exceptions occur swapoff virtual memory system pages valid blocks memory error detected application data belongs junk data future memory access lead application crashes data corruption write errors detected read page write error virtual memory system page-in disk block previous contents missing errors lead application crashes application data corruption bad data system security problems application possibly read data belongs process swapon swapoff pagetouch readahead madvise pageout umount user data user stack shared mmapped user code rea err swap header user data user stack shared mmapped user code ite swap header user data user stack shared mmapped user code uptio swap header symbols errorcode sanity experiment applicable comments sanity checks swap space signature version number bad block count table linux detection techniques table presents linux detection techniques read write corruption errors combinations data type rows context columns comments provided tables corruption errors detected corrupted data returned application exception sanity swap header swapon sanity checks correct swap space signature correct version number number bad blocks maximum allowable recovery cases disk error detected linux basic recovery mechanisms read error application-accessed page sigbus signal inform application error propagate case shared memory page processes touch page read error occurs receive sigbus signal words virtual memory system retry read process accesses page propagate swap header corrupted case error returned swapon call experiments memory-mapped file data user code retry observed retry specific disk block system original operation involved disk blocks retry performed block retry initiated file system virtual memory system read swap header fails swapon aretryisperformed retry due implementation bugs results retry swapon returns success read errors call fails swapon swapoff pagetouch readahead madvise pageout umount user data user stack shared mmapped user code rea err swap header user data user stack shared mmapped user code ite swap header user data user stack shared mmapped user code uptio swap header symbols propagate retry record experiment applicable comments sigbus signal separate retry block needed original request retry operation fails success returned error propagated operation remembered page touched error propagates processes touch page read error occurs table linux recovery techniques table presents linux recovery techniques read write corruption errors combinations data type rows context columns bug implementation comments provided tables internally propagate error record handle read errors readahead madvise byusingr record system records failure read future readaheadand madvise data required immediately read-ahead optimization virtual memory system madvise hint block accessed readahead case error propagated page touched madvise retry performed page touched actions fact read unsuccessful freebsd tables present results fault injection freebsd virtual memory system detection errorcode single case detecting read write errors freebsd virtual memory system checks error code returned freebsd detect block corruption leads application crash data corruption cases leads kernel crash corruption kernel swapon swapoff pagetouch madvise complete pageout umount user data user stack shared mmapped user code rea err kernel stack user data user stack shared mmapped user code ite kernel stack user data user stack shared mmapped user code uptio kernel stack symbols errorcode experiment applicable table freebsd detection techniques table presents freebsd detection techniques read write corruption errors combinations data type rows context columns freebsd read block swapon read pages madvise table thread stack blocks detected case errors system unbootable recovery recovery mechanisms freebsd deal detected errors retry memory-mapped data written file system unmount fact system retries times umount call retries performed file system virtual memory system document behavior behavior observed application memory-mapped file data feature supported virtual memory system read errors page accesses virtual memory system deliver sigsegv segmentation fault application instance propagate experiments showed case shared memory unlike linux processes sharing memory region operate independently error propagated processes accessed page disk access retried process accesses page propagate write retries failed umount error returned application stop read errors swapoff read errors page-in kernel thread stack cases result kernel panic conservative action pageout virtual memory system attempts free memory pages writing swap swapon swapoff pagetouch madvise complete pageout umount user data user stack shared mmapped user code rea err kernel stack user data user stack shared mmapped user code ite kernel stack user data user stack shared mmapped user code uptio kernel stack symbols propagate retry record stop experiment applicable comments kernel crashes stack sigsegv signal kernel panic memory page freed alternate victim chosen page-out upto retries write operation blocks error returned table freebsd recovery techniques table presents freebsd recovery techniques read write corruption errors combinations data type rows context columns comments provided tables freebsd read block swapon read pages madvise table space write errors occur page-out process freebsd virtual memory system recovers record case virtual memory system remembers write operation performed successfully memory page freed virtual memory system successfully free memory page proceeds select alternate victim page-out prevention techniques determining prevention policies difficult determining detection recovery policies prevention policy triggered disk fault methodology uncovering prevention policy specific test prevention technique remember technique triggered faults test remember injecting sticky error repeatedly disk block checking virtual memory system stops disk block workload performs iterations page-out page-in victim pages linux freebsd find bad disk block repeatedly spite returning error time results obtained read write errors linux freebsd track bad blocks remember test loadbalance causing virtual memory system page-out pages numerous times checking blocks swap area fairly evenly workload performs iterations pageout page-in victim pages linux freebsd disk blocks reused repeatedly blocks swap area written systems perform wear-leveling loadbalance finally detect reboot scan simply observe activities occur interval virtual memory system observe instance ofp reboot orp scan experiments infer linux freebsd techniques summary experiments linux freebsd prevention techniques windows section outlines features windows virtual memory system discusses failure handling policies windows file ntfs partition store memory pages paged-out failure handling 
policy extract combination policies ntfs virtual memory system windows paging user kernel memory inject faults user data pages read corruption errors injected pagetouch write errors injected pageout error code status device data error read write errors detection windows error code returned disk detect read write errors errorcode corruption errors detected recovery recovery read errors terminating user application reporting error inpageerror propagate recovery write errors involved primarily record memory pages error occurs written selected paging disk block error read back read succeeds half-block write performed read fails half-block read performed irrespective success failure halfblock operations block future writes record deal errors writes identify purpose half-block operations transient write error disk blocks subsequently successfully written read back application accesses data leading application receiving junk data bug handling write errors investigation required ascertain behavior prevention error injection experiments demonstrated disk block re-used errors block remember block added bad cluster file disk re-formatted failure handling approaches section discuss approaches current virtual memory systems adopt handle disk failures contrasting techniques identifying deficiencies systems compare approach virtual memory systems file systems explored start summarizing approaches virtual memory systems linux linux fails detect disk errors error codes returned simple recovery schemes deal detected errors respect corruption swap header corruption detected freebsd freebsd correctly detects disk errors error codes ignores corruption errors simple recovery schemes deal errors conservative linux cases kernel calls panic stop entire system read fails swapoff read affects single application windows windows detects disk errors error codes ignores corruption errors simple recovery schemes system observed prevention technique remember general systems suffer deficiencies simple recovery techniques virtual memory systems studied simple recovery techniques deal disk errors attempt techniques redundancy completely recover disk errors ignoring data corruption data corruption experiments case linux swap header detected virtual memory systems assume disks store data reliably true commodity hardware under-developed mechanisms aprimeexampleofan under-developed mechanism remembering bad blocks linux swap header provision store list bad blocks list effectively prevent data loss remember list initialized mkswap updated afterward errors occur hand windows actively updates bad cluster file avoid error-prone blocks memory abstraction mismatch applications expect pages behave memory virtual memory system maintain memory abstraction disk errors important part maintaining abstraction error reporting error handled system propagated manner fits memory abstraction linux thesigbussignal propagate page read errors definition hardware failures sigbus generated freebsd sigsegv signal intended programming error propagate read errors retries instances retrying operation error occurs retrying solve problem case transient error systems benefit greatly employing retries illogical inconsistency error recovery techniques employed inconsistent cases freebsd read error user data page result propagate case pagetouch results kernel panic swapoff buggy implementation observed linux failure handling code buggy result retry making useless suspect failure handling code rarely tested bugs security issues system fairly secure normal operation insecure partial failure linux data read back failed write disk block previous contents returned application possibly delivering data application authorized read failures dealt increasing awareness exploiting transient hardware errors attack systems kernel exposure systems special care kernel-mode data stored disk freebsd corruption kernel thread stack detected result undesirable crashes severe data corruption policies virtual memory systems compared file systems observe kinds systems share problems illogical inconsistency implementation bugs failure handling code points general disregard partial disk errors exposing commodity computer systems data loss data corruption inexplicable crashes linux virtual memory system file systems misses large number write errors virtual memory systems file systems deal corruption errors elegant manner file systems perform sanity checking deal corruption file system data structures protection user data data handled virtual memory systems part freebsd windows leverage important difference file systems virtual memory systems writes required succeed file systems virtual memory systems alternatives choosing page victim writing disk experience experimenting multiple systems helps compare systems insight advantages limitations methodology experience techniques simple applied systems tool rewritten environment find task onerous observed limitation easy identify source disk accesses accesses attributed error recovery unrelated problem occurs read error injected user data page freebsd windows observe retry read retry succeeds application terminated indicating bug retry code closer examination revealed read performed create core dump recover error interesting explore techniques identify exact source disk accesses future work related work techniques developed years inject faults systems techniques fault injection studies explore operating system behavior errors studies explore partial disk failures detail bring techniques policies operating system deal failures study similar spirit brown patterson study failure policies software raid systems software raids type context agnostic behavior virtual memory system differs considerably data types contexts requires complex fault injection analysis study related earlier studies file systems handle partial disk failures file systems virtual memory system important operating system components disks significantly applications aware disk store files disks virtual memory system completely transparent applications requiring virtual memory system robust disk failures conclusions commodity hardware increasingly unreliable due escalating complexity cost pressures operating systems longer assume hardware components hard disks work fail virtual memory system important subsystem modern operating system virtual memory systems designed deal partial disk failures fault injection experiments find current virtual memory systems employ consistent failure policies provide complete recovery partial failures improving failure-awareness systems enable virtualize memory providing applications robust memory abstraction acknowledgments meenali rungta helping experimental setup windows nitin agrawal comments paper anonymous reviewers thoughtful suggestions finally computer systems lab csl providing great environment research work sponsored nsf ccritr- cnsnetwork appliance emc freebsd operating system http freebsd journalling flash file system version http sourceware jffs anderson drive manufacturers typically don talk disk failures personal communication dave anderson seagate anderson dykes riedel interface scsi ata proceedings usenix symposium file storage technologies fast san francisco california april bartlett spainhower commercial fault tolerance tale systems ieee transactions dependable secure computing january barton czeck segall siewiorek fault injection experiments fiat ieee transactions computers april bovetandm cesati understanding linux kernel edition reilly december brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june candea kawamoto fujiki friedman fox microreboot technique cheap recovery proceedings symposium operating systems design implementation osdi pages san francisco california december chou yang chelf hallem engler empirical 
study operating system errors proceedings acm symposium operating systems principles sosp pages banff canada october corbett english goel grcanac kleiman leong sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april engler chen hallem chou chelf bugs deviant behavior general approach inferring errors systems code proceedings acm symposium operating systems principles sosp pages banff canada october govindavajhala appel memory errors attack virtual machine proceedings ieee symposium security privacy page washington usa gray van ingen empirical measurements disk failure rates error rates microsoft research technical report msrtr- december green eide controller flaws version http mindprod eideflaw html february kalbarczyk iyer yang characterization linux kernel behavior error proceedings international conference dependable systems networks dsnpages san francisco california june ishikawa nakajima oikawa hirotsu proactive operating system recovery poster session acm symposium operating systems principles sosp brighton united kingdom october johnson shasha low-overhead high performance buffer management replacement algorithm proceedings international conference large databases vldb pages santiago chile september kanawati kanawati abraham ferrari flexible software-based fault error injection system ieee transactions computing kari saikkonen lombardi detection defective media disks ieee international workshop defect fault tolerance vlsi systems pages venice italy october lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick neville-neil design implementation freebsd operating system addison-wesley professional august musuvathi park chou engler dill cmc pragmatic approach model checking real code proceedings symposium operating systems design implementation osdi boston massachusetts december prabhakaran arpaci-dusseau arpaci-dusseau model-based failure analysis journaling file systems proceedings international conference dependable systems networks dsnpages yokohama japan june prabhakaran bairavasundaram agrawal gunawi arpaci-dusseau arpaci-dusseau iron file systems proceedings acm symposium operating systems principles sosp pages brighton united kingdom october schwarz xin miller long hospodor disk scrubbing large archival storage systems proceedings annual meeting ieee international symposium modeling analysis simulation computer telecommunication systems mascots volendam netherlands october shah elerath reliability analysis disk drive failure mechanisms proceedings annual reliability maintainability symposium pages alexandria january swift bershad levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing lake george york october talagala patterson analysis error behaviour large storage system ieee workshop fault tolerance parallel distributed systems san juan puerto rico april data clinic hard disk failure http dataclinic hard-disk-failures htm tsai iyer measuring fault tolerance ftape fault injection tool international conference modeling techniques tools computer performance evaluation pages september tweedie journaling linux ext file system fourth annual linux expo durham north carolina wehman den haan enhanced ide fast-ata faq http thef-nym sci kun cgi-pieterh atazip atafq html 
semantically-smart disk systems past present future andrea arpaci-dusseau remzi arpaci-dusseau lakshmi bairavasundaram timothy denehy florentina popovici vijayan prabhakaran muthian sivathanu computer sciences department google wisconsin madison mountain view california abstract paper describe research on-going group past years semantically-smart disk systems semantically-smart system typical blockbased storage systems extracting higher-level information stream traffic disk enables interesting pieces functionality implemented lowlevel storage systems describe development efforts past years highlighting key technologies needed build semantically-smart systems main weaknesses approach discuss future directions design implementation smarter storage systems introduction past years group working ways increase functionality performance reliability security storage systems approach consistent build storage systems tomorrow living constraints real world notion design constraint major research thrusts group real world deal things world storage main constraints encounters presented interface storage typically disk raid presents linear array blocks clients block read written scsi good interface advantages primarily simple portable file systems direct clients storage access disk drives virtually complexity head positioning error handling details drive access hidden client high-level abstraction downsides lampson famously don hide power array-based interface storage preventing large number interesting pieces functionality implemented research suggests rotationally-aware disk schedulers greatly improve performance low-level information required perform scheduling hidden disk interface examples exist flavor desired functionality requires information higher-level system file system lowerlevel system disk natural solution problem simply change interface storage change fraught peril requiring broad industry consensus massive upheaval existing infrastructure embarked alternate required build storage systems tomorrow limitations today interfaces researchers including high road end building file systems awareness disk system underneath chose low road enhancing low-level storage systems knowledge file system motivating reason choosing storage system target innovation practical multi-billion dollar storage industry largely builds ships block-level storage systems paper describe work semantically-smart disk systems compared typical dumb storage device semantically-smart system knowledge file system data structures operations knowledge build interesting storage systems work focused improving performance reliability security storage systems applying techniques typical devices present development work past years discussing key pieces technology reflecting step led paths discuss technology filter industrial world utility semantically-smart techniques time surprising aspect work sound theoretical underpinnings theory practice required build correctly functioning semantically-smart disks rest paper organized section provide background describing work gray-box techniques present generation semantically-smart disk prototypes sections discuss importance theoretical framework section present future directions section conclude section background work semantically-smart disk systems finds roots earlier research gray-box techniques basic idea gray-box approach simple building component system great deal knowledge components system designed implemented taking advantage knowledge component building exploiting fact components black boxes inner-workings exploited knowledge components perfect white boxes dealing imperfect knowledge critical challenge leveraging gray-box techniques application scan set files application underlying operating system files cache application flexibility choose order access access cached files improves latency files cache accessed quickly bandwidth files accessed displace files cache operating system fetch disk problem arises operating system reveal information challenge advantage gray-box knowledge operating systems caches cache replacement policies well-known determine contents cache approaches earlier work demonstrated utility probing cache accessing blocks file timing long takes access application determine file present cache high probability subsequent work route learning behavior cache-replacement algorithm recencyor frequency-based history make replacement decisions application simulate replacement algorithm build accurate model contents cache initial line work targeting applicationos boundary began interface file systems block-level storage systems beneath clear simple interface storage benefits limited interesting optimizations enhancements functionality require information file system storage system information difficult layer storage stack information layer solution information gap change interface storage people advocating change years change problematic reasons broad industry consensus required instantiate change enormous disks raids add capabilities clients systems migrate benefits obvious chicken-and-egg problem changing successful interface requires anticipate usage scenarios good design team relevant situations account finally change expensive huge investments required enable make pervasive problem presented limited interface file systems storage explicit interface change unattractive variety reasons found wondering didn change interface built smarter storage systems learned inferred information file systems essence obtain benefits interface requiring change storage interface first-generation systems mindset began work effort semantically-smart storage systems published fast work major thrusts tool eof automatically infer data structures client file system set run-time techniques disk requires determine relevant pieces file system information collection case studies demonstrate utility semantic-awareness storage additional case study published isca discuss pieces turn describing challenges learned offline techniques extracting static information eof challenge simple disk raid system gain knowledge file system data structures approach simply embed static file system information disk raid system built-in knowledge file system data structures locations disk approach work initially felt approach limiting wondered automate process eof extraction file systems tool basic operation simple userlevel process host issues series disk requests disk in-disk agent monitors resultant traffic carefully controlling exact file operations issued file system eof infer great deal knowledge on-disk structures technique eof isolation combined patterns blocks written disk test data block inode identify fill data block pattern monitoring contents written blocks storage system detect data blocks disk workload inode data block written disk successfully isolate inode block block filled pattern manner eof acquire remarkable amount detailed information on-disk structures file system on-line techniques classification association operation inferencing eof realized important component semantically-smart disk system on-line inference specifically static knowledge disk system monitor current traffic make inferences state file system disk system block live dead make inference simply knowing static location bitmaps examine contents bitmap complicated section developed set basic on-line techniques semantically-smart disk systems garner type knowledge basic called direct classification technique disk system examines block address read write request determine type block read write directed inode region disk simply checking address sufficient determine block inodes slightly sophisticated form classification indirect classification technique examines contents blocks determine type block determine block holds directory contents typical unix file system examine inode points block call process monitoring inode contents inode snooping indirect blocks similarly identified technique call block association technique connect related blocks simple efficient manner data block read written inode block belongs table maps associations delivers information final technique term operation inferencing method semantically-smart disk infer higher-level operations invoked file system infer file creations deletions unix file system operations 
detected monitoring file system state observing change inode bitmap infer creation deletion file bit set creation bit deletion case studies basic infrastructure techniques place constructed set prototype semantically-smart disks demonstrate utility prototypes built software pseudo-device drivers mounted beneath real file systems cases mentioned utilized simulation explore idea case study discuss in-disk implementation track-aligned extents basic idea allocate files fit track avoiding costly track-switches file access performance improved disk-level implementation semantic knowledge file system structures influence file system placement track-aligned specifically marking blocks track boundaries allocated disk coerce file system allocating files proper manner resulting performance improvement noticeable case study focuses caching system x-ray infer contents cache monitoring stream traffic disk generates key insight x-ray time file read inode updated access time eventually flushed disk watching inode accesstime updates x-ray build coarse model cache x-ray cache job managing cache aiming exclusivity simulations show performance dramatically improved smarter second-level caching strategy x-ray employs final case study focus implementation journaling beneath non-journaling file system turned difficult case study implement block level semantically-smart disk infer file system transaction taking place group related updates occurring makes challenging file system behavior file systems fundamentally delay reorder filter operations disk disk difficult time decoding happened solution problem point simple mount file system synchronously guaranteeing updates reflected disk complete timely manner result disk implemented journaling benefits non-journaling file system case linux ext lessons learned year working project yielded interesting results infer on-disk structures automatically techniques developed eof developed numerous online techniques determine true state file system infer operations invoking finally case studies observed great potential semantically-smart disk systems enabling interesting storage functionality change file system initial work demonstrated numerous difficulties approach originally thought on-line techniques challenging develop understood asynchronous nature modern file systems greatly complicate on-line inference wished perform understood embedding static information data structures disk reasonable on-disk data structures tend evolve slowly file systems world work improve eof automatic data structure inference tools assuming semantically-smart ship built-in knowledge important file system structures surprised learn difficulties working underneath linux ext file system chose ext thought simplest operate underneath proved challenging primary reason hardship laissez faire manner ext writes blocks disk unlike unix-based file systems ext imposes ordering kind disk writes making semantic inference challenging discuss broadest conclusion work experience case studies clear case study learned lot technology needed build semantically-smart systems develop technology find interesting pieces storage functionality develop semantically-smart found ruminating possibilities bit functionality lucky found second-generation systems generation semantically-smart prototypes semantically-smart technology heights greatly increasing understanding systems work began limitations approach extremes pushed technology primary contribution work understanding operate file systems asynchronous operations correctness required generation semantically-smart systems comprised in-depth case studies d-graid raid array degrades gracefully faded secure-deleting disk operates asynchronous file systems removing major limitation earlier attempt secure delete discuss turn present lessons learned works d-graid degrading gracefully d-graid exploits semantic intelligence disk array place file system structures disks fault-contained manner unexpected failure disk occurs d-graid continues operate serving files accessed key techniques d-graid provide higher level availability technique replicate naming metadata structures file system high degree standard redundancy techniques data small amount overhead excess disk failures render entire array unavailable entire directory hierarchy traversed fraction files missing proportional number missing disks technique fault-isolated data placement ensure meaningful units data failure d-graid places semantically-related blocks blocks file storage array unit fault-containment disk observing natural failure boundaries found array failures make semantically-related groups blocks unavailable leaving rest file system intact fault-isolated data placement improves availability cost related blocks longer striped drives reducing parallelism found raid techniques remedy d-graid implements access-driven diffusion improve throughput frequently-accessed files copying blocks hot files drives system underneath linux ext determining blocks semantically-related challenging blocks dynamically typed block user-data block indirect-pointer block directory-data block order writes file system disk arbitrary result storage system accurately classify type block block filled indirect pointers identified observing inode due reordering behavior file system time disk writes inode indirect block block freed original inode reallocated file normal data block disk operations place memory reflected disk inference made semantic disk wrong due inherent staleness information d-graid deals uncertainty allowing fault-isolated placement file compromised limited amount time time bounded inode file written d-graid detect correct classification move block d-graid optimizations reduce number misclassifications checking contents indirect blocks valid number valid unique pointers null pointers slots non-null implemented d-graid ext vfat d-graid behaves desired analysis shows d-graid users access files additional disk failures occur raid naming meta-data replication percentage accessible files matches percentage working disks utilize process availability figure merit number processes run unaffected disk failure d-graid degrades expected linear drop-off processes access user files run successfully storage unavailable faded forgotten smarter storage systems understand blocks live dead investigated block liveness inferred semanticallysmart storage specifically explored difficult case infer generational liveness block belongs live file context implemented faded file-aware dataerasing disk implements secure delete ensuring deleted data recovered disk secure delete functionality pushes disk ability perform correct inferences false positive detecting delete leads irrevocable deletion valid data false negative results deleted data recoverable faded detects file deleted faded shreds blocks belonging file overwriting block multiple times specific patterns fact block shredded detected ways faded bit bitmap cleared indicating block freed generation count inode incremented indicating inode freed reallocated block pointed inode indicating block freed reallocated file challenge address reordering reuse file system block pointed inode faded definitively current contents block file faded deal uncertainty conservative converting apparent correctness problem performance problem faded perform shredding operations required mechanism introduce conservative overwrite erases past layers data block leaves current contents block intact conservative overwrites means valid data inadvertently shredded overhead suspicious blocks tracked shredded multiple times prototype implementation found minor needed ext operate correctly top faded ensures file truncates treated deletes ensures inability definitively classify indirect blocks lead missed deletes faded typical unix workload find implicit inferences conservative overwrites impose approximately overhead compared disk perfect information lessons learned implementing challenging case studies learned great deal semantically-smart disk systems fundamental challenges pose system designers important lesson living uncertainty core building systems due asynchronous nature file systems worst case disk system receives incomplete 
information state file system time learned imprecision interesting prototypes constructed careful design d-graid faded worked lack complete information achieved goals cases subtle reasoning required order build robust working prototypes handled corner cases times deep implementation realized problem approach requiring back drawing board rethink realized needed careful needed theory file systems disks interacted systems theory began effort build formal logic file system disk interactions logic began means reasoning semantically-smart disks realized possibilities broader logic file system developers understand complex interactions file systems disks logic begins set basic entities containers pointers generations file system simply collection containers linked pointers container reused freed represents generation logic formulated beliefs actions belief model state file system on-disk in-memory action state file system beliefs true time fundamental understanding impact actions beliefs ordering actions special care constructing temporal relationship actions proofs finally constructed starting basic axioms applying series event sequence substitutions implies observe simply replace subsequence initial results prove correctness existing file system consistencymaintenance techniques soft updates show linux ext file system needlessly conservative performs transaction commit demonstrating logic enable aggressive performance optimizations show logic aid development functionality building analyzing correctness consistent undelete functionality linux found simple logical framework critical development semantic technology reasoning disk interaction required formal approach required build robust correct systems future directions semantic disks project learned great deal file systems disk systems interactions harness experience forward ruminate future semantic disk technology block-level storage primary question semantic techniques applicability real world case studies complex industry fundamentally conservative adopt approach successful industry adoption aimed radical case studies imagine disk array performed smarter prefetching paying attention file boundaries requires semantic knowledge require wrong performance suffer question semantic inference applied clients disk systems database management systems performed initial work lines met mixed success techniques translate readily complex specific data structures typical dbms complicate matters occasionally dbms structures ripe kind reverse engineering advocate transaction log replete information dbms candidate future semantic technology lines noticed sea change modern file systems journaling make semantic inference easier difficult dbms file system journal takes chaotic update sequence simple file system ext turns orderly understandable affair linux ext perfect file system study semantically-smart disks underneath pushed deal extreme asynchrony arbitrary ordering writes future systems interpret log contents simpler easily verified correct major change storage interface object-based disks horizon change semantic inference obviated drives generally information clients typical block-based disks straight oneto-one file-to-object mapping drive easily determine blocks free evolved interface room inference semantic technology directory structure part interface journaling file systems databases place logs disk structures require semantic inference valuable sources information storage systems finally broader place semantic inference technology simply building storage systems current work explores low-level tracing fault injection understand file system performance failure characteristics systems grow increasingly complex tools deconstruct behavior integral part design implementation maintenance systems conclusions presented retrospective work semantically-smart disk systems work began simple question smart make block-level disks changing disk interface evolved development series increasingly challenging case studies beginnings formal theory understanding file system disk interactions modern world avoiding constraints layering system structuring artifacts impossible semantic inference provided means reclaim lost nature designs anderson osd drives snia events past developer dba snia osd pdf arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca pages munich germany june burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey california june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june denehy arpaci-dusseau arpaci-dusseau journalguided resynchronization software raid proceedings usenix symposium file storage technologies fast pages san francisco california december denehy bent popovici arpaci-dusseau arpaci-dusseau deconstructing storage arrays proceedings international conference architectural support programming languages operating systems asplos pages boston massachusetts october ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december ganger patt metadata update performance file systems proceedings symposium operating systems design implementation osdi pages monterey california november ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement proceedings twenty-sixth annual hawaii international conference system sciences volume pages gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon gray computers stop international conference reliability distributed databases june gunawi agrawal arpaci-dusseau arpaci-dusseau schindler deconstructing commodity storage clusters proceedings annual international symposium computer architecture isca pages madison wisconsin june jacobson wilkes disk scheduling algorithms based rotational position technical report hpl-csp- hewlett packard laboratories lampson hints computer system design proceedings acm symposium operating system principles sosp pages bretton woods hampshire october lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october nugent arpaci-dusseau arpaci-dusseau controlling place file system gray-box techniques proceedings usenix annual technical conference usenix pages san antonio texas june prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems proceedings usenix annual technical conference usenix pages anaheim california april prabhakaran arpaci-dusseau arpaci-dusseau modelbased failure analysis journaling file systems proceedings international conference dependable systems networks dsnpages yokohama japan june prabhakaran bairavasundaram agrawal gunawi arpaci-dusseau arpaci-dusseau iron file systems proceedings acm symposium operating systems principles sosp pages brighton united kingdom october ridge field book scsi starch june schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november schindler griffin lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix symposium file storage technologies fast monterey california january schindler schlosser shao ailamaki ganger atropos disk array volume manager orchestrated disks proceedings usenix symposium file storage technologies fast san francisco california april seltzer chen ousterhout disk scheduling revisited proceedings usenix winter technical conference usenix winter pages washington january sivathanu arpaci-dusseau arpaci-dusseau jha logic file systems proceedings usenix symposium file storage technologies fast pages san francisco california december sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau database-aware semantically-smart storage proceedings usenix symposium file storage 
technologies fast pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpacidusseau improving storage system availability d-graid proceedings usenix symposium file storage technologies fast pages san francisco california april sivathanu prabhakaran popovici denehy arpacidusseau arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california april talagala arpaci-dusseau patterson microbenchmarkbased extraction local global disk characteristics technical report csd- california berkeley wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans louisiana february wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october 
logic file systems muthian sivathanu andrea arpaci-dusseau remzi arpaci-dusseau somesh jha google computer sciences department wisconsin madison muthian google fdusseau remzi jhag wisc abstract years innovation systems highly successful improving performance functionality cost complicating interaction disk variety techniques exist ensure consistency integrity system data precise set correctness guarantees provided technique unclear making hard compare reason absence formal framework hampered detailed veri cation system correctness present logical framework modeling interaction system storage system show apply logic represent prove correctness properties demonstrate logic main bene enables reasoning existing system mechanisms allowing developers employ aggressive performance optimizations fear compromising correctness logic simpli introduction adoption system functionality facilitating rigorous proof correctness finally logic helps reason smart storage systems track semantic information system key aspect logic enables incremental modeling signi cantly reducing barrier entry terms actual system designers general framework transforms hitherto esoteric error-prone art system design readily understandable formally veri process introduction reliable data storage cornerstone modern computer systems file systems responsible managing persistent data essential ensure function correctly modern systems evolved extremely complex pieces software incorporating sophisticated performance optimizations features disk key bottleneck system performance optimizations aim minimizing disk access cost complicating interaction system storage system early systems adopted simple update policies easy reason modern systems signi cantly complex interaction disk stemming asynchrony updates metadata work wisconsin-madison reasoning interaction system disk paramount ensuring system corrupts loses data complex update policies precise set guarantees system obscured reasoning behavior translates manual intuitive exploration scenarios developers hoc exploration arduous possibly error-prone recent work found major correctness errors widely systems ext reiserfs jfs paper present formal logic modeling interaction system disk formal modeling show reasoning system correctness simple foolproof formal model illustrated existence similar frameworks areas correctness paramount existing models authentication protocols database reliability database recovery examples general theories modeling concurrent systems exist frameworks general model systems effectively domainspeci logic greatly simpli modeling logic systems serves important purposes enables prove properties existing system designs resulting understanding set guarantees enabling aggressive performance optimizations preserve guarantees signi cantly lowers barrier providing mechanisms functionality system enabling rigorous reasoning correctness absence framework designers tend stick time-tested alternatives finally logic helps design functionality class storage systems facilitating precise characterization proof properties key goal logic framework simplicity order general system designers barrier entry terms applying logic low logic achieves enabling incremental modeling complete model system starting logic simply model piece functionality mechanism isolation prove properties case studies demonstrate utility cacy logic reasoning system correctness properties represent prove soundness important guarantees provided existing techniques system consistency soft updates journaling logic prove linux ext system needlessly conservative transaction commits resulting sub-optimal performance case study demonstrates utility logic enabling aggressive performance optimizations illustrate utility logic developing system functionality propose system mechanism called generation pointers enable consistent undelete les prove correctness design incremental modeling mechanism logic demonstrating simplicity process implement mechanism linux ext system verify correctness logic empirically show inconsistency occur undeletes absence mechanism rest paper organized rst present extended motivation background systems present basic entities logic formalism represent common system properties logic logic prove consistency properties existing systems prove correctness unexploited performance optimization ext reason technique consistent undeletes apply logic semantic disks finally present related work conclude extended motivation systematic framework reasoning interaction system disk multifarious benets describe key applications framework reasoning existing systems important usage scenario logic model existing systems key bene modeling enables clear understanding precise guarantees mechanism assumptions guarantees hold understanding enables correct implementation functionality system layers disk system ensuring adversely interact system assumptions write-back caching disks results reordering writes media negate assumptions journaling based logic enables aggressive performance optimizations reasoning complex interactions hard system developers tend conservative perform unnecessarily waits logic helps remove barrier enabling developers aggressive performance optimizations con dent correctness section analyze real opportunity optimization linux ext system show logic framework prove correctness nal bene logic framework potential implementation-level model checkers clear model expected behavior validate existing system enable comprehensive cient model checking current technique relying fsck mechanism expensive cost fsck explored state limits scalability model checking building system functionality recovery consistency traditionally viewed tricky issues reason classic illustration view arises database recovery widely aries algorithm pointed correctness issues earlier proposals ironically success aries stalled innovation database recovery due dif culty proving correctness techniques innovation system deals interaction disk correctness implications inertia changing time-tested alternatives sti incorporation functionality systems systematic framework reason piece functionality greatly reduce barrier entry section propose system functionality logic prove correctness illustrate cacy logic reasoning functionality examine section common system feature journaling show starting simple logical model journaling systematically arrive corner cases handled involve complex interactions developers linux ext designing semantically-smart disks logic framework signi cantly simpli reasoning class storage systems called semantically-smart disk systems provide enhanced functionality inferring system operations inferring information accurately underneath modern systems complex dependent dynamic system properties section show logic simplify reasoning semantic disk turn enable aggressive functionality background system organizes disk blocks logical les directories order map blocks logical entities les system tracks forms metadata section rst describe forms metadata systems track discuss issue system consistency finally describe asynchrony systems major source complexity interaction disk file system metadata file system metadata classi types directories directories map logical perle metadata mapped directory directories enable hierarchy les user opens path system locates perle metadata reading directory path required file metadata file metadata information speci examples information set disk blocks comprise size systems fat metadata embedded directory entries systems metadata stored separately inodes pointed directory entries pointers metadata disk blocks indirected indirect pointer blocks case large les allocation structures file systems manage resources disk set free blocks allocated les track resources systems maintain structures bitmaps free lists point free resource instances addition systems track metadata super block focus types file system consistency proper operation internal metadata system data blocks consistent state metadata consistency state metadata structures obeys set invariants system relies directory entry point valid metadata structure directory points metadata uninitialized marked free system inconsistent systems provide metadata consistency crucial correct operation stronger form consistency data consistency system guarantees data block contents correspond metadata structures point discuss issue section modern systems linux ext 
reiserfs provide data consistency file system asynchrony important characteristic modern systems asynchrony exhibit updates data metadata updates simply buffered memory written disk delay interval reordering writes asynchrony crucial performance complicates consistency management due asynchrony system crash leads state arbitrary subset updates applied disk potentially leading inconsistent on-disk state asynchrony updates principal reason complexity interaction system disk raison etre logic basic entities notations section basic entities constitute system logic present notations section build entities present formalism operation system basic entities basic entities model containers pointers generations system simply collection containers containers linked pointers system differs exact types containers nes relationship container types abstraction based containers pointers general describe system containers system freed reused container considered free pointed container live instance container reuse free called generation generation speci incarnation container generations reused container reused previous generation container freed generation container life generation fully ned container logical generation number tracks times container reused note generation refer contents container abstraction current incarnation contents change affecting generation illustrate notion containers generations simple typical unix-based system system xed set designated inodes inode slot container point inode slot inode generation corresponds speci deleted inode generation deleted forever inode container simply marked free created reuse inode container logically inode generation note single container inode point multiple containers data blocks single container pointed multiple containers hard links unix systems notations notations depict basic entities relationships listed table note notations table ned section containers denoted upper case letters generations denoted lower case letters entity description represents container generation symbol description set entities point container set entities pointed container jaj container tracks container live set entities point generation set entities pointed generation denotes container pointer denotes entity points kth epoch container type kth epoch container generation kth epoch container container generation generation container table notations containers generations pointer denoted symbol container pointer container paper pointers containers live section relax assumption introduce notation pointers involving dead containers attributes containers make logic expressive modern systems extend vocabulary attributes container generation attributes container epoch epoch container ned time contents container change memory epoch incremented system sets elds inode step results epoch inode container system batch multiple contents due buffering set epochs visible disk subset total set epochs container denote epoch superscript notation denotes kth epoch note nition epoch expressivity logic imply system tracks epoch note distinction epoch generation generation change occurs reuse container epoch change contents container reused type containers type type container static change lifetime system dynamic container belong types points time ffsbased systems inode containers statically typed block containers change type data directory indirect pointers denote type container notation shared unshared container pointed container called shared container container pointer leading unshared default assume containers shared denote unshared containers operator unshared note unshared property container type system ensures container belonging type unshared pointer pointing systems designate data block containers unshared memory disk versions containers system manage structures domains volatile memory disk accessing contents container system read on-disk version container memory subsequently system makes modi cations inmemory copy container modi contents periodically written disk system writes modi container disk contents container memory disk formalism present formal model operation system rst formulate logic terms beliefs actions introduce operators logic proof system basic axioms logic beliefs state system modeled beliefs belief represents state memory disk statement enclosed represents belief beliefs memory beliefs disk beliefs denoted fgm fgd bgm belief system memory container points memory bgd means disk belief timing belief begins hold determined context formula logic describe subsection terms timing belief ned relative beliefs actions speci formula isolated belief temporal dimension memory beliefs represent state system tracks memory on-disk beliefs ned belief holds disk time crash system conclude belief purely based scan on-disk state time ondisk beliefs solely dependent on-disk data system manages free reuse containers beliefs terms generations fak bjgm valid note refers generation container on-disk beliefs deal containers generation information lost disk sections propose techniques expose generation information disk show enables improved guarantees actions component logic actions result system state actions alter set beliefs hold time actions ned logic read operation system read contents on-disk container current generation memory system container memory modify read contents memory on-disk fagm fagd write operation results ushing current contents container disk operation contents memory on-disk fagd fagm ordering beliefs actions fundamental aspect interaction system disk ordering actions ordering actions determines order beliefs established order actions resulting beliefs operators means occurred time note ordering beliefs notation indicating event creation belief state existence belief belief agm represents event system assigns pointers special ordering operator called precedes belief left operator operator ned means belief occurs means belief holds occurs implies intermediate action event invalidates belief operator transitive imply belief hold necessarily note simply shortcut note implies beliefs grouped parentheses semantics precedes group beliefs precedes belief belief parentheses precedes belief proof system primitives sequencing beliefs actions rules formulas logic terms implication event sequence sequence traditional operators implication double implication logical combine sequences logical rule notation means time event action occurs event occurs point occurrence rule occurs absolute time occur order occurs rule valid occurred general left hand side rule involves complex expression disjunction components belief rhs holds point occurrence rst event makes lhs true occurrence makes sequence true rule rule denotes time occurs occurred note rule event occurs sides event constitutes temporal point referring time instant lhs rhs temporal interpretation identical events crucial rule serving intended implication rhs refer instant rules logical proofs event sequence substitution rule subsequence occurs sequence events logically implies event apply rule event sequence replacing subsequence matches left half rule half rule postulate proof system enables deriving invariants system building basic axioms basic axioms subsection present axioms govern transition beliefs memory disk container points memory current generation points memory fbx agm agm points memory write lead disk belief points agm write agd converse states disk belief implies belief rst occurred memory agd agm agd similarly points disk read result system inheriting belief agd read agm on-disk contents container pertain epoch generation pointed generation memory write converse holds faygd write faygd akgm write faygd note refers generation rule generation 
points akg ajg hold memory points time container freed instants akgm ajgm akgm ajgm note rule includes scenario intermediate generation occurs container pointed disk subsequently system removes pointer memory write lead disk belief point agd bgm write bgd unshared container write lead disk belief container points free agd write dynamically typed container type instants freed xgm ygm xgm ygm completeness notations notations discussed section cover wide range set behaviors model system means complete set notations model aspect system show section section speci system features require notations main contribution paper lies putting framework formally reason system correctness notations introduced speci system features framework apply modi cation connections temporal logic logic bears similarity linear temporal logic syntax linear temporal logic ltl dened formula ltl formula set atomic propositions ltl formulas ltl formulas nition time future release temporal operators formalism fragment ltl set atomic propositions consists memory disk beliefs actions temporal operators allowed formalism equivalent execution sequence states ltl formula denotes true execution system satis ltl formula executions satisfy precise semantics satisfaction relation meaning found chapter semantics formalism standard semantics ltl proof system set axioms section desired property data consistency property section prove axioms denoted system satis properties set satisfy property file system properties systems provide guarantees update behavior guarantee translates rules logical model system complement basic rules reasoning system section discuss properties container exclusivity system exhibits container exclusivity guarantees on-disk container dirty copy container contents system cache requires system ensure in-memory contents container change container written disk systems bsd ffs linux ext vfat exhibit container exclusivity journaling systems ext exhibit property equations refer containers memory refer latest epoch container memory case systems obey container exclusivity means time container latest epoch memory points similarly write means latest epoch time written referring speci version epoch notation container exclusivity holds epoch container exists memory container exclusivity stronger converse agd agm agd assume unshared stronger equation equation disk belief agd hold written system note containers typical systems data blocks unshared agd agm write agd reuse ordering system exhibits reuse ordering ensures reusing container commits freed state container disk pointed generation memory freed generation made point freed state container generation pointer removed written disk reuse occurs agm agm write agm reuse results commit freed state extend rule agm agm write agm ffs soft updates linux ext examples systems exhibit reuse ordering pointer ordering system exhibits pointer ordering ensures writing container disk system writes containers pointed agm write agm write write ffs soft updates system exhibits pointer ordering modeling existing systems ned basic formalism logic proceed logic model reason system behaviors section present proofs properties important system consistency discuss data consistency problem system model journaling system reason non-rollback property journaling system data consistency rst problem data consistency system crash data consistency contents data block containers consistent metadata data blocks words end data system recovers crash assume metadata container pointers data blocks respective data block container disk belief points holds on-disk contents written generation epoch pointed time past kth generation memory generation rule summarizes fbx agd faygd fbx akgm fbx agd prove system exhibits reuse ordering pointer ordering suffers data consistency violation show system obey ordering data consistency compromised crashes simplicity make assumption data containers system nonshared les share data block pointers assume system obeys container exclusivity property modern systems ext vfat properties block exclusivity fbx agd fbx agm fbx agd rewrite rule fbx akgm fbx agd faygd rule hold means represented generation points generation contents written generation case data corruption show rule hold assume negation prove reachable sequence valid system actions faygd write event sequences implied lhs fbx akgm fbx agd write order prove prove interleaving sequences clause invalid disprove prove interleavings valid fbx akgm fbx agd event occur events due container exclusivity unshared similarly fbx akgm occur write interleavings fbx akgm fbx agd write write fbx akgm fbx agd case applying akgm fbx agd write applying akgm fbx agd write step valid sequence system execution generation freed due delete represented generation subsequent generation block reallocated represented generation memory shown violation occur assume system obeys reuse ordering equation additional constraint equation imply akgm fbx agd write write akgm fbx agd write facgd contradiction initial assumption started bgd reuse ordering shown scenario arise case write fbx akgm fbx agd applying write akgm fbx agd eqn write akgm fbx agd valid system sequence generation pointed data block generation generation deleted generation container assigned generation consistency violation occur scenario interestingly apply write write akgm fbx agd apply case belief agd hold rule led belief immediately write belief overwritten fbx agd sequence invalidate sequence reuse ordering guarantee data consistency case make assumption system obeys pointer ordering assume unshared container exclusivity holds apply equation write akgm write fbx agd applying pointer ordering rule eqn write akgm write write fbx agd agm write faygd write fbx agd faygd fbx agd contradiction implies contents disk belong generation started assumption reuse ordering pointer ordering system suffers data consistency violation system obey ordering ext data consistency compromised crashes note inconsistency fundamental xed scan-based consistency tools fsck veri inconsistency occurs practice reproduce case experimentally ext system modeling system journaling extend logic rules behavior journaling system model reason key property journaling system journaling technique commonly systems ensure metadata consistency single system operation spans multiple metadata structures system groups transaction guarantees transaction commits atomically preserving consistency provide atomicity system rst writes writeahead log wal propagates actual on-disk location transaction committed log transaction committed logged special commit record written log indicating completion transaction system recovers crash checkpointing process replays belong committed transactions model journaling logical transaction object determines set log record containers belong transaction logically pointers log copies containers modi transaction denote log copy journaled container symbol top container container log journal system note assume physical logging block-level logging ext physical realization transaction object commit record logically points containers changed transaction wal property hold commit container written log copy modi containers transaction points written commit container wal property leads rules axgm write axgm write write axgm write axgm write write rst rule states transaction committed commit record written containers belonging transaction 
written disk rule states on-disk home copy container written transaction container modi committed disk note unlike normal pointers considered point containers generations pointers container rules point epochs epoch pointers commit record speci epoch snapshot container replay checkpointing process depicted rules axgd ftgd write faxgd axgd aygd write faygd rst rule container part transaction transaction committed disk on-disk copy container updated logged copy pertaining transaction rule container part multiple committed transactions on-disk copy container updated copy pertaining transactions belief transitions hold bxgm fbx agm write fbx agd axgm write faxgd rule states points belongs transaction commit leads disk belief fbx agd rule disk belief faxgd holds immediately commit transaction part creation belief require checkpoint write happen disk belief pertains belief system reach start current disk state journaling systems containers types journaled updates containers directly disk transaction machinery proofs cases complete journaling containers journaled selective journaling containers type selective case address possibility container changing type journaled type non-journaled type vice versa container belongs journaling type converse equation fbx agd bxgm fbx agm write fbx agd show complete journaling data inconsistency occurs omit due space constraints non-rollback property introduce property called non-rollback pertinent system consistency rst formally property reason conditions required hold journaling system non-rollback property states contents container disk overwritten older contents previous epoch property expressed faxgd faygd faxgm faygm rule states on-disk contents move epoch logically imply epoch occurred epoch memory non-rollback property crucial journaling systems absence property lead data corruption proof logically derive corner cases handled property hold show journal revoke records effectively ensure disk believes xth epoch possibilities type journaled type belonged transaction disk observed commit record transaction belief faxgd occurs immediately commit point actual contents written system part checkpoint propagation actual on-disk location re-establishing belief faxgd set journaled types faxgd jgm faxgm axgm write faxgd write faxgd possibility type journaled case disk learnt prior commit faxgd jgm faxgm write faxgd journaled rst assume belong journaled type prove non-rollback property lhs faxgd faygd journaled sequence events led beliefs faxgm axgm write faxgd write faxgd faygm aygm write faygd write faygd omitting write actions sequences simplicity sequences events faxgm faxgd faxgd faygm faygd faygd note sequence instances disk belief created rst instance created transaction committed instance checkpoint propagation time snapshot-based coarse-grained journaling systems ext transactions committed order epoch occurred committed rst instance faxgd occur rst instance faygd property true journaling checkpointing in-order committed transactions copies data version pertaining transaction propagated checkpoint sequences events lead interleavings depending epoch occurs epoch vice versa ordering epoch xed rest events constrained single sequence interleaving faxgm faygm faxgd faygd faygd faxgm faygm interleaving faygm faxgm faygd faxgd faxgd faygd faxgd interleaving results contradiction initial statement started faxgd faygd rst interleaving legal sequences events combined rst interleaving implies faxgm faygm proved epochs journaled non-rollback property holds journaled case type epochs belongs journaled type start statement faxgd faygd equations sequences events faygm aygm write faygd write faygd faxgm write faxgd omitting write actions sake readability sequences faygm faygd faygd faxgm faxgd prove non-rollback property show interleaving sequences faygm faxgm results contradiction co-exist faxgd faygd interleavings faygm faxgm faygm faxgm faxgd faygd faygd faygm faygd faxgm faxgd faygd faygm faygd faygd faxgm faxgd faygm faxgm faygd faxgd faygd faygm faxgm faygd faygd faxgd faygm faygd faxgm faygd faxgd scenarios imply faygd faxgd invalid interleavings scenarios valid interleavings contradict initial assumption disk beliefs time imply faygm faxgm scenarios violate non-rollback property dynamic typing journaling mechanism guarantee nonrollback due violation contents corrupted stale metadata generations scenario occur checkpoint propagation earlier epoch journaled occurs overwritten non-journaled epoch prevent impose checkpoint propagation container context transaction happen on-disk contents container updated commit journal revoke records ext precisely guarantee revoke record encountered log replay pre-scan log block propagated actual disk location scenario epoch committed disk transaction modi earlier epoch committed prevent form reuse ordering imposes container type reused memory transaction freed previous generation committed transactions commit order freeing transaction occur transaction guarantee jgm jgm faygm faxgm faygm write faxgm rule scenario handled revoke record solution properties non-rollback property holds redundant synchrony ext examine performance problem ext system transaction commit procedure arti cially limits parallelism due redundant synchrony disk writes ordered mode ext guarantees newly created point stale data blocks crash ext ensures guarantee ordering commit procedure transaction committed ext rst writes disk data blocks allocated transaction waits writes complete writes journal blocks disk waits complete writes commit block inode container data block container transaction commit container commit procedure ext expressed equation fix fkgm ixgm write fix fkgm ixgm write write write examine condition ensure no-stale-data guarantee rst formally depict guarantee ext ordered mode seeks provide equation fix fkgm fix fgd ffygd fix fgd equation states disk acquires belief fix contents data container disk pertain generation pointed memory note ext obeys reuse ordering ordered mode guarantee cater case free data block container allocated prove equation examining conditions hold equation true lhs equation fix fkgm fix fgd applying equation fix fkgm ixgm write fix fgd applying equation fix fkgm ixgm write write write fix fgd equation fix fkgm ixgm ygd write write fix fgd ygd fix fgd current ext commit procedure equation guarantees no-stale-data property waits procedure required reorder actions write write fix fkgm ixgm write write write fix fgd applying equation ygd fix fgd ordering actions write write inconsequential guarantee ext ordered mode attempts provide conclude wait ext employs write data blocks redundant unnecessarily limits parallelism data journal writes severe performance implications settings log stored separate disk illustrated previous work speci points general problem system design developers rigorous frameworks reason correctness tend conservative conservatism translates unexploited opportunities performance optimization systematic framework enables aggressive optimizations ensuring correctness support consistent undelete section demonstrate logic enables quickly formulate prove properties system features mechanisms explore functionality traditionally considered part core system design ability undelete deleted les consistency guarantees ability recover deleted les demonstrated large number tools purpose tools rebuild deleted les scavenging on-disk metadata extent systems freed metadata containers simply marked free unix system block pointers deleted inode blocks belong deleted existing tools undelete guarantee consistency assert recovered contents 
valid undelete fundamentally best-effort les recovered blocks subsequently reused user trustworthy recovered contents demonstrate logic existing systems consistent undelete impossible provide simple solution prove solution guarantees consistent undelete finally present implementation solution ext undelete existing systems model undelete logic express pointers containers holding dead generation introduce notation pointer call dead pointer operator container denotes set dead live entities pointing container undel undelete action container undelete process summarized equation undel fbx agd fbggd fbx agd fby agd words dead free container points disk container alive dead pointing undelete makes generation live makes point guarantee hold consistency dead pointer brought alive ondisk contents time pointer brought alive correspond generation epoch originally pointed memory similar data consistency formulation fbx akgm fbx agd fby agd fbx agd fazgd note clause required lhs cover case generation brought life true undelete show guarantee hold necessarily negation rhs fazgd show condition co-exist conditions required undelete equation words show undel fbx agd fbggd fazgd arise valid system execution utilize implications proof fbx agd fbx akgm write fazgd write interleaving event sequences write fbx akgm write valid system sequence represented generation points written disk block freed killing generation generation allocated generation deleted written disk disk beliefs fbx agd fazgd initial state disk sequence simultaneously lead disk belief fbggd shown conditions fbx agd fbggd fazgd hold simultaneously undelete point lead violation consistency guarantee associate stale generation undeleted shown reuse ordering pointer ordering guarantee consistency case undelete generation pointers propose notion generation pointers show pointers consistent undelete guaranteed assumed pointers disk point containers discussed section pointer pointed speci generation leads set system properties implement generation pointers on-disk container generation number incremented time container reused addition on-disk pointer embed generation number addition container generation pointers on-disk contents container implicitly generation fbkgd valid belief means disk contents belong generation generation pointers criterion undelete undel fbx akgd fakgd fbx akgd fby akgd introduce additional constraint fazgd left hand side equation previous subsection fbx akgd fakgd fazgd denote on-disk container holds generation number fahgd equation fbx akgd fakgd fahgd contradiction means ondisk container generations simultaneously undelete occur scenario alternatively agged inconsistent undeletes occurring generation pointers consistent implementation undelete ext proof consistent undelete implemented generation pointer mechanism linux ext block generation number incremented time block reused generation numbers maintained separate set blocks ensuring atomic commit generation number block data straightforward data journaling mode ext simply add generation update create transaction block pointers inode extended generation number block implemented tool undelete scans on-disk structures restoring les undeleted consistently speci cally restored generation information metadata block pointers match block generation data blocks ran simple microbenchmark creating deleting directories linux kernel source tree observed roughly deleted les les roughly detected inconsistent undeletable remaining les successfully undeleted illustrates scenario proved section occurs practice undelete tool generation information wrongly restore les corrupt misleading data application semantic disks interesting application logic framework systems enables reasoning recently proposed class storage systems called semanticallysmart disk systems sds sds exploits system information storage system provide functionality admitted authors reasoning correctness knowledge tracked semantic disk hard formalism memory disk beliefs sds model extra system state tracked sds essentially disk belief section rst logic explore feasibility tracking block type semantic disk show usage generation pointers system simpli information tracking sds block typing important piece information required semantic disk type disk container identifying type statically-typed containers straightforward dynamically typed containers hard deal type dynamically typed container determined contents parent container indirect pointer block identi observing parent inode block indirect pointer eld tracking dynamically typed containers requires correlating type information typedetermining parent information interpret contents dynamic container accurate type detection sds guarantee hold kgd kgm words disk interprets contents epoch belonging type contents belonged type memory guarantees disk wrongly interpret contents normal data block container indirect block container note equation impose guarantee disk identies type container states association type contents correct prove rst state algorithm disk arrives belief type sds snoops metadata traf type-determining containers inodes container written observes pointers container concludes type pointers assume pointer type points container disk examines container written time freed interprets current contents belonging type written time contents type equation kgd fby agd faxgd words interpret belonging type disk container points current on-disk epoch type function abstracts indication disk contents epoch order associate contents type explore logical events led components side equation applying fby agd fby agm fby agd fby agm kgm fby agd similarly component faxgd faxgd write faxgd verify guarantee equation assume hold observe leads valid scenario add clause jgm equation equation prove fby agd faxgd jgm event sequences fby agm kgm fby agd jgm write type epoch unique write container implies type jgm write jgm write sequences interleaved ways epoch occurs epoch kgm interleaving fby agm kgm fby agd jgm write fby agm kgm fby agd jgm write valid sequence container freed disk acquired belieffb agand version written actual type changed memory leading incorrect interpretation belonging type order prevent scenario simply reuse ordering rule rule sequence imply fby agm kgm fby agd write jgm write fby agm kgm fby agd jgm write written disk treating free wrongly associate type interleaving proceeding similarly interleaving epoch occurs assigned type arrive sequence jgm write fby agm kgm fby agd simply applying reuse ordering rule prevent sequence stronger form reuse ordering freed state includes containers pointed allocation structure jaj tracking liveness rule sequence jgm write write jaj fby agm kgm fby agd add behavior sds states sds observes allocation structure indicating free inherits belief free write jaj applying sds operation eqn jgm write fby agm kgm fby agd sequence sds observe write treated free associate type subsequently written shown sds accurately track dynamic type underneath system ordering guarantees shown system exhibits strong form reuse ordering dynamic type detection made reliable sds utility generation pointers subsection explore utility systemlevel generation pointers context sds illustrate utility show tracking dynamic type sds straightforward system tracks generation pointers generation pointers equation kgd fby aggd faggd causal event sequences explored previous subsection fby aggm kgm fby aggd jgm write sequences imply generation types violates rule straightaway arrive contradiction proves violation 
rule occur related work previous work recognized modeling complex systems formal frameworks order facilitate proving correctness properties logical framework reasoning authentication protocols proposed burrows related work spirit paper authors formulate domain-speci logic proof system authentication showing protocols veri simple logical derivations domain-speci formal models exist areas database recovery database reliability body related work involves generic frameworks modeling computer systems wellknown tla framework automaton framework frameworks general model complex systems generality curse modeling aspects system extent paper tedious generic framework tailoring framework domain-speci knowledge makes simpler reason properties framework signi cantly lowering barrier entry terms adopting framework speci cations proofs logic lines contrast thousands lines tla speci cations automated theorem-proving model checkers bene generic framework tla previous work explored veri cation correctness system implementations recent body work model checking verify implementations body work complementary logic framework logic framework build model invariants hold model implementation veri finally system properties listed section identi previous work soft updates recent work semantic disks conclusions dependability computer systems important essential systematic formal frameworks verify reason correctness systems critical component system dependability formal veri cation correctness largely making systems vulnerable hidden errors absence formal framework sti innovation skepticism correctness proposals proclivity stick time-tested alternatives paper step bridging gap system design showing logical framework substantially simplify systematize process verifying system correctness acknowledgements lakshmi bairavasundaram nathan burnett timothy denehy rajasekar krishnamurthy florentina popovici vijayan prabhakaran vinod yegneswaran comments earlier drafts paper anonymous reviewers excellent feedback comments greatly improved paper work sponsored nsf ccrccr- ccrngs- itribm network appliance emc attie lynch dynamic input output automata formal model dynamic systems acm podc jfs overview ibm developerworks library jfs html bjorner browne colon finkbeiner manna sipma uribe verifying temporal properties reactive systems step tutorial formal methods system design fmsd burrows abadi needham logic authentication acm sosp pages clarke grumberg peled model checking mit press ganger mckusick soules patt soft updates solution metadata update problem file systems acm tocs hadzilacos theory reliability database systems acm hagmann reimplementing cedar file system logging group commit sosp nov kuo model veri cation data manager based aries acm trans database systems lamport temporal logic actions acm trans program lang syst mckusick joy lef fabry fast file system unix acm transactions computer systems august mogul update policy usenix summer boston june mohan haderle lindsay pirahesh aries transaction recovery method supporting finegranularity locking artial rollbacks write-ahead logging acm tods march musuvathi park chou engler dill cmc pragmatic approach model checking real code osdi dec pnueli temporal semantics concurrent programs theoretical computer science tcs prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems usenix r-undelete r-undelete file recovery software http undelete reiser reiserfs namesys restorer restorer data recovery software http bitmart net sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level osdi pages san francisco december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid fast tweedie future directions ext filesystem freenix monterey june tweedie ext journaling file system http olstrans sourceforge net release ols -ext ols ext html july yang twohey engler musuvathi model checking find file system errors osdi dec manolios lamport model checking tla specications lecture notes computer science 
database-aware semantically-smart storage muthian sivathanu lakshmi bairavasundaramy andrea arpaci-dusseauy remzi arpaci-dusseauy google ycomputer sciences department wisconsin madison abstract recent research demonstrated potential bene building storage arrays understand systems semantically-smart disk systems knowledge system structures operations improve performance availability security ways precluded traditional storage system architecture paper study applicability semantically smart disk technology underneath database management systems case studies analyze differences building database-aware storage semantically-smart disk systems successfully applied underneath database techniques log snooping explicit access statistics needed introduction processing power increasing modern storage systems symmetrix storage array highend raid emc processors memory ability leverage computational power traditional storage systems limited due narrow blockbased interface protocols scsi storage arrays receive simplest commands read write range blocks storage system knowledge blocks part block live dead bridge information gap recent research proposed idea semantically smart disk system learns embedded knowledge system semantic information storage system vendors build functional reliable higher-performing secure storage systems exploiting knowledge directory structures storage system deliver improved data availability failure previous research semantically smart disk systems assumed commodity work wisconsin-madison system linux ext linux ext netbsd ffs windows fat windows ntfs interacting disk paper explore techniques semanticallysmart disk systems operate beneath database management systems dbms database systems form signi important group clients storage systems bene semantically smart storage applied realm operating beneath system database semantically smart disk system performs similar operations tracking table block allocated dbms tracks information organizes data disk differently system systems record metadata statistics recent access modi time dbms specialized track general statistics system workloads directory structure reasonable approximation semantic groupings users place related les single directory dbms semantic grouping tables indexes dynamic depending query workload general nding differences fundamental require semantically smart storage build database-aware storage investigate techniques required systems explore log snooping storage system observes write-ahead log wal records written dbms monitoring log storage system observe operation performed dbms effect reaches disk explore bene dbms explicitly gather access statistics write statistics storage simple add statistics dbms investigate database-aware storage implement analyze case studies found work underneath systems study improve storage system availability d-graid raid system degrades gracefully failure implement dbms-specialized version faded storage system guarantees data unrecoverable user deleted finally explore improve second-level storage-array cache hit rates technique x-ray experience semantically-smart disks work underneath database systems cases database systems systems semantically-smart storage secure delete case presence transactional semantics dbms disk accurately track dynamic information result functionality requires absolutely correct inferences implemented changing dbms contrast functionality required system case studies d-graid x-ray dbms supply desired access information storage system result results obtained slightly modify dbms rest paper organized section review related work database-aware storage discuss advantages disadvantages semantically-smart disks section describe general techniques needed semantic disk extract information dbms sections present case studies finally discuss range techniques section conclude section background placing intelligence disk systems database systems favor years summary work area keeton dissertation page earliest examples idea logic track devices proposed disk computational ability head natural application lter data passes rest system idea database-speci machines refuted boral dewitt primary reason failure approaches required non-commodity components outperformed technology moved ahead worse database vendors rewrite substantial code base advantage speci features offered specialized architectures processing power faster cheaper idea active disks focus recent work includes acharya riedel efforts portions applications downloaded disks tailoring disk running program research focuses partition applications host disk cpus minimize data transferred contrast previous work semantically-smart approach require specialized hardware components sophisticated programming environments high-end storage arrays good match technology multiple processors vast quantities memory building semantic knowledge higher-level systems storage array bene drawbacks main bene semantic-disk approach increases functionality placing high-level semantic knowledge storage system enables systems require low level control storage array high level knowledge dbms systems precluded traditional storage architectures previous research shown semantic disks improve performance layout caching improve reliability provide additional security guarantees semantically-smart approach leads concerns concern processing required disk system researchers noted trend increasing intelligence disk systems modern storage arrays exhibit fruits moore law emc symmetrix storage server con gured processors ram resources idle nonetheless hint relative simplicity adding intelligence concern placing semantic knowledge disk system ties disk system intimately system dbms dbms on-disk structure storage system change systems ondisk formats rarely change format ext system signi cantly changed years existence current modi cations great pains preserve full backwards compatibility older versions system case dbms format concern gain insight storage vendor deliver rmware updates order pace dbms-level studied development postgres times revision history dump restore required migrate version found dump restore needed months average frequent expected commercial databases store terabytes data requiring dump restore migrate tolerable users recent versions oracle great lengths avoid on-disk format nal concern storage system semantic knowledge layer system dbms possibly run fortunately systems database systems supported cover large fraction market functionality semantic disk independent layer small portion code handle issues speci system dbms finally storage vendor reduce burden supporting database platforms target single important database oracle provide standard raid functionality systems interestingly highend raid systems perform bare minimum semantically-smart behavior storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically summary storage vendors commit resources support database technology database-aware techniques implement powerful functionality storage system leverage higher-level semantic information system dbms running top section describe types information semantic disk requires underneath dbms discuss information acquired database-speci semantic information broadly categorized types static dynamic experience primarily predator dbms built shore storage manager illustrate techniques speci examples predator techniques general database systems static information static information comprised facts database change database running storage system obtain static information knowledge embedded rmware explicitly communicated out-of-band channel system installation cases static information describes format on-disk structures knowing format database log record semantic disk observe update operation disk knowing structure b-tree pages disk determine internal pages versus leaf pages nally understanding format data pages semantic disk perform operations scanning page holes byte ranges deleted cases static information describes location on-disk structures predator knowing names ids system catalog tables rootindex sindxs table dynamic information dynamic information pertains information dbms continually operation examples dynamic information include set disk 
blocks allocated table disk block belongs table index unlike static information dynamic information continually tracked disk track dynamic information semantic disk utilizes static information data structure formats monitor key data structures correlated higher level operations systems databases buffer reorder writes performing accurate inference higher level operations complex solve problem technique log snooping storage system observes log records written dbms log snooping storage system leverages fact database write-ahead log wal track operation on-disk contents wal property log operation reaches disk effect operation strong ordering guarantee makes inferences underneath dbms accurate straightforward implementation log snooping assume log record log sequence number lsn lsn byte offset start record log volume lsn semantic disk accurately infer exact ordering events occurred database presence group commits log blocks arrive order order events disk maintains expected lsn pointer lsn log record expected disk semantic disk receives write request log block block log record semantic disk processes log record advances expected lsn pointer point record log blocks arrive order semantic disk utilizes lsn ordering process blocks order log blocks arriving order deferred expected lsn reaches block describe detail implementation database-aware storage log snooping infer important pieces dynamic information transaction status block ownership block type relationships blocks describe importance nal piece dynamic information access statistics transaction status basic piece dynamic information current state transaction written disk transaction pending committed pending transaction aborted performing work transaction semantic disk choose pessimistically recognize committed transactions optimistically begin work pending transactions trade-offs pessimistic optimistic approaches pessimistic approach semantic disk implements functionality requires correctness implementing secure delete section semantic disk shred data belonging pending transaction transaction abort dbms require data pessimistic approach worse performance optimistic approach pessimistic version delay work require signi amount buffering optimistic approach bene cial aborts rare dbms implements group commits delay committing individual transactions long period determining status transaction straightforward log snooping semantic disk observes log record written adds list pending transactions disk observes commit record log determines transactions committed moves committed list block ownership semantic disk understand logical grouping blocks tables indices involves associating block table index store logically owns block performing association semantic disk straight forward effect allocating block recoverable dbms rst logs operation performing allocation semantic disk observes traf disk block simple associate block owning table index show cases suf cient semantic disk map blocks store owning table cases semantic disk map store actual table index allocating block shore writes create extlog record block number owning store semantic disk observes log entry records block number store internal block store hash table map store actual table index disk static knowledge system catalog tables predator mapping maintained tree called rootindex logical store statically disk observes btree add records log rootindex semantic disk identify newly created mappings add store hash table block type piece information semantic disk type store block block data page index page track information semantic disk watches updates system catalog tables names part static information disk predator sindxs table indexes database tuple sindxs index table attribute index built semantic disk detects inserts table page insert records log semantic disk determine block part table index owning store information derived sindxs table block relationships type information consists relationships blocks relationships semantic disk table set indices built table stated predator association indices tables sindxs catalog table semantic disk consult information derived sindxs table associate table indices vice versa access patterns addition previous dynamic information semantic disk tables indexes accessed current workload transaction status block ownership block type block relationships inferred easily log snooping access patterns dif cult infer inferring access patterns found easy underneath general-purpose system fact set les lies directory implicitly conveys information storage system les accessed similarly systems track time accessed periodically write information disk modern database systems track access statistics performance diagnosis statistics gathered coarse granularity automatic workload repository oracle maintains access statistics experience revealed dbms track types statistics information optimize behavior dbms write statistics periodically disk additional catalog tables transactional avoid logging overhead basic statistic dbms communicate semantic disk access time block table statistic derive statistics statistic summarizes access correlation entities tables indexes dbms record query set tables indexes accessed correlation statistics capture semantic groupings tables collocating related tables finally statistic tracks access counts number queries accessed table duration piece information conveys importance tables indexes case studies actual static dynamic information required database-aware disk depends functionality disk implementing investigate number case studies previously implemented underneath systems investigate d-graid raid system degrades gracefully failure implement faded guarantees data unrecoverable user deletes finally explore x-ray implements second-level storage-array cache partial availability d-graid rst case study implement d-graid underneath dbms d-graid semantically-smart storage system lays blocks ensures graceful degradation availability unexpected multiple failures d-graid enables continued operation system complete unavailability multiple failures previous work shown approach signi cantly improves availability systems section begin reviewing motivation partial availability d-graid summarize past experience implementing d-graid underneath systems describe techniques implementing d-graid underneath dbms finally evaluate version d-graid discuss lessons motivation importance data availability emphasized settings downtime cost millions dollars hour cope failures systems database systems store data raid arrays employ redundancy automatically recover small number disk failures existing raid schemes effectively handle catastrophic failures number failures exceeds tolerance threshold array multiple failures occur due primary reasons faults correlated single controller fault component error render number disks unavailable system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure page extra failures existing raid schemes lead complete unavailability data contents array restored backup effect severe large arrays disks roughly raidarray fully operational disk system database completely unavailable availability cliff arises traditional storage systems employ simplistic layout techniques striping oblivious semantic importance blocks relationships blocks excess failures occur odds semantically-meaningful data table remaining low modern storage arrays export abstract logical volumes single disk system dbms control data placement ensure semantically-meaningful data remains single disk failure filesystem-aware d-graid basic goal d-graid make semantically meaningful fragments data failures workloads access fragments run completion oblivious data loss parts system working top redundancy technique raidd-graid graceful degradation number failures exceed tolerance threshold redundancy technique implemented 
d-graid system found layout techniques important fault-isolated data placement needed ensure semantic fragments remain entirety fault isolated placement entire semantic fragment collocated single disk found system workloads reasonable semantic fragment consists single entirety data blocks inode block potentially indirect blocks les single directory selective replication needed ensure essential meta-data data required system context essential meta-data found consist directories data inode blocks structures system superblock bitmap blocks essential data found system binaries directories usr bin bin lib access-driven diffusion popular data striped disks needed improve throughput large single disk found popular data dynamically identi tracking logical segments semantic knowledge access-driven diffusion implemented manner beneath system dbms database-aware d-graid describe techniques implementing graid underneath dbms explore techniques fault-isolated data placement target widely database usage patterns moderately-sized tables coarse-grained fragmentation large tables ne-grained fragmentation explore structures selectively replicated describe implementation accessdriven diffusion finally describe infallible writes technique required systems identifying semantic fragments fundamental differences dbms versus system dbms extremely large tables single disk describe techniques separately moderately-sized tables coarse-grained fragmentation entire table disk large tables ne-grained fragmentation stripe tables indexes multiple disks dbms queries performed directly impact tables indexes accessed describe semantic groupings affected popular types queries scans index lookups joins fault-isolated placement coarse-grained simplest case occurs database large number moderately-sized tables situation semantic fragment ned terms entire table present layout strategies improved availability query type scenario scans queries selection queries lter non-indexed attribute aggregate queries single table involve sequential scan entire table scan requires entire table order succeed simple choice semantic fragment set blocks belonging table entire table single disk failures occur subset tables entirety scans involving tables continue operate oblivious failure index lookups index lookups form common class queries selection condition applied based indexed attribute dbms index tuple record ids reads relevant data pages retrieve tuples traversing index requires access multiple pages index collocation index improves availability index table viewed independently placement index query fails index table unavailable decreasing availability strategy improve availability collocate table indexes call strategy dependent index placement joins queries involve joins multiple tables queries typically require joined tables order succeed improve availability join queries d-graid collocates tables joined single semantic fragment laid single disk identi cation join groups requires extra access statistics tracked dbms implementation modi predator dbms record set stores tables indexes accessed query construct matrix access correlation pair stores information written disk periodically seconds modi cations predator straight-forward involving lines code d-graid information collocate tables accessed fault-isolated placement fine-grained collocation entire tables indexes single disk enhanced availability single table index large single disk disk capacities roughly doubling year scenario require ne-grained approach semantic fragmentation approach graid stripes tables indexes multiple disks similar traditional raid array adopts techniques enable graceful degradation detailed scans scans fundamentally require entire table striping strategy impact availability scan queries availability hierarchical approach large table split minimal number disks hold disk group treated logical faultboundary d-graid applied logical fault-boundaries alternatively database supports approximate queries provide partial availability scan queries missing data index lookups large tables index-based queries common oltp workload tpc-c involves index lookups small number large tables queries require entire index table d-graid simple techniques improve availability queries internal pages b-tree index aggressively replicated failure instance root b-tree index page collocated data pages tuples pointed index page collocation d-graid probabilistic strategy leaf index page written d-graid examines set rids contained page rid determines disk tuple places index page disk greatest number matching tuples note assume table clustered index attribute page-level collocation effective case non-clustered indexes joins similar indexes page-level collocation applied tables join group collocation feasible tables join group clustered join attribute alternatively tables join group small replicated disks larger tables striped selective replication data structures dbms query system run system catalogs information table index frequently consulted structures unavailable partial failure fact data remains accessible practical d-graid aggressively replicates system catalogs extent map database tracks allocation blocks stores experiments employ -way replication important meta-data -way replication feasible readmostly nature meta-data minimal space overhead entails database log plays salient role recoverability database ability make partial availability important log multiple failures providing high availability log size active portion log determined length longest transaction factored concurrency workload portion log highly reasonable modern storage arrays large amounts persistent ram obvious locations place log high availability replicating multiple nvram stores addition normal on-disk storage log ensure log remains accessible face multiple disk failures access-driven diffusion stated coarse-grained fragmentation entire table single disk table large accessed frequently performance impact parallelism obtained disks wasted remedy d-graid monitors accesses logical address space tracks logical segments bene parallelism d-graid creates extra copy blocks spreads disks array normal raid blocks hot d-graid regains lost parallelism due collocated layout providing partial availability guarantees reads writes rst diffused copy background updates actual copy technique underneath dbms essentially identical underneath system infallible writes partial availability data introduces interesting problems transaction recovery mechanisms dbms transaction declared committed ected log partially system crash redo transaction fail pages affect durability semantics transactions problem considered solved aries context handling ine objects deferred restart ensure transaction durability d-graid implements infallible writes guarantees write succeeds block written destined dead disk d-graid remaps live disk writes assuming free space remaining live disk remapping prevents failure ushing committed transaction disk evaluation evaluate availability improvements performance d-graid prototype implementation d-graid prototype functions software raid driver linux kernel operates underneath predator shore dbms availability improvements evaluate availability improvements d-graid d-graid array disks study fraction queries database serves successfully increasing number disk failures layout techniques d-graid complementary existing raid schemes parity mirroring show d-graid level redundancy data measurements simplicity microbenchmarks analyze availability provided layout techniques d-graid coarse-grained fragmentation rst evaluate availability improvements due coarse-grained fragmentation techniques d-graid figure presents availability scan index lookup join queries synthetic workloads multiple disk failures percentage queries complete successfully reported leftmost graph figure shows availability scan queries database tables tuples workload query chooses table random computes average non-indexed attribute requiring scan entire table graph shows collocation tables enables database partially serving proportional fraction queries comparison queries succeed failed disks table scans availability queries succeed failed disks index lookup queries dependent 
index placementindependent index placement queries succeed failed disks join queries join collocationwithout collocation figure coarse-grained fragmentation graphs show availability degradation scans index lookups joins varying number disk failures -disk d-graid array steeper fall availability higher number failures due limited -way replication metadata straight diagonal line depicts ideal linear degradation queries succeed failed disks split tables indexes replication colocationwith replication plain striping queries succeed failed disks split tables indexes hot-cold tuples hot-cold -wayhot-cold -way hot-cold -way figure index lookups ne-grained fragmentation graphs show availability degradation index lookup queries left graph considers uniformly random workload graph considers workload small set tupes popular failure traditional raidsystem results complete unavailability note redundancy maintained parity mirroring d-graid traditional raid tolerate failure availability loss middle graph figure shows availability index lookup queries similar workload layouts layouts entire store index table collocated disk independent index placement d-graid treats index table independent stores possibly allocates disks dependent index placement d-graid carefully allocates index disk table dependent placement leads availability failure finally evaluate bene join-group collocation micro-benchmark database pairs tables joins involving tables pair join queries randomly select pair join tables rightmost graph figure shows collocating joined tables d-graid achieves higher availability fine-grained fragmentation evaluate effectiveness ne-grained fragmentation focus availability index lookup queries interesting category workload study consists index lookup queries randomly chosen values primary key attribute single large table plot fraction queries succeed varying number disk failures left graph figure shows results layouts examined graph lowermost line shows availability simple striping replication system catalogs availability falls drastically multiple failures due loss internal b-tree nodes middle line depicts case internal b-tree nodes replicated aggressively expected achieves availability finally line shows availability data index pages collocated addition internal b-tree replication techniques ensure linear degradation availability graph figure considers similar workload small subset tuples hotter compared speci cally tuples accessed queries workload simple replication collocation provide linear degradation availability hot pages spread uniformly disks hot-cold workload d-graid improve availability replicating data index pages d-graid raidslowdown table scan index lookup bulk load table insert table time overheads d-graid table compares performance d-graid ne-grained fragmentation default raidunder microbenchmarks array disks hot tuples lines depict availability hot pages replicated factors small fraction read data hot d-graid utilizes information enhance availability selective replication performance overheads evaluate performance implications faultisolated layout d-graid experiments section -disk d-graid array comprised ibm ultrastar lzx disks peak throughput database single table records sized bytes index primary key time space overheads rst explore time space overheads incurred d-graid prototype tracking information database laying blocks facilitate graceful degradation table compares performance graid ne-grained fragmentation linux software raid basic query workloads workloads examined scan entire table index lookup random key table bulk load entire indexed table inserts indexed table graid performs raidfor workloads scans poor performance scans due predator anomaly scan workload completely saturated cpu table disks extra cpu cycles required d-graid impacts scan performance interference prototype competes resources host hardware raid system interference exist overheads d-graid reasonable evaluated space overheads due aggressive metadata replication found minimal overhead scales number tables database tables overhead -way replication important data access-driven diffusion evaluate bene diffusing extra copy popular tables table shows time scan scan time raidd-graid d-graid diffusion table diffusing collocated tables table shows scan performance -disk array coarse-grained fragmentation table coarse-grained fragmentation d-graid simple collocation leads poor scan performance due lost parallelism extra diffusion aimed performance d-graid performs closer default raidcomparison implementation d-graid underneath dbms uncovered fundamental challenges present system notion semantically-related groups complex dbms inter-relationships exist tables indexes system case les directories reasonable approximations semantic groupings dbms goal graid enable serving higher level queries notion semantic grouping dynamic depends query workload identifying popular data aggressively replicated easier systems standard system binaries libraries obvious targets independent speci system running dbms set popular tables varies dbms dependent query workload effectively implementing d-graid underneath dbms requires slightly modifying dbms record additional information finally ensure transaction durability implemented infallible writes version dbms comparing d-graid performs beneath dbms versus system similarities versions d-graid successfully enable graceful degradation availability versions enable expected number processes queries complete successfully xed number disk failures fact versions enable expected number complete subset data popular similarly versions d-graid introduce time overhead interestingly slowdowns database version generally lower system version finally versions require access-driven diffusion obtain acceptable performance secure delete faded case study implement faded underneath dbms faded semantically smart disk detects deletes records tables dbms level securely overwrites shreds relevant data make irrecoverable extend previous work implemented functionality systems motivation deleting data recovery impossible important system security government regulations require guarantees sensitive data forgotten requirements important databases recent legislations data retention sarbanes-oxley act accentuated importance secure deletion secure deletion data magnetic disks involves overwriting disk blocks sequence writes speci patterns cancel remnant magnetic effects due past layers data block early work overwrites block required secure erase recent work shows overwrites suf modern disks system dbms ensure secure deletion functions top modern storage systems transparently perform optimizations storage system buffer writes nvram writing disk presence nvram buffering multiple overwrites system dbms collapsed single write physical disk making overwrites ineffective presence block migration storage system overwrites system dbms miss past copies secure deletion requires low level information control storage system time higher level semantic information system dbms detect logical deletes semanticallysmart disk system ideal locale implement secure deletion filesystem-aware faded running underneath system faded infers deleted tracking writes inodes indirect blocks bitmap blocks due asynchronous nature systems faded guarantee current contents block belong deleted newly allocated shredded ensure shred valid data faded conservative overwrites shreds version block restoring current contents block previous work implemented faded systems linux ext linux ext windows vfat faded work correctly system changed linux ext modi ensure data bitmap blocks ushed indirect block allocated freed windows vfat changed track generation number nally linux ext modi list modi data blocks included transaction database-aware faded implement faded beneath dbms semantic disk identify handle deletes entire tables individual records discuss cases turn simplest case faded table deleted drop table command issued faded shred blocks belonging table faded log snooping identify log records freeing extents stores 
shore free ext list log record written extent freed faded list freed blocks issue secure overwrites pages transaction aborts undoing deletes contents freed pages required faded pessimistically waits transaction committed performing overwrites handling record-level deletes faded challenging speci tuples deleted sql delete statement speci byte ranges pages tuples shredded delete dbms typically marks relevant page slot free increments free space count page freeing slots logged faded learn record deletes log snooping faded shred page records page valid read current page disk defer shredding faded receives write page ecting relevant delete receiving write faded shreds entire page disk writes data received complications basic technique rst complication identify correct version page deleted record assume faded observes record delete page waits subsequent write written faded detect version written reects version stale dbms wrote page delete block reordered disk scheduler arrives disk issue similar le-system version faded conservative overwrites database-aware version wal property dbms ensure correct operation speci cally database-aware faded pagelsn eld page identify ects delete pagelsn page tracks sequence number run time workload workload default faded faded faded table overheads secure deletion table shows performance faded overwrites workloads workload deletes contiguous records workload deletes records randomly table latest log record describing change page faded simply compare pagelsn lsn delete complication dbms bytes belonged deleted records result data remains page faded observes page write scans page free space explicitly zeroes deleted byte ranges page remain dbms cache subsequent writes page scanned zeroed appropriately evaluation brie evaluate cost secure deletion faded prototype implementation prototype implemented device driver linux kernel works underneath predator workloads operating table -byte records rst workload perform delete rows half table deleted deleted pages contiguous workload tuples deleted selected random table compares default case faded faded overwrite passes expected secure deletion performance cost due extra disk multiple passes overwrites modern disks effectively shred data overwrites focus faded case performance slower overhead incurred deletes sensitive data deleted manner costs reasonable situations additional security required comparison primary difference versions faded database-aware version leverage transactional properties dbms nitively track block shredded result system version faded required system exception data journaled ext implementation faded require dbms version require detailed information on-disk page layout dbms record-level granularity deletes dbms makes secure deletion complex system counterpart versions faded incur overhead depending workload number overwrites delete-intensive database workloads faded slower overwrites similarly system workloads faded slower overwrites table summary slowdown incurred faded depends workload number overwrites dbms system exclusive caching x-ray nal case study implement x-ray underneath dbms x-ray exclusive caching mechanism storage arrays attempts cache disk blocks present higher-level buffer cache providing illusion single large lru cache previous work demonstrated approach performs buffer cache maintained system motivation modern storage arrays possess large amounts ram caching disk blocks instance high-end emc storage array main memory caching typically cache second-level cache system database system maintains buffer cache host main memory current caching mechanisms storage arrays account block array cache read duplicating blocks cached cache space wasted due inclusion strategy contents buffer cache disk array cache exclusive wong proposed avoid cache inclusion modifying system disk interface support scsi demote command enables treating disk array cache victim cache database system approach require dbms inform disk evictions buffer pool requiring explicit change scsi storage interface makes scheme hard deploy industry consensus required adopting change filesystem-aware x-ray x-ray predicts contents system buffer pool chooses cache recent victims cache x-ray requires storage interface x-ray access time statistics block accessed perform predictions hit rate array cache size blocks hit rate x-ray multi-queue lru execution time seconds array cache size blocks execution time lru multi-queue x-ray hit rate write period write period variation period variation hit rate segment size blocks segment size variation segment size variation figure x-ray performance gure presents evaluation x-ray tpc-c benchmark dbms buffer cache set blocks studies hit rate x-ray compared caching mechanisms segment size blocks access information written execution times compared times based buffer cache hit time disk array cache hit time disk read time hit-rate x-ray measured segment sizes write period write period varied x-ray hit rate measured segment size blocks systems linux ext access statistics recorded granularity directly inodes x-ray access statistics maintain ordered list block numbers lru block mru block complicated fact access statistics tracked perle basis ordered list updated x-ray obtains information system reads disk making read block recently accessed system writes access time disk disk read arrives block x-ray infers evicted buffer cache time past infer block earlier access time evicted assuming lru policy access time block updated x-ray observe disk read x-ray infers block blocks access time present buffer cache higher-level cache policy lru usual case blocks close mru end list predicted system buffer cache blocks lru part list considered exclusive set x-ray caches recent blocks exclusive set extra internal array bandwidth idle time disk requests read blocks cache database-aware x-ray database-aware version x-ray similar system-aware version primary difference creating database-aware x-ray occurs dbms typically track access statistics database systems maintain access statistics administrative purposes awr oracle statistics coarse granularity written long intervals implement database-aware x-ray modify database buffer manager write access statistics periodically speci cally table index divided xed-sized segments buffer manager periodically writes disk access time segments accessed period time x-ray assumes blocks segment accessed sees access time statistic updated accuracy x-ray predict contents database cache sensitive size segment update interval advantage explicitly adding information tune implementation changing size segment update interval alternative adding access information modify dbms directly report evicted block cache demote adding access statistics approach statistics general semantic disks implementing functionality d-graid evaluation evaluate performance database-aware version x-ray simulation database buffer cache disk array cache evaluation lesystem-aware x-ray performed simulation database buffer cache maintained lru fashion dbms periodically writes access information granularity segment array cache managed x-ray assume x-ray suf cient internal bandwidth block reads instrumented buffer cache manager postgres dbms generate traces page requests buffer cache level postgres predator programming api linux required implement tpc-c approximate implementation tpc-c benchmark evaluation adheres tpc-c speci cation access pattern total transactions performed evaluate performance x-ray terms array cache hit rate execution time 
compare static dynamic catalog tables log record format b-tree page format data page format ransaction status block wnership block type block relationships access statistics d-graid basic ne-grained frags join-collocation faded basic record-level delete x-ray basic table dbms information required case studies table lists static information embedded semantic disk dynamic state automatically tracked disk ray plain lru multi-queue mechanism designed level caches explore sensitivity segment size access time update periodicity figure compares hit rate x-ray schemes figure compares execution times segment size set blocks access information written study x-ray hit rate lru multi-queue hit rate advantage extends execution time overhead writing access information x-ray performs lru multi-queue figure evaluates sensitivity x-ray cache hit rate segment size expected hit rate drops slightly increase segment size figure shows sensitivity access information update interval x-ray tolerate reasonable delay seconds obtaining access updates comparison system database versions x-ray similar implement x-ray semantic disk requires access statistics blocks accessed layer systems track periodically write statistics dbms x-ray dbms modi explicitly track access times segments table advantage explicitly adding information tune statistics appropriately size segment update interval running beneath system database ray found substantially improve array cache hit rate relative lru multi-queue information case studies section review static dynamic information required database-aware disk needed information depends functionality implemented exact information required variants case studies listed table biggest concern database vendors static information exported storage system understands format catalog table database vendor loathe change format amount static information varies bit case studies case studies format catalog tables log records d-graid support ne-grained fragmentation faded record-level deletes detailed knowledge b-tree page format data page format dynamic information varies case studies fundamental piece dynamic information block ownership shown fact required case study block type generally property needed d-graid faded pieces dynamic information widespread faded precisely transaction committed correct pessimistic determining overwrite data d-graid associate blocks table blocks index vice versa finally access correlation access count statistics needed d-graid variants collocate related tables aggressively replicate hot data simple access time statistic needed x-ray predict contents higher-level buffer cache conclusions today database community sort simple-minded model disk arm platter holds database fact holding database raid arrays storage area networks kinds architectures underneath hood masked logical volume manager written operating system people databases transparency good makes productive care details hand optimizing entire stack elds talk hand accept things -pat selinger semantic knowledge storage system enables powerful functionality constructed storage system improve performance caching improve reliability provide additional security guarantees paper shown semantic storage technology deployed beneath commodity systems beneath database management systems found techniques required handle database systems investigated impact transactional semantics dbms cases transactions simplify work semantic disk log snooping enables storage system observe operations performed dbms nitively infer dynamic information changing dbms storage system ensure interfere transactional semantics found infallible writes ensure transaction durability disks failed explored lack access statistics dbms complicates interactions semantic disk case found helpful slightly modify database system gather relay simple statistics acknowledgements david black encouraging extend semantic disks work databases david dewitt jeff naughton rajasekar krishnamurthy vijayan prabhakaran insightful comments earlier drafts paper jeniffer beckham pointing pat selinger quote finally anonymous reviewers thoughtful suggestions greatly improved paper work sponsored nsf ccrccr- ccrngs- itribm network appliance emc acharya uysal saltz active disks programming model algorithms evaluation proceedings international conference architectural support programming languages operating systems asplos viii san jose california october agrawal kiernan srikant hippocratic databases vldb bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids isca bauer priyantha secure data deletion linux file systems usenix security august boral dewitt database machines idea time passed international workshop database machines brown yamaguchi oracle hardware assisted resilient data oracle technical bulletin note chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks usenix pages emc corporation symmetrix enterprise information storage systems http emc ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december gray computers stop international conference reliability distributed databases june gribble robustness complex systems eighth workshop hot topics operating systems hotos viii schloss elmau germany grochowski emerging trends data storage magnetic hard disk drives datatech september gutmann secure deletion data magnetic solidstate memory usenix security july hellerstein haas wang online aggregation sigmod pages hughes coughlin secure erase disk drive data idema insight magazine keeton computer architecture support database applications phd thesis california berkeley keeton wilkes automating data dependability proceedings acm-sigops european workshop pages saint-emilion france september carey shoring persistent applications proceedings acm sigmod conference mohan haderle lindsay pirahesh schwarz aries transaction recovery method supporting finegranularity locking partial rollbacks write-ahead logging acm tods march oracle self-managing database automatic performance diagnosis https oracleworld published doc patterson gibson katz case redundant arrays inexpensive disks raid sigmod pages patterson availability maintainability performance focus century key note lecture fast postgres postgresql database http postgresql riedel gibson faloutsos active storage largescale data mining multimedia vldb selinger winslett pat selinger speaks sigmod record december seshadri paskin predator or-dbms enhanced data types sigmod sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid fast sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems usenix symposium file storage technologies fast pages slotnick logic track devices volume pages academic press tpc-c transaction processing performance council http tpc tpcc tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wong wilkes cache making storage exclusive usenix zhou philbin multi-queue replacement algorithm level buffer caches usenix pages 
model-based failure analysis journaling file systems vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau wisconsin madison computer sciences department west dayton street madison wisconsin vijayan dusseau remzi wisc abstract propose method measure dependability journaling file systems approach build models journaling file systems behave journaling modes models analyze file system behavior disk failures techniques measure robustness important linux journaling file systems ext reiserfs ibm jfs analysis identify design flaws correctness bugs present file systems file system errors ranging data corruption unmountable file systems introduction disks fail modern file systems storage systems include internal machinery cope failures ensure file system integrity reliability presence failures disks fail changing traditional systems assume disks fail-stop assumption disk working failure easily detectable disk complexity increases pressures time-to-market cost increase disk failure modes common specifically latent sector faults occur specific block faulty transient permanent manner disk viewing disk working longer paper investigate modern file systems cope class fault modern file systems journaling systems logging data separate journal writing fixed locations file systems maintain file system integrity presence crashes analyze file systems develop modelbased fault-injection technique specifically file system test develop abstract model update behavior orders writes disk maintain file system consistency model inject faults interesting points file system transaction monitor system reacts failures paper focus write failures file system writes change on-disk state potentially lead corruption properly handled fault-injection methodology test widely linux journaling file systems ext reiserfs ibm jfs analysis find design flaws file systems catastrophically affect on-disk data specifically find ext ibm jfs designed handle sector failures failures file systems shown commit failed transactions disk lead problems including unmountable file system contrast find reiserfs part paranoid write failures specifically reiserfs crashes system write journal fails crashing manner reiserfs ensures file system integrity maintained cost potentially expensive restart configurations reiserfs abide general policy coerced committing failed transactions result corrupted file system reiserfs assumes failures transient repeated failure block result repeated crashes restarts rest paper organized give introduction journaling file systems explain methodology analyzing journaling file systems discuss results analysis ext reiserfs jfs present related work finally conclude background file system update takes place set blocks written disk system crashes middle sequence writes file system left inconsistent state repair inconsistency earlier systems ffs ext scan entire file system perform integrity checks fsck mounting file system scan time-consuming process hours large file systems journaling file systems avoid expensive integrity check recording extra information disk form write-ahead log writes successfully committed log transfered final fixed locations disk process transferring writes log fixed location disk referred checkpointing crash occurs middle checkpointing file system recover data log write fixed locations modern file systems provide flavors journaling subtle differences update behavior disk discuss approaches data journaling ordered journaling writeback journaling journaling modes differ kind integrity provide type data write log order data written data journaling strongest data integrity block written disk irrespective data metadata block written log transaction committed journaled data written fixed file system locations writeback journaling logs file system metadata enforce ordering data writes journal writes ensuring metadata consistency writeback journaling guarantee data consistency specifically file metadata updated in-place data reaches disk file data contents data block ordered journaling adds data consistency writeback mode enforcing ordering constraint writes data blocks written fixed locations metadata blocks committed ordering constraint ensures file system metadata points corrupt data methodology section describe methodology testing reliability journaling file systems basic approach simple inject disk faults beneath file system key points operation observe resultant behavior testing framework shown figure consists main components device driver called fault-injection driver user-level process labeled coordinator driver positioned file system disk observe traffic file system inject faults points stream coordinator monitors controls entire process informing driver specific fault insert running workloads top file system observing resultant behavior flow diagram benchmarking process shown figure describe entire process detail fault-injection driver fault-injection driver driver pseudodevice driver appears typical block device file system internally simply interposes requests real underlying disk driver main roles system classify block written disk based type specific file-system data structure write represents developed techniques perform classification simply employ techniques driver model journaling file system specifically model represents correct sequence states transaction committing disk inserting failures specific points transaction sequence observe file system handles types faults judge correctly handles faults injected driver inject faults system faults occur state transitions based model file system stream coordinator coordinator monitors entire benchmarking process inserts fault-injection driver linux kernel coordinator constructs file system passes fault specification driver spawns child process run workload errors running tests coordinator process moves file system state mounting file system cleanly depending type block fail coordinator process passes fault specification driver spawns child process run workload top file system expected block written file system driver injects linux vfs layer jfsreiserfsext idescsi log system log workload fault injection driver ioctl coordinator block match fault pass request disk inject fault block match model pass error file system receive file system read write requests save fault specification model build journaling specification classify block types report error yesno yesno coordinator figure benchmarking framework algorithm flow figure shows benchmarking framework measure fault tolerance journaling file systems write failures main components figure user level process issues fault sba driver classifies blocks injects faults figure shows simplified flowchart benchmarking algorithm implemented sba driver fault failing block write errors manifest numerous locales log errors coordinator collate specifically child process receive errors file system driver observe errors sequence state transitions coordinator system logs errors reported file system reflected calling child process journaling models describe model journaling file systems explained section journaling modes journaling modes differs type data journals order writes blocks build model journaling modes based functionality models represent journaling modes type data accept order data written model ordered journaling mode specifies ordered data written metadata committed log build models construct regular expression journaling mode regular expressions represent journaling modes concisely easy construct understand build model based regular expression figure shows models journaling mode journaling models consist states states represent state on-disk file system ondisk file system moves state based type write receives file system track state change moving correspondingly model explain 
briefly regular expression journaling mode represent journal writes represent data writes represent journal commit writes represent journal super block writes represent checkpoint data writes represent write failures data journaling data journaling expressed regular expression data journaling mode file system writes journaled represented ordered unordered writes writing journal blocks commit block represented byc written file system mark end transaction file system write transactions log transactions committed file system write checkpoint blocks represented fixed locations journal super block represented mark head tail journal convert regular expression state diagram shown figure add failure state ordered journaling ordered journaling exs data journaling model ordered journaling model writeback journaling model figure journaling models figure shows models verifying journaling modes model built based regular expression state represents state reached write failure added models represents journal writes represents data writes represents journal commit writes represents checkpoint writes represents journal super block writes represents write failure pressed regular expression ordered mode ordered data writes written metadata blocks committed journal note data blocks parallel journal writes writes commit block written commit block written transaction transactions similar data journaling file system write checkpoint blocks journal super block transactions regular expression converted state diagram failure state added shown figure writeback journaling writeback journaling regular expression writeback journaling mode unordered data written time file system written journal writes journal writes commit block written transaction committed file system write journal super block checkpoint blocks unordered writes writeback journaling model figure obtained regular expression adding state error model error model assume latent errors originate storage subsystem errors accurately modeled software-based fault injection linux low-level errors reported file system uniform manner errors device-driver layer errors inject block write stream attributes similar classification faults injected linux kernel coordinator passes fault specification fault-injection driver attributes specifies file system test driver understands ext reiserfs ibm jfs file system semantics attribute specifies block type determines request traffic stream failed request types supported file systems attribute change file system request failed dynamically-typed journal commit block statically typed journal super block long determines fault injected transient error fails requests succeeds permanent fails subsequent requests failure classification classify ways file system fail due write failures type losses incur write failure loss file system handles write failure properly prevents data corrupted lost data corruption case write failures lead data corruption metadata corruption file system metadata structures remain consistent data files corrupted type failure occur data block pointers metadata blocks point invalid contents disk note type errors detected fsck data loss type failure file data lost due transient permanent write failures data loss occur data block pointers updated correctly files directories loss case file system metadata corrupted result lost files directories unmountable file system write failures happen file system corrupt important metadata blocks super block group descriptors result unmountable file system crash write failures lead file system reactions system-wide crash failure initiated explicit call panic due reasons dereferencing null pointer semantic fault injection question address fault injection technique file-system aware conduct similar analysis semantic knowledge device driver fail disk writes understands file system block types transaction boundaries high-level information driver type block receives determine failing journal block data block information important file systems behave differently block-write failures reiserfs crashes journal write failures crash data-block write failures depending type block failed file system errors vary data corruption unmountable file systems filesystem knowledge answer file system fails higher-level semantic knowledge enables identify design flaws identified fault injection performed semantic information analysis putting fault injection conclude methodology section fault injected journaling model figure shows sequence steps fault-injection driver track file system writes inject fault failing commit block write transaction ordered journaling mode step figure captures transition state initially transaction starts set ordered data writes figure data writes journal blocks logged figure commit block written data journal writes failed figure file system oblivious commit block failure continue checkpoint journaled blocks figure file system recognize failure steps figure fault injection figure shows sequence steps fault-injection driver track file system writes fail specific writes prevent file system corruption moving state figure state file system abort failed transaction bad block remapping remount read-only crash system sufficient block types inject fault file system requests model fault-injection driver reason requests write failure belong failed transaction transactions file system keeping track writes journaling model fault-injection driver explain block write failure leads file system errors fault injection experiments statistical carefully choose fault injection points inject faults main points ordered data writes journal writes commit writes checkpoint writes superblock writes journal writes perform fault injection journal metadata journal data blocks fault injection experiment proceeds file system tested freshly created files directories needed testing created fault specification attributes passed sba driver controlled workload creating file directory generate block write failed run child process driver injects fault reports file system writes violate journaling model fault injected coordinator collects error logs child process system log driver process automated error logs interpreted manually figure extent file system damaged extraneous writes analysis section explain failure analysis linux based journaling file systems ext reiserfs ibm jfs ext analysis ext journaling file system based ext file system ext logs file system writes journal block level types journal metadata blocks track transactions blocks logged journal descriptor blocks store fixed location block numbers journaled data journal revoke blocks prevent file system replaying data replayed recovery journal commit blocks mark end transactions journal super block stores information journal head tail transaction journal metadata blocks log stores journal data blocks journaled versions fixed location blocks ext designed journal metadata blocks journal super block descriptor block revoke block commit block magic number identifies journal metadata blocks journal metadata blocks sequence number denotes transaction number transaction occur recovery block read journal correct magic number treated journal data block magic number sequence number match transaction expected blocks skipped based ext analysis found design flaws handling write failures committing failed transactions write transaction fails ext continues write transaction log commits fixing failed write affect file system integrity ordered data write fails ordered journaling mode expect file system abort transaction commits transaction metadata blocks end pointing wrong data contents disk problem occurs 
ext failure ordered write data corruption checkpointing failed transactions write transaction fails file system checkpoint blocks journaled part transaction checkpointing crash occurs file system replay failed transaction properly recovery phase result corrupted file system ext commits transaction transaction write fails committing failed transaction ext checkpoints blocks journaled transaction depending journaling mode checkpointing partial complete partial checkpointing cases ext checkpoints blocks failed transaction data journaling mode journal descriptor block journal commit block write fails cases checkpointing file system metadata blocks transaction checkpointed data blocks checkpointed data journaling mode file created data blocks transaction descriptor block fails metadata blocks file inode data bitmap inode bitmap directory data directory inode blocks written fixed locations data blocks file journaled data journaling mode written data blocks written fixed locations metadata blocks file end pointing wrong contents disk complete checkpointing ordered writeback journaling mode file system metadata blocks journaled data blocks written log modes ext checkpoints journaled blocks failed transaction describe generic case file system corruption transactions committed block journaled blocks journaled assume transaction fails file system continues checkpoint blocks failed transaction crash occurs writing blocks fixed locations file system log recovery runs mount recovery transaction recovered failed transaction recovered contents block overwritten contents recovery file system inconsistent state block transaction block transaction problem occurs ext happen journal metadata block descriptor block revoke block commit block fails lead file system corruptions resulting loss files inaccessible directories replaying failed checkpoint writes checkpointing process writing journaled blocks log fixed locations checkpoint write fails file system attempt write mark journal checkpoint write happen log replay ext replay failed checkpoint writes data corruption data loss loss files directories replaying transactions journaling file systems maintain state variable mark log dirty clean file system mounted log dirty transactions log replayed fixed locations journaling file systems update state variable starting transaction checkpointing transaction write update state variable fails things possibly happen file system replay transaction replayed fail replay transaction recovery replaying transaction integrity problems possibility replaying journal contents lead corruption loss data files directories ext maintains journal state journal super block ext clears field writes journal super block clean journal mark journal dirty journal super block written non-zero field journal super block write fails ext attempt write save super block locations journal super block failure ext continues commit transactions log journal super block written mark journal dirty failed journal appears clean mount transaction needed replay due previous crash ext fails replay result lost files directories replaying failed transactions journal data block write fails transaction aborted replayed transaction replayed journal data blocks invalid contents read written fixed location handled properly lead file system errors earlier ext abort failed transactions continues commit log recovery write invalid contents file system fixed location blocks corrupt important file system metadata result unmountable file system show created transaction journaled group descriptor block file system failed journal write group descriptor block ext committed transaction failed mark invalid commit crashed file system forced ext recovery mount recovery ext read block journal supposed group descriptor block overwrote fixed location group descriptor block invalid contents journal corrupted group descriptor block resulted unmountable file system ext summary find ext designed system crash mind ext effectively handle single block write failures features ext designed ext crash entire system failed writes magic numbers transaction ids journal metadata blocks ext prevents replay invalid contents main weakness ext design abort failed transactions continues commit lead file system errors ranging data corruption unmountable file system found ext logs empty transactions transactions blocks commit block affect integrity result unnecessary disk traffic reiserfs analysis journaling reiserfs similar ext reiserfs circular log capture journal writes logs file system writes block level reiserfs supports journaling modes journal metadata blocks journal descriptor block journal commit block journal super block describe transactions fixed location blocks journal metadata blocks reiserfs magic number transaction number similar ext based analysis found design flaws reiserfs crashing file system write fails reiserfs time crashes file system making panic call necessitates entire system rebooted affect processes running reiserfs affects processes running system crashing entire file system single write error benefit journal write journal data journal metadata fails system crashes failed transaction committed disk system boots mounts file system reiserfs performs recovery recovery replays transactions successfully committed failed transaction failed transactions committed replayed file system remains consistent state recovery avoids problems ext checkpointing failed transactions replaying successful transactions replaying failed transactions words reiserfs converts problem fail-stutter fault tolerance fail-stop journal block write fails reiserfs repeatedly crash system reiserfs crashes system checkpoint write fails crash recovery takes place failed checkpoint write replayed properly note works fine transient write failures permanent write errors reiserfs requires fsck run handle replay failures crashing checkpoint write failures prevents problem replaying failed checkpoint writes ext committing failed transactions write failures reiserfs crash continues commit failed transaction ordered journaling mode ordered data block write fails reiserfs journals transaction commits handling write error result corrupted data blocks failed transactions metadata blocks file system end pointing invalid data contents reiserfs uniform failure handling policy crashes write failures file system corruption prevented reiserfs crashing system ordered write failures reiserfs summary find reiserfs avoids mistakes ext expensively cost crashing entire file system basically reiserfs converts fail-stutter system fail-stop handle write errors find committing failed transaction reiserfs desirable design decision solve problems ext block write errors permanent reiserfs make system unusable repeated crashing model find bug reiserfs linux data journaling mode version behaving ordered journaling mode journaling model find bugs semantics journaling violated jfs analysis ibm jfs works ordered journaling mode unlike ext reiserfs support data writeback journaling modes jfs differs ext reiserfs information written log ext reiserfs log blocks journal jfs writes records modified blocks log ordered data block writes written blocks similar file systems jfs record level journaling log blocks classified journal data blocks journal commit blocks single log write journal data records commit records hard separate commit record journal records transactions small fit single journal block modified ordered journaling model work jfs record level journaling performed failure analysis jfs found design mistakes crashing file system similar reiserfs jfs crashes file system 
write failures system crashes journal super block write fails mount operation earlier crashing system affects processes running system crashing system graceful provide fault tolerance write errors permanent replaying failed checkpoint writes checkpoint block write fails jfs attempt rewrite mark transaction replay jfs simply ignores error lead corrupted file system behavior similar ext file systems record failed checkpoint writes identifying transactions replayed committing failed transactions found journaling file systems commit failed transaction ordered block write failure jfs notify application ordered write failure commits transaction lead data corruption failing recover journal block write fails jfs abort failed transaction commits crash journal write failure logredo routine jfs fails unrecognized log record type lead unmountable file system jfs summary jfs design flaws ext reiserfs jfs commits failed transactions replay failed checkpoint writes crashes file system reiserfs journal super block write failures found bug jfs jfs flush blocks file sync call created sized file called fsync file descriptor fsync call returned flushing blocks ext reiserfs ibm jfs committing failed transactions checkpointing failed transactions replaying failed checkpoint writes replaying transactions replaying failed transactions crashing file system table design flaws table summary type design flaws identified ext reiserfs ibm jfs ext reiserfs ibm jfs block type journal descriptor block journal revoke block journal commit block journal super block journal data block checkpoint block data block table analysis summary table presents summary type failures occur ext reiserfs ibm jfs block writes fail data block represents ordered unordered writes ext reiserfs represents ordered writes jfs dcmeans data corruption means data loss fdl means files directory loss ufs means unmountable file system means crash means block type file system jfs separate commit revoke blocks records type journal expect file system write metadata blocks file disk analysis summary summary analysis presented table table table lists design flaws identified linux journaling file systems table types file system failures happen block writes fail find linux journaling file systems uniform failure handling policies handle fail-stutter systems related work section discuss related work talk related work fault injection general specific work file storage systems testing fault injection fault injection long time measure robustness systems koopman argues faults injected directly modules test give representative results dependability evaluation fault injected external environments module test fault activated inputs real execution similar approach inject faults external file system module activate running workloads top file system software simulate effects hardware faults inject faults dynamically determining block types file system ftape tool performs dynamic workload measurements inject faults automatically determining time location maximize fault propagation fiat early systems fault injection techniques simulate occurrences hardware errors changing contents memory registers fine tool developed kao inject hardware induced software faults unix kernel trace execution flow kernel recent work fault injection techniques test linux kernel behavior errors file storage system testing file system testing tools test file system api types invalid arguments siewiorek develop benchmark measure system robustness test dependability file system libraries similarly koopman ballista testing suite find robustness problems safe fast sfio library test file system robustness model checking techniques apply file system code recent work yang model checking comprehensively find bugs file systems ext reiserfs jfs formal verification techniques systematically enumerate set file system states verify valid file system states work identify problems deadlock null pointers work focuses file systems handle latent sector errors previous work studied reliability storage systems brown developed method measure system robustness applied measure availability software raid systems linux solaris windows emulate disk disk emulator inject faults test software raid systems work targets file systems file system knowledge carefully select fail specific block types don require semantic information fault injection studies evaluated raid storage systems reliability availability studies developed detailed simulation models raid storage arrays network clusters obtain dependability measures conclusion paper propose evaluate robustness journaling file systems disk write failures build semantic models journaling modes semantic block-level analysis technique inject faults file system disk requests evaluate widely linux journaling file systems analysis find ext ibm jfs violate journaling semantics block write failures result corrupt file systems contrast reiserfs maintains file system integrity crashing entire system write failures permanent write failures result repeated crashes restarts based analysis identify design flaws correctness bugs file systems catastrophically affect on-disk data find modern file systems uniform failure handling policy jfs overview ibm developerworks library jfs html brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june corbett english goel grcanac kleiman leong sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april devale koopman performance evaluation exception handling libraries dependable systems networks june gray reuter transaction processing concepts techniques morgan kaufmann kalbarczyk ravishankar yang characterization linux kernel behavior error dependable systems networks pages june huang kalbarczyk iyer dependability analysis cache-based raid system fast distributed simulation ieee symposium reliable distributed systems barton czeck fault injection experiments fiat ieee transactions computers volume pages april kaniche romano kalbarczyk iyer karcich hierarchical approach dependability analysis commercial cache-based raid storage architecture twenty-eighth annual international symposium fault-tolerant computing june koopman wrong fault injection dependability benchmark workshop dependability benchmarking conjunction dsn washington july lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems proceedings usenix annual technical conference usenix april reiser reiserfs namesys schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december siewiorek hudak suh segal development benchmark measure system robustness twenty-third international symposium fault-tolerant computing sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january tsai iyer measuring fault tolerance ftape fault injection tool intl conf modeling techniques tools conp perf evaluation pages sept tweedie journaling linux ext file system fourth annual linux expo durham north carolina yang twohey engler musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december 

geiger monitoring buffer cache virtual machine environment stephen jones andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison stjones dusseau remzi wisc abstract virtualization increasingly address server management administration issues flexible resource allocation service isolation workload migration virtualized environment virtual machine monitor vmm primary resource manager attractive target implementing system features scheduling caching monitoring lack runtime information vmm guest operating systems called semantic gap significant obstacle efficiently implementing kinds services paper explore techniques vmm passively infer information guest operating system unified buffer cache virtual memory system created prototype implementation techniques inside xen vmm called geiger show accurately infer pages inserted evicted system buffer cache explore nuances involved passively implementing eviction detection previously addressed importance tracking disk block liveness effect file system journaling importance accounting unified caches found modern operating systems case studies show information provided geiger enables vmm implement vmm-level services implement working set size estimator vmm make informed memory allocation decisions show vmm drastically improve hit rate remote storage caches eviction-based cache placement modifying application operating system storage interface case studies hint future inference techniques enable broad class vmm-level functionality categories subject descriptors organization design general terms design measurement performance keywords virtual machine inference permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee asplos october san jose california usa copyright acm reprinted asplos proceedings international conference architectural support programming languages operating systems october san jose california usa introduction virtualizationtechnology increasingly common component serverand desktop pcsystems asboth software hardware support forlow-overhead virtualization develops virtualization included popular commercial systems expect virtualized computing environments ubiquitous virtualization prevalent virtual machine monitor vmm naturally supplants operating system primary resource manager machine vmm attractive target implementing traditionally considered operating system features flexible resource management service device driver isolation load balancing security monitoring fault tolerance transition functionality vmm hasmany potentialbenefits forexample aguestoperating system isolated vmm stable virtual hardware interface services implemented vmm portable guest operating systems vmm place innovative features inserted system guest operating system legacy closedsource finally virtualized environment vmm theonly component thathas totalcontrol oversystem resources make informed resource management decisions pushing functionality layer software stack vmm disadvantages significant problem lack higher-level knowledge vmm referred semantic gap previous work virtualized environments partially recognized dilemma researchers developed techniques infer information guests utilize virtual hardware resources techniques vmm manage resources system effectively reallocating idle page thatcould useit paper describes techniques vmm infer information performance-critical software component operating system buffer cache virtual memory system specifically show vmm carefully observe guest operating system interactions virtual hardware mmu storage devices detect pages inserted evicted operating system buffer cache geiger isa prototype implementation techniques xen virtual machine monitor paper discuss detailsofgeiger simplementationand geiger eviction detection techniques geiger inferencing techniques vmm similar chen withina pseudo-device driver evaluation focuses geiger techniques needed circumstances show unified buffer caches virtualmemory systems found modern operating systems require vmm track disk traffic memory allocations show vmm basic storage system behavior account accurately detect cache eviction vmm track data block live dead disk order avoid reporting spurious evictions show journaling file systems ext linux require vmm distinguish writes journal writes parts storage avoid aliasing problem leads false eviction reporting summary passively detecting cache eventswithinmodern operating systemsrequiresnew sophistication techniques passive inferencing result inaccurate information worse information case studies demonstrate inferred eviction information provided geiger enable services inside vmm case study implement vmmbased working set size estimator complements existing techniques allowing estimation case virtual machine thrashing study explores geiger-inferred evictions vmm enable remote storage caches implement eviction-based cache placement changing application operating system storage interface enhancing adoption feature rest paper organized begin presenting extended motivation eviction information address related work section section describe techniques geiger detail section discusses implementation geiger section evaluate accuracy overhead prototype geiger implementation sections discuss case studies finally section summarize conclude motivation related work vmm understand hosted virtual machines memory section describe contextswhereavmmcan exploitinformation aboutbuffercache promotion eviction events working set size virtualized environment knowing working set size virtual machine allocating amount memory hosted environments popular common vms running simultaneously physical host situation knowing theworking setsize ofeach vmallows thevmm toallocate dynamically re-allocate amount memory competing migrating vms grid computing environment working set size information enables job scheduler intelligently select host adequate amount memory techniques estimating working set size virtual machine explored waldspurger part vmware esx server product esx server technique determine working set size virtual machines full allocation guest begins thrash sampling technique esx server simply reports working set memory allocation fact larger determine true working set size esx technique trial error vmm repeatedly giveathrashing virtualmachine incrementallymorememory re-measure working set size drops system physical memory accommodate full working set trial error method fail contrast vmm detect eviction events virtual machine vmm directly model hit rate virtual machine function amount memory result vmm quickly efficiently determine physical memory give competing virtual machine migrate virtual machine secondary-level caching avirtualized environment knowing contents buffer cache implementing effective secondary-level cache multiple vms run machine thevmmcanmanage ashared secondary cache initsownmemory thevmssharepages additionally hosted legacy system address large amount memory secondary cache enable legacy exceed natural addressing limits finally vmm explicitly communicate remote storage server cache informing itofwhich pages cached withineach designing secondary cache management policy non-trivial secondary storage caches exhibit locality client caches stream filtered client cache fact secondary storage caches size client caches led innovations cache replacement policies cache placement policies promising placement policy called eviction-based placement inserts blocks secondary cache evicted client cache approach make caches overlap leads effective secondary cache utilization eviction-based placement similar micro-architectural victim caches processor cache hierarchy modify eviction-based cache placement straight-forward wong extend block-based storage interface demote operation explicitly notifies interested parties remote storage caches pages evicted client caches goal modify vmm passive techniques infer page evicted page cache vmm explicitly notify storage cache passive eviction detection explored extent exclusive caching storage systems x-ray file system semantic information storage blocks inodes snoop updates file accessed time field knowing files recently accessed x-ray build approximate model client cache x-ray limited 
inferences storage system access block stream exclusive caching work assumed access information chen perform inferencing pseudo-device driver access addresses memory pages read written infer eviction occurred memory page storing disk data reused disk data geiger approach similar chen additional information toa vmm improve itsability infer cache events geiger builds previous work important ways geiger handles guest operating systems implement unified buffer cache virtual memory system unified buffer cache events disk evict pages geiger techniques handle anonymous memory allocation cache evictions geiger recognizes blocks disk freed block free disk reuse memory buffer recently held contents imply eviction distinction important counterproductive cache blocks file system believes free geiger supports journaling file systems file system writes block distinct locations disk occurs journaling file system geiger avoids reporting false evictions additional techniques geiger handle range situations occur modern operating systems geiger techniques section discuss techniques geiger begin providing relevant background virtual machine monitors andcontinue describing thebasictechniques geigerusestoinfer page cache promotion eviction describe geiger performs complex inferences handles unified buffer caches virtual memory systems present inallmodernoperating systems due storage system interactions background virtualization studied technique recently revitalized ubiquitous environment virtualized environment thin layer software virtual machine monitor vmm virtualizes components host computer system allowing guest operating systems safely share resources transparently key feature virtualized environment guest operating systems execute unprivileged mode processor vmm runs full privilege guest accesses sensitive system components mmu peripherals exception control transferred vmm point vmm decide action emulate behavior privileged hardware operation software paper advantage vmm entry points observe interesting activity geiger observes architecturally visible events page faults page table updates disk reads disk writes infer occurrence buffer cache eviction promotion major benefit observing entry points performance adding small piece observation code points induces overhead basic techniques buffer cache promotion occurs disk page added cache buffer cache eviction occurs cache page freed operating system previous contents remain reloaded disk eviction occurs contents anonymous page written swap partition page freed similarly eviction occurs page read file system freed writing back disk data reloaded original location disk eviction occur frees page contents lost anonymous page process exits detect promotion eviction geiger performs tasks geiger tracks contents page diskand ifso wecalltheondisk location memory page page disk location adl geiger detect page freed describe steps turn disk locations geiger associates disk location physical memory page disk location adl issimply pair device block offset representing recent disk location vmm associate page vmm associates disk location memory page page involved disk read write operation page target read disk location page similarly page source write disk location page associations persist replaced association memory page freed relevant disk blocks freed vmm virtualizes disk disk reads writes initiatedby guest areexplicitly visibleto thevmm special action part vmm required establish adl page tocorrectlyinvalidate anadlwhen thedisk block refers longer requires detecting disk block freed discuss section detecting page reuse geiger mustalso determine memory page freed guest explicitly notify vmm frees page difference active free page entry private data structure free list bitmap assume vmm detailed os-specific information required locate interpret data structures detecting page freed geiger detects page reused reuse implies page freed proxy page free event geiger numerous heuristics detect page reused heuristic corresponds scenario guest allocates page memory geiger detects page allocation newly allocated page current adl geiger signals previous contents page defined adl evicted basic techniques geiger monitoring disk reads disk writes builds previous work chen monitors reads writes device driver disk read geiger disk reads infer page allocated page read disk page allocated buffer cache allocated page current adl refers disk location read geiger reports page previous contents evicted adl affected page updated point disk location consequence kind eviction disk write geiger disk writes infer page allocated full page data written disk page reside page cache allocate page buffer data asynchronously written disk geiger detects case observing disk writes signaling eviction write source page current adl target disk location write note thatifa previous read writecaused diskblock toalready exist cache geiger erroneously signal duplicate eviction page adl change read-eviction heuristic adl affected page updated refer target disk location techniques unified caches techniques previous research work old-style file system buffer caches distinct virtual memory system virtually modern operating systems including linux bsd solaris windows unified buffer cache virtual memory system unification complicates inferences geiger detect page reuse additional cases virtual memory system introduce detection techniques microbenchmark description read evict sequentially reads section file larger memory multiple times write evict sequentially writes file larger memory repeated multiple times cow evict allocates memory buffer approximately size physical memory writes virtual page ensure physical page allocated forks writes page child allocation evict allocates memory buffer exceeds size memory writes virtual page ensure page allocated figure microbenchmark workloads table describes microbenchmarks isolate specific type page eviction application description read write cow alloc dbench file system benchmark simulating load network file server mogrify scales converts large bitmap image osdl-dbt tpc-w-like web commerce benchmark simulating web purchase transactions online store spc web search storage performance council block device traces web search engine server traces replayed real file system figure application workloads table describes application workloads reports percentage total eviction events caused eviction type copy write copy-on-write cow technique widely operating systems implement efficient read sharing memory page shared cow marked read-only process virtual address space shares processes attempts write cow-shared page action page fault operating system transparently allocates private page copies data page page subsequently writable virtual memory mapping established refers page private copy requires allocation free page lead page reuse geiger detects page reuse occurs result cow observing page faults page table updates geiger detects page fault write read-only page saves affected virtual address page table entry small queue short time guest creates writable mapping virtual address physical page geiger infers physical page newly allocated newly allocated page active adl geiger signals eviction modified private copy existing page disk location allocation modern operating systems allocate memory lazily application requests memory brk anonymous mmap immediately allocate physical memory virtual address range reserved physical memory allocated on-demand page accessed property means physical memory allocation occurs context servicing page 
fault similar cow heuristic geiger observes page faults due guest accessing virtual page virtualto-physical mapping saves affected virtual address small queue short time guest creates writable mapping faulting virtual address geiger infers page allocation newly allocated physical page current adl geiger signals eviction techniques storage storage systems introduce nuances inferences made geiger file system features journaling lead aliasing problem fact disk blocks deleted leads problem liveness detection describe issues geiger handles turn journaling basic write heuristic signals eviction contents page adl written location disk match adl page adl written disk location eviction reported contents disk location basic write heuristic over-reports evictions cases data written buffer cache page multiple disk locations view aliasing problem page wrongly disk addresses journaling file systems linux ext reiserfs jfs andxfs diskfrom cache page journal location fixed disk location inthe worst-case journaling scenario dataand metadata written journal actual number write evictions reported common case metadata-only journaling smaller penalty incurred negative effect journaling virtual memory mitigated vmm identifies reads writes file system journal straightforward systems journal separate easily identifiable partition file file system partition made file system superblock avoid problem journal aliasing geiger monitors disk addresses write requests ignores writes directed journal block liveness geiger signals page evicted page current adl blocks adl refers deallocated disk time adl mapping established geiger detects page reused case geiger falsely report eviction adl exists data adl refers deallocated longer accessible problem block liveness lead large numbers false evictions workloads files regularly deleted truncated processes die significant parts virtual memory swapped disk eviction count time allocation eviction actual inferred eviction count time cow eviction actual inferred eviction count time read eviction actual inferred eviction count time write eviction actual inferred figure eviction inference counts figure compares inferred actual eviction counts time microbenchmarks isolate eviction type inferred geiger fraction evictions lag time allocation lag fraction evictions lag time cow lag fraction evictions lag time read lag fraction evictions lag time write lag figure eviction lag figure shows cumulative lag distribution microbenchmarks isolate eviction type file systems virtual machine monitor passively track file systemblock liveness inthesameway asmartdisksystem cantrack block liveness allocation state file system block typically noted on-disk structure bitmap file system superblock stored fixed location disk locate bitmap structures examining guest operating system writes on-disk areas vmm snoop file system determine disk blocks adl refers freed blocks adl refers deallocated adl invalidated future reuse affected page misinterpreted eviction implementing block liveness observing disk writes significant drawback substantial lag file system structure allocation bitmap updated memory written disk operating systems interval seconds geiger observe file system blocks page adl points deallocated page reused false eviction occur timeliness block deallocation notification important avmmcan deallocation notification tracking updates in-memory versions allocation bitmaps locations bitmaps vmm observe bitmaps loaded disk memory time vmm mark buffers read-only guest updates in-memory bitmap minor page fault occur vmm observe fault due attempted bitmap update respond invalidating affected adls geigerimplements thisstyleof in-memory block livenesstracking bitmap blocks identified reading parsing file system superblock forknown filesystem types pagesused tocache file system allocation bitmaps marked read-only memory geiger write page detected due page protection fault effect faulting instruction emulated guest memory register state faulting instruction skipped observed handled vmm overhead block liveness tracking low spite additional minor page faults due low frequency disk bitmap updates sivathanu embedding file system layout information format superblock vmm reasonable technique commonly file systems on-disk data structure formats file systems change slowly vmm provided layout information commonly file systems information expected remain valid long time on-disk format ext changed introduction longer interval typical system software upgrade cycle swapspace liveness tracking techniques geiger file system partitions apply disk space swap area rule swap space on-disk data structures track block allocation data swap expected persist system restarts swap allocation information managed exclusively volatile system memory swap liveness tracking techniques found effective workloads preventing false evictions due adls point deallocated swap space technique invalidates adl points set disk blocks overwritten disk blocks overwritten data adl refers destroyed adl invalidation isappropriate thistechnique isimplemented maintaining reverse mapping cached disk blocks adls technique makes implicitly obtained process lifetime information provided antfarm accurate information guest processes mapping memory pages owning process adls invalidated process exits specifically adl page belonging dead process points swap space disk block invalidated technique appears promising buthas fully implemented inthe current version ofgeiger implementation geiger implemented extension xen virtual machine monitor version xen open source virtual machine monitor intel architecture xen paravirtualized processor interface enables lower overhead virtualization expense porting system software explicitly make feature xen mechanisms describe equally applicable conventional virtual machine monitor vmware geiger consists set patches xen hypervisor xen block device backends concentrated handlers events page faults page table updates block device reads writes geiger patches consist approximately workload false neg false pos read evict write evict cow evict alloc evict figure microbenchmark heuristic accuracy table reports false positive false negative ratios complete set eviction heuristics microbenchmark workloads workload journal opt journal opt neg pos neg pos journal metadata data figure effect journaling table reports false positive false negative ratios write-eviction microbenchmark workload run journaling metadata journaling ordered mode data journaling linux ext file system table shows benefits turning geiger specialization detect writes journal lines code files files xen hypervisor linux kernel required small order implement instrumentation tracing allexperiments paper performed witha ghzpentiumivprocessor gbofsystem memory ata disk drives linux kernel version xen control domain linux kernel version unprivileged domains ext ext file system depending theexperiment thexencontrol domain configured memory noted unprivileged guest virtualmachine isassigned mbof memory evaluation section evaluate ability geiger accurately infer page cache evictions promotions occurring guest operating systems begin describing workloads metrics evaluate geiger set microbenchmarks application workloads conclude measuring overheads geiger imposes system workloads experimental evaluation geiger sets workloads workload set consists microbenchmarks microbenchmarks constructed generate specific type page cache eviction read write copy-on-write cow orallocate thesemicrobenchmarks isolate geiger ability track evictions due specific events microbenchmarks detail figure set workloads consists application benchmarks represent realistic workloads workload mix eviction types read write cow allocation figure lists application workloads breakdown eviction types generated application workloads 
stress geiger ability track evictions occur reasons metrics methodology evaluating accuracy geiger compare trace evictions signaled geiger trace evictions produced guest operating system modified linux kernel generate trace guest operating system complete information pages evicted comparison ideal eviction detector workload geiger opts false neg false pos dbench block liveness dbench block liveness mogrify block liveness mogrify block liveness tpc-w spc web figure application heuristic accuracy table reports false positive false negative ratios geiger application workloads dbench mogrify workloads evaluate geiger optimizations detect block live disk alloc-evict thrash dbench thrash alloc-evict thrash dbench thrash runtime geiger runtime overhead unmodified xen geiger figure geiger runtime overhead figure shows geiger imposes small runtime overheads workloads stress inference heuristics eviction records traces physical memory address disk address evicted data time stamp weconsider thefirstmetric simply eviction count reported geiger compared reported guest time metric detection lag time eviction takes place detected geiger finally metric detection accuracy tracks percentage records inferred actual traces match one-toone mapping report percentage false negatives actual evictions detected geiger false positives toos-reportedevictions microbenchmarks begin running workloads consisting microbenchmarks figure shows resulting eviction count time-lines microbenchmarks eviction counts inferred geiger closely match actual counts depending workload interesting differences occur cow workload guest reclaims pages groups leading slight stair-step eviction pattern geiger inferences lag slightly case write workload guest begins evicting pages early continues evict eagerly experiment pages reused time geiger inferences based page reuse eviction detected page reused inferred evictions lag noticeably actual evictions caused writes figure shows thecumulative microbenchmarks expected lag times read cow allocation eviction concentrated small values lag times write microbenchmark concentrated seconds due operating system eager reclamation behavior figure reports geiger detection accuracy false negatives false positives workloads false negatives uncommon worst fewer total number evictions missed geiger false positives common worst geiger over-reports inferred evictions final microbenchmark experiment explore geiger ability detect aliased writes file system journal write workload stress detection figure shows accuracy geiger specialization disregard write traffic file system journal specialization geiger performs satisfactorily journaling disabled metadata journaled linux ext ordered-mode metadata journaling blocks aliases data journaling blocks aliases result half evictions reported un-specialized geiger false positives contrast full version geiger accurately handles journaling modes linux ext data journaling geiger false positive percentage application benchmarks workloads realistic applications figure reports detection accuracy geiger application workloads workloads false negative ratios small worst case geiger misses evictions reported dbench mogrify workloads interesting behavior false positives block liveness dbench mogrify workloads illustrate benefit geiger attempt track liveness block disk dbench creates deletes files result pages memory reused files disk blocks mogrify large amounts swap allocated deallocated execution vmm change association memory page disk block infer eviction vmm concludes evictions occurred false positives live block detection geiger false positive rate dbench false positive rate mogrify geiger tracks disk block free detect page simply reused previous contents evicted result false positive rate improves dramatically dbench mogrify adequately handle deleteintensive truncate-intensive workloads geiger includes techniques track disk block liveness limitations mentioned previously expect current techniques tracking block liveness swap space adequate situations demonstrate remaining problem microbenchmark crafted results large numbers false positives efforts geiger track block liveness program forces large buffer allocated mmap swapped disk buffer released linux buffer released swap space deallocated geiger detect event additional memory allocated program pages reused adls point deallocated swap space resulting eviction false positive ratio overhead geiger observes events intrinsically visible vmm page faults page table updates disk case disk block liveness tracking additional memory protection figure memrxoperation figure shows schematic cache simulation implemented memrx page evicted guest event detected geiger entry added head series queues queue entries ripple tail queue head reload queue entry removed array entry queue incremented entry tracks sub-queue appears enable fast depth estimation traps requests caused geiger liveness tracking imposes additional minor page fault disk bitmap update occur rarely expect runtime overhead imposed geiger small validate expectation compare runtime workloads running unmodified version xen geiger interested performance regimes regime common case workload sufficient memory evictions occur regime occurs machine thrashing implies evictions taking place geiger inference mechanisms stressed evaluate cases carefully chosen workloads geiger interposes code paths handling page faults page table updates disk microbenchmark allocation-evict figure dbench figure allocation-evict page faults page-table updates stressing portion geiger inference machinery dbench large number file creations reads writes deletes exercise portions geiger heuristics figure shows results experiment shown average runs standard deviation shown error bars largest observed overhead occurs thrashing dbench cases results geiger unmodified xen comparable geiger requires extra space physical memory page track adls prototype amounts bytes memory page testsystem configured physical memory total additional memory allocated vmm leading space overhead approximately space overhead concern substantially reduced preallocated fixed size sparsely-populated data structures prototype hardware trends major microprocessor vendors intel amd ibm begun include optional hardware virtualization features server desktop products reduce overhead imposed virtualization features hide information vmm favor reducing guest-to-vmm transitions cases page faults guest page table updates vmm invoked geiger page fault page table update information detect cache eviction events absence impact geiger functionality future benchmark activity sequential sequentially scan section file system file times sequential sequentially scan section allocated virtual memory times random randomly read page-sized blocks file system file times random randomly touch virtual memory pages virtual memory allocation times figure calibrated microbenchmarks table describes microbenchmarks evaluate vmm-memrx techniques required provide geiger-like functionality latest vmm-aware architectures summary order handle modern operating systems unified system caches journaling file systems geiger number sophisticated inferencing techniques microbenchmarks geiger highly accurate measured eviction counts oneto-one eviction accuracy write-intensive workloads geiger experience significant lag detects eviction occurred application workloads geiger swap-intensive workloads additional techniques required avoid detection false evictions due difficulty tracking liveness swap shown geiger full range techniques needed circumstances cow allocation techniques needed handle mogrify tpc-w workloads unified buffer cache live block detection improves accuracy delete-intensive workloads finally writes journal isolated handle file system data journaling case study working set size estimation eviction detection techniques geiger implementing number pieces functionality case study show geiger implement memrx vmm tracks working sets guest vms begin describing implementation memrx present 
performance results memrx implementation previous research waldspurger esx server shown vmm determine system working set size memory footprint fits physical memory memrx complements esx server technique enabling vmm determine working set size thrashing virtual machine memrx accomplishes thisusing geiger toobserve evictions subsequent reloads guest operating system buffer cache memrx quantifies number memory accesses transformed misses hits memory sizes memrx simulates page cache behavior virtual machine memory increment method similar patterson ghost buffering figure shows schematic page cache simulation implemented memrx page evicted page location disk inserted head queue maintained lru order memrx subsequent evictions push previous deeper inthe queue previously evicted page read disk page removed queue distance head queue computed distance approximately equal number evictions place page eviction subsequent reload memrx compute amount memory required prevent original eviction taking place sizepage information compute miss-ratio curve working set size read miss-ratio curve locating curve primary knee evaluation evaluate accuracy memrx measure working set size microbenchmark workloads working set size approximately table lists microbenchmarks actions perform working set size approximately virtual machine configured memory compare working set size predicted memrx working set size determined trialand errorfor realisticapplication workloads mogrify dbench figure shows predicted actual miss ratio curves microbenchmark workloads miss ratio curve shows fraction capacity cache misses occurring smallest memory configuration remain misses larger memory configurations predicted curve calculated memrx measurements single run smallest memory configuration simulating page cache behavior guest operating system on-line larger memory configurations increments actual curve calculated running workload noted memory sizes counting actual capacity misses page cache calibrated tests show memrx locate working set size simple workloads accurately prediction made memrx identical found direct measurement trial error result surprising simple workloads geiger incurs eviction false positives figure shows results application workloads mogrify dbench leftmost graphs show predicted actual missratiocurves cases inferred working set size predicted memrx slightly larger actual working set size found trial error determine discrepancy due geiger false positive negative evictions lag memrx cache simulation error implemented memrx linux compared predicted actual miss ratio curves produced version operating system memrx access precise eviction promotion information eliminates geiger source error rightmost graphs figure show miss ratio curves obtained mogrify dbench workloads operating system implementation memrx dbench workload version memrx shows deviation produced memrx vmm leads conclude deviation memrx simulation error memrx models guest buffer cache strict lru policy match policy linux akin difference modeled policy true policy leads simulation errors shown case mogrify os-based miss ratio curve matches actual curve closely leading error observed vmmpredicted working set size due small inference errors imposed geiger granularity experiment summary information provided geiger enabling vmm estimate working set sizes thrashing vms predictions made memrx accurate highly allocating memory competing vms single machine selecting target host virtual machine migration miss ratio memory size sequential predicted actual miss ratio memory size sequential predicted actual miss ratio memory size random predicted actual miss ratio memory size random predicted actual figure vmm-memrxpredicted actual miss ratio figure shows miss ratio predicted vmm-memrx actual miss ratio measured varying memory sizes working set marked vertical dashed line miss ratio memory size dbench vmm predicted actual miss ratio memory size mogrify vmm predicted actual miss ratio memory size dbench predicted actual miss ratio memory size mogrify predicted actual figure application predicted actual miss ratio figure shows miss ratio curve predicted memrx actual miss ratio measured varying memory sizes application workloads results memrx implemented vmm left memrx implemented shown cache hit ratio cache size mogrify eviction-os eviction-geiger eviction-buffer demand cache hit ratio cache size dbench cache hit ratio cache size tpc-w cache hit ratio cache size spc web search figure secondary cache hit ratio figure compares cache hit ratio secondary storage cache workloads demand placement demand eviction placement based inferred evictions eviction-buffer eviction-geiger eviction placement based actual evictions eviction-os experiments performed cache sizes case study eviction-based cache placement case study show geiger convey eviction information secondary cache basic idea vmm geiger infer pages evicted buffer cache sends information demoteoperation tothe storage server ispotentially remote storage server explicitinformation perform eviction-based cache placement implementation implementation eviction-based secondary cache components vmm interposes virtual block device interface provided byxen tosee block request streamgenerated workload vmm geiger infer blocks evicted page cache events communicated remote storage server simulate behavior storage server actual trace gathered running geiger workload input refer approach eviction-geiger evaluate implementation compare alternatives approach eviction-os operating system modified report actual evictions represents ideal case approach eviction-buffer vmm performs eviction detections client buffer addresses chen read write evictions finally simulate storage cache information client evictions performs traditional demand-based placement cases lru-based replacement policy evaluation application workloads listed figure evaluate vmm-implementation eviction-based cache placement workload remote caches evaluate placement policies eviction-os evictiongeiger eviction-buffer demand metric cache hit ratio figure shows graphs cache hit ratio cache size workloads cache policies cases geiger eviction-based placement outperform demand-based placement significantly largest gains occur moderate cache sizes working set application fits client cache storage cache individually fit aggregate cache geiger evictionbased placement improve cache hit rate percentage points workloads mogrify workload secondary cache size cache hit ratio demand placement eviction-based placement secondary cache size large full system working set geiger eviction-based placement perform similarly demandbased placement case spc web search traces exhibit locality results included completeness workload dbench eviction-based placement support outperforms inferred evictions geiger secondary cache size observe difference hit rates percentage points performance difference due significant time lag actual inferred write-eviction events approximately seconds events experiment inferred evictions delayed secondary cache loses opportunity place block prior block referenced client cache miss occurs eviction-based approaches perform significantly demand-based placement buffer fact eviction-buffer performs significantly worse straight-forward demand-based placement problem occurs eviction-buffer detect fewer evictions occur large false negatives workloads mogrify tpc-w significant number non-i based evictions occur missing evictions lead poorer cache performance missing evictions problem large secondary caches blocks effectively adequate cache space case tpc-w missing eviction events change cache hit rate percentage points mogrify difference points summary geiger effectively notify secondary cache evictions performed clients confirmed studies secondary caches eviction-based placement perform demand-based placement results show eviction information provided geiger good provided directly 
modified exception occurs significant lag occurs time actual eviction inference case geiger enables hit rates simple demand-based placement eviction-based placement essential miss evictions clients eviction detection based reads writes miss important evictions leading hitrates thatare worse simple demand-based placement full set techniques geiger buffer cache inferences conclusion backend servers desktop pcs virtualization commonplace virtual machine monitor sole resource manager system pieces interesting functionality migrate operating system vmm key enabling vmm-level functionality information knowledge os-level constructs typically needed implement features paper explored techniques required make inferences pages added removed buffercache wehave found thatcertain key featuresof modern operating systems including unified buffer caches andvirtualmemorysystems journalingfilesystems diskblock liveness techniques efficient implement prototype case studies working set size estimator eviction-based cache placement second-level caches inferring information boundary vmm itsguestoperating systemsisapowerful technique thatenables systems innovation implemented portably vmm accurate timely general techniques made successfully applied commercial domain acknowledgments anonymous reviewers thoughtful comments suggestions research sponsored sandia national laboratories doctoral studies program nsf ccritr- cnsand generous donations network appliance emc arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca munich germany june ballmer keynote address microsoft management summit april jfs overview ibm developerworks library jfs html bressoud schneider hypervisor-based fault tolerance sosp proceedings fifteenth acm symposium operating systems principles pages acm press bugnion devine rosenblum disco running commodity operating systems scalable multiprocessors proceedings acm symposium operating systems principles sosp pages saint-malo france october chen noble virtual real hotos proceedings eighth workshop hot topics operating systems page ieee computer society chen ahang zhou scott schiefer empirical evaluation multi-level buffer cache collaboration storage systems proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics banff canada june chen zhou eviction-based placement storage caches proceedings usenix annual technical conference usenix pages san antonio texas june clark fraser hand hansen jul limpach pratt warfield live migration virtual machines proceedings symposium networked systems design implementation nsdi boston massachusetts denning working set model program behavior communications acm denning working sets past present ieee transactions software engineering sejanuary dragovic fraser hand harris pratt warfield barham neugebauer xen art virtualization proceedings acm symposium operating systems principles sosp bolton landing lake george york october figueriredo dinda fortes case grid computing virtual machines proceedings international conference distributed computing systems icdcs fraser hand neugebauer pratt warfield williamson safe hardware access xen virtual machine monitor oasis asplos workshop garfinkel pfaff chow rosenblum boneh terra virtual machine-based platform trusted computing proceedings acm symposium operating systems principles sosp bolton landing lake george york october goldberg survey virtual machine research ieee computer gum system extended architecture facilities virtual machines ibm journal research development november imagemagick studio llc imagemagick image processing software http imagemagick intel corporation intel virtualization technology specification ftp download intel technology computing vptech pdf johnson shasha low-overhead high performance buffer management replacement algorithm proceedings international conference large databases vldb pages santiago chile september jones arpaci-dusseau arpaci-dusseau antfarm tracking processes virtual machine environment proceedings usenix annual technical conference usenix boston massachusetts june jouppi improving direct-mapped cache performance addition small fully-associative cache prefetch buffers proceedings annual international symposium computer architecture isca pages seattle washington king chen backtracking intrusions proceedings acm symposium operating systems principles sosp banff canada october muntz honeyman multi-level caching distributed file systems cache ain nuthin trash proceedings usenix winter conference pages january open source development labs osdl database test suite http osdl lab activities kernel testing osdl database test suite patterson gibson ginting stodolsky zelenka informed prefetching caching proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december reiser reiserfs namesys sapuntzakis chandra pfaff chow lam rosenblum optimizing migration virtual computers proceedings symposium operating systems design implementation osdi pages boston massachusetts december sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau lifeordeath atblock level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid proceedings usenix symposium file storage technologies fast san francisco march storage performance council spc web search engine storage traces http traces umass storage sugerman venkitachalam lim virtualizing devices vmware workstation hosted virtual machine monitor proceedings usenix annual technical conference usenix boston massachusetts june sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january tridgell dbench filesystem benchmark http samba ftp tridge dbench tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june tweedie ext journaling filesystem olstrans sourceforge net release ols -ext ols -ext html july waldspurger memory resource management vmware esx server proceedings symposium operating systems design implementation osdi boston massachusetts december whitaker shaw gribble scale performance denali isolation kernel proceedings symposium operating systems design implementation osdi boston massachusetts december wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june zhao zhang figueriredo distributed file system support virtual machines grid computing proceedings high performance distributed computing hpdc july zhou philbin multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference usenix pages boston massachusetts june 
journal-guided resynchronization software raid timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison abstract investigate problem slow scan-based software raid resynchronization restores consistency system crash augmenting raid layer quicken process leverage functionality present journaling file system analyze linux ext introduce mode operation declared mode guarantees provide record outstanding writes case crash utilize information augment software raid interface verify read request repairs redundant information block combination features provide fast journal-guided resynchronization evaluate effect journal-guided resynchronization find improved software raid reliability availability crash suffering performance loss normal operation introduction providing reliability storage level entails raid prevent data loss case disk failure high-end storage arrays specialized hardware provide utmost performance reliability solutions multimillion dollar price tags infeasible small medium businesses organizations cost-conscious users turn commodity systems collection disks house data popular low-cost solution reliability arena software raid range platforms including linux solaris freebsd windows-based systems software-based approach attractive specialized cluster-in-a-box systems instance emc centera storage system built cluster commodity machines linux software raid manage disks life storage arrays pay case software raid lack non-volatile memory introduces consistent update problem specifically write issued raid layer disks updated consistent manner possibility crashes makes challenge raidarray untimely crash occurs parity write completes data block written writes issued parallel completed stripe left inconsistent state inconsistency introduces window vulnerability data disk fails stripe made consistent data disk lost automatic reconstruction missing data block based inconsistent parity silently return bad data client hardware raid circumvents problem gracefully non-volatile memory buffering update nvram disks consistently updated hardware-based approach avoids window vulnerability outcome ideal performance reliability excellent current software-based raid approaches performance reliability trade-off made current software raid implementations choose performance reliability simply issue writes disks parallel hoping untimely crash occur crash occur systems employ expensive resynchronization process scanning entire volume discrepancies found repaired large volumes process hours days alternate software raid approach chooses reliability performance applying write-ahead logging array record location pending updates issued systems avoid time-consuming resynchronization recovery raid simply repairs locations recorded log removing window vulnerability high performance cost update raid preceded synchronous write log greatly increasing total load disks solve consistent update problem software raid develop solution high performance reliability global view storage stack leverage functionality layers system assist cases client software raid system modern journaling file system default linux file system ext reiserfs jfs windows ntfs standard journaling techniques maintain consistency file system data structures solve consistent update problem raid level find journaling readily augmented specifically introduce mode operation linux ext declared mode writing permanent locations declared mode records intentions file system journal functionality guarantees record outstanding writes event crash consulting activity record file system blocks midst updated dramatically reduce window vulnerability crash complete process file system communicate information vulnerabilities raid layer purpose add interface software raid layer verify read receiving verify read request raid layer reads requested block mirror parity group verifies redundant information irregularity found raid layer re-writes mirror parity produce consistent state combine features integrate journal-guided resynchronization file system recovery process record write activity vastly decreases time needed resynchronization cases period days mere seconds approach avoids performance reliability trade-off found software raid systems performance remains high window vulnerability greatly reduced general key solution cooperative nature removing strict isolation file system software raid layer subsystems work solve consistent update problem sacrificing performance reliability rest paper organized section illustrates software raid consistent update problem quantifies likelihood crash lead data vulnerability section introduction ext file system operation section analyze ext write activity introduce ext declared mode addition software raid interface merge raid resynchronization journal recovery process section evaluates performance declared mode effectiveness journal-guided resynchronization discuss related work section conclude section consistent update problem introduction task raid maintain invariant data redundant information stores invariants provide ability recover data case disk failure raidthis means mirrored block data parity schemes raidthis means parity block stripe stores exclusive-or data blocks blocks reside disk updates applied atomically maintaining invariants face failure challenging crash occurs write array blocks left inconsistent state mirror successfully written disk data block written parity update note consistent update problem solutions distinct traditional problem raid disk failures failure occurs redundant information array lost data vulnerable disk failure situation solved process reconstruction regenerates data located failed disk failure models illustrate consistent update problem shown figure diagram depicts state single stripe blocks disk raidar- ray time progresses left software raid layer residing machine servicing write data block update parity block machine issues data block write time written disk time machine notified completion time similarly parity block issued time written time notification arrives time data write block time stripe enters window vulnerability denoted shaded blocks time failure disks result data loss stripe data parity blocks exist inconsistent state data residing failed disk reconstructed inconsistency corrected time write failure models possibility independent failures host machine time cpczip zdisk array machine figure failure scenarios diagram illustrates sequence events data block write parity update disk raidarray time progresses left boxes labeled request issued labeled represent completions shaded blocks denote window vulnerability array disks discuss turn relate consequences figure machine failure model includes events operating system crashes machine power losses machine crashes times array remains active stripe left inconsistent state write completes time model disk failure model considers power losses disk array failure occurs time time stripe left vulnerable state note disk failure model encompasses non-independent failures simultaneous power loss machine disks measuring vulnerability determine crash failure leave array inconsistent state instrument linux software raidlayer scsi driver track statistics record amount time write issued stripe write issued stripe measures difference times figure corresponds directly period vulnerability machine failure model record amount time write completion stripe write completion stripe measures difference time time note vulnerability disk failure model occurs time time measurement approximation results slightly overestimate underestimate actual vulnerability depending vulnerable time number writers software raid vulnerability disk failure model write completion machine failure model write issue figure software raid vulnerability graph plots percent time duration experiment inconsistent disk state exists raidarray number writers increases x-axis vulnerabilities due disk failure machine failure plotted separately time takes completion processed host machine finally track number stripes 
vulnerable models calculate percent time stripe array vulnerable type failure test workload consists multiple threads performing synchronous random writes set files array experiments performed intel pentium xeon ghz processor ram running linux kernel machine ibm lzx disks configured software raidarray raid volume sufficiently large perform benchmarks small reduce execution time resynchronization experiments figure plots percent time duration experiment array stripe vulnerable number writers workload increased x-axis expected cumulative window vulnerability increases amount concurrency workload increased vulnerability disk failure model greater dependent response time write requests small number writers disk failure result inconsistent state higher concurrency array exists vulnerable state length experiment period vulnerability machine failure model lower depends processing time needed issue write requests experiment vulnerability reaches approximately higher concurrencies ability issue requests impeded full disk queues case machine vulnerability depend disk response time increase solutions solve problem high-end raid systems make non-volatile storage nvram write request received log request data written nvram updates propagated disks event crash log records data present nvram replay writes disk ensuring consistent state array functionality expense terms raw hardware cost developing testing complex system software raid hand frequently employed commodity systems lack non-volatile storage system reboots crash record write activity array indication raid inconsistencies exist linux software raid rectifies situation laboriously reading contents entire array checking redundant information correcting discrepancies raidthis means reading data mirrors comparing contents updating states differ raidscheme stripe data read parity calculated checked parity disk re-written incorrect approach fundamentally affects reliability availability time-consuming process scanning entire array lengthens window vulnerability inconsistent redundancy lead data loss disk failure additionally disk bandwidth devoted resynchronization deleterious effect foreground traffic serviced array exists fundamental tension demands reliability availability allocating bandwidth recover inconsistent disk state reduces availability foreground services giving preference foreground requests increases time resynchronize observed brown patterson default linux policy addresses trade-off favoring availability reliability limiting resynchronization bandwidth disk slow rate equate days repair time vulnerability moderately sized arrays hundreds gigabytes figure illustrates problem plotting analytical model resynchronization time disk array raw size array increases x-axis disks default linux policy minutes time scan repair gigabyte disk space equates half days terabyte capacity disregarding availability array modern interconnects approximately hour full bandwidth resynchronize terabyte array days days hours hour mins time raw array size software raid resynchronization time linux default disk gigabit ethernet sata fibre channel ultra scsi serial attached scsi figure software raid resynchronization time graph plots time resynchronize disk array raw capacity increases x-axis solution problem add logging software raid system manner similar discussed approach suffers drawbacks logging array disks decrease performance array interfering foreground requests high-end solution discussed previously benefits fast independent storage form nvram adding logging maintaining acceptable level performance add considerable complexity software instance linux software raid implementation buffering discarding stripes operations complete logging solution buffer requests significantly order batch updates log improve performance solution perform intent logging bitmap representing regions array mechanism solaris volume manager veritas volume manager provide optimized resynchronization implementation linux software raidis development merged main kernel logging array approach suffer poor performance instance linux implementation performs synchronous write bitmap updating data array ensure proper resynchronization performance improved increasing bitmap granularity cost performing scanbased resynchronization larger regions software raid layer storage hierarchy configuration modern journaling file system layer logging disk updates maintain consistency on-disk data structures sections examine journaling file system solve software raid resynchronization problem ext background section discuss linux ext file system operation data structures details analysis write activity description modifications support journal-guided resynchronization section focus ext techniques general apply journaling file systems reiserfs jfs linux ntfs windows linux ext modern journaling file system aims complex on-disk data structures consistent state file system updates written log called journal journal records stored safely disk updates applied home locations main portion file system updates propagated journal records erased space occupied re-used mechanism greatly improves efficiency crash recovery crash journal scanned outstanding updates replayed bring file system consistent state approach constitutes vast improvement previous process fsck relied full scan file system data structures ensure consistency natural make journaling mechanism improve process raid resynchronization crash modes ext file system offers modes operation data-journaling mode ordered mode writeback mode data-journaling mode data metadata written journal coordinating updates file system strong consistency semantics highest cost data written file system written journal home location ordered mode ext default writes file system metadata journal file data written directly home location addition mode guarantees strict ordering writes file data transaction written disk metadata written journal committed guarantees file metadata data block written mechanism strong consistency data-journaling mode expense multiple writes file data writeback mode file system metadata written journal ordered mode file data written directly home location unlike ordered mode writeback mode ordering guarantees metadata data offering weaker consistency instance metadata file creation committed journal file data written event crash journal recovery restore file metadata contents filled arbitrary data writeback mode purposes weaker consistency lack write ordering transaction details reduce overhead file system updates sets grouped compound transactions transactions exist phases lifetimes transactions start running state file system data metadata updates current running transaction buffers involved linked in-memory transaction data structure ordered mode data running transaction written time kernel pdflush daemon responsible cleaning dirty buffers periodically running transaction closed transaction started occur due timeout synchronization request transaction reached maximum size closed transaction enters commit phase buffers written disk home locations journal transaction records reside safely journal transaction moves checkpoint phase data metadata copied journal permanent home locations crash occurs checkpoint committed transaction checkpointed journal recovery phase mounting file system checkpoint phase completes transaction removed journal space reclaimed journal structure tracking contents journal requires file system structures journal superblock stores size journal file pointers head tail journal sequence number expected transaction journal transaction begins descriptor block lists permanent block addresses subsequent data metadata blocks descriptor block needed depending number blocks involved transaction finally commit block signifies end transaction descriptor blocks commit blocks begin magic header sequence number identify transaction design implementation goal resynchronization correct raid inconsistencies result system crash failure identify outstanding write requests time crash significantly narrow range blocks inspected result faster resynchronization 
improved reliability availability hope recover record outstanding writes file system journal end begin examining write activity generated phase ext transaction ext write analysis section examine ext transaction operations detail emphasize write requests generated phase characterize disk states resulting crash specifically classify write request targeting location unknown location bounded location based record activity journal goal restarting system failure recover record outstanding write requests time crash running ext ordered mode pdflush daemon write dirty pages disk transaction running state crash occurs state affected locations unknown record ongoing writes exist journal commit ext writes un-journaled dirty data blocks transaction home locations waits complete step applies ordered mode data datajournaling mode destined journal crash occurs phase locations outstanding writes unknown ext writes descriptors journaled data metadata blocks journal waits writes complete ordered mode metadata blocks written journal blocks written journal data-journaling mode system fails phase specific record ongoing writes exist writes bounded fixed location journal ext writes transaction commit block journal waits completion event crash outstanding write bounded journal block type data-journaling mode superblock fixed location journal bounded fixed location home metadata journal descriptors home data journal descriptors block type ordered mode superblock fixed location journal bounded fixed location home metadata journal descriptors home data unknown table journal write records table lists block types written transaction processing locations determined crash checkpoint ext writes journaled blocks home locations waits complete system crashes phase ongoing writes determined descriptor blocks journal affect locations ext updates journal tail pointer superblock signify completion checkpointed transaction crash operation involves outstanding write journal superblock resides fixed location recovery ext scans journal checking expected transaction sequence numbers based sequence journal superblock records committed transaction ext checkpoints committed transactions journal steps write activity occurs locations table summarizes ability locate ongoing writes crash data-journaling ordered modes ext case data-journaling mode locations outstanding writes determined bounded crash recovery journal descriptor blocks fixed location journal file superblock existing ext datajournaling mode amenable assisting problem raid resynchronization side data-journaling typically performance ext family ext ordered mode hand data writes permanent home locations recorded journal data structures located crash recovery address deficiency modified ext ordered mode declared mode ext declared mode previous section concluded crash occurs writing data directly permanent location ext ordered mode journal record outstanding writes locations raid level inconsistencies caused writes remain unknown restart overcome deficiency introduce variant ordered mode declared mode declared mode differs ordered mode key guarantees write record data block resides safely journal location modified effectively file system declare intent write permanent location issuing write track intentions introduce journal block declare block set declare blocks written journal beginning transaction commit phase collectively list permanent locations data blocks transaction written construction similar descriptor blocks purpose descriptor blocks list permanent locations blocks journal declare blocks list locations blocks journal descriptor commit blocks declare blocks begin magic header transaction sequence number declared mode adds single step beginning commit phase proceeds declared commit ext writes declare blocks journal listing permanent data locations written part transaction waits completion ext writes un-journaled data blocks transaction home locations waits complete ext writes descriptors metadata blocks journal waits writes complete ext writes transaction commit block journal waits completion declare blocks beginning transaction introduce additional space cost journal cost varies number data blocks transaction case declare block added data blocks space overhead worst case declare block needed transaction single data block investigate performance consequences overheads section implementing declared mode linux requires main guarantee data buffers written disk declared journal accomplish refrain setting dirty bit modified pages managed file system prevents pdflush daemon eagerly writing buffers disk running state mechanism metadata buffers data buffers data-journaling mode ensuring written written journal track data buffers require declarations write declare blocks beginning transaction start adding declare tree in-memory transaction structure ensure declared mode data buffers tree existing data list beginning commit phase construct set declare blocks buffers declare tree write journal writes complete simply move buffers declare tree existing transaction data list tree ensures writes occur efficient order sorted block address point commit phase continue modification implementation minimizes shared commit procedure ext modes simply bypass empty declare tree software raid interface initiating resynchronization file system level requires mechanism repair suspected inconsistencies crash viable option raidarrays file system read re-write blocks deemed vulnerable case inconsistent mirrors newly written data data restored block achieves results current raidresynchronization process raidlayer imposes ordering mirrored updates differentiate data data chooses block copy restore consistency read re-write strategy unsuitable raidhowever file system re-writes single block desired behavior raid layer calculate parity entire stripe data raid layer perform read-modify-write reading target block parity re-calculating parity writing blocks disk operation depends consistency data parity blocks reads disk consistent produce incorrect results simply prolonging discrepancy general interface required file system communicate inconsistencies software raid layer options interface requires file system read vulnerable block re-write explicit reconstruct write request option raid layer responsible reading remainder block parity group re-calculating parity writing block parity disk dissuaded option perform unnecessary writes consistent stripes vulnerabilities event crash opt add explicit verify read request software raid interface case raid layer reads requested block rest stripe checks make parity consistent newly calculated parity written disk correct problem linux implementation verify read request straight-forward file system wishes perform verify read request marks buffer head raid synchronize flag receiving request software raidlayer identifies flag enables existing synchronizing bit stripe bit perform existing resynchronization process presence read entire stripe parity check functionality required verify read request finally option added software raidlayer disable resynchronization crash significant modification strict layering storage stack raid module asked entrust functionality component good system apprehensive software raid implementation delay efforts hopes receiving verify read requests file system requests arrive start resynchronization ensure integrity data parity blocks recovery resynchronization ext data-journaling mode declared mode guarantees accurate view outstanding write requests time crash restart utilize information verify read interface perform fast file system guided resynchronization raid layer make file system journal ordering constraints operations combine process journal recovery dual process file system recovery raid resynchronization proceeds recovery resync ext performs verify reads superblock journal superblock ensuring consistency case written crash ext 
scans journal checking expected transaction sequence numbers based sequence journal superblock records committed transaction committed transaction journal ext performs verify reads home locations listed descriptor blocks ensures integrity blocks undergoing checkpoint writes time crash transaction examined checkpoints occur order checkpointed transaction removed journal processed note verify reads place writes replayed guarantee parity up-to-date adding explicit reconstruct write interface mentioned earlier negate step process ext issues verify reads committed transaction head journal length maximum transaction size corrects inconsistent blocks result writing transaction journal reading ahead journal ext identifies declare blocks descriptor blocks uncommitted transaction descriptor blocks found performs verify reads permanent addresses listed declare block correcting data writes outstanding time crash declare blocks transactions descriptors presence constitutes evidence completion data writes permanent locations ext checkpoints committed transactions journal section implementation re-uses existing framework journal recovery process issuing verify reads means simply adding raid synchronize flag buffers reading journal replaying blocks verify reads locations listed descriptor blocks handled replay writes processed journal verify reads declare block processing uncommitted transaction performed final pass journal recovery bandwidth random write performance ext ordered sorted ext declared ext ordered ext journaled slowdown amount written figure random write performance top graph plots random write performance amount data written increased x-axis data-journaling mode achieves writing data bottom graph shows relative performance declared mode compared ordered mode sorting evaluation section evaluate performance ext declared mode compare ordered mode datajournaling mode hope declared mode adds overhead writing extra declare blocks transaction performance evaluation examine effects journal-guided resynchronization expect greatly reduce resync time increase bandwidth foreground applications finally examine complexity implementation ext declared mode begin performance evaluation ext declared mode microbenchmarks random write sequential write test performance random writes existing file call fsync end experiment ensure data reaches disk figure plots bandwidth achieved ext mode amount written increased x-axis graphs plot experimental trials identify points interest graph data-journaling mode underperforms ordered mode amount written increases note data-journaling mode achieves writing data random write stream transformed large sequential write fits journal amount data written increases outgrows size journal performance datajournaling decreases block written journal home location ordered mode garners performance writing data directly permanent location bandwidth sequential write performance ext ordered ext declared ext journaled slowdown amount written figure sequential write performance top graph plots sequential write performance amount data written increased x-axis bottom graph shows relative performance declared mode compared ordered mode find declared mode greatly outperforms ordered mode amount written increases tracing disk activity ordered mode reveals part data issued disk sorted order based walking dirty page tree remainder issued unsorted commit phase attempts complete data writes transaction adding sorting commit phase ordered mode solves problem evidenced performance plotted graph rest performance evaluations based modified version ext ordered mode sorted writing commit finally bottom graph figure shows slowdown declared mode relative ordered mode sorting performance modes extremely close differing experiment tests sequential write performance existing file figure plots performance ext modes amount written increased x-axis fsync ensure data reaches disk ordered mode declared mode greatly outperform data-journaling mode achieving compared bottom graph figure shows slowdown ext declared mode compared ext ordered mode declared mode performs ordered mode data points disk traces reveal performance loss due fact declared mode waits fsync begin writing declare blocks data ordered mode begins writing data disk slightly earlier declared mode alleviate delay implement early declare mode begins writing declare blocks journal data blocks modified fill declare block operations sprite microbenchmark create phase ext ordered ext declared ext journaled slowdown number files figure sprite create performance top graph plots performance create phase sprite lfs microbenchmark number files increases x-axis bottom graph shows slowdown declared mode compared ordered mode modification result performance improvement early writing declare blocks data blocks offset seek activity journal home data locations shown examine performance sprite lfs microbenchmark creates reads unlinks number files figure plots number create operations completed number files increased x-axis bottom graph shows slowdown declared mode relative ordered mode declared mode performs ordered mode cases performance declared mode ordered mode identical phases benchmark ssh benchmark unpacks configures builds version ssh program tarred compressed distribution file figure plots performance mode stages benchmark execution time stage normalized ext ordered mode absolute times seconds listed bar data-journaling mode slighter faster ordered mode configure phase slower build slower unpack declared mode comparable ordered mode running faster unpack configure slower build phase examine ext performance modified version postmark benchmark creates files directories performs number transactions deletes files directories modification involves addition call sync phase benchmark ensure data written disk unmodified version exhibits unusually high variances modes operation execution time benchmark shown figbuildconfigureunpack normalized execution time ssh benchmark ext ordered ext journaled ext declared figure ssh benchmark performance graph plots normalized execution time unpack configure build phases ssh benchmark compared ext ordered mode absolute execution times seconds listed bar execution time postmark ext journaled ext ordered ext declared slowdown number transactions figure postmark performance top graph plots execution time postmark benchmark number transactions increases x-axis bottom graph shows slowdown declared mode compared ordered mode ure number transactions increases axis data-journaling mode extremely slow concentrate modes identify interesting points large numbers transactions declared mode compares favorably ordered mode differing approximately worst cases small number transactions declared mode outperforms ordered mode disk traces reveal reason ordered mode relies sorting provided per-file dirty page trees write requests scattered disk declared mode sort performed commit global view data written transaction sending write requests device layer efficient order finally examine performance tpc-blike workload performs financial transaction execution time tpc-b ext journaled ext declared ext ordered slowdown number transactions figure tpc-b performance top graph plots execution time tpc-b benchmark number transactions increases x-axis bottom graph shows slowdown declared mode compared ordered mode files adds history record fourth file commits disk calling sync execution time benchmark plotted figure number transactions increased x-axis case declared mode consistently underperforms ext ordered mode approximately data-journaling mode performs slightly worse highly synchronous nature benchmark presents worst case scenario declared mode tpc-b transaction results small ext transaction data blocks descriptor block journaled metadata block commit block declare block 
beginning transaction adds overhead number writes performed benchmark compound problem data writes serviced parallel array disks accentuating penalty declare blocks examine problem test modified version benchmark forces data disk frequently effect increasing size application level transaction alternatively simulating concurrent transactions independent data sets figure shows results running tpc-b benchmark transactions interval calls sync increases x-axis interval increases performance declared mode datajournaling mode quickly converge ordered mode declared mode performs ordered mode sync intervals transactions conclusion find declared mode routinely outperforms data-journaling mode performance close ordered mode random write sequential write file creation microbenchmarks performs ordered mode macrobenchmarks execution time tpc-b varied sync intervals ext journaled ext declared ext ordered slowdown tpc-b transactions sync figure tpc-b varied sync intervals top graph plots execution time tpc-b benchmark interval calls sync increases x-axis bottom graph shows slowdown declared mode compared ordered mode ssh postmark worst performance declared mode occurs tpc-b small application-level transactions improves greatly effective transaction size increases results declared mode attractive option enabling journal-guided resynchronization journal-guided resynchronization final set experiments examine effect journal-guided resynchronization expect significant reduction resync time shortening window vulnerability improving reliability addition faster resynchronization increase amount bandwidth foreground applications crash improving availability compare journal-guided resynchronization linux software raid resync default rate rates availability versus reliability spectrum experimental workload consists single foreground process performing sequential reads set large files amount read bandwidth achieves measured intervals approximately seconds experiment machine crashed rebooted machine restarts raid resynchronization process begins foreground process reactivates figure shows series experiments plotting foreground bandwidth y-axis time progresses x-axis note origin x-axis coincides beginning resynchronization duration process shaded grey top left graph figure shows results default linux resync limit disk prefers availability reliability process takes seconds bandwidth time software raid resync default disk bandwidth time software raid resync disk bandwidth time software raid resync disk bandwidth time software raid resync journal-guided resync resync foreground vulnerability vulnerability type rate limit bandwidth window default default disk medium disk high disk journal-guided figure software raid resynchronization graphs plot bandwidth achieved foreground process performing sequential scans files software raid array system crash ensuing array resynchronization recovery period highlighted grey duration listed graphs bandwidth allocated resynchronization varied default disk disk disk final graph depicts recovery journal guidance table lists availability foreground service vulnerability array compared default resynchronization period seconds restart scan raw disk space raidarray time period foreground process bandwidth drops unimpeded rate resynchronization completes foreground process receives full bandwidth array linux resynchronization rate adjusted sysctl variable top graph figure shows effect raising resync limit disk representing middle ground reliability availability case resync takes seconds bandwidth afforded foreground activity drops bottom left graph resync rate set disk favoring reliability availability effect reducing resync time seconds foreground bandwidth drops period bottom graph figure demonstrates journal-guided resynchronization knowledge write activity crash performs work correct array inconsistencies process finishes seconds greatly reducing window vulnerability present previous approach foreground service activates access full bandwidth array increasing availability results experiments summarized table figure metric calculated period restart machine order compare default linux resynchronization resync processes sacrifice availability foreground bandwidth variability improve reliability array reducing vulnerability windows default journal-guided resync process hand improves availability foreground process reliability array reducing vulnerability default case important note execution time scan-based approach scales linearly raw size array journal-guided resynchronization hand dependent size journal expect complete matter seconds large arrays complexity table lists lines code counted number semicolons braces modified added linux software raid ext file system journaling modules modifications needed add verify read interface software raid module core functionality existed needed activated requested stripe ext involved hiding dirty buffers declared mode orig mod percent module lines lines lines change software raid ext journaling total table complexity linux modifications table lists lines code counting semicolons braces original linux source number modified added software raid ext file system journaling modules verify reads recovery majority occurred journaling module writing declare blocks commit phase performing careful resynchronization recovery point comparison experimental version linux raidbitmap logging consists approximately lines code increase raidalone journaling module increasing size modifications consist lines code change modules observations support claim leveraging functionality cooperating layers reduce complexity software system related work brown patterson examine software raid systems work availability benchmarks find linux solaris windows implementations offer differing policies reconstruction process regenerating data parity disk failure solaris windows favor reliability linux policy favors availability unlike work authors focus improving reconstruction processes identifying characteristics general benchmarking framework stodolsky examine parity logging raid layer improve performance small writes writing parity blocks directly disk store log parity update images batched written disk large sequential access similar discussion nvram logging authors require fault tolerant buffer store parity update log reliability performance efforts avoid small random writes support argument maintaining performance raid level logging complex undertaking veritas volume manager facilities address faster resynchronization dirty region log speed raidresynchronization examining regions active crash log requires extra writes author warns coarse-grained regions needed maintain acceptable write performance volume manager supports raidlogging non-volatile memory solid state disk recommended support extra log writes contrast declared mode offers fine-grained journal-guided resynchronization performance degradation additional hardware schindler augment raid interface provide information individual disks atropos volume manager exposes disk boundary track information provide efficient semi-sequential access two-dimensional data structures database tables similarly raid disk boundary performance information augment functionality informed file system verify read interface complex providing file system access functionality exists software raid layer conclusions examined ability journaling file system provide support faster software raid resynchronization order obtain record outstanding writes time crash introduce ext declared mode mode guarantees declare intentions journal writing data disk extra write activity declared mode performs predecessor order communicate information software raid layer file system utilizes verify read request request instructs raid layer read block repair redundant information combining features implement fast journal-guided resynchronization process improves software raid reliability availability hastening recovery process crash general approach advocates system-level view developing storage stack file system journal improve raid system leverages existing functionality maintains performance avoids duplicating complexity multiple layers layers implement abstractions protocols mechanisms policies interactions define properties system acknowledgements john bent nathan burnett anonymous reviewers excellent feedback work sponsored 
nsf ccrccr- ngsitr- network appliance emc jfs overview ibm developerworks library jfs html brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june clements bottomley high availability data replication proceedings linux symposium ottawa canada june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june emc emc centera content addressed storage system http emc emc corporation symmetrix enterprise information storage systems http emc mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june reiser reiserfs namesys rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february schindler schlosser shao ailamaki ganger atropos disk array volume manager orchestrated disks proceedings usenix symposium file storage technologies fast san francisco california april solomon inside windows microsoft programming series microsoft press stodolsky gibson holland parity logging overcoming small write problem redundant disk arrays proceedings annual international symposium computer architecture isca pages san diego california sun solaris volume manager administration guide http docs sun app docs doc july teigland mauelshagen volume managers linux proceedings usenix annual technical conference freenix track boston massachusetts june tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june tweedie journaling linux ext file system fourth annual linux expo durham north carolina tweedie ext journaling file system olstrans sourceforge net release ols -ext ols ext html july veritas features veritas volume manager unix veritas file system http veritas products volumemanager whitepaperhtml july 
improving storage system availability d-graid muthian sivathanu vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau wisconsin madison present design implementation evaluation d-graid gracefully degrading quickly recovering raid storage array d-graid ensures files file system remain unexpectedly high number faults occur d-graid achieves high availability aggressive replication semantically critical data fault-isolated placement logically related data d-graid recovers failures quickly restoring live file system data hot spare graceful degradation live-block recovery implemented prototype scsi-based storage system underneath unmodified file systems demonstrating powerful file-system functionality implemented semantically smart disk system narrow block-based interface categories subject descriptors operating systems storage management secondary storage operating systems reliability fault tolerance general terms design algorithms reliability additional key words phrases disk array raid block-based storage fault isolation file systems smart disks introduction tree falls forest hears make sound george berkeley storage systems comprised multiple disks backbone modern computing centers storage system entire center grind halt downtime expensive on-line business world millions dollars hour lost systems keeton wilkes patterson work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation earlier version article appeared proceedings usenix symposium file storage technologies fast san francisco authors addresses computer sciences statistics wisconsin madison dayton street madison muthian vijayan dusseau remzi wisc permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit direct commercial advantage copies show notice page initial screen display full citation copyrights components work owned acm honored abstracting credit permitted copy republish post servers redistribute lists component work works requires prior specific permission fee permissions requested dept acm broadway york usa permissions acm acm acm transactions storage vol pages sivathanu storage system availability formally defined time failure mtbf divided sum mtbf time recovery mttr mtbf mtbf mttr gray order improve availability increase mtbf decrease mttr surprisingly researchers studied components storage availability increase time failures large storage array data redundancy techniques applied bitton gray burkhard menon chen gray hsiao dewitt orji solworth park balasubramanian patterson savage wilkes wilkes keeping multiple copies blocks sophisticated redundancy schemes parity-encoding storage systems tolerate small fixed number faults decrease time recovery hot spares employed holland menon mattson park balasubramanian reddy banerjee failure occurs spare disk activated filled reconstructed data returning system normal operating mode quickly problem reduced availability due semantic ignorance techniques proposed improve storage availability narrow interface file systems storage ganger curtailed opportunities improving mtbf mttr raid redundancy schemes typically export simple failure model fewer disks fail raid continues operate correctly disks fail raid unavailable problem corrected time-consuming restore tape raid schemes small disks working users observe failed disk system availability cliff result storage system laying blocks oblivious semantic importance relationship files corrupted inaccessible extra disk failure storage array information blocks live file system recovery process restore blocks disk unnecessary work slows recovery reduces availability ideal storage array fails gracefully disks system data unavailable ideal array recovers intelligently restoring live data effect important data disappear failure data restored earlier recovery strategy data availability stems berkeley observation falling trees file isn process access recovered failure solution d-graid explore concepts provide storage array graceful failure semantics present design implementation evaluation d-graid raid system degrades gracefully recovers quickly d-graid exploits semantic intelligence sivathanu disk array place file system structures disks fault-contained acm transactions storage vol improving storage system availability d-graid manner analogous fault containment techniques found hive operating system chapin distributed file systems saito unexpected double failure occurs gray d-graid continues operation serving files accessed d-graid utilizes semantic knowledge recovery specifically blocks file system considers live restored hot spare aspects d-graid combine improve effective availability storage array note d-graid techniques complementary existing redundancy schemes storage administrator configures d-graid array utilize raid level single disk fail data loss additional failures lead proportional fraction unavailable data article present prototype implementation d-graid refer alexander alexander semantically-smart disk system sivathanu built underneath narrow block-based scsi storage interface disk system understands on-disk file system data structures including superblock allocation bitmaps inodes directories important structures knowledge central implementing graceful degradation quick recovery intricate understanding file system structures operations semantically smart arrays tailored file systems alexander functions underneath unmodified linux ext vfat file systems make important contributions semantic disk technology deepen understanding build semantically smart disk systems operate correctly imperfect file system knowledge demonstrate technology applied underneath widely varying file systems demonstrate semantic knowledge raid system apply redundancy techniques based type data improving availability key techniques key aspects alexander implementation graceful degradation selective meta-data replication inwhich alexander replicates naming system meta-data structures file system high degree standard redundancy techniques data small amount overhead excess failures render entire array unavailable entire directory hierarchy traversed fraction files missing proportional number missing disks fault-isolated data placement strategy ensure semantically meaningful data units failure alexander places semantically related blocks blocks file storage array unit fault-containment disk observing natural failure boundaries found array failures make semantically related groups blocks unavailable leaving rest file system intact fault-isolated data placement improves availability cost related blocks longer striped drives reducing natural acm transactions storage vol sivathanu benefits parallelism found raid techniques ganger remedy alexander implements access-driven diffusion improve throughput frequently-accessed files spreading copy blocks hot files drives system alexander monitors access data determine files replicate fashion finds space replicas preconfigured performance reserve opportunistically unused portions storage system evaluate availability improvements d-graid trace analysis simulation find d-graid excellent job masking arbitrary number failures processes enabling continued access important data evaluate prototype alexander microbenchmarks trace-driven workloads find construction d-graid feasible imperfect semantic knowledge powerful functionality implemented block-based storage array find run-time overheads d-graid small storage-level cpu costs compared standard array high show access-driven diffusion crucial performance live-block recovery effective disks under-utilized combination replication data placement recovery techniques results storage system improves availability maintaining high level performance rest article structured section present extended motivation section discuss related work present design principles d-graid section section present trace analysis simulations discuss semantic knowledge section section present prototype implementation evaluate prototype section section present custom policies levels d-graid discuss resilience d-graid incorrect information section conclude section extended motivation section discuss graceful degradation multiple failures describe semantically smart disk system locale incorporate support graceful 
degradation case graceful degradation motivation graceful degradation arises fact users applications require entire contents volume present matters set files question arises realistic expect catastrophic failure scenario raid system raidsystem high mtbf reported disk manufacturers disk failure highly occur failed disk repaired multiple disk failures occur primary reasons correlated faults common systems expected gribble raid carefully designed orthogonal manner single controller fault component error render fair number acm transactions storage vol improving storage system availability d-graid disks unavailable chen redundant designs expensive found higher end storage arrays gray pointed system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure gray evidence suggests multiple failures occur ibm serveraid array controller product includes directions attempt data recovery multiple disk failures occur raidstorage array ibm organization data stored file servers raidin servers single disk failed indicator informed administrators problem problem discovered disk array failed full restore backup ran days scenario graceful degradation enabled access large fraction user data long restore approach dealing multiple failures employ higher level redundancy alvarez burkhard menon enabling storage array tolerate greater number failures loss data techniques expensive three-way data mirroring bandwidth-intensive write redundant store graceful degradation complementary techniques storage administrators choose level redundancy common case faults graceful degradation enacted worse expected fault occurs mitigating ill effect semantically smart storage basic design principles d-graid apply equally implementation alternatives tradeoffs subsection motivate decision implement d-graid semantically smart disk system discuss benefits approach addressing obvious concerns compare semantic disk approach alternatives implementing d-graid benefits semantic disk approach implementing functionality semantically smart disk system key benefit enabling wide-scale deployment underneath unmodified scsi interface modification working smoothly existing file systems software base desire evolve interface file systems storage gibson reality current interfaces survive longer anticipated bill joy systems protocols live forever similarly modern processors innovate beneath unchanged instruction sets semantic disk-level implementation nonintrusive existing infrastructure making technology d-graid adopted semantically smart storage systems require detailed knowledge file system concerns arise commercial feasibility systems main concerns acm transactions storage vol sivathanu concern arises placing semantic knowledge disk system ties disk system intimately file system on-disk structure file system storage system change issue problematic on-disk formats evolve slowly reasons backward compatibility basic structure ffs-based file systems changed introduction period years mckusick linux ext file system introduced roughly exact layout lifetime finally ext journaling file system tweedie backward compatible ext on-disk layout extensions freebsd file system dowse malone backward compatible evidence storage vendors maintain support software specific file system emc symmetrix storage system emc corporation software understand format common file systems concern storage system semantic knowledge file system interacts fortunately large number file systems supported cover large fraction usage population semantic storage system file system support storage system detect turn special functionality case d-graid revert normal raid layout detection simple techniques observing file system identifier partition table final concern arises processing required disk system major issue general trend increasing disk system intelligence acharya riedel processing power increases disk systems substantial computational abilities modern storage arrays exhibit fruits moore law emc symmetrix storage server configured processors ram emc corporation comparison alternative approaches semantic disk approach clear benefits detailed cost rediscovering semantic knowledge underneath modern file system entails fair amount complexity alternative approach change interface file systems storage convey richer information layers instance storage system expose failure boundaries file system denehy file system explicitly allocate blocks fault-isolated manner placing semantically related blocks alternatively file system tag write logical fault-container storage system implement fault-isolated data placement techniques conceivably complex approach drawback intrusive existing infrastructure software base requiring wide industry agreement adopted acm transactions storage vol improving storage system availability d-graid object-based storage gibson interface considered makes file boundaries visible storage layer object-based interface semantically smart technology relevant discover semantic relationships objects instance inferring directory object points set file objects single fault boundary finally approximate version fault-isolated layout implemented traditional block based storage system semantic understanding storage system simply identify sequences blocks accessed infer blocks logically related main disadvantage black-box approach fragile concurrent interleavings independent streams scheme identify critical data purposes aggressive replication hot blocks cached file system frequent reads visible storage system related work d-graid draws related work number areas including distributed file systems traditional raid systems discuss turn distributed file systems designers distributed file systems long ago realized problems arise spreading directory tree machines system walker discussed importance directory namespace replication locus distributed system popek coda mobile file system takes explicit care regard directory tree kistler satyanarayanan specifically file cached coda makes cache directory root directory tree coda guarantee file remains accessible disconnection occur interesting extension work reconsider host-based in-memory caching availability mind slice anderson route namespace operations files directory server recently work wide-area file systems reemphasized importance directory tree pangaea file system aggressively replicates entire tree root node file accessed saito island-based file system points fault isolation context wide-area storage systems island principle similar fault-isolated placement d-graid finally systems past place entire file single machine similar load balancing issues rowstron druschel problem difficult space due constraints file placement block migration simpler centralized storage array traditional raid systems draw long history research classic raid systems autoraid wilkes learned complex functionality acm transactions storage vol sivathanu embedded modern storage array background activity utilized successfully environment afraid savage wilkes learned flexible tradeoff performance reliability delaying updates raid research focused redundancy schemes early work stressed ability tolerate single-disk failures bitton gray park balasubramanian patterson research introduced notion tolerating multiple-disk failures array alvarez burkhard menon stress work complementary line research traditional techniques ensure full file system availability number failures d-graid techniques ensure graceful degradation additional failures related approach parity striping gray stripes parity data parity striping achieve primitive form fault isolation layout oblivious semantics data blocks level redundancy irrespective importance meta-data data multiple failures make entire file system inaccessible number earlier works emphasized importance hot sparing speed recovery time raid arrays holland menon mattson park balasubramanian work semantic recovery complementary approaches finally note term graceful degradation refer performance characteristics redundant disk systems failure hsiao dewitt reddy banerjee type graceful degradation 
discuss article systems continues operation unexpected number failures occurs design d-graid expectations discuss design d-graid present background information file systems data layout strategy required enable graceful degradation important design issues arise due layout process fast recovery file system background semantic knowledge system specific discuss d-graid design implementation widely differing file systems linux ext tweedie microsoft vfat microsoft corporation file systems inclusion vfat represents significant contribution compared previous research operated solely underneath unix file systems ext file system intellectual descendant berkeley fast file system ffs mckusick disk split set block groups akin cylinder groups ffs bitmaps track inode data block allocation inode blocks data blocks information file including size block pointers found file inode vfat file system descends world operating systems article linux vfat implementation fatvfat acm transactions storage vol improving storage system availability d-graid operations centered eponymous file allocation table entry allocatable block file system entries locate blocks file linked-list fashion file block address entry fat find block file entry hold end-of-file marker setting block free unlike unix file systems information file found inode vfat file system spreads information fat directory entries fat track blocks belong file directory entry information size permission type information graceful degradation ensure partial availability data multiple failures raid array d-graid employs main techniques fault-isolated data placement strategy d-graid places semantically related set blocks unit fault containment found storage array simplicity discussion assume file semantically related set blocks single disk unit fault containment generalize easily generalized failure boundaries observed scsi chains refer physical disk file belongs home site file disk fails fault-isolated data placement ensures files disk home site unavailable files remain accessible files technique selective meta-data replication inwhich d-graid replicates naming system meta-data structures file system high degree directory inodes directory data unix file system d-graid ensures live data reachable orphaned due multiple failures entire directory hierarchy remains traversable fraction missing user data proportional number failed disks d-graid lays logical file system blocks availability single file depends disks traditional raid array dependence set entire set disks group leading entire file system unavailability unexpected failure unix-centric typical layout fault-isolated data placement selective meta-data replication depicted figure note techniques d-graid work meaningful subset file system laid single d-graid array file system striped multiple d-graid arrays single array meaningful view file system scenario d-graid run logical volume manager level viewing arrays single disk techniques remain relevant d-graid treats file system block type differently traditional raid taxonomy longer adequate describing d-graid behaves fine-grained notion raid level required d-graid employ redundancy techniques types acm transactions storage vol sivathanu fig comparison layout schemes parts figure depict layouts file foo bar unix file system starting root inode directory tree file data vertical column represents disk simplicity assumes data redundancy user file data top typical file system layout non-d-graid disk system blocks pointers spread file system single fault render blocks file bar inaccessible left figure bottom fault-isolated data placement files directories scenario access inode file access data indirect pointer blocks constrained disk finally bottom selective meta-data replication replicating directory inodes directory blocks d-graid guarantee users files requisite pointers removed rightmost figure simplicity color codes white user data light shaded inodes dark shaded directory data data d-graid commonly employs n-way mirroring naming system meta-data standard redundancy techniques mirroring parity encoding raidfor user data note administrative control determines number failures d-graid degrade gracefully section explore data availability degrades varying levels namespace replication design considerations layout replication techniques required enable graceful degradation introduce number design issues highlight major challenges arise semantically related blocks fault-isolated data placement d-graid places logical unit file system data file faultisolated container disk blocks d-graid considers related acm transactions storage vol improving storage system availability d-graid determines data remains failure basic approach file-based grouping single file including data blocks inode indirect pointers treated logical unit data technique user find files directory unavailable frustration confusion groupings preserve meaningful portions file system volume failure directory-based grouping d-graid ensures files directory unit fault containment automated options allowing users arbitrary semantic groupings d-graid treats unit load balance fault-isolated placement placing blocks file disks blocks isolated single home site isolated placement improves availability introduces problem load balancing space time components terms space total utilized space disk maintained roughly level fraction disks fail roughly fraction data unavailable balancing addressed foreground data allocated background migration files directories larger amount free space single disk handled potentially expensive reorganization reserving large extents free space subset drives files larger single disk split disks pressing performance problems introduced fault-isolated data placement previous work striping data disks performance compared sophisticated file placement algorithms ganger wolf d-graid makes additional copies user data spread drives system process call access-driven diffusion standard d-graid data placement optimized availability access-driven diffusion increases performance files frequently accessed surprisingly access-driven diffusion introduces policy decisions d-graid including place replicas made performance files replicate create replicas meta-data replication level degree meta-data replication d-graid determines resilient excessive failures high degree replication desirable meta-data replication costs terms space time space overheads tradeoffs obvious replicas imply resiliency difference traditional raid d-graid amount space needed replication naming system meta-data dependent usage volume directories induces greater amount overhead time overheads higher degree replication implies lowered write performance naming system meta-data operations observed lack update activity higher levels directory tree popek lazy update propagation employed reduce costs savage wilkes acm transactions storage vol sivathanu fast recovery main design goal d-graid ensure higher availability fast recovery failure critical straightforward optimization d-graid recover live file system data assume restoring data live mirror hot spare straightforward approach d-graid simply scans source disk live blocks examining file system structures determine blocks restore process readily generalized complex redundancy encodings d-graid potentially prioritize recovery number ways restoring important files importance domain specific files orindicated users manner similar hoarding database coda kistler satyanarayanan exploring graceful degradation section simulation trace analysis evaluate potential effectiveness graceful degradation impact semantic grouping techniques quantify space overheads d-graid demonstrate ability d-graid provide continued access proportional fraction meaningful data arbitrary number failures importantly demonstrate d-graid hide failures users replicating important data simulations file system traces collected labs riedel cover days activity data 
spread logical volumes space overheads examine space overheads due selective meta-data replication typical d-graid-style redundancy calculate cost selective meta-data replication percentage overhead measured volumes trace data laid ext vfat file system running underneath ext selective meta-data replication applied superblock inode data block bitmaps inode data blocks directory files blocks replicated case vfat comprise fat directory entries calculate highest percentage selective meta-data replication overhead assuming replication user data user data mirrored overheads cut half table shows selective meta-data replication induces mild space overhead high levels meta-data redundancy linux ext vfat file systems -way redundancy meta-data space overhead incurred worst case vfat -kb blocks increasing block size ext space due internal fragmentation larger directory blocks overheads decrease vfat phenomenon due structure vfat fixed-sized file system block size grows file allocation table shrinks blocks directory data grow acm transactions storage vol improving storage system availability d-graid table space overhead selective meta-data replication table shows space overheads selective meta-data replication percentage total user data level naming system meta-data replication increases leftmost column percentage space overhead meta-data replication shown columns depict costs modest four-way paranoid -way schemes row shows overhead file system ext vfat block size set level replication -way -way -way ext ext vfat vfat fig static data availability percent entire directories shown increasing disk failures simulated system consists disks loaded trace strategies semantic grouping shown file-based directory-based line varies level replication namespace meta-data point shows average deviation trials trial randomly varies disks fail static availability examine d-graid availability degrades failure semantic grouping strategies strategy file-based grouping information single file failure boundary disk directory-based grouping allocates files directory analysis place entire files directories trace simulated -disk system remove simulated disks measure percentage directories assume user data redundancy d-graid level figure shows percent directories directory files accessible subdirectories files figure observe graceful degradation works amount data proportional number working acm transactions storage vol sivathanu fig dynamic data availability figure plots percent processes run unaffected disk failure busy hour trace degree namespace replication set aggressively line varies amount replication popular directories oneway implies directories replicated eight-way -way show modest extreme amount replication means deviations trials shown disks contrast traditional raid disk crashes lead complete data unavailability fact availability degrades slightly expected strict linear fall-off due slight imbalance data placement disks directories modest level namespace replication four-way leads good data availability failure conclude file-based grouping files directory disappear failure leading user dissatisfaction dynamic availability finally simulating dynamic availability examine users applications oblivious d-graid operating degraded mode specifically run portion trace simulator number failed disks record percent processes observed failure run experiment find namespace replication files needed processes replicated experiment set degree namespace replication full replication vary level replication contents popular directories usr bin bin lib figure shows replicating contents directories percentage processes run ill-effect lower expected results figure directories replicated percentage processes run completion disk failure expected reason clear substantial number processes require executable libraries run correctly popular directory replication excellent availability acm transactions storage vol improving storage system availability d-graid failure fortunately popular files read-only directories wide-scale replication raise write performance consistency issues space overhead due popular directory replication minimal sized file system trace directories account total file system size semantic knowledge move construction d-graid prototype underneath block-based scsi-like interface enabling technology underlying d-graid semantic knowledge sivathanu understanding file system utilizes disk enables d-graid implement graceful degradation failure quick recovery exact details acquiring semantic knowledge disk raid system sivathanu assume basic understanding file system layout structures storage system specifically assume d-graid static knowledge file system layout including regions disk block types contents specific block types fields inode describe d-graid builds basic knowledge infer detailed dynamic information file system file system behaviors article extend understanding semantically smart disks presenting techniques handle general file system behaviors previous work required file system mounted synchronously implementing complex functionality disk relax requirement describe set typical file system properties important viewpoint semantically smart disk modern file systems adhere behaviors blocks file system dynamically typed file system locate types blocks physical location disk lifetime file system unix file system block data region user-data block indirect-pointer block directory-data block file system delay updates disk delayed writes file system facilitate batching small writes memory suppressing writes files subsequently deleted consequence delayed writes order file system writes data disk arbitrary file systems order writes carefully ganger remain general make assumptions ordering note properties identified practical reasons linux ext file system exhibits aforementioned behaviors accuracy information assumptions general file system behavior imply storage system accurately classify type block block classification straightforward type block depends location disk acm transactions storage vol sivathanu berkeley fast file system ffs mckusick regions disk store inodes fixed file system creation traffic regions inodes type information spread multiple blocks block filled indirect pointers identified observing inode specifically inode indirect pointer field address indirect block formally identify indirect block semantic disk inode block indirect pointer field relevant inode block written disk disk infers indirect block observes block written information classify treat block indirect block due delayed write reordering behavior file system time disk writes block freed original inode reallocated inode type normal data block disk operations place memory reflected disk inference made semantic disk block type wrong due inherent staleness information tracked implementing correct system potentially inaccurate inferences challenges address article implementation making d-graid discuss prototype implementation d-graid alexander alexander fault-isolated data placement selective metadata replication provide graceful degradation failure employs access-driven diffusion correct performance problems introduced availability-oriented layout alexander replicates namespace system meta-data administrator-controlled stores user data raidor raidmanner refer systems d-graid levels pursuing d-graid level implementation log-structuring rosenblum ousterhout avoid small-write problem exacerbated fault-isolated data placement section present implementation graceful degradation live-block recovery complexity discussion centered graceful degradation simplicity exposition focus construction alexander underneath linux ext file system end section discuss differences implementation underneath vfat graceful degradation present overview basic operation graceful degradation alexander describe simple cases proceeding intricate aspects implementation indirection map similarly scsi-based raid system alexander presents host systems linear logical block address space acm transactions storage vol improving storage system availability d-graid internally alexander place blocks facilitate graceful degradation 
control placement alexander introduces transparent level indirection logical array file system physical placement disks indirection map imap similar structures english stepanov wang wilkes unlike systems imap maps live logical file system block replica list physical locations unmapped blocks considered free candidates d-graid reads handling block read requests d-graid level straightforward logical address block alexander imap find replica list issues read request replicas choice replica read based criteria wilkes alexander randomized selection presence access-driven diffusion diffused copy preference fault-isolated copy writes contrast reads write requests complex handle alexander handles write request depends type block written figure depicts common cases block static meta-data block inode bitmap block unmapped alexander allocates physical block disks replica reside writes copies note alexander easily detect static block types inode bitmap blocks underneath unix file systems simply observing logical block address inode block written d-graid scans block newly added inodes understand inodes d-graid compares newly written block copy process referred block differencing inode d-graid selects home site lay blocks belonging inode records inode-to-home-site hashtable selection home site balance space allocation physical disks d-graid greedy approach selects home site free space write unmapped block data region data block indirect block directory block allocation d-graid file block belongs actual home site case d-graid places block deferred block list write disk learns file block crash inode write make block inaccessible file system in-memory deferred block list reliability concern d-graid newly added block pointers inode indirect block written newly added block pointer refers unmapped block d-graid adds entry imap mapping logical block physical block home site assigned inode newly added pointer refers block deferred list d-graid removes block deferred list issues write physical block writes deferred blocks written acm transactions storage vol sivathanu fig anatomy write parts figure depicts control flow sequence write operations alexander figure inode block written alexander observes contents inode block identifies newly added inode selects home site inode creates physical mappings blocks inode home site inode block aggressively replicated part alexander observes write data block inode mapped write directly physical block part alexander write unmapped data block defers writing block alexander finally observes inode fourth part creates relevant mappings observes blocks deferred issues deferred write relevant home site owner inode blocks inode written subsequent data writes mapped disk directly block type interest d-graid data bitmap block data bitmap block written d-graid scans newly freed data blocks freed block d-graid removes logical-to-physical mapping exists frees physical blocks block deferred list freed block removed deferred list write suppressed data blocks written file system deleted inode written disk generate extra disk traffic similarly optimizations found file systems rosenblum ousterhout removing blocks deferred list important case freed blocks alexander observe owning inode deferred block stays deferred list bounded amount time inode owning block written bitmap acm transactions storage vol improving storage system availability d-graid block indicating deletion block written exact duration depends delayed write interval file system block reuse discuss intricate issues involved implementing graceful degradation issue block reuse existing files deleted truncated files created blocks part file reallocated file d-graid place blocks correct home site reuse blocks detected acted d-graid handles block reuse manner inode block indirect block written d-graid examines valid block pointer physical block mapping matches home site allocated inode d-graid mapping block correct home site write block made context file home site copied physical location location blocks copied added pending copies list background thread copies blocks homesite frees physical locations copy completes dealing imperfection difficulty arises semantically smart disks underneath typical file systems exact knowledge type dynamically typed block impossible obtain discussed section alexander handle incorrect type classification data blocks file data directory indirect blocks d-graid understand contents indirect blocks pointers place file blocks home site due lack perfect knowledge fault-isolated placement file compromised note data loss corruption issue goal dealing imperfection conservatively avoid eventually detect handle cases specifically block construed indirect block written assume valid indirect block live pointer block d-graid action cases case pointer refer unmapped logical block mentioned d-graid creates mapping home site inode indirect block belongs indirect block pointer valid mapping correct mapping indirect block misclassified pointer invalid d-graid detects block free observes data bitmap write point mapping removed block allocated file bitmap written d-graid detects reallocation inode write file creates mapping copies data contents home site discussed case potentially corrupt block pointer point mapped logical block discussed type block reuse results mapping copy block contents home site indirect block pointer valid mapping acm transactions storage vol sivathanu correct block indirect block misclassification alexander wrongly copies data home site note data accessible original file block belongs blocks incorrect home site fortunately situation transient inode file written d-graid detects reallocation creates mapping back original home site restoring correct mapping files accessed properly laid infrequent sweep inodes rare cases improper layout optimizations d-graid eventually move data correct home site preserving graceful degradation reduce number times misclassification occurs alexander makes assumption contents indirect blocks specifically number valid unique pointers null pointers alexander leverage assumption greatly reduce number misclassifications performing integrity check supposed indirect block integrity check reminiscent work conservative garbage collection boehm weiser returns true pointers -byte words block point valid data addresses volume nonnull pointers unique set blocks pass integrity check corrupt data contents happened evade conditions test run data blocks local file system small fraction data blocks pass test blocks pass test reallocated file data block indirect block misclassified access-driven diffusion issue d-graid address performance fault-isolated data placement improves availability cost performance data accesses blocks large file directorybased grouping files directory longer parallelized improve performance alexander performs access-driven diffusion monitoring block accesses determine block ranges hot diffusing blocks replication disks system enhance parallelism access-driven diffusion achieved logical physical levels disk volume logical approach access individual files monitored considered hot diffused per-file replication fails capture sequentiality multiple small files single directory pursue physical approach alexander replicates segments logical address space disks volume file systems good allocating contiguous logical blocks single file files directory replicating logical segments identify exploit common access patterns sensitive data contents semantically smart disks place requirement file system traces include user data 
blocks privacy concerns campaign encounter difficult overcome acm transactions storage vol improving storage system availability d-graid implement access-driven diffusion alexander divides logical address space multiple segments normal operation gathers information utilization segment background thread selects logical segments remain hot number consecutive epochs diffuses copy drives system subsequent reads writes replicas background updates original blocks imap entry block copy date policy deciding segments diffuse simplistic prototype implementation detailed analysis policy space access-driven diffusion left future work amount disk space allocate performance-oriented replicas presents important policy decision initial policy alexander implements reserve minimum amount space system administrator replicas opportunistically free space array additional replication approach similar autoraid mirrored data wilkes autoraid identify data considered dead file system written contrast d-graid semantic knowledge identify blocks free live-block recovery implement live-block recovery d-graid understand blocks live knowledge correct block live considered dead lead data loss alexander tracks information observing bitmap data block traffic bitmap blocks liveness state file system reflected disk due reordering delayed updates uncommon observe write data block bit set data bitmap account d-graid maintains duplicate copy bitmap blocks sees write block sets bit local copy bitmap duplicate copy synchronized file system copy data bitmap block written file system conservative bitmap table reflects superset live blocks file system perform live-block recovery note assume preallocation state bitmap written disk subsequent allocation locking linux modern systems ensures technique guarantees live block classified dead disk block live longer situation arise file system writes deleted blocks disk implement live-block recovery alexander simply conservative bitmap table build list blocks restored alexander proceeds list copies live data hot spare aspects alexander number aspects implementation required successful prototype subsection briefly describe key aspects acm transactions storage vol sivathanu physical block allocation logical array blocks exported scsi property block numbers contiguous logical address space mapped contiguous physical locations disk property empowers file systems place data contiguously disk simply allocating contiguous logical blocks data traditional raid property straightforward preserve physical blocks assigned round-robin fashion disks contiguity guarantees continue hold physical block assign logical block simple arithmetic calculation logical block number d-graid deciding physical block allocate newly written logical block straightforward decision depends file logical block belongs logical offset file fault-isolated placement set contiguous logical blocks belong single file map contiguous physical blocks disk logical block set mapped physical block block set mapped physical block order preserve contiguity expectations larger granularity d-graid balances space utilization files allocation policy large values block map physical block number disks array choice policies requires estimates file size dynamic prototype addresses issue simple technique space reservations alexander utilizes knowledge inodes indirect blocks priori estimates exact size entire file large segment file case indirect block observes inode written file size blocks reserves contiguous blocks home site assigned file actual logical blocks written subsequently reserved space note blocks deferred inodes indirect blocks observed write logical block prior reservation inodes indirect blocks written periodically size information obtained writes stable just-in-time commit space reservations depend size information extracted inode indirect blocks indirect block detection fundamentally inaccurate misclassified indirect block result spurious reservations hold physical space prevent alexander employs lazy allocation actual physical blocks committed logical block written reservation priori reservations viewed soft space reclaimed required interaction deferred writes sync alexander defers disk writes logical blocks observed owning inode arbitrary deferral potentially conflict application-level expectations sync operation issued sync returns application expects acm transactions storage vol improving storage system availability d-graid data disk preserve semantics d-graid handles inode indirect block writes specially d-graid return success write inode indirect block deferred writes blocks pointed inode indirect block reached disk sync operation complete inode block write returns deferred writes guaranteed complete sync returns argument extends fsync return writes pertaining file complete weakness approach application performs equivalent fdatasync flushes data blocks disk metadata technique preserve expected semantics inconsistent fault behavior linux ext interesting issue required change design behavior linux ext partial disk failure process read data block unavailable ext issues read returns failure process block recovery process issues read ext issue read works expected process open file inode unavailable ext marks inode suspicious issue request inode block alexander recovered block avoid change file system retain ability recover failed inodes alexander replicates inode blocks namespace meta-data collocating data blocks file persistence data structures number structures alexander maintains imap reliably committed disk preferably good performance buffered small amount nonvolatile ram note nvram serve cache actively accessed entries data structures space requirements acceptable level current prototype simply stores data structures memory complete implementation require backed persistently popular directory replication important component missing alexander prototype decision popular read-only directories usr bin replicate widely alexander proper mechanisms perform replication policy space remains unexplored initial experience simple approach based monitoring frequency inode access time updates effective alternative approach administrators directories treated manner alexander fat surprised similarities found implementing d-graid underneath ext vfat vfat overloads data blocks user data blocks directories alexander acm transactions storage vol sivathanu defer classification blocks manner similar ext implementation expected implementation basic mechanisms d-graid physical block allocation allocation home sites files tracking replicas critical blocks shared versions d-graid instances vfat implementation d-graid differed interesting ways ext version fact pointers file located file allocation table made number aspects d-graid simpler implement vfat indirect pointers worry copy fat block written version directly compared previous contents block accurate information blocks newly allocated deleted ran occasional odd behavior linux implementation vfat linux write disk blocks allocated freed avoiding obvious common file system optimization behavior vfat estimate set live blocks strict superset blocks live indicative untuned nature linux implementation served indicator semantic disks wary assumptions make file system behavior evaluating alexander present performance evaluation alexander focus primarily linux ext variant include baseline measurements vfat system answer questions alexander work correctly time overheads introduced effective access-driven diffusion fast live-block recovery benefits expect d-graid complex implementation platform alexander prototype constructed software raid driver linux kernel file systems mount pseudodevice normal disk environment excellent understanding issues involved construction real hardware d-graid system limited ways importantly alexander runs system host applications interference due competition cpu memory resources performance characteristics microprocessor memory system found actual raid system 
experiments utilize -mhz pentium iii k-rev min ibm disks acm transactions storage vol improving storage system availability d-graid fig errors placement figure plots number blocks wrongly laid alexander time running busy hour trace experiment run disks total number blocks accessed trace alexander work correctly alexander complex simple raid systems ensure alexander operates correctly put system numerous stress tests moving large amounts data system problems extensively tested corner cases system pushing situations difficult handle making system degrades gracefully recovers expected repeatedly crafted microbenchmarks stress mechanisms detecting block reuse handling imperfect information dynamically typed blocks constructed benchmarks write user data blocks disk worst-case data data appears valid directory entries indirect pointers cases alexander detect blocks indirect blocks move files directories proper fault-isolated locations verify alexander places blocks disk instrumented file system log block allocations addition alexander logs events interest assignment home site inode creation mapping logical block remapping blocks home site receipt logical writes file system evaluate behavior alexander workload ran workload alexander obtained time-ordered log events occurred file system alexander processed log off-line looked number blocks wrongly laid time ran test hours traces found hours examined number blocks misplaced temporarily low fewer blocks report detailed results hour trace observed greatest number misplaced blocks hours examined figure shows results acm transactions storage vol sivathanu fig time overheads figure plots time overheads observed d-graid level versus raid level series microbenchmarks tests run disk systems experiment operations enacted file creations operation -kb file figure parts bottom part shows normal operation alexander capability react block reuse remapping copying blocks correct homesite figure shows alexander quickly detect wrongly blocks remap appropriately number blocks misplaced temporarily total number blocks accessed trace top part figure shows number misplaced blocks experiment assuming remapping occur expected delinquent blocks remain misplaced dip end trace occurs misplaced blocks assigned file home site preceding delete accidentally correcting original misplacement time overheads introduced explore time overheads arise due semantic inference primarily occurs blocks written file system file creation figure shows performance alexander simple microbenchmark allocating writes slower due extra cpu cost involved tracking fault-isolated placement reads overwrites perform comparably raidthe high unlink times d-graid fat fat writes data pertaining deleted files processed d-graid newly allocated data implementation untuned infrastructure suffers cpu memory contention host worst-case estimates overheads cost d-graid explored overhead metadata replication purpose chose postmark katcher acm transactions storage vol improving storage system availability d-graid table performance postmark table compares performance d-graid level raidon postmark benchmark row marked d-graid specific level metadata replication column reports benchmark run-time column shows number disk writes incurred column shows number disk writes metadata blocks fourth column number unique metadata blocks written experiment run disks blocks written run-time total meta-data unique raidd-graid d-graid d-graid d-graid meta-data-intensive file system benchmark slightly modified postmark perform sync deletion phase meta-data writes accounted making pessimistic evaluation costs table shows performance alexander degrees meta-data replication table synchronous replication meta-data blocks significant effect performance meta-data-intensive workloads file sizes postmark range bytes note alexander performed default raidfor lower degrees replication physical block allocation ext contiguous free chunk blocks allocate file layout suboptimal small files pack table shows number disk writes incurred benchmark percentage extra disk writes roughly accounts difference performance replication levels extra writes meta-data blocks counted number unique physical writes meta-data blocks absolute difference replication levels small suggests lazy propagation updates meta-data block replicas idle time freeblock scheduling greatly reduce performance difference cost added complexity lazy update propagation replicas updated d-graid incur extra disk writes played back portion traces min standard raidsystem d-graid disks playback engine issued requests times trace optional speedup factor speedup implies idle time requests reduced factor speedup factors d-graid delivered per-second operation throughput raidutilizing idle time trace hide extra cpu overhead scaling factor operation throughput lagged slightly d-graid showing slowdown one-third trace execution caught due idle time acm transactions storage vol sivathanu fig access-driven diffusion figure presents performance d-graid level standard raidunder sequential workload experiment number files size read sequentially total volume data fixed d-graid performs smaller files due physical block layout effective access-driven diffusion show benefits access-driven diffusion trial experiment performed set sequential file reads files increasing size compared standard raidstriping d-graid access-driven diffusion figure shows results experiment figure access-driven diffusion sequential access larger files ran rate single disk system benefit potential parallelism access-driven diffusion performance improved reads directed diffused copies disks system note case arranged files diffused start experiment reading threshold number times investigating sophisticated policies initiate access-driven diffusion left future work fast live-block recovery explore potential improvement live-block recovery figure presents recovery time d-graid varying amount live file system data figure plots lines worst-case best-case live-block recovery worst case live data spread disk case compacted single portion volume graph live-block recovery successful reducing recovery time disk half full note difference worst-case best-case times difference suggests periodic disk reorganization ruemmler wilkes speed recovery moving live data localized portion acm transactions storage vol improving storage system availability d-graid fig live-block recovery figure shows time recover failed disk hot spare d-graid level mirrored system live-block recovery lines d-graid plotted worst case live data spread entire -mb volume case compacted smallest contiguous space plotted recovery time idealized raid level fig availability profile figure shows operation d-graid level raid failures -gb array consisted data disks hot spare failure data reconstructed hot spare d-graid recovering faster raid failures occur raid loses files d-graid continued serve files workload consisted read-modify-writes -kb files randomly picked froma -gb working set benefits expect d-graid demonstrate improved availability alexander failures figure shows availability performance observed process randomly accessing -kb files running d-graid raidto ensure fair comparison d-graid raidlimited reconstruction rate acm transactions storage vol sivathanu table iii code size alexander implementation number lines code needed implement alexander shown column shows number semicolons column shows total number lines including white spaces comments semicolons total d-graid generic setup fault-isolated placement physical block allocation access driven diffusion mirroring live block recovery internal memory management hashtable avl tree file system specific sds inferencing ext sds inferencing vfat total figure shows reconstruction -gb volume -gb live data completed faster d-graid compared raids extra 
failure occured availability raiddropped d-graid continued availability surprisingly restore raidstill failed files linux retry inode blocks fail remount required raidreturns full availability complex implementation briefly quantify implementation complexity alexander table iii shows number statements required implement components alexander table core file system inferencing module ext requires lines code counted number semicolons core mechanisms d-graid contribute lines code rest spent hash table avl tree wrappers memory management compared tens thousands lines code comprising modern array firmware added complexity d-graid significant academic prototype complexity numbers slight underestimate required production quality implementation analysis intended approximate estimate d-graid levels discussion focused implementing d-graid storage system redundancy user data raidor mirrored storage system raidhowever mentioned layout mechanisms d-graid orthogonal underlying redundancy scheme section formalize levels d-graid popular traditional raid levels present custom policies d-graid level tailored underlying redundancy mechanism acm transactions storage vol improving storage system availability d-graid note contrast traditional raid levels levels d-graid differ type redundancy normal user data system meta-data maintained raidwith configured replication degree d-graidno redundancy simplest d-graid level redundancy mechanism employed normal user data single disk failure results data loss contrast traditional raidwhere single disk failure results complete data loss d-graidensures proportional data availability failure figure shows d-graidconfiguration absence redundancy normal data additional storage required access-driven diffusion d-graidneeds separate performance reserve asdescribed section reserve fixed percentage storage volume size tunable administrator tuning parameter administrator control tradeoff performance storage efficiency issue changing size performance reserve dynamically file systems equipped deal variable volume size limitation addressed simple technique administrator creates file file system reserved diffuse size file implicitly conveys d-graid size performance reserve file system blocks assigned reserved file file d-graid free storage space file system runs short storage administrator prune size special file dynamically reducing size performance reserve d-graidmirroring mirrored d-graid system stripes data multiple mirrored pairs similar raidnote d-graid meaningful storage system comprised single mirrored pair raidbecause system fundamentally partial failure mode access-driven diffusion policy d-graidis similar d-graidwhere dynamic performance reserve hold diffused copies figure depicts configuration note diffused copies mirrored d-graidrequires half percentage space d-graidrequires order achieve level diffusion slight variant d-graidcan make access-driven diffusion effective cost slight degradation reliability disks mirrored pair physical mirrors discussed employ logical mirroring impose logical disk block copies disks relaxed definition d-graid store copy file traditional striped fashion copy file stored fault-isolated fashion figure depicts configuration file fault-isolated copy laid single disk copy striped disks single disk failure result data loss logical mirroring data achieves benefits acm transactions storage vol sivathanu fig d-graid levels figures depict data layout d-graid redundancy schemes style shading represents file d-graidfigure color shading physical raidstripe diffusion segments striped region d-graidlogical separate regions disk simplicity practice interleaved fault-isolated copies acm transactions storage vol improving storage system availability d-graid fault-isolated placement impact performance parallelism striped copies note scenario extra space required access-driven diffusion variant d-graidimproves performance efficient access-driven diffusion reduces reliability compared traditional d-graidin traditional d-graidi physical mirroring single disk failure failure mirror disk lead loss data logical mirroring failure results loss data proportionally irrespective disk incurred failure d-graidparity d-graidis counterpart traditional raidredundancy user data maintained form parity encoding small number disks resulting space efficiency fine grained block-level striping fundamental raidwould conflict fault isolated placement d-graid techniques orthogonal fine-grained striping required raidoccurs physical level actual physical disk blocks fault-isolated placement logical assignment files physical blocks d-graidwould maintain invariant kth parity block xor kth block disk difference kth block disk data pertaining file d-graid raid part file configuration shown figure blocks belonging physical raidstripe shaded color fault-isolated placement raidlike redundancy leads performance issue blocks raidstripe longer part single file logically related full stripe writes uncommon block allocation policies writes partial stripes small writes performance problem requiring disk operations block written patterson address small write problem d-graidwe customized block allocation policy allocation policies section targeted preserving logical contiguity perceived file system d-graidrequires policy minimizes impact small writes policy log-structured allocation rosenblum ousterhout wilkes blocks written place allocated empty segments invalidating locations log structured allocation d-graidwould simply divide disk multiple segments time d-graidwould operate segment stripe comprises kth segment disk write arrives fault isolation module d-graidwould decide disk block laid allocate tail physical block segment logical block typical workload writes spread multiple files graid balances space utilization disks writes acm transactions storage vol sivathanu multiple files spread segments current segment stripe resulting full stripe writes note technique effective log cleaner coordinate cleaning entire set disks set freed segments comprise full segment stripes summary summary find basic layout techniques d-graid orthogonal underlying redundancy mechanism building top physical redundancy scheme d-graid strictly improves availability storage array custom policies access-driven diffusion physical block allocation make d-graid effective redundancy mechanism discussion impact wrong section fair amount complexity identifying logical file block belongs order place correct home site graceful degradation interesting question arises light complexity d-graid makes wrong inference d-graid permanently associates block wrong file places wrong home site incorrect inferences affect parts d-graid design differently graceful degradation component d-graid robust incorrect inferences incorrect association block wrong file affect fault isolation impact correctness d-graid miscalculates large fraction associations reliability resulting storage layout strictly traditional raid level d-graid builds top existing raid redundancy incorrect association lead layout completely fault isolated alayout exhibit fault isolation compared traditional raid face incorrect inference storage system correctness affected making d-graid ideal candidate make aggressive semantic information contrast live block recovery component d-graid depend semantic information correctness requires conservative estimate set live blocks volume d-graid requires estimate strictly conservative live block inferred dead lead loss data section tracking block liveness information conservatively simple straightforward realize d-graid requires accuracy simple piece semantic information implementing fast recovery design complexity d-graid related fault isolation graceful degradation component robust incorrect inference wrong bad acm transactions storage vol improving storage system availability d-graid conclusions robust system continues operate correctly presence class errors robert hagmann hagmann d-graid turns simple binary failure model found storage systems continuum increasing availability storage continuing operation partial failure quickly restoring live data failure occur article shown potential benefits d-graid established limits semantic knowledge shown successful d-graid implementation achieved limits simulation evaluation prototype implementation found d-graid 
built underneath standard block-based interface file system modification delivers graceful degradation live-block recovery access-driven diffusion good performance conclude discussions lessons learned process implementing d-graid limited knowledge disk imply limited functionality main contributions article demonstration limits semantic knowledge proof implementation limitations interesting functionality built inside semantically smart disk system semantic disk system careful assumptions file system behavior hope work guide pursue similar semantically smart disks easier build file systems reorder delay hide operations disks reverse engineering scsi level difficult small modifications file systems substantially lessen difficulty file system inform disk believes file system structures consistent on-disk state challenges disk lessened small alterations ease burden semantic disk development semantically smart disks stress file systems unexpected ways file systems built operate top disks behave d-graid specifically behave part volume address space unavailable heritage inexpensive hardware linux file systems handle unexpected conditions fairly exact model dealing failure inconsistent data blocks missing reappear true inodes semantically smart disks push functionality storage file systems potentially evolve accommodate acknowledgments anurag acharya erik riedel yasushi saito john bent nathan burnett timothy denehy brian forney florentina popovici lakshmi bairavasundaram insightful comments earlier drafts article jack harwood helpful discussions acm transactions storage vol sivathanu richard golding excellent shepherding earlier version article anonymous reviewers thoughtful suggestions greatly improved content article finally computer systems lab providing terrific environment computer science research acharya uysal saltz active disks programming model algorithms evaluation proceedings international conference architectural support programming languages operating systems asplos viii san jose alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca denver anderson chase vahdat interposed request routing scalable network storage acmtrans comput syst feb bitton gray disk shadowing proceedings international conference large data bases vldb los angeles boehm weiser garbage collection uncooperative environment softw pract exper sep burkhard menon disk array storage system reliability proceedings international symposium fault-tolerant computing ftcstoulouse france chapin rosenblum devine lahiri teodosiu gupta hive fault containment shared-memory multiprocessors proceedings acm symposium operating systems principles sosp copper mountain resort chen lee gibson katz patterson raid highperformance reliable secondary storage acm comput surv june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix monterey dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey emc corporation symmetrix enterprise information storage systems emc corporation hopkinton web site http emc english stepanov loge self-organizing disk controller proceedings usenix winter technical conference usenix winter san francisco ganger blurring line oses storage devices tech rep cmu-cs- carnegie mellon pittsburgh ganger mckusick soules patt soft updates solution metadata update problem file systems acmtrans comput syst ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement hicss gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings international conference architectural support programming languages operating systems asplos viii san jose gray computers stop proceedings international conference reliability distributed databases gray horst walker parity striping disc arrays low-cost reliable storage acceptable throughput proceedings international conference large data bases vldb brisbane australia gribble robustness complex systems proceedings eighth workshop hot topics operating systems hotos viii schloss elmau germany acm transactions storage vol improving storage system availability d-graid hagmann reimplementing cedar file system logging group commit proceedings acm symposium operating systems principles sosp austin texas holland gibson siewiorek fast on-line failure recovery redundant disk arrays proceedings international symposium fault-tolerant computing ftcstoulouse france hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international data engineering conference ibm serveraid recovering multiple disk failures web site http ibm qtechinfo migrhtml felten wang singh archipelago island-based file system highly scalable internet services proceedings usenix windows symposium katcher postmark file system benchmark tech rep trnetwork appliance sunnyvale web site http netapp keeton wilkes automating data dependability proceedings acm-sigops european workshop saint-emilion france kistler satyanarayanan disconnected operation coda file system acm trans comput syst feb mckusick joy leffler fabry fast file system unix acmtrans comput syst aug menon mattson comparison sparing alternatives disk arrays isca gold coast australia microsoft corporation web site http microsoft hwdev orji solworth doubly distorted mirrors proceedings acm sigmod international conference management data sigmod washington park balasubramanian providing fault tolerance parallel secondary storage systems tech rep cs-tr- princeton princeton patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod chicago patterson availability maintainability greatermuch performance focus century key note speech fast popek walker chow edwards kline rudisin thiel locus network transparent high reliability distributed system proceedings acm symposium operating systems principles sosp pacific grove reddy banerjee gracefully degradable disk arrays proceedings international symposium fault-tolerant computing ftcsmontreal canada riedel gibson faloutsos active storage large-scale data mining multimedia proceedings international conference large databases vldb york riedel kallahalla swaminathan framework evaluating storage system security proceedings usenix symposium file storage technologies fast monterey rosenblum ousterhout design implementation log-structured file system acmtrans comput syst feb rowstron druschel storage management caching past large-scale persistent peer-to-peer storage utility proceedings acm symposium operating systems principles sosp banff alto canada ruemmler wilkes disk shuffling tech rep hpl- hewlett packard laboratories palo alto saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area file system proceedings symposium operating systems design implementation osdi boston acm transactions storage vol sivathanu savage wilkes afraid frequently redundant array independent disks proceedings usenix annual technical conference usenix san diego sivathanu prabhakaran popovici denehy arpaci-dusseau arpacidusseau semantically-smart disk systems fast san francisco tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans wilkes golding staelin sullivan autoraid hierarchical storage system acmtrans comput syst feb wolf placement optimization problem practical solution disk file assignment problem proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics berkeley received august revised august accepted september acm transactions storage vol 
deconstructing commodity storage clusters haryadi gunawi nitin agrawal andrea arpaci-dusseau remzi arpaci-dusseau jiri schindler computer sciences department emc corporation wisconsin madison hopkinton massachusetts abstract traditional approach characterizing complex systems run standard workloads measure resulting performance end user unique opportunities exist characterizing system constructed standardized components inside system instrumenting components paper show intra-box instrumentation understand behavior large-scale storage cluster emc centera analysis leverage standard tools tracing disk network traffic emanating node cluster correlating traffic running workload infer structure software system write update protocol policies performs caching replication load-balancing imposing variable intra-box delays network disk traffic confirm causal relationships network disk events infer semantics messages nodes examining single line source code introduction systems community long understood benefits separating architecture implementation clean separation clients assured consistent standard interface designers freedom innovate interface common result simple interface hides growing amount internal implementation complexity trend occurred implementation microprocessors instruction sets storage systems modern storage systems simple interfaces hide great deal internal complexity high-end storage servers continue scsi interface simple read write interface exporting array blocks client storage servers implement range complex functionality tens processors gigabytes memory hundreds disks emc symmetrix storage server implements redundant data paths end-to-end checksumming machinery fence portions caches failure disk scrubbing technology discover latent errors proactively standard scsi interface today fundamental change occurring storage systems past storage systems built specialized parts assembled commodity components high-end storage servers built collections commodity pcs running commodity operating system connected ethernet network modern storage systems simply instances cluster workstations domains users understand behavior systems understanding enables critical evaluation design implementation choices users build models system behaves workloads tune debug performance enables administrators identify system behaving correctly drawbacks standard interfaces hide interesting information internal behavior measure evaluate systems application-level benchmarks microbenchmarks traditional approaches assume observe behavior system external interface shift storage system design leverage commodity components greatly increases ability analyze storage system behaves effect building system commodity components opens box users directly observe occurring inside users leverage existing standardized tools perform analysis paper develop set intra-box techniques analyze structure policies commodity-based storage clusters analysis major components monitor perturb network disk traffic internal storage cluster order deduce structure main communication protocols build protocol knowledge dissect internal policy decisions caching prefetching write buffering load balancing apply techniques important instance commodity storage cluster emc centera content-addressable storage system centera designed provide low-cost easy-to-manage scalable storage fixed content data medical images electronic documents archives content addressability centera advantage massive redundancy present data attachments reduce capacity requirements main contribution paper analysis centera protocols policies note results appears proceedings international symposium computer architecture isca achieved assistance emc verify accuracy relevance include metaanalysis results emc find centera chooses simplicity reliability sophisticated performance optimization good choice early implementation analysis reveals structure write update protocol standard two-phase commit writes committed synchronously disks policies centera caching prefetching mechanisms commodity file systems storage nodes leaving complicated caching schemes client applications derive interesting properties load balancing centera storage nodes gather disperse load information locally global effects network link performance account main contribution paper development intra-box techniques applied solely centera techniques widely applicable general lesson draw power probe points system observation internal cluster network disk traffic crucial approach future systems architectural support enable type detailed analysis paper structured present methodology structural analysis centera protocols policy inference analyze results including accuracy confirmation emc engineer discuss related work conclude methodology section begin presenting general overview storage cluster test emc centera describe intra-box techniques analyzing structure policies distributed systems system overview emc centera content addressable storage cas cluster designed storing retrieving fixed content information centera handles management physical storage resources transparently applications designed highly scalable no-singlepoint-of-failure platform centera content addressing applications access data objects blobs binary large objects -bit content address derived contents object hash function applied data architecture centera cluster redundant array independent nodes rain nodes connected private lan node runs centrastar software linux kernel operates storage node access node storage nodes hold objects local disks access nodes manage read write requests external client application server storage nodes general protocol centera application contacts access node delivers data object file centera write blobwrite api centera calculates unique content address object records metadata object separate file called c-clip descriptor file cdf cdf calculated returned application copies cdf blob stored data objects retrieved contacting access node read blobread api providing c-clip evaluate centera cluster access storage nodes utilizing smaller clusters experimental purposes node runs linux centrastar version secondgeneration hardware -ghz pentium memory intel etherexpress pro ethernet ports maxtor ata disks access node connected clients ethernet intra-box analysis traditional approach understanding behavior systems run standard workloads measure resulting performance end user unique opportunities exist characterizing system constructed standardized components intra-box analysis inside system instrumenting constituent components analyze behavior distributed storage server intra-box techniques observation delay traffic nodes disks passive observation network disk traffic track correlations requests general idea protocol structure internal policies observation enable definitively conclude causality requests infer message depends specific previous event delay messages disk requests observation derive general protocol structure policies begin simply observing traffic node disk case traffic observed straightforward tools trace tcp udp communication nodes tcpdump trace disk traffic insert kernel pseudo-device driver disk mount file system driver records start time end time block number read write request collect suitable observations run simple workload client application server workload repeatedly creates objects trace traffic network disk assume workload running centera perform multiple trials filter traffic occur consistently analysis performed offline avoid interference system activity analysis determine general properties communication disk activity system appears proceedings international symposium computer architecture isca nodes send messages nodes nodes read write local disk size network message time delay network disk events infer properties specific centera nodes cluster act access nodes versus primary secondary storage nodes delay passive observation network disk events correlate events observation enable determine exact dependencies events precisely repeatedly observes event occurs event infer depends correlation derive causality successful number potential weaknesses unrelated events consistently occur order simply due performance characteristics system large number iterations required observe event orderings occur consistently filter background noise finally difficult discover event dependent multiple preceding events derive causal 
relationships events delay network disk event observe subsequent events delayed delay receipt message observe sending message delayed infer dependent indirectly delayed infer independent complicating factor determining amount event delayed important issue events dependent multiple events suppose sending message dependent receiving messages arrives delay arrive usual time event triggers incorrectly conclude independent avoid situation inject large delay exceeds durations system tuned system interest delaying dependent events delayed desired delay network traffic employ nistnet modified version sits top tcpdumpand delays incoming packets framework select amount delay define criteria packets delayed centera messages uniquely identified size size protocol type tcp udp determine messages delayed utilize fields protocol headers addresses port numbers delay disk traffic pseudo-device driver tracing requests configure duration delay requests delayed reads writes delayed requests separate queue delay time expires deducing system structure section apply intra-box analysis derive internal structural protocols employed emc centera begin passive observations infer basic protocol storing accessing objects actively delay network disk events determine relationships events storing objects passive observations step passively trace network disk traffic emc centera analyze distinct protocols events occur user stores objects writes events occur user accesses objects reads analysis focused writes complex object write protocol figure pictorial representation findings centera protocol writing objects figure shows network disk events occur client centera nodes request figure reports duration intervals tcp message analysis presented diagram infer basic centera protocol storing objects note description tend nodes perform action action send udp message disk operation completes verified causal relationship events section sequences conjectures object write protocol begins client contacts access node directly sending -byte message access node issues disk read sets tcp connection primary storage node communicates primary storage node performs disk read sets connection secondary storage node communicates time client stores object tcp connection created access node storage nodes structural analysis protocol determine access node storage nodes selected selection policy decision explore section series messages propagates back secondary storage node client client receives response sends variable number messages access node infer messages data object size varies size objects access node forwards data primary storage node forwards data secondary storage node sending transfer acknowledgment storage node writes local disk communicates udp centera storage nodes udpx udpy events storage nodes propagate series -byte -byte acknowledgments access node finally access node informs client request appears proceedings international symposium computer architecture isca commit node access primary commit commit ack setuptcp conn tcp conn abe bce req complete tcp conn transfer ack req ack write req write req req ack transfer ack transfer fin commit tear downtcp conn write req req ack transfer fin commit commit ack tear transfer fin setup client storage secondary storage disk writes sync timeout storage node secondary primary storage node storage node transfer ack data transfer transfer fin udpy udpy update ack write update udpx udpx udpx snx commit figure anatomy centera write protocol figure left pictorial representation findings shows network disk events occur client centera nodes -kb object write blobwrite api tcp messages labeled values message unique label identifying endpoints simple message initial message nodes access node primary storage node average size bytes protocol message shown message assigned label reflecting understanding semantic purpose protocol interval tcp events centera node labeled unique identifier duration intervals reported figure single block disk reads occur intervals figure top shows detail events occur intervals udp messages shown designated label identifying destination node simple message udpx udp message storage node round protocol complete centera nodes tear tcp connections abe bce timing results reported figure reveal time spent storing -kb objects note client sees object stored latency approximately roughly sum times figure shows latency occurs intervals matching intervals figure corresponds access node waiting storage nodes read disk data object transferred access node storage nodes writing object disk object read protocol reads client node communicates access node turn reads disk establishes tcp connection storage nodes storage node receives request reads disk sends data object back access node access node transmits object client exchanges acknowledgments storage node tears tcp connection storage node finally informs client request complete figure shown due space limitation delaying events observing network disk events enables learn internal protocol centera observations conclude events occur delay events infer dependencies network disk events write protocol due space constraints present similar analysis read protocol message protocol determine set events depends delaying preceding events node observing delay subsequent sending message structure discussion investigating type event turn tcp traffic udp traffic completion disk reads writes appears proceedings international symposium computer architecture isca latency a-xx period access node latency b-xx period primary storage node latency c-xx period secondary storage node figure intervals writing objects figures report average duration intervals tcp message events graphs examine access node primary storage node secondary storage node graph examine intervals node x-labels match figure send receive time access node primary storage node secondary storage node abe send receive time packet abe bce packet bcebc packet figure impact delaying tcp packets write protocol delay interesting subset tcp packets pairs graphs involved centera nodes access primary storage secondary storage nodes y-axis report send receive time packet relative start request sharp increase send receive time packet packet delayed packet depends packet stands delay tcp traffic begin investigating straightforward case dependency outgoing tcp packet depends previously-arriving tcp packets note determine outgoing message dependent multiple tcp packets node due nature tcp tcp reliable byte-stream protocol receiver guaranteed packets order sender transmitted packets delayed lost subsequently resent check dependent receiving receiving dependent receiving guaranteed arrive depends waiting sufficient figure shows impact delaying interesting subset tcp packets pairs graphs report send receive time packet centera nodes involved access primary storage secondary storage nodes note sharp increase send receive time packet packet delayed packet depends packet measurements imply dependencies initial request exchange sending primary storage node depends receiving secondary storage node serves acknowledgment primary secondary storage nodes received request sending primary storage node depends receiving acknowledges primary secondary storage nodes received data object sending primary storage node depends receiving secondary serves acknowledgment storage nodes written object disk details finally measurements show interesting set relationships request complete specifically delaying impacts 
delaying impacts delaying impact protocol packet serves acknowledgment primary storage node received packet access node secondary storage node independence confirmed delay technique passive measurements observed udp traffic isolate events dependent arriving udp traffic interesting udp traffic occurs intervals primary secondary storage nodes non-trivial amount additional udp traffic occurs measurements udp traffic filtered occur regular points write protocol found delaying protocol events impact background udp traffic conclude udp traffic related write protocol appears proceedings international symposium computer architecture isca trial time delay send udp recv udp send time sec delay udp sec send udp recv udp send time sec delay udp sec send udp recv udp send figure impact delaying udp packets write protocol graphs show time primary storage node sends receives udp packets udp udp sends tcp packet graph default case extra delay graph delay udp graph delay udp seconds experiment repeated times total data received file size understanding cas snsn- figure content addressability figure plots amount data written storage nodes storing file file size increased file filled single repeated byte candidate space-savings due content-based addressing figure addresses relationship udp traffic events measurements performed primary storage node relationships identical secondary storage node graph independent trials show time udp packets storage nodes udp packets received tcp packet back access node graph reports observed timings default centera system delays udp traffic figure shows ordering sending udp packet storage node receiving udp packet storage node fact udpx udpy parallel passive observations sufficient show dependency messages graph show udp arrives udp udp arrives confirm true dependencies delay udp messages graph delay request udp seconds results show udp delayed udp acknowledgment udp interesting property apparent udp unreliable protocol centera software implements timeout-retry policy resend messages response received graph centera implements timeout udp packets point resends packets explore impact timeout policy depth graph experiments increase delay udp seconds circumstances storage node receives acknowledgment packet udp centera protocol performs timeout-retry intervals stops retrying point storage node sends final tcp packet receiving udp depends receiving udpx udpy waiting time-out approximately seconds disk events determine tcp udp messages dependent completion disk reads disk writes disk read operations occur centera nodes request initiated intervals disk write operations occur storage nodes data object received intervals figure shows delaying disk reads writes storage nodes impact subsequent messages graphs examine access node primary storage node secondary storage node make observations measurements storage nodes tcp message immediately disk read fact dependent disk read send times increase disk read delayed confirms initial intuition related note delaying reads nodes subsequent events delayed indicating reads overlapped storage nodes delaying disk writes delays sending udpx packet send time udpx delayed seconds storage nodes perform disk writes succession conclusion delaying network disk events centera write protocol identify events dependent cases analysis passive observations correlations case occurs end write protocol primary storage node sends commit-ack message access node received an-commit message access node received commit-ack storage node relying correlations determine commit-ack primary independent commit-ack secondary occurred case occurs storage nodes written disk storage nodes send tcp message data committed disk node attempted communicate udp storage nodes udp messages succeeded relying correlations inferred storage nodes receive udp replies sending tcp commit appears proceedings international symposium computer architecture isca abe send receive time packet access node write pri write sec read pri read sec read bce udp send receive time packet primary pri storage node write pri write sec read pri read sec read bcebc udpbc send receive time packet secondary sec storage node write pri write sec read pri read sec read figure impact delaying disk activity write protocol delay reading writing disk graphs involved centera nodes access primary storage secondary storage nodes y-axis graph report send receive time packet relative start request udp sending udpx sharp increase send receive time packet disk event delayed packet depends disk event inferring policies previous section analyzed protocol structure centera write read operations section infer policy decisions protocols analysis focus important functionality expect storage system replication load balancing caching prefetching key approach utilize derived structure write figure read shown protocols fine-tune analysis analysis caching information enable fine-grained accounting disk accesses enabling filter traffic system unrelated current request object write policies begin analyzing decisions occur write protocol originally shown figure continue assume observe delay network disk traffic content addressability begin inferring basic decision large units data centera segments file multiple blobs stored replicated accessed independently blob unit granularity content hashing duplicate detection performed storage allocated storage nodes determine size blob write file byte repeated size file written blob size file internally comprised identical blobs amount traffic halved behavior observed multiples blob size figure shows amount data transferred storage nodes primary secondary network increase size file written amount data transferred climbs steadily x-axis increases point drops cyclical pattern repeats indicating unit content addressability level replication centera data replication protect data unavailability corruption face failures fundamental choice level replication number copies data objects level readily apparent previous structural analysis figure shows replicas made object written experimentation shown range object sizes reveals level replication objects load balancing dissect load balancing strategy writes write enters system centera chooses primary storage node data primary storage node chooses secondary location infer load balancing policy factors determine storage nodes selected factors influence decision nodes place data item including current performance amount space analysis focus performance factors cpu utilization disk usage network connections network delay vary factors time controlled manner cpu run high priority loop varying fraction sleep time network delay varied modified nistnet disk usage generate background traffic file copy program open varying number tcp connections primary secondary nodes observe internal message traffic determine induced load impact centera placement decisions figure plots amount data written node load experiments figure factors influence selection nodes writes heavily skewing writes unloaded nodes cpu load disk load number network connections node interestingly observe increasing network delay incoming link storage node affect load balancing performance writes decreases dramatically shown increase latency incoming link storage node centera incorporate delay load balancing strategy hypothesize centera collecting performance appears proceedings international symposium computer architecture isca net delay tcp conn disk 
load cpu ratio normalized variety pertubation snsn- figure write load balancing results experiments shown run system normal mode time labeled add load resource storage node labeled y-axis plot normalized ratio traffic primary storage nodes storage node configuration load addition expect writes roughly balanced nodes expect imbalance skewing unloaded primary node number writes ----sn- --------- sn----- time delay reaction delay sec delay sec delay sec delay figure impact delaying udp message traffic graph illustrates impact delaying distribution load information y-axis plots difference number writes snand sninitially writes served snthe bold vertical line sec marks cpu load addition snthe arrows point times writes switch loaded node snto unloaded node sndue write load balancing strategy vary udp traffic delay seconds ratio writes normalized unload-load combination storage nodes distribution writes storage nodes unloaded loaded figure write constraints graph shows percentage writes directed pair nodes system configured storage nodes pair storage nodes loaded cpu load induced unloaded load induced experiment varied x-axis pair nodes unloaded loaded pair bars y-axis plots percentage writes directed loaded unloaded pair statistics storage node distributing information system periodically basing load balancing decisions confirm belief run experiment increase load storage node cpu load case delay udp message traffic cluster protocol analysis tcp centera writes reads udp virtually inter-node communication slowing udp messages hope slow spread load information confirm hypothesis figure reveals method load information dispersed figure longer udp message traffic delayed longer takes load balancing decision affected increased cpu load storage node confirm hypothesis load information dispersal finally additional constraints determine primary secondary copies data item isolate constraints experiments identical cpu loads pairs storage nodes figure shows results cases greater cpu load pair nodes greater fraction writes unloaded storage nodes cases number writes adjust cpu load pair nodes nodes unloaded load balancing policy react writes allocated roughly evenly loaded unloaded pairs centera ensures data item copy node node copy node node inspection power distribution centera reveals reason pair nodes separate power supply centera write load balancing sensitive performance factors constrained factors influence reliability power source caching buffering important performance optimization present storage systems write buffering write transforming writes asynchronous operations application-perceived latency greatly reduced copying data in-memory buffer faster committing disk trade-off terms reliability delaying commit disk chance data loss failure increases protocol analysis shown figure revealed access client nodes notified disk write committed storage nodes conclude centera performs write operations synchronously centera developers chose safety reliability performance object read policies turn attention read protocol reads complex performance characteristics crucial applications caching begin determining caching data objects performed centera read protocol demonstrate benefits intra-box techniques begin assuming access internals centera observe performance client begin simple workload repeatedly reads file comparing difference time read subsequent reads environments determine caching present identical latency numbers row table conclude caching taking place case wrong appears proceedings international symposium computer architecture isca client latency delay delay data read an-cli sn-an disk-sn table read caching table left shows time read file centera column shows time read column shows average time subsequent reads row shows experiment large disk delay induced table shows breakdown traffic access node client storage node access node disk storage node leverage ability observe delay events inside centera insert substantial disk delay read disk storage node row table illustrates numbers show large difference time read subsequent reads file caching taking place centera presence caching observable centera mbit ethernet delivers data quickly ide disks inserting delay disks change relative ratios network disk observe caching taking place system experiment reveal system caching occurring complete read caching analysis monitor network disk traffic previously-described experiment results analysis presented table table shows data transferred access node client storage node access node disk storage node subsequent file accesses table shows requests amount data transferred access node client storage node access node client access nodes performing caching table shows data transferred disk storage node subsequent requests storage node performs in-memory caching prefetching prefetching important optimization storage systems experiments determine centera performs prefetching components perform prefetching experiment read data sequentially file small chunks time read client slow disks exacerbate difference on-disk in-memory accesses graph figure shows results experiment graph observe read takes significant amount time requests completed rapidly clientperceived timing result conclude prefetching taking place centera specifically centera time seconds iteration client-perceived prefetching disk delay byte bytes transferred request size read request sequential requests access node prefetching sn-an an-client figure prefetching graph left shows time sequential read file graph shows amount bytes transferred network storage node access node access node client workload rightmost graph run multiple tests varying size request byte prefetches block read caching unearth system prefetching occurs graph figure plots network traffic experiment x-axis vary size read file y-axis plot data transferred request graph shows amount data transferred centera nodes slightly size requested data specifically client requests bytes centera bytes storage node access node bytes access node client results draw conclusions extra bytes passed centera nodes prefetching occuring network prefetching occuring storage node extra information roughly bytes passed header storage node access node node passed client load balancing examine load balancing reads load balancing writes great deal flexibility large-scale system primary copy half nodes secondary node half reads constrained read storage nodes data located experiments seek understand factors centera determine copy data accessed examine performance factors cpu utilization disk usage network connections network delay surprisingly found centera read balancing policy completely insensitive loads induced shown node responds slowly read requests nodes read requests directed appears proceedings international symposium computer architecture isca analysis analyze design implementation centera storage server subsection present perspectives wisconsin emc adding emc perspective offer insight accuracy relevance wisconsin analysis protocol structure wisconsin protocol structure reveals basic elements centera design observe basic two-phase commit protocol writes generation secondary copy handled primary storage node implementation access node send data storage nodes trade-off clear reasonable centera latency potentially higher load access node decreased finally tcp udp purposes centera communication system tcp important aspects data transfer writes reads udp contrast traffic periodic heartbeats load balancing propagating load information observe tcp connection created data transfer large cost current generation system future centera implementations 
caching connections storage nodes avoiding costly three-way tcp handshake teardown emc analysis correctly identifies majority protocol features illuminates centera design principles workload characteristics centera designed on-line archival fixed content mostly-write operations mediumand large-sized objects reliability write-ingest important read performance lower latency extra latency writes introduced storage node relay typically small congestion occurs rarely internal network dual paths switches shielded traffic finally cost setting tearing tcp connection write initially deemed negligible targeted object sizes provided simple scalable solution clusters nodes recent centrastar versions reuse tcp connections tune number open connections based cluster size load udp messages write protocol updates distributed hash table translating content address location constant time message delivered attempts write transaction reports success logs exceptional case retries update time observed occasional disk reads observed intervals directly related write transaction priori knowledge content address write protocol performs lookup distributed hash table found data transferred cluster observed analysis read caching prefetching wisconsin analysis reveals storage nodes perform caching prefetching expect nodes runs commodity file system caching prefetching performed client access nodes decision reasonable benefit client terms latency data fetched access node storage node cases data cross network client host remember user application accesses data running host designers consume precious memory resources client node caching prefetching emc emphasis leveraging commodity components storage node file system disk drive caches emphasis access latencies extra hop internal network warrant impact complexity performance access nodes file server-like environments repeated reads objects centera offers separate gateway sits front cluster translates nfs cifs requests centera api operations implements caching avoids accesses cluster altogether design goal provide light-weight centera api library applications existing caches implement short gateway application-specific caching reduce caching access nodes write caching buffering wisconsin analysis shows centera synchronous system writes write buffering performed centera leans simplicity reliability write completes successfully means reliably committed disks storage nodes synchronous writing slow next-generation centera options improve performance nvram found higher-end emc products emc patch developed linux community centera ensures data reliably written media arguably non-commodity nvram increase complexity handling exceptional states hardware costs emc products include nvram make trade-off favor increased performance read-modify-write workloads replication load balancing wisconsin investigation centera replication reveals uniform approach objects found disks system control applications enabling create copies valuable data centera found perform load balancing storage nodes writes deappears proceedings international symposium computer architecture isca cisions based locally observable storage nodes demonstrated inducing delay incoming network link centera approach load balancing perform load measurable perspective storage node future important gather information load balancing decisions higher levels system measuring write perspective access node measured history make robust placement decisions centera takes power distribution network account system built similar orthogonal raid designs sources failure account placing data replicas disks finally clear presence load balancing machinery writes centera perform load balancing reads node performing poorly read performance system suffers future versions centera correcting oversight emc network delays issue paths node node load balanced paths load observations local node propagated nodes periodic broadcasts observations additional udp traffic piggybacked messages observations extra data transfers section access nodes information selecting storage nodes nature content addresses distribution data storage nodes reads spread equally nodes balancing load aggregate emphasis write operations fact network delays due congestion occur centrastar version analyzed employ load balancing individual read operations added versions similar load balancing writes intra-box analysis observed node balances load internal disks finally current hardware power distribution system eliminates constraints placing replicas nodes content addressability wisconsin observed centera blob size potentially missing opportunities capacity savings achieved smaller blobs smaller blobs imply metadata blob tracking desirable application wishes maximize usage content addressability expecting system find detailed content similarity objects emc implementing single-instance storage object level chunks efficient storage management hundreds millions objects applications advantage single-instance feature combine fast lookup potentially eliminate unnecessary data transfer related work intra-box techniques similar recent line work performance debugging complex systems major difference work related work level detail infer assume knowledge storage systems generally function support caching prefetching domain-specific functions discover specific structural policy details general techniques goals work related work differ specifically work seeks understand structure policies storage system approaches primarily aimed performance debugging specifically aguilera infer causal paths distributed systems message level traces techniques finding component performance bottleneck approach limited assume message dependent arrival previous message complex dependencies found storage systems similarly chen detect failures diagnose performance problems runtime path analysis unlike aguilera analysis chen assume existence message tags system track dependencies advantage approaches intra-box techniques run system interest online running real workload contrast approach applied quiesced system controlled workloads future hope extend approach operational systems previous research characterizing behavior storage systems operated domains work focused single disk worthington identify characteristics disk mapping logical block numbers physical locations size prefetch window prefetching algorithm caching policy previous work characterize traditional raid systems automatically infer number disks chunk size level redundancy layout scheme related work similar approach slowing components learn behavior system brown table locking infer dependence higher-level queries database tables comparison slow network disk traffic understand aspects storage system test communication slowdown mechanism similar presented martin approach learn aspects network performance affects application performance network slowdown infer dependencies components storage cluster appears proceedings international symposium computer architecture isca conclusion paper shown intra-box techniques applied deconstruct protocols policies modern commodity-based storage cluster emc centera analysis infer design implementation system access single line source code general study demonstrates power probe points system observing slowing system components learned structure complex system systems continue grow complexity intra-box techniques much-needed addition toolbox systems analysts techniques developed hope systems built intra-box approach mind externally visible probe points opening box systems readily understood analyzed debugged result generation higher performing robust reliable computer systems centera generation hardware centrastar releases place version observations made longer apply nonetheless work slow-down causality analysis helped emc fine-tune aspects centera protocols acknowledgments lakshmi bairavasundaram todd jones james nugent florentina popovici vijayan prabhakaran muthian sivathanu helpful discussions comments paper ana bizarro assistance setting access centera finally anonymous reviewers helpful suggestions work sponsored nsf ccrccr- ngsitr- itribm network appliance emc aguilera mogul wiener reynolds muthitacharoen performance debugging distributed systems black boxes sosp bolton landing 
amdahl blaauw brooks architecture ibm system ibm journal research development april anderson culler patterson team case networks workstations ieee micro february arpaci culler krishnamurthy steinberg yelick empirical evaluation cray-t compiler perspective isca santa margherita ligure italy bagchi kar hellerstein dependency analysis distributed systems fault injection international workshop distributed systems nancy france october barham isaacs mortier narayanan magpie real-time modeling performance-aware systems hotos lihue hawaii bohossian fan lemahieu riedel bruck computing rain reliable array independent nodes ieee transactions parallel distributed computing brown kar keller active approach characterizing dynamic dependencies problem determination distributed environment ifip ieee internationalsymposium integrated network management cantin hill cache performance selected spec cpu benchmarks computer architecture news september carson santay nist network emulation tool snad ncsl nist gov nistnet january cas-community http cascommunity chen accardi kiciman patterson fox brewer path-based failure evolution management nsdi san francisco march chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june chen patterson approach performance evaluation self-scaling benchmarks predicted performance sigmetrics pages santa clara cypher konstantinidou messina architectural requirements parallel scientific applications explicit communication isca san diego denehy bent popovici arpaci-dusseau arpaci-dusseau deconstructing storage arrays asplos pages boston massachusetts october emc emc centera content addressed storage system http emc gray reuter transaction processing concepts techniques morgan kaufmann hennessy patterson editors computer architecture quantitative approach edition morgan-kaufmann lee thekkath petal distributed virtual disks asplos vii cambridge october martin vahdat culler anderson effects communication latency overhead bandwidth cluster architecture isca denver mcvoy staelin lmbench portable tools performance analysis usenix san diego january panasas panasas active-scale storage cluster http panasas patterson gibson ginting stodolsky zelenka informed prefetching caching sosp pages copper mountain resort december policroniades pratt alternatives detecting redundancy storage systems data usenix boston june rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saavedra smith measuring cache tlb performance effect benchmark runtimes ieee trans-actions computers saito frolund veitch merchant spence fab building reliable enterprise storage systems cheap asplos boston massachusetts october schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november staelin mcvoy mhz anatomy micro-benchmark usenix pages orleans june sterling editor beowulf cluster computing linux mit press october talagala arpaci-dusseau patterson microbenchmark-based extraction local global diskcharacteristics technical report csd- california berkeley woo ohara torrie shingh gupta splashprograms characterization methodological considerations isca santa margherita ligure italy worthington ganger patt wilkes online extraction scsi disk drive parameters sigmetrics pages ottawa canada 
explicit control batch-aware distributed file system john bent douglas thain andrea arpaci-dusseau remzi arpaci-dusseau miron livny computer sciences department wisconsin madison abstract present design implementation evaluation batch-aware distributed file system bad-fs system designed orchestrate large o-intensive batch workloads remote computing clusters distributed wide area bad-fs consists components storage layer exposes control traditionally fixed policies caching consistency replication scheduler exploits control workloads extracting control storage layer placing external scheduler bad-fs manages storage computation coordinated gracefully dealing cache consistency fault-tolerance space management issues workload-specific manner microbenchmarks real workloads demonstrate performance benefits explicit control delivering excellent end-to-end performance wide-area introduction traditional distributed file systems nfs afs built solid foundation empirical measurement studying expected workload patterns researchers developers long make trade-offs system design building systems work workloads interest previous distributed file systems targeted computing environment collection interactively client machines past work demonstrated workloads lead designs filenet google file system assumptions usage patterns sharing characteristics aspects workload change reexamine design decisions embedded distributed file systems area increasing interest batch workloads long popular scientific community batch computing increasingly common broad range important commercially viable application domains including genomics video production simulation document processing data mining electronic design automation financial services graphics rendering batch workloads minimally present system set jobs run ordering environments approximate run times requirements advance scheduler information dispatch jobs maximize throughput batch workloads typically run controlled localarea cluster environments organizations large workload demands increasingly ways share resources wide-area lower costs increase productivity approach accessing resources wide-area simply run localarea batch system multiple clusters spread wide-area distributed file system backplane data access approach fraught difficulty largely due handled primary problem traditional distributed file system approach control decisions caching consistency fault tolerance made implicitly file system decisions reasonable workloads file systems designed ill-suited wide-area batch computing system minimize data movement wide-area system carefully cache space remote clusters caching decisions buried deep distributed file systems preventing control mitigate problems enable utilization remote clusters o-intensive batch workloads introduce batch-aware distributed file system badfs bad-fs differs traditional distributed file systems approach control bad-fs exposes decisions commonly hidden inside distributed file system external workload-savvy scheduler bad-fs leaves consistency caching replication decisions scheduler enabling explicit workload-specific control file system behavior main reason migrate control file system scheduler information scheduler intimate knowledge workload running exploit knowledge improve performance streamline failure handling combination workload information explicit control file system leads distinct benefits traditional approaches appears usenix symposium networked systems design implementation nsdi enhanced performance carefully managing remote cluster disk caches cooperative fashion controlling needed data transported wide-area bad-fs minimizes wide-area traffic improves throughput workload knowledge bad-fs improves performance capacityaware scheduling avoid thrashing improved failure handling detailed workload information scheduler determine make replicas data based cost generating indiscriminately typical file systems data loss treated uniformly performance problem scheduler ability regenerate lost file rerunning application generated replicates cost regeneration high simplified implementation detailed workload information simpler implementation bad-fs cooperative cache implement cache consistency protocol exact knowledge data dependencies scheduler ensures proper access ordering jobs previous work demonstrated difficulties building general cooperative caching scheme demonstrate benefits explicit control prototype implementation bad-fs synthetic workloads demonstrate bad-fs reduce wide-area traffic order magnitude avoid performance faults capacity-aware scheduling proactively replicate data obtain high performance spite remote failure real workloads demonstrate practical benefits system o-intensive batch workloads run remote resources easily high performance finally bad-fs achieves ends maintaining site autonomy support unmodified legacy applications practical constraints important acceptance wide-area batch computing environments rest paper organized section describe assumptions expected environment workload section discuss architecture system section present experimental evaluation section examine related work finally section conclude background section describe setting bad-fs present expected workloads basing assumptions recent work batch workload characterization describe computing environment users workloads difficulty encounter executing workloads conventional tools workloads illustrated figure data-intensive workloads composed multiple independent vertical sequences job job job job job job job job job job job endpoint batch batchendpoint input input pipeline data pipeline data pipeline data input output job width depth figure typical batch-pipelined workload single pipeline represents logical work user wishes complete comprised series jobs users assemble pipelines batch explore variations input parameters input data processes communicate ancestors relatives private data files workload generally consists large number sequences incidentally synchronized beginning logically distinct correctly execute rate siblings refer vertical slice workload pipeline horizontal slice batch entire set batch-pipelined workload note pipeline generically processes connected unix-style pipes communicate files key differences single application batch-pipelined workload file sharing behavior instances pipeline run executable potentially input files characterize sharing occurs batch-pipelined workloads breaking activity types shown figure endpoint unique input final output pipeline-shared shared write-then-read data single pipeline batchshared input data shared multiple pipelines environment wide-area sharing untrusted arbitrary personal computers platform batch workloads platform types throughput-intensive workloads clusters managed machines spread wide area assume cluster machine processing memory local disk space remote users cluster exports resources cpu sharing system obvious bottleneck system wide-area connection managed carefully appears usenix symposium networked systems design implementation nsdi ensure high performance simplicity focus efforts case single cluster accessed remote user section present preliminary results multi-cluster environment refer organized hostile managed collection clusters cluster-to-cluster system contrast popular peer-to-peer systems environment organized effort share computing resources corporations organizations assume environments stable powerful trustworthy technologies designs directly applicable domain make practical important assumption site local autonomy resources autonomy primary implications design bad-fs workload remote resources time resources arbitrarily revoked system built exploit remote resources tolerate unexpected resource failures due physical breakdowns software failures deliberate preemptions autonomy prohibits deployment arbitrary software remote cluster designing bad-fs assume remote cluster ability dispatch well-defined job ordinary unprivileged user mandating single distributed file system viable solution finally assume jobs run systems modified experience scientific workloads product years fine-tuning complete viewed untouchable ease important work user current solutions user wishes run batchpipelined workload environment user developed debugged workload home system ready run batches hundreds thousands computing resources remote batch execution systems condor lsf pbs grid engine pipeline workload expected input data varying parameters small inputs input data begins user home storage server ftp server output data generated eventually committed home server conventional batch computing systems present 
user options running workload option remote simply submit workload remote batch system option input output occur demand back home storage device approach simple throughput data-intensive workload drastically reduced factors wide-area network bandwidth sufficient handle simultaneous batch reads data-intensive pipelines running parallel pipeline output directed back home site including temporary data needed computation completes option pre-staging user manually configure system replicate batch data sets remote environment approach requires user obtain account remote environment identify input data transfer data remote site log remote system unpack data location configure workload recognize correct directories possibly tmp temporary pipeline data submit workload manually deal failures entire process repeated data processed batch systems existing systems longer capacity offer user obvious description configuration process laborintensive error-prone additionally tmp challenging availability guaranteed limitation user made configurations independently scheduling system scheduling system correctly checkpoint pipelines workload users lengths simply run workloads traditional distributed file systems solution typically due administrative desire preserve autonomy domain boundaries systems fixed policies prevent viable batch-pipelined workloads blast commonly genomic search program consisting single stage pipeline searches large shared dataset protein string matches assume user run blast compute cluster nodes equipped conventional distributed file system afs nfs cold caches nodes individually simultaneously access home server large demands resulting poor performance dataset redundantly transferred wide area network caches loaded node run local disk speeds dataset fit cache node thrash generate enormous amount repetitive traffic back home server lacking workload information node employ mechanism protect consistency availability cached data contrast batch-aware system bad-fs global view hardware configuration workflow structure execute workloads efficiently copying dataset single time wide appears usenix symposium networked systems design implementation nsdi queries catalogscheduler home storage ssss cccccccc movementdata remote cluster remote cluster statusupdatesjob data placements complete job notices figure system architecture circles compute servers execute batch jobs squares storage servers hold cached inputs temporary outputs types servers report catalog server records state system scheduler information catalog direct system configuring storage devices submitting batch jobs gray shapes elements design white standard components found batch systems area sharing duplicating data remote cluster explicit knowledge sharing characteristics permits system dispense expense complexity consistency checks allowing nodes continue executing disconnected architecture section present architecture implementation bad-fs recall main goal design bad-fs export sufficient control remote scheduler deliver improved performance fault-handling o-intensive batch workloads run remote clusters figure summarizes architecture bad-fs elements shaded gray bad-fs structured types server processes manage local resources compute server exports ability transfer execute ordinary user program remote cpu storage server exports access disk memory resources remote procedure calls resemble standard file system operations permits remote users allocate space abstraction called volumes interposition agents bind unmodified workloads running compute servers storage servers types servers periodically report catalog server summarizes current state system scheduler periodically examines state catalog considers work assigns jobs compute servers data storage servers scheduler obtain data executables inputs number external storage sites simplicity assume user data stored single home storage server standard ftp server perspective scheduler compute storage servers logically independent specialized device run type server process diskless workstation runs compute server storage appliance runs storage server typical workstation cluster node computing disk resources runs bad-fs run environment multiple owners high failure rate addition usual network system errors bad-fs prepared eviction failures shared resources revoked warning rapid rate change systems creates possibly stale information catalog bad-fs prepared discover servers attempts harness longer bad-fs makes standard components compute servers condor startd processes storage servers modified nest storage appliances interposition agents parrot agents catalog condor matchmaker servers advertise catalog classad resource description language storage servers storage servers responsible exporting raw storage remote sites manner efficient management remote schedulers storage server fixed policy managing space makes policies accessible external users carve space caching buffering tasks fit abstraction called volumes storage servers users allocate space lifetime type specifies policy internally manage space bad-fs storage server exports distinct volume types scratch volumes cache volumes scratch volume self-contained read-write file system typically localize access temporary data scheduler scratch volumes pipeline data passed jobs buffer endpoint output scratch volumes scheduler minimizes home server traffic localizing pipeline writing endpoint data pipeline successfully completes cache volume read-only view home server created home server path caching policy lru mru maximum storage size multiple cache volumes bound cooperative cache volume catalog server storage servers query discover peers number algorithms exist managing cooperative cache intent explore range algorithms describe reasonable algorithm system explain scheduler cooperative cache built distributed hash table keys table block addresses values server primarily responsiappears usenix symposium networked systems design implementation nsdi ble block avoid wide-area traffic primary server fetch block home server servers create secondary copies primary space needed secondary data evicted primary approximate locality initial implementation forms cooperative caches peers subnetwork initial analysis suggests sufficient future plan investigating complicated grouping algorithms failures cooperative cache including partitions easily managed slowdown cooperative cache internally partitioned primary blocks assigned missing peers reassigned long home server accessible partitioned cooperative caches refetch lost data continue noticeable disturbance running jobs approach cooperative caching important differences previous work data dependencies completely scheduler implement cache consistency scheme read data considered current scheduler invalidates volume design decision greatly simplifies implementation previous work demonstrated difficulties building general cooperative caching scheme unlike previous cooperative caching schemes manage cluster memory cooperative cache stores data local disks managing memory caches cooperatively advantageous important optimization make environment avoid data movement wide-area managing remote disk caches simplest effective local global control note volumes export degree control scheduler creating deleting volumes scheduler controls data sets reside remote cluster storage servers retain control per-block decisions important decisions made locally storage servers assignment primary blocks cooperative cache cache victim selection scheduler careful space allocation cache victimize blocks longer needed general found separation global local control suitable workloads work precisely identify balance point clear trade-off extreme complete local control current approach suffers policies embedded distributed file systems inappropriate batch workloads extreme complete global control scheduler makes decisions block data require exorbitant complexity scheduler incur excessive network traffic exert fine-grained control interposition agents order permit ordinary workloads make storage servers interposition agent transforms posix operations storage server calls 
agent mapping logical path names physical storage volumes provided scheduler runtime agent volume abstraction hide large number errors job end user volume longer exists due accidental failure deliberate preemption storage server returns unique volume lost error agent discovering agent forcibly terminates job indicating run correctly environment scheduler clear indication failures transparent recovery actions scheduler bad-fs scheduler directs execution workload compute storage servers combining static workload description dynamic knowledge system state specifically scheduler minimizes traffic wide-area differentiating types treating appropriately carefully managing remote storage avoid thrashing replicating output data proactively data expensive regenerate workflow language shown figure declarative workflow language describes batch-pipelined workload shows scheduler converts description execution plan keyword job names job binds description file specifies information needed execute job parent ordering jobs volume keyword names data sources required workload volume ftp server volumes empty scratch volumes volume sizes provided scheduler allocate space appropriately themountkeyword binds volume job namespace jobs access volume mydata jobs share volume path tmp extract command files interest committed home server case pipeline produces file retrieved uniquely renamed readers accustomed working interactive environment language unusual burden point user intending execute batch-pipelined workloads exceptionally organized batch users provide information scattered shell scripts make files batch submission files addition imparting needed information bad-fs scheduler workflow language reduces user burden collecting dispersed information coherent appears usenix symposium networked systems design implementation nsdi job condor job condor job condor job condor parent child parent child volume ftp home data volume scratch volume scratch mount mydata mount mydata mount tmp mount tmp mount tmp mount tmp extract ftp home extract ftp home joba jobb mount extract extract mount mount volume volume mount mount jobc mount volume job home storage data agent job scheduler catalog home storage execute cleanup submit compute server query extract configure storage server figure workflow scheduler examples simple workflow script directed graph jobs constructed job parent file system namespace presented jobs configured volume mount extract keyword files committed home storage server pipeline completion graphical representation workflow scheduler plan job scheduler queries catalog current system state decides place job data scheduler creates volumes storage server job dispatched compute server job executes accessing volumes agent jobs complete scheduler extracts scheduler frees volumes scoping unlike file systems badfs aware flow data workflow language scheduler data originates needed create customized environment job minimize traffic home server refer scoping scoping minimizes traffic ways cooperative cache volumes hold read-only batch data figure volumes reused modification large number jobs scratch volumes figure localize pipeline data job executes accesses volumes explicitly created home server accessed batch data pipeline consistency management workload information expressed workflow language scheduler neatly addresses issue consistency management required dependencies jobs data directly scheduler runs jobs meet constraints implement cache consistency protocol bad-fs storage servers user make mistakes workflow description affect cache consistency correct failure recovery understanding expected workload behavior user scheduler easily detect mistakes warn user results workload compromised implemented detection features architecture readily admits capacity-aware scheduling scheduler responsible throttling running workload avoid performance faults maximize throughput carefully allocating volumes scheduler avoids overflowing storage thrashing caches disk capacity rapidly increasing size data sets growing space management remains important scheduler manages space retrieving list storage catalog server selecting ready job unfulfilled storage pipe batch scheduler allocate job volumes allocates configures volumes schedules job jobs execute space scheduler waits job complete resources arrive failure occur note due lack complete global control scheduler slightly overprovision needed volume size approaches storage capacity scheduling domains selecting smallest job result starvation domain starvation avoided workflow static entity executed scheduler smaller jobs run jobs eventually run failure handling finally scheduler makes bad-fs robust failures handling failures jobs storage servers catalog aspect batch workloads leverage job idempotency job simply rerun order regenerate output scheduler log allocations persistent storage transactional interface compute storage servers scheduler fails allocated volumes running jobs continue operate unappears usenix symposium networked systems design implementation nsdi aided scheduler recovers simply re-reads log discover resources allocated resumes normal operations recording allocations persistently re-discovered released timely manner log irretrievably lost workflow resumed beginning previously acquired leases eventually expire contrast catalog server soft state catalog discover resources recover state crash catalog unavailable scheduler continue operate resources discover catalog server recovers rebuilds knowledge compute storage servers send periodic updates scheduler waits passive indications failure compute storage servers conducts active probes verify job exits abnormally error indicating failure detected interposition agent scheduler suspects storage servers housing volumes assigned job faulty scheduler probes servers volumes healthy assumes job encountered transient communication problems simply reruns volumes failed unreachable period time assumed lost failure volume affects jobs design simplification scheduler considers partial volume failure failure entire volume future plan investigate trade-offs involved choice failure granularities running jobs rely failed volume stopped addition failures cascade completed processes wrote volume rolled back re-run order avoid expensive restarts pipeline scheduler checkpoint scratch volumes pipeline stages complete determining optimal checkpoint interval problem solution depends likelihood failure checkpoint cost create unlike systems bad-fs solve problem automatically scheduler unique position measure controlling variables scheduler performs simple cost-benefit analysis runtime determine checkpoint worthwhile algorithm works scheduler tracks average time replicate scratch volume cost initially assumed order trigger replication measurement determine benefit replication scheduler tracks number job storage failures computes mean-time-to-failure devices system benefit replicating volume sum run times jobs completed applicable pipeline multiplied probability failure benefit exceeds cost scheduler replicates volume storage server insurance failure original fails scheduler restarts pipeline saved copy due robust failure semantics scheduler handle network partitions differently failures partitions formed scheduler compute servers scheduler choose reschedule jobs running side partition situation partition resolved point scheduler find multiple servers executing jobs note introduce errors job writes distinct scratch volumes scheduler choose output extract discard practical issues primary obstacles deploying distributed system friendly administrator deploying operating system file system batch system vast majority software requires privileged user install oversee software requirements make forms distributed computing practical impossibility larger powerful facility difficult ordinary user obtain administrative privileges end bad-fs packaged virtual batch system deployed existing batch system special privileges technique patterned glide-in job frey similar spirit recursive virtual machines run bad-fs 
ordinary user submit jobs existing batch system bad-fs bootstraps systems relying basic ability queue run self-extracting executable program storage compute servers interposition agent deployed servers report catalog server scheduler harness resources note scheduling virtual batch jobs discretion host system jobs interleaved time space jobs submitted users technique deploy badfs existing condor pbs batch systems practical issue security bad-fs grid security infrastructure gsi public key system delegates authority remote processes time-limited proxy certificates bootstrap system submitting user enter password unlock private key home node generate proxy certificate user-settable timeout proxy certificate delegated remote system storage servers authenticate back home storage server requires users trust host system steal secrets reasonable environment appears usenix symposium networked systems design implementation nsdi pipebatch mixed home traffic traffic remote pipe-local caching bad-fs pipebatch mixed runtime runtime figure scoping traffic reduction run times graphs show total amount network traffic generated runtimes number workloads optimizations enabled experiment run synthetic pipelines depth generates total x-axis vary relative amounts batch pipeline batch workload generates batch pipeline common types workloads amount endpoint small leftmost graph shows total amount home server traffic shows total runtimes home server accessed emulated wide-area network set experimental evaluation section present experimental evaluation bad-fs variety workloads present methodology focus scoping capacity-aware scheduling failure handling synthetic workloads understand system behavior present experience running real workloads system controlled environment finally discuss initial experience bad-fs run real workloads multiple clusters wild methodology initial experiments section build environment similar section assume user input data stored home server pipelines run output data safely stored back home server workload considered complete assume workload run remote cluster machines accessible user home server wide-area link emulate scenario limit bandwidth home server simple network delay engine similar dummynet remotely run jobs home server traverse slow link cluster dedicated compute pool condor nodes wisconsin connected mbit ethernet switch node pentiumprocessors physical memory ibm scsi drive partition made condor jobs partitions typically half time rest awaits lazy garbage collection explore performance bad-fs range workload scenarios utilize parameterized synthetic batch-pipelined workload synthetic workload configured perform varying amounts endpoint batch pipeline compute lengths time exhibit amounts batch pipeline parallelism experiment requires parameters leave descriptions individual figure captions previous results workload analysis focus batch-intensive workloads exhibit high degree batch sharing pipeline endpoint pipeintensive perform large amounts pipeline generate batch endpoint scoping results experiment shown figure demonstrate bad-fs scoping minimize traffic wide area localizing pipeline scratch volumes reusing batch data cooperative cache volumes optimizations straightforward ability increase throughput significant experiment repeatedly run synthetic workload vary relative amount batch pipeline compare number system configurations remote configuration home node baseline compare pipeline localization caching optimizations finally optimizations combined bad-fs configuration note experiments assume copious cache space controlled environment capacity-aware scheduling failure recovery needed left-hand graph shows total transferred wide-area network surprisingly cooperative cache greatly reduces batch traffic home node ensuring batch data set retrieved cache pipeline localization optimizations work expected removing pipeline home server finally optimization isolation sufficient bad-fs configuration combines minimize network traffic entire workload range right-hand graph figure shows runtimes workloads emulated remote cluster graph direct impact wide-area traffic runtime capacity-aware scheduling examine benefits explicit storage management previous experiments run environment storage capacity increasing size batch data sets storage sharing jobs users scheduler carefully manage remote space avoid wide-area thrashing experiments compare capacity-aware bad-fs scheduler simple variants depth-first scheduler breadth-first scheduler algorithms aware data workload appears usenix symposium networked systems design implementation nsdi runtime hours batch intensive dfs bfs bad-fs normalized home traffic batch data total coop cache dfs bfs bad-fs figure batch-intensive explicit storage management graphs show benefits explicit storage management batchintensive workload workload consists -stage pipelines stage process streams shared batch file batch files total batch file size varied percentage total amount cooperative cache space nodes experiment amounts negligible nodes local storage portion cache total cache size set x-axis reflects observations storage condor pool base decisions solely job structure workflow depth-first simply assigns single pipeline cpu runs jobs pipeline completion starting conversely breadth-first attempts execute jobs batch completion descending horizontal batch slice correct types workloads lead poor storage allocations depth-first scheduling batch-intensive workload thrashing attempts simultaneously cache batch datasets similarly breadth-first scheduling pipe-intensive workload over-allocate storage creates allocations pipelines completing batch-intensive capacity-aware scheduling figure illustrates importance capacity-aware scheduling measurements batch-intensive workloads scheduled algorithms workload depth large batch data sets takes sizable fraction cooperative cache remote cluster varied x-axis upper graph shows runtime lower presents amount wide-area traffic generated normalized size batch data make number observations graphs similarity graphs validates wide-area network link bottleneck resource expected policies achieve similar results long entirety batch data sets fits caches size runtime hours pipe intensive bfs dfs bad-fs failures hundreds pipe size shared disk bfs dfs bad-fs figure pipe-intensive explicit storage management graphs depict benefits explicit storage management pipeintensive workload workload consists -stage pipelines pipe data size varied percent total storage amounts negligible compute servers storage server experiment representing set diskless clients single server storage space server constrained batch data approaches total capacity cooperative cache runtime wide-area traffic increase depth-first scheduling total batch data longer fits cache depth-first scheduling refetch batch data pipeline case results extra fetches pipelines compute servers server executes pipelines note runtime begins increase slightly reason inefficiency lack complete global control allowed current volume interface case local cooperative cache hash function perfectly distributing data peers cache nears full utilization skew overloads nodes results extra traffic home server trade-off local global control correct implication scheduler aware utilization cooperative cache utilization peer finally breadth-first bad-fs scheduling retain linear performance regime ensure total amount batch data accessed time exceed capacity cooperative cache individual batch dataset exceeds capacity cooperative cache performance breadth-first bad-fs scheduling converges depth-first note inefficiency caused depth-first deviate slightly happen slightly pipe-intensive capacity-aware scheduling set cache management experiments focus pipeline-intensive workload batchappears usenix symposium networked systems design implementation nsdi mtbf secsno failure largesmalllargesmall throughput jobs minute always-copy never-copy bad-fs figure failure handling graph shows behavior cost-benefit strategy failure scenarios shown workloads width depth minute cpu time performs small amount pipeline large amount run periods high low rates failure failures induced artificial failure 
generator formatted disks random time failures seconds total runtime single pipe intensive case expect capacity-aware approach follow depth-first strategy closely results presented figure lower graph plot number failed jobs strategy induces job failure arise workload shortage space pipeline output scenario job runs space pipeline data aborts rerun time number job failures due lack space good indicator scheduler success scheduling pipeline-intensive jobs space constraints graph observe breadth-first scheduling unable prevent thrashing contrast capacity-aware bad-fs scheduler exceed space pipelines observes aborted job careful allocation results drastically reduced runtime shown upper graph stair-step pattern runtime bad-fs results careful allocation size data pipeline total storage bad-fs schedules workload jobs cpus data exceeds bad-fs allocates single cpu time notice bad-fs achieves runtimes comparable depth-first scheduling wasted resource consumption failure handling show behavior bad-fs varying failure conditions recall unlike traditional distributed systems bad-fs scheduler re-create lost output file make replica file remote cluster depend cost generating data versus cost replicating choice varies workload system conditions figure shows bad-fs cost-benefit analysis adapts variety workloads conditions compare naive algorithms always-copy replicates pipeline volume stages completes never-copy replicate draw conclusions graph environment failure replication leads excessive overhead increases amount data case bad-fs outperforms always-copy match never-copy initial replication seed analysis environment frequent failure surprising bad-fs outperforms never-copy intuitively bad-fs outperforms always-copy case particulars workload failure rate replicating worthwhile stage bad-fs correctly avoids replicating stage always-copy naively replicates stages workload experience conclude demonstrations system running real workloads demonstration presented figure compare runtime performance badfs methods utilizing local storage resources remote configuration local storage utilized executed directly home node standalone emulates afs caching data execute nodes cooperative caching storage servers leftmost graph shows results remote workload execution bandwidth home server constrained rightmost shows local workload execution home server situated local area network execute nodes graphs draw conclusions bad-fs equals exceeds performance remote standalone caching workloads configurations workloads discussed great detail earlier profiling work large degrees batch pipeline data sharing note workloads consists endpoint data gain benefit system benefit caching cooperatively standalone mode greater batch-intensive workloads blast pipe-intensive pipe-intensive workloads important optimization pipeline localization performed bad-fs standalone cooperative caching bad-fs outperform standalone cold warm phases execution entire batch data set fits storage server cooperative caching improvement data initially paged data exceed capacity caches cooperative caching unlike standalone aggregate cache space fit working set benefit cooperative caching warm caches illustrated blast measurements graph left figure logfile analysis showed appears usenix symposium networked systems design implementation nsdi ibis cms blast amanda runtime wide-area execution remote standalone bad-fs ibis cms blast amanda runtime local-area execution remote standalone bad-fs figure workload experience graphs show runtime measurements real workloads workload submit pipelines dedicated condor pool cpus condor pool accesses local storage resources configurations remote redirected back home node standalone emulates afs-like caching home server bad-fs measurement present average runtime jobs run storage server storage cache cold subsequent jobs run cache warm graph left shows runtimes workload executed cluster separated home node emulated wide-area link set home node located local area network note y-axis shown log scale accentuate points interest detailed information workloads found profiling study storage servers slightly cache space needed total blast batch data subsequent jobs accessed servers forced refetch data refetching wide-area home server standalone case expensive refetching cooperative cache bad-fs local-area home server performance advantage disappears behavior servers explains increased variability shown measurements fourth penalty performing remote home node severe significant home node local-area network execute cluster result illustrates bad-fs improve performance bandwidth home server limiting resource finally comparing graphs make observation bad-fs performance independent connection home server caches cold independent warm scoping bad-fs achieve local performance remote environments wild evaluations conducted controlled environments conclude experimental presentation demonstration bad-fs capable operating uncontrolled real world environment created wide-area bad-fs system existing batch systems wisconsin large condor system thousand cpus including workstations clusters classroom machines shared large number users mexico unm pbs system manages cluster dedicated machines established personal scheduler catalog home storage server wisconsin submitted large number bad-fs bootstrap jobs batch systems installing special software locations directed scheduler execute large workload consisting cms pipelines resources figure timeline execution workload expected number cpus varied widely due competition users availability idle workstations vagaries batch scheduler unm consistently provided twenty cpus jumping forty hours spikes cpus hours due crash recovery catalog server resulted loss monitoring data running jobs benefits cooperative caching underscored dynamic environment bottom graph cumulative read traffic home node shown hills plateaus hills correspond large spikes number cpus cpus subnet begin executing fetch batch data home node smaller hills number cpus effect amount home read traffic server entering established cooperative cache fetch batch data peers finally figure illustrates design implementation bad-fs suitable running intensive batch-pipelined workloads multiple uncontrolled real world clusters failures disconnections bad-fs continues making steady progress removing burden user scheduling monitoring resubmitting jobs appears usenix symposium networked systems design implementation nsdi cpus total cpus cpus unm jobs running tens jobs complete writes server reads server figure wild graphs present timeline behavior large cms workload run bad-fs workload consisted cms pipelines run resources scavenged collection cpus mexico running pbs cpus wisconsin running condor topmost timeline presents total number cpus middle shows number jobs running cumulative jobs completed bottom shows cumulative traffic incurred home server related work designing bad-fs drew related work number distinct areas workflow management historically concern high-level business management problems involving multiple authorities computer systems large organizations approval loans bank customer service actions company scheduler works lower semantic level systems borrow lessons integration procedural data elements automatic management dependencies performance fault tolerance found variety tools systems managed dependencies jobs basic found unix tool make elaborate dependency tracking explored vahdat anderson work transparent result caching work authors build tool tracks process lineage file dependency automatically workflow description static encoding knowledge manner scheduler constructs private namespaces running workloads reminiscent database views private namespace simpler construct maintain views contrast present systems implementation challenges handling updates base tables propagation extant materialized views bad-fs improved prefetching batch datasets work noted difficulty correctly predicting future access patterns bad-fs explicitly supplied user declarative workflow description recent work peer-to-peer storage systems 
systems interesting solutions problem domain intended falls short applied context batch workloads reasons distributed file systems good match overlays developed environments communication clusters plan investigate future work similar work grid computing techniques designed bad-fs environments cluster-on-demand offers sophisticated resource clustering techniques badfs form cooperative cache groupings extensible systems share approach allowing application control recent work recently revisited approach extensible systems commercially successful specialized policies great greater batch workloads running systems designed interactive research mobile computing bears similarity flinn discuss process data staging untrusted surrogates ways surrogate similar bad-fs storage server major difference surrogate primarily concerned trust servers primarily concerned exposing control zap vmware checkpointing migration processes operating systems create remote virtual environment higher level batch system systems secure interposition janus complement badfs make resource owners donate resources shared pools finally bad-fs similar distributed file systems google file system motivated workloads deviate earlier file system assumptions additional similarity simplified consistency implementation gfs relax consistency semantics enable bad-fs explicit control earlier work coda afs applicable systems caching availability disconnected operation bad-fs storage servers enact similar role appears usenix symposium networked systems design implementation nsdi conclusions big bad wolf neighborhood bad meaning bad bad meaning good run dmc peter piper allowing external control long recognized powerful technique improve aspects system performance moving control external user system system user dictate policy individual nature work systems lacking mechanisms external control speculate systems proven adept speculation work majority workloads paper argued distinct nature batch workloads matched design traditional distributed file systems external control greater bad-fs distributed file system exposes internal control decisions external scheduler detailed knowledge workload characteristics scheduler carefully manages remote resources facilitates execution intensive batch jobs wide-area local-area clusters synthetic real workload measurements controlled uncontrolled environments demonstrated ability bad-fs workload specific knowledge improve throughput selecting storage policies scoping space allocation cost-benefit replication acknowledgments nate burnett nicholas coleman tim denehy barry henrichs florentina popovici muthian sivathanu vijayan prabhakaran department helpful discussions comments paper grateful excellent support provided members csl state appreciation jeff chase thoughtful analysis work development project finally anonymous reviewers helpful suggestions eric brewer excellent insightful shepherding substantially improved content presentation paper work sponsored part nsf ccrnsf ngsccr- ccritr- itrdoe de-fc wisconsin alumni research foundation emc ibm adya bolosky castro cermak chaiken douceur howell lorch theimer wattenhofer farsite federated reliable storage incompletely trusted environment proceedings symposium operating systems design implementation osdi boston dec agrawal imielinski swami database mining performance perspective ieee transactions knowledge data engineering dec altschul madden schaffer zhang zhang miller lipman gapped blast psi-blast generation protein database search programs nucleic acids research pages anderson dahlin neefe patterson wang serverless network file systems proceedings acm symposium operating systems principles sosp pages copper mountain dec arpaci-dusseau arpaci-dusseau burnett denehy engle gunawi nugent popovici transforming policies mechanisms infokernel proceedings acm symposium operating systems principles sosp bolton landing lake george oct avery cms virtual data requirements kholtman home cern kholtman tmp cmsreqsv baker hartman kupfer shirriff ousterhout measurements distributed file system proceedings acm symposium operating systems principles sosp pages pacific grove oct bent venkataramani leroy roy stanley arpaci-dusseau arpaci-dusseau livny flexibility manageability performance grid storage appliance proceedings high-performance distributed computing hpdcpages edinburgh scotland jul bershad savage pardyak sirer fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles sosp pages copper mountain dec breitbart deacon schek sheth weikum merging application-centric data-centric approaches support transaction-oriented multi-system workflows sigmod record cantin hill cache performance selected spec cpu benchmarks computer architecture news sep chandra dahlin richards wang anderson larus experience language writing coherence protocols proceedings usenix conference domain-specific languages santa barbara oct chang gibson automatic hint generation speculative execution proceedings symposium operating systems design implementation osdi pages orleans louisiana feb chase grit irwin moore sprenkle dynamic virtual clusters grid site manager proceedings ieee international symposium high performance distributed computing hpdc seattle june dabek kaashoek karger morris stoica wide-area cooperative storage cfs proceedings acm symposium operating systems principles sosp banff canada oct dahlin wang anderson patterson cooperative caching remote client memory improve file system performance proceedings symposium operating systems design implementation osdi monterey nov eda industry working group eda resource http eda edwards mckendry exploiting read-mostly workloads filenet file system proceedings acm symposium operating systems principles sosp pages litchfield park arizona dec engler kaashoek toole exokernel operating system architecture application-level resource management proceedings acm symposium operating systems principles sosp pages copper mountain dec appears usenix symposium networked systems design implementation nsdi feeley morgan pighin karlin levy implementing global memory management workstation cluster proceedings acm symposium operating systems principles sosp pages copper mountain dec flinn sinnamohideen tolia satyanarayanan data staging untrusted surrogates proceedings usenix symposium file storage technologies fast san francisco apr ford hibler lepreau tullman back clawson microkernels meet recursive virtual machines proceed-ings symposium operating systems design implementation osdi seattle oct foster avery petascale virtual data grids data intensive science griphyn white paper foster kesselman tsudik tuecke security architecture computational grids proceedings acm conference computer communications security conference pages foster kesselman tuecke anatomy grid enabling scalable virtual organizations international journal supercomputer applications frey tannenbaum foster livny tuecke condor-g computation management agent multiinstitu- tional grids proceedings ieee international symposium high performance distributed computing hpdc san francisco aug gelenbe optimal checkpoint interval journal acm apr georgakopoulos hornick sheth overview workflow management process modeling workflow automation infrastructure distributed parallel databases ghemawat gobioff leung google file system proceedings acm symposium operating systems principles sosp bolton landing lake george oct goldberg wagner thomas brewer secure environment untrusted helper applications proceedings sixth usenix security symposium july gribble brewer hellerstein culler scalable distributed data structures internet service construction proceedings symposium operating systems design implementation osdi san diego oct gupta mumick maintenance materialized views problems techniques applications ieee quarterly bulletin data engineering special issue materialized views data warehousing jones interposition agents transparently interposing user code system interface proceedings acm symposium operating systems principles sosp pages asheville north carolina dec kistler satyanarayanan disconnected operation coda file system acm transactions computer systems feb kubiatowicz bindel eaton chen geels gummadi rhea weimer wells weatherspoon zhao oceanstore architecture global-scale persis-tent storage proceedings international conference architectural support 
programming languages operating systems asplos pages cambridge nov lancaster renderman web site http renderman litwin neimat schneider family order preserving scalable distributed data structures proceedings international conference large databases vldb pages santiago chile sep litzkow livny mutka condor hunter idle workstations proceedings acm computer network performance symposium pages june muthitacharoen morris gil chen ivy read write peer-to-peer file system proceedings symposium operating systems design implementation osdi boston dec osman subhraveti nieh design implementation zap system migrating computing environments proceedings symposium operating systems design implementation osdi boston dec ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system proceedings acm symposium operating system principles sosp pages orcas island dec platform computing improving business capacity distributed computing platform industry financial raman matchmaking frameworks distributed resource management phd thesis wisconsin-madison oct rizzo dummynet simple approach evaluation network protocols acm computer communication review roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference usenix pages san diego june rowstron druschel storage management caching past large-scale persistent peer-to-peer storage utility proceedings acm symposium operating systems principles sosp banff canada oct rusinkiewicz sheth specification execution transactional workflows modern database systems object model interoperability pages saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area file system proceedings symposium operating systems design implementation osdi boston dec sapuntzakis chandra pfaff chow lam rosenblum optimizing migration virtual computers proceedings symposium operating systems design implementation osdi boston dec satyanarayanan study file sizes functional lifetimes proceedings acm symposium operating systems principles sosp pages pacific grove dec seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating systems design implementation osdi pages seattle oct soderbergh mac lies videotape apple hotnews articles fullfrontal sullivan werthimer bowyer cobb gedye anderson major seti project based project serendip data personal computers proceedings international conference bioastronomy thain bent arpaci-dusseau arpaci-dusseau livny pipeline batch sharing grid workloads proceedings high-performance distributed computing hpdcpages seattle june thain livny parrot transparent user-level middleware data-intensive computing workshop adaptive grid middleware orleans louisiana sep vahdat anderson transparent result caching proceedings usenix annual technical conference usenix orleans louisiana june vogels file system usage windows proceedings acm symposium operating systems principles sosp pages kiawah island resort south carolina dec 
x-ray non-invasive exclusive caching mechanism raids lakshmi bairavasundaram muthian sivathanu andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin-madison abstract raid storage arrays possess gigabytes ram caching disk blocks raid systems lru lru-like policies manage caches array caches recognize presence system buffer caches redundantly retain blocks cached system wasting precious cache space paper introduce x-ray exclusive raid array caching mechanism x-ray achieves high degree perfect exclusivity gray-box methods observing les accessed updates system meta-data x-ray constructs approximate image contents system cache information determine exclusive set blocks cached array microbenchmarks demonstrate x-ray prediction system buffer cache contents highly accurate trace-based simulation show x-ray considerably outperforms lru performs invasive approaches main strength x-ray approach easy deploy performance gains achieved scsi protocol system introduction modern systems comprised multiple levels caching processor level rest storage hierarchy hierarchy performance secondlevel caches important important data sets rst-level caches increase effectiveness second-level caches previous work processor caching introduced concept exclusive caching avoiding duplication data levels memory hierarchy effective amount cache real estate increased potentially improving performance exclusivity studied levels storage hierarchy including distributed systems storage arrays raids problem inclusion importance modern storage systems built two-level hierarchy system rst top level storage array multiple disks beneath rst-level cache managed operating system implements lru-based replacement policy storage array hardware manages memory memory serves second-level cache managed lru fashion worsening problem fact hosts disk arrays caches similar size high-end disk arrays gigabytes memory run similarly con gured hosts cache exclusion cache space storage arrays wasted previous storage research addressed problem ways approach change second-level policy incorporate access characteristics frequency make replacement decisions approach avoids cache inclusion policies carefully tailored work beneath lru cache speci workloads workloads highly-specialized schemes function desired approach change interface systems storage wong wilkes propose scsi command demote moves block cache directly raid cache enabling manage array cache explicitly lru discipline approach readily deployed inducing instruction-set change processor level dif cult leads researchers focus micro-architectural innovations changing systems interface systems storage scsi result storage vendors implement mechanism remaining challenge storage array cache design derive second-level caching scheme lru-based work workloads require change interface systems storage broadly deployable paper introduce x-ray exclusive array cache array caching scheme designed meet goals primary dif culty building exclusive lru-based raid cache changing interface storage system activity observed raid reads cached pages raid fuzzy picture contents cache appropriately adjust contents x-ray sharpens picture gray-box methods observing updates access time eld inode x-ray infer blocks accessed build approximate view contents cache x-ray combines knowledge traditional monitoring data accesses relevant data cache approaching performance globally-managed lru cache study x-ray series simulations x-ray accurately predict contents system appears proceedings international symposium computer architecture isca cache accurate prediction enables highly exclusive cache delivering noticeably higher hit rates simple lru cache policy real workloads x-ray improves hit rates factor compared lru approaching performance perfect exclusive cache requiring system interface storage rest paper structured section overview system operation explains problem cache inclusion storage hierarchy section discusses semantic information obtained disk implications section describes x-ray cache section evaluates caching mechanism section discusses related work section summarizes work outlines future research directions background section outlines operation system viewpoint caching explains problem cache inclusion storage hierarchy proposes solution based utilizing semantic information disk array file system assumptions begin presenting assumptions system information present inode found xed location disk included inode pointers data blocks pointers direct inode indirect block pointed inode levels indirection larger les inode tracks information activity creation time access time system maintains buffer cache variable size caches disk blocks cache managed lrulike caching policies addition cache data blocks operating systems separate inode cache inodes open recently accessed les size system cache varies due pressure virtual memory system pages required address spaces fewer pages caching opened system rst identi inode number pathname traversal checks inode present inode cache present disk block inode read disk application reads system calculates block numbers desired bytes inode indirect blocks buffer cache read disk locate disk block numbers correspond requested blocks buffer cache searched block disk reads issued blocks present cache priority blocks buffer cache updated cache policy lru mostrecently read block highest priority finally access time eld in-memory inode updated inode marked dirty writing similar main differences read application-generated blocks cache marked dirty modi cation time eld inode updated system periodically ush modi dirty blocks disk modi meta-data blocks superblocks bitmap blocks inode blocks generally ushed sooner modied data blocks typical setting meta-data blocks linux ext system seconds data ushed seconds access time information periodically written observed disk problem disk arrays lru-like policies manage cache block array cache miss block potentially replacing recently accessed block policies recognize fact array cache second-level cache services accesses miss system buffer cache block read system system buffer cache subsequent requests block handled cache disk reads block evicted block recently read block array cache stay array cache signi period time wasting cache space array caches greatly affected cache inclusion size comparable size host memory ideal disk block array cache evicted system buffer cache array cache acting victim cache disk array observe disk block requests hit system cache information decide block evicted complicate situation system buffer cache xed size due varying degrees memory pressure solution question addressing storage system learn contents system cache make decisions blocks array cache system interface storage key building system semantic knowledge storage array semantically-smart storage array knowledge system structures embedded sense storage typical array semantically-smart storage system observe inode updated access time eld inode changed array infer read valuable information intelligent caching scheme section explore information obtained disk array semantic awareness inferences made information semantic information semantically-smart storage array knowledge higher level system data structures instance block identify inode data block inode block block identify individual elds inode semantic information embedded appears proceedings international symposium computer architecture isca information inferences requirements disk read request data block block system cache mru block identify data blocks disk read request previously read block block victim system cache past identify data blocks remember previously read blocks access time inode disk read observed blocks present system cache identify inode blocks note 
inode elds cache inode blocks remember disk reads table semantic inferences table presents inferences made disk array piece information capabilities required make inference disk array learned array careful observation system traf note embedding knowledge disk array reasonable on-disk system structures change modern systems great lengths preserve on-disk layout revisions order ensure backward compatibility existing systems semantic knowledge disk array observe elds inode time accessed modi information facilitate prediction ordering blocks system cache size system cache describe speci inferences disk array make based extra semantic information table summarizes inferences speci mechanisms required enable inferences disk array identify data blocks infer data block read system recently mru block system cache assumed system cache lru-based disk array infer block read replaced system disk array make stronger inference disk read observes previously read block block system cache earlier present block victim point time disk reads block make inference disk array remember block numbers previously read blocks finally updates access timestamp eld inode enable inference disk array identify inode blocks compare contents inode block written previous on-disk state identifying inodes change access time semantic knowledge enables disk array associate inode data blocks represented inode information disk array infer changed access time inode implies blocks accessed system blocks read disk access time disk array infer accessed blocks present system cache assuming time loosely synchronized host array order make inference disk array remember past disk accesses assume change access time implies accessed opposed block studies system activity reasonable explore rami cations assumption set inferences aids constructing access information blocks information block victim required system cache information derived late scheme access information block information blocks assumed system cache lru replacement policy systems conform assumption information gleaned disk array maintain list block numbers data blocks read system ordered access times blocks obtained inode writes actual disk reads list reects recency access perceived system attempts mirror ordering system cache block lru end list earliest access time mru end latest access time maintaining ordered list extend inferences disk read previously read block ordered list implies block evicted blocks block lru end list evicted system cache blocks earlier access times block read similarly system level access block generate disk read implies block blocks block mru end ordered list present system cache basic inferences driven semantic information approximating contents system cache section describe details transforming base idea working cache mechanism preserve exclusivity x-ray cache section rst describe build approximate image system cache contents semantic information discuss limitations approach finally describe cache content prediction build exclusive array cache mechanism tracking file system cache track contents system cache maintain ordered list block numbers accessed system call list recency list r-list entry r-list access time block inferred array identi denotes block belongs moving pointer called cache begin pointer appears proceedings international symposium computer architecture isca actions disk read block actions inode write lruend mru disk read block time lru end block belongsinode write file end access time lru end end inclusive region inclusive region inclusive region exclusive region exclusive region exclusive region end mru mru lru end end inclusive regionexclusive region mru block timestamp figure r-list pointer operation contents system cache tracked disk array r-list pointer gures show status r-list pointer disk read inode write block r-list disk block number access time pointer slides list demarcates set blocks presumed resident cache blocks r-list outline r-list pointer managed r-list maintained basic rules block read added moved mru end r-list access time change observed result inode write blocks belonging removed list re-inserted list ecting access time pointer called ideal point array cache placement occur number blocks pointer mru end r-list estimate size system cache set blocks range approximation contents system cache label region pointer mru end r-list inclusive region region pointer lru end list exclusive region pointer maintained read observed block r-list indicating victim block inclusive region implies system cache size overestimated pointer moved position block read shrinking inclusive region victim block present exclusive region pointer moved block mru end action required account possibility reading block result eviction lru block system cache case conservative estimate cache size system level access blocks inferred inode write set blocks r-list belong examined block earliest access time chosen block lies exclusive region implies size system cache underestimated pointer moved position accessed block expanding inclusive region accessed block present inclusive region action required figure illustrates operation system cache tracking mechanism rst disk read block time initially pointer positioned blocks indicating blocks expected present system cache blocks expected victims system cache read block implies blocks system cache victims past block read longer victim recently block moved mru end r-list timestamp updated pointer repositioned blocks ect fact block expected victim illustrates inode write initially pointer positioned blocks indicating block expected system cache inode write block observed access timestamp noted assume timestamp changed assuming disk reads observed infer block present system cache pointer moved past block ect information entry block repositioned r-list timestamp handling partial file access techniques assume les accessed entirety access time update inode infer information blocks accesses typical system workloads involve les require mechanisms robust occasional partial accesses read time block accessed initially victim moved exclusive region r-list nally block accessed receiving access time update due accessed mechanism appears proceedings international symposium computer architecture isca wrongly increase cache size position assumes fullle access handle situation adopt simple heuristic long block belonging inclusive region blocks exclusive region disregarded access time updates received improve robustness techniques occasional skews access pattern updates pointer performed suf cient evidence observed speci cally update performed threshold number accesses suggest update accesses pertaining blocks les limits accuracy mechanisms track contents system cache reasonable level accuracy fundamental limitations interval typically seconds inode writes creates window uncertainty accesses evictions blocks system interval potentially unknown x-ray mismatch actual ordering blocks system maintained x-ray blocks moved exclusive region recently blocks system access information blocks observed time pointer moved exclusive region ect access information pointer update account access blocks account blocks evicted system time interval cache size estimate ated x-ray observes victim read shrinks inclusive region inode timestamps granularity multiple les accessed access time ordering 
relevant blocks unknown x-ray lead error predicted cache size future read evicted block change access time limitation hold systems netbsd ffs maintains timestamps microsecond granularity assume worst -second granularity found linux ext system block accessed evicted system cache inode write interval case x-ray wrongly blocks accessed blocks mru end r-list present system cache error occur block system cache accessed single inode write interval show potential limitations prediction mechanism achieves high level accuracy enabling cache mechanism ideal exclusivity ray cache mechanism section describes x-ray cache built top system cache tracking mechanism size array cache blocks ideally x-ray cache rst blocks past pointer exclusive region r-list unlike simple policies lru place block cache block read system x-ray block ready system issues read block x-ray explicitly fetch block place exclusive cache decide blocks fetch cache x-ray periodically examines r-list blocks added n-block window past pointer r-list blocks removed blocks newly added window replace removed blocks one-by-one x-ray cache fetched recently blocks removed blocks cache replaced rst blocks present system cache x-ray avoid inclusion order x-ray cache organized access time ordered list blocks blocks required placement obtained source unlike mechanisms demote change interface accommodate special cache place command system supply evicted block change interface system disk blocks read disk schedule cache placement reads x-ray requires additional disk bandwidth additional bandwidth forms workload suf cient idle time requests x-ray idle time schedule disk reads placement section explore idle time required purpose workload idle time x-ray perform timely placement internal bandwidth disk array higher external bandwidth limited bandwidth single scsi bus large storage arrays internal buses substantial extra internal bandwidth perform replication migration data storage array scenarios x-ray leverage small amount extra bandwidth schedule placement reads freeblock scheduling shown capable extracting signi amount free bandwidth busy disks negligible impact foreground workload x-ray potentially freeblock scheduling cases internal bandwidth scarce evaluation section trace-based simulation evaluate x-ray rst describe simulation environment section evaluate x-ray system cache tracking mechanism terms accurately predict cache contents finally section evaluate performance improvements x-ray caching mechanism synthetic real workloads simulator built trace-driven simulator system disk system underneath simulator takes trace system requests open read write models system behaviorally similar linux ext system appears proceedings international symposium computer architecture isca simpli system model consists inode data blocks meta-data blocks including superblock bitmap blocks indirect pointer blocks modeled traf blocks minimal system cache lru replacement default investigate cache replacement policies section size cache dynamically changed model virtual memory pressure system block size writes blocks marked dirty buffer cache dirty blocks written periodically dirty inode blocks written disk seconds speci dirty data blocks written seconds inode timestamps granularity disk array simulator models simple disk constant access time model single disk simplicity results hold interesting disk arrays evaluation concentrates hit rates disk-speed insensitive access times levels hierarchy hits system cache hits array cache cost disk accesses cost prediction accuracy section quantify degree accuracy x-ray track size contents system cache explore sensitivity mechanism system workload parameters metrics error cache size prediction predicted system cache size number blocks inclusive region r-list difference predicted cache size actual system cache size measured regular intervals average error computed fraction false positives effective x-ray cache blocks recent victims system cache due prediction inaccuracy x-ray wrongly conclude blocks system cache victims measure ratio number false positives predicted cache size fraction false negatives maintain exclusion x-ray avoid wrongly identifying blocks system cache victims metric ratio number false negatives actual cache size synthetic workloads random zipf similar wong wilkes evaluate exclusive caching experiments subsection workloads warmup phase set les read fully sequentially les selected order set read fully random benchmark selection les uniform random zipf benchmark selects les based zipf distribution selection highly biased small number popular les prediction file system cache size run random benchmark system cache size initially set blocks record cache size predicted x-ray execution benchmark order evaluate reactivity x-ray cache size change cache size blocks time seconds predicted size actual size figure cache size prediction cache size prediction compared actual cache size execution random benchmark shown benchmark les blocks size performs fullle reads warmup phase actual cache size changed blocks blocks read warmup size prediction error file system cache size blocks size prediction error random file size block file size blocks file size blocks file size blocks file size blocks size prediction error file system cache size blocks size prediction error zipf file size block file size blocks file size blocks file size blocks file size blocks figure estimating cache size average percentage error cache size prediction function system cache size plotted sizes random zipf benchmarks working set size constant blocks benchmarks executed sizes total number blocks accessed benchmark constant warmup period measurements actual cache size system multiple times execution benchmark figure compares cache size prediction x-ray actual system cache size warmup phase rst seconds benchmark scans blocks working set x-ray receives information contents system cache random selection phase starts x-ray predict size system cache high degree accuracy x-ray highly responsive system cache size inaccuracies due reasons cited earlier responsible slight overestimate cache size observe obtained similar results zipf benchmark shown explore average percentage error cache size prediction random zipf benchmarks mechanism evaluated sizes system cache sizes figure shows sensitivity size prediction error sizes system cache sizes file size factor mechanism performs access time information obtained granularity appears proceedings international symposium computer architecture isca fraction false positives file system cache size blocks false positives random file size block file size blocks file size blocks file size blocks file size blocks fraction false positives file system cache size blocks false positives zipf file size block file size blocks file size blocks file size blocks file size blocks fraction false negatives file system cache size blocks false negatives random file size block file size blocks file size blocks file size blocks file size blocks fraction false negatives file system cache size blocks false negatives zipf file size block file size blocks file size blocks file size blocks file size blocks figure false positives false negatives fraction false positives fraction false negatives function system cache 
size plotted sizes random zipf benchmarks working set size constant blocks benchmarks executed sizes total number blocks accessed benchmark constant warmup period seconds measurements size prediction error percentage files accessed partially size prediction random zipf fraction false positives percentage files accessed partially false positives random zipf fraction false negatives percentage files accessed partially false negatives random zipf figure partial file access performance system cache tracking evaluated percentage partially accessed les increases working set consists les blocks size system cache size blocks random performs reads zipf performs reads varying percentages les read partially gure shows average size prediction error false positives fraction false negatives graphs observe system cache size increases percentage error decreases larger cache size fewer blocks evicted cache inode writes leading lower misclassi cation errors size prediction error depends extent size error increases size increases considerably signi fraction system cache size prediction file system cache contents cacy x-ray predicting contents cache terms metrics outlined fraction false positives false negatives important fractions low high number false positives imply x-ray ignore signi number recently accessed blocks cached system cache high number false negatives lead x-ray redundantly caching blocks present system cache figure plots fraction false positives fraction false negatives predicted x-ray system cache sizes sizes random zipf workloads graphs show fraction false positives low range parameters fraction false positives decreases increase cache size similar trend observed size prediction error cache tracking mechanism effective identifying recent victims cache quickly respect false negatives trends similar earlier graphs average fraction low x-ray cache high degree exclusivity sensitivity partial file access evaluate robust x-ray prediction mechanism partial access modi random zipf benchmarks access random number blocks percentage les rest les read fully figure shows prediction error x-ray error metrics percentage les accessed partially increased graphs x-ray tolerates partial reads studies shown accesses typical system workloads reads speci cally baker found read-only accesses read bytes sequential transfers maximum reads non-full size content predictions accurate sensitivity inode write interval x-ray obtains access information inode writes performance x-ray sensitive long inode blocks dirty written figure graphs performance x-ray inode write delay increased gures show small write delays seconds prediction error tolerable excessively long write delays size prediction error fraction false negatives increase considerably inode access time update fundamental sources information x-ray surprising reasonable inode write frequency required prediction typical systems linux ext system small write delays inode blocks appears proceedings international symposium computer architecture isca size prediction error inode write interval seconds size prediction zipf random fraction false positives inode write interval seconds false positives zipf random fraction false negatives inode write interval seconds false negatives zipf random figure inode write interval performance system cache tracking evaluated inode write interval increases working set les blocks size random performs fullle reads zipf performs fullle reads gure shows average size prediction error false positives fraction false negatives fraction inode write intervals cache workload size false false size size prediction positives negatives blocks blocks error fraction fraction table scaling behavior quality system cache tracking evaluated system cache size workload size scaled random benchmark executed size set blocks number les increases working set increases scaling behavior table shows scaling x-ray terms error metrics workload size system cache size increased random benchmark observe effect errors decreases benchmark system cache size workload scaled error sources independent system cache size percentage error reduces system cache size scaled cache performance section performance x-ray cache mechanism evaluated synthetic workloads real traces compare hit rates array cache approaches examine resulting response time read average read latency compare performance x-ray alternative approaches compare array cache managed simple lru fashion represents array caches managed today multi-queue cache policy speci cally designed second-level caches utilizes frequency access prioritize blocks compare approach demote cache mechanism achieves exclusivity modifying system explicitly supply victim blocks array cache finally compare ideal case scenario array cache added system cache upper bound array cache achieve perfect exclusivity note case relevant read latency measurements hit rate measurements separate array cache experiments section assume x-ray suf cient extra bandwidth schedule cache placement disk reads constrained scenario extra bandwidth quantify idle time needed x-ray work effectively finally compare performance x-ray approaches system cache policy lru synthetic workloads set experiments random zipf workloads section figure figure show array cache hit rates x-ray lru multi-queue demote array cache sizes random zipf workloads measurements warmup phase benchmarks gures x-ray outperforms lru multi-queue signi cantly random workload x-ray compares indistinguishably demote implying x-ray effective enforcing exclusivity explicit accurate information demote zipf workload hit rate x-ray close demote reason x-ray performs slightly worse case compared random workload impact false positives higher zipf workload important capture recent victims system cache workloads multi-queue policy performs lru due consideration frequency access figure figure compare average read latency workloads x-ray mechanisms compare x-ray ideal scenario array cache space added host system surprisingly demote performs close ideal scenario perfect information enforce exclusivity latency x-ray lru multi-queue policies close ideal scenarios higher hit rates achieved x-ray lead signi improvements read latency benchmarks real workloads evaluate performance x-ray real workloads set experiments http traces web servers evaluate x-ray convert trace requests appears proceedings international symposium computer architecture isca hit rate array cache size blocks random hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks random read latency lru multi-queue x-ray demote extra figure random workload array cache hit rate average read latency x-ray lru multi-queue demote random benchmark presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks benchmark les blocks size reads performed hit rate array cache size blocks zipf hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks zipf read latency lru multi-queue x-ray demote extra figure zipf workload array cache hit rate average read latency x-ray lru multi-queue demote zipf benchmark presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks benchmark les blocks size reads performed web servers system read operations assume objects referred trace static les generated dynamically traces requests image les requests html les 
assume bytes data returned client rst bytes issue requests times speci trace preserving idle time requests traces section -minute section http trace heavily accessed soccer world cup website section http trace nasa web server recorded august figure shows hit rate array cache average read latency worldcup workload gure shows hit rate achieved x-ray lru multi-queue entire range array cache sizes hit rate comparable demote hit rate improvements translate improvements response time x-ray improves read latency times compared lru times compared multi-queue performs similar demote maximum slowdown figure shows hit rate array cache average read latency nasa trace system cache size set blocks x-ray performs demote signi cantly lru multi-queue performance gain x-ray signi array cache small compared system cache approximate information system cache contents x-ray perform invasive methods demote require storage interface sensitivity idle time section explore idle time required x-ray timely fetch exclusive cache blocks placeappears proceedings international symposium computer architecture isca hit rate array cache size blocks worldcup trace hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks worldcup trace read latency lru multi-queue x-ray demote extra figure worldcup trace hit rate read latency array cache hit rate average read latency x-ray lru multiqueue demote worldcup trace presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks hit rate array cache size blocks nasa trace hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks nasa trace read latency lru multi-queue x-ray demote extra figure nasa trace hit rate read latency array cache hit rate average read latency x-ray lru multi-queue demote nasa trace presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks ment estimate idle time requirements x-ray implementation issues disk read time idle time assume free internal bandwidth run worldcup benchmark varying degrees idle time purpose scale inter-request times recorded trace broad range scaling factors system cache size array cache size set blocks figure shows hit rate array cache amounts idle time observe graph hit rates multi-queue lru demote independent idle time hit rate x-ray decreases idle time decreases foreground requests greater portion disk bandwidth nasa benchmark signi cantly idle time worldcup benchmark observed hit rate affected factor reductions idle time results shown suf cient idle time present workload x-ray schedule cache placement reads requiring extra internal bandwidth workloads idle time spare internal disk array bandwidth freeblock scheduling schedule reads file system cache policies x-ray designed assumption system cache managed lru fashion systems lru section evaluate performance x-ray system cache managed replacement policies clock clock widely approximation lru cache policy linux variation manage page cache figure presents array cache hit rates worldcup trace study cases ordering demote x-ray multi-queue lru remains slightly larger difference demote x-ray hinting array tuned speci caching algorithm host subject leave future investigation appears proceedings international symposium computer architecture isca hit rate idle time reduction factor x-ray demote multi-queue lru lru multi-queue demote x-ray idle time idle time figure hit rate idle time hit rate array cache worldcup benchmark factor reductions idle time shown x-ray implementation extra internal disk array bandwidth study fractional factor reduction idle time reciprocal factor increase idle time factor reduction retaining original idle time trace hit rate array cache size blocks clock demote x-ray multi-queue lru hit rate array cache size blocks demote x-ray multi-queue lru figure file system cache policy array cache hit rate x-ray lru multi-queue demote worldcup benchmark presented cases system cache managed clock system cache size set blocks related work cache replacement algorithms explored good detail years algorithms proposed single level cache mind include lru lfu fifo mru lee explored spectrum policies subsume lru lfu number earlier works distributed systems demonstrated multi-level cache hierarchy rethinking efforts investigated policies method avoid inclusion focus frequency-based policies recently zhou proposed multiqueue algorithm level caches put minimal lifetime frequency-based priority temporal frequency desirable qualities cache replacement algorithm multiqueue algorithm satis requirements multiple lru queues second-level cache recent block queue minimum frequency threshold selected replacement multi-queue eliminate cache inclusion cache placement occurs disk read block system demote array cache management mechanism considers cache exclusion central managing array cache demote system informs disk array blocks discards block present array cache system supplies block requires changing system-disk interface hinders deployment demote increases interconnect bandwidth requirement moving blocks system cache back array cache problem interconnect-constrained environments finally recent work eviction-based cache placement cache exclusion manage array cache similar work unlike demote mechanism attempts retain current interfaces requires software installed host machine change interface storage specifically eviction-based cache placement virtual memory addresses supplied system modi device driver track contents system mechanism relies idle time extra bandwidth disks read blocks array cache mechanism change interface device driver requires interface disk order communicate needed information storage server mechanism provisions detect system cache size introducing possibility misjudging contents work correctly moves cache pages original location system page migration conclusions technology trends point availability smarter disk array systems future semantic intelligence disk arrays manage large caches present systems semantic information avoids system-disk interface providing information infer contents system cache paper shown create image system cache information inferred disk traf introduced metrics evaluate tracking system cache contents viewpoint information exclusive caching image system cache helps identify set exclusive blocks array cache x-ray array cache based semantic information good cache hit rates improves execution time considerably x-ray achieves ends modi cations system storage interface readily deployed future plan explore number extensions x-ray infer occurrence deletions information remove invalid data cache x-ray possibly detect caching algorithm system assuming lru-like finally explore utility x-ray underneath classes systems windows ntfs underneath database management systems extensions lead robust deployable exclusive caching mechanism storage arrays appears proceedings international symposium computer architecture isca acknowledgments anuradha vaidyanathan involvement input early stages project saisanthosh balakrishnan nathan burnett timothy denehy florentina popovici vijayan prabhakaran insightful comments earlier drafts paper anonymous reviewers comments suggestions helped signi cantly improve paper work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation arlitt williamson web server workload characterization 
search invariants proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics martin arlitt tai jin world cup web site access logs http acm sigcomm ita august martin arlitt tai jin workload characterization world cup web site technical report hpl- hewlett packard labs andrea arpaci-dusseau remzi arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october mary baker john hartman martin kupfer ken shirriff john ousterhout measurements distributed file system proceedings acm symposium operating systems principles sosp pages paci grove california october rohit chandra scott devine ben verghese anoop gupta mendel rosenblum scheduling page migration multiprocessor compute servers proceedings international conference architectural support programming languages operating systems asplos pages san jose california october zhifeng chen yuanyuan zhou kai eviction-based placement storage caches proceedings usenix annual technical conference usenix pages san antonio texas june zarka cvetanovic dileep bhandarkar characterization alpha axp performance spec workloads proceedings international symposium computer architecture pages ian dowse david malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track emc corporation symmetrix enterprise information storage systems http emc theodore johnson dennis shasha low-overhead high performance buffer management replacement algorithm proceedings international conference large databases vldb pages santiago chile september norman jouppi improving direct-mapped cache performance addition small fully-associative cache prefetch buffers proceedings annual international symposium computer architecture isca pages seattle washington norman jouppi steven wilton tradeoffs two-level on-chip caching proceedings international symposium computer architecture pages kimberly keeton david patterson yong qiang roger raphael walter baker performance characterization quad pentium pro smp oltp workloads proceedings annual international symposium computer architecture isca pages june donghee lee jongmoo choi jun-hum kim sam noh sang lyul min yookum cho chong sang kim existence spectrum policies subsumes recently lru frequently lfu policies proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics atlanta georgia lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october makaroff derek eager disk cache performance distributed systems international conference distributed computing systems icdcs pages paris france muntz honeyman multi-level caching distributed file systems cache ain nuthin trash proceedings usenix winter technical conference usenix winter pages san francisco california january drew roselli jacob lorch thomas anderson comparison file system workloads proceedings usenix annual technical conference usenix pages san diego california june muthian sivathanu vijayan prabhakaran florentina popovici timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california march theodore stephen tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track john wilkes richard golding carl staelin tim sullivan autoraid hierarchical storage system acm transactions computer systems february darryl willick derek eager richard bunt disk cache replacement policies network fileservers international conference distributed computing systems icdcs pages pittsburgh pennsylvania theodore wong john wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june yuanyuan zhou james philbin kai multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference usenix pages boston massachusetts june 
deploying safe user-level network services ictcp haryadi gunawi andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison haryadi dusseau remzi wisc abstract present ictcp information control tcp implementation exposes key pieces internal tcp state tcp variables set safe fashion primary benefit ictcp enables variety tcp extensions implemented user-level ensuring extensions tcp-friendly demonstrate utility ictcp collection case studies show exposing information safe control tcp congestion window readily implement user-level versions tcp vegas tcp nice congestion manager show user-level libraries safely control duplicate acknowledgment threshold make tcp robust packet reordering wireless lans show retransmission timeout adjusted dynamically finally find converting stock tcp implementation ictcp straightforward prototype requires approximately lines kernel code introduction years networking research suggested vast number modifications standard tcp protocol stack proposals eventually adopted suggested modifications tcp stack widely deployed paper address problem deployment proposing small enabling change network stack found modern operating systems specifically introduce ictcp pronounced tcp slightly modified in-kernel tcp stack exports key pieces state information safe control user-level libraries exposing state safe control tcp connections ictcp enables broad range interesting important network services built user-level user-level services built ictcp deployable services implemented tcp stack services packaged libraries easily downloaded interested parties approach inherently flexible developers tailor exact applications finally extensions composable library services build powerful functionality lego-like fashion general ictcp facilitates development services reside key advantage ictcp compared approaches upgrading network protocols simplicity implementing ictcp framework platform simplicity virtue reasons ictcp leverages entire existing tcp stack simple convert traditional tcp implementation ictcp linux-based implementation requires approximately lines code small amount code change reduces chances introducing bugs protocol previous tcp modifications property advantage ictcp safe manner user-level control safety issue time users allowed modify behavior ictcp users allowed control set limited virtual tcp variables cwnd dupthresh rto users download arbitrary code safety concern remaining concern network safety applications implement tcp extensions friendly competing flows building top extant tcp reno stack restricting virtual variables safe range values ictcp ensures extensions aggressive tcp reno friendly addition providing simplicity safeness framework ictcp address additional questions overheads implementing variants tcp ictcp reasonable measurements show services built ictcp scale incur minimal cpu overhead ictcp waiting mechanisms wide range functionality implemented conservative approach demonstrate utility ictcp implementing extensions tcp set case studies focus modifications alter behavior transport appears sixth symposium operating systems design implementation osdi regard congestion tcp vegas tcp nice congestion manager set focus tcp modifications behave differently presence duplicate acknowledgments build reodering-robust extension misinterpret packet reordering packet loss extension efficient fast retransmit efr set explore tcp eifel adjusts retransmit timeout finally services developed easily framework show amount code required build extensions user-level services ictcp similar original native implementations rest paper structured section compare ictcp related work extensible network services section present design ictcp section describe methodology section evaluate important aspects ictcp simplicity implementing ictcp platform network safety ensured user-level extensions computational overheads range tcp extensions supported complexity developing extensions conclude section related work section compare ictcp approaches provide networking extensibility upgrading tcp recent projects proposed frameworks providing limited extensions transport protocols protocols tcp evolve improve ensuring safety tcp friendliness compare ictcp proposals mogul propose applications radically set tcp state terms tcp state ictcp similar proposal greater philosophical difference arises internal tcp state set mogul arbitrary state setting suggest safety provided cryptographic signature previously exported state restricting ability super-user ictcp conservative allowing applications alter parameters restricted fashion trade-off ictcp guarantee network services behaved mogul approach enable broader range services session migration web net projects developing management interface tcp similar information component ictcp web instruments tcp export variety per-connection statistics web propose exporting detailed information ictcp web export timestamps message acknowledgment tcp-tuning daemon net similar control component ictcp daemon observes tcp statistics responds setting tcp parameters key difference ictcp net propose allowing complete set variables controlled ensure network safety net appears suitable tuning parameters set frequently ictcp frequently adjust in-kernel variables per-message statistics ability block in-kernel events occur stp addresses problem tcp deployment stp enables communicating end hosts remotely upgrade protocol stack stp authors show broad range tcp extensions deployed emphasize major differences stp ictcp stp requires invasive kernel support safe downloading extension-specific code support in-kernel extensibility fraught difficulty contrast ictcp makes minimal kernel stp requires additional machinery ensure tcp friendliness ictcp guarantees friendliness design stp powerful framework tcp extensions ictcp provided easily safely finally information component ictcp derived infotcp proposed part infokernel previous work showed infotcp enables userlevel services indirectly control tcp congestion window cwnd ictcp improves infotcp main ways ictcp exposes information complete set tcp variables ictcp services directly set cwnd inside tcp applications perform extra buffering incur sleep wake events finally ictcp tcp variables cwnd controlled ictcp tcp extensions implemented efficient accurate user-level tcp researchers found move portions conventional network stack userlevel user-level tcp simplify protocol development ictcp userlevel tcp implementation typically struggles performance due extra buffering context switching assurance network safety application-specific networking large body research investigated provide extensibility network services projects network protocols specialized applications ictcp improve performance dramatically approaches tend require radical restructuring networking stack guarantee tcp friendliness protocol languages architectures network languages structured tcp implementations simplify development network protocols ability replace specialize modules appears sixth symposium operating systems design implementation osdi states snd nxt snd una information cwnd ssthresh msglist ack dsack seq rtt acklist timeout cwnd cwnd cnt ssthresh rcv wnd rcv nxt snd una snd nxt dupthresh rto retransmits control tcp clients ictcp vegas ictcp nice ictcp ictcp efr ictcp eifel ictcp ictcp figure ictcp architecture diagram shows ictcp architecture base stack ictcp slightly modified tcp stack exports information limited control top ictcp built number user-level libraries implement pieces functionality suggested literature libraries composed applicable enabling construction powerful services plug-and-play fashion applications sit top stack choose libraries match directly kernel transport generally easier extend existing tcp implementations ictcp design ictcp framework exposes information control key parameters tcp protocol implementation section give high-level overview user-level network services deployed ictcp describe classes information control exported ictcp system architecture figure presents schematic ictcp framework illustrated user-level libraries implementing variants tcp built top ictcp user-level libraries transparently applications 
standard interfaces tcp connections ictcp libraries design ictcp sending side ictcp deployed receivers running ictcp unmodified kernel stack simplify implementation ictcp bsd socket interface exporting information providing control socket options approach minimized implementation work imposes unnecessary run-time overhead obtaining state requires copy kernel user space evaluation shows user-level network services naively poll ictcp frequently state information incur significant increase cpu overhead minimize overhead ictcp polling interrupt-based interface tcp variables updated acknowledgment arrives end round roundtrip time elapsed applications receive interrupt condition case studies ictcp user-level libraries structured threads thread injects packets kernel performs sleep wait set operations information goal ictcp expose information traditionally internal tcp challenge determine information exposed information exposed build interesting extensions information exposed future kernel implementations tcp constrained undesirable expanded interface tcp implementations constrained adhere tcp specification internal variables required ictcp explicitly exports variables part tcp specification sequence number snd nxt oldest unacknowledged sequence number snd una congestion window cwnd slow start threshold ssthresh exposing information tcp implementation straightforward found interesting services access information needed libraries ictcp-nice ictcp-rr examine information message ictcp exposes standard information packet message list history recent packets reporting packet sequence number round-trip time time-out fast retransmit ack list history recent acknowledgments recording packet acknowledgment number type normal ack duplicate ack sack dsack exposing per-packet per-ack information trivial tcp implementations exist tcp reno track round-trip time packet add high resolution timer ictcp record information additional complexity recording per-message information requires additional memory ictcp creates lists enabled user-level services control goal ictcp variables internal tcp externally set safe manner challenge determine variables modified values ensuring resulting behavior tcp-friendly philosophy appears sixth symposium operating systems design implementation osdi variable description safe range usage cwnd congestion window limit number packets cwnd cnt linear cwnd increase increase cwnd aggressively ssthresh slow start threshold move rcv wnd receive window size reject packet limit sender rcv nxt expected seq num vrcv wnd reject packet limit sender snd nxt seq num send vsnd una reject ack enter snd una oldest unacked seq num vsnd nxt reject ack enter frfr dupthresh duplicate threshold vcwnd enter frfr rto retransmission timeout exp backoff srtt rttvar enter retransmits number consecutive timeouts threshold postpone killing connection table safe setting tcp variables table lists tcp variables set ictcp range variable safely set ensuring result aggressive baseline tcp implementation give usage intuition control variable notation refers tcp original copy variable refers virtual copy set slow start congestion avoidance frfr fast retransmit fast recovery finally srtt rttvar exp backoff represent smoothed round-trip time round-trip time variance rto exponential backoff ictcp conservative control allowed aggressive transmission basic idea variable interest ictcp adds limited virtual variable terminology tcp variable original foo introduce limited virtual variable vfoo meaning clear simply original restrict range values virtual variable allowed cover resulting tcp behavior friendly ensure tcp actions aggressive original tcp implementation acceptable range variable function fluctuating tcp variables check call time user valid reject invalid settings ictcp accepts settings coerces virtual variable valid range safe range virtual congestion window vcwnd vcwnd cwnd vcwnd rises cwnd cwnd converting variable virtual variable ictcp stack trivial simply replace instances original variable virtual ensure virtual change original variable simplest case statement cwnd cwnd replaced cwnd vcwnd complex cases control flow require careful manual inspection limit extent original variable replaced virtual variable foremost goal ictcp ensure ictcp create aggressive flows conservative virtual variables introduce interesting tcp variables set current implementation ictcp control ten variables convinced safely set analysis linux tcp implementation introduce virtual variables original variable set interfaces sysctl tcp retries user mss approximated ways set rto srtt mdev rttvar mrtt claim ten variables represent complete collection settable values form set ten variables safe ranges summarized table briefly discuss range values safe ictcp variable variables cwnd cwnd cnt ssthresh property safe strictly lower case sender directly transmits data congestion window smaller cwnd cwnd cnt slow-start entered congestion avoidance ssthresh set variables determine packets acknowledgments accepted constraints variables complex receiver packet accepted sequence number falls inside receive window rcv nxt rcv nxt rcv wnd increasing rcv nxt decreasing rcv wnd effect rejecting incoming packets forces sender reduce sending rate sender acknowledgment processed sequence number snd una snd nxt increasing snd una decreasing snd nxt sender discard acks reduce sending rate case modifying values effect dropping additional packets tcp backs-off appropriately final set variables dupthresh rto retransmits control thresholds timeouts variables set independently original values increasing decreasing dupthresh believed safe changing values increase amount traffic sender transmit packets increase congestion window appears sixth symposium operating systems design implementation osdi information loc control loc states cwnd message list dupthresh ack list rto high-resolution rtt ssthresh wakeup events cwnd cnt retransmits rcv nxt rcv wnd snd una snd nxt info total control total ictcp total table simplicity environment table reports number statements counted number semicolons needed implement current prototype ictcp linux methodology prototype ictcp implemented linux kernel experiments performed exclusively netbed network emulation environment single netbed machine mhz pentium cpu main memory intel etherexpress pro ethernet ports sending endpoints run ictcp receivers run stock linux experiments dumbbell topology senders routers interconnected potential bottleneck link receivers experiments modified nistnet router nodes emulate complex behaviors packet reordering experiments vary combination bottleneck bandwidth delay maximum queue size intermediate router nodes experiments run multiple times averages reported variance low cases shown evaluation evaluate ictcp reasonable framework deploying tcp extensions user-level answer questions easily existing tcp implementation converted provide information safe control ictcp ictcp ensure resulting network flows tcp friendly computation overheads deploying tcp extensions user-level processes ictcp scale fourth types tcp extensions built deployed ictcp finally difficult develop tcp extensions note spend bulk paper addressing fourth question range extensions implemented discussing limitations approach set internal tcp variables tcp setsockopt option val switch option case tcp vcwnd vcwnd val case tcp set vcwnd vcwnd val check data put wire tcp snd test vcwnd min cwnd min vcwnd cwnd min cwnd cwnd transmit tcp packets flight min cwnd rules return return figure in-kernel modification adding vcwnd tcp stack requires lines code ictcp applications set virtual variables bsd setsockopt interface 
based congestion window tcp snd test checks data put wire show adding virtual cwnd decision-making process simple straightforward cwnd ictcp minimum vcwnd cwnd simplicity environment begin addressing question difficult convert tcp implementation ictcp initial version ictcp implemented linux experience implementing ictcp fairly straightforward requires adding lines code table shows added statements tcp create ictcp number statements added perfect indicator complexity non-intrusive modifications figure partial vcwnd variable added ictcp stack network safety investigate ictcp flows tcp friendly perform evaluation measure throughput default tcp flows competing ictcp flows measurements show ictcp tcp friendly desired default tcp flows obtain bandwidth competing ictcp competing default tcp flows show constraining values valid range ictcp illustrate created unconstrained ictcp virtual variables set default tcp flows compete unconstrained ictcp flows throughput appears sixth symposium operating systems design implementation osdi throughput ratio cwnd cwnd cwnd cwnd cnt cwnd cnt cwnd cnt ssthresh ssthresh ssthresh throughput ratio flows set snd una snd una mss snd una mss flows set dupthresh dupthresh dupthresh flows set rto srtt var srtt var figure network safety ictcp graph shows lines line default default ictcp enforces parameters values safe range line unconstrained ictcp parameters set dupthresh graph unconstrained ictcp lines metric ratio throughput achieved default tcp flows competing ictcp flows versus competing default tcp flows graphs vary ictcp parameters set case set variable unsafe cwnd packets larger cwnd cnt times larger ssthresh times larger snd una packets lower dupthresh random values default rto remaining initial srtt rttvar packets dropped topology dumbbell senders receivers experiments rto experiments bottleneck bandwidth mbps delay rto experiments bottleneck bandwidth mbps percents drop rate default tcp flows reduced measurements shown figure graphs evaluate ictcp parameters explicitly setting parameter safe range x-axis graph increase number competing ictcp tcp flows graph shows lines line ictcp flows matching proposal virtual variables limited safe range line unconstrained ictcp flows metric ratio throughput achieved default tcp flows competing ictcp flows versus competing default tcp flows throughput ratio ictcp flows friendly ictcp flows unfriendly cwnd cwnd cnt ssthresh experiments show variables set safe range ensure friendliness expected ictcp flows allowed increase congestion window default tcp remain tcp friendly unconstrained ictcp flows larger congestion windows overly aggressive result competing tcp flows obtain fair share bandwidth evaluate variables control acknowledgments packets accepted behavior snd una shown fourth graph snd una variable represents highest unacknolwedged packet virtual snd una set safe range actual unconstrained ictcp over-estimates number bytes acknowledged increases congestion window aggressively ictcp correctly constrains snd una flow remains friendly results variables rcv wnd rcv nxt snd nxt shown cases ictcp flows remain friendly desired unconstrained ictcp flows fail completely increasing rcv wnd variable safe range receive buffer overflow final graphs explore dupthresh rto thresholds experiment retransmits variable decide connection terminated expected dupthresh decreasing increasing default unfriendliness dupthresh constrained case rto graph shows rto set exp backoff srtt rttvar resulting flow aggressive graphs represent small subset experiments conducted investigate tcp friendliness experimented setting ictcp variables random values safe range controlled ictcp parameters isolation sets parameters simultaneously cases tcp reno flows competing ictcp obtain appears sixth symposium operating systems design implementation osdi cpu utilization number connections scaling cpu per-ack intr per-round intr mstlist per-round intr reno throughput number connections scaling throughput reno per-round intr per-round intr msglist per-ack intr figure cpu overhead throughput scaling ictcp connect sender host receiver hosts network interfaces links mbps delay links aggregrate sender host send data outward mbps x-axis increase number connections sender host connections spread evenly receivers figure compares cpu utilization reno ictcp per-ack per-round interrupt figure shows ictcp throughput degradation sender load high bandwidth competing tcp reno flows desired summary results empirically demonstrate ictcp flows require safe variable settings tcp friendly experiments prove ictcp ensures network safety measurements combined analysis give confidence ictcp safely deployed cpu overhead evaluate overhead imposed ictcp framework ways explore scalability ictcp synthetic user-level libraries experiments explore ways user-level library reduce cpu overhead minimizing interactions kernel implement tcp vegas user-level top ictcp experiments directly compare ictcp infotcp scaling ictcp evaluate ictcp scales number connections increased host user-level extensions built ictcp expected set pieces tcp information rates factors determine amount overhead user process requires per-ack per-round interrupts user process ictcp message list ack list data structures show scaling properties user libraries built ictcp construct synthetic libraries mimic behavior case studies synthetic library per-ack interrupts representing ictcp-efr ictcp-eifel library per-round interrupts iccm final library per-round interrupts message ack list data structures ictcp-vegas ictcp-nice ictcp-rr graphs figure show ictcp tcp reno scale number flows increased host figure reports cpu utilization figure reports throughput figure shows ictcp per-ack per-round interrupts reaches cpu utilization connections additional cpu overhead ictcp message list negligible comparison tcp reno reaches roughly utilization connections slowly increases roughly connections figure shows throughput ictcp starts degrade connections depending per-ack per-round interrupts flows ictcp throughput per-ack per-round interrupts lower tcp reno ictcp cpu overhead noticeable prohibitive measure extent user-level library accurately implement tcp functionality measure interrupt miss rate defined frequently user misses interrupt ack end round scaling experiments connections observed worst-case miss rate per-ack interrupts per-round interrupts low miss rates imply functionality userlevel responsive current network conditions ictcp-vegas evaluate ictcp implement tcp vegas congestion avoidance user-level library tcp vegas reduces latency increases throughput relative tcp reno carefully matching sending rate rate packets drained network avoiding packet loss specifically sender sees measured throughput differs expected throughput fixed threshold increases appears sixth symposium operating systems design implementation osdi cpu utilization bandwidth delay ictcp-vegas cpu overhead infovegas ictcp-vegas per-ack intr ictcp-vegas polling ictcp-vegas per-round intr reno figure ictcp-vegas cpu overhead figure compares cpu utilization reno infovegas versions ictcp-vegas vary bottleneck-link bandwidth x-axis decreases congestion control window cwnd implementation implementation vegas congestion control algorithm ictcp-vegas structured operation vegas user-level library library simply passes messages directly ictcp buffering layer implement versions vary point poll ictcp information time send packet time acknowledgment received round ends library relevant tcp state calculates target congestion window vcwnd vcwnd ictcp-vegas sets explicitly inside ictcp note implementation ictcp-vegas similar infovegas part infokernel primary difference infotcp manage vcwnd provide control tcp variables infovegas 
calculates vcwnd actual cwnd infovegas buffer packets transfer tcp layer infovegas blocks acknowledgment arrives point recalculates vcwnd send messages evaluation verified ictcp-vegas behaves in-kernel implementation vegas due space constraints show results focus evaluation cpu overhead figure shows total user system cpu utilization function network bandwidth tcp reno versions ictcp-vegas infovegas network bandwidth increases cpu utilization increases implementation cpu utilization system utilization increases significantly infovegas due frequent user-kernel crossings extra overhead reduced ictcp-vegas polls ictcp message send wakes arrival acknowledgment document latency bottleneck link bandwidth mbps link capacity latency reno tcp nice ictcp-nice figure ictcp-nice link capacity latency foreground flow competes background flows line corresponds run experiment protocol background flows ictcp tcp nice reno vegas y-axis shows average document transfer latency foreground traffic foreground traffic consists -minute section squid proxy trace logged berkeley background traffic consists long-running flows topology dumbbell sending nodes receiving nodes foreground flow sender receiver pairs background flows distributed remaining sender receiver pairs bottleneck link bandwidth varied x-axis noticeable ictcp information getsockopt interface incurs significant overhead ictcp-vegas greatly reduce overhead information frequently vegas adjusts cwnd end round ictcp-vegas behave accurately waking round optimization results cpu utilization higher ictcp-vegas in-kernel reno tcp extensions fourth axis evaluating ictcp concerns range tcp extensions importance issue spend remaining paper topic address question demonstrating tcp variants built top ictcp case studies explicitly meant exhaustive illustrate flexibility simplicity ictcp briefly discuss ictcp implement wider set tcp extensions ictcp-nice case study show tcp nice implemented user-level ictcp study demonstrates algorithm differs radically base ictcp reno algorithm implemented ictcp-nice requires access internal state ictcp complete message list overview tcp nice zero-cost background transfer tcp nice background flow interferes foreground flows reaps large fraction spare network bandwidth tcp nice appears sixth symposium operating systems design implementation osdi similar tcp vegas additional components multiplicative window reduction response increasing round-trip times ability reduce congestion window discuss components turn tcp nice halves current congestion window long round-trip times measured unlike vegas reduces window halves window packets lost determine window size halved tcp nice algorithm monitors round-trip delays estimates total queue size bottleneck router signals congestion estimated queue size exceeds fraction estimated maximum queue capacity specifically tcp nice counts number packets delay exceeds minrtt maxrtt minrtt fraction delayed packets round exceeds tcp nice signals congestion decreases window multiplicatively tcp nice window effect congestion window tcp nice adds timer waits number rtts sending packets implementation implementation ictcp-nice similar ictcp-vegas slightly complex ictcp-nice requires information packet summary statistics ictcp-nice obtains full message list sequence number seqno round trip time usrtt packet implementation windows tricky vcwnd mechanism case window ictcp-nice sets vcwnd single rtt period periods evaluation demonstrate effectiveness ictcp approach replicate experiments original tcp nice paper figures results show ictcp-nice performs identically in-kernel tcp nice desired figure shows latency foreground connections competes background connections spare capacity network varied results ictcp-nice tcp nice background connections latency foreground connections order magnitude faster tcp reno background connections desired ictcp-nice tcp nice perform similarly graphs figure show latency foreground connections throughput background connections number background connections increases graph top shows background flows added document latency remains essentially constant ictcp-nice tcp nice background flows graph bottom shows ictcp-nice tcp nice obtain document latency sec number background latency reno tcp nice ictcp-nice throughput number background throughput reno tcp nice ictcp-nice figure ictcp-nice impact background flows graphs correspond experiment graph shows average document latency foreground traffic graph shows number bytes background flows manage transfer minutes period line corresponds protocol background flows tcp reno ictcp-nice tcp nice number background flows varied x-axis bottleneck link bandwidth set kbps delay experimental setup identical figure throughput number flows increases desired ictcp-nice tcp nice achieve similar results iccm show important components congestion manager built ictcp main contribution study show information shared ictcp flows multiple ictcp flows sender cooperate overview congestion manager architecture motivated types problematic behavior exhibited emerging applications applications employ multiple concurrent flows sender receiver flows compete resources prove overly aggressive share network information applications udp-based flows sound congestion control adapt changing network conditions addresses problems inserting module sender receiver layer appears sixth symposium operating systems design implementation osdi maintains network statistics flows orchestrates data transmissions hybrid congestion control algorithm obtains feedback receiver implementation primary difference iccm location iccm built top ictcp layer top iccm leverages congestion control algorithm statistics present tcp iccm considerably simpler implement iccm guarantees congestion control algorithm stable friendly existing tcp traffic iccm approach drawback non-cooperative applications bypass iccm tcp directly iccm guarantee fairness flows aware iccm architecture running sending endpoint components iccm clients individual flow iccm server component receiving endpoint iccm server roles identify macroflows flows endpoint destination track aggregate statistics macroflow identify macroflows client flow registers process destination address iccm server track statistics client flow periodically obtains network state ictcp number outstanding bytes snd nxt snd una shares state iccm server iccm server periodically updates statistics macroflow sums outstanding bytes flow macroflow client flow obtain aggregate statistics macroflow time intervals implement bandwidth sharing clients macroflow client calculates window limit number outstanding bytes specifically iccm client obtains server number flows macroflow total number outstanding bytes flow statistics client calculates number bytes send obtain fair share bandwidth client tcp transport simply sets vcwnd ictcp number iccm clients macroflow compete share bandwidth evenly evaluation demonstrate effectiveness ictcp build congestion manager replicating experiments performed figure experiments shown figure place flows macroflow shown graph tcp reno flows macroflow share bandwidth fairly performance connections varies standard deviation contrast shown graph iccm flows macroflow connections progress similar consistent rates iccm flows achieve throughputs sequence number time seconds reno sequence number time seconds iccm figure iccm fairness graphs compare performance concurrent transfers sender receiver bottleneck link set delay graph stock reno graph iccm manages tcp flows roughly standard deviation ictcp-rr tcp fast retransmit optimization fairly sensitive presence duplicate acknowledgments specifically tcp detects duplicate acks arrived assumes loss occurred triggers retransmission recent research packet reordering common internet earlier designers suspected frequent reordering occurs tcp sender receives rash duplicate acks wrongly concludes loss occurred result segments unnecessarily retransmitted wasting bandwidth congestion window needlessly reduced lowering client performance overview 
number solutions handling duplicate acknowledgments suggested literature high level algorithms detect presence reordering dsack increase duplicate threshold dupthresh avoid triggering fast retransmit base implementation blanton allman work limits maximum dupthresh window appears sixth symposium operating systems design implementation osdi false fast retransmits packet delay rate false fast retransmits linux linux dsack ictcp-rr throughput packet delay rate throughput linux dsack ictcp-rr linux figure avoiding false retransmissions ictcp-rr top number false retransmissions bottom throughput vary fraction packets delayed reordered modified nistnet router compare implementations text experimental setup includes single sender receiver bottleneck link set delay nistnet router runs router introducing distributed packet delay standard deviation size timeout occurs sets dupthresh back original implementation user-level library implementation ictcp-rr straight-forward library history acks received list larger kernel exported ack list kernel aggressive pruning size losing potentially valuable information dsack arrives ictcp places sequence number falsely retransmitted packet ack list library consults ack history frequently occurrences found library searches past history measure reordering length sets dupthresh evaluation figure shows effects packet reordering compare implementations stock linux dsack enhancement linux dsack reordering avoidance built kernel user-level ictcp-rr implementation graph show number false fast retransmisretransmitted packets loss rate retransmissions ictcp-efr reno reno ictcp-efr throughput loss rate throughput ictcp-efr reno figure aggressive fast retransmits ictcp-efr top number retransmitted packets reno ictcp-efr due retransmission timeouts fast retransmits bottom achieved bandwidth x-axis vary loss rate mimic wireless lan single sender single receiver bottleneck link set delay sions occur false retransmission caused reordering stock kernel issues false retransmits incorrectly believes reordering actual packet loss graph observe resulting bandwidth dsack in-kernel ictcp-rr versions perform essentially ignoring duplicate acks achieving higher bandwidth ictcp-efr previous case study showed increasing dupthresh contrast environments wireless lans loss common duplicate acks strong signal packet loss window size small case opposite solution desired dupthresh lowered invoking fast retransmit aggressively avoid costly retransmission timeouts overview discuss ictcp-efr user-level library implementation efr efficient fast retransmit observation underlying efr simple appears sixth symposium operating systems design implementation osdi rto sec time eifel rto ictcp user-level karn-patridge ul-kp ul-kp lines spike fixed ictcp-eifel measured rtt rto time self-trained eifel rto rto measured rtt figure adjusting rto ictcp-eifel graph top shows versions ictcp-eifel experiment measured round-trip time identical calculated rto differs line shows karn-partridge rto algorithm disabled kernel implemented user-level ictcp experiment remove lines tcp code added fix rto spike show fix easily provided user-level experiment implement full eifel rto algorithm user-level experiments emulate bandwidth kbps delay queue size graph bottom shows full adaptive eifel rto algorithm bandwidth kbps delay queue size sender adjust dupthresh match number duplicate acks receive implementation ictcp-efr implementation straightforward simplicity modify dupthresh window small efr scheme relevant window small library frequently checks message list duplicate acks sees computes sets dupthresh evaluation figure shows behavior ictcpefr versus in-kernel reno function loss rate emulated wireless network ictcp-efr interprets duplicate acknowledgments signs loss number fast retransmits increases shown graph top importantly number costly retransmission timeouts reduced graph bottom shows bandwidth increases result ictcp-eifel retransmission timeout rto determines time elapse packet sender considers lost retransmits rto prediction upper limit measured round-trip time mrtt correctly setting rto greatly influence performance overly aggressive rto expire prematurely forcing unnecessary spurious retransmission overly-conservative rto long idle times lost packets retransmitted overview eifel rto corrects problems traditional karn-partridge rto immediately mrtt decreases rto incorrectly increased period time rto decay correct magic numbers rto calculation assume low mrtt sampling rate sender load assumptions incorrect rto incorrectly collapses mrtt implementation implemented eifel rto algorithm user-level library ictcp-eifel library access ictcp variables mrtt ssthresh cwnd mrtt calculates values srtt smoothed round-trip rttvar round-trip variance ictcp-eifel library operates wakes acknowledgment arrives polls ictcp mrtt mrtt changed calculates rto sets ictcp library requires safe control rto evaluation graph figure shows progression improvements ictcp-eifel experiments approximately match eifel rto paper figure implementation disable karn-partridge rto algorithm kernel implement ictcp-eifel expected version incorrectly increases rto mrtt decreases implementation corrects problem additional lines code user-level rto eventually collapses mrtt finally version ictcp-eifel adjusts rto conservative avoids spurious retransmissions graph figure similar figure eifel paper shows implemented full eifel rto algorithm user-level algorithm rto increasingly aggressive spurious timeout occurs point backs conservative summary case studies number strengths ictcp approach ictcp easily enables tcp variants aggressive reno implemented simply efficiently user-level tcp vegas tcp nice push kernel ictcp ideally appears sixth symposium operating systems design implementation osdi suited tuning parameters optimal values depend environment workload dupthresh ictcp correcting errors parameter values behavior rto case studies illustrated limitations ictcp iccm assemble framework shares information flows information shared flows voluntarily congestion state learned previous flows directly inherited flows limitation arises ictcp reliance inkernel tcp stack forcibly set starting congestion state implementing extensions evaluate ability ictcp implement wider range tcp extensions list discussed stp extensions standardized linux sack dsack fack tcp high performance ecn reno syn cookies implemented ictcp rr-tcp vegas nice discuss challenges implementing remaining extensions place extensions categories introduce algorithms existing variables modify packet format modify tcp algorithm structure mechanisms existing variables classify extensions changing behavior existing variables byte counting abc tcp westwood equation-based tcp tfrc recently proposed tcp extensions fall category include fast tcp limited slow-start highspeed tcp extensions natural match ictcp implemented extent aggressive tcp reno equationbased tcp specifies congestion window increase decrease gradually reno ictcpeqn cwnd increase gradually desired forces cwnd decrease usual reno rate conservative implementations extensions beneficial abc implemented ictcp aggressively increase cwnd receiver delays ack ictcp-abc correct ack division case highspeed tcp extension supported manner strictly aggressive cwnd decreased smaller amount tcp reno issue arises extensions ictcp enforces tcp friendliness ictcp constrains tcp virtual variable safe range overly conservative ictcp small increases tcp initial congestion window long time period flows generally considered tcp friendly alternatively stp separate module enforce tcp friendliness module monitors sending rate verifies upper-bound determined state connection packet size loss event rate round-trip time retransmission timeout ictcp similar modular approach equation-based enforcer important drawback non-conforming flows terminated packets buffered bounded size tcp-friendly rate terminate flows ictcp naturally modulates agressive flows manner efficient space time packet format classify extensions changing 
format contents packets extensions put bits tcp reserved field eifel algorithm robust congestion signaling extensions implemented easily ictcp current form compelling expand ictcp variables packet header set difficult ensure safely approximate behavior encapsulating extra information application data requiring sender receiver ictcp-enabled kernel library technique extra information passed protocol stacks remaining transparent applications technique implemented functionality similar dccp implementation user-level library transmits packets udp obtains network information ictcp flow sender receiver investigating approach detail structure mechanism approximately extensions modify fundamental aspects tcp algorithm extensions follow existing tcp states tcp limited transmit define mechanisms sctp checksum extensions deviate substantially base tcp reno algorithm ictcp implement behavior approach addressing limitation modifying packet headers ictcp provide control underneath kernel stack packet filter users exert control packets changing timing ordering altogether suppressing duplicating subset packets pass filter control meted caution ensuring remain tcp friendly central challenge summary ictcp powerful stp implement smaller range tcp extensions appears sixth symposium operating systems design implementation osdi throughput kbps packet delay rate composing ictcp-rr ictcp-vegas ictcp-rr ictcp-vegas ictcp-rr ictcp-vegas reno figure composing ictcp-vegas ictcp-rr figure shows strength composing multiple ictcp libraries environment reordering occurs space bottleneck queue low libraries time environment throughput higher compared libraries experimental setup includes single sender receiver bottleneck queue size set link set delay nistnet router runs router introducing distributed packet delay standard deviation x-axis vary percentage delayed packets simplicity providing ictcp layer real system outweigh drawback ease development final question address complexity ictcp framework develop tcp extensions answer question showing ease user-level libraries ictcp combined perform functionality directly compare complexity building tcp extensions user-level building directly kernel ictcp framework enables functional composition user-level library exports interface ictcp library services stacked build powerful functionality simplest case stacked libraries control disjoint sets ictcp variables ictcp-vegas ictcp-rr libaries stacked combination controls values cwnd dupthresh figure shows advantage stacking libraries flows running environment packet reordering small bottleneck queues exhibit higher throughput libraries libary alternatively stacked libaries control overlapping sets ictcp variables case layer constrains range safe values virtual variable quantify complexity building functionality top ictcp kernel count number statements implementation number semicolons removing printing debugging table shows number case study ictcp native ictcp-vegas ictcp-nice iccm ictcp-rr table ease development ictcp table reports number statements counted number semicolons needed implement case studies ictcp compared native implementation native vegas implementation count entire patch linux tcp nice count statements changing core transport layer algorithm quantifying number needed statements complicated fact authors provide complete linux kernel modifications distributed count transport layer comparison fair functionality iccm count number lines linux calculate amount reordering in-kernel sack dsack ictcp-rr traverses ack list statements required case studies implementations vegas nice comparing ictcp user-level libraries native implementations number statements comparable conclude developing services ictcp complex building natively advantage debugging analysis performed user-level conclusions presented design implementation ictcp slightly modified version linux tcp exposes information control applications userlevel libraries evaluated ictcp axes findings converting tcp stack ictcp requires small amount additional code determining precisely limited virtual parameters place original tcp parameters nontrivial exercise ictcp ten internal tcp variables safely set user-level processes values chosen user resulting flow tcp friendly ictcp incurs minimal additional cpu overhead relative in-kernel implementations long ictcp polled excessively information reduce overhead ictcp processes block acknowledgment arrives end round fourth ictcp enables range tcp extensions implemented user-level found ictcp framework suited extensions implement congestion control algorithms aggressive reno adjusting parameters match workload environment conditions support radical tcp extensions ictcp developed allowing tcp headers safely set packets acknowledgments reordered delayed finally developing tcp extensions appears sixth symposium operating systems design implementation osdi top ictcp complex implementing directly kernel easier debug exposing information control layers network stack similarity tcp sctp sctp extended straight-forward manner icsctp icsctp framework user-level libraries deal problems spurious retransmission implement functionality network failure detection recovery conclusion ictcp powerful proposals extending tcp networking protocols advantage ictcp simplicity pragmatism easy implement ictcp flows built ictcp remain tcp friendly computational overheads reasonable systems ictcp practice theory reap benefits userlevel tcp extensions acknowledgments experiments paper performed exclusively netbed network emulation environment utah greatly indebted robert ticci tim stack leigh stoller kirk webb jay lepreau providing superb environment networking research nitin agrawal lakshmi bairavasundaram nathan burnett vijayan prabhakaran muthian sivathanu helpful discussions comments paper jeffrey mogul excellent shepherding substantially improved content presentation paper finally anonymous reviewers helpful suggestions work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation abbott peterson language-based approach protocol implementation ieee acm transactions networking feb allman tcp congestion control byte counting rfc feb allman balakrishnan floyd enhancing tcp loss recovery limited transmit jan rfc allman floyd patridge increasing tcp initial window rfc internet engineering task force allman paxson stevens tcp congestion control rfc internet engineering task force apr armando caro iyengar amer ladha gerard heinz shah sctp proposed standard robust internet data transport ieee computer november arpaci-dusseau arpaci-dusseau burnett denehy engle gunawi nugent popovici transforming policies mechanisms infokernel sosp balakrishnan rahul seshan integrated congestion management architecture internet hosts sigcomm pages bellardo savage measuring packet reordering proceedings acm usenix internet measurement workshop marseille france nov biagioni structured tcp standard proceedings sigcomm pages london united kingdom aug blanton allman making tcp robust packet reordering acm computer communication review jan blanton allman tcp dsacks sctp duplicate tsns detect spurious retransmissions rfc internet engineering task force february braden tcp tcp extensions transactions rfc internet engineering task force brakmo malley peterson tcp vegas techniques congestion detection avoidance proceedings sigcomm pages london united kingdom aug cardwell bak tcp vegas implementation linux http flophouse neal linux-vegas carson santay nist network emulation tool snad ncsl nist gov nistnet january cheng jin low fast tcp motivation architecture algorithms performance infocom dunigan mathis tierney tcp tuning daemon nov edwards muir experiences implementing high-performance tcp user-space sigcomm pages cambridge massachusetts aug ely savage wetherall alpine user-level infrastructure network protocol development proceedings usenix symposium internet technologies systems usits pages san francisco california mar ely spring wetherall savage robust congestion signaling icnp nov fiuczynski bershad extensible protocol architecture application-specific networking proceedings usenix annual technical conference usenix san diego california jan floyd reno modification tcp fast 
recovery algorithm rfc internet engineering task force floyd highspeed tcp large congestion windows rfc internet engineering task force floyd limited slow-start tcp large congestion windows rfc internet engineering task force floyd handley padhye widmer equationbased congestion control unicast applications proceedings sigcomm pages stockholm sweden aug floyd mahdavi mathis podolsky extension selective acknowledgment sack option tcp rfc internet engineering task force ganger engler kaashoek briceno hunt pinckney fast flexible application-level networking exokernel systems acm tocs feb appears sixth symposium operating systems design implementation osdi isi usc transmission control protocol rfc sept jacobson congestion avoidance control proceedings sigcomm pages stanford california aug jacobson braden borman tcp extensions high performance rfc internet engineering task force iyengar amer heinz two-level threshold recovery mechanism sctp sci karn partridge improving round-trip time estimates reliable transport protocols proceedings sigcomm aug kohler handley floyd designing dccp congestion control reliability icir kohler dcp dccp-icnp pdf kohler kaashoek montgomery readable tcp prolac protocol language proceedings sigcomm pages cambridge massachusetts aug ludwig sklower eifel retransmission timer acm computer communications review july maeda bershad protocol service decomposition high-performance networking proceedings acm symposium operating systems principles sosp pages asheville north carolina dec mahdavi floyd tcp-friendly unicast ratebased flow control end end-interest mailing list http psc networking papers tcp friendly html jan mathis heffner reddy web extended tcp instrumentation acm computer communications review july mathis mahdavi floyd romanow tcp selective acknowledgment options rfc internet engineering task force mogul brakmo lowell subhraveti moore unveiling transport hotnets mogul rashid accetta packet filter efficient mechanism user-level networkcode proceedings acm symposium operating systems principles sosp austin texas november padhye floyd inferring tcp behavior sigcomm pages august patel whitaker wetherall lepreau stack upgrading transport protocols untrusted mobile code sosp paxson allman dawson fenner griner heavens lahey semke volz tcp implementation problems rfc internet engineering task force mar pradhan kandula shaikh nahum daytona user-level tcp stack http nms lcs mit kandula data daytona pdf ramakrishnan floyd black addition explicit congestion notification ecn rfc internet engineering task force seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating systems design implementation osdi pages seattle washington oct stone stewart otis stream control transmission protocol rfc sept tamura tobe tokuda efr retransmit scheme tcp wireless lans ieee conference local area networks pages thekkath nguyen moy lazowska implementing network protocols user level ieee acm transactions networking venkataramani kokku dahlin tcp-nice mechanism background transfers proceedings symposium operating systems design implementation osdi pages boston massachusetts dec von eicken basu buch vogels u-net user-level network interface parallel distributedcomputing proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado dec wallach engler kaashoek ashs application-specific handlers high-performance messaging ieee acm transactions networking aug wang valla sanadidi gerla adaptive bandwidth share estimation tcp westwood ieeeglobecom white lepreau stoller ricci guruprasad newbold hibler barb joglekar integrated experimental environment distributed systems networks proceedings symposium operating systems design implementation osdi pages boston massachusetts dec zhang karp floyd peterson rr-tcp reordering-robust tcp dsack international conference network protocols icnp june 
iron file systems vijayan prabhakaran lakshmi bairavasundaram nitin agrawal haryadi gunawi andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison vijayan laksh nitina haryadi dusseau remzi wisc abstract commodity file systems trust disks work fail completely modern disks exhibit complex failure modes suggest fail-partial failure model disks incorporates realistic localized faults latent sector errors block corruption develop apply failure-policy fingerprinting framework investigate commodity file systems react range realistic disk failures classify failure policies taxonomy measures internal robustness iron includes failure detection recovery techniques show commodity file system failure policies inconsistent buggy generally inadequate ability recover partial disk failures finally design implement evaluate prototype iron file system linux ixt showing techniques in-disk checksumming replication parity greatly enhance file system robustness incurring minimal time space overheads categories subject descriptors operating systems file systems management operating systems reliability general terms design experimentation reliability keywords iron file systems disks storage latent sector errors block corruption fail-partial failure model fault tolerance reliability internal redundancy introduction disks fail commodity file systems expect years file system storage system designers assumed disks operate fail stop manner classic model disks working perfectly fail absolutely easily detectable manner fault model presented modern disk drives complex modern drives exhibit latent sector faults block set blocks inaccessible worse blocks silently corrupted finally disks exhibit transient performance problems permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee sosp october brighton united kingdom copyright acm reasons complex failures disks buggy disk controller issue misdirected write placing correct data disk wrong location interestingly failures exist today simply waiting disk technology improve remove errors errors worsen time due increasing drive complexity immense cost pressures storage industry escalated reliable ata disks desktop pcs large-scale clusters storage systems developers high-end systems realized nature disk faults built mechanisms systems handle redundant storage systems incorporate background disk scrubbing process proactively detect subsequently correct latent sector errors creating copy inaccessible blocks recent storage arrays incorporate extra levels redundancy lessen potential damage undiscovered latent errors similarly highly-reliable systems tandem nonstop utilize end-to-end checksums detect block corruption occurs technology filtered realm commodity file systems including linux file systems ext reiserfs ibm jfs windows file systems ntfs file systems pervasive home environment storing valuable non-archived user data photos home movies tax returns internet services google paper question pose modern commodity file systems react failures common modern disks answer query aggregate knowledge research literature industry field experience form model disk failure label model fail-partial failure model emphasize portions disk fail block errors data corruption model place develop apply automated failure-policy fingerprinting framework inject realistic disk faults beneath file system goal fingerprinting unearth failure policy system detects recovers disk failures approach leverages gray-box knowledge file system data structures meticulously exercise file system access paths disk characterize failure policy develop internal robustness iron taxonomy catalogs broad range detection recovery techniques output fingerprinting tool broad categorization iron techniques file system constituent data structures study focuses important substantially open-source file systems ext reiserfs ibm jfs closed-source file system windows ntfs platforms find great deal illogical inconsistency failure policy due diffusion failure handling code kernel inconsistency leads substantially detection recovery strategies similar fault scenarios resulting unpredictable undesirable fault-handling strategies discover systems implement portions failure policy incorrectly presence bugs implementations demonstrates difficulty complexity correctly handling classes disk failure observe tolerance transient failures file systems assume single temporarilyinaccessible block fatal whole-disk failure finally show file systems recover partial disk failures due lack in-disk redundancy behavior realistic disk failures leads question change file systems handle modern disk failures advocate single guiding principle design file systems don trust disk file system view disk utterly reliable component blocks corrupt file system apply measures detect recover corruption running single disk approach instance end-to-end argument top storage stack file system fundamentally responsible reliable management data metadata initial efforts develop family prototype iron file systems robust variants linux ext file system iron ext ixt investigate costs checksums detect data corruption replication provide redundancy metadata structures parity protection user data show techniques incur modest space time overheads greatly increasing robustness file system latent sector errors data corruption implementing detection recovery techniques iron taxonomy system implement well-defined failure policy subsequently provide vigorous protection broader range disk failures contributions paper define realistic failure model modern disks fail-partial model formalize techniques detect recover disk errors iron taxonomy develop fingerprinting framework determine failure policy file system analyze popular commodity file systems discover handle disk errors build prototype version iron file system ixt analyze robustness disk failure performance characteristics bring paper close discuss related work finally conclude disk failure reasons file system errors storage system section discuss common disk failure present realistic failpartial model disks discuss aspects model storage subsystem figure presents typical layered storage subsystem file system error occur layers propagate file system generic block device driver device controller firmware media transport host disk generic file system specific file system storage subsystem electrical mechanical cache figure storage stack present schematic entire storage stack top file system beneath layers storage subsystem gray shading implies software firmware white unshaded hardware bottom storage stack disk magnetic storage media mechanical motor arm assembly electrical components busses important component firmware code embedded drive control higher-level functions including caching disk scheduling error handling firmware code substantial complex modern seagate drive roughly lines code connecting drive host transport low-end systems transport medium bus scsi networks common higher-end systems fibrechannel top stack host hardware controller communicates device software device driver controls hardware block-level software forms layer providing generic device interface implementing optimizations request reordering software file system layer split pieces high-level component common file systems specific component maps generic operations data structures file system standard interface vnode vfs positioned disks fail motivate failure model describe errors layers storage stack failures media primary errors occur magnetic media classic problem bit rot occurs magnetism single bit bits flipped type problem detected corrected low-level ecc embedded drive physical damage occur media quintessential head crash culprit drive head contacts 
surface momentarily media scratch occur particle trapped drive head media dangers well-known drive manufacturers modern disks park drive head drive reduce number head crashes scsi disks include filters remove particles media errors lead permanent failure corruption individual disk blocks mechanical wear tear eventually leads failure moving parts drive motor spin irregularly fail completely erratic arm movements head crashes media flaws inaccurate arm movement misposition drive head writes leaving blocks inaccessible corrupted subsequent reads electrical power spike surge damage in-drive circuits lead drive failure electrical problems lead entire disk failure drive firmware interesting errors arise drive controller consists thousands lines real-time concurrent firmware disks return correct data circularly shifted byte memory leaks lead intermittent failures firmware problems lead poor drive performance firmware bugs well-enough field specific names misdirected writes writes place correct data disk wrong location phantom writes writes drive reports completed reach media phantom writes caused buggy misconfigured cache write-back caching enabled summary drive firmware errors lead sticky transient block corruption lead performance problems transport transport connecting drive host problematic study large disk farm reveals systems tested interconnect problems bus timeouts parity errors occurred frequency causing requests succeed slowly fail altogether transport transient errors entire drive bus controller main bus controller problematic eide controller series motherboards incorrectly completion disk request data reached main memory host leading data corruption similar problem controllers return status bits data floppy drive time hard drive observed ide protocol version problems yield corrupt data summary controller problems lead transient block failure data corruption low-level drivers recent research shown device driver code bugs rest operating system bugs crash operating system issue disk requests bad parameters data resulting data corruption fail-partial failure model discussion root failure ready put realistic model disk failure model failures manifest ways entire disk failure entire disk longer accessible permanent classic fail-stop failure block failure blocks accessible referred latent sector errors block corruption data individual blocks altered corruption insidious silent storage subsystem simply returns bad data read term model fail-partial failure model emphasize pieces storage subsystem fail discuss key elements fail-partial model including transience locality frequency failures discuss technology market trends impact disk failures time transience failures model failures sticky permanent transient temporary behavior manifests depends root problem low-level media problem portends failure subsequent requests contrast transport higher-level software issue block failure corruption operation succeed retried locality failures multiple blocks disk fail block failures dependent root block failure suggest forms block failure exhibit spatial locality scratched surface render number contiguous blocks inaccessible failures exhibit locality corruption due misdirected write impact single block frequency failures block failures corruptions occur commercial storage system developer succinctly stated disks break lot guarantees fiction frequently errors occur modeling reliability deciding failures important handle talagala patterson point disk drive manufacturers loathe provide information disk failures people industry refer implicit industry-wide agreement publicize details surprisingly actual frequency drive errors errors disk fail well-known literature previous work latent sector errors errors occur commonly absolute disk failure recent research estimates errors occur times absolute disk failures terms relative frequency block failures occur reads writes due internal error handling common disk drives failed writes sector remapped distant sector allowing drive transparently handle problems remapping imply writes fail failure component media stuttering transport lead unsuccessful write attempt move network-attached storage serves increase frequency class failures remapping succeed free blocks large scratch render blocks unwritable quickly reserved space reads problematic media unreadable drive choice return error trends areas processor performance technology market trends combine improve aspects computer systems contrast technology trends market forces combine make storage system failures occur frequently time reasons reliability greater challenge drives made increasingly dense bits packed smaller spaces drive logic complexity increases low-end drive market cost-per-byte dominates corners cut save pennies ide ata drives low-cost class drives tend tested internal machinery prevent failures occurring result field ata drives observably reliable cost pressures serve increase usage server environments finally amount software increasing storage systems noted software root errors storage system hundreds thousands lines software present lower-level drivers firmware low-level code generally type code difficult write debug source increased errors storage stack iron taxonomy section outline strategies developing iron file system file system detects recovers range modern disk failures main focus develop strategies disks common storage arrays single disk internal robustness iron needed protection file system cope failures modern disks iron file system includes machinery detect level partial faults recover level tables present iron detection recovery taxonomies note taxonomy means complete techniques exist raid variations proposed years detection recovery mechanisms employed file system define failure policy difficult discuss failure policy system iron taxonomy describe failure policy file system describe cache replacement file-layout policy levels detection level techniques file system detect problem occurred block accessed corrupted simplest detection strategy file system assumes disk works check return codes approach surprisingly common applied unintentionally errorcode pragmatic detection strategy file system implement check return codes provided lower levels storage system sanity sanity checks file system verifies data structures consistent check performed single block blocks checking single block file system verify individual fields pointers valid ranges verify type block file system superblocks include magic number older file systems pilot include header data block checking block correct type information file system guard forms block corruption checking blocks involve verifying blocks bitmap corresponds allocated blocks involve periodically scanning structures determine intact consistent similar fsck journaling file systems benefit periodic full-scan integrity checks buggy journaling file system unknowingly corrupt on-disk structures running fsck background detect recover problems redundancy final level detection taxonomy redundancy forms redundancy detect block corruption checksumming reliable systems years detect corruption recently applied improve security checksums number reasons assist detecting classic bit rot bits media flipped in-media ecc catches corrects errors checksums well-suited detecting corruption higher levels storage system stack buggy controller misdirects disk updates wrong location write block disk checksums carefully implemented detect problems specifically checksum level technique comment dzero detection assumes disk works derrorcode check return codes assumes lower level lower levels detect errors dsanity check data structures require extra consistency space block dredundancy redundancy detect corruption blocks end-to-end table levels iron detection taxonomy level technique 
comment rzero recovery assumes disk works rpropagate propagate error informs user rstop stop activity limit amount crash prevent writes damage rguess return guess wrong block contents failure hidden rretry retry read write handles failures transient rrepair repair data structs lose data rremap remaps block file assumes disk informs locale failures rredundancy block replication enables recovery forms loss corruption table levels iron recovery taxonomy stored data checksums detect misdirected phantom writes higher levels redundancy block mirroring parity error-correction codes detect corruption file system copies block reading comparing determine corrupted techniques designed correction discussed assume presence lower-overhead detection mechanism detection frequency detection techniques discussed applied lazily block access eagerly scanning disk idle time iron file systems form lazy detection additionally eager methods disk scrubbing classic eager technique raid systems scan disk discover latent sector errors disk scrubbing valuable means recovery replica exists repair nowunavailable block detect error occurred scrubbing typically leverages return codes explicitly provided disk discovers block failure corruption combined detection techniques checksums scrubbing discover block corruption levels recovery level iron taxonomy facilitates recovery block failure single disk drive techniques handle latent sector errors block corruptions simplest approach implement recovery strategy notifying clients failure occurred propagate straightforward recovery strategy propagate errors file system file system informs application error occurred assumes client program user respond appropriately problem stop recover disk failure stop current file system activity action levels granularity coarsest level crash entire machine positive feature recovery mechanism turns detected disk failures fail-stop failures preserves file system integrity crashing assumes problem transient faulty block repeatedly-accessed data script run initialization system repeatedly reboot attempt access unavailable data crash intermediate level kill process triggered disk fault subsequently mount file system read-only mode approach advantageous entire system processes continue finest level journaling file system abort current transaction approach lead system complex implement guess recently suggested rinard reaction failed block read manufacture response allowing system running spite failure negative artificial response desirable failing retry simple response failure retry failed operation retry appropriately handle transient errors wastes time retrying failure permanent repair iron file system detect inconsistency internal data structures repair fsck block pointed marked allocated bitmap freed discussed techniques context journaling file systems bugs lead corruption file system integrity remap iron file systems perform block remapping technique fix errors occur writing block recover failed reads specifically write block fails file system choose simply write block location sophisticated strategies remap entire semantic unit time user file preserving logical contiguity redundancy finally redundancy forms recover block loss simplest form replication block copies locations disk redundancy approach employs parity facilitate error correction similar raid adding parity block block group file system tolerate unavailability corruption block group complex encodings tornado codes subject worthy future exploration redundancy disk negative consequences replicas account spatial locality failure surface scratch corrupts sequence neighboring blocks copies allocated remote parts disk lower performance in-disk redundancy techniques incur high space cost desktop settings drives sufficient free space iron file system natural question file system implement detection recovery disk modern disks internal mechanisms detecting recovering errors sufficient view primary reason detection recovery file system found end-to-end argument lower-levels system implement forms fault tolerance file system implement guard forms failure file system place detect corruption data higher levels storage stack device driver drive controller reason implementing detection recovery file system file system exact knowledge blocks file system apply detection recovery intelligently block types file system provide higher level replication metadata leaving failure detection correction user data applications specific solution explore similarly file system provide machinery enable application-controlled replication important data enabling explicit performance reliability trade-off reason performance file systems storage systems unwritten contract file system lay blocks achieve high bandwidth unwritten contract stipulates adjacent blocks logical disk address space physically proximate disk-level recovery mechanisms remapping break unwritten contract performance problems file system assumes responsibility remap logically-related blocks file avoid problems complexities placing iron functionality file system techniques require persistent data structures track redundant copies parity blocks located mechanisms require control underlying drive mechanisms recover on-disk data modern drives attempt positioning reading strategies interface exists control low-level strategies current systems doesn raid make storage reliable question answered simply raid techniques provide reliable robust storage raid improve storage reliability complete solution reasons systems incorporate disk sine qua redundant storage systems desktop pcs ship single disk included cost driving force marketplace adding disk solely sake redundancy palatable solution raid protect failures higher storage stack shown figure layers exist storage subsystem file system errors occur layers file system ultimately responsible detecting recovering errors ironically complex raid controller consist millions lines code source faults depending raid system employed types disk faults handled lower-end raid controller cards checksums detect data corruption recently companies included machinery cope latent sector errors iron techniques file system single-disk systems multiple drives raid-like manner focus single-disk systems paper rich space left exploration iron file systems redundant storage arrays failure policy fingerprinting describe methodology uncover failure policy file systems main objective failure-policy fingerprinting determine detection recovery techniques file system assumptions makes underlying storage system fail comparing failure policies file systems learn file systems robust disk failures suggest improvements analysis helpful inferring iron techniques implemented effectively approach inject faults beneath file system observe file system reacts fault policy consistent file system simply run workload fail blocks accessed conclude reaction block failure fully demonstrates failure policy system file systems practice complex employ techniques depending operation performed type faulty block extract failure policy system trigger interesting cases challenge coerce file system code paths observe path handles failure requires run workloads exercising relevant code paths combination induced faults file system data structures describe create workloads inject faults deduce failure policy applied workload goal applying workloads exercise file system claim stress code path leaving avenue future work strive execute interesting internal cases workload suite sets programs run unix-based file systems fingerprinting ntfs requires set similar programs set programs called singlets focus single call file system api mkdir set generics stresses functionality common api path traversal table summarizes test suite file system test introduces special cases stressed ext inode imbalanced tree indirect doubly-indirect triply-indirect pointers support large 
files workloads ensure sufficiently large files created access structures file systems similar peculiarities make exercise -tree balancing code reiserfs type-aware fault injection step inject faults emulate disk adhering fail-partial failure model standard fault injectors fail disk blocks type oblivious manner block failed file system repeatedly injecting faults random blocks waiting uncover aspects failure policy laborious time-consuming process yielding insight key idea test file system efficient manner type-aware fault injection builds previous work semantically-smart disk systems type-aware fault injection failing blocks obliviously fail blocks specific type inode type information crucial reverse-engineering failure policy allowing discern strategies file system applies data structures disadvantage type-aware approach fault injector tailored file system tested requires solid understanding workload purpose singlets access chdir chroot stat statfs lstat open utimes read readlink exercise getdirentries creat posix api link mkdir rename chown symlink write truncate rmdir unlink mount chmod fsync sync umount generics path traversal traverse hierarchy recovery invoke recovery log writes update journal table workloads table presents workloads applied file systems test set workloads stresses single system call group invokes general operations span calls path traversal on-disk structures benefits typeawareness outweigh complexities block types file systems test listed table mechanism injecting faults software layer directly beneath file system pseudo-device driver layer injects block failures reads writes block corruption reads emulate block failure simply return error code issue operation underlying disk emulate corruption change bits block returning data cases inject random noise cases block similar expected corrupted fields software layer models transient sticky faults injecting failures file system emulate faults caused layers storage subsystem unlike approaches emulate faulty disks additional hardware imitate faults introduced buggy device drivers controllers drawback approach discern lower layers handle disk faults scsi drivers retry commands failure characterizing file systems react faults correct layer fault injection failure policy inference running workload injecting fault final step determine file system behaved determine fault affected file system compare results running fault perform comparison observable outputs system errors codes data returned file system api contents system log low-level traces recorded fault-injection layer human-intensive part process requires manual inspection visible outputs summary developed three-step fingerprinting methodology determine file system failure policy approach strikes good balance straightforward run exercises file system test workload suite roughly programs file system order block types block failed read write data corrupted file system amounts roughly relevant tests ext structures purpose inode info files directories directory list files directory data bitmap tracks data blocks group inode bitmap tracks inodes group indirect large files exist data holds user data super info file system group descriptor holds info block group journal super describes journal journal revoke tracks blocks replayed journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled reiserfs structures purpose leaf node items kinds stat item info files directories directory item list files directory direct item holds small files tail file indirect item large files exist data bitmap tracks data blocks data holds user data super info tree file system journal header describes journal journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled root internal node tree traversal jfs structures purpose inode info files directories directory list files directory block alloc map tracks data blocks group inode alloc map tracks inodes group internal large files exist data holds user data super info file system journal super describes journal journal data records transactions aggregate inode info disk partition bmap descriptor describes block allocation map imap control summary info imaps ntfs structures purpose mft record info files directories directory list files directory volume bitmap tracks free logical clusters mft bitmap tracks unused mft records logfile transaction log file data holds user data boot file info ntfs volume table file system data structures table presents data structures interest file systems test ext reiserfs jfs ntfs table list names major structures purpose note knowledge ntfs data structures incomplete closed-source system failure policy results present results failure policy analysis commodity file systems ext reiserfs version ibm jfs linux ntfs windows file system present basic background information discuss general failure policy uncovered bugs illogical inconsistencies source code explain problems discover due sheer volume experimental data difficult present results reader inspection file system studied depth present graphical depiction results showing workload blocktype pair detection recovery technique figure presents complex graphical depiction results caption interpretation details provide qualitative summary results presented figure linux ext linux ext similar classic unix file systems berkeley fast file system ext divides disk set block groups statically-reserved spaces bitmaps inodes data blocks major addition ext ext journaling ext includes set ondisk structures manage write-ahead log detection detect read failures ext primarily error codes derrorcode write fails ext record error code dzero write errors potentially leading file system problems checkpointing transaction final location ext performs fair amount sanity checking dsanity ext explicitly performs type checks blocks superblock journal blocks type checking important blocks directories bitmap blocks indirect blocks ext performs numerous sanity checks file-size field inode overly-large open detects reports error recovery detected errors ext propagates error user rpropagate read failures ext aborts journal rstop aborting journal leads readonly remount file system preventing future updates explicit administrator interaction ext retry rretry sparingly prefetch read fails ext retries originally requested block bugs inconsistencies found number bugs inconsistencies ext failure policy errors propagated user truncate rmdir fail silently important cases ext immediately abort journal failure implement rstop journal write fails ext writes rest transaction including commit block journal journal recovery file system easily corrupted ext perform sanity checking unlinkdoes check thelinkscount field modifying corrupted lead system crash finally ext redundant copies superblock rredundancy copies updated file system creation reiserfs reiserfs comprised vastly data structures ext virtually metadata data balanced tree similar database index key advantage tree structuring scalability allowing files coexist directory read failure write failure corruption ext detection j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ext recovery j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode reiserfs detection internalroot j-dataj-commit j-descj-header superdata indirectbitmap dir itemstat item reiserfs recovery internalroot j-dataj-commit j-descj-header superdata indirectbitmap dir itemstat item jfs detection imap-cntlbmap-desc aggr-inodej-data j-supersuper datainternal imapbmap 
dirinode jfs recovery imap-cntlbmap-desc aggr-inodej-data j-supersuper datainternal imapbmap dirinode figure file system failure policies tables detection recovery policies ext reiserfs jfs read write corruption faults injected block type range workloads workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery log write operations gray box workload applicable block type multiple mechanisms observed symbols superimposed key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate rredundancy rstop detection analysis reveals reiserfs pays close attention error codes reads writes derrorcode reiserfs performs great deal internal sanity checking dsanity internal leaf nodes balanced tree block header information level block tree number items free space super block journal metadata blocks magic numbers identify valid journal descriptor commit blocks additional information finally inodes directory blocks formats reiserfs checks blocks expected values pipeline batch sharing grid workloads douglas thain john bent andrea arpaci-dusseau remzi arpaci-dusseau miron livny computer sciences department wisconsin madison abstract present study batch-pipelined scientific workloads candidates execution computational grids studies focus behavior single applications study characterizes workloads composed pipelines sequential processes file storage communication share significant data batch study includes measurements memory cpu requirements individual components analyses sharing complete batches conclude discussion ramifications workloads end-to-end scalability system design introduction years researchers understood importance studying workload characteristics order evaluate impact current future systems architecture previous application studies focused detailed behavior single applications sequential parallel caching behavior spec workloads long topic intense fields scrutiny communication blocks characteristics checked parallel carefully applications similarly bitmaps data blocks type information type-checked recovery prominent aspect recovery policy reiserfs tendency panic system detection virtually write failure rstop reiserfs calls panic file system crashes leading reboot recovery sequence reiserfs attempts ensure ondisk structures corrupted reiserfs recovers read write failures differently read failures reiserfs propagates error user rpropagate performs single retry rretry data block read fails indirect block read fails unlink truncate write operations reiserfs retries write failure bugs inconsistencies reiserfs exhibits inconsistencies bugs ordered data block write fails reiserfs journals commits transaction handling error rzero expected rstop lead corrupted data blocks metadata blocks point invalid data contents dealing indirect blocks reiserfs detects ignores read failure atruncate unlink updates bitmaps super block incorrectly leaking space reiserfs calls panic failing sanity check simply returning error code finally sanity type checking detect corrupt journal data replaying corrupted journal block make file system unusable block written super block ibm jfs jfs modern techniques manage data block allocation journaling scalable tree structures manage files directories block allocation unlike ext reiserfs jfs record-level journaling reduce journal traffic detection error codes derrorcode detect read failures ext write errors dzero exception journal superblock writes jfs employs minimal type checking superblock journal superblock magic version numbers checked sanity checks dsanity block types internal tree blocks directory blocks inode blocks number entries pointers block jfs checks make number maximum block type equality check field performed block allocation maps verify block corrupted recovery recovery strategies jfs vary dramatically depending block type error occurs journal superblock write jfs crashes system rstop write errors rzero block read failure primary superblock jfs accesses alternate copy rredundancy complete mount operation corrupt primary results mount failure rstop explicit crashes rstop block allocation map inode allocation map read fails error codes metadata reads handled generic file system code called jfs generic code attempts recover read errors retrying read single time rretry finally reaction failed sanity check propagate error rpropagate remount file system read-only rstop journal replay sanity-check failure replay abort rstop bugs inconsistencies found problems jfs failure policy jfs built-in redundancy expect jfs secondary copies aggregate inode tables special inodes describe file system error code returned aggregate inode read blank page returned user rguess design bug occurs read internal tree block pass sanity check bugs limit utility jfs recovery generic code detects read errors retries bug jfs implementation leads ignoring error corrupting file system windows ntfs ntfs non-unix file system study analysis requires detailed knowledge on-disk structures complete analysis figure find ntfs error codes derrorcode detect block read write failures similar ext jfs data write fails ntfs records error code dzero corrupt file system ntfs performs strong sanity checking dsanity metadata blocks file system unmountable metadata blocks journal corrupted ntfs surprisingly perform sanity checking corrupted block pointer point important system structures corrupt block pointed updated cases ntfs propagates errors rpropagate ntfs aggressively retry rretry operations fail times read failures writes number retries varies times data blocks times mft blocks file system summary present qualitative summary file systems tested table presents summary techniques file system employs excluding ntfs ext simplicity ext implements simple reliable failure policy matching design philosophy found ext family file systems checks error codes modest level sanity checking recovers propagating errors aborting operations main problem ext failure handling write errors problems including file system corruption reiserfs harm reiserfs concerned disk failure concern evident write failures induce panic reiserfs takes action ensure file system corrupted reiserfs great deal sanity type checking behaviors combine form hippocratic failure policy harm jfs kitchen sink jfs consistent diverse failure detection recovery documented applications isolation production settings computational science desired end-result product group applications run hundreds thousands times varied inputs applications executed high throughput computing system condor managed high-level workflow software chimera refer workloads batch-pipelined illustrated figure batch-pipelined workload composed independent pipelines pipeline sequential processes communicate preceding succeeding processes private data files shared input files pipelines process pipeline sharing pipeline sharing batch batch width figure batch-pipelined workload stages figure suggests workload generally submitted large batches pipelines incidentally synchronized beginning pipeline logically distinct correctly execute faster slower siblings key difference studying behavior single application batch-pipelined workload sharing behavior batch-pipelined workload understood instances application run executable potentially input files realistically capture full diversity production workloads study behavior entire pipeline account effects sharing paper present study production scientific workloads collected application pipelines diverse fields computational science including astronomy biology geology physics applications representative broad class important workloads present basic characterization computational memory demands workloads find individually single pipeline place tremendous load system resources combination loads overwhelming focus behavior workloads primary source sharing characterize sharing occurs workloads breaking activity categories endpoint represents input final output pipeline-shared shared write-then-read fashion single pipeline batch-shared comprised input shared pipelines characterization show shared dominant component traffic importantly study implications systems design find wide-area network bandwidth scalability problem applications attempts made eliminate shared successful systems workloads segregate types traffic order scale successfully submit pipeline-shared data significant problem batch-shared data elucidate traditional file systems workloads rest paper organized section describe general characteristics batchpipelined workloads specific application pipelines section describe experimental method sections analyze data discuss implications discuss related work section conclude section applications applications characterize chosen range scientific disciplines selection criteria applications attacking major scientific objective composed sequential applications require scalable computing environment accomplish high throughput focus applications measurements include seti home point guidance users chose workloads input parameters correspond production descriptions applications found figure applications variable granularity cms amanda process variable number small independently generated events applications chose pipeline sizes events cms showers amanda typical production cases cpu resources consumed pipeline scale linearly number events ibis multiple datasets differing resolutions granularity resolution reflects size dataset experiments medium sized dataset ibis nautilus perform single simulations variable length seti blast operate work unit fixed size applications observed characteristic behaviors diamond-shaped storage profile small initial inputs generally created humans initialization tools expanded early stages large intermediate results intermediates reduced stages small results interpreted humans incorporated database intermediate data serves checkpoint cached values ephemeral nature multi-level working sets users easily identify large logical collections data needed application calibration tables physical constants execution applications tend select small working set users aware significant consequences data replication caching techniques significant data sharing application large configuration space users submit large numbers similar jobs access similar working sets analysis condor logs shows usual batch size thousand amanda cms blast property exploited efficient widearea distribution modest communication links method application capture cpu memory behavior cpu memory behavior tracked hardware counters statistics instrument behavior make shared-library interposition agent replaces routines standard library explicit event requested application library records event marking start end operation instruction count details request technique applied application dynamically linked care avoid additional overheads due tracing access memory-mapped files traced userlevel paging technique posix mprotect feature access memory-mapped regions generates user-level page fault sigsegv handled traced shared library application blast memory-mapped analysis page faults considered equivalent explicit read operations page size non-sequential access memorymapped pages recorded explicit seek operation workload analysis overview resources consumed application figure applications wide variance run times current hardware ranging minute blast day ibis considered individually applications spend majority time consuming cpu memory requirements program sizes modest comparison total volume matches search string database mbmips blastp blast inputs climate data climate forecast analyze mips ibis triggered events raw events geometry configuration configuration cmkin mips mips cmsim cms problem solution initial state integral mips scf mips argos mips setup initial state intermediate states initial state coordinate files visualization physics mips nautilus bin coord mips mips rasmol nautilus inputs ice tables raw events standard events noisy events triggered events geometry physics corsika corama amasim kbmips mips mips mmc mips amanda figure application schematics schematics summarize structure application pipeline circles individual processes labeled instruction counts rounded boxes data private pipeline double boxes data shared pipelines batch arrows data flow blast searches genomic databases matching proteins nucleotides queries archived data include errors gaps acceptable match similarity parameterized exhaustive search single executable blastp reads query sequence searches shared database outputs matches ibis global-scale simulation earth systems ibis simulates effects human activity global environment global warming ibis performs simulation emits series snapshots global state cms high-energy physics experiment begin operation cms testing software two-stage pipeline stage cmkin random seed generates models behavior particles accelerated ring output set events fed cmsim simulates response detector final output represents events exceed triggering threshold detector messkit hartree-fock simulation non-relativistic interactions atomic nuclei electrons allowing computation properties bond strengths reaction energies distinct executables comprise calculation setup initializes data files input parameters argos computes writes integrals atomic configuration scf iteratively solves self-consistent field equations nautilus simulation molecular dynamics input configuration describes molecules threedimensional space newton equation solved particle incremental snapshots periodically capture particle coordinates final snapshot passed back program initial configuration simulation eventually snapshots converted standard format bin coord consolidated images rasmol amanda astrophysics experiment designed observe cosmic events gamma-ray bursts collecting resulting neutrinos interaction earth mass stage calibration software corsika simulates production neutrinos primary interaction creates showers muons corama translates 
output standard high-energy physics format mmc propagates muons earth ice introducing noise atmospheric sources finally amasim simulates response detector incident muons real millions instructions memory traffic application time integer float burst text data share ops seti seti blast blastp ibis ibis cms cmkin cmsim total setup argos scf total nautilus nautilus bin coord rasmol total amanda corsika corama mmc amasim total figure resources consumed shown total amounts resources consumed subsequent tables shading differentiate application pipelines real time refers total wall-clock time applications run instrumentation overhead burst average number instructions executed operations instruction counts obtained performance monitoring counters pmcs -class processors notice exception application pipelines modest bandwidth requirements total reads writes application files traffic unique static files traffic unique static files traffic unique static seti seti blast techniques detection jfs sanity checks error codes recovery jfs redundancy crashes system retries operations depending block type fails error detection api called level ext reiser jfs dzero derrorcode dsanity dredundancy rzero rpropagate rstop rguess rretry rrepair rremap rredundancy table iron techniques summary table depicts summary iron techniques file systems test check marks higher relative frequency usage technique ntfs persistence virtue compared linux file systems ntfs persistent retrying failed requests times giving propagate errors user reliably testing ntfs needed order broaden conclusions part ongoing work technique summary finally present broad analysis techniques applied file systems detect recover disk failures concentrate techniques underused overused inappropriate manner detection recovery illogical inconsistency common found high degree illogical inconsistency failure policy file systems observable patterns figure reiserfs performs great deal sanity checking important case journal replay result single corrupted block journal corrupt entire file system jfs illogically inconsistent employing techniques scenarios similar note inconsistency problematic logically inconsistent good idea file system provide higher level redundancy data structures deems important root directory criticizing inconsistencies undesirable unintentional jfs attempt read alternate superblock read failure occurs reading primary superblock attempt read alternate deems primary corrupted estimation root illogical inconsistency failure policy diffusion code implements failure policy spread kernel diffusion encouraged architectural features modern file systems split generic specific file systems observed cases developers implement portions code implement failure policies cases reiserfs panic write failure arises due inconsistency indicative lack attention paid failure policy detection recovery bugs common found numerous bugs file systems tested found sophisticated techniques generally indicative difficulty implementing correct failure policy hints effort put testing debugging code suggestion literature helpful periodically inject faults 
normal operation part fire drill method reveals testing broad cover code paths testing indirect-block handling reiserfs observe classes fault mishandling detection error codes amazingly error codes file system common jfs found occasionally file systems testing framework part file system developer toolkit tools class error easily discovered detection sanity checking limited utility file systems sanity checking ensure metadata meets expectations code modern disk failure modes misdirected phantom writes lead cases file system receive properly formatted incorrect block bad block passes sanity checks corrupt file system file systems tested exhibit behavior stronger tests checksums recovery stop correctly file systems employed form rstop order limit damage file system types errors arose reiserfs calls panic virtually write error prevent corruption structures careful techniques write failure ext abort transaction correctly squelch writes file system leading corruption fine-grained rebooting difficult apply practice recovery stop overused blastp ibis ibis cms cmkin cmsim total setup argos scf total nautilus nautilus bin coord rasmol total amanda corsika corama mmc amasim total figure volume shown total amounts performed traffic number bytes flow process unique considers unique byte ranges total traffic notice cms perform large proportions reread traffic indicating caching important static refers total size files accessed unique applications read portions files notice blast reads total data files accesses suggests systems prestage data sets performing unnecessary work appl open dup close read write seek stat seti blastp ibis cmkin cmsim total setup argos scf total nautilus bin coord rasmol total corsika corama mmc amasim total figure instruction mix shown total number type instructions executed applications seek column includes non-sequential access memory-mapped pages ignores lseek operations change file offset column sums number generally uncommon operations ioctl access high numbers column reflect fact bin coord rasmol driven shell scripts perform readdir operations notice applications high degrees random access shown ratio seeks reads writes contradicts previous file system studies dominance sequential endpoint pipeline batch appl files traffic unique static files traffic unique static files traffic unique static seti blastp ibis cmkin cmsim total setup argos scf total nautilus bin coord rasmol total corsika corama mmc amasim total figure roles shown total amounts type performed endpoint traffic consists initial inputs final outputs unique application pipeline traffic intermediate data passed pipeline stages intermediate data passed phases single stage batch traffic input data shared instances pipeline traffic number bytes flow process unique considers unique byte ranges total traffic static refers total size files accessed unique applications read portions files notice applications exception ibis endpoint traffic relative total traffic scalability systems run applications downside halting file system activity reaction failure inconvenience recovery takes time requires administrative involvement fix file systems form rstop innocuous read failure occurred simply returning error requesting process entire system stops draconian reactions possibly temporary failures avoided recovery retry underutilized file systems assume failures transient lower layers system handle failures retry requests time systems employ retry generally assume read retry write retry transient faults due device drivers transport issues equally occur reads writes retry applied uniformly ntfs lone file system embraces retry issue higher number requests block failure observed recovery automatic repair rare automatic repair rarely file systems rstop technique file systems require depend ability differentiate types hit rate blast ibis cms hit rate cache size cache size nautilus cache size amanda figure batch cache simulation hit rate blast ibis cms hit rate cache size cache size nautilus cache size amanda figure pipeline cache simulation figure details volume produced pipeline stage applications conceived pipeline multiple stages connected simple data streams makes complex read write file system number files accesses seti cms lesser degree blast read input data multiples times overwriting output data found pipelines exception amanda output over-writing update application-level checkpoints place alarmed observe checkpoints unsafely written directly existing data written file atomically replaced renaming pipelines distributed large collections data runs typical run accesses small portion common similar runs static size blast dataset exceeds unique amount read application distribution operations figure notice applications high degree random access shown ratio seeks reads writes results nature data files accessed programs generally complex self-referencing internal structure contradicts file system studies dominance sequential characterize types sharing batchpipelined workloads divided traffic roles endpoint traffic consists initial inputs final outputs unique pipeline read written central site system design pipeline traffic consists intermediate data passed pipeline stages intermediate data passed phases single stage batch traffic input data identical pipelines understanding application identified file accessed endpoint pipeline batch computed traffic performed category shown figure immediately comparatively traffic needed endpoints bulk pipeline batch depending application examining figures note large number opens issued relative number files accessed typically designed standalone workstations applications optimized realities distributed computing opening file access times expensive issuing read write figures show working set sizes batchshared pipeline-shared data workload values computed simulations performed trace data batch width varying lru cache size blocks executable files implicitly included batch-shared data general types sharing cache sizes small respect volume sizes typical main memories today outliers amanda large amount batch shared data half read cache effective large sizes amanda high pipeline hit rate small cache sizes due large number singlebyte requests due high degree re-reading output overwriting cms small cache sizes effectively maximize hit rates blast pipeline data ibis stage pipeline data form checkpoints written read multiple times figure shows applications relate amdahl long standing system balance ratios recently amended gray workloads cpu-io ratios measured mips mbps exceeding amdahl ideal indicating reliance computation ratio memory cpu speed alpha amdahl exception component amanda close gray reliance computation memory finally ratio cpu instructions instructions orders magnitude larger respect single instance pipeline commodity computing node engineered amdahl metrics manual intervention attempt fix observed problem running fsck detection recovery redundancy considerably overprovisioned bandwidth memory capacity cpu mem cpu cpu appl mips mbps mips instr seti blastp ibis cmkin cmsim total setup argos scf total nautilus bin coord rasmol total corsika corama mmc amasim total amdahl gray figure amdahl ratios system implications workloads potentially infinite problem domains ability harness computing power enables higher resolution parameters lower statistical uncertainties current users applications scale throughput running hundreds thousands simultaneously scale applications considered cpu-bound bound considered aggregate give idea growing envelope current scientific computing spring cms pipeline simulate million events divided pipelined jobs consuming cpu-years producing terabyte output batch small fraction attempted test run full production begins successive yearly workloads planned grow code data published authoritative form experiment central site likewise simulation outputs eventually moved back archival storage section explore general properties computing storage systems built satisfy workloads explore detailed algorithms data management provisioning resources endpoint scalability capacity individual computing nodes ultimate scalability workloads limited competition shared resources assume workload relies central site authenticity archival input output data demonstrated actual endpoint traffic small fraction total applications eliminate non-endpoint traffic endpoint server techniques caching replication significant gains scalability traffic elimination carried carefully pipeline-shared traffic eliminated end user intermediate data return debugging 
archival ability reproduce questionable batchshared eliminated constraints maintaining consistency authenticity potentially changing input data traffic elimination blindly consideration data computing system limits system executing workloads based ability eliminate shared traffic figure shows selected applications scale systems eliminating category traffic assume presence buffering structure sufficient completely overlap cpu figures assume mips cpu show cpu time horizontal lines show milestones bandwidth lower represents capable commodity hard disk upper represents aggressive storage server network leftmost graph shows scalability system carries traffic endpoint server discipline high end storage device needed systems modest size overwhelmed applications ibis seti scale batch-shared traffic eliminated make significant improvements cms nautilus shown graph hand pipelineshared traffic eliminated observe significant gains seti nautilus shown endpoint performed reach limit shown rightmost graph applications shown scale workers modest storage high-end storage seti potentially scale million cpus indicator specialized design wide-area deployment valuable limits workload scalability cpu hardware improve performance time limits space prevent detailed discussion found technical report rate batch width batch width batch width storage center commodity disk rate batch width blast cms amanda nautilus ibis setiathome data endpoint pipeline endpoint batch endpoint figure scalability roles graphs show scalability applications improved orders magnitude batch-shared pipeline-shared performed endpoint server horizontal lines show milestones bandwidth upper represents high-end storage center lower represents current commodity disk software architecture order scale large sizes software architectures workloads strive eliminate batch-shared pipeline-shared data endpoint interactions constraints security persistence performance traditional file systems serve applications naming consistency requirements targeted interactive cooperating users applications require data management system specialized requirements workload analysis failure recovery resource management issue batch input sharing received significant attention grid computing community deployed systems srb gdmp manage widely-distributed well-known batch shared data techniques discovering replicating batchshared data proposed diminishing importance batch sharing submit issue pipeline sharing problem neglected figure shows localization types achieve high scalability treatment pipelineshared data necessarily batch shared data writer reader discarded pipeline-shared outputs require facility discovery reader data advertised degree batch-shared data loss pipeline-shared output require re-execution previous computation stage solutions pipeline batch sharing problems require application classified roles degree accuracy custom applications seti succeeded wide scalability virtue manual division endpoint explicit network communication expect valuable applications re-written distributed environment ideally roles detected automatically approach trec system deduces program dependencies behavior user provide hints roles system modifying applications directly number file systems account conventional wisdom quickly-deleted data significant source traffic general-purpose workload recognition limited application due requirements reliability consistency interactive systems nfs permits delay application writes data movement server delay made minutes hours order accommodate pipeline sharing reduction finally importantly virtually file systems include machinery detect disk failures apply redundancy enable recovery failures lone exception minimal amount superblock redundancy found jfs redundancy inconsistently jfs places copies close proximity making vulnerable spatiallylocal errors explored potentially handling failures common drives today investigate inclusion forms redundancy failure policy file system read failure write failure corruption ixt detection j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ixt recovery j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure ixt failure policy tables plot detection recovery policies ixt read write corruption faults injected block type range workloads workloads varied columns figure block types ixt file system varied rows workloads grouped manner figure key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate dredundancy rredundancy rstop iron file system describe implementation evaluation iron ext ixt ixt implement family recovery techniques commodity file systems provide increase robustness ixt applies checksums metadata data blocks pure replication metadata employs parity-based redundancy protect user data section describe implementation demonstrate robust broad class partial disk failures investigate time space costs ixt showing time costs small modest space costs reasonable performance measurements activate deactivate iron features independently understand cost approach implementation briefly describe ixt implementation explain add checksumming metadata replication user parity performance enhancement transactional checksums existing ext file system framework checksumming implement checksumming ixt borrow techniques recent research checksumming file systems specifically place checksums journal checkpoint checksums final location distant blocks checksum checksums small cached read verification current implementation shato compute checksums incorporating checksumming existing transactional machinery ixt cleanly integrates ext framework metadata replication apply similar approach adding metadata replication ixt metadata blocks written separate replica log checkpointed fixed location block group distant original metadata transactions ensure copies reach disk consistently parity implement simple parity-based redundancy scheme data blocks parity block allocated file simple design enables recover data-block failure file modify inode structure ext associate file parity block data blocks parity blocks allocated files created file modified parity block read updated respect contents improve performance file creates preallocate parity blocks assign files created transactional checksums explore idea leveraging checksums journaling file system specifically checksums relax ordering constraints improve performance updating journal standard ext ensures previous journal data reaches disk commit block enforce ordering standard ext induces extra wait writing commit block incurs extra rotational delay avoid wait ixt implements call transactional checksum checksum contents transaction placing checksum journal commit block ixt safely issue blocks transaction concurrently crash occurs commit recovery procedure reliably detect crash replay transaction checksum journal data match checksum commit block note transactional checksum crash semantics original ext iron extensions cleaning overheads note cleaning overhead large problem pure log-structured file systems major performance issue journaling file systems ixt -style checksumming replication journaling file systems incorporate cleaning on-line maintenance costs ext writes metadata journal cleans journal checkpointing data final fixed location additional cleaning performed ixt increases total traffic small amount evaluation evaluate prototype implementation ixt focus major axes assessment robustness ixt modern disk failures time space overhead additional redundancy mechanisms employed ixt robustness test robustness ixt harness fault injection framework running partial-failure experiments ixt results shown figure ixt detects read failures ext error codes lower level derrorcode metadata block read fails ixt reads replica copy rredundancy replica read fails behaves ext propagating error rpropagate stopping file system activity rstop data block read fails parity block data blocks file read compute failed data block contents rredundancy ixt detects write failures error codes derrorcode aborts journal mounts file system read-only stop writes disk rstop data metadata block read checksum contents computed compared checksum block dredundancy checksums match read error generated rpropagate read errors contents failed block read replica computed parity block rredundancy process building ixt fixed numerous bugs ext avoided cases ext commit failed transactions disk potentially corrupt file system employing checksumming detect corruption replication parity recover lost blocks ixt robust file service spite partial disk failures quantitatively ixt detects recovers partial-error scenarios induced result logical well-defined failure policy time overhead assess performance overhead ixt isolate overhead iron mechanism enabling checksumming metadata data metadata replication parity user data transactional checksumming separately combinations standard file system benchmarks ssh-build unpacks compiles ssh source distribution web server benchmark responds set static http requests postmark emulates file system traffic server tpc-b runs series debit-credit transactions simple database run experiment times present average results benchmarks exhibit broad set behaviors specifically ssh-build good albeit simple model typical action developer administrator web server read intensive concurrency postmark metadata intensive file creations deletions tpc-b induces great deal synchronous update traffic file system table reports relative performance variants ixt 
workloads compared stock linux ext numbers draw main conclusions ssh-build web server workload time overhead iron techniques enabled ssh-build indicative typical activity checksumming replication parity incurs cost similarly web server unnecessary benchmark writes conclude accompanied read-intensive workloads increased suffer danger data addition loss iron crash techniques unusual ssh web consistency post semantics tpcb session baseline semantics afs ext worse closing file blocking operation forces write-back dirty data vertically shared data written back numerous close operations cpu held idle pipelines offering possibility cpu-i overlap general-purpose file systems operate assumption data eventually flow back archival site workloads require opposite assumption created data remain created explicit operation writer system user forces archival storage improves overlap eliminates unnecessary writes increases danger operations waiting written back fail due permissions disconnection sources error distributed file system acceptable batch system long failed detected matched process issued force re-execution job table overheads suggest ixt file problem system variants attacked results coupling running workflow variants manager ixt condor ssh-build dagman ssh web globus server web chimera postmark tracks post dependencies tpc-b tpcb general benchmarks graphs jobs presented ssh-build time systems measures activity time presumed unpack configure reliable build centralized ssh side source effect tree tar execution source creation size positioning web server pipeline-shared benchmark data transfers integrated data workflow http data requests postmark efficiently run shared transactions maintaining possibility error recovery related work cpu memory communication characteristics applications studied years research community roughly categorized type workloads generalpurpose workloads applications sequential applications examined isolation parallel applications isolation summarize work categories focusing examined file system activity file system activity examined range general-purpose workloads studies greatly influenced file system design years focused academic research workloads studies found files short lifetimes access patterns exhibit high degree locality read-write sharing rare missing broad studies traffic linkage applications generate traffic similar work studies focused behavior individual applications commercial workloads domain interaction pipeline behavior sequential applications examined interesting study detailed memory-system behavior applications opportunities sharing fundamentally studies parallel applications ways similar pipelined batch applications cpu memory communication behavior parallel vector applications quantified number studies impact explicit study complements works studying sharing behavior important class workload studies demonstrate file sizes ranging subdirectories files tpc-b run randomly generated debitcredit transactions rows vary redundancy technique implemented combinations implies metadata checksumming enabled data checksumming enabled replication metadata turned parity data blocks enabled transactional checksums results normalized performance standard linux ext interested reader running times standard ext ssh-build web postmark tpc-b seconds slowdowns greater marked bold speedups relative base ext marked brackets testing linux kernel ghz intel memory western digital wdc bbdaa disk metadata intensive workloads postmark tpc-b overhead noticeable postmark tpc-b row workloads metadata intensive results represent worst-case performance expect observe implementation metadata replication row incurs substantial cost data checksumming row user parity metadata checksums contrast incur cost rows untuned implementation ixt results demonstrate worst case costs robustness prohibitive finally performance synchronous tpc-b workload demonstrates benefits transactional checksum base case technique improves standard ext performance row combination parity checksumming replication parity reduces overhead roughly row row additional robustness checksums applied improve performance journaling file systems space overhead evaluate space overhead measured number local file systems computed increase space required metadata replicated room checksums included extra block parity allocated found space overhead checksumming metadata replication small range found parityblock overhead user files bit substantial range depending volume analyzed summary investigated family redundancy techniques found ixt greatly increases robustness file system partial failures incurring modest time space overheads work left designs implementation techniques explored understand benefits costs iron approach related work effort builds related work bodies literature file system analysis related efforts inject faults test robustness systems failure prototype iron file system draws recent efforts building software robust hardware failure discuss turn fault injection robustness testing fault-tolerance community worked years techniques injecting faults system determine robustness fiat simulates occurrence hardware errors altering contents memory registers similarly fine inject software faults operating system major difference previous work approach focuses file systems handle broad class modern disk failure modes previous work approach assumes implicit knowledge file-system block types ensure test paths file system code previous work inserts faults blind fashion uncover problems found work similar brown patterson work raid failure analysis authors suggest hidden policies raid systems worth understanding demonstrate fault injection software raid systems qualitatively failure-handling recovery policies discover failure policy target file system raid requiring complex type-aware approach recent work yang model-checking find host file system bugs techniques well-suited finding classes bugs approach aimed discovery file system failure policy interestingly approach uncovers file system bugs yang reason testing scale model-checking limited small file systems reduce run-time approach applied large file systems work builds earlier work failure injection underneath file systems work developed approach test file systems handle write failures journal updates current work extends data types read write corruption failures iron file systems work iron file systems partially inspired work google acharya suggests cheap hardware paranoid assume fail unpredictable ways google good reason treats application-level problem builds checksumming top file system disk-level redundancy drives machines drive extend approach incorporating techniques file system applications benefit note techniques complimentary application-level approaches file system metadata block inaccessible user-level checksums replicas enable recovery now-corrupted volume related approach driver hardening effort linux stated hardened driver extends realm well-written include professional paranoia features detect hardware software problems page drivers generally improve system reliability faults handled file system end-to-end argument fail-partial failure model disks understood high-end storage high-availability systems communities network appliance introduced row-diagonal parity tolerate disk faults continue operate order ensure recovery presence latent sector errors virtually network appliance products checksumming detect block corruption similarly systems tandem nonstop kernel include end-to-end checksums handle problems misdirected writes interestingly redundancy single disk instances ffs internal replication limited fashion specifically making copies superblock platters drive noted earlier commodity file systems similar provisions suggest making replicas disk raid array reduce rotational latency primary intention copies recovery storage array difficult apply techniques selective manner metadata work replication improving performance fault-tolerance future investigation iron strategies checksumming commonplace improve system security patil stein suggest implement evaluate methods incorporating checksums file systems systems aim make corruption file system data attacker difficult finally dynamic file system sun good file system iron techniques dfs checksums detect block corruption employs redundancy multiple drives ensure recoverability contrast emphasize utility replication drive suggest evaluate techniques implementing redundancy show embellish existing commodity file system dfs written scratch limiting impact conclusions commodity operating systems grown assume presence reliable hardware result case file systems commodity file systems include requisite machinery handle types partial faults expect modern disk drives time reexamine file systems handle failure excellent model operating system kernel networking subsystem network hardware long considered unreliable hardware medium software stacks designed well-defined policies cope common failure modes disks viewed fully reliable mistrust woven storage system framework challenges remain failures disks expose layers file system software architecture redesigned enable consistent well-defined failure policy kind controls exposed applications users low-overhead detection recovery techniques iron file systems employ answers questions lead understanding effectively implement generation robust reliable iron file systems acknowledgments extend steve kleiman network appliance dave anderson jim dykes seagate insights disks work fail liuba shrira shepherd dave dewitt mark hill jiri schindler mike swift anonymous reviewers members adsl excellent suggestions comments himani apte meenali rungta invaluable work implementing parity ext finally computer systems lab csl providing terrific computing environment systems research work sponsored nsf ccrccr- ngsitr- ibm network appliance emc acharya reliability cheap learned stop worrying love cheap pcs easy workshop october altaparmakov linux-ntfs project http linuxntfs sourceforge net ntfs august alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca pages denver colorado anderson drive manufacturers typically don 
talk disk failures personal communication dave anderson seagate anderson dykes riedel interface scsi ata proceedings usenix symposium file storage technologies fast san francisco california april arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance eighth workshop hot topics operating systems hotos viii pages schloss elmau germany bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca pages munich germany june bartlett spainhower commercial fault tolerance tale systems ieee transactions dependable secure computing january barton czeck segall siewiorek fault injection experiments fiat ieee transactions computers april jfs overview ibm developerworks library jfs html bitton gray disk shadowing proceedings international conference large data bases vldb pages los angeles california august brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june candea kawamoto fujiki friedman fox microreboot technique cheap recovery proceedings symposium operating systems design implementation osdi pages san francisco california december chou yang chelf hallem engler empirical study operating system errors proceedings acm symposium operating systems principles sosp pages banff canada october corbett english goel grcanac kleiman leong sankar drastic row-diagonal parity differences double disk behavior failure parallel correction applications compared proceedings general-purpose usenix workloads symposium file parallel scientific storage workloads technologies fast high bursty pages san rates francisco california constant behavior april devale runs koopman input performance evaluation parameters exception parallel handling workloads tend libraries dominated proceedings storage international retrieval costs conference large dependable files systems check-point networks files dsngoteborg finally quick sweden deletion june uncommon douceur conclusions applications bolosky run large-scale isolation study production file-system settings contents scripting proceedings workflow tools acm sigmetrics glue conference series measurement applications modeling pipelines computer systems pipeline sigmetrics run pages thousands atlanta times georgia varied inputs achieve goals dykes users modern disk term workloads roughly batch-pipelined lines batches code pipelines personal run communication james instant dykes seagate paper characterize august emc collection emc centera scientific content batch-pipelined addressed workloads storage system typical http characterizations emc processing memory emerson demands essays bring english traits sharing characteristics self-reliance harvard workloads classics demonstrate edited charles importance eliot scalability york key managing collier workloads son classification volume segregating foolish consistency traffic type hobgoblin aggressively exploiting minds sharing adored characteristics statesmen scalability philosophers improved divines orders engler magnitude acknowledgements chen gratefully hallem acknowledge chou people chelf bugs helped deviant behavior install operate understand applications behavior including paolo desiati zach miller amadeu sum general juan approach pablo inferring jon errors foley dierk systems polzin code daniel reed proceedings alan smet acm symposium operating systems brian principles forney muthian sosp sivathanu pages florentina banff popovici canada timothy denehy october helpful ghemawat discussion gobioff comments paper leung work google file sponsored system nsf proceedings ccrccr- acm symposium operating ccrngs- systems principles itran sosp ibm faculty pages award bolton landing wisconsin lake alumni george research york foundation douglas thain october supported gibson lawrence landweber rochberg ncr fellowship zelenka distributed nagle systems amiri chang acharya uysal feinberg bennett gobioff mendelson lee beynon ozceri hollingsworth riedel saltz file server scaling sussman tuning network-attached performance secure disks intensive proceedings parallel applications joint proceedings international conference fourth workshop measurement input modeling output computer parallel systems sigmetrics distributed systems performance iopads pages pages seattle philadelphia washington pennsylvania june gray altschul census madden tandem system schaffer availability zhang zhang technical miller report tandem lipman gapped blast computers green psi-blast eide controller generation flaws protein version database http search mindprod programs nucleic eideflaw acids html research february pages kalbarczyk amdahl storage iyer parameters yang system characterization potential linux kernel ieee behavior computer group error conference proceedings pages international june conference baker dependable systems hartmann kupfer shirriff ousterhout measurements distributed file system proceedings acm symposium operating systems principles sosp july barroso gharachorloo bugnion memory system characterization commercial workloads proceedings international symposium computer architecture isca pages cantin hill cache performance selected spec cpu benchmarks computer architecture news september crandall aydt chien reed input output characteristics scalable parallel applications proceedings ieee acm conference supercomputing san diego california cypher konstantinidou messina architectural requirements parallel scientific applications explicit communication proceedings annual international symposium computer architecture pages san diego california acm sigarch ieee computer society tcca computer architecture news foley networks integrated biosphere model dsnpages land san surface francisco processes california terrestrial carbon june balance vegetation gunawi dynamics agrawal global biogeochemical arpaci-dusseau cycles arpacidusseau foster schindler voeckler 
deconstructing wilde zhou chimera virtual data system representing querying automating data derivation proceedings conference scientific statistical database management edinburgh scotland july gray shenoy rules thumb data engineering proceedings sixteenth ieee international conference data engineering icde pages holtman cms data grid system overview requirements cms note cern july hulith amanda experiment proceedings xvii international conference neutrino physics astrophysics helsinki finland june kuo winslett cho lee chen efficient input output scientific simulations proceedings parallel distributed systems iopads pages lee crowley bear anderson bershad execution characteristics desktop applications windows proceedings international symposium computer architecture isca pages litzkow livny mutka condor hunter idle workstations proceedings international conference distributed computing systems june miller katz input output behavior supercomputing applications proceedings acm ieee conference supercomputing pages ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system procedings acm symposium operating systems principles sosp pages december pasquale polyzos static analysis characteristics scientific applications production workload proceedings acm ieee conference supercomputing pages november rajasekar wan moore mysrb srb components data grid proceedings eleventh ieee symposium high performance distributed computing hpdc edinburgh scotland july ranganathan foster decoupling computation data scheduling distributed data-intensive applications proceedings eleventh ieee symposium high performance distributed computing hpdc edinburgh scotland july roselli lorch anderson comparison file system workloads usenix annual technical conference rosti serazzi smirni squillante impact program behavior parallel scheduling proceedings joint international conference measurement modelling computer systems sigmetrics pages samar stockinger grid data management pilot proceedings iasted international conference applied informatics february satyanarayanan study file sizes functional lifetimes proceedings symposium operating systems principles sosp pages sullivan werthimer bowyer cobb gedye anderson major seti project based project serendip data personal computers proceedings international conference bioastronomy sum pablo nautilus molecular simulations code technical report wisconsin madison dept chemical engineering thain bent arpaci-dusseau arpaci-dusseau livny architectural implications pipeline batch sharing scientific workloads technical report cstr- wisconsin computer sciences department january thain livny multiple bypass interposition agents distributed computing journal cluster computing vahdat anderson transparent result caching technical report csd- computer science division california-berkeley vazhkudai tuecke foster replica selection globus data grid ieee international symposium cluster computing grid ccgrid wong martin arpaci-dusseau culler architectural requirements scalability nas parallel benchmarks supercomputing portland oregon nov woo ohara torrie shingh gupta splashprograms characterization methodological considerations proceedings annual international symposium computer architecture pages santa margherita ligure italy june acm sigarch ieee computer society tcca computer architecture news 
commodity storage clusters proceedings annual international symposium computer architecture isca pages madison wisconsin june henson history unix file systems http infohost nmt val slides pdf hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference usenix winter san francisco california january hughes murray reliability security raid storage systems archives sata disk drives acm transactions storage february intel corp ibm corp device driver hardening http hardeneddrivers sourceforge net kari latent sector faults reliability disk arrays phd thesis helsinki technology september kari saikkonen lombardi detection defective media disks ieee international workshop defect fault tolerance vlsi systems pages venice italy october katcher postmark file system benchmark technical report trnetwork appliance october kleiman vnodes architecture multiple file system types sun unix proceedings usenix summer technical conference usenix summer pages atlanta georgia june lewis smart filers dumb disks nsic osd working group meeting april luby mitzenmacher shokrollahi spielman stemann practical loss-resilient codes proceedings twenty-ninth annual acm symposium theory computing stoc pages paso texas lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick joy leffler fabry fast file system unix acm transactions computer systems august mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- department computer science princeton november patil kashyap sivathanu zadok in-kernel integrity checker intrusion detection file system proceedings annual large installation system administration conference lisa atlanta georgia november patterson brown broadwell candea chen cutler enriquez fox kiciman merzbacher oppenheimer sastry tetzlaff traupman treuhaft recovery oriented computing roc motivation definition techniques case studies technical report csd- berkeley march patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june postel rfc transmission control protocol september ftp ftp rfc-editor in-notes rfc txt august prabhakaran arpaci-dusseau arpaci-dusseau model-based failure analysis journaling file systems proceedings international conference dependable systems networks dsnyokohama japan june redell dalal horsley lauer lynch mcjones murray purcell pilot operating system personal computer communications acm february reiser reiserfs namesys ridge field book scsi starch june rinard cadar dumitran roy leu william beebe enhancing server availability security failure-oblivious computing proceedings symposium operating systems design implementation osdi san francisco california december rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saltzer reed clark end-to-end arguments system design acm transactions computer systems november schindler experienced severe performance degradation identified problem disk firmware disk drives reprogrammed fix problem personal communication schindler emc july schlosser ganger mems-based storage devices standard disk interfaces square peg round hole proceedings usenix symposium file storage technologies fast pages san francisco california april schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december schwarz xin miller long hospodor disk scrubbing large archival storage systems proceedings annual meeting ieee international symposium modeling analysis simulation computer telecommunication systems mascots volendam netherlands october seltzer bostic mckusick staelin implementation log-structured file system unix proceedings usenix winter technical conference usenix winter pages san diego california january siewiorek hudak suh segal development benchmark measure system robustness proceedings international symposium fault-tolerant computing ftcstoulouse france june sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid proceedings usenix symposium file storage technologies fast pages san francisco california april sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california 
april solomon inside windows microsoft programming series microsoft press edition stein howard seltzer unifying file system protection proceedings usenix annual technical conference usenix boston massachusetts june sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january swift bershad levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing lake george york october talagala patterson analysis error behaviour large storage system ieee workshop fault tolerance parallel distributed systems san juan puerto rico april data clinic hard disk failure http dataclinic harddisk-failures htm transaction processing council tpc benchmark standard specification revision technical report tsai iyer measuring fault tolerance ftape fault injection tool international conference modeling techniques tools computer performance evaluation pages september tweedie journaling linux ext file system fourth annual linux expo durham north carolina wehman den haan enhanced ide fast-ata faq http thef-nym sci kun cgi-pieterh atazip atafq html weinberg solaris dynamic file system http members visi net thedave sun dynfs pdf wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february yang twohey engler musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october 
antfarm tracking processes virtual machine environment stephen jones andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison stjones dusseau remzi wisc abstract virtualized environment vmm system primary resource manager services implemented layer scheduling kinds security monitoring naturally implemented inside vmm implementing services vmm layer complicated lack vmm paper describes techniques vmm independently overcome part semantic gap separating guest operating systems supports techniques enable vmm track existence activities operating system processes antfarm implementation techniquesthat works detailed knowledge guest internal architecture implementation evaluation antfarm virtualization environments operating systems shows accurately infer process events incurring small runtime overhead worst case demonstrate practical benefits process information vmm implement anticipatory disk scheduler vmm level case study shows significant disk throughput improvements virtualized environment exploiting process information vmm introduction virtual machine technology increasingly deployed range platforms high-end servers desktop pcs large growing list reasons virtualization diverse computing environments including server consolidation support multiple operating systems including legacy systems sandboxing security features fault tolerance optimization specialized architectures software hardwaresupport develops virtualization included dominant commercial operating systems expect virtualized computing environments ubiquitous virtualization prevalent virtual machine monitor vmm naturally supplants operating system primary resource manager machine main target innovationin systemservices oneshouldnowconsiderhow implement services vmm transition functionality vmm potential benefits implementing feature single time vmm becomesavailabletoall vmm place features introduced system operating system legacy closed-source finally vmm locale system total control system resources make informed resource management decisions pushing functionality layer software stack vmm drawbacks significant problem lack higher-levelknowledge vmm referred semantic gap previousworkin partially recognized dilemma researchers developed techniques infer higher-level hardware resource utilization techniques vmm manage resources system reallocating idle page virtual machine virtual machine addition recently proposed vmm-based services explicit information software abstractions operating systems running bridgethe semantic gap previouswork thoroughlyexplored vmm learn operatingsystemsrunning information explicitly implicitly learn operating systems vmm important guest proprietary untrusted managed entity proceedings usenix annual technical conference june boston managing vmm cases explicit information details guest memory layout implementation unavailable unreliable paper developa set techniquesthat enable virtual machine monitor implicitly discover exploit information important operating system abstractions process monitoring lowlevelinteractionsbetweenguestoperatingsystemsandthe memory management structures depend show vmm accurately determine guest operating system creates processes destroys context-switches techniques operate explicit information guest operatingsystem vendor version implementationdetails demonstrate utility efficacy vmm-level process awareness building anticipatory disk scheduler vmm virtual machine environment anticipatory disk scheduler requires information vmm layers implemented exclusively making vmm process aware overcomesthis limitation os-neutral implementation vmm layer modifications detailed knowledge implementation vmm improve throughput competing sequential streams processes virtual machines single guest operating system factor addition scheduling process information vmm applications security domain system monitoringtools malicious software identify code data sensitive processes monitored runtime modification patterns system calls process recognize process compromised additiontodetection trusions process level affecting process scheduling finally process information parent-child relationship processes identify groups related processes user applications feasible vmm process information antfarm implementation process identification techniques virtualization environments xen simics antfarm evaluated applied linux windows sparc linux guest operating systems range environments spans processor families significantly virtualmemory managementinterfacesand operating systems process management semantics antfarm imposes small runtime overhead worst case scenario common process-intensive compilation environment rest paper organized section place antfarm context related work section cover required background material relating implementation architectures virtual machines general section discussion techniques underlying antfarm section covers implementation details antfarm evaluate accuracy overhead imposed antfarm section section present anticipatory scheduling case study conclude section related work antfarm informs vmm important operating system abstraction process aboutwhich information research recognized information explicitly vmm implementing vmm features services cases information relates hardware disco determines guest executing idle loop detecting enters lowpower processor mode vmware esx server page sampling determine utilization physical memoryassigned virtualmachines antfarm differs efforts focuses inferring information processes software construct projects recognized oslevel information vmm cases detailed version-specific memory layout information semantic knowledge make information exported directly vmm vmi implement security techniques detecting malicious hidden processes guest introvirt memory layout implementation details enable host-based intrusion detection features vmm antfarm contrast enables limited inexact levelof informationto inferredbya vmm itdoesthis explicit information memory layout implementation affected guests deployed broader set environments work uhlig similar shows infer guest-level information processor managementmore intelligentlyin multiprocessor environment specifically deduce kernel locks held observing executing user versus kernel mode antfarm complementary observes virtual resource mmu infer information operating system processes proceedings usenix annual technical conference june boston finally alternative inferring os-level information knowledge passed explicitly vmm extent paravirtualized architectures explicit information supplied paravirtualized guaranteedto match inside metric paravirtual information considered gold standard information vmm important environments explicitapproachis lessvaluable paravirtualization requires os-level modification implies functionality deployed vmm running beneath legacy closed-source operating systems reasons dependence explicit interfaces forces innovation vmm requiresos-levelinformationtobe coupledwithchangesto supported operating systems inferring guest information vmm innovate independent implementation finally case security applications guest trusted report activities compromised intentionally mislead vmm background techniques describe paper based observations vmm make interactions guest virtual hardware specifically antfarm monitors guest virtual mmu implement virtual address spaces section review pertinent details intel sparc architectures antfarm discuss basic features virtual machine monitors runtime information virtual memory architecture implementation platform intel family microprocessors chose frequently virtualized processor architecture today section reviews features virtual memory architecture important inference techniques architecture two-level in-memory architecturally-defined page table page table organized tree single memory page called page directory root -byte entry page directory point page page table process page table entry pte active address physical page virtual mapping exists page protection status bits pte page writable access page restricted privileged software single address space active processor time system software informs processor mmu address space active writing physical address page directory newaddressspace processorcontrolregister access register privileged vmm virtualize behalf guest operating systems tlb entries loaded on-demand active page tables processor operating system participate handling tlb misses operatingsystem tlb ways single entry removed invlpg instruction non-persistent entries entries page table entries marked global flushed tlb writing address space process tag maintained tlb non-shared entries flushed context switch sparc virtual memory architecture section review key aspects sparc mmu differsfrom chose sparc implementation architecture significantly memory management interface system software architecturally-defined hardware-walked page tables sparc software managed tlb system software implements virtual 
address spaces explicitly managing contents hardware tlb memory made tlb entry translation processor raises exception operating system opportunity supply valid translation deliver error offending process cpu aware operating system page table organization order avoid flushing entire tlb process context switches sparc supplies tag tlb entry called context associates entry specific virtual address space memory current context supplied mmu desired virtual address order match virtual page number context tlb entry identical supplied values entries distinct address spaces exist tlb simultaneously operatingsystem tlb granularity single page granularity entire address space operations called page demap context demap proceedings usenix annual technical conference june boston virtual machines vmm implements hardware interface software interface includes privileged system portions microprocessor architecture peripherals disk network user interface devices note non-privileged user portion microprocessor instruction set virtualized running unprivileged instructions guest directly executes processor additional overhead key feature virtualized system environment guest operating systems execute unprivileged mode processor vmm runs full privilege guest accesses sensitive system components mmu peripherals processor trap vmm vmm virtualizesensitive system featuresby mediatingaccess feature emulating mmu virtualized attempts guest operating system establish trapped vmm vmm observe attempts similarly request virtual disk device vmm examine vmm choose service request made virtualized interface sees fit requests virtual mappings altered disk requests reordered process identification key process inference techniques logical correspondencebetween abstraction process directly visible vmm virtual address space correspondence due traditional single address space process paradigm shared modern operating systems major process events seek observe creation exit context switch extent address spaces correspond processes events approximated address space creation destruction context switch techniques track processes tracking address spaces approach tracking address spaces sparc identify vmm-visible associate specific address space call address space identifier asid tracking address space creation context switch simply observing piece vmmvisible operating system state asid asid observed infer address space created asid replaced asid conclude address space contextswitch hasoccurred thetechniquewe identify address space deallocation consists detecting asid reuse assume address space asid refers beendeallocatedif asid reuse techniques architecture physical address page directory asid page directory serves root page table tree describes address space address page directory characteristic single address space process creation context switch detect address space creation observe page directories page directory physical address resides vmm notified guest writes privileged register observe asid usedthathas notbeen inferthata addressspace hasbeencreated whenan asidis seenfor time vmm adds asid registry akin operating system process list tracking purposes writes imply addressspace contextswitch monitoring events vmm asid active process exit detect address space deallocation knowledge generic responsibilities operating system maintain address space isolation requirements lead distinctive behavior observed exploited vmm infer address space destroyed operating systems strictly control contents page tables implement virtual address spaces process isolation breached page directory page table page reused distinct processes withoutfirst cleared previousentries ensure invariant holds windows linux systematically clear non-privilegedportionsof page table pages process prior reusing privileged portions page tables implement protected kernel address space cleared shared processes map memory accessible untrusted software ensure stale entries remain tlb address space deallocated architecture provide entries multiple address spaces coexist tlb tlb completely flushed prior reusing address space structures page directory tlb proceedings usenix annual technical conference june boston flushed writing event vmm observe detect user address space deallocation vmm count number user virtual mappingspresent page tables describing address space count drops vmm infer itis simplefora vmmtomaintainsucha counterbecause vmm informed updates process page tables order updates effective requirement vmm role virtualizing mmu multi-threading introduce additional complexity updates process page tables synchronized vmm correctness monitoring tlb flushes processors vmm detect requirementfor address space deallocation met events observed asid vmm address space dead entry asid registry removed subsequent asid implies creation distinct process address space techniques sparc key aspect enable process awareness present sparc vmm-visible identifier virtual address space physical address page directory sparc virtual address space context asid making obvious substitution leads process detection technique sparc similar creation context switch sparc installing contextid privilegedoperationandsoitisalwaysvisibletoavmm byobserving operation vmm maintain registry asids asid observed asid registry vmm infers creation address space context switch detected sparc context installed processor exit requirement reuse context sparc stale entries previously address space removed processor tlbs sparc context demap operation thispurpose vmm observe contextdemap operations entries context flushed sparc asid page directory context creation asid asid exit user mappings tlb flushed context demap context switch change context change table process identification techniques table lists techniques antfarm detect process event sparc architectures processorit impliesthatthe addressspace longer valid implementation antfarm implemented virtualization environments xen true vmm low-level system simulator called simics architectures supported xen antfarm xen xen open source virtual machine monitor intel architecture xen paravirtualized processor interface enables lower overhead virtualization expense porting system software explicitly make feature xen mechanisms describe equally applicable conventional virtual machine monitor vmware operating systems ported run xen proprietary commercial operating systems microsoft windows supported antfarm xen implemented set patches xen hypervisor concentrated handlers events page faults page table updates privileged register access additional hooks added xen back-end block device driver antfarm patches xen including debugging measurement infrastructure total approximately lines files antfarm simics simics isafullsystem unmodified commercial operating systems applications variety processor architectures simics virtual machine monitor strict sense direct execution user instructions play proceedings usenix annual technical conference june boston role vmm allowing antfarm observe interpose operating system application hardware requests vmm simics explore process awareness techniques sparc linux windows xen-only implementation antfarm simics implemented simics extension module simics extension modules shared libraries dynamically linked main simics executable extension modules read write application memory registers vmm simics hooks called haps hardwareeventsforwhichextensionmodulescanregistercall- back functions antfarm simics hap detect writes antfarm simics sparc hap detect processor context changed invocation callback akin exception raised guest accesses privileged processor registers true vmm memory write breakpoint installed antfarm simics pages page tables page table updates detected vmm xen marks page tables read-only detect event antfarm simics consists lines code simics sparc total approximately lines process awareness evaluation section explorethe accuracyof antfarmin implementation environments characterize runtime overhead antfarm xen analysis accuracy decomposed components thefirstis abilityto correctlydetectprocess creations exits context switches call aspect completeness component time difference lag process events occur operating 
system detected vmm evaluation evaluation xen version version linux kernel xen privileged control linux kernel version unprivileged vms noted evaluation hardware consists ghz pentium ram virtual machines allocated ram environment microsoft windows windows run xen simics purpose simics virtual machines configured ghz pentium ram completeness quantify completeness guest operating system exit andcontext switch event records include asid time event obtained processor cycle counter traces compared similar traces generated antfarm guest traces functionally equivalent information provided paravirtualized included process event interface evaluation implicitly compares accuracy antfarm ideal represented paravirtual interface addition process creation exit context switch guests report address space creation destruction events discriminate errors caused mismatch processes address spaces errors induced inaccurate address space inferences made antfarm categorize incorrect inferences false negatives false positives false negative occurs true process event missed antfarm false positive occurs antfarm incorrectly infers events exist determine false negatives occurred one-to-one matches found os-reported event pair traces required matching event asid occur range event plausible match process-creation event inferred event occur previous os-reported process os-reported process creation events asid table reports process address space event countsgatheredby ourguest oses antfarmduring experiment utilizing process intensive workloads workload synthetic creates processes runs seconds exits process creation rate processes linux synthetic workload variants creates processes fork fork exec employs vfork exec windows processes created createprocess api workload parallel compile bash shell sources command make clean object directory compilation workload chosen creates large number short-lived processes stressing antfarm ability track concurrent processes varying runtimes antfarm incurs false negatives tested proceedings usenix annual technical conference june boston process addr spc inferred process addr spc inferred context create create create exit exit exit switch inferred linux fork fork exec vfork exec compile linux fork fork exec vfork exec compile windows create compile table completeness table shows total number creations exits processes address spaces reported operating system total number process creations exits inferred antfarm shown comparison antfarm detects process creates exits false positives false negatives linux windows fork exec lead false positives linux bold face values false positives due mismatch address spaces processes matching counts address space creates inferred creates actual inferred context switch counts shown completeness accurate expected cases process-related events reported instrumented oses detected vmm fact inferred counts greater equal reported counts suggests verified os-reported event properly matched vmm-inferred event linux windows false positives occur indicatingantfarmcan precisely detectaddressspace events one-to-one match address spaces processes operating systems linux false positives occur indicatedin table bythe os-reported counts discrepancy due implementationof linux fork exec system calls unix programscreate user processes invoking theforksystemcallwhich amongotherthings constructs address space child process child address space copy parent address space cases newly created child process immediately invokes exec system call replaces child virtual memory image program read disk linux exec invoked existing process address space cleared reused newly loaded program contrast linux destroys releases address space process invoking exec address space allocated newly exec program linux process invokes exec distinct address spaces overlap time words runtime process partitioned segments segment corresponds period fork exec corresponds period exec process exit antfarm based address space tracking concludes processes created leading inferred process creations exits occurred due idiomatic fork exec process partitioned distinctive linux case figure depicts temporal relationship inferred pseudo-processes duration pseudo-process small case compilation workload average time fork exec compared average lifetime pseudoprocess seconds difference orders magnitude pseudo-processesare separated shorttime period active interval corresponds time original address space destroyed address space created compilation workload interval averaged larger user instructions executed absence user address space combination pseudo-processes detected antfarm encompasses user activity true process conventional fork exec imply proceedings usenix annual technical conference june boston figure effects error figure shows type process identification error occurs tested platform error lag true event occurs vmm detects figure consists falsely partitioning single process multiple inferred processes linux occurs exec typically immediately fork sparc partitioning process calls fork exec create lag linux avg max exit lag linux concurrent processes windows figure lag system load figure shows average maximum create exit lag time measurements variety system load levels evaluation environments average worst case create lag affected system load linux windows small constant linux large exit lag competing processes linux exit lag sensitive system load substantive activity true user process captured pseudo-process lag aspect process identification accuracy consideristhe timedifferencebetweena processevent eventis detected vmm define process exist instant fork equivalent system call invoked exit defined start exit system call definitions maximally conservative figure create lag labeled exit lag labeled lag similar nature response time expect sensitive system load evaluate sensitivity conduct experiment measures lag times levels system load linux linux windows experiment cpubound processes created additional test processes created create exit lag time computed test process creationswere separated test process slept exiting results experiments presented figure graph x-axis shows number concurrent cpu-bound processes y-axis shows lag time create lag sensitive system load linux windows steadily increasing lag time increasing system load result intuitive call scheduler occur invocation create process api parent process begins child process runs vmm detects linux exhibits process creation policy leads small constant creation lag antfarm detects process creation process runs vmm informed process existence user instructions executed exception idle linux shows large exit lag average reason anomaly linux kernel tasks including idle task user address space borrow previously active user address space run mechanism kernel task run incurring expense tlb flush case experiment test processes started intervals process sleeps processes ready run approximately mselapse processexitand process begins interval linux idle task active preventsthe previousaddress space released leads observed delay big picture figure shows set timelines depicting antfarm tracksprocessactivity overtime parallelcompilation proceedings usenix annual technical conference june boston process count vmm linux bash compile time diff process count vmm linux bash compile time diff process count vmm bash compile time diff figure compilation workload timelines linux linux windows process count timeline shown timeline depicts os-reported process count vmm-inferred process count difference versus time lag larger impact accuracy false positives linux exhibits significantly smaller lag linux track process counts accurately workload platforms top curve eachgraphshowsthetrue reported operating system middle curve shows current process count inferred antfarm bottom 
curve shows difference curves calculated inferred actual result large creation lag linux apparent larger negative process count differencescomparedto linux workload metriccombination false positives experienced linux environmentsuch lightly loaded system tend reduce lag metric total cumulative process count false positives incurred linux problematic exit lag prominent graphs large persistent exit lag effects show significant positive deviations difference curves fact errors due fork exec accumulate time linux apparent increasing inaccuracy trend present overhead evaluate overhead process awareness techniqueswe measure comparethe runtime workloads antfarm pristine build xen workload microbenchmark represents worst case performance scenario antfarm experiments performed linux guests vmm extensions affect code paths page tables updated microbenchmark focuses execution paths program allocates memory touches page ensure page table entry allocated page created exits causing page tables cleared released program run times total elapsed time computed experiment repeated times average duration reported negligible variance experiments unmodified version xen experiment required average seconds complete antfarm xen experiment average seconds complete average slowdown worst case runtime configuringand buildingbash compared modified unmodified versions xen unmodifiedcase average measured runtime trials average runtime experiment modified xen variance experiments negligible yielding slowdown process-intensive application workload proceedings usenix annual technical conference june boston process addr spc inferred process addr spc inferred context create create create exit exit exit switch inferred sparc linux fork fork exec vfork compile table completeness sparc table shows results experiments reported table sparc linux false positives occur fork due implementation copy-on-write antfarm infers additional non-existent exit create event pair exec error due multiple address spaces process stems flush occurs clear caller address space exec sparc evaluation implementation process tracking sparc simics virtual machine configured mhzultrasparc iiprocessorand mbofram sparc linux version guest operating system tests guest operating system instrumented report information completeness criteria evaluate process awareness sparc table lists total event counts process creation micro-benchmark bash compilation workload false negatives occur contrast fork-only variant microbenchmark incurs false positives reason copy-on-write implementation fork linux fork writable portionsof parent saddress space marked read-only copy-on-write shared child entries parent page tables updated tlb entries flushed sparc linux accomplishes efficiently flushing parent current tlb entries context demap operation context demap incorrectlyinterpretedbyantfarmasa processexit assoonas parent scheduled run detect address space signal matching spurious process creation false positives caused fork sparc character caused exec errors limited convention tiny time interval fork exec fork invoked processes user shell occur repeatedly process lifetime linux sparc case figure depicts process repeatedly invokes fork partitioned inferred pseudo-processesby antfarm execis additionalfalse positives reason linux case process inference technique falsely reports creation address spaces don treally exist behavior tlb demap operation occurswhena error mode differentthan observed errors due faulty assumption single address space process sparc error occurs chosen indicator context demap happen correspondingaddress space deallocated sources false positives expect compilation workload experience approximately multiple false positives fork exec synthetic benchmark fewer false positives expect due vfork gnu make gcc vfork creates process duplicate parent address space parent page tables changed flush required exec invoked detect creation single address space vfork exec linux antfarmexperiences false positives build process consists processes created make gcc processes created calls external shell process creations induce false positives observe lag lag os-recorded vmm-inferred process events sparc linux comparable linux average maximum lag values sparc linux system loads shown figure create lag sensitive system load exit lag unaffected load proceedings usenix annual technical conference june boston create lag sparc-linux avg max exit lag concurrent processes figure lag system load sparc figure shows average maximum create exit lag time measurements experiments figure create lag grows system load exit lag small constant independent load process count vmm sparc linux bash compile time diff figure compilation workload timeline compilation timeline comparable figure sparc linux limitations sparc inferencetechnique simple suffers drawbacks relative shown technique incurs false positives techniques spite additional false positives figure shows technique track process events parallel compilation workload accurately linux unlike assume page directory page shared multiple runnable processes make assumption context ids sparc reason vastly smaller space unique context ids sparc bits field distinct contexts represented concurrently system exceeds number active processes context ids necessarily recycled cases system softprocess count vmm sparc context overflow time diff figure context overflow processes exist represented sparc context ids techniques fail detect context reuse ware limit number concurrent contexts supports linux sparc architectures context bits concurrent address spaces supported recycling figure shows behavior sparc process detection techniques processes exist distinguished context ids limit reached technique fails detect additional process creations importance limitation reduced busy servers rarely active processes fact doubt influenced selection context field size discussion process event detection techniques antfarm based mechanisms provided cpu architecture implement manage virtual address spaces responsibilities general-purpose operating systems maintain process isolation techniques assume follow address space conventions suggested mmu features architecture deviates convention detection accuracy differ reported evaluation shows widely operatingsystems adhereto ourassumptions antfarmprecisely identifies desired process events windows linux false positives occur linux andsparc linux thefalsepositivesare stylized affectthe ability antfarmto accurate process count architectures devoted hardware-assisted virtualization configurations reduce eliminate vmm track guest page taproceedings usenix annual technical conference june boston ble updates context switches amd private guest-cr options secure virtual machine svm architecture fact prevent vmm observing guest operating systems shadow page tables explicitly supported architectures increase performancepenalty exacted techniques antfarm case study anticipatory scheduling order disk requests serviced make large difference disk performance requests adjacent locations disk serviced consecutively time spent moving disk head unproductively minimized primary performance goal disk scheduling algorithms case study explores application innovative scheduling algorithm called anticipatoryscheduling virtualmachine environment ofantfarmforxen background iyer demonstrated phenomenon call deceptive idleness disk access patterns generated competing processes performing synchronous sequential reads deceptive idleness leads excessive seeking locations disk solution called anticipatory scheduling introduces small amount waiting time completion request initiation process disk request completed issue request nearby location strategy leads substantial seek savings throughput gains concurrent disk access streams exhibit spatial locality anticipatory scheduling makes process-specific information decides wait process issue read request long wait based statistics disk scheduler processes recent disk accesses average distance request stored estimate process access distance large sense waiting process issue request nearby statistics long 
process waits request completes issues order determine long make sense wait request issued anticipatory scheduling work virtual machine environment system-wide information disk requests required estimate disk head located essential deciding request nearby obehavior requiredto determine whetherand howlong wait information completely single guest requests vmm distinguish guestlevel processes guests vmm cooperate implement anticipatory scheduling requires introduction additional specialized vmm-to-guest interfaces interfaces case legacy binary-only components case interfaces exist today information implement anticipatory scheduling effectively vmm vmm distinguish guest processes additionally associate disk read requests specific guest processes pieces information vmm implementation anticipatory scheduling maintain average seek distance inter-request waiting time processes guests antfarm inform implementation anticipatory scheduling inside xen associate disk read requests processes employ simple context association strategy associates read request process active simple strategy potential asynchrony operating system account due request queuing inside read issued vmm process originated blocked context switched processor leads association error researched accurate ways associating reads true originating process tracking movement data disk memory requesting process methods proven effective overcoming association error due queuing limited space present techniques implementation anticipatory scheduling paper simple context association implementation xen implements device driver virtual machines ddvm ddvm virtual machine allowed unrestricted access physical devices ddvms logically part xen vmm operationally guests runningin disk requests ddvm idealized disk device interface ddvm carries behalf current versions xen driver vms run linux advantage broad device support offers device back-end driver services requests submitted instance front-end driver located proceedings usenix annual technical conference june boston vmas aggregate throughput sec scheduler configuration comparison layer schedulers processes vms process vms processes figure benefit process awareness anticipatory scheduling graph shows theaggregate throughput forvarious configurations scheduler number virtual machines number processes virtual machine experiment linux deadline scheduler standard anticipatory scheduler vmm-level anticipatory scheduler vmas adding process awareness enables vmas achieve single process sequential read performance aggregate competing sequential streams running guest layer effective process case global disk request information normal vms standard linux kernel includes implementation anticipatory scheduling implement anticipatory scheduling vmm layer enabling linux anticipatory scheduler xen ddvm manages disk drive make existing implementationprocessaware resents processes running vms disk request arrives foreign virtual machine xen hypervisorabout process active foreign virtual machine ability distinguish processes expect vmm-level anticipatory scheduler vmas competing processes exist vms evaluation vmas repeat experiments original anticipatory scheduling paper virtual machine environment experiment consists running multiple instances program sequentially reads segment private file vary number processes assignment processes virtual machines disk scheduler guests vmm explore process awareness influences effectiveness anticipatory scheduling vmm make linux deadline scheduler nonanticipatory baseline results scheduler configurations combined workloads shown figure workloads virtual machine processes virtual machines process virtual machines processes experiment shows results configuration anticipatory scheduling demonstrates expected performance anticipation workloads test system results aggregate throughputof sec configurationenables anticipatory scheduling guest deadline scheduler xen virtual machine process case guest complete information processes actively reading disk expect anticipatory scheduler guest level effective figure shows fact case anticipatory scheduling improve aggregate throughput sec sec cases deadline scheduler due lack information processes virtual machines experiment demonstrates performance unmodified anticipatory scheduling vmm layer similar case anticipatory scheduling running guest layer expect performance improvement two-virtual-machine one-process-each case good vmm distinguish virtual machines operating system distinguish processes improvement occur implementation detail xen ddvmback-enddriver requestsin thecontextofa single dedicatedtask anticipatory scheduler interprets presented stream single process making alternating requests parts disk performance comparable configuration anticipation workloads final configuration shows benefit process awareness anticipatory scheduling implemented vmm layer workload configurations anticipatory scheduling works improving aggregate throughput factor sec sec implemented atthevmm layer anticipatoryschedulingin thisconfiguration complete information requests reaching disk process awareness extensions track statistics individual process enabling make effective anticipation decisions proceedings usenix annual technical conference june boston conclusion widespread adoption virtual machines brings interesting research opportunities reevaluate operating system services implemented implementing os-like services vmm made challenging lack high-level application information techniquesdevelopedin paperand implementation antfarm explicit information important operating system abstraction process inside vmm observing interaction guest virtual hardware method alternative explicitly exporting required information vmm directly enabling vmm independently infer information vmm decoupled specific vendor version correctness guests supports acknowledgments work sponsored sandia national laboratories doctoral studies program nsf ccritr- cnsand generous donations network appliance emc amd amd programmer manual volume system programming december ballmer keynote address microsoft management summit april bressoud schneider hypervisor-based fault tolerance proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december bugnion devine rosenblum disco running commodity operating systems scalable multiprocessors proceedings acm symposium operating systems principles sosp pages saint-malo france october chen noble virtual real hotos proceedings eighth workshop hot topics inoperating systems page ieeecomputer society dragovic fraser hand harris pratt warfield barham neugebauer xen art virtualization proceedings acm symposium operating systems principles sosp bolton landing lake george york october fraser hand neugebauer pratt warfield williamson safe hardware access xenvirtual machine monitor oasis asplos workshop gao reiter song gray-box program tracking foranomaly detection proceedings usenix security symposium pages san diego usa august garfinkel pfaff chow rosenblum boneh terra virtual machine-based platform fortrusted computing proceedings acm symposium operating systems principles sosp bolton landing lake george york october garfinkel rosenblum virtual machine introspection based architecture intrusion detection proc network distributed systems security symposium february goldberg survey virtual machine research ieee computer gum system extended architecture facilities virtual machines ibm journal research development november intel intel virtualization technology specification iaintel architecture april iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous proceedings acm symposium operating systems principles sosp pages banff canada october joshi king dunlap chen detecting past present intrusions vulnerabilityspecific predicates proceedings acm symposium operating systems principles sosp pages brighton united kingdom october king chen backtracking intrusions proceedings acm symposium operating systems principles sosp banff canada october magnusson christensson eskilson forsgren allberg ogberg larsson moestedt werner simics full system simulation platform ieee computer february popek goldberg formal requirements virtualizable generation architectures communications acm sekar bowen segal preventing intrusions process behavior monitoring proc workshop intrusion detection network monitoring pages berkeley usa usenixassociation sivathanu bairavasundaram arpacidusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december somayaji forrest automated response system-call 
delays proceedings usenix annual technical conference usenix san diego california june sugerman venkitachalam lim virtualizing devices vmware workstation hosted virtual machine monitor proceedings usenix annual technical conference usenix boston massachusetts june uhlig levasseur skoglund dannowski scalable multiprocessor virtual machines proceedings virtual machine research technology symposium pages san jose california waldspurger memory resource management vmware esx server proceedings symposium operating systems design implementation osdi boston massachusetts december whitaker shaw gribble scale performance denali isolation kernel proceedings symposium operating systems design implementation osdi boston massachusetts december 
controlling place file system gray-box techniques james nugent andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract present design implementation place gray-box library controlling layout top ffslike systems place exploits knowledge ffs layout policies users place les directories speci localized portions disk applications place collocate les exhibit temporal locality access improving performance series microbenchmarks analyze overheads controlling layout top system showing overheads prohibitive discuss limitations approach finally demonstrate utility place case studies demonstrate potential layout rearrangement web-server environment build benchmarking tool exploits control placement quickly extract low-level details disk system traditional gray-box manner place library achieves ends user level changing single line operating system source code introduction creators high-performance o-intensive applications including database management systems web servers long yearned control placement data disk proper data allocation exploit locality access workload increasing disk ciency improving performance systems provide explicit controls needed applications affect desired layouts unix systems based berkeley fast file system ffs group les set heuristics speci cally group inodes data blocks les reside directory applications full control layout traditionally avoided systems altogether relinquishing convenience control gray-box techniques promising approach gather information exert control systems export interfaces treating system gray box assumes general knowledge system behaves implemented knowledge combined run-time observations system enables construction powerful services exported base system paper explore application gray-box techniques placement problem speci cally retain convenience system regaining control placement introduce place positional layout controller system exploits gray-box techniques give applications improved control placement system depicted figure important component place place information control layer icl place icl applications group les directories localized portions disk speci cally group proper placement data improve read write performance collocating les accessed time applications improve performance short-stroking disk reducing cost seeks limiting arm movement portion disk applications place library operate expected key place implementation shadow directory tree sdt sdt hidden control structure place icl control les disk carefully creating structure exploiting gray-box knowledge system behavior sdt enables place icl place les user preferences correct cient manner creating maintaining structure central challenges implementing place rst evaluate place icl set microbenchmarks understand basic costs potential bene place costs place reasonable controlled directory creation costly standard versions proceedings usenix annual technical conference june san antonio texas unmodified ffs file system place icl place tools command line place application pmkfs group group group figure place system place system consists components highlighted gray gure important component place information control layer icl gray-box techniques discover information system exploits knowledge enable applications link control directory layout components place tool pmkfs initialize place on-disk structures set place command-line tools place works top ffs-like systems learning internal group structure exposing structure place icl operation prototype implementation potential bene substantial random performance improves dramatically related data items grouped small portion disk large les outer tracks disk improve throughput due zoning effects demonstrate utility place separate application studies rst show web server place group les exhibit temporal access locality utilizing simple placement heuristics collocation place improves web server throughput noticeably show high-speed user-level benchmarking tool called fast place rapidly construct testing infrastructure controlled placement les fast extract important disk characteristics seek time bandwidth seconds rest paper organized section describe design implementation place section measure costs show bene section present case studies place usage describe related work section future directions section conclude section place design implementation section describe place system controlling layout rst provide background describe goals implementing place describe api exposed place icl presenting programming interface discuss place implementation including system initialization shadow structures control placement discuss general operation issues concurrency limitations current implementation background modern unix systems based berkeley fast file system including direct descendants found bsd solaris families intellectual descendants linux ext main innovations ffs emphasis locality placing related data objects disk ffs provided quantum leap performance systems scattered data disk oblivious manner primary construct ffs manage disk locality cylinder group block group ext terms interchangeably simplicity cylinder group divides disk number contiguous regions consists inodes data blocks bitmaps tracking inode block usage small number blocks store implementation-speci information placing related data objects cylinder group conversely spreading unrelated objects groups locality access achieved dif culty deciding objects related typically simple heuristics based system namespace speci cally group related objects implementations place inodes data blocks les directory group assuming locality access les conversely directories groups spread unrelated les disk leaving room grow group original ffs implementation descendants spread large les groups avoid lling group single large designing place icl seek exploit gray-box knowledge ffs-like systems perform layout order users control les disk understand limits gray-box control including types functionality realized top modern systems proceedings usenix annual technical conference june san antonio texas design goals designing place goals simple intuitive control layout applications straightforward representation disk locality exploit application-speci knowledge improve performance easy place easy substantial code modi cations required programming apis commandline tools provided compatible non-place applications applications place top system operate basic system structure usage unmodi applications change unaffected system namespace applications users les conventions desire layout dependent specialized naming schemes goals impact design implementation place abstractions api basic abstraction place exposes underlying groups ffs-like systems applications link place library applications knowledge access patterns place related les directories speci group exploiting spatial locality improved performance number group applications pieces information applications safely assume les proximate groups close object group close object group close object group lower group numbers located outer tracks disk higher group numbers located tracks applications utilize zone-sensitive placement large les improve throughput note abstract virtual groupings group hierarchies layered top physical group interface desired simplicity focus solely lowest level abstraction rest paper applications place les directories speci groups place basic functions applications place createfile char pathname mode mode int group creates speci pathname mode set mode group number group rst arguments identical creat system call place createdir char pathname mode mode int group creates directory speci bypathnamewith mode set mode group number group rst arguments identical mkdir system call theplace createfilecall ne-grained placement les groups place createdir function applications create directory controlled manner subsequent allocations directory place collocated due standard ffs policy place allocate directory group 
due insuf cient resources free data blocks inodes left group case standard behavior routine return error indicating object created alternative interface routines search nearby group failure place directory utility convenience functions provided applications discover number groups system current utilization level group number group utilized user re-write application place api set command-line tools utilized tools users move directories les speci groups create speci groups subsequent data access unmodi applications enjoy bene rearrangement basic operation place exploits ffs tendency namespace hint placement gain control layout place rst create structure les directories created controlled fashion created place library renames les moving back proper location system namespace system structure shadow directory tree sdt central place implementation initialization process performed system place produces sdt structure appears system namespace shown figure important entities found sdt superblock persistent inproceedings usenix annual technical conference june san antonio texas hidden superblock hidden concurrency hidden hidden hidden figure shadow directory tree hidden shadow directory tree structure presented superblock persistent information needed place concurrency manage multi-user access finally directories control placement formation place concurrency manage concurrent access les place api les discussed detail interesting set directories namedd throughdn number cylinder groups system initialization procedure detail ensures directory cylinder group note structures hidden directory applications traversing directory tree controlling file creation sdt place creating group straightforward application calls place createfile passing pathname created mode bits desired group place internally place icl creates shadow directory simply calls rename put proper location namespace place checks make allocated group user requested i-number newly allocated initialization place learns records number group mapping information determine allocation successful controlling directory creation placing directory proper group place createdir challenging creating directory proper shadow directory suf ffs-like systems place child directory cylinder group parent approach required shown algorithm algorithm works creating temporary directory checking desired group i-number repeat tmp picknewname mkdir tmp indesiredgroup tmp break end fillothergroups forever rename tmp dirname algorithm directory creation algorithm basic algorithm create directory speci group disk presented repeating process temporary directory created correct group directory created renamed proper location namespace complication arises due directory allocation policies ffs-like systems linux ext policy searches group above-average number free inodes fewest allocated data blocks netbsd ffs picks group above-average number free inodes fewest allocated directories algorithm create temporary les directories coerce system creating directory desired group process referred algorithm fillothergroups creates number les non-target groups ensure les spread groups uncontrolled manner place creates small les les utilize indirect pointers basic algorithm slow demonstrate section speed process common case build shadow cache directories group numbers sdt attempting create directory group directory creation algorithm rst consults shadow cache directory group exists place simply renames directory nished avoiding expensive directory creation algorithm place directory cache performs fulledged algorithm case directories created algorithm added cache repopulating shadow cache periodically sdt initialization discuss initialization process required place encapsulated tool call pmkfs place mkfs steps pmkfs proceedings usenix annual technical conference june san antonio texas pmkfs discovers system parameters algorithms pmkfs creates sdt on-disk data structures populates shadow cache parameter discovery place requires pieces information create on-disk structures support controlled allocation number groups system ngrp number blocks bgrp inodes igrp group total number blocks inodes system readily statfs system call finding number groups slightly challenging current algorithm calculates number allocating directories recording difference inode numbers subsequently allocated directories directory group common difference number inodes group place detects allocation wrapped fact directory i-number close previously allocated directory igrp calculate group number gnum object inode number inum computing gnum inum igrp system calculates number direct pointers inode size small required directory creation algorithm work multiple ffs platforms discovered synchronously writing blocks monitoring number free blocks system statfs small size discovered point single allocating block write decreases free block count blocks indicating indirect block allocated current implementation place requires exclusive access empty system initialization reason restriction igrp exported system procedure previously determine number made source system administrator place initialized top system sdt creation step pmkfs stores information superblock creates directory tree directoriesd throughdn assuming groups process creating directories identical directory creation algorithm found algorithm typical directory creation procedure excess directories created added shadow cache general place maintain minimal threshold shadow directories group avoid costly directory creation algorithm obtain understanding threshold examined system traces labs typical busy day found thousand long-lived directories created giving rough upper bound number shadow directories place maintain absorb day worth controlled directory creation environment issues crash recovery concurrency directory creation place create les directories sdt potential data accrue time occur created sdt system crashes rename place worse job killed midst place library call place include basic crash recovery mechanism order periodically remove les refer process sdt cleaning current implementation sdt cleaner scans sdt directory structures removes data objects left system crashes run cleaner alternatives current implementation invokes cleaner invocations place set conservative longer directory-allocation process run alternatives include running cleaner time interval day background process issues arise place usage multiple processes users concurrent place processes problem current implementation basic algorithm allocate directory situation competing controlled directory creations groups lead signi dif culty creating directory desired location avoid problem place acquires advisory lock concurrency mode lock signify usage basic algorithm practice usage basic algorithm repopulates shadow cache reducing mode operation cooperative approach processes share work gaining control introduce signi complexity multiple users introduce issue sdt shared private user sharing requires level trust applications sdt writable location shared sdt vulnerable types attacks changing structures place lead poor allocations lling sdt causing denial service proceedings usenix annual technical conference june san antonio texas environments problem single user application sole access system trustworthy settings sdt replicated per-user basis increases space utilization duplicates effort circumvents security issues arise due sharing limitations primary limitation place implemented ffs-like systems modern unix systems ffs-like recent features including journaling ext soft updates found bsd family ffs implementations affect ability control placement techniques limitation arises due internal implementation ffs implementations spread larger les cylinder groups order avoid lling single group quickly ffs behavior prevents place controlling large les laid disk provide interface query place largest size allocation guaranteed controllable notable exception standard 
ffs implementation strategy occurs ext spread larger les groups implementation strategy hints graybox implementors inside systems build top behavior simple understand easy control place directly ne-grained placement les group applications modify order creation pack les group controlled fashion alternative initially explored overcomes limitations mesh applications place alternative approach place initially lls target system set dummy les discovering exact locations place free space applications request space data allocations controlled deemed approach unacceptable unmodi applications work correctly applications system appears full analysis section analyze behavior place demonstrating functionality basic overheads rst discuss experimental environment proceed series microbenchmarks demonstrating effectiveness layout control revealing approach group number group utilization figure controlled allocation graph depicts experiments creates -kb les rst standard system interfaces number directories les created varied experiments labeled fourth experiment place api create les directories place single group middle disk labeled graph group number varied x-axis shaded bar data group darker bars indicating data debugfs command gather needed information costs system creation usage show improvement expected reorganizing data controlling layout account zonedbit recording finally discuss experience broader range platforms experimental environment present results place top linux ext system experiments platform performed mhz pentium-iii processor main memory ibm lzx disk default ext system built disk consists block groups report experience systems end section layout control begin simple experiment demonstrate place effectively collocate les speci group disk speci cally compare methods creating directory tree allocated uniformly-sized les rst standard system interfaces alter number directories place les fourth place icl create les underneath directories direct system proceedings usenix annual technical conference june san antonio texas time number shadow directories system creation time place figure system initialization system initialization time plotted x-axis vary number shadow directories created group y-axis plots total time initialization complete place les directories single group middle disk figure shows group utilization approach directory trees data gure directories standard layout algorithms data les scattered disk contrast place data located middle group system desired system creation demonstrated basic control layout seek understand costs system rst cost present system initialization performed pmkfs tool figure presents system initialization time dominant cost system initialization number shadow directories created shadow cache present sensitivity initialization time number shadow directories created group gure cost scale increasing number directories linux ext system increasing amount data created order allocate directories groups successfully api overheads present overheads controlled directory creation place goal understand costs gray-box control data placement table breaks cost creating different-sized les place createfile interface costs presented table broken time percentage base state alloc ren misc total table file allocation overheads result shows average controlled creations place icl variance runs time percentage shadow cache shadow cache min median max base state alloc ren clean misc total table directory allocation overheads result shows average controlled directory creations place icl note times milliseconds rightmost column max shows time seconds time shown appears due rounding categories creation tests categories base time create standard interfaces state time read superblock access system statistics con guration information alloc time control allocation case stat system call check inode number ren time rename correct namespace misc additional software processing overhead table place api creation adds roughly overhead creation cost due place initialization amortized multiple calls place library signi overhead allocation rename software overheads finally size increases overheads unsurprisingly amortized explore overheads directory creation place createdir api table presents cost breakdown controlled directory allocation shadow cache note category included labeled cleanup includes time spent cleaning sdt directory-allocation process run note alloc case refers proceedings usenix annual technical conference june san antonio texas time number directories directory tree create performance place naive place directories place directories shadow cache standard figure create performance cost moving directory tree speci group presented varying number sub-directories structure x-axis xed amount data spread evenly les approaches creating structure compared text y-axis presents total time bulk collocation log scale costs creating les directories required directory-allocation algorithm table make number observations shadow cache time controlled directory creation reasonable roughly substantially higher base directory creation cost approximately factor faster shadow cache times higher median cost column lists maximum time shadow cache potential cost running full directorycreation process worst case takes seconds create directory correct group dif culty arises controlled creation smaller group ext allocates directories based free bytes remaining takes excessively long time groups coerce directory allocation group base alloc state times essentially constant constitute negligible part total maximum case shadow cache basic algorithm clean afterward bulk collocation costs common usage place move entire directory tree speci group disk accomplished place command-line tools improving storage system availability d-graid muthian sivathanu vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison fmuthian vijayan dusseau remzig wisc abstract present design implementation evaluation d-graid gracefully-degrading quickly-recovering raid storage array d-graid ensures les system remain unexpectedly high number faults occur d-graid recovers failures quickly restoring live system data hot spare graceful degradation live-block recovery implemented prototype scsibased storage system underneath unmodi systems demonstrating powerful le-system functionality implemented narrow block-based interface introduction tree falls forest hears make sound george berkeley storage systems comprised multiple disks backbone modern computing centers storage system entire center grind halt downtime expensive on-line business world millions dollars hour lost systems storage system availability formally ned time failure mtbf divided sum mtbf time recovery mttr mtbf mtbf mttr improve availability onecan increase mtbf decrease mttr surprisingly researchers studied components availability increase time failures large storage array data redundancy techniques applied keeping multiple copies blocks sophisticated redundancy schemes parity-encoding storage systems tolerate small xed number faults decrease time recovery hot spares employed failure occurs spare disk activated lled reconstructed data returning system normal operating mode quickly narrow interface systems storage curtailed opportunities improving mtbf mttr raidstorage array disk fails repaired entire array corrupted availability cliff result storage system laying blocks oblivious semantic importance relationship les corrupted inaccessible extra disk failure time-consuming restore backup entire array remains unavailable disks operational storage array information blocks live system recovery process restore blocks disk unnecessary work slows recovery reduces availability ideal storage array fails gracefully disks system data unavailable ideal array recovers intelligently restoring live data effect important data disappear failure data restored earlier recovery strategy data availability stems berkeley observation falling trees isn process access recovered failure explore concepts provide storage array graceful failure semantics present design implementation evaluation d-graid raid system degrades gracefully recovers quickly d-graid exploits semantic intelligence disk array place system structures disks fault-contained manner analogous fault containment techniques found hive operating system distributed systems unexpected double failure occurs d-graid continues operation serving les accessed d-graid utilizes semantic knowledge recovery speci cally blocks system considers live restored hot spare aspects d-graid combine improve effective availability storage array note graid techniques complementary existing redunappears usenix symposium file storage technologies fast dancy schemes storage administrator con gures d-graid array utilize raid level single disk fail data loss additional failures lead proportional fraction unavailable data paper present prototype implementation d-graid refer alexander alexander semantically-smart disk system built underneath narrow block-based scsi storage interface disk system understands system data structures including super block allocation bitmaps inodes directories important structures knowledge central implementing graceful degradation quick recovery intricate understanding system structures operations semantically-smart arrays tailored systems alexander functions underneath unmodi linux ext vfat systems make important contributions semantic disk technology deepen understanding build semantically-smart disk systems operate correctly imperfect system knowledge demonstrate technology applied underneath widely varying systems demonstrate semantic knowledge raid system interested apply strategy tool redundancy techniques based moving large type amount data data improving source nal destination availability group key small aspects number alexander groups time number directories implementation directory graceful tree read degradation performance rst standard selective random place metadata random replication standard optimal alexander place replicates optimal naming figure system smallle meta-data reads structures time system read -kb high les degree data standard shown redundancy techniques settings data varying number small amount directories overhead excess data failures render settings entire array standard unavailable system apis entire directory create hierarchy les traversed settings place fraction les collocate missing data proportional single group number orders missing disks shown random reads fault-isolated data les placement random strategy order ensure optimal semantically reads meaningful data units single scan disk failure figure alexander presents places time semantically-related perform blocks bulk blocks collocation data storage spread array evenly unit -kb fault-containment les disk varying number observing sub-directories natural failure schemes boundaries found compared rst array place failures make naive semantically-related fashion groups creating blocks directories unavailable les leaving rest target group recursively system intact assuming shadow fault-isolated cache data placement exists improves approach availability dramatically cost slow related blocks directory creation longer algorithm striped nds increasingly drives dif reducing cult force natural data bene parallelism target found group raid approach creates techniques directories rst remedy performance alexander improves tremendously implements access-driven ext diffusion allocation improve policy throughput number frequently-accessed bytes les allocated spreading group-selection copy policy blocks hot creating les les target drives group system easier alexander monitors coerce access system data determine choosing les scheme replicate shows time fashion nds space approach assuming replicas directories pre-con gured allocated performance shadow reserve cache opportunistically improves unused performance portions bulk storage collocation system evaluate availability seconds improvements finally traditional d-graid directory-tree copy trace analysis shown simulation comparison point d-graid fast excellent job overhead masking arbitrary number failures processes spread enabling data continued access disk important data localized evaluate manner bene prototype alexander collocation quantify microbenchmarks potential read trace-driven performance workloads improvement place construction perform d-graid nal set feasible microbenchmarks imperfect figure shows semantic knowledge performance powerful functionality rst set tests implemented present block-based time storage takes array read set run-time overheads les d-graid small collocated disk cpu costs gure compared standard array application high reads show proceedings access-driven diffusion usenix crucial annual technical performance conference live-block june recovery san effective antonio disks under-utilized combination texas replication bandwidth data placement group number recovery exploiting techniques results zoned-bit storage recording system ext ffs improves figure availability largele reads maintaining performance high level reading performance rest shown paper ext structured ffs varying section group present extended motivation created section x-axis discuss cache design ushed principles umount d-graid mount cycle section present read trace ensure analysis disk simulations bandwidth discuss properly semantic measured knowledge point section average section present trials prototype variance implementation trials evaluate low prototype set les section discuss random order alternative collocating methods implementing localized d-graid portion commercial disk improves feasibility performance semantic disk factor based approach section random lines section graph present related work les conclude read section extended optimal motivation order case essentially graceful scanning 
disk single sweep bene collocation small case spreading data disk results additional seeks makes difference performance demonstrate place advantage zoned bandwidth characteristics modern disks degradation figure plots raid performance redundancy large techniques typically sequential scans les export simple speci failure model groups gure fewer depicts disks fail performance raid place continues operate systems correctly standard ext degraded modi performance ext acts disks traditional fail bsd raid ffs unavailable gure problem corrected ext platform placing restore data tape lower-numbered raid groups schemes corresponds directly small placing data outer zones disks working disk users improving observe performance failed disk observe system graceful degradation experiment raid run system top absolutely ffs tolerate system xed number zoning nature faults disks excess hidden failures ffs spreads catastrophic blocks large les data amount disk proportional place number control disks placement blocks system experience continues systems primary allowing focus access data ext system failed data modern restored matter users applications entire contents volume present matters set les question realistic expect catastrophic failure scenario raid system raidsystem high mtbf reported disk manufacturers appears usenix symposium file storage technologies fast disk failure highly occur rst failed disk repaired multiple disk failures occur primary reasons correlated faults common systems expected raid carefully designed orthogonal manner single controller fault component error render fair number disks unavailable redundant designs expensive found higher end storage arrays gray points system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure page evidence suggests multiple failures occur ibm serveraid array controller product includes directions attempt data recovery multiple disk failures occur raidstorage array organization data stored servers raidin servers single disk failed indicator informed administrators problem problem discovered disk array failed full restore backup ran days scenario graceful degradation enabled access large fraction user data long restore approach dealing multiple failures employ higher level redundancy enabling storage array tolerate greater number failures loss data techniques expensive three-way data mirroring bandwidth-intensive write redundant store graceful degradation complementary techniques storage administrators choose level redundancy common case faults graceful degradation enacted worse expected fault occurs mitigating ill 
effect semantically-smart storage implementing functionality semantically-smart disk system key bene enabling wide-scale deployment underneath unmodi scsi interface modi cation working smoothly existing systems software base desire evolve interface systems storage reality current interfaces survive longer anticipated bill joy systems protocols live forever mechanism d-graid deployed non-intrusive existing infrastructure semantic disks ensure design d-graid expectations section discuss design d-graid present background information systems data layout strategy required enable graceful degradation important design issues arise due layout process fast recovery file system background semantic knowledge system speci discuss d-graid design implementation widely differing systems linux ext microsoft vfat system inclusion vfat represents signi contribution compared previous research operated solely underneath unix systems ext system intellectual descendant berkeley fast file system ffs disk split set block groups akin cylinder groups ffs bitmaps track inode data block allocation inode blocks data blocks information including size block pointers found inode vfat system descends world operating systems paper linux vfat implementation fatalthough work general applies variants vfat operations centered eponymous allocation table entry allocatable block system entries locate blocks linked-list fashion rst block address entry fat block entry hold end-ofle marker setting block free unlike unix systems information found inode vfat system spreads information fat directory entries fat track blocks belong directory entry information size permission type information graceful degradation ensure partial availability data multiple failures raid array d-graid employs main techniques rst fault-isolated data placement strategy d-graid places semanticallyrelated set blocks unit fault containment found storage array simplicity discussion assume semantically-related set blocks single disk unit fault containment generalize easily generalized failure boundaries observed scsi chains refer physical disk belongs home site implementation ffs concepts disk fails popular fault-isolated data system placement ensures linux community les designed disk aspects appears place usenix general symposium ffslike file systems mind storage technologies fast interested foo studying bar behavior inode place foo inode bar platforms data bar rst data test bar data generality root data run foo place inode root top foo ext bar system inode foo journaling inode bar version data ext ext great lengths preserve backwardscompatibility ext on-disk structures utilized surprised place works issue top ext tested place top implementation ffs allocation algorithms linux kernel place worked modi cation environment limitations discussed section shown directly figure relating placement large les case studies section describe place library rst demonstrate web server reorganize les place improve server throughput response time describe high-speed system benchmarking infrastructure place quickly extract characteristics underlying system improving web server throughput rst apply place order understand potential performance improvement web server environment reorganizing les popularly accessed les close disk seek costs reduced web service good target place structure typical web directory tree necessarily match locality assumptions encoded ffs-like systems change source code web server reorganization performed off-line command-line tools study potential bene simpli trace-based approach utilize web trace wisconsin-madison web server trace rst preprocessed remove requests induce system activity errors redirects requests remain transfer data http replies reply cache coherence check trace roughly million requests accesses total directory tree size understand potential gains collocation run trace system request generator trace entry request generator invokes system call records response time specifically model http requests generator performs stat system call requests transfer data maps memory touches page approach capture full complexity web environment give proceedings usenix annual technical conference june san antonio texas time memory size standard place figure web server performance time play back component web trace shown standard line plots performance typical layout place approach packs data small portion disk point represents trials variance low size directory tree served baseline potential performance improvement system reorganization utilize place command-line tools collocate directory tree outer-most tracks disk compare organization directory tree spread drive determined typical system heuristics figure time replay trace function amount memory range memory sizes collocation place improves performance roughly bene result directly reduction seek costs demonstrated instrumentation testing apparatus speci cally recording group number access compute average group distance traversed requests standard layout average number groups requests place performance gains limited due access patterns found web trace trace requests les single image directory les collocated standard ffs policy reducing placeassisted placement environment place improve performance simple direct manner greater bene expected environments access patterns match directory structure closely rapid file system microbenchmarking examine place context fast discovery performance characteristics tools developed time extract performance characteristics underlying system benchmarking tools run root run uncontrolled potentially lengthy amount time chen patterson self-scaling benchmark runs hours days reporting results back user settings system benchmarking tool ran quickly trading accuracy shorter run-time running application foreign computing environment seti home wide-area shared computing system condor globus mobile application quickly extract characteristics underlying system parameterize properly system benchmark run user-level requiring special privileges discover system parameters develop benchmarking tool fast fast bar data bar data root data foo inode root inode foo inode bar data bar data bar foo data root bar data foo inode root inode root inode root inode root inode fooinode fooinode foo foo data root foo data root foo data root bar data foo bar data foo bar data foo figure comparison layout schemes gures depict layouts foo bar unix system starting root inode directory tree data vertical column represents disk simplicity assumes data redundancy user data left typical system layout non-d-graid disk system blocks pointers spread system single fault render blocks bar inaccessible middle fault-isolated data placement les directories scenario access inode access data indirect pointer blocks constrained disk finally selective meta-data replication replicating directory inodes directory blocks d-graid guarantee users les requisite pointers removed rightmost gure simplicity color codes white user data light shaded inodes dark shaded directory data home site unavailable les remain accessible les technique selective meta-data replication d-graid replicates naming system meta-data structures system high degree directory inodes directory data unix system d-graid ensures live data reachable orphaned due failure entire directory hierarchy remains traversable fraction missing user data proportional number failed disks d-graid lays logical system blocks availability single depends disks traditional raid array dependence set entire set disks group leading entire system unavailability unexpected failure unix-centric typical layout fault-isolated data placement selective meta-data replication depicted figure note techniques d-graid work meaningful subset system laid single d-graid array system striped multiple d-graid arrays single array meaningful view system scenario d-graid run logical volume manager level viewing arrays single disk techniques remain relevant d-graid treats system block type differently traditional raid taxonomy longer adequate describing d-graid behaves ner-grained notion raid level required graid employ redundancy techniques types data d-graid commonly employs n-way mirroring naming system meta-data standard redundancy techniques mirroring parity encoding raidfor user data note administrative control determines number failures d-graid degrade gracefully section explore data availability degrades varying levels namespace replication design considerations layout replication techniques required enable graceful degradation introduce host design issues highlight major challenges arise semantically-related blocks fault-isolated data placement d-graid places logical unit system data fault-isolated container disk blocks d-graid considers related determines data remains failure basic approach le-based grouping single including data blocks inode indirect pointers treated logical unit data technique user les directory unavailable frustration confusion groupings preserve meaningful portions system volume failure directory-based grouping d-graid ensures les directory unit fault containment automated options allowing users arbitrary semantic groupings d-graid treats unit load balance fault-isolated placement placing blocks disks blocks isolated single home site isolated placement improves availability introduces problem load balancing space time components terms space total utilized space disk maintained roughly level fraction disks fail roughly fraction data unavailable balancing addressed foreground data rst allocated background migration files directories larger amount free space single disk handled potentially appears usenix symposium file storage technologies fast expensive reorganization reserving large extents free space subset drives files larger single disk split disks pressing performance problems introduced fault-isolated data placement previous work striping data disks performance compared sophisticated placement algorithms d-graid makes additional copies user data spread drives system process call access-driven diffusion standard d-graid data placement optimized availability access-driven diffusion increases performance les frequently accessed surprisingly access-driven diffusion introduces policy decisions d-graid including place replicas made performance les replicate create replicas meta-data replication level degree meta-data replication d-graid determines resilient excessive failures high degree replication desirable meta-data replication costs terms space time space overheads trade-offs obvious replicas imply resiliency difference traditional raid d-graid amount space needed replication naming system meta-data dependent usage volume directories induces greater amount overhead time overheads higher degree replication implies lowered write performance naming system meta-data operations observed lack update activity higher levels directory tree lazy update propagation employed reduce costs fast recovery main design goal d-graid ensure higher availability fast recovery failure critical straightforward optimization d-graid recover live system data assume restoring data live mirror hot spare straightforward approach d-graid simply scans source disk live blocks examining system structures determine blocks restore process readily generalized complex redundancy encodings d-graid potentially prioritize recovery number ways restoring important les rst importance domain speci les users manner similar hoarding database coda exploring graceful degradation section simulation trace analysis 
evaluate potential effectiveness graceful degradation impact semantic grouping techniques rst quantify space overheads level replication -way -way -way ext ext vfat vfat table space overhead selective meta-data replication table shows space overheads selective metadata replication percentage total user data level naming system meta-data replication increases leftmost column percentage space overhead meta-data replication shown columns depict costs modest -way paranoid -way schemes row shows overhead system ext vfat block size set graid demonstrate ability d-graid provide continued access proportional fraction meaningful data arbitrary number failures importantly demonstrate d-graid hide failures users replicating important data simulations accurate system extraction mobile application extract performance characteristics underlying system xed time budget user level fast extract information system memory system system component utilizes place mobile application examine single processor version now-sort worldrecord-breaking sorting application traditionally thought database contexts sorting commonly found scienti computation pipelines reasonable candidate mobile execution scienti peer-to-peer shared computing systems now-sort requires parameters tune host system rst parameters bandwidth expected local disk worst-case seek time numbers sort estimate large buffers merge phase order amortize seek costs size caches memory-hierarchy sorting data cachesized chunks sorting proceeds faster rate dif cult parameters generally extract maximum seek cost place api fast tool create les disk issue synchronous update rst start timer issue synchronous update record elapsed time write giving coarse estimate full-stroke seek nements made time order remove rotational costs desired table presents costs running fast test system mode operation fast runs quickly garnering coarse estimates required system parameters table observe proceedings usenix annual technical conference june san antonio texas time cache bandwidth max seek pmkfs total table fast performance table presents time fast takes discover system parameters mode fast congured run quickly extracting coarse estimates consuming time total time extract needed information sorting roughly seconds sorts massive data sets spending extra seconds con gure application worth time finally note pmkfs specialized task hand giving command-line options prevent creation shadow directories initialization time reduced small xed overhead related work work directly related place gray-box file layout detector controller fldc original gray-box paper fldc components rst decide order access set les re-write directory improve accesses components place fldc exposing ne-grained control directory layout applications applications long sought control underlying operating system policies mechanisms response demand previous research developed operating systems including spin exokernel vino much-improved control operating system behavior gray-box approach route improved control underlying exploiting knowledge behavior place demonstrates directory layout realized user-level place method exposing group numbers conceptually similar exokernel philosophy exposing physical names place treats system underlying entity exposes internal structure ext block groups exokernel exposes details hardware physical sector numbers moving data blocks spatial arrangement web server case study explored contexts work disk shuf ing ruemmler wilkes track frequency block accesses reorder disk blocks reduce seek times higher level staelin garcia-molina rearrange les system major difference approaches place performed transparently users applications control exposed sophisticated tracking blocks les accessed temporal succession hope develop access-tracking tool future work improving web server performance similar work improving web proxy performance hummingbird library-based system designed web proxies place features hummingbird users collocate les tie locality naming contrast place implemented top ffs-like system hummingbird performs functions library runs raw disk hummingbird specialized web-proxy environment place 
general-purpose tool finally fast tool bears similarity recent work database management systems online aggregation dbms returns approximate result selection query user immediately includes statistical estimate accuracy result user query running system nes result time data sampled answer precise fast tool applies philosophy benchmarking system future work number avenues exist future research plan explore breadth applicability place top systems platform interested bsd family challenges domain recent bsd implementations ffs utilize dirprefs algorithm directory group selection algorithm places directories parents attempt increase performance common operations unpacking large directory tree building gray-box controller place top dirprefs require extra care spread directories groups building place top log-structured system lfs interesting grouping related les generally straightforward user wishes group les les written time aspects make lfs challenging including grouping les span multiple segments controlling off-line behavior cleaner generally interested developing techniques control alloproceedings usenix annual technical conference june san antonio texas cation broader range systems investigate utility methods range storage devices specifically determine controlled placement top modern disk arrays finally tool place low-level mechanism placing les controlled manner disk les higher-level policy decision requires detailed knowledge les accessed time similar previous work data rearrangement plan develop tool track les blocks accessed generate system traces collected labs cover days activity data spread logical volumes space overheads rst examine space overheads due selective meta-data replication typical d-graidstyle redundancy calculate cost selective meta-data replication percentage overhead measured volumes trace data calculate highest selective meta-data replication overhead percentage assuming replication user data user data mirrored overheads cut half table shows selective meta-data replication induces mild space overhead high levels meta-data redundancy linux ext vfat systems -way redundancy meta-data space overhead incurred worst case vfat blocks increasing block size ext space due internal fragmentation larger directory blocks overheads decrease vfat phenomenon due structure vfat xed-sized system block size grows allocation table shrinks blocks directory data grow static availability examine d-graid availability degrades failure semantic grouping strategies rst strategy le-based grouping information single failure boundary disk directory-based grouping allocates les directory analysis place entire les directories trace simulated -disk sysappears usenix symposium file storage technologies fast percent directories completely number failed disks static data availability directory-based -way directory-based -way file-based -way directory-based -way figure static data availability percent entire directories shown increasing disk failures simulated system consists disks loaded trace strategies semantic grouping shown le-based directory-based line varies level replication namespace meta-data point shows average deviation trials trial randomly varies disks fail tem remove simulated disks measure percentage directories assume user data redundancy d-graid level figure shows percent directories directory les accessible subdirectories les gure observe graceful degradation works amount data proportional number working disks contrast traditional raid disk crashes lead complete data unavailability fact availability degrades slightly expected strict linear fall-off due slight imbalance data placement disks directories modest level namespace replication -way leads good data availability failure conclude le-based grouping les directory disappear failure inputs leading user dissatisfaction placement dynamic conclusions availability classic finally paper simulating hints dynamic computer availability system examine design users lampson applications tells don oblivious hide graid power operating degraded mode speci higherlevel cally abstractions run portion hide trace undesirable properties simulator functionality number contrast failed disks exposed record client percent processes unix observed systems failure expose explicit controls run laying experiment les user namespace replication demands standard layout les heuristics workloads needed conform processes locality replicated assumptions set stone experiment set years ago degree perform poorly namespace replication paper full present replication design vary implementation level evaluation replication place gray-box contents information popular directories control layer usr bin bin exposes lib directory information applications exploiting knowledge internal figure algorithms percent unaffected common processes ffs-like number systems failed place disks dynamic control per-process availability directory popular allocations replication -way microbenchmarks -way -way shown figure costs dynamic gray-box data control availability gure overly plots burdensome percent potential processes bene run unaffected controlled allocation disk failure substantial busy hour case studies trace demonstrated degree namespace place system replication set aggressively realistic diverse line application varies settings amount replication discussed popular limitations directories place -way implies gray-box directories approach controlled replicated allocation -way highlighting -way features show system allocation policies modest make extreme amount simple dif replication cult means build deviations control top trials shown shows gray-box approach replicating alternative contents path innovation directories requiring percent processes run underlying operating ill-effect system lower expected dif cult results implement maintain figure distribute gray-box icl embeds directories knowledge replicated underlying system percentage processes exploits run knowledge completion implement disk functionality failure portable manner expected important reason question remains clear substantial full number range processes functionality implemented require gray-box manner executable ultimate libraries limitations run icl correctly popular small step directory replication nal excellent answer availability acknowledgments failure nathan fortunately burnett tim denehy popular florentina les popovici read vijayan prabhakaran directories muthian wide-scale sivathanu replication feedback raise write paper performance special consistency issues muthian space assistance overhead due popular traces directory replication geoffrey minimal kuenning excellent sized shepherding system anonymous trace reviewers directories thoughtful account suggestions combination greatly total improved system content size paper semantic knowledge john heim move staff construction doit d-graid providing prototype recent underneath web trace block-based scsi-like included path interface names enabling tom technology engle underlying d-graid implementation semantic ffs allocation knowledge algorithm understanding linux kernel system finally utilizes disk computer enables systems lab d-graid providing implement superb graceful environment degradation computer failure science research quick recovery work exact sponsored details nsf acquiring semantic ccrccr- knowledge disk ccrngs- raid system itran ibm faculty award assume wisconsin alumni basic research foundation arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau culler hellerstein patterson highperformance sorting networks workstations proceedings acm sigmod conference management data sigmod tucson arizona bershad savage understanding system layout structures storage system speci cally assume d-graid static knowledge system layout including regions przemyslaw disk pardyak fiuczynski block becker types contents speci chambers block types eggers extensibility elds safety inode performance appears spin usenix operating symposium system file proceedings storage technologies acm symposium fast file system behaviors paper extend understanding semanticallysmart disks presenting techniques handle general system behaviors previous work required system mounted synchronously implementing complex functionality disk relax requirement describe assumptions general system behavior modern systems adhere behavioral guidelines blocks system dynamically typed system locate types blocks physical location disk lifetime system unix system block data region user-data block indirect-pointer block directory-data block system delay updates disk delayed writes system facilitate batching small writes memory suppressing writes les subsequently deleted consequence delayed writes order system writes data disk arbitrary systems order writes carefully remain general make assumptions ordering note assumptions made practical reasons linux ext system exhibits aforementioned behaviors accuracy information assumptions general system behavior imply storage system accurately classify type block block classi cation straightforward type block depends location disk berkeley fast file system ffs regions disk store inodes xed system creation traf regions inodes type information spread multiple blocks block lled indirect pointers identi observing inode speci cally inode indirect pointer eld address indirect block formally identify indirect block semantic disk inode block indirect pointer eld relevant inode block written disk disk infers indirect block observes block written information classify treat block indirect block due delayed write reordering behavior system time disk writes block freed original inode reallocated inode type normal data block disk operations place memory ected disk inference made semantic disk block type wrong due inherent staleness information tracked implementing correct system potentially inaccurate inferences challenges address paper implementation making d-graid discuss prototype implementation graid alexander alexander faultisolated data placement selective meta-data replication provide graceful degradation failure employs access-driven diffusion correct performance problems introduced availability-oriented layout alexander replicates namespace system meta-data administrator-controlled stores user data raidor raidmanner refer systems d-graid levels pursuing d-graid level implementation logstructuring avoid small-write problem exacerbated fault-isolated data placement section present implementation graceful degradation live-block recovery complexity discussion centered graceful degradation simplicity exposition focus construction alexander underneath linux ext system end section discuss differences implementation underneath vfat graceful degradation present overview basic operation graceful degradation alexander indirection map similar scsi-based raid system alexander presents host systems linear logical block address space internally alexander place blocks facilitate graceful degradation control placement alexander introduces transparent level indirection logical array system physical placement disks indirection map imap similar structures unlike systems imap maps live logical system block replica list physical 
locations unmapped blocks considered free candidates d-graid reads handling block read requests d-graid level straightforward logical address block alexander imap replica list issues read request replicas choice replica read based criteria alexander randomized selection appears usenix symposium file storage technologies fast figure anatomy write gure depicts control sequence write operations alexander rst gure inode block written alexander observes contents inode block identi newly added inode selects home site inode creates physical mappings operating blocks inode home site inode block aggressively replicated gure alexander observes write data block inode mapped write directly physical block gure alexander write unmapped data block defers block alexander nally observes inode fourth gure creates relevant mappings observes blocks deferred transforming issues policies deferred mechanisms write infokernel relevant andrea home arpaci site dusseau writes remzi contrast arpaci reads dusseau write nathan requests burnett complex timothy handle denehy thomas alexander engle handles haryadi write request gunawi depends james nugent type florentina block popovici written department figure computer depicts sciences common cases wisconsin block madison fdusseau static remzi meta-data ncb block tedenehy englet inode haryadi damion bitmap popovicig block wisc abstract unmapped describe alexander allocates evolutionary path physical block operating systems disks replica exible reside writes manner higher-level copies services note infokernel alexander exposes key easily pieces detect information static block types algorithms inode internal state bitmap blocks default underneath policies unix mechanisms systems simply observing controlled logical block user-level address implemented inode block prototype infokernels written based d-graid scans linux block newly netbsd added kernels inodes called understand infolinux inodes infobsd d-graid compares infokernels export newly key written abstractions block basic copy information process primitives referred block infolinux differencing implemented case studies inode showing d-graid selects policies home site linux lay manipulated blocks belonging kernel inode speci records cally show inode-to-homesite default hashtable cache selection replacement home algorithm site layout policy balance disk space scheduling allocation algorithm tcp physical congestion disks control d-graid algorithm greedy turned approach base selects mechanisms home site case study disk space found utilization infokernel abstractions write implemented unmapped block code data region overhead accuracy data block synthesizing policies indirect block user-level acceptable directory categories block subject allocation descriptors operating d-graid systems organization block belongs design general terms design actual home experimentation site performance keywords case policy d-graid mechanism places information block introduction deferred separating block policy list mechanism long write goal disk operating system learns design informal block nition view crash policy scheme inode write deciding make block mechanism inaccessible tool system implementing set in-memory policies deferred block conceptually list design simpler reliability concern d-graid view distinct newly added block pointers minimum separating inode policy indirect mechanism block written build newly modular added block pointer refers managing unmapped processes block graid cpu adds traditional entry view imap dispatcher mapping performs logical block low-level physical context-switch block home mechanism site assigned scheduler decides inode process newly added run pointer refers policy block conceptual deferred division list ectively d-graid isolates removes code block change deferred list issues permission write make digital hard copies physical block part writes work deferred personal classroom blocks granted written fee provided owner copies inode blocks made inode distributed written pro rst subsequent commercial data advantage writes copies bear mapped notice disk full directly citation block rst type page interest copy d-graid republish post data servers bitmap block redistribute data lists bitmap requires block prior speci written d-graid permission scans fee sosp newly freed data october blocks bolton landing freed york block usa graid removes copyright logical-to-physical mapping exists acm ported frees dispatcher physical blocks change block handle erent workloads deferred list scheduler freed block ambitious removed goals separation deferred list enables extensible write systems suppressed kernel data blocks implements mechanisms written processes implement system policies deleted user-level suit inode written disk alternatively generate extra design disk traf kernel similar processes optimizations download found ideal policies systems directly removing blocks deferred practice list important cleanly case separate freed policies blocks mechanisms alexander simplest observe mechanisms owning inode decisions embedded deferred block stays deferred list bounded amount time inode owning block written bitmap block indicating deletion block written exact duration depends delayed write interval system block reuse discuss intricate issues involved implementing graceful degradation rst issue block reuse existing les deleted truncated les created blocks part reallocated graid place blocks correct home site reuse blocks detected acted d-graid handles block reuse manner inode block indirect block written d-graid examines valid block pointer physical block mapping matches home site allocated inode d-graid mapping block correct home site write block made context home site copied physical location location blocks copied added pending copies list background thread copies appears usenix symposium file storage technologies fast blocks home site frees physical locations copy completes dealing imperfection dif culty arises semantically-smart disks underneath typical systems exact knowledge type dynamically-typed block impossible obtain discussed section alexander handle incorrect type classi cation data blocks data directory indirect blocks d-graid understand contents indirect blocks pointers place blocks home site due lack perfect knowledge fault-isolated placement compromised note data loss corruption issue goal dealing imperfection conservatively avoid eventually detect handle cases speci cally block construed indirect block written assume valid indirect block live pointer block d-graid action cases systems principles december burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey june chen patterson approach performance evaluation self-scaling benchmarks predicted performance inproceedings acm sigmetrics conference pages dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey california june engler kaashoek toole exokernel operating system architecture proceedings usenix annual technical conference june san antonio texas application-level resource management proceedings acm symposium operating systems principles december foster kesselman globus metacomputing infrastructure toolkit international journal supercomputer applications hellerstein haas wang online aggregation sigmod international conference management data sigmod pages tucson arizona holtman cms data grid system overview requirements cms note cern july lampson hints computer system design proceedings acm symposium operating system principles pages bretton woods december acm litzkow livny mutka condor hunter idle workstations proceedings acm computer network performance symposium pages june mckusick joy lef fabry fast file system unix acm transactions computer systems august mcvoy staelin lmbench portable tools performance analysis proceedings usenix winter rst case pointer refer unmapped logical block mentioned d-graid creates mapping home site inode indirect block belongs indirect block pointer valid mapping correct mapping indirect block misclassi pointer invalid d-graid detects block free observes data bitmap write point mapping removed block allocated bitmap written d-graid detects reallocation inode write creates mapping copies data contents home site discussed case potentially corrupt block pointer point mapped logical block discussed type block reuse results mapping copy block contents home site indirect block pointer valid mapping correct block indirect block placing misclassi cation waiting alexander process wrongly copies queue makes data policy decision home site note process data inserted accessible queue original presence speci block mechanisms belongs dictates policies blocks ciently incorrect pragmatically home implemented site fortunately words situation levin transient decision inode exclude policies written d-graid lower-level detects kernel reallocation policy creates research mapping explicitly back strives original push home site nition mechanism restoring extreme correct mapping files safe accessed protected image properly hardware laid operation infrequent sweep implement inodes policies ensure fairness rare cases competitors improper layout result mechanism policy optimizations d-graid viewed continuum eventually policies move implemented data component correct act home site mechanisms preserving graceful degradation layer reduce system number words times features misclassi policy cation occurs alexander mechanisms makes depends assumption perspective contents indirect important blocks challenge speci cally next-generation systems determine number valid policies unique implemented pointers traditional null pointers converted alexander leverage mechanisms assumption higher-level services greatly reduce large number amount misclassi cations functionality performing hundreds integrity millions check dollars invested supposed indirect thousands block developer integrity years spent check commodity reminiscent operating work systems conservative view garbage existing collection policies returns true mechanisms pointers -byte words higher-level services block built point top valid data radical addresses restructuring volume operating system non-null pointers advocate unique evolutionary approach set thesis blocks paper pass integrity key check transforming corrupt policies implemented data contents happened mechanisms evade export conditions information test internals run data blocks operating system system enhanced expose small internal fraction information data infokernel blocks work show pass test functionality system blocks signi pass cantly test enhanced existing reallocated modi data block provide key pieces indirect block information misclassi internal state access-driven diffusion algorithms issue infokernel d-graid higher-level address services require performance fault-isolated policies data placement improves availability implement cost functionality performance data accesses services blocks large leverage default directory-based policies grouping demonstrate les feasibility directory approach longer modify parallelized linux improve kernel performance create alexander infolinux performs access-driven implementation diffusion monitoring infolinux block exposes accesses values determine key data structures hot simple diffusing descriptions blocks algorithms employs replication illustrate disks power system information enhance investigate parallelism access-driven technical conference january meter observing effects multi-zone disks proceedings usenix case studies user-level services convert default linux policies controllable mechanisms illustrate succinctness approach show tens hundreds lines code required export information linux case studies focus major components cache management placement disk scheduling networking rst case study show user-level service convert linux q-based page replacement algorithm building block replacement algorithms mru lru lfu fifo show applications turn range system allocation policies controllable placement mechanism applications les directories disk show linux c-look disk scheduling policy transformed building block algorithms idle queue free-bandwidth scheduler finally demonstrate exporting information tcp reno altered controllable transport mechanism enabling user-level policies tcp vegas experience reveals fundamental abstractions infokernel support exporting comprehensive information infokernel retains level secrecy implementation enable innovation infokernel interface provide portability applications infokernel strives export fundamental abstractions expected hold policies represent page replacement algorithm infokernel report direct state page bits frequency bits dirty bits relevant state depends exact algorithm generalized abstraction desired list pages ordered expected evicted level detail export fundamental tension infokernel design case studies describe abstraction prioritized lists provide ective means exporting requisite information prototype infobsd derived netbsd veri abstractions easily implemented erent systems case studies uncover limitations infokernel approach relevant kernel state expressed ordered list user-level service directly manipulate ordering touching page increase priority primitives types services implemented limited disk scheduling networking case studies target policy substantially ers underlying kernel policy cult accurately mimic target policy behavior arises emulating mru top default linux algorithm finally list-reordering operations expensive involve disk accesses achieving control user-level prohibitively costly controlled placement case study exhibits property suggesting additional policy-manipulation machinery bene cial implementing infolinux discovered number information primitives streamline interactions user-level services infokernel application obtain information memorymapped read-only pages system calls interface depends frequency information needed internal structure application obtain information polling blocking kernel informs information changed case studies illustrate primitives structure remainder paper begin section addressing primary issues building infokernel section compare infokernels previous work section describe details primary implementation infolinux section describe case studies section discuss preliminary experience infobsd conclude section infokernel issues section discuss general issues infokernel begin presenting bene exposing information policies state higher-level services applications discuss fundamental infokernel tensions information exposed bene information transform policy implemented mechanism user-level process understand behavior policy erent conditions behavior policy function algorithms current state export information captures aspects providing information user-level processes manipulate underlying policy adapt behavior manipulate policy exposing algorithms internal state higher-level services running implement policies tailored knowledge services predict decisions make set inputs current state change decisions speci cally service implemented user-level library probes normal inputs reacts controlled manner application system performs prefetching observing sequential access pattern application blocks squelch prefetching intervening read random block policy limits preferences limits ensure measure fairness competing processes preferences workload behavior improve system performance converting source infokernel policy target user-level policy implies limits imposed source policy circumvented preferences biased general service bias preferences source policy overhead incurs disincentive obtaining control system-wide goals system allocation policy takes advantage higher bandwidth outer tracks multi-zone disk giving preference large les outer zone fairness system limits space allocated user zone process wishes allocate small outer tracks exceeded user per-zone quota process bias behavior system padding larger size case process pays time space overhead create large actions match default preferences system enable adaptation secondary bene providing information diffusion applications adapt achieved behavior logical physical levels improved disk performance memory-intensive application amount physical memory process data multiple passes limiting working set avoid thrashing extreme process amount time remaining time slice decide acquire 
contentious lock expects preempted nishing critical section straightforward services directly adapt behavior indirectly manipulate behavior focus challenging issue controlling policy paper tensions design infokernel number design decisions made discuss issues pertaining amount information exposed exposing information process boundaries exposing information adding mechanisms amount information tension designing infokernel decide information exported hand exporting information bene cial priori information higher-level services hand exposing information greatly expands api presented applications destroys encapsulation put simply knowledge power ignorance bliss unfortunate implications application api expanded user-level service control page replacement algorithm page evicted service developed infokernel clock replacement application examines clock hand position bits service moved infokernel pure lru replacement service examine position page lru list perspective user-level service api implies service longer operates correctly signi cantly rewritten perspective xed api discourages developers implementing algorithms constrains evolution application portability infokernel information hidden provide abstractions sake innovation abstractions ciently high level operating system easily convert internal representations abstractions precisely determining correct infokernel abstractions requires experience large number case studies operating systems paper important rst step ning abstractions examining major components operating system cache management placement disk scheduling networking case studies describe abstraction infokernel export represent cache replacement algorithm prioritized list resident pages user-level services ciently determine pages evicted implementation illustrates implementing abstractions existing simple involves lines code process boundaries tension designing infokernel determine information competing processes exposed hand information processes exposed process optimize behavior relative entire system hand information processes process learn secrets harm performance process information processes hidden security privacy data read written contents memory pages information resource usage processes increase prevalence covert channels information higher cost resident page list curious process infer process accessing speci timing open system call curious process infer fast time inode cache infokernel hide information process boundaries performing work resident page list block number removed pages belong calling process issue addresses suitability competing applications running infokernel concern infokernel services encouraged game control harm processes acquire locks performing control potentially competes processes greedy process avoid advisory lock information greedy process acquire fair share resources greedy service pages memory touching evicted steal frames processes infokernel provide mechanisms behavior original albeit costly achieve conference january nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference riedel kallahalla swaminathan aframework evaluating storage system security proceedings usenix symposium file storage technologies fast pages monterey california january rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february infokernel support ruemmler greedy process wilkes disk continually shuf touch ing pages technical blindly report imposing additional hpl- overhead hewlett packard entire laboratories system summary oct infokernel saavedra stresses role smith measuring cache arbitrate tlb resources performance competing effect applications bench-mark runtimes limits ieee transactions existing policies impart computers schindler responsibilities infokernel ganger adequate automated policy disk limits drive characterization suited technical report non-competitive server cmu-cs- carnegie workloads adding mellon mechanisms nal seltzer issue endo determine small kernel add smith mechanisms dealing control disaster surviving simply misbehaved exposing kernel information extensions question proceedings cult answer usenix symposium general operating requires systems design side-by-side comparison implementation desired osdi piece functionality october leave seltzer future work ganger mckusick adding smith mechanism soules stein journal-ing complex versus soft exposing updates information asynchronous meta-data reasons protection file systems consistent proceedings existing policy usenix annual mechanism technical conference pages preferences san diego policy june violate shriver limits gabber policy huang stein mechanism storage explicitly management check web proxies current invocation proceedings violate usenix annual policy technical limits conference system usenix user-level policy pages implemented boston infokernel massachussetts performs june check automatically staelin complexity garcia-molina arises smart filesystems notifying proceedings user reasons usenix winter mechanism technical failure conference cient dallas level texas detail january user stonebraker submit operating system request support database cient management information communications acm july sullivan werthimer bowyer cobb gedye anderson major seti project based project serendip data personal computers proceedings international conference bioastronomy talagala arpaci-dusseau patterson microbenchmark-based extraction localand global disk characteristics technical report csd- california berkeley tweedie future directions theext filesystem proceedings usenix annual technical conference freenix track monterey california june 
request succeed future exposing details mechanism invocation violated policy similar exposing basic policy information task infokernel related philosophies infokernel extensible systems goal tailoring operating system workloads services user-speci policies primary erence infokernel strives evolutionary design realistic discard great body code contained current operating systems infokernel transforms existing operating system suitable building block infokernel approach erence extensible systems application-speci code run protected environment disadvantages advantages disadvantages infokernel exible range policies provide higher overhead indirectly controlling policies userlevel policies voluntarily processes advantage approach infokernel require advanced techniques dealing safety downloaded code software-fault isolation type-safe languages in-kernel transactions open question address simple control provided infokernel cient implement range policies idea exposing information explored speci components instance bene knowing cost accessing erent pages state network connections demonstrated infokernel generalizes concepts compare infokernel philosophy related philosophies detail exokernel open implementation gray-box systems goal exposing information stated exokernels exokernel takes strong position xed high-level abstractions avoided information page numbers free lists cached tlb entries exposed directly exokernel sacri ces portability applications erent exokernels information standard interfaces supplied library operating systems alternately infokernel emphasizes importance allowing operating systems evolve maintaining application portability exposes internal state abstractions systems map data structures philosophy open implementation project similar infokernel philosophy states part implementation details hidden interface mere details details bias performance resulting implementation authors propose volume logical approach access individual les monitored considered hot diffused perle replication fails capture sequentiality multiple small les single directory pursue physical approach alexander replicates segments logical address space disks volume systems good allocating contiguous logical blocks single les directory replicating logical segments identify exploit common access patterns implement access-driven diffusion alexander divides logical address space multiple segments normal operation gathers statistics utilization access patterns segment background thread selects logical segments bene access-driven diffusion diffuses copy drives system subsequent reads writes rst replicas background updates original blocks imap entry block copy date amount disk space allocate performanceoriented replicas presents important policy decision initial policy alexander implements reserve minimum amount space speci sysappears usenix symposium file storage technologies fast tem administrator replicas opportunistically free space array additional replication approach similar autoraid mirrored data autoraid identify 
data considered dead system written contrast d-graid semantic knowledge identify blocks free live-block recovery implement live-block recovery d-graid understand blocks live knowledge correct block live considered dead lead data loss alexander tracks information observing bitmap data block traf bitmap blocks liveness state system reected disk due reordering delayed updates uncommon observe write data block bit set data bitmap account d-graid maintains duplicate copy bitmap blocks sees write block sets bit local copy bitmap duplicate copy synchronized system copy data bitmap block written system conservative bitmap table ects superset live blocks system perform live-block recovery note assume pre-allocation state bitmap written disk subsequent allocation locking linux modern systems ensures technique guarantees live block classi dead disk block live longer situation arise system writes deleted blocks disk implement live-block recovery alexander simply conservative bitmap table build list blocks restored alexander proceeds list copies live data hot spare aspects alexander host aspects implementation required successful prototype discuss length due space limitations found preserving logical contiguity system important block allocation developed mechanisms enable placement directory-based grouping requires sophistication implementation handle deferral writes parent directory block written time block allocation prevents misclassi indirect blocks causing spurious physical block allocation deferred list management introduces tricky issues memory alexander preserves sync semantics returning success inode block writes deferred block writes waiting inode complete number structures alexander maintains imap reliably committed disk preferably good performance buffered small amount non-volatile ram important component missing alexander decision popular read-only directories usr bin replicate widely alexander proper mechanisms perform replication policy space remains unexplored initial experience simple approach based monitoring frequency inode access time updates effective alternative approach administrators directories treated manner interesting issue required change design behavior linux ext partial disk failure process read data block unavailable ext issues read returns failure process block recovery process issues read ext issue read works expected process open inode unavailable ext marks inode suspicious issue request inode block alexander recovered block avoid change system retain ability recover failed inodes alexander replicates inode blocks namespace meta-data collocating data blocks alexander fat surprised similarities found implementing d-graid underneath ext vfat vfat overloads data blocks user data blocks directories alexander defer classi cation blocks manner similar ext implementation instances vfat implementation d-graid differed interesting ways ext version fact pointers located allocation table made number aspects d-graid simpler implement vfat indirect pointers worry ran occasional odd behavior linux implementation vfat linux write disk data blocks allocated freed avoiding obvious common system optimization indicative untuned nature linux implementation served indicator semantic disks wary assumptions make system behavior evaluating alexander present performance evaluation alexander focus primarily linux ext variant appears usenix symposium file storage technologies fast misplaced blocks time sec misplaced blocks remapping remapping close-up close-up figure errors placement gure plots number blocks wrongly laid alexander time running busy hour trace experiment run disks total number blocks accessed trace include baseline measurements vfat system answer questions alexander work correctly time overheads introduced effective access-driven diffusion fast live-block recovery bene expect d-graid complex implementation platform alexander prototype constructed software raid driver linux kernel file systems mount pseudo-device normal disk environment excellent understanding issues involved construction real hardware d-graid system limited ways importantly alexander runs system host applications interference due competition resources performance characteristics microprocessor memory system found actual raid system experiments utilize mhz pentium iii k-rpm ibm disks alexander work correctly alexander complex simple raid systems ensure alexander operates correctly put system numerous stress tests moving large amounts data system problems extensively tested corner cases system pushing situations dif cult handle making system degrades gracefully recovers expected repeatedly crafted microbenchmarks stress mechanisms detecting block reuse handling imperfect information dynamically-typed blocks constructed benchmarks write user data blocks disk slowdown versus raidoperational overheads ext fat create read overwrite unlink figure time overheads gure plots time overheads observed d-graid level versus raid level series microbenchmarks tests run disk systems experiment operations enacted creations operation worst case data data appears valid directory entries indirect pointers cases alexander detect blocks indirect blocks move les directories proper fault-isolated locations verify alexander places blocks disk instrumented system log block allocations addition alexander logs events interest assignment home site inode creation mapping logical block re-mapping blocks homesite receipt logical writes system evaluate behavior alexander workload run workload alexander obtain time-ordered log events occurred system alexander process log off-line number blocks wrongly laid time ran test hours traces found hours examined number blocks misplaced temporarily low blocks report detailed results hour trace observed greatest number misplaced blocks hours examined figure shows results gure parts bottom part shows normal operation alexander capability react block reuse remapping copying blocks correct homesite gure shows alexander quickly detect wrongly blocks remap appropriately number blocks misplaced temporarily total number blocks accessed trace top part gure shows number misplaced blocks experiment assuming remapping occur expected delinquent blocks remain misplaced dip end trace occurs appears usenix symposium file storage technologies fast run-time blocks written seconds total meta uniquedata raidd-graid d-graid ways changing interface clients modules allowing clients anticipated usage outline requirements download code module clients choose module implementation btree linkedlist hashtable approach exposes algorithm employed infokernel address importance exposing state finally relationship infokernels authors work gray-box systems philosophy gray-box systems acknowledges information applications existing operating systems leveraged graybox approach takes extreme position modi applications assume infer information number limitations implementing user-level services gray-box system removed infokernel gray-box system user-level services make key assumptions incorrect ignore important parameters operations performed service infer internal state impose signi overhead web server simulate cache replacement algorithm on-line infer current contents memory finally make correct inference circumstances service observe inputs outputs infokernel retains advantages leveraging commodity operating system user-level services built infokernel robust powerful gray-box system implementation infolinux section describe experience building prototype infokernel based linux infolinux strict superset linux interfaces added expose information control mechanisms policies modi point exercise demonstrate traditional operating system easily converted infokernel result prototype infolinux cient functionality demonstrate higher-level services extend existing linux policies abstractions policy information structure initial version infolinux abstractions key policies linux abstraction composed data structures algorithms enable portability user-level services erent infokernels data structures standardized data structures exported infolinux system calls user-level libraries user-level library accesses kernel memory directly key pages case study abstraction description inforeplace pagelist prioritized list in-memory pages victimlist list pending victim pages infoplace fsregionlist prioritized list disk groups dir allocation infosched diskrequestlist queue disk requests fileblocks list blk numbers inode data infovegas msglist list message segments table case studies infolinux abstractions case study paper present names abstractions employs short description mapped read-only address space memory-mapped interface processes avoid overhead system call rarely interesting data structures scattered kernel memory disk scheduling queue signi restructuring kernel place related information page case studies ned number fundamental infokernel abstractions summarized table detail section found commonality exists abstractions needed disparate policies case essential state information expressed terms prioritized list cases version list exists kernel cases infokernel construct list varied sources information abstract list disk scheduling policy simply 
existing scheduling queue separated device abstract list allocation policy cylinder groups group selected allocation head list constructed combining knowledge cylinder groups picked current state group represent algorithm infolinux prototype exports logical disk scheduling algorithm represented c-look sstf sptf naming method primitive sufcient initial demonstration existing policies infolinux controlled investigating general representation key aspects policy infokernel exports rules determine items list abstraction moved priority user processes predict item inserted list information primitives converting internal format data structures linux general representation required infokernel interface requires careful implementation choices found number information primitives making conversion simpler developer cient run time ers application periodically poll state infokernel polling performed frequently application miss important state infolinux mechanism recording data structure circular ers amortizing overhead system call values noti ers poll state service noti key data structure infolinux mechanism process block speci abstraction changed timers amount time operation takes reveal information infokernel time valuable inferring properties resource autonomy network disk infolinux mechanism add timers kernel return results user processes procedure counters infokernel export estimate state kernel count number times procedure called piece code executed infolinux mechanism add counters speci locations code primitives d-graid d-graid table performance postmark table compares performance d-graid level raidon postmark benchmark row marked d-graid speci level metadata replication rst column reports benchmark run-time column shows number disk writes incurred column shows number disk writes metadata blocks fourth column number unique metadata blocks written experiment run disks misplaced blocks assigned homesite accidentally correcting original misplacement time overheads introduced explore time overheads arise due semantic inference primarily occurs blocks written system creation figure shows performance alexander simple microbenchmark allocating writes slower due extra cpu cost involved tracking fault-isolated placement reads overwrites perform comparably raidthe high unlink times graid fat fat writes data pertaining deleted les processed d-graid newly allocated data implementation untuned infrastructure suffers cpu memory contention host worst case estimates overheads cost d-graid explore overhead metadata replication purpose choose postmark metadata intensive system benchmark slightly modi postmark perform sync deletion phase metadata writes accounted making pessimistic evaluation costs table shows performance alexander degrees metadata replication table synchronous replication metadata blocks signi effect performance metadata intensive workloads sizes postmark range bytes note alexander performs default raidfor lower degrees replication physical block allocation ext contiguous free chunk 
blocks allocate layout sub-optimal small les table shows number disk writes incurred benchmark percentage extra disk writes roughly accounts difference perbandwidth file size access-driven diffusion raidd-graid file-based access-driven diffusion d-graid directory-based access-driven diffusion d-graid file-based d-graid directory-based figure access-driven diffusion gure presents performance d-graid level standard raidunder sequential workload experiment number les size read sequentially total volume data xed d-graid performs implemented smaller dynamic les due kernel instrumentation physical block tool layout dynamic formance instrumentation infokernel replication developer levels easily extra trace writes variables incur metadata blocks overhead tracing count activated number unique user physical process writes preliminary metadata experience blocks absolute difference tool replication overhead levels enabling small information suggests primitives lazy low propagation updates ering metadata variable block replicas typical system idle time operations freeblock infolinux scheduling routine ext greatly getblk reduce incurs performance negligible difference overhead case cost studies added demonstrate complexity power lazy infokernel update approach propagation implemented number replicas case updated studies show d-graid policies incur infolinux converted extra disk writes controllable mechanisms played back examples portion focus traces major minutes policies linux standard cache raidsystem management d-graid placement disk disks scheduling playback networking engine issues case requests studies emphasize times erent speci aspects trace infokernel optional speedup exibility factor control speedup provided implies range idle internal time policies requests mapped reduced general factor abstraction rate speedup factors state information d-graid delivered persecond converting operation throughput cache replacement policy raidutilizing idle time mechanism show trace hide wide range extra cpu target overhead policies fifo lru scaling mru factor lfu operation implemented throughput lagged user-level slightly transforming d-graid system showing placement policy slowdown mechanism show rst one-third infokernel abstraction trace execution ciently general caught capture due important idle details time variety effective policies access-driven diffusion directory allocation show bene ext ffs access-driven diffusion temporal trial locality experiment manipulations perform set disk scheduling sequential tcp reads congestion les control algorithms increasing show size user-level compare policies standard raidstriping frequent noti d-graid cation state access-driven diffusion figure implemented shows results case study experiment present infokernel abstraction gure suitably access-driven represents diffusion underlying sequential policy access larger describe les run export rate abstraction single ciently disk infolinux system present user-level bene library code potential implements parallelism policy access-driven top diffusion performance infokernel abstraction improved reads quantify accuracy directed appears overhead usenix controlling symposium policies file comparing storage infolinux technologies result modeled expectations in-kernel implementation fast reconstruction cases time approach live perfect volume accuracy percentage costs incur reconstruction additional d-graid overhead level common worst theme case d-graid case level studies case idealized overhead raid level controlling policies figure live-block user-level directly recovery depends gure shows time user recover desired failed control disk meshes hot spare preferences d-graid biases level mirrored policy system finally live-block case recovery study lines demonstrate graid usefulness plotted user-level policy worst case showing live data performance improvement spread workload entire compared volume default linux policy case experimental environment compacted smallest experiments contiguous space section employ plotted number recovery erent time machine con idealized gurations raid machine level ghz diffused pentium copies processor disks main memory system note -gb -rpm western case digital arrange les ata ide hard diffused drives machine start mhz experiment pentium reading processor threshold main number memory times investigating -gb -rpm sophisticated ibm policies lzx scsi hard initiate drives access-driven machine diffusion left mhz future pentium work fast main live-block memory recovery -gb explore -rpm ibm potential gxp improvement ata ide hard live-block drive recovery figure intel presents etherexpress recovery pro time ethernet d-graid ports varying netbed amount emulation live environment system data multi-disk machines gure plots disk lines worst case noted case live-block stress recovery erent components worst case system live machines data spread booted disk memory experiments case run multiple compacted times single portion volume averages graph reported variance low live-block cases recovery successful reducing shown file cache recovery replacement erent applications bene erent cache replacement algorithms modifying replacement policy demonstrate exibility extensible systems functionality approximated infokernel environment rst case study user-level library inforeplace demonstrates variety replacement algorithms fifo lru mru lfu implemented top unmodi linux replacement algorithm begin describing intuition cache replacement policy treated mechanism giving replacement control applications case application wishes hot list pages resident memory target policy supports simple lru-replacement policy source policy ensure hot list remains resident user process pages evicted user process accesses page number times source replacement policy increase priority page generally replacement policy converted accessing pages evicted source policy evicted target policy infokernel abstractions support inforeplace user-level library infolinux export information applications determine victim pages operations move pages priority state linux converted form low overhead linux uni page cache q-like replacement policy rst referenced page active queue managed two-handed clock evicted page inactive queue managed fifo provide general representation prioritized list kernel task statements memory-map counter setup track page movement reset counter export victimlist total victimlist abstraction user-level task statements setup simulation framework target policies fifo lru mru lfu check victimlist refresh total inforeplace library table code size cache replacement case study number statements counted number semicolons needed implement victimlist abstraction infolinux inforeplace library user-level shown physical pages pagelist infolinux exports concatenation queues system call information inforeplace examine end queue pages interest drawback pagelist abstraction large number elements imposes signi overhead copying queue user space call made infrequently queue checked infrequently pages evicted user-level library notices infolinux victimlist abstraction pages full queue mechanism quickly determine pages added list infolinux exports estimate rapidly queues changing reporting times items moved inactive queue ciently counting number times key procedures called counter activated service registers interest fast access user-space mapped address space user process counter approximately equal process performs expensive call state pages inactive queue shown top half table victimlist abstraction implemented statements fact half code needed setup memory-mapped counter user-level policies victimlist abstraction user-level inforeplace library frequently poll pages eviction obtain list pages time pages disk evicted half full note target policy difference inforeplace accesses worst case move case times active list difference suggests periodic roles disk inforeplace reorganization track pages speed recovery resident moving target live data policy localized simplicity portion inforeplace library bene exports expect set wrappers d-graid applications demonstrate call improved availability open alexander read write failures figure shows availability performance observed process randomly accessing les running d-graid raidto ensure fair comparison d-graid raidlimit reconstruction rate gure shows reconstruction volume live data completes faster graid compared raids extra failure occurs availability raiddrops d-graid continues availability surprisingly restore raidstill fails les linux retry inode blocks fail remount required availability ops succeed file throughput files sec time sec raid availability raid throughput d-graid availability d-graid throughput operation failure firstfailure reconcomplete failureafter recon secondfailure restore offailed disk re-mount reconcomplete figure availability pro gure shows operation d-graid level raid failures array consists data disks hot spare rst failure data reconstructed hot spare d-graid recovering faster raid failures occur raid loses les d-graid continues serve les workload consists read-modify-writes les randomly picked working set raidreturns full availability complex implementation brie quantify implementation complexity alexander table shows number statements required implement components alexander table core system inferencing module ext requires lines code counted number semicolons core mechanisms d-graid contribute lines code rest spent hash table avl tree wrappers memory management compared tens lines code comprising modern array rmware added complexity d-graid signi discussion section rst compare semantic-disk based approach alternative methods implementing graid discuss concerns commercial feasibility semantic disk systems alternative approaches semantic disk based approach ways implementing d-graid trade-offs similar modern processors innovate beneath unchanged instruction sets semantic disk level implementation facilitates ease deployment inter-operability unchanged client infrastructure making pragmatic cost approach complexity rediscovering semantic knowledge tolerant inaccuracies alternative approach change interface systems storage convey richer information layers instance storage system expose failure boundaries system appears usenix symposium file storage technologies fast semicolons total d-graid generic setup fault-isolated placement physical block allocation access driven diffusion mirroring live block recovery internal memory management hashtable avl tree file system speci sds inferencing ext sds inferencing vfat total table code size alexander implementation number lines code needed implement alexander shown rst column shows number semicolons column shows total number lines including white-spaces comments system explicitly allocate blocks fault-isolated manner alternatively system tag write logical fault-container storage system implement faultisolated data placement techniques intrusive existing infrastructure software base conceivably complex approach object-based storage interface considered makes boundaries visible storage layer objectbased interface semantically-smart technology relevant discover relationships objects instance inferring directory object points set objects collocated commercial feasibility nition d-graid semantically-smart storage systems detailed knowledge system embedding higher degree functionality storage system leads concerns commercial feasibility systems rst concern arises placing semantic knowledge disk system ties disk system intimately system system on-disk structure storage system change issue problematic on-disk formats evolve slowly reasons backwards compatibility basic structure ffs-based systems changed introduction period twenty years linux ext system introduced roughly exact layout lifetime nally ext journaling system backwards compatible ext on-disk layout extensions freebsd system backwards compatible evidence storage vendors maintain support software speci system emc symmetrix storage system software understand format common systems concern storage system semantic knowledge system interacts fortunately large number systems lseek linux procedures shrink cache macro del page inactive list lfumrulrufifo percent missing pages target replacement algorithm inforeplace inaccuracy lfumrulrufifo time read target replacement algorithm inforeplace overheads misc sim refresh check figure accuracy overhead inforeplace fifo lru mru lfu implemented top q-based replacement algorithm linux bar graph left shows inaccuracy inforeplace inaccuracy percentage pages memory workload ends bar graph shows average overhead incurred read write time divided time check victimlist abstraction refresh pages evicted simulate target replacement algorithm perform miscellaneous setup experiments run machine close system calls library tracks pages accessed explicit calls infolinux expanded return access information page process address space read write inforeplace library rst performs simulation target replacement algorithm determine speci page belongs page queue inforeplace victimlist pages high priority eviction accesses nally library wrapper performs requested read write returns basic steps implemented fifo lru mru lfu top linux q-based replacement algorithm bottom half table shows amount code needed implement inforeplace thousand statements required code straightforward bulk simulation erent replacement policies overhead accuracy evaluate overhead accuracy 
infokernel approach run synthetic workload speci cally crafted stress choices made replacement algorithms workload accesses large times size memory touching blocks initial access order recency frequency attributes block blocks evicted depends attributes replacement policy considers measure accuracy target policy end run comparing actual contents memory expected contents figure shows accuracy overhead implementing algorithms infolinux graph run time workload run time pinrange hit rate level index hit rate level index pinrange figure workload bene inforeplace graph left depicts run-time synthetic database index lookup workloads systems bars labeled show run time index lookups stock linux kernel bars labeled pinrange show run time specialized pinrange policy infolinux x-axis varies workload speci cally depth fan-out index implies index depth fan-out graph shows details pinrange speeds performance workload showing hit rate erent levels -level index experiments run machine left shows inaccuracy inforeplace ned percentage pages resident memory target replacement algorithm metric pages memory target policy pages resident inaccuracy general inaccuracy inforeplace low inaccuracy mru highest roughly resident pages preferences mru highly con ict emulating mru inforeplace constantly probe pages memory graph figure shows overhead implementing policy terms increase time read write operation time broken time check victimlist abstraction probe pages evicted simulate target replacement algorithm perform miscellaneous setup overhead inforeplace generally low read write call exception pure lfu incurs high simulation overhead roughly call due logarithmic number operations required read write order pages frequency assuming cost missing cache high overhead emulating lfu pays miss rate reduced workload bene database researchers observed policies provided general-purpose operating systems deliver suboptimal performance database management systems demonstrate utility inforeplace provide cache replacement policy inspired dbmin suited database index lookups indices dbms systems typically organized trees replacement policy nodes root tree memory pages higher probability accessed simplicity policy pinrange assumes index allocated root head leaves end pinrange pages preference based set pages rst bytes large lru queue remaining pages smaller queue pinrange simple implement requiring roughly statements inforeplace library demonstrate bene inforeplace repeated index lookups compare workload run-time pinrange versus default linux replacement policy note fairly sophisticated policy introduced database community speci cally handle types access patterns result preference pages top tree experiments run synthetic workloads emulating lookups index trees levels fan-out machine memory pinrange con gured prefer rst main memory graph left figure shows pinrange improves run-time erent trees illustrate pinrange improves performance graph figure plots hit rate function index level tree levels fan-out graph shows pinrange noticeably improves hit rate sixth level tree slightly reducing hit rate lowest seventh level tree improvement total hit rate results decrease run-time includes approximately seconds overhead inforeplace library summary case study shows replacement policies implemented information exposed victimlist abstraction ciently exible build variety classic replacement algorithms compares favorably direct in-kernel implementations cao work applications easily invoke policies combination lru mru strategies system difculty emulating behavior wider range policies lfu case study illustrates care ciently perform conversion internal state general victimlist abstraction inforeplace demonstrates target replacement algorithms similar source algorithm implemented accuracy overhead file directory placement o-intensive applications database systems web servers bene controlling layout data disk systems provide type control applications placement les demonstrate power extensible systems gray-box systems ability group related objects nextgeneration storage systems describe infoplace placement service case study demonstrate points placement functionality implemented lower kernel task statements collect region stats convert stats fsregionlist ext temporal data ffs export fsregionlist total fsregionlist abstraction user-level task statements framework setup directory allocation file allocation fill regions cache directories total infoplace library table code size placement case study number statements counted number semicolons needed implement fsregionlist abstraction infolinux shown kernel layout policies ext temporal data block ffs infoplace library overhead infokernel gray-box techniques erent kernel policies mapped common infokernel representation enabling innovation infokernel abstractions describe placement policy abstraction prioritized list regions disk fsregionlist list ordered beginning region allocation data operations performed reduce priority region infokernels freedom disk regions erently demonstrate generality abstraction explore erent kernel placement policies map representation begin default placement policy ext linux ffs-based systems maintain locality disk allocation cylinder group block group natural mapping region cylinder group abbreviated simply group placement policy ext les group parent directory directory group free data blocks average number free inodes simplicity focus abstracting directory placement policy placement exporting directory fsregionlist straightforward previous case study case priority group derived directory placement algorithm current state group speci cally free inodes free data block count map parameters precise ordering infolinux rst places cylinder groups categories average number free inodes average infolinux sorts category number free data blocks concatenates nal ordered list step infolinux describe erent system operations decrease priority group fsregionlist altering free inode data block counts ext placement algorithm decrease priority group future allocations infolinux considers cases groups average number free inodes infolinux reports creations zero-length les performed number free inodes average average infolinux reports creation block created number additional free data blocks group compared list understand challenges controlling directory placement top erent placement policies implemented variety policies linux kernel original ffs algorithm group fewest directories average number free inodes chosen case mapping prioritized list identical ext number directories data blocks simple placement algorithm selects group free data blocks case prioritized list simply orders groups number free data blocks finally temporal allocation cylinder group chosen previous group full algorithm prioritized list simply distance group hot group temporal allocation operation lowers group priority create empty les block number free inodes number free data blocks group choice made infolinux min amount code required implement fsregionlist abstraction shown top half table exporting system abstraction involves non-trivial amount code list explicitly exist linux expected creating fsregionlist complex directory placement policies ext ffs requires code straightforward policies temporal data user-level policies steps performed infoplace similar gray-box version place brie describe place place assumes running ext system group parent directory simple trick allocate named group place creates directory previously allocated group place renames speci user infoplace steps les controlling placement directories complicated place infoplace operation place repeatedly creates directory checks inode number directory allocated directory correct group steps move groups closer state target group chosen eventually place succeeds comparison infoplace begins 
obtaining fsregionlist priority list target group rst infoplace allocates directory veri infolinux directory created desired group race occur activity system target list infoplace performs designated time imbalance thousands inodes controlling ext place infoplace time percentage imbalance comparing allocation algorithms data ext ffs temporal figure overhead placement infoplace graph left compares overhead controlling allocation infoplace versus place information current state system time perform allocation target group shown y-axis target group fewest free inodes inode imbalance ned number inodes allocated groups identical number inodes graph compares overhead erent directory allocation policies linux data chooses group free data blocks ext default ext policy ffs original ffs policy temporal allocates directories hot group group completely lled nition imbalance varies allocation policies x-axis scenarios percentage maximum imbalance cylinder groups algorithm experiments run machine number type operations creating zero-length dummy les groups preceding re-obtains list groups repeating process target head successfully complete infoplace cleans removing zero-length dummy les place infoplace expensive directory allocation algorithm time user speci location directory common operation retain cache directories erent groups user speci target group libraries simply rename existing directories group target group cache empty explicit control needed analysis server traces typical day directories created upper bound entries added directory cache nighttime cron job amount code required implement infoplace library shown bottom half table overhead accuracy rst experiment shows information state system helps infoplace perform controlled placement ciently gray-box version place rst graph figure show time overhead place directory target group function imbalance imbalance ned number items inodes data blocks lled non-target groups target group move front list show accuracy versions provide complete accuracy expected versions overhead control increases inode imbalance increases representing amount ght placement preferences ext policy graph dramatically shows bene information overhead place times higher infoplace experiments shown graph figure illustrate cost infoplace depends directory placement algorithm key predicting cost correctly supported cover large fraction usage population semantic storage system system support storage system detect system conform expectations turn special functionality case d-graid revert normal raid layout detection simple techniques observing system identi partition table nal concern arises processing required disk system major issue general trend increasing disk system intelligence processing power increases disk systems substantial computational abilities modern storage arrays exhibit fruits moore law emc symmetrix storage server congured processors ram related work d-graid draws related work number areas including distributed systems traditional raid systems discuss turn distributed file systems designers distributed systems long ago realized problems arise spreading directory tree machines system walker discuss importance directory namespace replication locus distributed system coda mobile system takes explicit care regard directory tree speci cally cached coda makes cache directory root directory tree coda guarantee remains accessible disconnection occur interesting extension work reconsider host-based inmemory caching 
availability mind slice route namespace operations les directory server recently work wide-area systems re-emphasized importance directory tree pangaea system aggressively replicates entire tree root node accessed island-based system points fault isolation context widearea storage systems island principle similar fault-isolated placement d-graid finally systems past place entire single machine similar load balancing issues problem dif cult space due constraints placement block appears usenix symposium file storage technologies fast migration simpler centralized storage array traditional raid systems draw long history research classic raid systems autoraid learned complex functionality embedded modern storage array background activity utilized successfully environment afraid learned exible trade-off performance reliability delaying updates raid research focused redundancy schemes early work stressed ability tolerate single-disk failures research introduced notion tolerating multiple-disk failures array stress work complementary line research traditional techniques ensure full system availability number failures d-graid techniques ensure graceful degradation additional failures related approach parity striping stripes parity data achieve fault isolation layout oblivious semantics data blocks level redundancy irrespective importance meta-data data multiple failures make entire system inaccessible number earlier works emphasize importance hot sparing speed recovery time raid arrays work semantic recovery complementary approaches finally note term graceful degradation refer performance characteristics redundant disk systems failure type graceful degradation discuss paper systems continues operation unexpected number failures occurs conclusions robust system continues operate correctly presence class errors robert hagmann d-graid turns simple binary failure model found storage systems continuum increasing availability storage continuing operation partial failure quickly restoring live data failure occur paper shown potential bene d-graid established limits semantic knowledge shown successful d-graid implementation achieved limits simulation evaluation prototype implementation found d-graid built underneath standard block-based interface system modi cation delivers graceful degradation live-block recovery access-driven diffusion good performance conclude discussions lessons learned process implementing d-graid limited knowledge disk imply limited functionality main contributions paper demonstration limits semantic knowledge proof implementation limitations interesting functionality built inside semantically-smart disk system semantic disk system careful assumptions system behavior hope work guide pursue similar semantically-smart disks easier build systems reorder delay hide operations disks reverse engineering scsi level dif cult small modi cations systems substantially lessen dif culty system inform disk believes system structures consistent ondisk state challenges disk lessened small alterations ease burden semantic disk development semantically-smart disks stress systems unexpected ways file systems built operate top disks behave d-graid speci cally behave part volume address space unavailable heritage inexpensive hardware linux systems handle unexpected conditions fairly exact model dealing failure inconsistent data blocks missing reappear true inodes semantically-smart disks push functionality storage systems evolve accommodate detailed traces workload behavior invaluable excellent level detail traces simulate analyze potential d-graid realistic settings traces per-process information anonymize extent pathnames included trace utilize study remaining challenge tracing include user data blocks semantically-smart disks sensitive contents privacy concerns campaign encounter dif cult overcome acknowledgments anurag acharya erik riedel yasushi saito john bent nathan burnett timothy denehy brian forney florentina popovici lakshmi bairavasundaram insightful comments earlier drafts paper richard golding excellent shepherding anonymous reviewers thoughtful suggestions greatly improved content paper finally appears usenix symposium file storage technologies fast computer systems lab providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation acharya uysal saltz active disks programming model algorithms evaluation asplos viii san jose october alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering isca pages anderson chase vahdat interposed request routing scalable network storage acm transactions computer systems february bitton gray disk shadowing vldb pages los angeles august boehm weiser garbage collection uncooperative environment software practice experience september burkhard menon disk array storage system reliability ftcspages toulouse france june chapin rosenblum devine lahiri teodosiu gupta hive fault containment shared-memory multiprocessors sosp december chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks usenix june dowse malone recent filesystem optimisations freebsd freenix monterey june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge self-organizing disk controller usenix winter january ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december ganger mckusick soules patt soft updates solution metadata update problem file systems acm tocs ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement hicss gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka costeffective high-bandwidth storage architecture asplos viii october gray computers stop international conference reliability distributed databases june gray horst walker parity striping disc arrays low-cost reliable storage acceptable throughput proceedings international conference large data bases vldb pages brisbane australia august gribble robustness complex systems hotos viii schloss elmau germany hagmann reimplementing cedar file system logging group commit sosp november holland gibson siewiorek fast on-line failure recovery redundant disk arrays ftcsfrance hsiao dewitt chained declustering availability strategy multiprocessor database machines international data engineering conference ibm serveraid recovering multiple disk failures http ibm qtechinfo migrhtml felten wang singh archipelago islandbased file system highly scalable internet services usenix windows symposium august katcher postmark file system benchmark technical report trnetwork appliance oct keeton wilkes automating data dependability proceedings acm-sigops european workshop pages saint-emilion france september kistler satyanarayanan disconnected operation coda file system acm tocs february mckusick joy lef fabry fast file system unix acm tocs august menon mattson comparison sparing alternatives disk arrays isca gold coast australia microsoft corporation http microsoft hwdev december orji solworth doubly distorted mirrors sigmod washington park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- princeton november patterson gibson katz case redundant arrays inexpensive disks raid sigmod june patterson availability maintainability performance focus century key note fast january popek walker chow edwards kline rudisin thiel locus network transparent high reliability distributed system sosp december reddy banerjee gracefully degradable disk arrays ftcspages montreal canada june riedel gibson faloutsos active storage largescale data mining multimedia proceedings international conference large databases vldb york york august riedel kallahalla swaminathan framework evaluating storage system security fast pages monterey january rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february rowstron druschel storage management caching past large-scale persistent peer-to-peer storage 
utility sosp banff canada october ruemmler wilkes disk shuf ing technical report hpl- hewlett packard laboratories saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area system osdi boston december savage wilkes afraid frequently redundant array independent disks usenix pages san diego january sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems fast san francisco march tweedie future directions ext filesystem freenix monterey june wang anderson patterson virtual log-based file systems programmable disk osdi orleans february wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wolf placement optimization problem practical solution disk file assignment problem sigmetrics pages berkeley 
imbalance terms inodes data blocks overhead function imbalance cost creating needed items placement policies non-target groups primarily inodes ext ffs temporal cost policies data blocks data block algorithm cost graph show imbalance expected occur workload running placement policy expect imbalance ext ffs data block policies tend low algorithms balance usage disk imbalance temporal allocation tend high allocations group overhead controlling layout constant imbalance policies typical imbalance vary polices workload bene demonstrate utility infoplace library show bene results les accessed time reorganized disk shown bene general block-level le-level reorganization show simply standard tool modi advantage placement control provided infoplace speci cally modify tar program place les directories unpacked single localized portion disk demonstrate bene optimization unpack large archive case entire tree linux documentation project size unpacked directory tree roughly les directories run experiment machine memory infoplace initialized pre-built cache directories unmodi tar utility running linux takes seconds complete average enhanced tar completes unpacking roughly faster seconds bene achieved slightest modi cations tar statements added call infoplace library summary case study demonstrates erent directory placement policies implemented mapped infokernel abstraction infoplace takes initial steps showing abstract algorithm expressing operations lower priority group prioritized list case study shows overhead control strict function target end state ers desired native system placement policy finally study demonstrates standard utilities bene control provided infoplace library kernel task statements setup export diskrequestlist wait diskrequestlist change total diskrequestlist user-level task statements setup misc diskrequestlist issue request total infoidlesched library setup misc disk model pick background request total infofreesched library table code size disk scheduling case study number statements counted number semicolons needed implement diskrequestlist abstraction infolinux shown counts user-level libraries infoidlesched infofreesched disk scheduling researchers demonstrated applications bene advanced disk scheduling algorithms traditional sstf c-look algorithms case study demonstrate exposing information disk scheduler implement scheduling policies top infolinux speci cally show idle disk scheduler infoidlesched limited freeblock disk scheduler infofreesched implemented user-level libraries top infolinux infokernel abstractions describe disk scheduling policy infokernel exports requests disk scheduling queue cient detail disk scheduling algorithm predict queue request infolinux system calls obtain diskrequestlist processes block diskrequestlist exports scheduling algorithm c-look amount code needed export diskrequestlist reported table addition infofreesched detailed information overheads erent disk operations infofreesched timing primitives infolinux obtain coarse disk model times successive requests disk observed recorded linear block distance requests key index model shown capture seek head switch costs probabilistically capture rotational latency small enhancements needed capture aspects modern disks zoning user-level policies amount code scheduling policies shown table infoidlesched simple disk scheduling algorithm process schedule requests disk idle infoidlesched simply checks diskrequestlist scheduling queue remains empty threshold amount time infoidlesched issues single request queue empty infoidlesched waits state queue change waking items removed recheck queue infofreesched complex freeblock scheduler freeblock scheduling periods rotational latency disk lled data transfers words background tra serviced disk head flexibility manageability performance grid storage appliance john bent venkateshwaran venkataramani nick leroy alain roy joseph stanley andrea arpaci-dusseau remzi arpaci-dusseau miron livny department computer sciences wisconsin-madison abstract present nest flexible software-only storage appliance designed meet storage grid nest key features make well-suited deployment grid environment nest generic data transfer architecture supports multiple data transfer protocols including gridftp nfs easy addition protocols nest dynamic adapting on-the-fly runs effectively wide range hardware software platforms nest grid-aware implying features integration grid storage space guarantees mechanisms resource data discovery user authentication quality service part nest infrastructure introduction data storage movement increasing importance grid time scientific applications evolved process larger volumes data throughput inextricably tied timely delivery data usage grid evolves include commercial applications data management central today data management aspects performance long focus storage systems research recent trends factors including reliability availability manageability relevant argue manageability dominant criterion evaluating storage solutions cost storage management outweighs cost storage devices factor potential solution storage management problem specialized storage devices appliances pioneering products filers network appliance reduce burden management specialization specifically storage appliances designed solely serve files clients toaster designed solely toast results convincing field testing network appliance filers shown easier manage traditional systems reducing operator error increasing system uptime considerably storage appliances natural match storage grid easy manage provide high performance number obstacles prevent direct application commercial filers grid environment commercial storage appliances inflexible protocols support defaulting common local area unix windows environments nfs cifs filers readily mix world-wide shared distributed computing infrastructure non-standard specialized grid protocols data transfer commercial filers expensive increasing cost raw cost disks factor ten greater storage appliances missing features crucial integration grid environment ability interact larger-scale global scheduling resource management tools overcome problems bring appliance technology grid introduce nest open-source user-level software-only storage appliance compared current commercial storage appliances nest primary advantages flexibility cost grid-aware functionality briefly discuss advantages detail nest flexible commercial storage appliances nest generic data transfer architecture concurrently supports multiple data transfer protocols including gridftp nfs nest framework protocols added grid evolves nest open-source softwareonly appliance low-cost alternative commercial storage appliances expenses incurred raw hardware costs disks nest software-based appliance introduces problems traditional appliances encounter nest run hardware tailored tested nest ability adapt characteristics moving requests impacting foreground tra lumb implement freeblock scheduling disk rmware simpli service time prediction subsequent work freeblock scheduling implemented underlying hardware operating system allowing nest deliver control flow data flow dispatcher transfer managerstorage manager concurrency models physical storage chirp http nfsgrid ftpftp common protocol layer physical network figure nest software design diagram depicts nest major components protocol layer storage manager transfer manager dispatcher control data flow paths depicted high performance retaining ease management benefits storage appliances finally nest grid-aware key features storage space guarantees mechanisms resource data discovery user authentication quality service fundamental part nest infrastructure functionality enables nest integrated smoothly higher-level job schedulers distributed computing systems rest paper organized section describes design nest protocol layer mediates interaction clients section section describes transfer manager responsible monitoring scheduling concurrency section describes storage layer manages actual physical storage system usage nest traced section evaluation presented section comparisons related work section conclusions drawn section design overview grid storage appliance nest mechanisms file directory operations resource management implementation provide mechanisms heavily dependent nest modular design shown figure major components nest protocol layer storage manager transfer manager dispatcher briefly examine components separately show work tracing client interaction component descriptions protocol layer nest connectivity network client interactions freebsd mediated kernel user-level scheduling clients testbed implementing communicate freeblock scheduling nest top infolinux supported presents file transfer challenges protocols including user http speci restricted subset nfs set infofreesched ftp convert gridftp disk chirp addresses native protocol conversion nest performed role fileblocks protocol infokernel layer interface transform predicting specific protocol newly written blocks client allocated common disk request interface complex understood infofreesched schedules components read nest trafc refer limited setup virtual protocol connection tasks describe raid scrubbing motivation virus detection multiple protocol support backup section infofreesched dispatcher main complete control scheduler macrorequest requests router disk system scheduling queue responsible requests controlling flow ordered information infofreesched choose components examines background request client request inserted received protocol describe layer step routes appropriately detail list storage background requests transfer infofreesched manager data movement knowledge requests scheduling algorithm transfer manager c-look predict requests background resource request management directory inserted operation requests scheduling queue handled infofreesched storage manager determined dispatcher request periodically consolidates inserted information resource requests data availability infofreesched calculates nest background publish request information harm classad harm determined global indexing scheduling system disk timing storage table manager linear main block distance responsibilities virtualizing requests controlling physical storage machine background underlying request local impact filesystem raw time disk physical memory foreground request storage system allowed directly proceed executing non-transfer optimization requests infofreesched implementing schedules enforcing background access request control managing guaranteed impact storage space foreground tra form lots lots infofreesched discussed blocks waiting detail noti section infolinux storage state operations execute disk quickly queue changed order infofreesched milliseconds wakes chosen rechecks simplify background design requests storage manager serviced overhead requests accuracy execute evaluate synchronously overhead placing responsibility disk scheduling policy dispatcher user-level ensure storage requests complicated infofreesched serialized policy executed stress storage infofreesched manager random-i thread-safe workload schedule transfer disk manager idle foreground tra consists processes continuously reading small les chosen uniformly random single background process reads random blocks disk keeping requests outstanding read performed infofreesched incurs overheads roughly obtain diskrequestlist infolinux approximately background request examined determine issued disk infofreesched examines requests background queue overhead induced negligible sum compared multi-millisecond disk latencies workload bene demonstrate utility infoidlesched show ability expose idle queue system activity traces labs foreground tra run minutes 
trace starting december rst created directory structure les trace run issue background tra stream blocks disk sequentially shown figure infoidlesched support background requests idlesched bandwidth infoidlesched background foreground freesched bandwidth infofreesched background foreground figure workload bene infoidlesched infofreesched graph left shows performance foreground background tra infoidlesched leftmost bar shows foreground tra competing background tra middle bar competing tra standard linux rightmost bar infoidlesched graph shows similar graph infofreesched workloads erent graphs text experiments run machine signi cantly degrade foreground performance read requests trace achieve background tra grabs infoidlesched support background tra limited desired foreground read requests achieve bandwidth background tra obtains background request stream induce small decrease foreground performance infoidlesched library aggressive idle scheduler reduce overhead reduce background bandwidth achieved demonstrate ability infofreesched free bandwidth random-i workload overhead experiment figure random foreground tra isolation achieves background requests added support infofreesched foreground tra harmed proportionately achieving background tra achieves background requests infofreesched foreground tra receives background tra obtains free bandwidth summary case studies stress infokernel approach infolinux user-level processes uence decisions disk scheduler ordering requests disk queue result user-level policies decide perform disk request time cult implement freeblock scheduler top system due culty predicting write tra system investigate future work case study shows importance infolinux primitives blocking state abstraction timing duration operations kernel task statements rtt timers wait wake export msglist total msglist abstraction user-level task statements setup main algorithm error handling total infovegas library table code size networking case study number statements counted number semicolons needed implement tcp msglist abstraction infolinux shown upper table lower table presents code size implementing congestion-control policy infovegas user-level networking networking research shown variations tcp algorithm superior erent circumstances erent controls data flow nest specifically transfers data protocol connections allowing transparent threeand four-party transfers file data transfer operations managed asynchronously transfer manager synchronously approved storage manager transfer manager concurrency models threads processes events schedules transfer models scheduling policies preferential scheduling scheduling optimizations responsibility transfer manager discussed section client interaction examine components function tracing sequence events interacting client case client creates directory nontransfer request inserts file directory transfer request client initially connects nest request create directory dispatcher wakes asks protocol layer receive connection depending connecting port protocol layer invokes handler protocol handler authenticates client parses incoming request common request format returns virtual protocol connection dispatcher dispatcher asks storage manager create directory checking access permissions storage manager synchronously creates directory sends acknowledgment back client dispatcher virtual protocol connection point dispatcher assumes responsibility client listens requests channel client sees directory created successfully requests permission send file dispatcher invokes virtual protocol connection receive request queries storage manager storage manager transfer returns virtual protocol connection transfer written dispatcher passes connections transfer manager stops listening client channel sleeps waiting client request transfer manager free schedule queue request request scheduled transfer manager past information predict concurrency model provide service passes connection selected model transfer continues chosen concurrency model transfers data client connection storage connection performing acknowledgment client desired finally transfer status returned transfer manager dispatcher sections describe important aspects nest motivate importance supporting multiple communication protocols virtual protocol layer describe transfer manager adapts client workload underlying system pick concurrency model performance show transfer manager apply scheduling policies connections fourth explain role storage guarantees nest explain storage manager implements functionality protocol layer supporting multiple protocols fundamental requirement storage appliances grid standardization common protocols globus toolkit diversity reign community widespread fastmoving grid wide-area transfers conducted gridftp local-area file access dominated nfs afs cifs protocols multiple protocols 
supported nest virtual protocol layer design implementation virtual protocol layer clients communicate nest preferred file transfer protocol shields components nest detail protocol allowing bulk nest code shared protocols virtual protocol layer nest virtual file system vfs layer operating systems alternative approach single nest server virtual protocol layer implement separate servers understand individual protocol run simultaneously refer approach bunch servers jbos relative advantage jbos servers added upgraded easily immediately implementation workloads protocol nest incorporating implemented small upgraded variations protocol sending algorithm effort case protocol study operations show mapped tcp congestion control algorithm nest common exported framework user-level processes manipulate advantages behavior single speci server cally outweigh infovegas implementation penalty show tcp number vegas reasons implemented single server top enables tcp complete reno control algorithm underlying linux system similar research server network give protocols preferential service userlevel requests infokernel infrastructure protocols enables bene users shortened development protocols cycle easier debugging single interface improved tasks stability infokernel administering abstractions configuring manipulate nest congestion simplified control line algorithm main storage abstraction appliance infokernel philosophy msglist single list server optimizations packet part packet infolinux system exports state transfer manager waiting concurrency acknowledged model dropped applied protocols round-trip fourth time single acknowledged server variables memory snd footprint una snd considerably nxt speci smaller finally tcp implementation rfc penalty exported reduced derived protocol message implementation list nest tcp leverage reno existing implementations record implement gridftp server-side libraries provided globus toolkit sun rpc package rpc communication nfs point implemented file transfer protocols nest http subset nfs ftp gridftp nest native protocol chirp experience request types protocols similar directory operations create remove read file operations read write put remove query fit easily virtual protocol abstraction interesting exceptions instance nfs protocol lookup mount request chirp protocol supports lot management plan include grid-relevant protocols nest including data movement protocols ibp resource reservation protocols developed part global grid forum expect protocols added implementation effort focused mapping specifics protocol common request object format protocols require additions common internal interface authentication mechanism protocol specific protocol handler performs authentication clients drawback approach devious protocol handler falsify client authenticated grid security infrastructure gsi authentication chirp gridftp connections protocols allowed anonymous access transfer manager heart data flow nest round-trip time message high resolution timer times gathered infolinux timing primitives user-level policies basic intuition infovegas calculates target congestion window vcwnd msglist important parameters tcp vegas minrtt basertt derived information infovegas ensures forwards target vcwnd message segments time underlying infokernel finally infovegas blocks state message queue message acknowledged point infovegas send segment adjust calculation target vcwnd amount code implement functionality shown table overhead accuracy experiments verify infovegas behaves similarly in-kernel implementabandwidth queue size packets macroscopic behavior vegasinfovegas reno figure accuracy infovegas macroscopic behavior emulation environment figure experiments emulate network bandwidth delay vary router queue size x-axis y-axis reports bandwidth achieved linux vegas infovegas reno tion vegas linux macroscopic microscopic levels figure shows infovegas achieves bandwidth similar vegas variety network con gurations router queue size changed numbers illustrate space queue decreases reno unable obtain full bandwidth due packet loss infovegas vegas achieve full bandwidth queue space desired figure illustrates behavior infovegas time compared reno in-kernel vegas desired cwnd derived infovegas time closely matches vegas ering signi cantly reno finally infovegas accurately implements vegas functionality user-level low overhead measurements show cpu utilization increases infovegas compared approximately in-kernel vegas workload bene illustrate bene infovegas prototype clustered server specifically nfs storage server con gured front-end node handles client requests backend storage units machines connected switch runs infolinux back-end storage units continuously perform work background replicating important les disks performing checksums data reorganizing les disk ideally background tasks interfere foreground requests front-end workload stresses network front-end handles nfs requests les cached memory back-end node background replicator process replicates les back-end node back-end node results figure show contention network controlled infovegas streams contend network link bandwidth shared approximately equally foreground tra achieves replication process infovegas background tra interferes minimally foreground tra background tra obtains packets time reno cwnd time infovegas vcwnd cwnd diff time vegas cwnd diff figure accuracy infovegas microscopic behavior behavior reno infovegas in-kernel vegas compared time network con guration sender running infolinux receiver running stock linux machines acting routers single network exists source destination machine passes emulated bottleneck delay maximum queue size packets rst graph shows cwnd calculated reno graph shows infovegas cwnd exported reno derived target vcwnd derived parameter graph shows cwnd calculated linux native vegas implementation experiments run machines netbed testbed foreground tra achieves full line rate gure shows cpu utilization machines experiments revealing small additional cost infovegas service summary infovegas stresses limits infokernel control service react quickly frequent events occur inside kernel receiving acknowledgment overhead handling event circumstances infovegas pays overhead reduces bandwidth generally congestion control algorithm implemented infokernel viewed base sending mechanism aggressive policy allowed algorithm speci limit preference network resource congestion control policies built top infolinux send lower rate exposed primitive tcp friendly discussion brie compare user-level policies explored case studies discussion centers fact case study categories depending user-level process transfer manager transfer manager responsible moving data disk network request transfer manager protocol agnostic machinery developed manager generic moves data protocols highlighting advantages nest design mount technically part nfs protocol nest mount handled nfs handler multiple concurrency models inclusion grid environment mandates support multiple on-going requests nest provide means supporting concurrent transfers single standard concurrency operating systems platforms choice threads processes cases events making decision difficult fact choice vary depending workload requests hit cache perform events bound perform threads processes avoid leaving decision administrator avoid choosing single alternative perform poorly workloads nest implements flexible concurrency architecture nest supports models concurrency threads processes events future plan investigate advanced concurrency architectures seda crovella experimental server deliver high performance nest dynamically chooses architectures choice enabled distributing requests architectures equally monitoring progress slowly biasing requests effective choice scheduling multiple outstanding requests nest nest selectively reorder requests implement scheduling policies scheduling multiple concurrent transfers server decide resources dedicate request basic strategy service requests first-come first-served fcfs manner nest configured employ transfer manager control on-going requests scheduling policies nest supports proportional share cache-aware scheduling addition fcfs proportional-share scheduling deterministic algorithm fine-grained proportional resource allocation previously cpu scheduling network routers current implementation nest administrator proportional preferences protocol class nfs requests bandwidth gridftp requests future plan extend provide preferences per-user basis byte-based strides scheduling policy accounts fact requests transfer amounts data nfs client reads large file entirety issues multiple requests http client reading file issues give equal bandwidth nfs requests http requests transfer manager schedules nfs requests times frequently ratio average file size nfs block size nest proportional-share scheduling similar bandwidth request throttling module apache proportional-share scheduling nest offers flexibility schedule multiple protocols apache requestthrottling applies http requests apache server processes applied traffic streams jbos environment cache-aware scheduling utilized nest improve average client perceived response time server throughput modeling kernel buffer cache gray-box techniques nest predict requested files cache resident schedule requests files fetched secondary storage addition improving client response time approximating shortest-job scheduling scheduling policy improves server throughput reducing contention secondary storage earlier work examined cache-aware scheduling focus web workloads independence transfer manager virtual protocol layer clear policy works protocols illustrates major advantage nest jbos optimizations transfer code immediately realized protocols reimplemented multiple servers storage manager protocol layer multiple types network connections channeled single flow storage manager designed virtualize types physical storage provide enhanced functionality properly integrate grid environment specific roles fulfilled storage manager implement access control virtualize storage namespace provide mechanisms guaranteeing storage space access control provided nest generic framework built top collections reorder classad relevant afs-style items access control infokernel lists prioritized determine list read rst category write modify libraries insert control privileges policy changing typical notions users groups maintained nest support access control generic policies enforced protocols nest supports clients communicate native chirp protocol supported protocol access control semantics set nest virtualizes physical namespace underlying storage enabling nest run wide variety storage elements current implementation local filesystem underlying storage layer nest plan physical storage layers raw disk future running remote location grid users higher-level scheduling systems assured exists sufficient storage space save data produced computation stage input data subsequent access address problem nest interface guarantee storage space called lot requests made space allocations similar reservations network bandwidth lot defined characteristics owner capacity duration files owner client entity allowed lot individual owners allowed group lots included release capacity total amount data stored lot duration amount time lot guaranteed exist finally lot set files number files lot bounded file span multiple lots fit single duration lot expires files contained lot immediately deleted allowed remain indefinitely space reclaimed creation lot refer behavior best-effort lots investigating selection policies reclaiming space create files nest user access lot file transfer protocols support creating lots environment lot obtained ways system administrators grant access nest simultaneously make set default lots users client directly chirp protocol create lot accessing server alternative data-transfer protocol lots implemented current implementation relies quota mechanism underlying filesystem nest limit total amount disk space allocated user utilizing quota system affords number benefits direct access file system nest observe quota restrictions allowing clients utilize nest make space guarantee bypass nest transfer data directly local file system existing software file system nest implementation simplified approach benefits lots user overfill single lot fill lot capacity future plan investigate costs benefits nest-managed lot enforcement nest global execution manager nest argonne cluster chirp gftp nfs submit madison figure nest grid diagram illustrates information flow scenario multiple nest servers 
utilized grid nest grid basic understanding nest place illustrate multiple nest servers global grid environment figure depicts scenario major events labeled sequence numbers defined order items related infokernel list inforeplace touches page increase priority infoplace allocates inodes data blocks group decrease priority libraries uence current future requests handled relative existing items list category overhead implementing policy user-level direct function overhead reorganizing list case studies shown infoplace inforeplace provide performance bene applications extreme circumstances overhead performing probes high inforeplace category libraries reorder items related lists libraries exert preferences limiting requests inserted related lists infoidlesched infofreesched infovegas libraries maintain order queue requests issue requests well-controlled times process retract decision insert item policies conservative initiating requests adapt quickly changing conditions infoidlesched send background requests disk queue increasing chances background request interfere foreground request arrives likewise infovegas react immediately network conditions change group messages issued experience infobsd section describe initial experience building prototype infobsd netbsd discussion focus main erences infobsd infolinux implementations date implemented memory management disk scheduling networking abstractions leave placement future work pagelist abstraction infobsd similar infolinux netbsd xed-sized cache primary erence infokernels infolinux pagelist page memory infobsd pages cache netbsd cache managed pure 
lru replacement infobsd simply exports lru list pagelist elements victimlist enable processes quickly determine elements moving lists infobsd tracks number evictions occurred lru list statements needed export abstractions infobsd primary savings compared infolinux requires statements infobsd provide memory-mapped interface eviction count diskrequestlist abstraction straightforward export infobsd chief erence infolinux infobsd relates layer system responsible maintaining device scheduling bgfg throughput cluster throughput background foreground bgfg cpu utilization bandwidth normalized cpu utilization figure workload bene infovegas graph left shows impact contention network infovegas background replication stream y-axis plots bandwidth delivered bar represents experiment combination foreground background tra infovegas graph depicts normalized cpu utilizations experiments cpu utilization bandwidth observe additional cpu overhead running infovegas experiments run machines type queues linux generic level maintains queues block devices infolinux generic level exports diskrequestlist abstraction netbsd generic level exists device type scsi ide export diskrequestlist abstraction independently lines code needed provide information infobsd infobsd requires statements infolinux linux code required access queue check finally infobsd implementation msglist requires relative infolinux version primary erence tcp linux skb ers network packet tcp netbsd mbuf ers multiple packets data structure netbsd cult add time-stamp packet needed msglist infobsd creates maintains queue packets time unacknowledged packet result infolinux statements msglist infobsd requires statements signi increase nal amount code small exercise shown abstractions exported infolinux straight-forward implement infobsd hopeful list-based abstractions ciently general capture behavior unix-based operating systems creating infokernels cult note infokernels export interfaces directly leverage user-level libraries created case studies majority code resides conclusions layering technique long building computer systems breaking larger system constituent components layering makes process building system manageable resultant separation modules increases maintainability facilitating testing debugging layering negative side-e ects traditional arguments layering implementationoriented observation layers network protocol stacks induce extra data copies insidious impact design architects layer encouraged hide details information layer system concealed layers paper argued operating systems avoid pitfall design export general abstractions describe internal state abstractions list memory pages eviction disk requests scheduled user-level services control policies implemented surprising ways information policies implemented transformed mechanisms usable services case studies stressing erent components cache management placement disk scheduling networking explored issues infokernel design ned abstractions infokernel export experience shown abstractions represented prioritized lists found number information primitives implementing abstractions procedure counters timers ability block infokernel state general found power infokernel approach depends closely desired control matches policy kernel infokernel user-level policies operate limits underlying kernel policy user-level policies bias preferences policy result target policies mesh inherent preferences policy implemented high accuracy low overhead found ability user-level processes ciently manipulate internal lists touching page increase priority enables powerful services built top infokernel knowledge serve guide developing future infokernels infokernels export operations ciently reorder retract items in-kernel prioritized lists exible building blocks implementing user-level policies acknowledgments john bent brian forney muthian sivathanu vijayan prabhakaran doug thain helpful discussions comments paper john bent storage server implementation brian forney trace analysis michael marty jacob kretz initial gray-box implementation inforeplace kirk webb netbed team con guring linux netbed computer systems lab providing superb research environment john wilkes excellent demanding shepherding substantially improved content presentation numerous aspects paper anonymous reviewers helpful suggestions finally give special grandparents arpaci niara vedat grandparents dusseau anita richard traveling madison taking care anna weeks submission due authors worked requisite long nights nal weeks work sponsored part nsf ccrngs- ccrccr- itrthe wisconsin alumni research foundation ibm faculty award ndseg fellowship department defense akyurek salem adaptive block rearrangement acm transactions computer systems allman balakrishnan floyd rfc enhancing tcp loss recovery limited transmit august ftp ftp rfc-editor in-notes rfc txt august arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages ban canada october bershad savage pardyak sirer fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december brakmo malley peterson tcp vegas techniques congestion detection avoidance proceedings sigcomm pages london united kingdom august burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge ercache contents proceedings usenix annual technical conference usenix pages monterey california june cao felten implementation performance application-controlled file caching proceedings symposium operating systems design implementation osdi pages monterey california november cardwell bak tcp vegas implementation linux http flophouse neal linux-vegas august cheriton zwaenepoel distributed kernel performance diskless workstations proceedings acm symposium operating system principles sosp pages bretton woods hampshire october chou dewitt evaluation management strategies relational database systems proceedings international conference large data bases vldb pages stockholm sweden august dijkstra structure multiprogramming system communications acm druschel pai zwaenepoel extensible kernels leading research astray proceedings workshop workstation operating systems wwos-vi pages cape codd massachusetts ely savage wetherall alpine user-level infrastructure network protocol development proceedings usenix symposium internet technologies systems usits pages san francisco california march engler kaashoek toole exokernel operating system architecture applicationlevel resource management proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december gibson nagle amiri chang gobio riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon hoe improving start-up behavior congestion control sheme tcp proceedings sigcomm pages stanford california august iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous proceedings acm symposium operating systems principles sosp pages ban canada october jacobson wilkes disk scheduling algorithms based rotational position technical report hpl-csp- hewlett packard laboratories jacobson congestion avoidance control proceedings sigcomm pages stanford california august johnson shasha low-overhead high performance management replacement algorithm proceedings international conference large databases vldb pages santiago chile september kaashoek engler ganger brice hunt mazi eres pinckney grimm jannotti mackenzie application performance flexibility exokernel systems proceedings acm symposium operating systems principles sosp pages saint-malo france october kiczales lamping lopes maeda mendhekar murphy open implementation design guidelines international conference software engineering icse pages boston massachusetts kiczales lamping maeda keppel mcnamee customizable operating systems proceedings workshop workstation operating systems wwos-iv pages napa california october lampson hints computer system design proceedings acm symposium operating system principles sosp pages bretton woods hampshire october levin cohen corwin pollack wulf policy mechanism separation hydra proceedings acm symposium operating systems principles sosp pages texas austin november liedtke micro-kernel construction proceedings acm symposium operating 
systems principles sosp pages copper mountain resort colorado december lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october lumb schindler ganger freeblock scheduling disk firmware proceedings usenix symposium file storage technologies fast pages monterey california january matthews roselli costello wang anderson improving performance logstructured file systems adaptive methods proceedings acm symposium operating systems principles sosp pages saint-malo france october mckusick joy fabry fast file system unix acm transactions computer systems august nugent arpaci-dusseau arpaci-dusseau controlling place file system gray-box techniques proceedings usenix annual technical conference usenix pages san antonio texas june neil neil weikum lru-k page replacement algorithm database disk ering proceedings acm sigmod international conference management data sigmod pages washington peacock kamaraju agrawal fast consistency checking solaris file system proceedings usenix annual technical conference usenix pages orleans louisiana june pearce kelly harder field gilk dynamic instrumentation tool linux kernel proceedings international conference modeling tools techniques computer communication system performance evaluation tools pages london united kingdom april popovici arpaci-dusseau arpacidusseau robust portable scheduling disk mimic proceedings usenix annual technical conference usenix pages san antonio texas june postel rfc transmission control protocol september ftp ftp rfc-editor in-notes rfc txt august riedel kallahalla swaminathan framework evaluating storage system security proceedings usenix symposium file storage technologies fast pages monterey california january rosu rosu kernel support faster web proxies proceedings usenix annual technical conference usenix pages san antonio texas june ruemmler wilkes disk shu ing technical report hpl- hewlett packard laboratories schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november seltzer chen ousterhout disk scheduling revisited proceedings usenix winter technical conference usenix winter pages washington january seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating systems design implementation osdi pages seattle washington october shenoy vin cello disk scheduling framework next-generation operating systems proceedings joint international conference measurement modeling computer systems sigmetrics performance pages madison wisconsin june smaragdakis kaplan wilson eelru simple ective adaptive page replacement proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages atlanta georgia staelin garcia-mollina smart filesystems proceedings usenix winter technical conference usenix winter pages dallas texas january stonebraker operating system support database management communications acm july tamches miller fine-grained dynamic instrumentation commodity operating system kernels proceedings symposium operating systems design implementation osdi pages orleans louisiana february van meter gao latency management storage systems proceedings symposium operating systems design implementation osdi pages san diego california october van renesse masking overhead protocol layering proceedings sigcomm pages stanford california august venkataramani kokku dahlin tcp-nice mechanism background transfers proceedings symposium operating systems design implementation osdi pages boston massachusetts december wahbe lucco anderson graham cient software-based fault isolation proceedings acm symposium operating systems principles sosp pages asheville north carolina december white lepreau stoller ricci guruprasad newbold hibler barb joglekar integrated experimental environment distributed systems networks proceedings symposium operating systems design implementation osdi pages boston massachusetts december young tevanian rashid golub eppinger chew bolosky black baron duality memory communication implementation multiprocessor operating system proceedings acm symposium operating systems principles sosp pages austin texas november 
description figure user input data permanently stored home site case nest madison wisconsin step user submits number jobs remote execution global execution manager manager aware remote cluster labeled argonne cluster large number cycles nest gateway appliance argonne previously published resource data availability global grid discovery system manager aware argonne nest sufficient amount storage manager decides run user jobs argonne site staging user input data step manager chirp create lot user files argonne guaranteeing sufficient space input output files step manager orchestrates gridftp third-party transfer madison nest nest argonne cluster data movement protocols kangaroo utilized move data site site step manager begins execution jobs argonne jobs access user input files nest local file system protocol case nfs jobs execute output files generate stored nest note ability give preference users protocols harnessed local administrators ensure preference jobs global manager ensure timely data movement finally step jobs begin complete point manager moves output data back madison utilizing gridftp wide area movement manager free chirp terminate lot nest nest wu-ftpd nest apache nest linux nfsd nest jbos server bandwidth nest jbos chirp gridftp http nfs total figure multiple protocols experiment measures bandwidth clients request files protocol sets bars single protocol workload single server jbos set bars workload protocols pair bar shows performance nest bar jbos step inform user output files local nest note steps guaranteeing space moving input data executing jobs moving output data terminating reservations encapsulated request execution manager condor directedacyclic-graph manager dagman higherlevel storage resource managers srm nest services synchronize access globallyshared storage resources experiments section perform evaluation key components nest experiments performed cluster pentium-based machines running linux ibm lzx disks connected gigabit ethernet solaris-based runs performed cluster netra machines running solaris connected mbit ethernet support multiple protocols illustrate supporting multiple protocols nest framework incurs overhead compared native implementations individual protocol figure compares bandwidth delivered clients nest server delivered native servers implementing individual protocol sets bars graph evaluate workloads requests protocol fifo server bandwidth scheduling configuration proportional protocol scheduling total chirp gftp http nfs desired figure proportional protocol scheduling workload identical figure results shown nest set bars bar represents total delivered bandwidth protocols remaining bars show bandwidth protocol labels sets bars show proportional ratios desired lines show ideal proportions note nest achieve close desired ratios case right-most time jbos single server running make observations results delivered bandwidth varies widely protocols chirp http deliver in-cache files peak bandwidth determined network gridftp nfs achieve approximately half bandwidth importantly performance nest protocols similar native server rightmost pair bars show delivered bandwidth workload requests multiple protocols jbos multiple servers running simultaneously total delivered bandwidth nest jbos similar roughly allocation bandwidth protocols bandwidth delivered nfs clients lower nest jbos nfs block-based protocol protocols file-based default transfer manager nest ends disfavoring nfs schedules requests fifo order quality service advantage nest relative jbos transfer manager nest easily extended scheduling policies implemented simple stride scheduler nest proportional share server bandwidth delivered types requests results shown figure set bars shows base case nest transfer manager simple fifo scheduler sets bars adjust desired ratio bandwidth protocol make conclusions graph proportional share scheduler imposes slight performance penalty fifo scheduling delivering total approximately proportional-share scheduler achieves close desired ratios cases specifically jain metric fairness represents ideal allocation achieve values greater ratios exception allocating additional bandwidth nfs chirp gridftp http nfs extremely difficult jain fairness case drops challenge sufficient number nfs requests transfer manager schedule interval case nfs request current implementation work-conserving schedules competing request server idle implementing non-work-conserving policy idle server waits period time scheduling competitor policy pay slight penalty average response time improved allocation control concurrency architecture adaptation show benefits automatically adapting concurrency architecture platform workload run simple experiments results shown figure experiment shown left run nest solaris server clients small files cache results illustrate workload event-based model lower average response time threaded model nest adaptive scheme performing experiment shown run nest linux server clients larger files case threaded model higher bandwidth event model adaptive scheme close performance model experiments discern cost adaptation nest models periodically order find current workload components receive proportional shares fairness allocation defined fairness ratio delivered allocation desired allocation components ideal allocation events threads adaptive average time request model solaris events threads adaptive bandwidth model linux figure adaptive concurrency graph left experiment measures average request latency solaris requests events threads adaptive nest approach graph experiment measures bandwidth linux requests models cases nest adaptively picks model overhead note process model disabled experiments sake clarity lot management overhead measured overhead quota mechanism implement lots nest found quotas enabled write performance disk decreases roughly worst case single sequential write stream shown figure read performance unaffected surprisingly cost writing quotas enabled hidden server network-bound concurrent write streams investigating additional complexity implementing lots directly monitoring write operations nest worth performance improvement ability distinguish lots correctly related work storage appliance nest relates closely filers network appliance enterprise storage platforms emc nest attempt compete commercial offerings terms raw performance primarily intended target domain nest offers low-cost softwareonly alternative offers protocol flexibility grid-aware features needed enable scientific computations grid server bandwidth write size performance overhead lots quotas disabled quotas enabled figure overhead lots graph shows overhead imposed implementing lots kernel quota system notice small files cost negligible increases quickly file size grid community number projects related nest gara architecture advance reservations variety resources including computers networks storage devices nest gara reservations similar nest lots users make advance gara provide best-effort lots sophisticated user management nest disk resource managers srm storage depots ibp legionfs servers provide grid storage services projects designed provide local storage management global scheduling middleware conversely nest local storage management solution designed integrate number global scheduling systems distinction account key difference nest storage servers systems designed work primarily self-contained middleware projects protocol independence servers unique feature nest dynamic concurrency adaptation note intrinsic design nest incorporated systems srm ibp provide space guarantees manners similar nest lots difference srm srm guarantees space allocations 
multiple related files two-phased pinning lots nest provide functionality client flexibility control implementation complexity comparing nest lots ibp space guarantees difference ibp reservations allocations byte arrays makes extremely difficult multiple files contained allocation client build file system byte array difference ibp permanent volatile allocations nest permanent lots users allowed indefinitely renew best-effort lots analogous volatile allocations mechanism ibp switching allocation permanent volatile lots nest switch automatically best-effort duration expires nest legionfs recognizes importance supporting nfs protocol order unmodified applications benefit grid storage resources legionfs builds support client side nest server side legionfs client-based nfs easier server implementation makes deployment difficult legion-modified nfs module deployed client locations nest grid storage system supports multiple protocols server pfs srb middleware client side approaches complementary enable middleware server negotiate choose protocol transfer nfs locally gridftp remotely conclusion presented nest open-source user-level software-only storage appliance nest specifically intended grid designed concepts flexibility adaptivity grid-awareness flexibility achieved virtual protocol layer insulates transfer architecture particulars file transfer protocols dynamic adaptation transfer manager additional flexibility enabling nestto run effectively wide range hardware software platforms supporting key grid functionality storage space guarantees mechanisms resource data discovery user authentication quality service nest grid-aware integrate cleanly distributed computing systems experimental results demonstrated inclusion multiple protocols single storage appliances enables proportional-share scheduling jbos model presented experimental results showing nest adjusts workload solaris linux systems adjusts highest performing concurrency model finally illustrate vision nest role grid computation scenario utilizes grid middleware multiple nests coordinate reservation scheduling cpus reservation scheduling storage resources nest development release runs linux solaris version support nfs protocol operational supported download http wisc condor nest production release released end acknowledgments members condor team numerous list members wind group nathan burnett timothy denehy brian forney florentina popovici muthian sivathanu specially mention erik paulson douglas thain peter couvares todd tannenbaum condor team people anonymous reviewers contributed suggestions specifically paper development nest project general extend gratitude members computer systems lab outstanding job keeping computers running networks work sponsored nsf ccrccr- ngsccr- itrand wisconsin alumni research foundation allcock bester bresnahan chervenak liming tuecke grid ftp protocol extensions ftp grid http www-fp mcs anl gov dsl gridftpprotocol-rfc-draft pdf march arpaci-dusseau arpaci-dusseau information control gray-box systems symposium operating systems principles sosp oct baru moore rajasekar wan sdsc storage resource broker proceedings cascon toronto canada burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache management usenix chervenak foster kesselman tuecke protocols services distributed data-intensive science proceedings acat chiu jain analysis increase decrease algorithms congestion avoidance computer networks journal computer networks isdn volume pages june condor condor directed-acyclic-graph manager dagman http wisc condor dagman crovella frangioso harchol-balter connection scheduling web servers usenix symposium internet technologies systems emc corporation http emc fielding gettys mogul frystyk berners-lee rfchttp hypertext transfer protocol specification version network working group requests comments january foster kesselman globus metacomputing intrastructure toolkit international journal supercomputer applications foster kesselman tsudik tuecke security architecture computational grids proceedings acm conference computer communications security conference pages frey tannenbaum livny tuecke condor-g computation managament agent multiinstitutional grids proceedings tenth ieee symposium high performance distributed computing hpdc pages san francisco california august grimshaw wulf legion vision worldwide virtual computer communications acm january hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference pages san fransisco usa howe bandwidth request throttling apache http snert software throttle iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous acm symposium operating systems principles october kleiman vnodes architecture multiple file system types sun unix usenix conference proceedings pages kohler morris chen jannotti kaashoek click modular router acm transactions computer systems august lancaster rowe measuring real world data availability proceedings lisa systems administration conference pages san diego california december lohr supercomputing business move closer york times business financial desk february pai druschel zwaenepoel flash efficient portable web server proceedings usenix technical conference patterson availability maintainability performance focus century key note lecture usenix conference file storage technologies fast january plank bassi beck moore swany wolski managing data storage network ieee internet computing september october postel rfcftp file transfer protocol specification june raman matchmaking frameworks distributed resource management phd thesis wisconsin october raman livny solomon matchmaking distributed resource management high throughput computing proceedings seventh ieee international symposium high performance distributed computing hpdc july roy end-to-end quality service high-end applications phd thesis chicago satyanarayanan digest seventh ieee workshop hot topics operating systems rice conferences hotos digest digesthtml html march sharpe smb samba cifs docs whatis-smb html september shoshani sim storage resource managers middleware components grid storage nineteenth ieee symposium mass storage systems mss sun microsystems rfcnfs network file system protocol specification network working group requests comments march thain basney son livny kangaroo approach data movement grid proceedings tenth ieee symposium high performance distributed computing hpdc san francisco california august thain bent arpaci-dusseau arpaci-dusseau livny gathering creating communities grid proceedings supercomputing denver colorado november thain livny pluggable file system http wisc condor pfs vazhkudai tuecke foster replica selection globus data grid ieee international symposium cluster computing grid ccgrid waldspurger weihl stride scheduling deterministic proportional-share resource mangement technical report mit lcs tmmassachusetts institute technology mit laboratory computer science june walsh lyon sager chang goldberg kleiman lyon sandberg weiss overview sun network file system proceedings usenix winter conference pages jan welsh culler brewer seda architecture well-conditioned scalable internet services proceedings eighteenth symposium operating systems principles sospbanff canada october white walker humphrey grimshaw legionfs secure scalable file system supporting cross-domain high-performance applications proceedings supercomputing denver colorado november zhang deering estrin shenker zappala rsvp resource reservation protocol ieee networks magazine september 
life death block-level muthian sivathanu lakshmi bairavasundaram andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison muthian laksh dusseau remzi wisc abstract fundamental piece information required intelligent storage systems liveness data formalize notion liveness storage present classes techniques making storage systems liveness-aware explicit notification approach present robust techniques file system impart liveness information storage free block command implicit detection approach show information inferred storage system efficiently underneath range file systems storage interface demonstrate techniques prototype implementation secure deleting disk find explicit interface approach desirable due simplicity implicit approach easy deploy enables quick demonstration functionality facilitating rapid migration explicit interface introduction life pleasant death peaceful transition troublesome isaac asimov smarter storage systems understand blocks live dead previous work demonstrated utility knowledge dead blocks store rotationally optimal replicas data provide zero-cost writes failure recovery time reduced restoring live blocks liveness information modern storage systems due narrow blockbased interface file systems storage storage systems simply observe block-level reads writes aware logical operations deletes issued file system limitation precludes storage level optimizations makes effective paper address limitation presenting techniques storage systems imparted liveness information perform qualitative quantitative comparison approaches explicit notification augment interface storage free block command file systems modified properly implicit detection develop techniques enable storage system infer liveness information change interface evaluate approaches formalize notion liveness storage specifically identify classes liveness content block generation liveness present techniques explicit implicit tracking type techniques imparting liveness information dependent characteristics file system study range file systems including ext ext vfat identify key file system properties impact feasibility complexity techniques gain direct experience liveness-tracking methods design implement evaluate prototype secure deleting disk shreds blocks logically deleted file system making deleted data irrecoverable implement secure delete due extreme requirements type accuracy liveness information surface explicit implicit approaches obvious benefits drawbacks explicit notification promises simplicity implementation requires broad industry consensus implicit detection suggests ease deployment cost complexity analysis reveals complex trade-offs find qualitatively explicit approach complicated design implement straightforward modify file systems issue free block commands accurate notification presence crashes entails careful integration file system consistency management schemes noticeably increasing complexity find implicit liveness detection feasible underneath range modern file systems file system behaviors prohibit classes liveness inference identify properties hold order enable simplify implicit liveness inference propose implement minor modifications file systems conform properties finally show implicit liveness detection accurate underneath modern asynchronous file systems secure delete prototype utilizes implicit liveness shred blocks inferred dead proving correct operation implicit secure delete demonstrate implicit liveness storage applications appears sixth symposium operating systems design implementation osdi extreme correctness requirements evaluating performance implicit liveness tracking find comparable explicit approach conclude storage systems easily implement explicit approach interface embellished support implicit approach complementary competitive technology industry consensus interface change slow-moving implicit techniques complex specifically deploying technology explicit interface change implicit techniques readily demonstrate benefits move industry rapidly explicit change paper organized present extended motivation taxonomy liveness list file system properties impact techniques imparting liveness information proceed discussing explicit notification implicit detection describe secure deletion describe initial experience implicit detection ntfs closed-source file system finally present discussion relative merits implicit explicit approaches finish discussing related work concluding appendix includes proof correctness implicit secure delete extended motivation section present examples functionality enabled liveness information motivate alternative approaches gathering information utility liveness liveness information enables variety functionality performance enhancements storage system enhancements implemented higher layers require low-level control storage system eager writing workloads write-intensive run faster storage system capable eager writing writing free block closest disk arm traditional in-place write order select closest block storage system information blocks live existing proposals function long exist blocks written file system writes block storage system identify subsequent death block result delete disk empowered liveness information effective eager writing adaptive raid information block liveness storage system facilitate dynamic adaptive raid schemes autoraid system autoraid utilizes free space store data raidlayout migrates data raidwhen runs short free space knowledge block death make schemes effective optimized layout techniques optimize on-disk layout transparently storage system explored adaptive reorganization blocks disk replication blocks rotationally optimal locations examples knowing blocks free greatly facilitate techniques live blocks collocated minimize seeks free space dead blocks hold rotational replicas smarter nvram caching buffering writes nvram common optimization storage systems synchronous write workloads benefit in-memory delayed writes file system nvram buffering improves performance absorbing multiple overwrites block deleteintensive workloads unnecessary disk writes occur absence liveness information deleted blocks occupy space nvram written disk nvram fills real file system traces found writes deleted typical delayed write interval seconds unnecessarily written disk knowledge block death storage removes overhead intelligent prefetching modern disks perform aggressive prefetching block read entire track block resides prefetched cached internal disk cache aged fragmented file system subset blocks track live caching track result suboptimal cache space utilization reading track efficient disk knowledge liveness enable disk selectively cache blocks live faster recovery liveness information enables faster recovery storage arrays storage system reduce reconstruction time disk failure reconstructing blocks live file system self-securing storage liveness information storage build intelligent security functionality storage systems storage level intrusion detection system ids perimeter security monitoring traffic suspicious access patterns deletes truncates log files detecting patterns requires liveness information secure delete ability delete data manner makes recovery impossible important component data security government regulations require strong guarantees sensitive data forgotten requirements expected widespread government industry future secure deletion requires low-level control block placement storage appears sixth symposium operating systems design implementation osdi system implementing storage level secure delete requires liveness information storage system explore secure deletion section acquiring liveness information clear benefits liveness information storage systems information natural question arises convey liveness information storage systems discuss approaches explicit notification implicit detection explicit notification explicit notification involves augmenting existing storage interface allocate block free block commands modifying file systems commands explicitly convey liveness information storage system main benefit explicit approach potential simplicity interface deployed conveying liveness information seemingly straightforward appearing natural achieve goal problems approach changing interface storage raises legacy issues requires broad industry consensus demand interface requires agreement clear benefits interface difficult achieve deployment interface chicken-and-egg problem implicit detection implicit detection 
intended solve bootstrapping problem explicit interface evolution approach storage system monitors block-level reads writes issued file system underneath unmodified interface infers liveness information implicitly ideally change file system implicit approach enables demonstration benefits due proposed interface change making evolutionary step eventual interface modification previous work semantically-smart storage systems explored implicit detection forms file system information storage system storage-level enhancements degree accuracy required implicit detection techniques case depends nature application information x-ray storage system utilizes implicit information file accesses implement exclusive storage array cache inaccurate information x-ray simply reduces potential performance gain d-graid storage system utilizes implicit information file block belongs order place blocks fault-isolated fashion improving availability storage system multiple disk failures inaccurate information graid leads poor fault isolation impact correctness array exhibits strictly liveness description type utility content data block versioning block block holds eager write valid data fast recovery generation block lifetime secure delete context file storage ids table forms liveness availability traditional raid paper investigate limits implicit detection applications utilize implicit liveness information directly impacts correctness primary concern implicit interface evolution ties interacting layers file system storage system change issue problematic on-disk formats evolve slowly reasons backwards compatibility linux ext file system introduced roughly layout lifetime ext journaling file system backwards compatible on-disk layout ext extensions freebsd file system backwards compatible evidence commercial storage vendors maintain support software specific file system emc symmetrix storage system software understand common file systems trends point commercial viability implicit detection approach liveness storage taxonomy discussed utility liveness information storage system present taxonomy forms liveness information relevant storage liveness information classified dimensions granularity accuracy timeliness granularity liveness depending specific storage-level enhancement utilizes liveness information logical unit liveness tracked vary identify granularities liveness information meaningful content block generation summary presented table content liveness content liveness simplest form liveness unit liveness actual data context block death granularity occurs overwrite block block overwritten data storage system infer contents dead approximate form content liveness readily existing storage systems explored previous work wang appears sixth symposium operating systems design implementation osdi virtual log disk frees past location block block overwritten contents tracking liveness granularity on-disk versioning self-securing storage systems completely accurate storage system block freed file system contents stored block dead overwritten block liveness block liveness tracks disk block valid data data accessible file system unit interest case container contents block liveness granularity required applications intelligent caching prefetching eager writing deciding propagate block nvram disk storage system block live granularity form liveness information tracked traditional storage systems storage system unaware blocks file system thinks live weak form liveness tracked block written inferred dead generation liveness generation disk block lifetime block context file death generation block written disk context file free reallocated file tracking generation liveness ensures disk detect logical file system delete block contents reached disk context deleted file storage level functionality requires generation liveness secure delete track block live contained data belonged file generation longer alive application requires generation liveness information storage-based intrusion detection generation liveness tracked existing storage systems accuracy liveness information dimension liveness accuracy refer degree trust disk place liveness information inaccuracy liveness information lead disk overestimating underestimating set live entities blocks generations degree accuracy required varies specific storage application deletesquashing nvram acceptable storage system slightly overestimate set live blocks performance issue correctness issue hand underestimating set live blocks catastrophic disk lose valid data similarly generation liveness detection secure delete acceptable miss intermediate generation deaths block long latest generation death block timeliness information final axis liveness timeliness defines time death occurring file system disk learning death explicit notification approach file system delays free notifications similar delayed writes time lag disk learns block generation death similarly implicit approach periodicity file system writes metadata blocks imposes bound timeliness liveness information inferred applications eager writing delete-aware caching delayed knowledge liveness acceptable long information changed meantime applications secure delete timely detection provide stronger guarantees file system properties explicit implicit methods imparting liveness information storage dependent characteristics file system storage system study range techniques required liveness notification detection experimenting underneath file systems ext ext vfat experimented ntfs limited scale due lack source code access ntfs experience section ext modes operation synchronous asynchronous modes ext modes writeback ordered data journaling modes update behaviors form rich set file systems begin background file systems outline high level behavioral properties file system relevant context liveness information sections discuss properties influence techniques storage-level liveness tracking file system background subsection provide background information file systems study discuss key on-disk data structures update behavior common properties begin properties common file systems viewpoint liveness tracking basic level file systems track kinds on-disk metadata structure tracks allocation blocks bitmap freelist index structures inodes map logical file groups blocks common aspect update behavior modern file systems asynchrony data metadata appears sixth symposium operating systems design implementation osdi block updated contents block immediately flushed disk buffered memory interval delayed write interval blocks dirty longer delayed write interval periodically flushed disk order delayed writes committed potentially arbitrary file systems enforce ordering constraints linux ext ext file system intellectual descendant berkeley fast file system ffs disk split set block groups akin cylinder groups ffs inode data blocks allocation status live dead data blocks tracked bitmap blocks information file including size block pointers found file inode accommodate large files pointers inode point indirect blocks turn block pointers committing delayed writes ext enforces ordering whatsoever crash recovery requires running tool fsck restore metadata integrity data inconsistency persist ext synchronous mode operation metadata updates synchronously flushed disk similar early ffs linux ext ext file system journaling file system evolved ext basic on-disk structures ext ensures metadata consistency write-ahead logging metadata updates avoiding perform fsck-like scan crash ext employs coarse-grained model transactions operations performed epoch grouped single transaction ext decides commit transaction takes in-memory copy-on-write snapshot dirty metadata blocks belonged transaction subsequent updates metadata blocks result in-memory copy ext supports modes operation ordered data mode ext ensures transaction commits data blocks dirtied transaction written disk data journaling mode ext journals data blocks metadata modes ensure data integrity crash mode data writeback order data writes data integrity 
guaranteed mode vfat vfat file system descends world operating systems paper linux implementation vfat vfat operations centered file allocation table fat entry allocatable block file system entries locate blocks file linkedlist fashion file block address property syn reuse ordering block exclusivity generation marking delete suppression consistent metadata data-metadata coupling table file system properties table summarizes properties exhibited file systems study entry fat find block file entry hold endof-file marker setting block free unlike unix file systems information file found inode vfat file system spreads information fat directory entries fat track blocks belong file directory entry information size type information pointer start block file similar ext vfat preserve ordering delayed updates properties update behavior file system direct influence techniques liveness information imparted storage system based experience aforementioned file systems identify high-level file system properties relevant liveness tracking table summarizes properties reuse ordering file system guarantees reuse disk blocks freed status block bitmaps metadata pointed block reaches disk file system exhibits reuse ordering property sufficient ensure data integrity absence property file end partial contents deleted file crash journaling file system vfat asynchronous mode ext reuse ordering modes ext ext synchronous mode exhibit reuse ordering block exclusivity block exclusivity requires disk block dirty copy block file system cache requires file system employ adequate locking prevent update in-memory copy dirty copy written disk property holds file systems ext vfat ext conform property snapshot-based journaling dirty copies metadata block previous transaction committed current transaction generation marking generation marking property requires file system track reuse file pointer obappears sixth symposium operating systems design implementation osdi jects inodes version numbers ext ext file systems conform property inode deleted reused file version number inode incremented vfat exhibit property delete suppression basic optimization found file systems suppress writes deleted blocks file systems discuss obey property data blocks vfat obey property directory blocks consistent metadata property file system conveys consistent metadata state storage system journaling file systems exhibit consistent metadata property transaction boundaries on-disk log implicitly convey information ext vfat exhibit property data-metadata coupling data-metadata coupling builds consistent metadata property requires notion consistency extended data blocks words file system conforming property conveys consistent metadata state set data blocks dirtied context transaction file systems ext data journaling mode conforms property explicit liveness notification proceed techniques imparting forms liveness information storage systems section discuss explicit notification approach assume special allocate free commands added scsi optimization obviate explicit allocate command treating write previously freed block implicit allocate modifying file systems interface trivial find supporting free command ramifications consistency management file system crashes modified linux ext ext file systems free command communicate liveness information discuss issues free command implemented ioctl pseudo-device driver serves enhanced disk prototype granularity free notification issue arises explicit notification exact semantics free command granularities liveness outlined section block liveness content liveness tracked file system lazy initiating free commands suppressingfreeto blocks subsequently reused generation liveness file system notify disk delete block contents reached disk context deleted file multiple intermediate layers buffering file system contents block reached disk context file simplify file system implementation file system concerned form liveness disk functionality requires approach file system invokes free command logical delete receiving free command block disk marks block dead internal allocation structure bitmap write marks block live responsibility mapping thesefreecommands form liveness information lies disk disk track generation deaths interested free command block thinks live internal bitmaps redundant free block free disk block deleted written disk viewed generation death correct operation file system guarantee write block disk prior allocation write treated implicit allocate guarantee delete suppression property write freed block allocation result incorrect conclusion generation liveness disk note free issued block disk safely block possibly erasing contents timeliness free notification important issue arises explicit notification free file system issues notification option notification file system issues free immediately block deleted memory solution result loss data integrity crash scenarios crash occurs immediately free notification block metadata indicating delete reaches disk disk considers block dead recovery file system views block live delete reached disk live file freed block scenario violation data integrity violations acceptable file systems ext weak data integrity guarantees file systems preserve data integrity ext delay notification effect delete reaches disk delayed notification requires file system conform reuse ordering property block reused live file system effect previous delete reaches disk delayed free command suppressed means disk miss generation death orphan allocations finally explicit notification handle case orphan allocations file system considers block dead disk considers live assume block newly allocated file written disk conappears sixth symposium operating systems design implementation osdi text file crash occurs point metadata indicating allocation written disk disk assume block live restart file system views block dead on-disk contents block belong file longer extant file system block suffered generation death disk free notification mechanism enable accurate tracking liveness orphan allocations handling orphan allocations file system specific describe explicit notification ext mentioned ext provide data integrity guarantees crash notification deletes ext invokes free command synchronously block freed memory dealing orphan allocations ext requires simple expensive operation recovery fsck utility conservatively issuesfreenotifications block dead file system explicit notification ext ext guarantees data integrity ordered data journaling modes free notification ext delayed effect delete reaches disk words notification delayed transaction performed delete commits record in-memory list blocks deleted part transaction issue free notifications blocks transaction commits ext conforms reuse ordering property delayed notification feasible crash occur invocation free commands immediately commit transaction free operations redo-able recovery purpose log special free records journal replayed recovery part delete transaction recovery multiple committed transactions propagated on-disk locations block deleted transaction reallocated subsequent committed transaction replay loggedfree commands guarantee completing free commands transaction committing transaction replay free commands successfully committed transaction log earlier committed transactions replayed deal orphan allocations log block numbers data blocks written written disk recovery ext issuefree commands set orphan data blocks part uncommitted transaction implicit liveness detection section analyze issues implicit detection liveness storage system implicit liveness inference requires storage system semantic understanding on-disk format file system running coupled careful observation file system traffic implicit liveness detection file system dependent discuss feasibility generality 
implicit liveness detection file systems ext ext vfat section discuss initial experience implicit detection underneath windows ntfs file system forms liveness address granularity accuracy axes mentioned section accuracy axis accurate approximate inferences approximate instance refers strict over-estimate set live entities timeliness axis address common complex case lack timely information modern file systems delay metadata updates timeliness guaranteed guarantees timeliness synchronously mounted file system implicit inference liveness trivial content liveness discussed section disk observes write contents live data block infer previous contents stored block suffered content death completely accurate content liveness inference requires information block liveness block liveness block liveness information enables storage system block valid data time track block liveness storage system monitors updates structures tracking allocation ext ext specific data bitmap blocks convey information vfat information embedded fat entry fat block free file system writes allocation structure storage system examines entry concludes relevant block dead live allocation bitmaps buffered file system written periodically liveness information storage system stale account allocations deletes occurred interval table depicts time line operations leads incorrect inference storage system bitmap block tracking liveness written step indicating dead subsequently allocated file written disk indicating live buffered memory point disk wrongly believes dead on-disk contents valid appears sixth symposium operating systems design implementation osdi operation in-memory on-disk initial free write disk free alloc alloc write disk written liveness belief live free table naive block liveness detection table depicts time line events leads incorrect liveness inference problem solved shadow bitmap technique address inaccuracy disk tracks shadow copy bitmaps internally file system writes bitmap block disk updates shadow copy copy written addition data block written disk disk pro-actively sets bit shadow bitmap copy block live write leads disk live preventing incorrect conclusion drawn file system properties block liveness shadow bitmap technique tracks block liveness accurately underneath file systems obey block exclusivity data-metadata coupling property block exclusivity guarantees bitmap block written reflects current liveness state relevant blocks file system tracks multiple snapshots bitmap block ext write version bitmap block indicating dead subsequent allocation write ofb disk wrongly infer dead fact on-disk contents valid belongs newer snapshot uncertainty complicates block liveness inference file system exhibit block exclusivity block liveness tracking requires file system exhibit data-metadata coupling group metadata blocks bitmaps actual data block contents single consistent group file systems typically enforce consistent groups transactions observing transaction boundaries disk reacquire temporal information lost due lack block exclusivity ext data journaling mode transaction newly allocated data blocks bitmap blocks indicating allocation part consistent group commit point disk conclusively infers liveness state state bitmap blocks transaction data writes actual in-place locations occur transaction commits disk guaranteed transaction commit blocks marked dead previous transaction remain dead absence data-metadata coupling newly allocated data block reach in-place location transaction commits live disk disk detects operation in-memory on-disk initial alloc live write disk written delete free alloc alloc write disk live liveness belief missed gen death table missed generation death block liveness table shows scenario illustrate simply tracking block liveness insufficient track generation deaths accuracy block liveness requires file system conform delete suppression property delete suppression hold write block imply file system views block live shadow bitmap technique overestimate set live blocks bitmap write table ext vfat ext data journaling mode readily facilitate block liveness detection generation liveness generation liveness stronger form liveness block liveness builds shadow bitmap technique generation liveness goal find on-disk block generation data file stored block dead block liveness special case generation liveness block dead latest generation stored dead conversely block liveness information sufficient detect generation liveness block live stored dead generation past table depicts case block initially stores generation inode disk thinks block live deleted freeing immediately reallocated file written time continues marked live disk missed generation death occurred bitmap writes generation liveness reuse ordering tracking generation liveness general challenging file system reuse ordering property makes simple track reuse ordering block reused file deleted status block reaches disk reused bitmap block written disk detect dead presence reuse ordering tracking block liveness accurately implies accurate tracking generation liveness file systems ext conform reuse ordering facilitate accurate tracking generation liveness generation liveness reuse ordering underneath file systems ext vfat exhibit reuse ordering property tracking generation appears sixth symposium operating systems design implementation osdi liveness requires disk detailed information specifically disk monitor writes metadata objects link blocks single logical file inode indirect blocks ext directory fat entries vfat disk explicitly track generation block belongs inode written disk records block pointers belong specific inode extra knowledge file block belongs disk identify generation deaths ownership table disk tracked belongs eventually written disk observe change ownership owns block owned past disk conclude generation death occurred complication arises reused reused representing file belongs generation scenario detected generation death ownership change monitor miss detect case require file system track reuse inodes generation marking property ext maintains version number enables detection cases generation deaths version numbers disk tracks block generation belonged generation number combination inode number version number disk observes inode written incremented version number concludes blocks belonged previous version inode incurred generation death call technique generation change monitoring finally pertinent note generation liveness detection generation change monitoring approximate assume disk observes block belongs generation time observes belongs generationg generation change monitoring disk conclude generation death occurred disk generation deaths occurred relevant period freed allocated freed reallocated disk owningb due delayed write show case study weaker form generation liveness summary file system properties required forms implicit liveness inference presented table case study secure delete demonstrate techniques imparting liveness storage present design implementation evaluation secure deleting disk explicit imliveness type properties blockapprox block exclusivity data-metadata coupling blockaccurate blockapprox delete suppression generationapprox blockapprox generation marking generationaccurate blockaccurate reuse ordering table properties implicit liveness detection approx set live entities over-estimated plicit approaches describe implicit secure delete detail briefly discuss explicit secure delete primary reasons chose secure deletion case study secure delete requires tracking generation liveness challenging track secure delete liveness information context correctness paramount false positive detecting delete lead irrevocable deletion valid data false negative result long-term recoverability deleted data violation secure deletion guarantees compared 
previous work functioned simplistic assumption synchronously mounted file system demonstrate accurate inference liveness feasible underneath variety modern file system behaviors implicit secure deletion prototype called faded file-aware data-erasing disk faded works underneath file systems ext vfat ext complete lack ordering guarantees ext presented challenges specifically ext reuse ordering property detecting generation liveness requires tracking generation information disk section focus implementation faded underneath ext finally discuss key differences implementation file systems goals faded desired behavior faded block reaches disk context file delete file trigger secure overwrite shred block behavior corresponds notion generation liveness defined section shred involves multiple overwrites block specific patterns erase remnant magnetic effects past layers recovered techniques magnetic scanning tunneling microscopy recent work suggests overwrites sufficient ensure non-recoverability modern disks traditionally secure deletion implemented file system implementations unreliable modern storage systems high security overwrites off-track writes writes straggling physical track boundaries external erase programs file system perform storage system buffers writes nvram multiple overwrites file system appears sixth symposium operating systems design implementation osdi collapsed single write physical disk making overwrites ineffective finally presence block migration storage system overwrite file system overwrite current block location stray copies deleted data remain storage system proper locale implement secure deletion note faded operates granularity entire volume control individual files shredded limitation dealt storing sensitive files separate volume secure delete functionality enabled basic operation discussed section faded monitors writes inode indirect blocks tracks inode generation block belongs augments information block liveness information collects shadow bitmap technique note ext obeys block exclusivity delete suppression properties block liveness detection reliable block death detected faded safely shred block hand faded detects generation death ownership change generation change monitors block live block liveness module faded simply shred block faded current contents block belong generation deleted generation subsequently allocated block due block reuse current contents block valid shredding block catastrophic deal uncertainty conservative approach generation-death inference conservative convert apparent correctness problem performance problem end performing overwrites required fundamental approach notion conservative overwrite conservative overwrites conservative overwrite block erases past layers data block leaves current contents intact faded subsequent valid write occurred predicted generation death conservative overwrite block safe shred valid data perform conservative overwrite block faded reads block non-volatile ram performs normal secure overwrite block specific pattern ultimately restores original data back block problem conservative overwrite block contents restored conservative overwrite fact data shredded conservative overwrite ineffective case faded guaranteed observe things block reused file system file valid data written eventually delayed write interval file system faded receives write buffers write writing data disk faded performs shred concerned block time faded restore data recent contents block identify writes treat special manner faded tracks list blocks subjected conservative overwrite suspicious blocks list write block list committed secure overwrite block overwrite block removed suspicious list note suspicious list stored persistently nvram order survive crashes block reused file system immediately faded guaranteed observe bitmap reset block flagged block death block liveness detector block liveness tracking reliable faded shred block destroying data cases wrongful restore data faded guaranteed opportunity make error cost conservatism conservative overwrites performance cost conservative overwrite results concerned block treated suspicious data restored conservative overwrite data faded information find stage uncertainty data restored data overwritten subsequent write block context file lead redundant shredding block performance cost faded pays circumvent lack perfect information coverage deletes previous subsection showed generation deaths detected faded ensures block version overwritten compromising valid data faded achieve goals detection techniques sufficient identify cases deletes file system level shredded section show faded detect deletes requires minor modifications ext undetectable deletes weak properties ext deletes missed faded present specific situations identification deletes impossible propose minor ext fix scenarios file truncates generation change monitor assumes version number inode incremented inode reused version number ext appears sixth symposium operating systems design implementation osdi operation in-memory on-disk initial bind bind delete free alloc write disk bind wrong type table misclassified indirect block table shows scenario normal data block misclassified indirect block bind treated indirect block reuse ordering indirect blocks prevents problem incremented complete delete reuse partial truncates affect version number block freed due partial truncate reassigned file faded misses generation death reuse partial truncate argued logical overwrite file delete adopt complex conservative interpretation treating delete handle deletes propose small change ext incrementing version number reallocation inode increment truncate alternatively introduce separate field inode tracks version information non-intrusive change effective providing disk requisite information technique result extra overwrites rare case partial truncates correctness guaranteed spurious overwrites conservative leave data intact reuse indirect blocks subtle problem arises due presence indirect pointer blocks indirect blocks share data region file system user data blocks file system reuse normal user data block indirect block vice versa presence dynamic typing disk reliably identify indirect block faded identify block indirect block observes inodei indirect pointer field faded records fact indirect block observes write faded contents indirect block deleted reused user data block inode scenario illustrated table faded trust block pointers suspected indirect block uncertainty lead missed deletes cases prevent occurrence data block misclassified indirect block ensure file system allocates immediately file system frees indirect block bind concerned data bitmap blockmbind flushed disk disk block freed note weak form reuse ordering indirect blocks show change operation in-memory on-disk initial free free alloc write disk written delete free alloc write disk missed delete table missed delete due orphan write table illustrates delete missed orphan block treated carefully block initially free allocated memory written disk written deleted reallocated written faded associate miss overwrite impact performance indirect blocks tend small fraction set data blocks practicality discussed minimal non-intrusive required modification lines code ext required weak ordering guarantees ext file systems ext exhibit reuse ordering required study ext aimed limit study minimal set file system properties required reliably implement secure deletion disk orphan allocations implicit block liveness tracking faded addresses orphan allocation issue discussed ext recovers crash fsck utility writes copy bitmap blocks block liveness monitor faded detect death orphan allocations orphan writes due arbitrary ordering ext faded observe write newly allocated data block observes owning inode orphan writes treated carefully owning 
inode deleted written disk faded block belonged inode block reused inode faded miss overwriting concerned block written context inode table depicts scenario address problem defer orphan block writes faded observes owning inode potentially memory-intensive solution suspicious block list conservative overwrites track orphan blocks faded observes write orphan block marks suspicious subsequent write arrives contents shredded inode owning block deleted reaching disk write block context file trigger shred block reused bitmap reset delete technique results redundant secure overwrite anytime orphaned block overwritten file sysappears sixth symposium operating systems design implementation osdi persistent liveness monitor block block inode mapping overwrite thread datadelayed overwrites suspicious list shadow bitmaps monitor generation change figure key components faded tem context file cost pay conservatism note overhead incurred time orphan block overwritten guaranteed detection deletes techniques prove block deleted file system reached disk faded overwrites deleted contents proof presented appendix delayed overwrites multiple overwrites block additional disk hurt performance incurred critical path performance faded delays overwrites idle time workload optionally minutes detection faded decides shred block queues low priority thread services queue faded observed foreground traffic duration delayed overwrites faded present writes disk sequential ordering reducing impact foreground performance delaying reduces number overwrites block deleted multiple times notion conservative overwrites crucial delaying overwrites arbitrarily block overwritten written context file note shredding required user perform sync summary key data structures components faded presented figure faded file systems implemented faded underneath file systems case validated implementation testing methodology section due space constraints point key differences observed relative ext faded vfat ext vfat conform reuse ordering faded track generation information block order detect deletes key difference vfat compared ext pre-allocated uniquely addressable inodes version information dynamically allocated directory blocks pointer start block file fat chains start block blocks file detecting deletes reliably underneath unmodified vfat impossible introduced additional field vfat directory entry tracks globally unique generation number generation number incremented create delete file system newly created file assigned current generation number small change lines code vfat generation change monitor accurately detects deletes interest faded ext ext exhibits reuse ordering tracking generation liveness ext tracking block liveness ext obey block exclusivity property tracking block liveness accurately impossible data journaling mode property data-metadata coupling ordered writeback modes make small change metadata transaction logged made ext log list data blocks allocated transaction change lines code coupled reuse ordering property enables accurate tracking deletes explicit secure delete built secure deletion explicit notification framework modified ext ext file systems notify disk logical delete file system modifications accounted lines code receiving notification disk decides shred block similar faded disk delays overwrites idle time minimize impact foreground performance evaluation section evaluate implicit explicit implementations secure delete enhanced disk implemented pseudo-device driver linux kernel driver observes information hardware prototype suffers contention cpu memory host ghz pentiumwith ram rpm ibm lzx disk due space constraints provide results ext version correctness accuracy test faded implementation detected deletes interest instrument file system log delete correlate log writes overwrites faded capture cases unnecessary missed overwrites tested system workloads technique including busy hours file system traces table presents results study trace hour experiment ran faded versions linux ext marked default appears sixth symposium operating systems design implementation osdi config delete overwrite excess miss indirect version table correctness accuracy table shows number overwrites performed faded configurations ext columns order number blocks deleted file system total number logical overwrites performed faded number unnecessary overwrites number overwrites missed faded note deletes occurred data write require overwrite config reads writes run-time version table impact performance performance file system configurations busy hour trace shown configuration show number blocks read written trace run-time ext file system indirect ext modified obey reuse ordering indirect blocks version ext modified increment inode version number truncate configuration represents correct file system implementation required faded column measure extra work faded order cope inaccurate information column number missed overwrites correct system fourth column cost inaccuracy reasonable faded performs roughly overwrites minimal amount note version number modification ext faded misses deletes reason missed overwrites reported version configuration rarity case involving misclassified indirect block performance impact evaluate performance impact made ext running trace versions ext table shows results performance reduction number blocks written marginally higher due synchronous bitmap writes indirect block reuse ordering conclude practical performance secure delete explore foreground performance implicit explicit secure delete cost overwrites foreground performance impact tracking block generation liveness requires faded perform extra processing cost reverse engineering directly impacts application performance incurred critrun-time system implicit explicit postmark trace trace default securedelete securedelete securedelete table foreground impact postmark trace run-times postmark trace shown faded overwrite passes comparison run-time explicit secure delete trace shown postmark configured files transactions ical path disk operation quantify impact extra processing required faded foreground performance software prototype competes cpu memory resources host worst case estimates overheads run postmark file system benchmark trace file system running top faded postmark metadata intensive small-file benchmark heavily exercises inferencing mechanisms faded arrive pessimistic estimate perform sync end phase postmark causing disk writes complete account time results note wait completion delayed overwrites numbers performance perceived foreground task table compares performance faded default disk explicit secure delete table overwrite passes foreground performance affected extra cpu processing faded lower performance compared modified file system running normal disk explicit implementation performs incur overhead inference require file system modifications reported table corresponds row table note model cost sending free command scsi bus overheads explicit case optimistic idle time required quantify cost performing overwrites shredding microbenchmarks verified overwrites obtained sequential bandwidth due delayed ordered issue found block reuse occurs file system resulting multiple deletes block delaying overwrites significantly reduces overwrite traffic omit results due space constraints explore time required overwrites postmark configuration measure time benchmark complete including delayed overwrites postmark deletes files end run face worst case scenario entire working set benchmark overwritappears sixth symposium operating systems design implementation osdi run-time overwrites system implicit explicit postmark trace trace default securedelete securedelete securedelete table idle time requirement table shows total run-time benchmarks postmark trace time reported includes completion delayed overwrites ten accounting large overwrite times reported table hp-trace overwrite times 
reasonable blocks deleted trace reused subsequent writes overwrites performed conservative accounts steep increase overwrite passes implicit case explicit implementation incurs lower overwrite times compared faded perfect information deletes avoids extra overwrites incurred due conservatism implicit detection ntfs section present experience building support implicit liveness detection underneath windows ntfs file system main challenge faced underneath ntfs absence source code file system basic on-disk format ntfs details update semantics journaling behavior publicly result implementation tracks block liveness requires knowledge on-disk layout generation liveness tracking implemented details ntfs journaling mechanism fundamental piece metadata ntfs master file table mft record mft information unique file piece metadata ntfs treated regular file file mft file recovery log allocation status blocks volume maintained file called cluster bitmap similar block bitmap tracked ext block allocations deletions ntfs regularly writes modified bitmap blocks prototype implementation runs device driver linux similar setup earlier file systems virtual disk interpose exported logical disk virtual machine instance windows running vmware workstation track block liveness implementation shadow bitmap technique mentioned section detailed empirical observation long-running workloads found ntfs exhibit violation block exclusivity delete suppression properties mentioned section due absence source code assert ntfs conforms properties limitation points general difficulty implicit techniques underneath closed-source file systems file system conforms properties guaranteed file system vendor absence guarantees utility implicit techniques limited optimizations afford occasionally wrong implicit inference experience ntfs points utility characterizing precise set file system properties required forms liveness inference set properties constitutes minimal interface communication file system storage vendors ntfs confirmed conformance block exclusivity delete suppression properties storage system safely implement aggressive optimizations rely implicit inference discussion section reflect lessons learned case study refine comparison strengths weaknesses explicit implicit approaches ideal scenario implicit approach required storage system file system interface practice accurate liveness detection requires file system properties means file system modified conform requisite properties face storage system file system implicit approach pragmatic explicit approach changing interface main reasons implicit approach file system required file system conforms requisite properties file systems ext vfat ext -data journaling ntfs amenable block liveness detection change file system ext file system data journaling mode conforms properties required generation liveness detection cases implicit approach enables non-intrusive deployment functionality modifying file system conform set well-defined properties general modifying file system interface convey specific piece information discussed file system properties viewpoint implicit liveness detection properties enable richer information inferred association block owning inode required applications file-aware layout tracked accurately file system obeys reuse ordering consistent metadata properties ultimate goal arrive set properties enable wide variety information tracked implicitly outlining file systems designed enable appears sixth symposium operating systems design implementation osdi transparent extension storage system contrast approach changing interface requires introducing interface time piece information required related work liveness information storage systems recognized previous work existing proposals interface communicate liveness part radical set existing storage interface logical disks list-based interface storage includes command delete block list recent work suggests object-like interface storage moves responsibilities low-level storage management liveness tracking file system drives contrast wide-scale explicit notification approach imparting liveness intrusive large body file systems utilize existing block-based interface storage work implementing smarts storage system interface change similar implicit approach systems utilize limited form liveness inference autoraid requires information free space decide amount data stored raidautoraid infers blocks written dead inference weak form liveness block written subsequent deletes detected systems programmable disk make similar inferences existence proposals liveness information important storage systems systematic techniques acquiring information missing related implicit techniques work previous work semantically-smart disks work presented techniques blockbased storage system infer file system level information implemented set case studies trackaligned extents journaling secure delete correctness-sensitive case studies implemented required file system synchronously mounted synchronous file systems implicit information tracking trivial recent work d-graid considered asynchronous file systems layout mechanisms d-graid depend accuracy correctness acceptable d-graid predictions wrong fast recovery d-graid utilized block liveness easier property track generation liveness specific assumptions file system behavior work previous work generalizing techniques inference underneath wide range realistic file system behaviors demonstrating storage-level functionality correctness paramount utilize information reliably conclusion system layers evolve explicit implicit file system storage system interface change change change time interfaces layers obsolete sub-optimal necessitating evolution presented approaches interface evolution explicit implicit context embedding liveness information storage qualitative summary complexity approaches axes presented figure shown explicit approach appearing straightforward entails fair amount file system change practice requiring minimal support storage system factors explicit approach results simpler systems implicit case main strength implicit approach permits demonstration functionality interface enabling seamless deployment catalyzing rapid interface evolution acknowledgments nitin agrawal john bent timothy denehy todd jones james nugent florentina popovici vinod yegneswaran helpful comments mendel rosenblum excellent shepherding anonymous reviewers thoughtful feedback gordon hughes comments secure delete work sponsored nsf ccrccr- ccrngs- itritr- ibm emc agrawal kiernan srikant hippocratic databases vldb bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids isca bauer priyantha secure data deletion linux file systems usenix security august jonge kaashoek hsieh logical disk approach improving file systems sosp denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks usenix monterey june dowse malone recent filesystem optimisations freebsd freenix june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge self-organizing disk controller usenix jan ganger blurring line oses storage devices scs cmu-cs- dec ganger mckusick soules patt soft updates solution metadata update problem file systems acm tocs golding bosch staelin sullivan wilkes idleness sloth usenix winter pages gutmann secure deletion data magnetic solidstate memory usenix security july hughes personal communication appears sixth symposium operating systems design implementation osdi hughes coughlin secure erase disk drive data idema insight magazine katcher postmark file system benchmark netapp troctober mckusick joy leffler fabry fast file system unix tocs aug mesnier ganger riedel object-based storage ieee communications magazine august pennington strunk griffin soules goodson ganger storage-based intrusion detection watching storage activity suspicious behavior usenix security riedel kallahalla swaminathan framework evaluating storage system security fast roselli lorch anderson comparison file system workloads usenix ruemmler wilkes disk shuffling technical report hpl- laboratories schindler griffin lumb ganger trackaligned extents matching access patterns disk drive characteristics fast january 
sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid fast mar sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems fast sourceforge srm secure file deletion posix systems http srm sourceforge net sourceforge wipe secure file deletion http wipe sourceforge net sourceforge linux ntfs project http linux-ntfs net strunk goodson scheinholtz soules ganger self-securing storage protecting data compromised systems osdi tweedie future directions ext filesystem freenix june vmware vmware workstation http vmware products wang anderson patterson virtual log-based file systems programmable disk osdi wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february gum chen wang krishnamurthy anderson trading capacity performance disk array osdi guaranteed detection deletes prove techniques faded ext guarantee shredding deletes blocks contents reached disk delete inode occurs ext set blocks freed file results increment version number reset relevant bits data bitmap block pertaining freed blocks block freed assume written disk context written disk disk perform overwrite case bitmap block status block inode possibilities reused file system written disk reused write case block reused reused immediately file bitmap block dirtied eventually written disk disk immediately delete block liveness module overwrite case block reused case whereb reused inode possibilities case point receiving write disk thinks belongs thinks free belongs inode case disk thinks disk knew disk tracked previous version number eventually observes write dirtied version number increment disk note version number increased overwrite blocks thought belonged case includesb thusb overwritten restoring newer discussed section conservative overwrite contents guaranteed shredded case disk thinks free disk thinksb free treat orphan block written mark suspicious written context inode contents shredded case disk thinks disk observed pointing point current write disk observed allocated file system case c-i disk observed allocated thinks written context means disk case block deleted time past order allocated led version number incrementing disk observes written perform overwrite thinks belong case c-ii occurs means written disk owning deleted written case written context live overwritten discussed section holds block exclusivity property ext note case block deleted file quickly reallocated file special case case cases block written disk context file delete block file lead shred deleted contents indirect block detection uncertain disk wrongly corrupt pointer false indirect block file system change reuse ordering indirect blocks prevents case 
robust portable scheduling disk mimic florentina popovici andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract propose approach scheduling performs on-line simulation underlying disk simulation integrated system key challenges addressed rst simulator portable full range devices conguration automatic computation memory overheads low simulator disk mimic achieves goals building table-based model disk observes times previous requests show shortest-mimicked-timerst smtf scheduler performs approach perfect knowledge underlying device superior traditional scheduling algorithms c-look sstf results hold seek rotational characteristics disk varied introduction high-performance disk schedulers explored research literature progressively tuned performance characteristics underlying disks generation disk schedulers accounted behavior storage devices time disk schedulers analyzed focused minimizing seek time seek time order magnitude greater expected rotational delay early focus disk schedulers shifted rotational delay account rotational delays seek costs balanced level sophistication disk scheduler takes aspects underlying disk account track cylinder switch costs cache replacement policies mappings logical block number physical block number zero-latency writes worthington demonstrate algorithms effectively utilize prefetching disk cache perform intricate knowledge scheduler disk barriers realization operating system kernels speci cally obstacles overcome scheduler discover detailed knowledge underlying disk variety tools automatically acquire portions knowledge embedded disk model employed scheduler resulting scheduler con gured handle single disk speci characteristics disk scheduler knowledge current state disk exact position disk head head position exposed current disk controllers position predictable due low-level disk techniques wear leveling predictive failure analysis log updates scheduler control current position non-trivial techniques finally computational costs detailed modeling high uncommon time model request time larger time service request due dif culties disk schedulers leverage basic seek costs implemented real disks rotational position previous work performed simulation environments schedulers recently implemented researchers contained substantial simpli cations painstakingly tuned small group disks surprisingly disk schedulers found modern operating systems linux netbsd solaris attempt minimize seek time approach promising alternative approach embedding detailed knowledge disk scheduler embed on-line simulator disk scheduler scheduler on-line simulation underlying storage device predict request queue shortest positioning time variety disk simulators exist targeted performing traditional off-line simulations infrastructure performing on-line simulation fundamentally respects requirements on-line simulator stringent off-line simulator on-line simulator portable simulator model behavior disk drive practice on-line simulator automatic run-time con guration precise characteristics proceedings usenix annual technical conference june san antonio texas underlying device constructing simulator highly undesirable human administrator interact simulator finally on-line simulator low overhead computation memory overheads on-line simulator minimized simulator adversely impact system performance addition complexity introduces on-line simulator ample opportunities simpli cation on-line simulator opportunity observe run-time behavior device simulator con gure simulator adjust behavior device time on-line simulator specialized problem domain question finally on-line simulator parameterizable on-line simulator exploring versions device simulator functional model device contributions address implement scheduler aware underlying disk technology simple portable robust manner achieve goal introduce disk mimic meets requirements on-line simulator disk scheduling disk mimic based simple table-based approach input parameters simulated device index table entry table predicted output device table-based approach on-line simulation portably capture behavior variety devices requires manual con guration performed computational overhead signi challenge size table tractable identify input parameters signi cantly impact desired outputs method reducing input space depends largely domain on-line simulator deployed show disk scheduling input parameters suf cient predicting positioning time logical distance requests request type inter-request distance prediction issues resolved inter-request distance fairly coarse predictor positioning time result high variability times requests distance implication disk mimic observe instances distance summary metric distribution experimentally found summarizing small number samples works large number inter-request distances modern disk drive disk mimic record distances table reasonable size show simple linear interpolation represent ranges missing distances long number interpolations range checked measured values propose disk scheduling algorithm shortestmimicked-time- rst smtf picks request predicted disk mimic shortest positioning time demonstrate smtf scheduler utilize disk mimic ways specifically disk mimic con gured off-line on-line approaches performed automatically disk mimic con gured off-line performs series probes disk inter-request distances records resulting times scenario disk mimic complete control inter-request distances observed interpolated disk mimic con gured on-line records requests running workload resulting times note disk mimic con gured off-line on-line simulation performed on-line active system show disk mimic significantly improve throughput disks high utilization speci cally variety simulated real disks c-look sstf perform slower smtf demonstrate disk mimic successfully con gured on-line show disk mimic learns storage device smtf performs worse base scheduling algorithm c-look sstf quickly performs close off-line con guration approximately requests rest paper organized section describe smtf scheduler detail section describe disk mimic describe basic methodology evaluation section investigate issues con guring disk mimic offline section describe additional complexities con guring disk mimic on-line show performance section finally describe related work section conclude section scheduler modern disks implement scheduling device suggest system scheduling obsolete reasons system perform scheduling disks schedule limited number simultaneous requests restrictive space computational power constraints instances increased functionality requires scheduling system level iyer druschel introduce short waiting times scheduler proceedings usenix annual technical conference june san antonio texas preserve continuity stream requests single process interleaving streams processes shenoy vin implement service requirements applications implementing scheduling framework system brie describe approach system scheduler leverages disk mimic refer algorithm implemented scheduler shortest-mimicked-timerst smtf basic function smtf performs order queue requests request shortest positioning time determined disk mimic scheduled basic role optimizations made assumptions paper assume goal scheduler optimize throughput storage system fairness scheduler techniques achieving fairness weighting request age added smtf assume scheduler operating environment heavy disk traf queues disk hundreds thousands requests computational complexity scheduling algorithm important issue large queue lengths feasible perform optimal scheduling decision considers combinations requests greedy approach time request minimized evaluate performance smtf compare algorithms practice rst-comerst-served fcfs shortest-seek-timerst sstf c-look fcfs simply schedules requests order issued sstf selects request smallest difference logical block number lbn accessed disk c-look variation sstf requests serviced lbn proximity request serviced scheduler picks requests ascending lbn order requests serviced algorithm picks request queue lowest lbn continues service requests ascending order compare performance case implemented best-case-greedy scheduler simulated disks best-case scheduler long request simulated disk greedily picks request shortest positioning time refer scheduler greedyoptimal scheduler disk 
mimic disk mimic capture behavior disk drive portable robust cient manner predict performance disk disk mimic simple table indexed relevant input parameters disk disk mimic attempt simulate mechanisms components internal disk simply reproduces output function inputs observed reducing input parameters disk mimic table-driven approach predict time request function observable inputs fundamental issue reducing number inputs table tractable number device treated true black box internal behavior device disk mimic assume service time request function previous requests request ned parameters read write block number size time request data leads prohibitively large number input parameters indices table tractable approach make assumptions behavior device problem domain interest goal scheduler portable realistic range disk drives necessarily work hypothetical storage device high-level assumptions disks behave eliminate signi number input parameters disk mimic make assumptions current implementation disk mimic predicts time request input parameters request type inter-request distance interrequest distance logical distance rst block current request block previous request conclusion request type inter-request distance key parameters agrees previous researchers brie argue inter-request distance request type suitable parameters domain begin summarizing characteristics modern disk drives discussion classic paper ruemmler wilkes interested reader referred paper details background disk drive platters platter surface disk head reading writing surface data stored series concentric circles tracks single stack tracks common distance spindle called cylinder modern disks ram perform caching proceedings usenix annual technical conference june san antonio texas caching algorithm dif cult aspects disk capture model accessing block data requires moving disk head desired block time dominant components rst component seek time moving disk head desired track seek time reads writes reads performed aggressively read performed block read repeated performed wrong sector write rst verify sector avoid overwriting data component rotation latency waiting desired block rotate disk head time platter rotate roughly constant vary nominal rate result dif cult predict location disk head disk idle revolutions important positioning components mechanical movements accounted head track switch time head switch time takes mechanisms disk activate disk head access platter surface track switch time takes move disk head track cylinder rst disk appears client linear array logical blocks logical blocks mapped physical sectors platters indirection advantage disk reorganize blocks avoid bad sectors improve performance disadvantage client logical block located client derive mapping multiple sources complexity tracks numbers sectors specifically due zoning tracks platter sectors subsequently deliver higher bandwidth tracks spindle consecutive sectors track cylinder boundaries skewed adjust head track switch times skewing factor differs zones awed sectors remapped sparing sparing remapping bad sector track xed alternate location slipping sector track subsequent sector track input parameters previously explained read write operations times execute addition type operation issued uences service time account factors table-based model record request type read write current previous requests input parameters input parameter inter-request distance logical block addresses captures aforementioned underlying characteristics disk missing note ordering requests based time distance signi cantly distance due complexity disk geometry requests separated larger logical distance positioned rapidly relationship logical block address distance positioning time linear opinion ruemmler wilkes aspects disk modeled accuracy seek time calculated separate functions depending seek distance current nal cylinder position disk head reads writes head track switches rotation latency data layout including reserved sparing areas zoning track cylinder skew data caching read-ahead write-behind brie discuss extent components captured approach approach accounts combined costs seek time head track switches rotation layout probabilistic manner inter-request distance probability request crosses track cylinder boundaries requests distance cross number boundaries total positioning time number track seeks number head track switches amount rotation note table-based method tracking positioning time accurate advocated ruemmler wilkes expressing positioning time computed sum functions seek time rotation time caching disk mimic records precise positioning time distance cost incurred rotation disk components rotational distance previous current request elapsed time requests amount rotation occurred inter-request distance probabilistically captures rotational distance disk mimic record amount time elapsed request omission issue disk scheduling presence full queue requests case inter-arrival time requests disk negligible ignoring time inaccuracies scheduling rst request idle period disk idle scheduling important problem data layout incorporated fairly disk mimic number sectors track number cylinders impact measured values sizes determine probability request inter-request distance crosses boundary sizes impact probability observed time distriproceedings usenix annual technical conference june san antonio texas time number requests time number requests time number requests figure distribution off-line probe times inter-request distances graph shows inter-request distance x-axis show probes performed sorted time y-axis show time probe times ibm lzx disk bution zoning behavior bad sectors tracked model previous research shown level detail scheduling aspect model directly general caching disk mimic capture effects simple prefetching important aspect caching scheduling read sector entire track cached disk mimic observe faster performance accesses distances track respect con guring disk mimic on-line observing actual workload accurate con guring off-line locality workload captured complexity inter-request distance concentrate issues related input parameter values request type output disk mimic characteristics explore combinations input parameters discussions refer inter-request distance assume request type xed results illustrate complexity inter-request distance predictor request time show distribution times observed experiments con gure disk mimic off-line disk mimic con gures probing device xed-size requests inter-request distances covering disk negative positive disk mimic samples number points distance accesses block speci distance previous block avoid caching prefetching performed disk disk mimic accesses random location probe required distance observed times recorded table indexed inter-request distance operation type figure show small subset data collected ibm lzx disk gure shows distribution samples inter-request distances case y-axis shows request time sample points axis represent sample sorted increasing request time make important observations sampled times inter-request distance observed request time constant distance requests require require require multi-modal behavior time single request reliably predicted interrequest distance predict request distance faster slower request distance make reasonable predictions based probabilities data conclude request distance longer examining distributions inter-request distances observe number transitions percentage samples time varies inter-request distances number transitions graph corresponds roughly number track cylinder boundaries crossed inter-request distance data shows number important issues remain con guration disk mimic signi variation request times single inter-request distance summary metric summarize distribution samples required adequately capture 
behavior distribution inter-request distance sampled interpolate intermediate distances investigate issues section methodology evaluate performance smtf scheduling range disk drive technology presented proceedings usenix annual technical conference june san antonio texas con guration rotation seek head cyl track cyl sectors num time cyl switch switch skew skew track heads base fast seek slow seek fast rotate slow rotate fast seek rot capacity capacity table disk characteristics con gurations simulated disks times rotation seek head cylinder switch milliseconds cylinder track skews expressed sectors experiments base disk table implemented disk simulator accurately models seek time xed rotation latency track cylinder skewing simple segmented cache rst disk named base disk simulates disk performance characteristics similar ibm lzx disk seek times cache size number segments head cylinder switch times track cylinder skewing rotation times measured issuing scsi commands measuring elapsed time directly querying disk similar approach schindler ganger values provided manufacturer curve seek time modeled probing ibm lzx disk range seek distances measured distance cylinders previous cylinder position current curve tting values two-function equation proposed ruemmler wilkes short seek distances seek time proportional square root cylinder distance longer distances seek time proportional cylinder distance middle seek column represents cylinder distance switch functions occurs base disk seek distance smaller cylinders square root function disk con gurations simulate start base disk vary parameters inuence positioning time disk con guration number fast seek represents disk fast seek time numbers compute seek curve adjusted number sectors constitute cylinder skew similarly disk conguration number fast rotate time execute rotation decreased factor number track cylinder skew sectors increased disk con gurations account disks slower seek time slower rotation time faster seek time faster rotation time capacity base disk addition simulated disks run experiments ibm lzx disk time scaling factor fcfs sstf smtf min smtf probabilistic smtf max smtf median smtf greedy-optimal figure sensitivity summary metrics graph compares performance variety scheduling algorithms base simulated disk week-long trace smtf schedulers interpolation performed samples obtained data point x-axis shows compression factor applied workload y-axis reports time spent disk evaluate scheduling performance show results set traces collected labs cases focus trace busiest disk week performance metric report time workload spent disk impact heavier workloads longer queue lengths compress inter-arrival time requests scaling time attempt preserve dependencies requests workload observing blocks requested assume request repeated block serviced request dependent previous request rst completing hold repeated requests subsequent requests previous identical request completes proceedings usenix annual technical conference june san antonio texas time ordered requests summary metric probability percentage error probabildisk time ordered requests summary metric percentage error meandisk time ordered requests summary metric maximum percentage error maxdisk figure demerit figures smtf probability maximum summary metrics graph shows demerit gure summary metric distributions correspond day experiments shown figure compression factor off-line con guration smtf scheduler con gured on-line off-line explore case disk mimic con gured off-line disk mimic con gured off-line simulation predictions required scheduler performed on-line system previously con guring disk mimic off-line involves probing underlying disk requests range inter-request distances note model con gured off-line process con guring smtf remains automatic portable range disk drives main drawback con guring disk mimic offline longer installation time device added system disk probed workload traf summary data enable smtf scheduler easily compare expected time requests queue disk mimic supply summary distribution function inter-request distance multi-modal characteristics distributions choice summary metric obvious evaluate summary metrics median maximum minimum probabilistic randomly picks sampled distribution probability results summary metrics base simulated disk shown figure workload week-long trace scaled compression factor noted x-axis graph shows fcfs sstf c-look perform worse smtf schedulers expected smtf schedulers perform worse greedy-optimal scheduler approach workload results show inter-request distance predict positioning time merits attention comparing performance smtf approaches summary metric performs time samples fcfs sstf smtf greedy-optimal figure sensitivity number samples graph shows performance smtf improves samples results simulated disk week-long trace compression factor x-axis number samples smtf y-axis shows time spent disk differently ordering performance worse median maximum probabilistic minimum interesting note scheduling performance summary metric correlated accuracy accuracy disk models evaluated demerit gure ned root square horizontal distance time distributions model real disk point brie illustrated figure shows distribution actual times versus predicted times metrics probabilistic maximum expected probabilistic model demerit gure requests distribution predicts expected match real device probabilistic model performs poorly smtf time predicts request differ signi cantly acproceedings usenix annual technical conference june san antonio texas time request inter-request distance full range time request inter-request distance close-up figure values samples function inter-request distance graph left shows time entire set inter-request distances simulated disk graph shows close-up inter-request distances distances qualitatively similar saw-tooth behavior tual time request conversely maximum results poor demerit gure performs adequately scheduling fact smtf maximum performs signi cantly minimum similar demerit gures finally summary distribution achieves performance result demerit gure found performs days traces examined remainder experiments observed samples summary data inter-request distance number samples large variation times single inter-request distance disk mimic perform large number probe samples true distribution reduce time required con gure disk mimic off-line perform samples evaluate impact number samples smtf performance figure compares performance smtf function number samples performance fcfs c-look sstf optimal expected performance smtf increases samples workload disk performance smtf continues improve approximately samples interestingly single sample inter-request distance disk mimic performs fcfs c-look sstf interpolation number samples performed interrequest distance impacts running time off-line probe process greater issue distance explicitly probed intere time percent error check checks checks interpolation sstf figure sensitivity interpolation graph shows performance interpolation function percent allowable error lines correspond numbers check points x-axis percent allowable error y-axis time spent disk results base simulated disk week-long trace compression factor polated distances due large number potential inter-request distances modern storage device times number sectors negative positive distances performing probes signi amount time storing values prohibitive disk size amount memory required table exceed explore distances interpolated making detailed assumptions underlying disk illustrate potential performing simple interpolations show function inter-request distance figure graph left proceedings usenix annual technical conference june san antonio texas check points acceptable error table allowable error 
interpolation table summarizes percentage interpolated relative probed order infer interpolation successful check points performed inter-request distances allowable error increases numbers gathered running number workloads simulated disks observing point performance interpolation degrades relative interpolation shows values inter-request distances simulated disk curve bands emanating middle point corresponds seek curve disk short seeks time proportional square root distance long time linear distance width bands constant corresponds rotation latency disk graph shows close-up inter-request distances graph shows times follow distinct saw-tooth pattern result simple linear model interpolate distances care ensure model applied short distances length linear regions varies disks function track cylinder size goal determine distances interpolated successfully challenge determine interpolated close actual scheduling performance impacted negligibly basic off-line interpolation algorithm disk mimic performs samples interrequest distances left chooses random distance middle left linearly interpolates middle means left interpolated middle error percent probed middle interpolation considered successful distances left interpolated interpolation successful disk mimic recursively checks smaller ranges distances left middle middle intermediate points successfully interpolated points probed additional con dence linear interpolation valid region slight variation points left interpolated checked points predicted desired level accuracy interpolation considered successful intuition performing check points higher error rate interpolation successful figure shows performance smtf distances interpolated graph shows effect increasing number intermediate points checked increasing acceptable error error interpolation make observations graph smtf performance decreases allowable error check points increases result expected note performance decreases dramatically error error checked distances increased interpolated distances inaccurate single check point error level found interpolated values accurate level average error interpolated values increases shown summary error increases signi cantly linear relationship distances left interpolation performed smtf performance xed error increases number intermediate check points effect performing checks con linear interpolation distances valid check points error interpolated points accurate level average error shown table summarizes ndings wider number check points table shows allowable error percentage function number check points achieve scheduling performance similar probes nal probe process operate interpolation distance left error deemed successful distances left errors interpolation successful progressively check points made higher error rates successful approach distances disk interpolated probed scheduling performance virtually unchanged interpolation leads fold memory savings disk characteristics demonstrate robustness portability disk mimic smtf scheduling full range simulated disks table performance fcfs c-look sstf smtf relative proceedings usenix annual technical conference june san antonio texas disk configuration slowdown fcfs sstf smtf figure sensitivity disk characteristics ure explores sensitivity scheduling performance disk characteristics shown table performance shown relative greedy-optimal report values smtf interpolation performance smtf interpolation probes similar greedy-optimal disks summarized figure show performance smtf interpolation performance smtf interpolation identical expected fcfs performs worst entire range disks performing factor slower greedy-optimal c-look sstf perform seek time dominates performance disks sstf performs c-look cases finally smtf performs rotational latency signi component request positioning disks summary range disks smtf performs c-look sstf scheduling greedy-optimal algorithm show smtf handle performance variation real disks compare performance implementation smtf c-look run ibm lzx disk week trace achieve performance improvement smtf compared c-look improvement idle time removed trace performance improvement signi reasons ibm lzx disk high ratio seek rotation time performance improvement smtf relative c-look greater rotation time signi component positioning trace exercises large amount data disk locality workload low trace seek time dominates positioning time explore effect workload locality create synthetic workload random reads writes idle time maximum inter-request distance varied speci x-axis figure graph shows performance improvement smtf relaslowdown maximum inter-request distance real disk comparison smtf off-line figure real disk performance graph shows slowdown c-look compared smtf congured off-line workload synthetically generated trace numbers averages runs standard deviation reported x-axis shows maximum inter-request distance existent trace y-axis reports percentage slowdown algorithm tive c-look varies interrequest distance varies systems linux ext optimize locality placing related les cylinder group smtf optimize accesses c-look practice smtf viable option scheduling real disks on-line con guration explore smtf scheduler con guration performed on-line approach overhead installation time probe disk drive disk mimic observes behavior disk workload runs off-line version disk mimic records observed disk times function inter-request distance case control inter-request distances observes general approach on-line version assume lessons learned off-line con guration hold continue represent distribution times inter-request distance continue rely interpolation note disk mimic con gured on-line interpolation saving space providing information distances observed primary challenge smtf address situation schedule requests inter-request distances unknown times inter-request distance observed proceedings usenix annual technical conference june san antonio texas sstf slowdown base-line pri interp pri interp set interp set interp percentage slowdown day performance hybrid versions sstf set sstf set sstf interp set set interp figure performance on-line smtf rst graph compares performance variations online smtf performance day week-long trace shown relative off-line smtf graph shows performance online-set improves time inter-request distances observed disk mimic disk mimic unable con interpolated successfully algorithms comparison algorithms assume base scheduler c-look sstf disk mimic suf cient information rst algorithm online-priority schedules requests disk mimic information speci cally online-priority strict priority requests queue inter-request distance time requests times request minimum time picked online-priority base scheduler c-look sstf inter-request distances current queue problems approach preference scheduling inter-request distances online-priority perform worse base scheduler schedules diversity distances produced disk mimic observe cient distances algorithm online-set improves limitations decision base scheduler starting point scheduling request disk mimic knowledge performance improved speci cally online-set rst considers request base scheduler pick time distance disk mimic request scheduled time requests inter-request distances considered fastest chosen online-set improve performance base scheduler schedule variety inter-request distances learning experimental results evaluate performance on-line algorithms return base simulated disk left-most graph figure compares performance online-priority online-set c-look sstf baseline algorithm interpolation performance expressed terms slowdown relative off-line version smtf make observations graph surprising c-look performs sstf workload disk smtf performs noticeably sstf c-look base c-look disk mimic observe inter-request distances negative backward discover distances close online-set performs onlinepriority sstf base scheduler interpolation signi cantly improve performance online-priority online-set c-look leads small improvement online-set sstf off-line con guration primary bene interpolation reduce memory requirements disk mimic opposed improving performance right-most graph figure illustrates performance online-set improves time inter-request distances observed performance 
online-set algorithms interpolation base-line schedulers sstf c-look day original trace approximately requests performance online-set sstf converges off-line version days requests point feel opportunities proceedings usenix annual technical conference june san antonio texas improving performance on-line smtf relative off-line smtf current on-line implementations slow time distance observed initially scheduler avoid distance faster address requiring distance minimum number samples classi current algorithm leverage idle time perform probes unknown inter-request distances idle times disk mimic learn characteristics disk related work approach propose brings areas study disk modeling disk scheduling present related work areas compare method disk modeling classic paper describing models disk drives ruemmler wilkes main focus work enable informed trade-off simulation effort resulting accuracy model ruemmler wilkes evaluate aspects disk modeled high level accuracy demerit gure researchers noted additional non-trivial assumptions made model disks desired accuracy level modeling cache behavior challenging aspect detailed knowledge modeling disks documentation researchers developed innovative methods acquire information worthington describe techniques scsi drives extract time parameters seek curve rotation speed command overheads information data layout disk caching prefetching characteristics techniques automated work modeling storage devices tables past performance explored previous work previous work high-level system parameters load number disks operation type indices table anderson results online assist recon guration disk arrays approach similar thornock work authors stochastic methods build model underlying drive application model standard off-line simulation specifically authors study block reorganization similar earlier work ruemmler wilkes higher level seltzer small suggest situ simulation method building adaptive operating systems work authors suggest operating systems utilize in-kernel monitoring adaptation make informed policy decisions tracing application activity vino system determine current policy behaving expected policy switched place actual simulations system behavior performed offline resort poor performance detected disk scheduling disk scheduling long topic study computer science rotationally-aware schedulers existence early work seltzer jacobson wilkes due dif culty implementation early works focused solely simulation explore basic ideas recently implementations rotationally-aware schedulers literature crafted extreme care recently worthington examine bene detailed knowledge disk drives os-level disk schedulers algorithms mesh modern prefetching caches perform detailed logical-to-physical mapping information anticipatory scheduling recent scheduling development complementary on-line simulationbased approach anticipatory scheduler makes assumption locality stream requests process waiting request servicing request process performance improved authors note dif culty building rotationally-aware scheduler empirically-generated curvetted estimate disk access-time costs disk mimic yield performance bene simpli approach conclusions paper explored issues simulation system make run-time scheduling decisions focused disk simulator automatically model range disks human intervention shown disk mimic model time request simply observing request type logical distance previous request predicting behave similarly past requests parameters disk mimic con gure disk probing disk off-line slight performance cost observing requests disk on-line demonstrated shortest-mimicked-timerst smtf disk scheduler signi cantly improve disk performance relative fcfs sstf c-look range disk characteristics future plan show smtf scheduling range storage devices disk drives raid systems networkproceedings usenix annual technical conference june san antonio texas attached storage devices mems-based devices tapes non-volatile memory building blocks storage system devices complex performance characteristics ideal scheduler automatically adapt devices acknowledgments nathan burnett timothy denehy brian forney muthian sivathanu feedback paper vern paxson shepherd anonymous reviewers thoughtful suggestions greatly improved content paper finally computer systems lab tireless assistance providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation anderson simple table-based modeling storage devices technical report hpl-ssp- laboratories july andrews bender zhang algorithms disk scheduling problem ieee symposium foundations computer science focs pages arpaci-dusseau arpaci-dusseau information control gray-box systems symposium operating systems principles sosp pages october ganger worthington patt disksim simulation environment version manual http citeseer nec article ganger disksim html gibson nagle amiri chang feinberg gobioff lee ozceri riedel rochberg zelenka file server scaling network-attached secure disks proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle june golding bosch staelin sullivan wilkes idleness sloth proceedings winter usenix technical conference pages orleans louisiana january gotlieb macewen performance movable-head disk storage devices journal association computing machinery grif schindler schlosser bucy ganger timing-accurate storage emulation proceedings usenix conference file storage technologies fast pages monterey january hillyer silberschatz modeling performance characteristics serpentine tape drive proceedings sigmetrics conference measurement modeling computer systems pages hofri disk scheduling fcfs sstf revisited communications acm huang chiueh implementation rotation latency sensitive disk scheduler technical report ecsl-tr suny stony brook march iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous acm symposium operating systems principles pages october jacobson wilkes disk scheduling algorithms based rotational position technical report hpl-csp- laboratories kotz toh radhakrishnan detailed simulation model disk drive technical report dartmouth college patterson gibson katz case redundant arrays inexpensive disks raid sigmod record acm special interest group management data september ruemmler wilkes disk shuf ing technical report hpl- hewlett packard laboratories october ruemmler wilkes unix disk access patterns proceedings usenix winter technical conference pages ruemmler wilkes introduction disk drive modeling ieee computer proceedings usenix annual technical conference june san antonio texas schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november schlosser grif nagle ganger designing computer systems memsbased storage architectural support programming languages operating systems pages seltzer chen ousterhout disk scheduling revisited proceedings usenix winter technical conference pages berkeley seltzer small self-monitoring self-adapting systems proceedings workshop hot topics operating systems pages chatham shenoy vin cello disk scheduling framework next-generation operating systems proceedings sigmetrics conference measurement modeling computer systems pages june shriver merchant wilkes analytic behavior model disk drives readahead caches request reordering proceedings sigmetrics conference measurement modeling computer systems pages talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley teorey pinkerton comparative analysis disk scheduling policies communications acm thornock flanagan stochastic disk simulation technique proceedings winter simulation conference pages wang reiher popek kuenning conquest performance disk persistent-ram hybrid system proceedings usenix annual technical conference usenix pages monterey june wilhelm anomaly disk scheduling comparison fcfs sstf seek scheduling empirical model disk accesses communications acm wilkes pantheon storage-system simulator technical report hpl-ssp- laboratories palo alto december worthington ganger patt scheduling algorithms modern disk drives proceedings acm sigmetrics conference measurement modeling computer systems pages nashville usa worthington ganger patt 
wilkes on-line extraction scsi disk drive parameters technical report cse-tr- carnegie mellon gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation pages san diego usenix association 
semantically-smart disk systems muthian sivathanu vijayan prabhakaran florentina popovici timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract propose evaluate concept semantically-smart disk system sds opposed traditional smart disk sds detailed knowledge system disk system including information on-disk data structures system sds exploits knowledge transparently improve performance enhance functionality beneath standard block read write interface automatically acquire knowledge introduce tool eof discover le-system structure types systems show sds exploit knowledge on-line understand le-system behavior quantify space time overheads common sds showing excessive study issues surrounding sds construction designing implementing number prototypes case studies case study exploits knowledge aspect system implement powerful functionality beneath standard scsi interface surprising amount functionality embedded sds hinting future disk manufacturers compete enhanced functionality simply cost-per-byte performance introduction true knowledge confucius microprocessors memory chips smaller faster cheaper embedding processing memory peripheral devices increasingly attractive proposition placing processing power memory capacity smart disk system functionality migrated system disk raid providing number potential advantages traditional system computation takes place data improve performance reducing traf host processor disk disk system exploit low-level information typically le-system level including exact head position blockmapping information finally unmodi systems leverage optimizations enabling deployment broad range systems smart disk systems great promise realizing full potential proven dif cult causative reason shortfall narrow interface systems disks disk subsystem receives series block read write requests inherent meaning data structures system bitmaps tracking free space inodes data blocks directories indirect blocks exposed research efforts limited applying disk-system intelligence manner oblivious nature meaning system traf improving write performance writing blocks closest free space disk ful potential retain utility smart disk systems smarter interface storage remains system acquire knowledge system exploit understanding order enhance functionality increase performance storage system understands blocks constitute perform intelligent prefetching perle basis storage system blocks unused system utilize space additional copies blocks improved performance reliability storage system detailed knowledge system structures policies semantically-smart disk system sds understands meaning operations enacted important problem solved sds information discovery disk learn details system on-disk data structures straight-forward approach assume disk exact white-box knowledge system structures access relevant header les cases information unavailable cumbersome maintain paper explore gray-box approach attempting proceedings usenix conference file storage technologies fast san francisco california automatically obtain le-system speci knowledge storage system develop present ngerprinting tool eof automatically discovers le-system layout probes observations show eof smart disk system automatically discover layout class systems similar berkeley fast file system ffs show exploit layout information infer higher-level le-system behavior processes classi cation association operation inferencing refer ability categorize disk block data inode bitmaps superblock detect precise type data block directory indirect pointer associate data block inode relevant information identify higher-level operations creation deletion sds techniques implement desired functionality prototype smart disk system software infrastructure in-kernel driver interposes read write requests system disk prototype environment explore challenges adding functionality sds adhering existing interfaces running underneath stock system paper focus linux ext ext systems netbsd ffs understand performance characteristics sds study overheads involved ngerprinting classi cation association operation inferencing microbenchmarks quantify costs terms space time demonstrating common overheads excessive finally illustrate potential semantically-smart storage systems implemented number case studies sds framework aligning les track boundaries increase performance smallle operations information lesystem structures implement effective secondlevel caching schemes volatile non-volatile memory secure-deleting disk system ensures non-recoverability deleted les journaling storage system improve crash recovery time case studies demonstrate broad range functionality implemented semantically-smart disk system cases demonstrate sds tolerate imperfect information system key building robust semantically-smart disk systems rest paper organized section discuss related work discuss lesystem ngerprinting section classi cation association section operation inferencing section evaluate system section present case studies section conclude section related work related work smart disks grouped categories rst group assumes interface storage systems xed changed category sds belongs research group proposes storage interface requiring systems modi leverage interface finally group proposes interface programming model applications fixed interfaces focus paper integration smart disks traditional system environment environment system narrow scsi-like interface storage disk persistent store data structures early smart disk controller loge harnessed processing capabilities improve performance writing blocks current disk-head position wang log-based programmable disk extended approach number ways quick crash-recovery free-space compaction systems assume require knowledge system structures storage system interfaces developed provided local setting opportunities functionality network packet lter slice virtual service slice interpose nfs traf clients implement range optimizations preferential treatment small les interposing nfs traf stream simpler scsi-disk block stream contents nfs packets well-de ned high-end raid products perfect place semantic smartness typical enterprise storage system substantial processing capabilities memory capacity emc symmetrix server eighty mhz motorola microprocessors con gured memory high-end raid systems leverage resources perform bare minimum semanticallysmart behavior storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically paper explore acquisition exploitation detailed knowledge system behavior expressive interfaces primary factors limits addition functionality smart disk narrow interface systems storage surprising research investigates changing interface brie highlight projects mime investigates enhanced interface context intelligent raid controller speci cally mime adds primitives proceedings usenix conference file storage technologies fast san francisco california clients control updates storage visible traf streams commit order operations logical disks expand interface allowing system express grouping preferences lists systems simpli maintain information raid exposes per-disk information informed system lfs providing performance optimizations control redundancy improved manageability storage finally ganger suggests reevaluation interface needed outlines relevant case studies track-aligned extents explore paper freeblock scheduling recent work storage community suggests evolution storage place disks general-purpose network standard scsi bus suggested network disks export higher-level object-like interface moving responsibilities low-level storage management system drives speci challenges context xed object-based interface systems storage provide interesting avenue research utility semantic awareness programming environments contrast integration underneath traditional system work focused incorporating active storage parallel programming environments recent work active disks includes acharya riedel amiri research focuses partition applications host disk cpus minimize data transferred system busses inferring on-disk structures fingerprinting file system semantically smart disk implement interesting functionality interpret types blocks read written disk speci characteristics blocks sds practical information obtained robust manner require human 
involvement alternatives obtaining information rst approach directly embeds knowledge system sds onus understanding target system developer sds obvious drawbacks sds rmware updated system upgraded sds robust target system approach target system informs sds data structures run-time case responsibilities target system numerous disadvantages approach importantly target system changed system process access information directly communicate sds communication channel existing protocols added target system sds finally dif cult ensure speci cation communicated sds matches actual system implementation approach sds automatically infers system data structures bene approach speci knowledge target system required sds developed assumptions made sds target system checked deployed additional work required con gure sds installed sds deployed environments difculty approach potential semantically-smart storage system explore sds automatically acquire layout information ngerprinting software automatically inferring system structures bears similarity research efforts reverseengineering researchers shown bit-level machine instruction encodings semantic meaning assembly instructions deduced developed techniques identify parameters tcp protocol extract lowlevel characteristics disks determine buffer-cache policies understand behavior real-time cpu scheduler assumptions automatically inferring layout information arbitrary system challenging problem important rst step developed utility called eof extraction filesystems extract layout information ffs-like systems journaling capabilities veri eof identify data structures employed linux ext ext netbsd ffs eof understand future ffs-like system adheres assumptions layout data structures disk general disk blocks statically exclusively assigned categories data inodes bitmaps free allocated data blocks inodes summary information superblock group descriptors log data eof identi block addresses disk allocated category data blocks data block dynamically data directory listings pointers data blocks indirect block data blocks shared les eof identi structure directory data proceedings usenix conference file storage technologies fast san francisco california eof assumes record directory data block length record entry length entry inode number entry eld directory entry assumed multiple bits eof assumes indirect blocks -bit pointers inode blocks inode block inodes inode consumes n-th block eof assumes nition inode eld static time eof identi location absence elds inode size blocks number data blocks allocated inode ctime time inode changed mtime time data changed dtime deletion time links number links inode generation number data pointers number combination direct pointers single double triple indirect pointers dir bits bits change directory inodes exception dir bits elds identify default assumed multiple bits multiple elds identi bits blocks links elds size eld assumed largest multiple bits lead overlapping elds bitmap blocks bitmaps data inodes share single block separate blocks bits data inode bitmap blocks one-to-one linear mapping data blocks inodes bitmap block valid log data log data journaling system managed circular contiguous buffer make assumptions contents log future feasibility inferring on-disk data structures depends assumption production systems change slowly time assumption hold system developers strong motivation on-disk structures legacy systems continue operate examining systems past present corroborates belief on-disk structure ffs system changed years linux ext system layout conception ext journaling system backward compatible ext extensions freebsd ffs designed avoid on-disk algorithm overview eof software system made sds partition eof run partition sds understands context deployed basic structure eof user-level probe process performs operations system generating controlled traf streams disk sds high-level operations performed disk traf result observing blocks written bytes blocks change sds infers blocks type system data structures offsets block type eld sds knowledge con gure simultaneously verifying target system behaves expected sds correlate traf observes system operations performed probe process correlation requires pieces functionality probe process ensure blocks operation ushed system cache written sds ensure probe process unmounts system unmounting re-mounting sparingly increases running time eof probe process occasionally inform sds speci operation ended probe process communicates sds writing distinct pattern fencepost sds pattern resulting traf message probe process general techniques eof identify blocks inode elds identify data blocks sds pattern probe process writes test les classify blocks elds sds attempts isolate unique unclassied block written operation set operations operations algorithm phases eof composed phases eof isolates summary blocks log eof identi data blocks data bitmaps eof inodes inode bitmaps blocks classi eof isolates inode elds finally eof identi elds directory entries bootstrapping phase goal bootstrapping isolate blocks frequently written phases tered blocks interest phase isolates summary blocks log inode data blocks fencepost test directory test les probe process creates fencepost number test les test directory sds identies data blocks searching patterns eof identi blocks belonging log exists step probe process synchronously appends data pattern proceedings usenix conference file storage technologies fast san francisco california test les sds observes blocks meta-data blocks written circular pattern belong log block traf matches pattern eof infers system perform journaling eof identi summary blocks probe process unmounts system written blocks classi log data identi summary blocks isolate inode blocks repeatedly written probe process performs chmod fencepost test directory test les case inode written allowing classied data blocks belonging test directory identi changing test blocks previously unidenti blocks written finally determine separate bitmap blocks data inode blocks linux ext ext single bitmap block shared netbsd ffs eof creates sds observes unclassi blocks determine bitmap blocks shared separate data inodes simplify presentation remainder discussion case data inode bitmaps separate blocks eof correctly handles shared case case eof isolates speci bits shared bitmap block devoted inode data block state data data-bitmap blocks phase eof continues identifying blocks disk data data bitmaps isolate blocks probe process appends blocks data pattern test les blocks match pattern classi assumed data-bitmap blocks indirect-pointer blocks eof differentiates inferring blocks written les data-bitmap blocks care create small les single lls bitmap block bitmap block special case smaller expected completely cleanup phase test les deleted inodes inode-bitmap blocks phase identifying inodes bitmaps requires creating les distinct steps required probe process creates les inodes inode bitmaps modi probe process performs chmod les inodes inode bitmaps written inodes inode bitmaps distinguished phase calculates size inode performed recording number times block identi inode dividing block size observed number inodes block inode fields phase point eof classi blocks disk belonging categories data structures phase identi elds inodes observing elds change operations brevity describe eof infers blocks links generation number elds rst inode elds eof identi size times requires steps probe process creates sds 
stores inode data compare inode data written steps probe process overwrites data inode elds change related time probe process appends small amount data data pointer added point size eld identi data changed step step fourth probe process performs operation change inode changing data adding link changing permissions sds isolate mtime changed step ctime changed finally deleted deletion-time eld observed eof identi location level data pointers inode probe process repeatedly appends sds observes bytes inode change changed previous step eof infers location indirect pointers observing additional data block written additional pointer updated inode improve performance write block probe process seeks progressively larger amount seek distance starts block increases size handled detected indirection level finally eof isolates inode bit elds designate directories probe process alternately creates les directories sds histograms directory inodes histogram eof records count times bit inode type determine directory elds eof isolates bits les directories vice versa bits values considered identify les versus directories soft link bits identi similar manner directory entries phase nal phase eof identi structure entries directory eof infers offsets entry length probe proproceedings usenix conference file storage technologies fast san francisco california cess creates sds searches directory data block eld designating length validation deleted step repeated numerous times lenames lengths eof nds location record length assumption length record remaining space directory data block length reduced record added probe process creates additional les sds simply records offsets change previous entries finally offset inode number found assumption directory entry step probe process creates empty directories sds isolates inode offset recording differences data blocks directories assertion assumptions major challenge automatic inferencing ensure sds correctly identi behavior target system robust system meeting assumptions eof mechanisms detect assumption fails case system identi non-supported sds operates correctly semantic knowledge blocks expected written speci step speci blocks observed eof detects violation veri violations identi appropriately eof run non-ffs systems msdos vfat reiserfs additional bene eof con gure sds system bugs identi running eof ext linux isolated bugs sds observed incomplete traf key steps problem tracked back ext bug data written seconds prior unmount ushed disk probe process noted error inodes allocated case ext incorrectly marks system dirty eof enables checks system easily obtained methods exploiting structural knowledge classi cation association key advantage sds ability identify utilize important properties block disk properties determined direct indirect classi cation association direct classi cation blocks easily identi location disk indirect classi cation blocks identi additional information identify directory data indirect blocks inode examined finally association data block inode connected cases sds requires functionality identify change occurred block functionality implemented block differencing infer data block allocated single-bit change data bitmap observed change detection potentially costly operations sds reasons compare current block version block sds fetch version block disk avoid overhead cache blocks employed comparison expensive location difference byte block compared byte block quantify costs section direct classi cation direct classi cation simplest cient form on-line block identi cation sds sds determines type block performing simple bounds check calculate set block ranges block falls ffs-like system superblock bitmaps inodes data blocks identi technique indirect classi cation indirect classi cation required type block vary dynamically simple direct classi cation precisely determine type block ffs-like systems indirect classi cation determine data block data directory data form indirect pointer block single double triple indirect block illustrate concepts focus directory data differentiated data steps identifying indirect blocks versus pure data similar identifying directory data basic challenge identifying data block belongs directory track inode points data check type directory perform tracking sds snoops inode traf disk directory inode observed data block numbers inserted hash table sds removes data blocks hash table observing blocks freed block differencing bitmaps sds identify block directory block presence table directory data discuss complications approach proceedings usenix conference file storage technologies fast san francisco california sds guarantee correctly identify blocks les directories speci cally data block present hash table sds infers data corresponds cases directory inode sds result hash table situation occur directory created blocks allocated existing directories system guarantee inode blocks written data blocks sds incorrectly classify newly written data blocks problem occur classifying data blocks read case system read inode block data block data block number sds inode rst correctly identify subsequent data blocks transient misclassi cation problem depends functionality provided sds instance sds simply caches directory blocks performance tolerate temporary inaccuracy sds requires accurate information correctness ways ensured rst option guarantee system writes inode blocks data blocks true default ffs soft updates linux ext mounted synchronous mode option buffer writes time classi cation made deferred classi cation occurs inode written disk data block freed inferred monitoring data bitmap traf sds perform excess work obliviously inserts data blocks hash table directory inode read written inode recently passed sds causing hash table updated optimize performance sds infer block added modi deleted time directory inode observed ensure blocks added deleted hash table process operation inferencing detail section identifying indirect blocks process identifying indirect blocks identical identifying directory data blocks case sds tracks indirect block pointers inodes read written maintaining hash table single double triple indirect block addresses sds determine data block indirect block association association connect data blocks inodes size creation date sds association achieved simple space-consuming approach similar indirect classi cation sds snoops inode traf inserts data pointers addressto-inode hash table concern table size accurate association table grows proportion number unique data blocks read written storage system system booted approximate information tolerated sds size table bounded detecting high-level behavior operation inferencing block classi cation association provide sds cient identifying special kinds blocks operation inferencing understand semantic meaning observed blocks outline sds identify system operations observing key challenge operation inferencing sds distinguish blocks valid version instance newly allocated directory block written compared contents block block contained arbitrary data identify versions sds simple insight metadata block written read contents block relevant detect situation sds maintains hash table meta-data block addresses read past meta-data block read added list block freed block bitmap reset removed list block allocated data freed reallocated directory block address present hash table sds contents illustrative purposes section 
examine sds infer create delete operations discussion speci ext similar techniques applied ffs-like systems file creates deletes steps identifying creates deletes rst actual detection create delete determining inode affected describe detection mechanisms logic determining inode rst detection mechanism involves inode block inode block written sds examines determine inode created deleted valid inode non-zero modi cation time proceedings usenix conference file storage technologies fast san francisco california deletion time modi cation time non-zero deletion time non-zero means inode newly made valid created similarly reverse change newly freed inode deleted indication change version number valid inode delete create occurred cases inode number calculated physical position inode disk on-disk inodes inode numbers detection mechanism involves inode bitmap block bit set inode bitmap created inode number represented bit position similarly newly reset bit deleted update directory block indication newly created deleted directory data block written sds examines block previous version directory entry dentry added inode number obtained dentry case removed dentry contents dentry inode number deleted newly created deleted choice mechanism combinations thereof depends functionality implemented sds sds identify deletion immediately creation inode number inode bitmap mechanism sds observe change bitmap operations grouped due delayed write system case modi cation times version numbers similarly newly created deleted directory block-based solution cient file system operations general technique inferring logical operations observing blocks versions detect system operations note cases conclusive inference speci logical operation sds observe correlated multiple meta-data blocks semantically-smart disk system infer renamed observes change directory block entry inode number stays note version number inode stay similarly distinguish creation hard link normal directory entry inode examined evaluation section answer important questions sds framework cost ngerprinting system time overheads classi cation association operation inferencing space overheads proceeding evaluation rst describe experimental environment platform prototype sds employ software-based infrastructure implementation inserts pseudo-device driver kernel interpose traf system disk similar software raid prototype appears systems device system mounted primary advantage prototype observes information traf stream actual sds system current infrastructure differs important ways true sds importantly prototype direct access low-level drive internals information made dif cult sds runs system host interference due competition resources initial case studies prime importance performance characteristics microprocessor memory system actual sds high-end storage arrays signi processing power processing capacity trickle lower-end storage systems experimented prototype sds linux linux netbsd operating systems underneath ext ext ffs systems experiments paper performed processor slow modern standards mhz pentium iii processor k-rpm ibm lzx k-rpm quantum atlas iii disk experiments employ fast system comprised ghz pentium k-rpm seagate cheetah disk gauge effects technology trends off-line layout discovery subsection show time run ngerprinting tool eof reasonable modern disks eof run system runtime eof determine common case performance sds runtime eof prohibitive disks larger potential solution parallelism proceedings usenix conference file storage technologies fast san francisco california time partition size costs fingerprinting eof phase phase slow system fast system figure costs fingerprinting gure presents time breakdown ngerprinting slow system ibm disk fast system running underneath linux ext x-axis vary size partition ngerprinted y-axis shows time phase time-consuming components eof parallelizable reduce run-time disk arrays figure presents graph time run eof single-disk partition size partition increased show performance results slow system ibm disk fast system graph shows phase determines locations data blocks data bitmaps phase determines locations inode blocks inode bitmaps dominate total cost ngerprinting time phases increases linearly size partition requiring approximately seconds slow system seconds fast system comparing performance systems conclude increases sequential disk performance directly improve eof ngerprinting time phases require small amount time partition size on-line time overheads classi cation association operation inferencing potentially costly operations sds subsection employ series microbenchmarks illustrate costs actions results experiments sds underneath linux ext presented table action microbenchmark cases rst case system mounted synchronously ensuring meta-data operations reach sds order allowing sds guarantee correct classi cation additional effort synchronous mounting linux ext similar traditional ffs handling meta-data updates case system mounted asynchronously case guarantee correct classi cation association sds perform operation inferencing microbenchmarks perform basic system operations including directory creates deletes report perle per-directory overhead action test experiments make number observations operations tend cost order tens microseconds directory operations require complete cost due per-block cost operation inferencing synchronous mode create workload takes roughly corresponds base cost create workload cost approximately block costs rise size increases sds incurs small per-block overhead compared actual disk writes number milliseconds complete cases overheads ext system run asynchronous mode lower run synchronous mode asynchronous mode numerous updates meta-data blocks batched costs block differencing amortized synchronous mode meta-data operation ected disk system incurring higher overhead sds observe synchronous mode classi cation expensive association expensive inferencing sds care employ actions needed implement desired functionality on-line space overheads sds require additional memory perform classi cation association operation inferencing speci cally hash tables required track mappings data blocks inodes caches needed implement cient block differencing quantify memory overheads variety workloads table presents number bytes hash table support classi cation association operation inferencing sizes maximum reached run workload netnews postmark modi andrew benchmark netnews postmark vary workload size caption table dominant memory overhead occurs sds performing block-inode association classi cation operation inferencing require table sizes proportional number unique meta-data blocks pass sds association requires information unique data block passes worst case entry required proceedings usenix conference file storage technologies fast san francisco california indirect block-inode operation classi cation association inferencing sync async sync async sync async create create delete delete mkdir rmdir table sds time overheads table breaks costs indirect classi cation block-inode association operation inferencing microbenchmarks row stress aspects action create benchmark creates les size delete benchmark similarly deletes les mkdir rmdir benchmarks create remove directories result presents average overhead operation extra time sds takes perform classi cation association inferencing experiments run slow system ibm lzx disk linux ext mounted synchronously sync asynchronously async data block disk memory disk space space costs tracking association information high prohibitive memory resources scarce sds choose tolerate imperfect information swap portions table disk addition hash tables needed perform classi cation association operation inferencing cache data blocks perform block differencing effectively recall differencing 
observe pointers allocated freed inode indirect block check time elds inode changed detect bitwise bitmap monitor directory data creations deletions performance system sensitive size cache cache small difference calculation rst fetch version block disk avoid extra size cache roughly proportional active meta-data working set postmark workload found sds cache approximately blocks hold working set cache smaller block differencing operations disk retrieve older copy block increasing run-time benchmark roughly case studies section describe case studies implementing functionality sds implement drive raid semantic knowledge case studies indirect block-inode operation classi cation association inferencing netnews netnews netnews postmark postmark postmark andrew table sds space overheads table presents space overheads structures performing classi cation association operation inferencing workloads netnews postmark modi andrew benchmark workloads netnews postmark run amounts input correspond roughly number transactions generates netnews implies transactions run number table represents maximum number bytes stored requisite hash table benchmark run hash entry bytes size experiment run slow system linux ext asynchronous mode ibm lzx disk built system proper implementing le-system functionality storage system advantages semantic intelligence storage-system manufacturers augment products broader range capabilities due space limitations fully describe case studies paper highlight functionality case study implements present performance evaluation conclude analyzing complexity implementing functionality sds performance evaluation included demonstrate interesting functionality implemented effectively sds leave detailed performance studies future work theme explore section usage approximate information scenarios sds wrong understanding system case studies track-aligned extents proposed schindler track-aligned extents traxtents improve disk access times placing medium-sized les tracks avoiding track-switch costs detailed level knowledge traxtents-enabled system requires underlying disk mapping logical block numbers physical tracks traxtents natural candidate implementation sds information readily obtained fundamental challenge implementing traxtents sds system adapting policies system system speci cally traxtent sds uence system allocation prefetching mid-sized les proceedings usenix conference file storage technologies fast san francisco california prefetching prefetching ext traxtent sds table track-aligned extents table shows bandwidth obtained reading les randomized order roughly size track case examine default track-aligned allocation varying track-sized prefetching enabled sds experiment run slow system running linux ext system mounted asynchronously quantum atlas iii disk allocated consecutive data blocks span track boundaries accesses track-sized units components interest traxtent sds implementation bitmap blocks rst read system sds marks bitmap block track allocated similar technique schindler wastes small portion disk fake allocation uences system allocate les span tracks system decides allocate tracks sds dynamically remaps blocks track-aligned locale similar block remapping loge smart disks major difference sds remaps blocks part mid-sized les benet track-alignment non-semantically aware disks make distinction traxtent sds performs additional prefetching ensure accesses smaller track linux ext ffs prefetches blocks initially read traxtent sds observes read rst block track-aligned requests remainder track places data blocks cache traxtent sds relies piece exact information correctness location bitmap blocks marks trick system track-aligned allocation information static obtained reliably eof performance cost runtime indirect classi cation data belonging medium-sized les occasionally incorrect remapping performance correctness table shows traxtent sds prefetching results roughly improvement bandwidth medium-sized les structural caching discuss semantic information caching sds simple lru management disk cache duplicate contents system cache wastes memory storage system waste onerous storage arrays due large amounts memory contrast sds structural undertpc-b tpc-b ffs lru sds file-aware caching sds table file-aware caching table shows time seconds takes execute tpc-b transactions experiments transactions rst run warm system large scan run series transactions timed table compares netbsd ffs standard disk sds lru-managed cache sds le-aware cache experiments run slow system ibm lzx disk standing system cache blocks intelligently avoid wasteful replication explore caching blocks volatile memory dram non-volatile memory nvram presents unique opportunities optimization rst examine simple optimization avoids worst-case lru behavior file-aware caching sds fac sds exploits knowledge size selectively cache blocks les small cache les accessed sequentially strategy avoids caching blocks large les scanned ush cache blocks implement le-aware caching fac sds identi cacheable blocks indirect classi cation association case hash table holds block addresses correspond les meet caching criteria previously sds misclassify blocks cases inode written disk data blocks fac sds small amount state active order detect sequential access patterns table shows performance fac sds database workload scenario run tpc-b transactions periodically intersperse large scans system emulating system running mixed interactive batch transactions large scan ushes contents traditional lru-managed cache degrades performance subsequent transactions le-aware cache cache blocks large scans keeping transactional tables sds memory improving performance examine sds semantic knowledge store important structures non-volatile memory explore possibilities rst exploit semantic knowledge store ext journal nvram implement journal caching sds sds sds recognize traf journal redirect nvram straightforward eof tool determines blocks belong journal classifying caching data reads proceedings usenix conference file storage technologies fast san francisco california create create sync ext lru sds lru sds journal caching sds table journal caching table shows time create -kb les ext sds sds performs lru nvram cache management cache journal caching sds storing journal nvram create benchmark performs single sync les created create sync benchmark performs sync creation inducing journaling-intensive workload experiments run slow system running linux utilizing ibm lzx disk writes journal sds implement desired functionality place meta-data bitmaps inodes indirect blocks directories netbsd ffs nvram inodes bitmaps identi location disk pointer blocks directory data blocks identi indirect classi cation occasionally miss blocks exploit fact approximate information adequate sds writes unclassi blocks disk nvram observes inode track meta-data blocks meta-data caching sds mdc sds additional map record in-core location tables show performance sds mdc sds cases simple nvram caching structures journal system meta-data effective reducing run times dramatically greatly reducing time write blocks stable storage lru-managed cache effective case cache large working set main bene structural caching nvram size cached structures sds guarantees effective cache utilization hybrid combine worlds storing important structures journal meta-data nvram managing rest cache space lru fashion future plan investigate ways semantic information improve storagesystem cache management sds types meta-data updates last-accessedtime updates inode order ascertain les system cache prefetching sds intelligent system awareness make guess block read finally blocks deleted removed cache freeing space live blocks create read delete postmark ffs lru sds lru sds 
mdc sds table meta-data caching left columns table show time seconds complete phase lfs microbenchmark experiment lfs benchmark creates reads deletes -kb les column shows total time seconds postmark benchmark run les transactions directories rows compare performance netbsd ffs slow system ibm disk sds sds performs lru nvram cache management cache mdc sds strategy secure deletion advanced magnetic force scanning tunneling microscopy stm person physical access drive lot time potentially extract sensitive data user deleted case study explore secure-deleting sds disk guarantees data deleted les unrecoverable previous approaches incorrectly functionality system over-writing deleted blocks multiple times patterns guarantee data removed disk copies data blocks exist due bad-block remapping storage system optimizations multiple consecutive le-system writes reach disk media due nvram buffering sds locale secure delete implemented ensure stray copies data exist over-writes performed disk nature case study approximate incorrect information blocks deleted acceptable secure-deleting sds recognizes deleted blocks operation inferencing overwrites blocks data patterns speci number times system reallocate blocks possibly write block fresh contents meantime sds tracks deleted blocks queues writes blocks overwrite nished note mount ext system synchronously secure deletion operate correctly investigating techniques relax requirement part future work table shows overhead incurred sds function number over-writes overwrites performed data recoverable noticeable price paid securedelete functionality loss acceptable highlysensitive applications requiring security performance improved delaying secureproceedings usenix conference file storage technologies fast san francisco california delete postmark ext secure-deleting sds secure-deleting sds secure-deleting sds table secure deletion table shows time seconds complete delete microbenchmark postmark benchmark secure-deleting sds delete benchmark deletes -kb les postmark benchmark performs transactions row secure-deleting sds shows performance number over-writes experiment place slow system running linux ext mounted synchronously ibm lzx disk overwrite disk idle performing immediately freeblock scheduling minimize performance impact journaling nal case study complex sds implements journaling underneath unsuspecting system view journaling sds extreme case helps understand amount information obtain disk level unlike case studies journaling sds requires great deal precise information system due space limitations present summary implementation fundamental dif culty implementing journaling sds arises fact disk transaction boundaries blurred instance system create system inode block parent directory block inode bitmap block updated part single logical create operation block writes grouped single transaction straight-forward fashion sds sees stream meta-data writes potentially interleaved logical system operations challenge lies identifying dependencies blocks handling updates atomic transactions result journaling sds maintains transactions coarser granularity journaling system basic approach buffer meta-data writes memory write disk in-memory state meta-data blocks constitute consistent metadata state logically equivalent performing incremental in-memory fsck current set dirty meta-data blocks writing disk check succeeds current set dirty meta-data blocks form consistent state treated single atomic transaction ensuring on-disk meta-data contents remain previous consistent state fully updated consistent state bene coarse-grained transactions batching commits performance improved traditional journaling systems create read delete ext sync ext async ext ext sync journaling sds table journaling table shows time complete phase lfs microbenchmark seconds -kb les con gurations compared ext linux mounted synchronously mounted asynchronously journaling ext linux journaling sds synchronously mounted ext linux experiment place slow system ibm lzx disk guarantee bounded loss data crash journaling sds limits time elapse successive journal transaction commits journaling daemon wakes periodically con gurable interval takes copy-on-write snapshot dirty blocks cache dependency information point subsequent meta-data operations update copy cache introduce additional dependencies current epoch similar secure-deleting sds current journaling sds implementation assumes system mounted synchronously robust sds requires verify assumption holds turn journaling meta-data state written disk journaling sds consistent synchronous asynchronous mount problem imposed asynchronous mount sds miss operations reversed create delete lead dependencies resolved inde nite delays journal transaction commit process avoid problem journaling sds suspicious sequence meta-data blocks single change expected multiple inode bitmap bits change part single write turns journaling cases fall-back journaling sds monitors elapsed time commit dependencies prolong commit time threshold suspects asynchronous mount aborts evaluate correctness performance journaling sds check correctness crashed system numerous times ran fsck verify inconsistencies reported performance journaling sds summarized table sds requires system mounted synchronously performance similar asynchronous versions semantically-smart disk system delays writing meta-data disk read test sds similar performance base system ext delete test similar performance journalproceedings usenix conference file storage technologies fast san francisco california eof fingerprinting probe process sds sds infrastructure case studies initialization traxtents hash table cache file-aware cache direct classi cation journal cache indirect classi cation meta-data cache association secure delete operation inferencing journaling table code complexity number lines code required implement aspects sds presented sds component eof tool lines code roughly lines shared system types rest le-system speci ing system ext creation sds pays signi cost relative ext overhead block differencing hash table operations noticeable impact purpose case study demonstrate sds implement complex functionality small overhead acceptable complexity analysis brie explore complexity implementing software sds table shows number lines code components system case studies table complexity found eof tool basic cache hash tables operation inferencing code case studies trivial implement top base infrastructure traxtent sds journaling sds require thousand lines code conclude including type functionality sds pragmatic conclusions beware false knowledge dangerous ignorance george bernard shaw recent article wise drives gordon hughes associate director center magnetic recording research writes favor smarter drives stressing great potential improving storage system performance functionality believes interface systems storage required widespread drive input output command requirements interface speci cation short industry consensus task general interest offers market opportunities multiple computer drive companies hughes comments illustrate dif culty interfaces require wide-scale industry agreement eventually limits creativity inventions existing interface framework information system disk low-level knowledge drive internals sds sits ideal location implement powerful pieces functionality disk system implement enabling innovations existing interfaces storage system manufacturers embed optimizations previously relegated domain systems enabling vendors compete axes cost performance paper demonstrated underneath class ffs-like systems le-system information automatically gathered exploited implement functionality drives heretofore implemented system implemented shown costs reverse-engineering system structure behavior reasonable challenges remain including understanding generality robustness semantic inference broader range systems sophisticated systems wider range platforms probed reveal workings approximate information exploited implement interesting functionality techniques tools developed assure correct operation 
semantic technology answer questions research experimentation nal answer elicited acknowledgments members wind research group feedback ideas presented paper keith smith excellent shepherding anonymous reviewers thoughtful suggestions greatly improved content paper finally computer systems lab tireless assistance providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation timothy denehy sponsored ndseg fellowship department defense proceedings usenix conference file storage technologies fast san francisco california acharya uysal saltz active disks proceedings conference architectural support programming languages operating systems asplos viii san jose october amiri petrou ganger gibson dynamic function placement dataintensive cluster computing proceedings usenix annual technical conference pages june anderson chase vahdat interposed request routing scalable network storage transactions computer systems tocs february arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october baker asami deprit ousterhout seltzer non-volatile memory fast reliable file systems proceedings international conference architectural support programming languages operating systems pages boston massachusetts october acm sigarch sigops sigplan bauer priyantha secure data deletion linux file systems tenth usenix security symposium washington august brown yamaguchi oracle hardware assisted resilient data oracle technical bulletin note burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey june chao english jacobson stepanov wilkes mime high performance parallel storage device strong recovery guarantees technical report hpl-csp- rev laboratories november collberg reverse interpretation mutation analysis automatic retargeting conference programming language design implementation pldi las vegas nevada june jonge kaashoek hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles pages asheville december denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey june dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey california june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge selforganizing disk controller proceedings usenix winter technical conference pages san francisco january ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings conference architectural support programming languages operating systems asplos viii october gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon gray storage bricks arrived invited talk usenix conference file storage technologies fast gutmann secure deletion data magnetic solid-state memory sixth usenix security symposium san jose california july hagmann reimplementing cedar file system logging group commit proceedings acm symposium operating systems principles november proceedings usenix conference file storage technologies fast san francisco california hsieh engler back reverseengineering instruction encodings proceedings usenix annual technical conference usenix boston massachusetts june hughes wise drives ieee spectrum august katcher postmark file system benchmark technical report trnetwork appliance october king dirty lesystem bug ext https listman redhat pipermail ext users -april html march lumb schindler ganger freeblock scheduling disk firmware proceedings usenix conference file storage technologies fast monterey january mckusick joy lef fabry fast file system unix acm transactions computer systems august morton data corrupting bug ext http uwsg hypermail linux kernel html dec ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference anaheim june padhye floyd inferring tcp behavior sigcomm san deigo august regehr inferring scheduling behavior hourglass proceedings usenix annual technical conference freenix track monterey june riedel gibson faloutsos active storage large-scale data mining multimedia vldb york august rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler grif lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey january seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems proceedings usenix annual technical conference pages san diego june swartz brave toaster meets usenet lisa pages chicago illinois october talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans february wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey june gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings fourth symposium operating systems design implementation osdi san diego october zhou philbin multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference boston june 
bridging information gap storage protocol stacks timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison ftedenehy dusseau remzig wisc abstract functionality performance innovations file systems storage systems proceeded largely independently past years result information gap information designed implemented result high cost maintenance poor performance duplication features limitations functionality bridge gap introduce evaluate division labor storage system file system develop enhanced storage layer exposed raid raid reveals information file systems built specifically raid exports parallelism failure-isolation boundaries storage layer tracks performance failure characteristics fine-grained basis advantage information made raid develop informed log-structured file system lfs lfs extension standard logstructured file system lfs altered advantage performance failure information exposed raid experiments reveal prototype implementation yields benefits management flexibility reliability performance storage system small increase file system complexity lfs raid incorporate disks system on-the-fly dynamically balance workloads disks system user control file replication delay replication files increased performance functionality difficult impossible implement traditional division labor file systems storage introduction chasm exists world file storage management hierarchical file system directories byte-accessible files norm years internals file systems underlying storage systems evolved substantially improving performance functionality file systems approaches developed improve performance including read-optimized inode file placement logging writes improved meta-data update methods scalable internal data structures off-line reorganization strategies techniques developed assumption file system run single traditional disk recently storage systems received attention smart disks improve read write performance block remapping techniques o-intensive workloads multiple-disk storage systems studied research community achieved success storage industry high-end storage systems provide illusion single fast disk unsuspecting file systems internally manage parallelism redundancy optimize performance capacity analogous file systems storage systems developed single ffs-like file system mind file systems parallel disk systems substantial separate result information gap file system understand true nature storage system runs storage system comprehend semantic relations blocks stores addition unaware state tracks optimizations performs gap arose historical source hardware software boundary file systems traditionally expected block-based read write interface storage interface similar single disk exports advent hardware-based raid systems storage vendors advantage freedom innovate interface developed high-performance high-capacity systems appeared single large fast disk file system software modifications required host operating system file systems continued operate correctly spite fact optimized single-disk system case ignorance bliss arrangement simple worked boundary file system storage system changing migrating softwarestructuring technique interface necessitated hardware software raid drivers platforms advent networkattached storage client-side striping software replace hardware-based raid systems software-based raids attractive due low cost linux-based system incurs cost machine disks term arrangement file system layer top software storage layer storage protocol stack akin networking protocol stacks prominent communication networks similarities layering simplify system design potentially cost performance crucial difference exists layers comprise network protocol stacks derived design architects carefully deciding specific element storage protocol stack developed single coherent manner end result poor performance potential duplication implementation limitations functionality performance suffer model file system storage layer accurate layout optimizations work single traditional disk logical-block physical-block mapping unknown feature duplication potential pitfall log-structured file system layered top disk array performs logging duplicating work increasing system complexity unnecessarily finally functionality limited pieces information live layer system storage system blocks constitute file perform per-file operations block longer live file deletion optimize system ways knowledge time ripe re-examine division labor file system storage system layers attempt understand structure storage protocol stack specifically piece storage functionality understand easily effectively implemented problem germane time move network-attached storage proposed higher-level disk interfaces paper step goal exploring single point spectrum designs bridge file system storage system information gap develop evaluate division labor file system storage realignment storage layer exposes parallelism failure isolation boundaries part full file systems built on-line performance failure characteristics call layer exposed raid layer raid advantage information provided raid introduce informed lfs lfs enhancement log-structured file system combining performance failure information presented raid file-system specific knowledge lfs flexible manageable traditional file system deliver higher performance availability adding disk lfs on-line easily accomplished lfs accounts potential heterogeneity introduced disk dynamically balances load disks system rates lfs increases flexibility storage enabling user control redundancy per-file basis implements lazy mirroring defer replication time potentially increasing performance system slight decrease reliability crucial lfs raid implementation aforementioned benefits significant increase complexity maintainability storage protocol stack careful design functionality mentioned implemented increase code size compared traditional system lfs raid panacea find managing redundancy file system onerous requiring careful placement inodes data blocks ensure efficient operation failure extending traditional file system structure support enhanced functionality lfs arduous task redesign age-old vnode layer support informed file systems warranted rest paper structured begin discussion related work section section give overview approach describe raid lfs sections section present evaluation system present discussion section future work section conclude section related work part motivation informing file system nature storage system reminiscent work berkeley fast file system ffs ffs early demonstration benefits low-level understanding disk technology colocating correlated inodes data blocks performance improved compared unix file system work goal multidisk storage systems mind file system base decisions reliablyobtained information characteristics storage relying assumptions hold time seek costs dominate rotational costs roselli discuss file system storage system gap talk file system fingerprinting solution enrich interface file systems storage systems giving storage system information blocks related blocks accessed future approach storage system information file system collected presumes storage layer make good information potential problem approach require agreement set interfaces cooperating storage vendors file-system implementors benefits low-level knowledge disk characteristics found schindler recent work track-aligned extents authors explore range performance improvements allocating accessing data disktrack boundaries avoiding rotational latency track-crossing overheads single-disk setting contrast raid exposes disk boundaries raid file systems detailed lower-level information future interesting investigate benefits lower-level knowledge specifics raid-based storage system network appliance pioneered ideas discuss work file server appliances development wafl write-anywhere file layout technique hitz hint information hidden inside raid layer advantage file system ensure writes raidlayer occur full-stripe-sized units avoid small-write penalty manifests raidand raidsystems step formalizing raid layer showing traditional file system easily modified advantage information provided raid demonstrating broader range optimizations attainable framework volume managers long ease management storage multiple devices raid layer simply type volume manager exposes information file 
systems specifically on-line performance failure information raid built presupposition single mounted file system utilize multiple volumes data volume managers assume one-to-one mapping mounted file system volume volume manager similar raid pool driver volume manager sans sub-pool concept file system group related data work gfs file system sub-pools separate journaled meta-data normal user data exposing disk storage system file system extension arguments made engler kaashoek authors argue software abstractions made operating systems fundamentally problematic high-level limit power functionality authors advocate solution exposing hardware features user missing argument minimalism observation hardware abstractions users operating systems change apropos data storage abstraction put raid systems high-level raid breaks revealing information hidden file system distributed file systems zebra xfs manage disk system individually manner similar lfs systems traditional storage management techniques raidstriping advantage potential possibilities raid layer makes future hope extend ideas distributed arena direct comparison recently nasd object interface introduced higher-level data repository sanbased distributed file systems interface advanced functionality storage layer raid designed functionality file system earlier work datamesh proposes sophisticated interfaces network-attached storage informed approach similar large body work parallel file systems parallel file systems expose disk parallelism application file system manage control redundancy parallel file system proposed work computation parity put user control user avoid well-known performance penalty raidand raidunder small writes overview sections present design implementation raid lfs primary goal designing system exploit information made raid allowing lfs implement functionality difficult impossible achieve traditional layering aim increase ease storage management performance multiple heterogeneous disks functionality meet demands diverse set applications primary goal implementing raid facilitate information provided raid simplest non-informed legacy file systems built top raid primary goal implementing lfs minimize impact transforming file system utilize storage interface require re-design vnode layer ruled mandate file systems changed order function system implementation effort integrate lfs highly localized modular fashion fewer lines code changed question addressed decision modify lfs traditional popular ffs-like journaling file system reason chose lfs natural flexibility data placement lfs modern write storage system write-anywhere systems provide extra level indirection writes location storage medium exploit aspect lfs part implementation mind number implementation techniques general applied file systems hope investigate future interested general lfs file system performance issues consult work rosenblum ousterhout subsequent research seltzer software developed context netbsd operating system raid implemented set hooks lower-level blockdriver calls detail section lfs implemented extending netbsd version lfs based original lfs bsd unix detail section chose netbsd version lfs stable solid implementation raid describe raid storage interface consists major components segmented address space exposes parallelism storage system file system functions inform file system dynamic state storage system mirror pairmirror pair linear address space blocks region region figure raid configuration diagram depicts raid configuration disks combined mirrored pair regions half size total address space presented client file system region layout performed mirror hidden file system segmented address space traditional raid array presents storage subsystem file system linear array blocks underneath true complexity raid scheme hidden file systems interact raid systems reading writing blocks keeping desire minimize change preserve backwards compatibility raid linear array blocks read written basic interface expose information storage system file system address space segmented specifically organized series contiguous regions mapped directly single disk set disks region boundaries made file system desires four-disk storage system disk capable storing blocks address space raid presents segmented blocks throughn map disk blocksn map disk exposing information raid enables file system understand performance failure boundaries storage system sections file system advantage place data region intelligently potentially improving performance reliability aspects storage system raid region represent single disk region configured represent mirrored pair disks raidcollection region viewed configurable software-based raid entire raid address space single representation conglomeration raid subsystems scenario information hidden file system cross-region optimizations region exists raid configuration mirrored pairs shown figure allowing region represent single disk primary benefits region configured raid mirrored pair disks file system forced manage redundancy choose desired arrangement backwards compatibility raid configured single striped mirrored raidregion allowing unmodified file systems change dynamic information segmented address space exposes nature underlying disk system file system part full knowledge make intelligent decisions data placement replication raid layer exposes dynamic information state region file system raid distinguishes traditional volume managers pieces information needed file system desire performance information per-region basis raid layer tracks queue lengths current throughput levels makes pieces information file system historical tracking information left file system file system resilience region failures occur failures region tolerate raid presents information file system figure file system mirror pair tolerate single disk failure informed failure occurs file system action directing subsequent writes regions moving important data bad region reliable portions raid address space implementation current implementation raid implemented thin layer file system storage system order implement striped mirrored raidregion simply utilize standard software raid layer provided netbsd prototype raid layer completely generalized date current form require effort file system lfs utilize segmented address space built interposing vnode strategy call remap requests logical block number virtual address space presented raid physical disk number block offset issued underlying disk raid dynamic performance information collected monitoring current performance levels reads writes prototype region boundaries failure information performance levels throughput queue length tracked low-levels file system complete implementation make information ioctl interface raid device note focus primarily utilizing performance information paper lfs describe lfs file system current design major pieces additional functionality compared standard lfs on-line expandability storage system dynamic parallelism account performance heterogeneity flexible user-managed redundancy lazy mirroring writes sum total added features make system manageable administrator easily add disk worry configuration flexible users control replication occurs higher performance lfs delivers full bandwidth system heterogeneous configurations flexible mirroring avoids costs rigid redundancy schemes discussion focus case separates lfs raid traditional raid raid layer exposes disk storage system separate region lfs on-line expansion contraction design ability upgrade storage system incrementally crucial performance capacity demands site increase administrator add disks ideally addition simple perform single command issued administrator automatic addition disk detected hardware require down-time keeping availability storage high immediately make extra performance capacity disk older systems on-line expansion storage system add disk on-thefly case administrator unmount partition expand 
tool similar re-mount file system worse systems require file system built forcing administrator restore data tape modern volume managers on-line expansion file system support lfs design includes ability incorporate disks raid regions on-line single command file system complicated support necessitated layers system hardware supports hot-plug detection disks power-cycle lfs add disks time reduction data availability amount work administrator put expand system small contraction important removal region simple addition incorporate ability remove region fly file system configured non-redundant manner data lost difference lfs traditional system scenario lfs files deliver applications implementation on-line expansion contraction storage file system views regions added extant fully utilized region added system blocks disk made allocation file system immediately begin write data conversely region removed viewed fully allocated technique general applied file systems similar ideas specifically log-structured file system composed collection lfs segments natural expand capacity lfs adding free segments implement functionality newfs ilfs program creates expanded lfs segment table file system entries segment table record current state segment raid region added file system pertinent information added superblock additional portion segment table activated approach limits number regions added fixed number flexible growth segment table file expanded dynamic parallelism design problem introduced flexibility administrator growing system increased potential performance heterogeneity disk subsystem disk raid segment performance characteristics disks system case traditional striping raid schemes work assume disks run identical rates traditionally presence multiple disks hidden storage layer file system current systems handle disk performance heterogeneity storage layer file system information research community proposed schemes deal static disk heterogeneity solutions require careful tuning administrator van jacobsen notes experience shows configured misconfigured complicating issue delivered performance device change time result workload imbalances fail-stutter nature modern devices present correct operation degraded performance clients advanced heterogeneous data layout schemes utilized work dynamic shifts performance handle static dynamic performance differences disks include dynamic segment placement mechanism lfs segment logically written free space file system exploit writing segments raid regions proportion current rate performance exploiting dynamic state presented file system raid dynamically balance write load system account static dynamic heterogeneity disk subsystem note performance disks roughly equivalent dynamic scheme degenerate standard raidstriping segments disks style dynamic placement performed traditional storage system autoraid basic mechanisms place unduly adds complexity system file system storage system track blocks pushing dynamic segment placement file system complexity reduced file system tracks blocks file located implementation original version lfs allocates segments sequentially based availability words free segments treated equally manage parallelism disks lfs develop segment indirection technique specifically modify ilfs newseg routine invoke data placement strategy ilfs newseg routine find free segment write alter region aware informed segment-placement decision choosing disks accordance performance levels information provided raid load set regions balanced major advantage decision implement functionality ilfs newseg routine localizes knowledge multiple disks small portion file system vast majority code file system aware region boundaries disk address space remains unchanged slight drawback decision region place segment made early segment written performance level disk segment fills significant placement decision potentially poor practice found performance problem flexible redundancy design typically redundancy implemented one-size-fits-all manner single raid scheme autoraid applied blocks storage system file system typically involved aware details data replication storage layer traditional approach limiting semantic information file system smart users applications exploited improve performance utilize capacity lfs explore management redundancy strictly file system managing redundancy file system greater flexibility control users current design users applications select file made redundant mirrored file mirrored users pay cost terms performance capacity file mirrored performance increases writes file capacity saved chances losing file increased turning redundancy well-suited temporary files files easily regenerated swap files lfs performs replication accounting system files users physical blocks contrast traditional file system mounted top advanced storage system autoraid users charged based logical capacity true usage storage depends access patterns usage frequency redundancy schemes implemented raid storage system notion file exists scheme easily implemented traditionally-layered system storage system wholly unaware blocks constitute file receive input user blocks replicate file system block block inode inode file file figure crossed pointer problem figure illustrates problem separate file means redundancy specifically element file inode data block replicated single lost disk make difficult find data block due extra requirement block pointer chain block live file inode number mirror inode consist single data block block disk crashes find data block copy exists remaining working disk storage system altered functionality realized future interesting investigate range policies top redundancy mechanisms automatically apply redundancy strategies class file akin elephant file system segregates files versioning techniques implementation accomplish goal per-file redundancy decided utilize separate unique meta-data original redundant files approach natural file system require on-disk data structures implementation straight-forward scheme assigns inode numbers original files odd inode numbers redundant copies method advantages original redundant files unique inodes data blocks distributed evolving rpc active storage muthian sivathanu andrea arpaci dusseau remzi arpaci dusseau department computer sciences wisconsin madison fmuthian dusseau remzig wisc abstract introduce scriptable rpc srpc rpc-based framework enables distributed system services advantage active components technology trends point world component system disk network interface memory substantial computational capabilities traditional methods building distributed services designed advantage architectures mandating wholesale change software base exploit powerful hardware contrast srpc direct simple migration path traditional services active environment demonstrate power exibility srpc framework series case studies focus active storage servers speci cally advantages approach srpc improves performance distributed servers reducing latency combining execution operations server srpc enables ready addition functionality powerful cache consistency models realized top server exports simple nfs-like interface srpc arbitrarily simpli disks construction distributed constraints services operations allowing dif cult redundancy coordinate combination client server file system co-executed features server number avoiding lfs costly inodes agreement unlimited crash-recovery protocols introduction written remote procedure log call rpc long standard implementing distributed services rpc simple extends well-known paradigm procedure call client server setting powerful serve substrate distributed services sun nfs development rpc changed functionality offers modernized versions rpc java rmi significantly alter basic rpc paradigm simply provide features language run-time context rpc stagnating architecture distributed systems changing rapidly technology trends point world active components commonplace intelligence form additional processing capabilities permission make digital hard copies part work personal classroom granted fee provided copies made distributed pro commercial advantage copies bear notice full citation rst page copy republish post servers redistribute lists requires prior speci permission fee asplos san jose california copyright acm found active disks smart network interfaces intelligent memories importance active storage potential greatly improve distributed storage services terms performance functionality simplicity design early research active storage demonstrated potential performance bene previous systems require programming environments target limited class applications parallel applications database primitives existing distributed services clear migration path coming active world paper present design implementation evaluation scriptable rpc srpc extensible framework developing active distributed services key difference srpc traditional rpc clients send scripts server implement inode map stored regular file expanded prime disadvantage approach limits redundancy copy easily extended n-way mirroring scheme reserving i-numbers file problem introduced decision utilize separate inodes track primary mirrored copy file refer crossed pointer problem figure illustrates difficulty arise simply requiring component file inode indirect blocks data blocks replicated sufficient guarantee data recovered easily single disk failure ensure data block reachable disk failure block reachable implies pointer chain exists figure file inode number replicated inode number inode located disk data block mirror copy file inode disk data block primary copy file disk fails data block easily recovered inode surviving disk points data block failed disk file systems fatal flaw data block unrecoverable lfs performance issue extra information found segment summary blocks full recovery disk crash mandate full scan disk recover data blocks number remedies problem perform explicit replication inode pointer-carrying structures indirect blocks doubly-indirect blocks require on-disk format change inefficient usage disk space inode indirect block logical copies file system simpler approach divide conquer disks system divided sets writing redundant file disk lfs decides set primary copy redundant copy set pointers cross set guarantee single failure harm fact tolerate number failures disks set finally incorporating redundancy lfs presents difficult implementation challenge replicate data inodes file 
system re-writing routine creates modifies data disk develop apply recursive vnode invocation ease task embellish lfs vnode operations short recursive tail routine invoked recursively arguments routine operating i-number primary copy data file designated redundancy user instance file created ilfs create recursive call ilfs create create redundant file recursion broken call perform identical operation redundant file lazy mirroring design user-controlled replication users control replication occurs shown previous work potential benefits arise allowing flexible control redundant copies made parity updated delaying parity updates shown beneficial raidschemes avoid small-write problem reduce load mirrored schemes implementing feature file system level user decide window vulnerability file losing data files tolerable note enhancements difficult implement traditional system information required resides file system raid necessitating non-trivial lfs incorporate lazy mirroring usercontrolled replication scheme users designate file non-replicated immediately replicated lazily replicated choosing lazy replica user increase chance data loss improved performance lazy mirroring improve performance reasons delaying file replication file system reduce load burst traffic defer work replication period lower system load file written disk deleted replication occurs cost replication removed systems buffer files memory short period time seconds file lifetimes recently shown longer average scenario common previously thought server-side implementation lazy functionality mirroring srpc implemented designed lfs assist developers embellishment evolving traditional file-system rpc-based cleaner services files rst-class designated active lazy services replicas srpc extra shares bit set features rpc segment usage automates table indicating service status development process cleaner scans segment increase finds ease-of-use blocks srpc automatically replicated embeds simply performs script interpreter replication directly server making providing safe place execution replicated blocks environment client-generated avoid scripts crossed demonstrate pointer problem primary bene associates srpc mirrored number inode case studies replication complete distributed bit system called cleared scfs scfs file improves system performance replicates files combining -minute operations delay server future avoiding costs set extra directly round-trip user latencies application scfs evaluation ready addition section present functionality evaluation evolving base raid system lfs protocol experiments enhanced performed virtual protocol intelbased speci cally scripts physical applied memory implement afs main processor sprite cache -ghz consistency intel top pentium iii base xeon protocol system houses simple nfs-like rpm seagate consistency finally scfs simple seq write implement seq functionality read rand write complex rand read execute throughput reliably access client pattern baseline server performance scfs slow directory disksfast update disks operations figure performed baseline performance script comparison directly figure server plots obviate performance lfs multi-client raid coordination sequential writes introducing sequential scripting reads random server writes number random issues reads arise tests concern run safety disks client kernels partially trusted induce server crash accidentally malicious intent monopolize resources performance issue script interpretation overhead negate advantages srpc finally ease-of-use important criterion language enable development short powerful scripts prototype implementation srpc tcl base scripting language similar languages exist feel tcl interesting choice reasons tcl addresses safety concerns providing limited execution environment ne-grained control operations script execute loops disabled previous studies tcl interpreter thousand times slower recent versions tcl interpreter sophisticated cient internal representations boost performance time ripe reexamining strengths weaknesses tcl performance tcl fairly high-level language ease-of-use main advantages srpc effective framework extending scfs distributed service performance functionality enhancements readily designed deployed system framework case studies scripts reduce latency factor strengthen consistency semantics system server functionality readily embellished srpc framework similar extension methodology client-side kernels evaluation tcl language extensibility powerful remaining easy complex extension requires tens lines code performance tcl improved warrant consideration language extension numerous safety features sophisticated extensions afford slightest overheads specialized domain-speci language active storage worth investigating rest paper structured present overview srpc system section describe experimental environment including details scfs empirical methodology section section section section illustrate manner srpc improve performance functionality simplicity active services section perform in-depth analysis extension scripts tcl cover related work section conclude section overview srpc srpc framework building active extensible distributed services srpc extends widely understood paradigm rpc exible scripting capability clients harness execute customized extensions server srpc designed meet goals important environment rst goal provide smooth migration path existing distributed services traditional rpc-based server active server goal accomplished making process developing srpc-based service similar developing service traditional rpc paradigm automating steps goal ensure script execution overhead severe compared traditional rpc call improve performance srpc caches scripts supports concurrency cient implementations performancesensitive commands srpc standard library goal provide safe environment script execution choice tcl scripting language enables meet goal migration path rst goal srpc enable varying disks slow disks developers easily move distributed services traditional rpc-based servers powerful active servers srpc engineered backwardly compatible rpc allowing unmodi clients send scripts server co-exist clients altered functionality srpc automatically generates code interface rpc server scripting language interpreter traditional rpc developer distributed service nes interface server exports speci interface interface nition language idl interface nition parsed idl compiler generate source code client server stubs call rpc library developer implement procedures speci interface compiled automatically generated stubs produce distributed service developing active distributed service srpc involves similar set steps basic interface exported server speci idl functionality idl compiler augmented ways rpc procedure added interface called scriptexec takes parameters script data buffer returns data buffer result implementation scriptexec generated automatically idl compiler automatically generates tcl wrappers interface procedures client scripts directly call routines simplify srpc programming interpreter server commands extract speci parameters character buffer pass arguments compiler generates code initialize set tcl interpreters registers interface procedures interpreter increasing ease-ofuse reducing burden programmers assist programmers development scripts additional pieces functionality supported srpc infrastructure scripts maintain state accessed scripts clients script store state script access state state variables uniquely named scripts inadvertently con ict current srpc implementation responsibility namespace management rests clients follow pre-de ned conventions clients easily access state variables piece functionality ability server invoke functionality client perform callback srpc supports allowing client run rpc server high performance order srpc appealing platform distributed systems performance comparable traditional rpc service script interpretation imposes overhead time script executed potentially negating bene executing operations server srpc simple techniques improve performance caching concurrency standard library primitives performance initial versions tcl shown 
signi overhead overhead occurred line script re-interpreted execution cient repeated execution code recent versions tcl translate interpreted code cient internal form leverage behavior srpc identi scripts executed previously reuses cached procedures subsequent invocations arguments sending script invocation clients register scripts server back script passed server script arguments subsequent invocations caching mechanism advantage reducing size messages client server improving observed network latency reducing amount network traf case studies section quantify performance bene srpc script caching improve performance srpc framework executes multiple tcl interpreters concurrently multiple interpreters scripts execute parallel server greatly improving system throughput concurrent execution complicates development scripts discuss describing safety issues performance optimization srpc implement performance-sensitive operations inside srpc standard library directly script tcl supports calling functions written language providing functionality straightforward issue identify operations included standard library general operations popular scripts costly implement tcl current implementation srpc includes standard library routines manipulate data buffers search buffers string send messages create manage lists access shared script state acquire release locks importance routines enable scripts avoid touching data inside tcl ensuring data buffers solely manipulated substructure avoid primary source additional overhead implementing server routines directly tcl bypasses security issues scripting language anticipate trusted administrators add primitive routines safety goal srpc design ensure scripts server easily corrupt server state consume undue resources problem arises extensible operating systems research srpc arbitrary user applications insert code server kernel clients trust boundary relaxed boundary advantageous enforce limited execution context scripts fortunately tcl needed functionality required limit script actions safetcl extensions server control functions script call control constructs implemented tcl procedures prevent execution whileand loops ensure scripts terminate nite amount time scripts require iteration provide simple safe callback interface procedure executed nite number times complication safety model scripts manipulate buffer pointers directly ensure invalid memory generated srpc implements run-time checks inside wrapper standard-library routines pointers passed arguments run-time type checking automatically generated wrappers ensures argument addresses correspond correct type preventing illegal dereferencing arbitrary memory addresses additional level safety cost worthwhile complication arises due concurrency concurrent execution mandates locks scripts access shared state execute correctly locking introduces problems realm safe extensibility client misbehave release lock negatively impacting well-behaved clients prevent problem occurring srpc automatically releases locks acquired released executing script script completion safeguard restricts scope locks found burdensome limitation access control locks prohibiting misbehaving clients acquiring locks privy implemented functionality proc spritecb arg res state lock global variable global load cacheable flag set cache srpc load state state cache add client callback list set list srpc load state list set list srpc makelist srpc install state list srpc add list list args invoke read set rdarg scfs make rdarg arg rdoff set rdres scfs read rdarg extract components result set cnt scfs rdres getlen rdres set data scfs rdres getdata rdres pack state results result srpc putint res cacheoff cache srpc putint res cntoff cnt srpc memcpy res dataoff data cnt return expr cnt cachesz cntsize figure srpc script script performs read presence write-sharing scripts enforce spritely cache consistency augmenting stateless protocol stateful functionality involves handful tcl commands figure present srpc script script part sprite consistency implementation executed client reads marked uncacheable explored detail case study section rpc interface server exports read procedure idl compiler automatically generates tcl wrapper scfs read commands extract individual elds scfs rdres getdata make arguments type character buffer scfs make rdarg automatically generated type checking performed ensure script access invalid memory addresses examples standard library routines include srpc loadstate srpc putint script begins loading cacheable object interest set address client added object callback list script creates valid read argument data passed script accessed arg variable invokes read argument results read cacheable form result script packed result variable res returned caller evaluation environment evaluate srpc framework context distributed system scfs section begin presenting general overview scfs describe details experimental platform finally describe mechanism systematically increase network delay order understand performance srpc environments scripted file system scfs distributed systems built network-attached disks potential delivering high bandwidth additional cost servers evaluate bene srpc introduce scfs scripted distributed system networkattached storage base version scfs supports multiple clients single disk acts repository system data scfs exports hierarchical namespace applications performs basic caching multiple outstanding requests tolerate latency implements nfs-like weak cache consistency scheme paper demonstrate scripting capability provided srpc enhance performance functionality scfs prototype network-attached disk con gured export interfaces clients rst commonly object-based interface similar proposed gibson exported object namespace nonhierarchical distinguish directories normal les read write requests arbitrarily ne-grained object identi offset length blockbased interface explore capabilities srpc restricted legacy environment mode disk reads writes size clients communicate disk protocol similar nfs disk server maintaining state behalf clients slight difference nfs calls directory operations exist clients perform operations client runs in-kernel rpc server receive messages scripts executing disk experimental platform system runs testbed intel-based machines running linux operating system scfs clientside code developed kernel standard linux vnode interface mhz pentium iii memory ibm ultrastar lzx disk machines connected mbit ethernet gigabit ethernet experiments primarily mbit ethernet environment network-attached disk emulated exact capabilities match network-attached disk processor network-attached disk engineered low power consumption absolute performance reasonable approximation network-attached storage unit disks high-end processing capabilities rivaling cpu power commodity network emulation methodology potential performance bene active disk servers depend exact characteristics environment perceived network latency low smaller performance bene combining operations script contrast high-latency scenario wide-area dial-up link home performance bene larger study impact network performance scfs implemented framework increasing network latency controlled amount approach similar introduced martin study effects latency overhead bandwidth parallel applications speci cally delay packet server queues packet internally records time packet received thread server removes messages queue designated delay passed services experiments shown validate delay mechanism behaves desired performance enhancements achieve single logical task network system series dependent interactions client server required dependencies impose synchrony clientserver interactions operation initiated previous returned clients incur multiple network round-trips accomplish single logical task result signi performance loss high-latency networks severe server load scfs srpc framework group dependent operations script executed disk single network round-trip illustrate cacy scripting infrastructure case studies rst study combine client 
lookup operation dependent getattr read calls single script merge nfs-like consistency check read modi page similar http get-if-modi ed-since request avoid readmodify-write cycle sending partial write script server motivation discuss case studies rst examine effects server load perceived client latency network latency concern wide-area dial-up connections round-trip times milliseconds uncommon tightly-coupled cluster high-performance communication subsystem latency considerable factor due server load illustrate effects server load perceived round-trip time perform simple experiment experiment measure round-trip times observed single client communicating server small byte messages vary number competing clients continually sends stream large requests server competing processes perceived average round-trip time reasonable roughly gigabit ethernet network traf c-inducing competitors added perceived round-trip time client increases dramatically competing processes note effect strictly due queuing network interface server touch data cpu under-utilized problem illustrated obvious inspection overlooked design distributed systems clusters fact messages transmitted quickly idle system avoid convoy effect small messages queue large network interface promptly reach destination response time performance metric throughput server load easily transform low-latency network perceived authors discuss assumptions made implementation distributed cluster-based service cluster low-latency san latency wide-area internet means two-phase commits prohibitively expensive made similar assumptions design cluster-based systems page section average cost readdir getattr additional network delay performance combined read-getattr separate combined tcl uncached combined tcl cached combined figure combining operations read getattr gure shows performance combining read getattr operations single script gure plots average cost performing directory read subsequent attribute request increase perceived round-trip latency request x-axis separate line shows performance network round-trips required lines graph show performance combining operations server combined tcl uncached line shows tcl performance procedure installed request combined tcl cached line shows performance script pre-installed finally combined line baseline performance implementation script data point represents average trials variance shown low differently clients operation load experiments section evaluate performance scfs function network latency read getattr optimization rst case study illustrate performance bene combining directory read attributes directory disk object model basis scfs lookup client involves reading directory path search path component worst case path components local cache round-trip network operations required component path rst read directory page parent attributes child object initialize vfs fast inode scfs cases srpc requests infrastructure generated disk tests enables client size send script total data-set reads size directory page searches cheetah disks speci lename refer object fast disks performs getattr rpm seagate object script barracuda returns disks results slow disks read fast getattr disks calls deliver data client completes roughly lookup operation slow disks path component approximately network apiece round-trip experiments perform client trials system show designed average performs standard lookups deviation components experiments compare pathname performance single script lfs raid avoids standard network raidstriping round-trips stripe sizes approach chosen integrate easily maximize performance linux in-kernel raidgiven framework evaluate workload hand trade-offs making scripts comparison fair read-getattr operation function slightly unfair network latency lfs compare raid perceived baseline latency performance implementations experiment figure demonstrate rst baseline implementation performance labeled lfs separate raid gure top standard homogeneous implementation storage configurations synchronously issues separate slow requests disks rpc fast labeled disks combined experiment tcl consists uncached sequential tcl write script sequential combine read random write operations random script read phases installed based patterns interpreter generated invocation bonnie labeled iozone combined tcl benchmarks cached perform considers experiment case demonstrate pre-installed script unexpected fourth overhead implementation labeled implementation combined scales combines higher-performance disks effectively figure sequential write sequential read random writes perform excellently achieving high bandwidth disk configurations surprisingly log-based file system random reads perform poorly achieving roughly slow disks fast disks line expect disks typical raid configuration throughput amount written performance expansion disk added disk added disk added figure storage expansion graph plots performance lfs storage expansion experiment begins lfs writing single disk time written disk brought on-line lfs immediately begins writing increased performance disk expansion accomplished simple command adds disk region file system time on-line expansion demonstrate performance system writes disks added system on-line experiment disks present expansion stresses software infrastructure hardware capabilities figure plots performance sequential writes time disks added system x-axis amount data written disk shown y-axis plots rate recent committed disk graph lfs immediately starts disks write traffic added system read traffic continue directed original disks older data lfs cleaner redistribute existing data newly-added disks explicitly cleaning explored possibility dynamic parallelism explore ability lfs place segments dynamically regions based current performance characteristics system order demonstrate ability lfs react static dynamic performance differences devices reasons performance variation drives disks added faster older unexpected dynamic performance variations due bad-block remapping hot spots workload uncommon lead performance random writes perform similarly due nature lfs throughput heterogeneity configuration fast disks slow disks performance static heterogeneity lfs exraid ffs ccd figure static storage heterogeneity figure plots performance lfs versus ffs ccd standard raidstriping series disk configurations x-axis number fast slow disks varied implies fast disks slow adjusting segments written dynamically lfs raid deliver full bandwidth disks contrast standard striping performs rate slowest disk system test written disk heterogeneity disks ability expand disk system on-line shown induces workload imbalance read traffic directed newly-added disks cleaner reorganized data disks system experiment static dynamic performance variations subsection figure shows results static heterogeneity test sequential write performance lfs dynamic segment placement scheme plotted ffs top netbsd concatenated disk driver ccd configured stripe data raidfashion experiments data written disks x-axis increase number slow disks system extreme left disks fast slow middle heterogeneous configurations figure writing segments dynamically proportion delivered disk performance lfs raid deliver full bandwidth underlying storage system applications performance degrades gracefully slow disks replace fast storage system raidstriping performs rate slowest disk performs poorly heterogeneous configuration perform misconfiguration test experiment configure storage system utilize partitions disk emulating misconfiguration administrator similar spirit tests performed brown patterson disk system appears separate disks case lfs raid throughput amount written performance dynamic heterogeneity lfs exraid ffs ccd figure dynamic storage heterogeneity figure plots performance lfs raid ffs ccd dynamic performance variation experiment performance single disk temporarily degraded faulty disk delays requests fixed time reducing throughput disk operations language version serves baseline ideal combined performance utilized understand overhead scripting gure draw conclusions fast interpreted language approaches speeds combining read getattr operations single script leads lower overhead base implementation amount network latency caching tcl procedures greatly reduces cost operation removing half millisecond overhead operation finally higher latencies combining operations slow interpreter uncached tcl procedures strictly standard implementation note latencies higher end tightly-coupled cluster environment low dial-up link latencies milliseconds uncommon demonstrate utility read-getattr optimization realistic benchmark utilize postmark benchmark postmark system benchmark constructed mimic workload typical mail server consists create phase transaction phase les randomly created deleted read appended generate realistic workload make postmark benchmark separate create phase transaction phase present results transaction phase left unmodi creation phase warms cache arti cially speeds transaction phase change make results realistic layout directories default behavior postmark creates directories level directory hierarchy measurements reveal read-getattroptimization removes round-trip messages combining readandgetattr pairs combined read-getattrscript calls benchmark total messages simple optimization reduces number round-trip latencies incurred client read modi optimization case study illustrate scripting interface make nfs-style consistency checks cient nfs primitive form consistency periodically validating cached copy client server time validation threshold cached copy considered suspect read request suspect copy client sends getattr request server check changed cached changed client invalidates cached copy fetches pages server operations dependent client issue read knowing cached copy stale wastes network bandwidth re-fetch valid pages srpc framework operations merged single script getattr checks modi cation time higher speci modi cation time cached copy sends desired page attributes refer combined functionality read-if-modi ed-since operation srpc support average time file read percentage modified data read-if-modified-since delay separate combined tcl cached combined average time file read percentage modified data read-if-modified-since delay separate combined tcl cached combined figure nfs read-if-modi ed-since graphs plot performance ofgetattrfollowed conditional readof data data changed check graph top additional network latency graph bottom add xed network latency x-axis increase percentage les changed check data point average trials system performs logical task incurs fewer synchronous network round-trip delays figure shows performance read-if-modi ed-since compared standard getattr conditional read x-axis graphs vary percentage data modi read re-fetched server graph top plot performance assuming additional network delay graph bottom assume additional network delay compare performance standard implementation cached tcl script combined functionality implemented gure draw conclusions lowlatency environment illustrated graph top bene combination script small realized highperformance interpreter cached tcl implementation adds overhead higher-latency environment illustrated graph bottom large fraction les changed tcl implementation perform standard implementation greater average cost partial write additional network delay performance partial writes read-modify-write partial write tcl cached partial write figure partial writes gure shows performance performing synchronous partial write script reading data server modifying writing data server gure plots average cost performing synchronous -byte write increasing perceived latency request x-axis read-modifywrite line shows performance standard read-modify-write case lines show performance scripted approach partial write tcl cached line shows tcl performance script pre-installed partial write line baseline performance implementation script point average trials les optimizations combine operations limited circumstances overhead interpretation dominant factor higher performance interpreter worth implementing kinds features partial write optimization writes size page problematic traditional systems partial-page write translates read-modify-write cycle read relevant page cache modi cation affected portion page write complete update read-modify-write cycle occurs blockbased systems export read write operations page granularity srpc framework avoid read-modifywrite 
cycle send data written script performs partial write server removing synchronous page-sized read critical path figure displays performance server-side execution partial-write compared standard read-modify-write cycle gure avoiding synchronous page read great bene additional network delay avoiding excessive data movement read-modifywrite cycle pays dividends performing single round-trip factor reduced latency easily achieved evaluate utility partial-write optimization realistic benchmark setting addition postmark benchmark investigate traf savings debit-credit benchmark tpc-b benchmark intended model workload database server manages bank transactions measurements reveal partial-write script reduces bandwidth considerably postmark debit-credit benchmarks postmark total message traf bytes reduced roughly debit-credit benchmark savings dramatic full traf removed reason substantial reductions network bandwidth straightforward benchmarks perform small writes avoiding page-level data transmissions required readmodify-write cycle network load considerably reduced discussion case studies power combine arbitrary operations server helps overcome limitations exported rpc interface enabling clients build customized performance optimizations important bene approach arises server-side srpc designer distributed service focus simpler task providing highly-tuned primitives allowing clients compose primitives full scope required functionality noted srpc optimize performance network systems ways case studies presented speci cally client performs predictable series dependent operations server incurs multiple network round-trips operations combined single script additional case system ensure set writes reach disk xed order crash recovery traditional system client ensure ordering perform write synchronously label type synchronous operation false synchrony client synchronous operations enforce order ensure operation reached stable storage srpc framework false synchrony avoided client perform writes asynchronously guaranteed writes reach disk desired order functionality enhancements traditional servers implement single xed protocol size solution limits functionality clients expect server exible approach wrongly assumes single protocol meets requirements clients respect les consistency semantics client system implement strongly constrained protocol interface exported server server exports nfs-like interface restricts clients weak level consistency server stronger consistency model forces clients incur consequent overhead srpc clients enhance physical protocol provided server implement enhanced virtual protocols srpc enable clients implement sophisticated consistency semantics top nfs-like physical protocol case studies implement afs sprite consistency semantics cases scfs interface remains existing clients nfs-like semantics continue operate smoothly key feature examples demonstration framework state easily added previously stateless protocols nfs earlier work spritely nfs demonstrated capabilities rewriting server client extensively note consistency semantics functionality enhancement scripts utilized implement object-based disk interface top block-based server hinted section average time close client callbacks broken afs close script overhead tcl-loop broadcast serial std lib broadcast serial std lib broadcast parallel figure afs close script overhead average cost executing afs close dirty script shown number callbacks broken increases afs consistency afs write-on-close consistency model clients consistent image open close operation open client installs callback server reads entire local disk cache subsequent reads writes performed local copy closed client writes back modi server client stores modi version back server callbacks broken clients invalidate cached copies forcing fetch server time open server actively involved ensuring afs consistency server exports xed nfs protocol support afs semantics srpc infrastructure disk implementing afs consistency semantics feasible servers track state variables live invocation single script afs relevant state disk object callback list list client machines cached scripts required afs open afs close dirty open client sends afs open script installs client address per-object callback list client reads object standard read interface caches local disk modi closed client client sends afs close dirty script server script loads callback list object invokes send rpc list library routine send callback break listed clients performance consistency model dif cult measure present time takes execute afs close dirty script number callbacks broken increased figure plots performance implementations script rst labeled tcl-loop broadcast serial script sends message synchronously clients callback list case labeled std lib broadcast serial tcl script makes single call standard library routine synchronously issues rpc callbacks clients performance improves version due fewer number crossings tcl boundary finally case labeled std lib broadcast parallel multiple threads standard library issue callexperiment avg read cost overhead direct read tcl cached table read write-sharing overhead average cost executing sprite read callback script shown rst row table labeled direct read shows cost reading page synchronously serves lower bound execution time rows table show cost executing tcl implementations script establishing overhead reads experience write-sharing experiment read cost calculated average reads occur large copy back requests asynchronously issuing rpc callbacks parallel improves performance slightly experiment demonstrates performance bene including primitives standard library sprite consistency sprite stronger consistency model afs semantics referred perfect consistency close approximation unix local system semantics sprite model clients cache les long write sharing write sharing occurs adaptively writing data disks lfs raid dynamic segment placement adjust imbalance deliver higher throughput writes data disk standard striping delivers dynamic segment striping lfs successfully balance load disks case properly assigning load partition accidentally over-burdened disk final heterogeneity experiment introduce artificial performance fault storage system consisting fast disks order confirm load balancing works face dynamic performance variations figure shows performance write lfs raid dynamic segment placement ffs ccd raidstriping case single disk exhibits performance degradation data written kernel-based utility temporarily delay completed requests disks delay effect reducing throughput impaired disk returned normal operation additional data written figure lfs raid job tolerating fluctuations induced phase experiment improving performance factor compared ffs ccd flexible redundancy redundancy experiment verify operation system face failure figure plots performance set processes performing random reads redundant files lfs initially bandwidth disks utilized balancing read load mirrored copies data throughput amount read performance failure disk failed disk failed figure storage failure figure 
plots random read performance set mirrored files disks lfs labeled points graph disk offline performance decreases lfs longer balance read load replicas note lfs raid survive single disk failure failure lfs raid tolerate loss disk set experiment progresses disk failure simulated disabling reads disks lfs continues providing data replicas performance reduced demonstrate flexibility per-file redundancy redundancy managed file system total files written concurrently system consisting fast disks percentage files mirrored increased x-axis results shown figure expected net throughput system decreases linearly files mirrored mirrored throughput roughly halved per-file redundancy users pay users file redundant performance cost replication paid write performance write reflects full bandwidth underlying disks lazy mirroring final experiment demonstrate performance characteristics lazy mirroring figure plots write performance set lazily mirrored files delay seconds cleaner begins replicating data normal file system traffic suffers small decline performance default replication delay system minutes length abbreviated delay reduce time experiments figure potential benefits lazy mirroring potential costs lazily mirrored files deleted replication bethroughput percent files written redundantly cost redundancy figure per-file redundancy figure plots performance writes separate files percent files mirrored increases files mirrored net bandwidth system drops roughly half peak rate expected peak bandwidth achieved lower previous experiments due increased number files subsequent meta-data operations experiment written disk gins full throughput storage layer realized lazily mirrored files deleted replication system incurs extra penalty files read back disk replicated affect subsequent file system traffic lazy mirroring carefully systems highly bursty traffic idle time lazy replicas created files easily distinguishable short-lived discussion implementing lfs raid concerned pushing functionality file system code unmanageably complex primary goals minimize code complexity achieve goal integrating major pieces functionality additional lines code increase original size lfs implementation additional code roughly half due redundancy management design standpoint find managing redundancy file system benefits difficulties solve crossed-pointer problem applied divide-andconquer technique placing primary copy file sets mirror enable fast operation failure solution limits data placement flexibility file assigned set subsequent writes file written set limitation affects performance heterogeneous configuthroughput time performance lazy redundancy lfs exraid cleaner figure lazy mirroring figure plots write performance client open opened write mode write-sharing occurs sprite turns caching clients send reads directly server finding write-shared requires server track state unmodi nfs-based system major design concern implementing sprite semantics scfs ensure reads writes slowed common case write sharing occurs executing scripts negative performance impact script part read write operation meet goal developed design object state variables read callback list list clients open cached reading writers list tracks clients open writing designating cacheable on-going write-sharing client state believes non-cacheable read request client sends sprite read callback script sprite read callback script checks cacheable set script adds address client read callback list returned client client mark state local inode cacheable subsequent reads scripted issued normal rpc read requests write sharing overhead paid scripting client opens write mode client sends sprite write open script script sends invalidate rpc members read callback list marks non-cacheable adds address writers list object registered writer client marks cacheable local inode in-memory cache satisfy read requests client closes sends sprite write close script disk removes client writers list marks cacheable writers list empty registered writers subsequent scripted-reads experiment avg update cost overhead base tcl cached table directory update script overhead average cost executing directory update script shown tcl implementations compared version concurrency control experiment object created inserted directory tcl implementations directory information server contrast base entry shows performance updating directory directly concurrency protocol case directory page written server update clients notice cacheable status cache overhead scripts reading shown table direct read entry shows average cost read copy assuming write-sharing case experiment read takes roughly entry shows cost executing 
version sprite read callback script adds roughly overhead finally tcl version script runs approximately slowly non-scripted read experiment importance calling sprite read callback script discussion general ability extend interface server scripts expands types functionality client systems implement case studies shown functionality requires state object server brie discuss examples stateful scripts implement scfs functionality client system implement ne-grained copy-onwrite system state required track byte ranges object copied script required client reads writes write script transparently redirects write copied version object read script directs read version object imagine optimizations similar sprite case study avoid invoking scripts read write operation clients associate arbitrary type meta-data virtual meta-data interesting piece extensible meta-data general access control list acl provide exible sharing permission bits provided server system read write operation client send script check credentials user fully ensure clients bypass protection check simply calling existing rpcbased read write calls permission bits disabled potential security hole illustrates general principle scripts assume cooperation set mutuallytrusting clients client call scripts enforce sprite consistency semantics clients desired behavior simplicity enhancements srpc arbitrary operations grouped executed server greatly simpli implementation atomic sets operations isolated respect read partial afs sprite directory operation attr modi write open close read callback write open write close update sec sec sec sec sec sec arithmetic ops control list ops state mgmt type checking library locks communication native calls total lines table tcl functional breakdown table categorizes static operation counts script script minimum maximal path costs shown left-most column presents categories arithmetic operations control statements list operations create add iterate state management stateful protocols type checking library searches copies utility functions locks lock unlock communication extra rpcs mandated rpc reply native calls calls raw rpc-exported functions total bottom column lists total number lines script note number lower higher sum previous operations cases statements accounted separately end brace simple assignment statements cases line consist multiple statements arguments math expressions script identi column header section number concurrent operations clients demonstrate case study srpc simplify implementation seemingly complex distributed concurrency problem concurrent directory updates disk object model directories considered equivalent objects disk directory operations create delete translate reads writes server create client rst reads directory page inserts directory entry pertaining vacant slot writes page back proper concurrency control simultaneous creates directory clients lead lost create scfs ensure directory readmodify-write sequence performed atomically traditional server common ensure atomicity distributed locks client rst acquires lock directory object performs read-modify-write nally unlocks directory approach result sub-optimal performance due multiple network roundtrip operations required phases importantly makes system signi cantly complex speci cally server track locks multiple client machines handle distributed failure scenarios client crashing momentarily losing network connectivity holding lock srpc framework greatly simpli implementation atomic operations co-locating server operations distributed machines concurrent directory update case study client sends script disk acquires in-memory lock server performs read-modify-write releases srpc reduce complex distributed concurrency problem simpler challenge ensuring mutual exclusion threads server address space table shows performance tcl implementation directory update compared cost simply sending directory page server approach provide concurrency control represents cost simply writing page perform directory update table overhead simple concurrent directory update satisfactory providing functionality dif culties encountered implementing robust three-phase protocol discussion case study brie illustrated srpc simplify client server code centralized script server simplify functionality cases traditional distributed algorithms required multiupdate atomic transactions natural provide scripting framework transactional capabilities mandate additional functionality srpc standard library including ability roll back perform crash recovery future plan investigate utility transactional support srpc framework analysis tcl section explore costs script execution tcl environment provide detailed accounting scripts implemented including functional cost breakdowns end discussion ndings functional breakdown table shows breakdown tcl commands scripts case studies grouped categories arithmetic operations control statements statements list operations number scripts lists basic data structure state management routines saving restoring longlived server state type checking ensure illegal memory dereferences occur library routines utility functions copies string searches locks communication single reply mandated rpc native calls rpc-exported routines table make number general observations line column total number lines script observe scripts implement powerful functionality small amount code scripts lines code regime ranging low high -line range low end afs open afs close dirty scripts simply manage state required track clients les open partial write script performs server-side read-modify-write expected manner complex script sprite write open script straight-forward lines consist simple state list management routines learn table scripts broken constituent commands scripts controlow decisions complex sprite write open sprite write close scripts statements largely composed straight-line code scripts require state afs sprite consistency scripts tcl command count consists retrieving state manipulating form list operations storing state finally type checking library operations comprise substantial component scripts cost breakdown functional breakdown instructive timebased analysis dif cult pinpoint location bottlenecks garner insight instrumented scripts case studies allowing collect detailed information time spent scripts figure presents results investigation gure make number observations scripts invocation overhead accounts substantial portion time execute scripts time consists statements set-up relevant environment script call tcl eval invoke script percentage varies scripts invocation overhead fairly constant varying detailed instrumentation reveals cost roughly two-thirds attributed tcl eval call type checking buffer pointers library commands combine signi amount time scripts current system type check requires call tcl substructure calls expensive future versions cost reduced batching type checks library commands routines primarily called data copy routines manipulate input parameters construct return results overheads dif cult avoid additional communication expensive dwar costs effect observed afs close dirty script experiment breaks single callback client cached copy relevant cost communication script dominates costs finally native routines account reasonable amount time scripts unlike components bar higher percentage native portion bar portion represents direct calls underlying service real work script performs native portion read-if-modi ed-since optimization consists getattr conditional read functions copy data perform type checking pure overhead discussion small code size demonstrated case studies arguments scripting approach extensibility small code segments easier write maintain fewer lines code implies fewer bugs leading cost breakdown tcl scripts time invocation math list ops state mgmt type check library locks comm native read-getattr read-if-mod partial-write afs open afs close sprite 
read sprite open sprite close dir update figure tcl cost breakdown graph depicts percentage time spent script typical scenario bar represents single script labeled x-axis broken categories based operational breakdown table additional item breakdown invocation cost time spent infrastructure tcl script control statements low cost accounted graph absolute execution times script read-getattr read-if-modi ed-since partial-write afs open afs close sprite read callback sprite write open sprite write close directory update measurements ect average runs robust reliable systems experience brought downsides scripting performance conscious tcl programming dif cult analogous process language environments costs arise unexpected sources making dif cult programmers optimize code simple math expression set expr executes roughly pentium-based platforms slight variant set expr extra spaces executes factor slower speci case parsing single argument passed exprcommand faster passing separate arguments cases found subtle differences programming style lead non-trivial differences performance general problem illustrated dif culty programming top system high-level tcl interpreter complex virtual machine dif cult process programming high-performance makes times slower expected related work active storage active storage forms existing literature previous work context programming environments existing rpc-based services easily earliest work found database literature researchers sought exploit processing capability disk arm increase database performance recent efforts active storage termed active disks studied independently acharya riedel acharya proposed specialized stream-based programming model parallel applications model applications re-partitioned host disk portions disk runs disklet small piece java code lters data per-record granularity riedel studied parallel applications focusing scan-intensive codes applications partitioned host disks disk runs small portion code lter requests reduce total bandwidth host active disk systems effective supporting variety user-level parallel applications developing general distributed service recently amiri introduced abacus object-oriented framework developing active storage systems abacus similar srpc authors developed distributed object storage system top abacus demonstrated performance bene abacus differs distributed object store built scratch distributed object environment code exists user-level abacus well-integrated kernel mount abacusbased system system calls redirected inside kernel user-level proxy inef cient main strengths abacus approach work dynamically migrated client server depending system workload characteristics similar adaptive framework utilized srpc extensibility srpc related long line work extensible systems pioneered systems spin exokernel vino systems sought enable extensibility operating systems seek enable extensibility rpcbased services lessons learned systems apply srpc techniques vino survive misbehaving kernel extensions directly applicable framework slice virtual service extends services client side interposition introducing client-side packet lters slice build virtual service top existing protocols nfs transparently system clients slice interposition srpc server-side scripting complimentary approaches slice adding activity client srpc adding server suggested utility scripting languages extensible systems direct found choices operating system authors suggest tcl-like scripting language extensions fail-stutter fault tolerance remzi arpaci-dusseau andrea arpaci-dusseau department computer sciences wisconsin madison abstract traditional fault models present system designers extremes byzantine fault model general difficult apply fail-stop fault model easier employ accurately capture modern device behavior address gap introduce concept fail-stutter fault tolerance realistic tractable fault model accounts absolute failure range performance failures common modern components systems built fail-stutter model perform highly reliable easier manage deployed introduction dealing failure large-scale systems remains challenging problem designing systems form backbone internet services databases storage systems account possibility likelihood components cease operate correctly handles failures determines system performance availability manageability traditionally systems built fault models extreme byzantine failure model lamport component exhibit arbitrary malicious behavior involving collusion faulty components assumptions contexts security make difficult reason system behavior extreme tractable pragmatic approach exists fail-stop model limited approach defined schneider response failure component state permits components detect failure occurred stops component working component fails components immediately made aware problem byzantine model general difficult apply problem fail-stop model simple account modern device behavior model realistic tractable fail-stop model good starting point model storage-aware caching revisiting caching heterogeneous storage systems brian forney andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison bforney dusseau remzi wisc abstract modern storage environments composed variety devices performance characteristics paper explore storage-aware caching algorithms file buffer replacement algorithm explicitly accounts differences performance devices introduce family storageaware caching algorithms partition cache partition device algorithms set partition sizes dynamically balance work devices simulation show storageaware policies perform similarly landlord costaware algorithm previously shown perform web caching environments demonstrate partitions easily incorporated clock replacement algorithm increasing likelihood deploying cost-aware algorithms modern operating systems introduction modern computer systems interact broad diverse set storage devices including local disks remote file servers nfs afs archival storage tapes read-only media compact discs dvds storage sites accessible internet storage components introduced behaviors properties divergent today set devices disparate commonality pervades time access high compared cpu cache memory latencies due cost fetching blocks storage media caching blocks main memory reduces execution time individual applications increases system performance argue orders exibility magnitude safety provided storage technology interpreted dramatically languages changed outweigh potential past performance loss decades important hypothesis aspects caching topics architectures investigate modern paper operating similarly systems swarm remained scalable unchanged storage system sends tcl innovations scripts mechanism servers including order read integration write file cache data virtual memory page authors state cache swarm makes copyon-write techniques feature software emulation bits change policy operating systems employing lru lru-like algorithms decide block replace problem lru related caching algorithms cost-oblivious blocks treated fetched identically performing devices re-fetched replacement cost blocks assumption increasingly problematic manifold device types correspondingly rich set performance characteristics simple block fetched local disk compared fetched remote highly contended file server case operating system prefer block file server heterogeneous environments file systems require caching algorithms aware replacement costs file blocks slowest device roughly determines throughput system storage-aware caching seeks balance work devices adjusting stream block requests heterogeneous environment storage-aware cache considers workload behavior device characteristics filter requests paper explore integration costaware algorithms operating system page cache simulation simulation accounts real-world factors integrated page cache simplicity design build previous work cost-aware caching web-cache theory communities demonstrating separate set partitioned algorithms effective simpler proposals research areas study set context network-attached disk system network-attached disks increasingly important storage paradigm present clients enhanced order account complex behaviors modern components main reason enhancement debugging rpc realm rpc aware system highly similar srpc recent work area concentrates increasing performance exibility rpc substrate reducing code size automatically-generated stubs excellent found work flick exible infrastructure building optimizing rpc layer main goal flick separate cient stub generation speci interface-de nition language idl underlying communication layer work scriptable rpc extended flick framework conclusions current technology trends expect core building blocks future systems signi processing capabilities imperative future distributed services ability effectively leverage active components paper introduced srpc scriptable rpc layer enables system developers migrate existing distributed services active servers designed srpc meet goals essential environment srpc designed provide smooth migration path existing distributed services traditional rpc-based server active server goal accomplished making process developing srpc-based service similar developing service traditional rpc paradigm automating steps enhanced idl compiler srpc engineered high performance srpc caches scripts support concurrent script execution key operations implemented directly efciency srpc targets safe extensibility tcl base scripting language crucial factor meeting goal number case studies demonstrated general bene scfs traditional rpc-based system shown srpc improve client performance scripting client merge operations dependencies single operation client disk reducing number network round-trips demonstrated srpc enables functionality easily integrated system cases original designers foresee bene functionality cases clients desire functionality les finally shown scripts permit operations co-located disk distributed multiple clients acquiring releasing locks avoiding complex code crash recovery distributed failure scenarios evaluation tcl tcl enables development short powerful scripts complex case studies required thirty lines tcl code performance tcl past tcl system extensions scenarios higher performance required specialized domain-speci language active disks worth investigating desirable feature language predictable cost model allowing programmers optimize code direct obvious manner future interesting examine srpc context multiple network-attached disks single server active framework disks communicate cooperate directly implementing advanced features snap-shots lazy redundancy key challenge system provide proper primitives distributed coordination server-side scripts removing burden implementing complex distributed systems protocols ideal storage system future simple collection srpc-enabled disks base primitives distributed computation srpc standard library higherlevel le-system functionality built top exible cient scripting substrate acknowledgments venkateshwaran venkataramani work project early stages skepticism led nement ideas members wind research group input ideas presentation paper csclass lively feedback finally anonymous reviewers excellent thoughtful suggestions greatly improved content paper work sponsored nsf ccrccr- ngsccr- itrand wisconsin alumni research foundation acharya uysal saltz active disks proceedings conference architectural support programming languages operating systems asplos viii san jose october amiri petrou ganger gibson dynamic function placement data-intensive cluster computing proceedings usenix annual technical conference pages june anderson chase vahdat interposed request routing scalable network storage transactions computer systems tocs february berners-lee fielding nielsen gettys mogul hypertext transfer protocol http technical report internet engineering task force january bershad savage przemyslaw pardyak fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles order december birrell increasing complexity nelson modern implementing systems remote procedure latest calls acm pentium transactions million computer transistors systems future hardware promises february boden complexity cohen advent felderman intelligent kulawik devices software seitz code bases seizovic mature code size myrinet increases gigabitper-second local-area complexity network linux ieee kernel source micro increased february factor campbell ten tan increasing choices complexity directly objectoriented affects multimedia component operating behavior system complex components workshop hot topics behave operating simple systems predictable ways hotos-v orcas island identical disks made chen manufacturer bershad receiving impact input operating stream system structure necessarily memory deliver system performance disks purveyor erratic performance discuss document similar behavior observed hardware software components systems built fail-stop illusion prone poor performance deployed performing working perfectly failing deliver good performance single component behave expected vulnerable systems make static parallelism assuming components perform identically striping raid techniques perform disk system delivers identical performance performance single disk consistently lower rest performance entire storage system tracks single slow disk parallel-performance assumptions common parallel databases search engines parallel applications account modern device behavior model fault behavior model account components fail perform erratically term unexpected low performance component performance fault introduce fail-stutter fault model extension fail-stop model takes performance faults account focus fail-stutter model component performance fail-stutter model building systems manageable reliable allowing plug-and-play operation incremental growth worry-free replacement workload modification fail-stutter fault tolerant systems decrease human intervention increase manageability diversity system design enabled reliability improved finally fail-stutter fault tolerant systems deliver consistent performance improves availability paper build case fail-stutter fault tolerance examination literature discuss fail-stutter model benefits review related work conclude erratic behavior systems section examine literature document places performance faults occur note list illustrative means exhaustive survey find device behavior increasingly difficult understand predict cases erratic performance detected investigated discovered hinting high complexity modern systems interestingly performance variations research papers well-controlled laboratory settings running single application homogeneous hardware speculate component behavior lesscontrolled real-world environments worse hardware begin investigation performance faults caused hardware focus important hardware components processors caches disks network switches case increasing complexity component time led richer set performance characteristics processors caches fault masking processors fault masking increase yield allowing slightly flawed chip result chips characteristics sold identical viking series processors sun examined authors measure cache size set viking processors micro-benchmark single ssis base case graphs reveal effective size level cache direct-mapped specifications suggest level-one data cache size -way set associativity chips produced portions caches turned produced times study measured application performance vikings finding performance differences pa-risc fault-masking cache schneider reports cache mechanism maps bad lines improve yield fault-masking present modern processors vaxhad -way set associative cache turn sets failure detected similarly vaxhad directmapped cache shut cache fault finally univac ability shut portions cache faults prediction fetch logic processor prediction instruction fetch logic complex parts processor performance characteristics sun ultrasparc-i studied kushman finds implementation next-field predictors fetching logic grouping logic branch-prediction logic lead unexpected run-time behavior programs simple code snippets shown exhibit nondeterministic performance program executed processor identical conditions run times vary factor kushman discovered anomalies anomalies remains unknown replacement policy hardware cache replacement policies lead unexpected performance work replicated fault-tolerance bressoud schneider find tlb replacement policy processors non-deterministic identical series location-references tlb-insert operations processors running primary backup virtual machines lead tlb contents reason non-determinism static dynamic forms performance heterogeneity algorithms develop general applied broader range storage devices main results show storageaware caching significantly performance robust cost-oblivious caching robust leading web-caching algorithm operating systems specific implementation develop evaluate version storage-aware caching extends commonly implemented clock algorithm rest paper organized section give overview algorithms investigate paper describe algorithm selecting partition sizes section section describes performance assumptions proceedings environment acm detail symposium explains operating simulation systems framework principles simulation results sosp pages section asheville compare contrast december work existing dahlin work wang section anderson section patterson future cooperative work caching finally conclude remote client section memory algorithm improve overview file system section performance overview proceedings algorithmic space usenix explore symposium operating describe systems existing design cost-aware algorithms implementation 
pages november davis principles software development mcgrawhill denehy arpaci-dusseau arpacidusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey june dewitt hawthorn performance evaluation data base machine architectures proceedings seventh annual conference large data bases vldb pages eide frei ford lepreau lindstrom flick flexible optimizing idl compiler pldi las vegas june engler kaashoek toole exokernel operating system architecture application-level resource management proceedings acm symposium operating systems principles december fiuczynski martin bershad culler spine operating system intelligent network adapters technical report tr- washington department computer science engineering august gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings conference architectural support programming languages operating systems asplos viii october gibson nagle amiri chang feinberg gobioff lee ozceri riedel rochberg zelenka file server scaling network-attached secure disks proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle june gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie-mellon gokhale schmidt measuring performance corba internet inter-orb protocol atm technical report wucs- washington louis gray storage bricks arrived invited talk usenix conference file storage technologies fast gribble brewer hellerstein culler scalable distributed data structures internet service construction proceedings fourth symposium operating systems design implementation osdi san diego october hartman murdock spalink swarm scalable storage system proceedings ieee international conference distributed computing systems icdcs austin texas june howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems tocs february katcher postmark file system benchmark technical report trnetwork appliance october kistler satyanarayanan disconnected operation coda file system acm transactions computer systems tocs february lee thekkath petal distributed virtual disks proceedings seventh conference architectural support programming languages operating systems asplos vii pages cambridge october lowell chen free transactions rio vista proceedings acm symposium set operating lazy systems redundant principles files lfs surprised numerous replication delay engineers disks seconds fault peak masking performance disks achieved perform initial degree fault basis comparison present caching algorithms based partitioning cache replacement cost existing cost-aware algorithms theoretical community studied cost-aware algorithms k-server problems restricted class k-server problems weighted caching closely related cost-aware caching landlord significant algorithm literature comparison landlord closely related leading web caching algorithm landlord combines replacement cost cache object size locality extending lru fifo include cost variable cache object sizes cache configured landlord lru describe lru version landlord associates cost object called object enters cache landlord sets retrieval cost object divided size object object eviction needed landlord finds object lowest removes ages remaining objects landlord ages pages decrementing remaining objects evicted object object cache landlord restores landlord degenerates strict lru values landlord attractive theoretical experimental properties shown theoretical analysis size cache objects landlord k-competitive size cache fixed object size cache landlord performs factor optimal off-line algorithm request sequences overview aggregate partitioned algorithms cost-aware algorithms literature place-anywhere place-anywhere algorithm characteristics blocks occupy logical location cache independent original source cost costs recorded page granularity advantage place-anywhere algorithms calculate single trade-off locality cost replacement time algorithms bias eviction pages low retrieval cost contrast place-anywhere algorithm aggregate partitioned algorithm divides cache logical partitions blocks logical partition device share replacement cost algorithm aggregates replacement cost function device performance aggregate partitioned algorithm benefits aggregation blocks cost metadata ways amount metadata reduced metadata closely reflects current replacement cost block device space overhead proportional sosp number pages devices saintmalo france blocks october mai replaced paaske jayasena replacement cost low dally conversely horowitz place-anywhere smart algorithms memories record modular cost recon gurable page architecture brought proceedings cache annual international cache symposium computer large number architecture pages pages common today june place-anywhere martin algorithm vahdat culler anderson effects communication latency overhead bandwidth cluster architecture proceedings annual international symposium computer architecture pages denver colorado june acm sigarch ieee computer society tcca muthitacharoen chen mazi eres lowbandwidth network file system proceedings acm symposium operating systems principles sosppages banff canada october nelson welch ousterhout caching sprite network file system acm transactions computer systems february nester philippsen haumacher cient rmi java proceedings acm java grande conference san francisco california june malley proebsting montz usc universal stub compiler proceedings conference communications architectures protocols applications sigcomm london august ousterhout tcl embedable command language proceedings usenix association winter conference patterson anderson cardwell fromm keeton kozyrakis thomas yelick intelligent ram iram chips remember compute ieee international solid-state circuits conference san francisco february riedel gibson faloutsos active storage large-scale data mining multimedia proc international conference large databases vldb august romer lee voelker wolman wong baer bershad levy structure performance interpreters proceedings seventh international conference architectural support programming languages operating systems pages cambridge massachusetts october acm sigarch sigops sigplan seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings usenix symposium operating systems design implementation october small seltzer comparison extension technologies proceedings usenix annual technical conference january srinivasan mogul spritely nfs experiments cache-consistency protocols proceedings twelfth acm symposium operating systems principles pages acm december order susceptible thekkath mann lee frangipani scalable distributed file system proceedings acm symposium operating systems principles sosp pages saint-malo france october transaction processing council tpc benchmark standard speci cation revision technical report von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings fifteenth acm symposium operating systems principles pages copper mountain resort usa von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedings annual symposium computer architecture gold coast australia welsh culler achieving robust scalable cluster java lcr workshop languages compilers run-time systems scalable computers rochester 
masking documented simple bandwidth experiment shows differing performance -rpm seagate hawk inconsistent cost values aggregate partitioned algorithms avoid problem aggregating cost metadata per-device basis performance device cost metadata rarely inconsistent period time placeanywhere algorithm recognize change cost device propagate cost pages cache cost update requires significant number pages updated increasing overhead implementation complexity aggregate partitioned algorithms strive set relative size partition balance work devices define work balanced cumulative delay device period time equal balance work size partition reflects relative cost blocks simple efficient manner storage system slow disk fast disk cache divided partitions slow disk receiving larger partition describe precisely relative sizes configured section choose victim block storage-aware algorithm selects victim partition victim block partition victim partition chosen resulting size relative partitions maintains desired proportions individual victim partition selected replacement algorithm lru lfu fifo distinctions prior work virtual memory systems noted unified partitioned virtual memory systems traditional sense partitioned virtual memory systems distinguish file system pages virtual memory pages managed separately storage-aware algorithms explicitly distinguish file system pages virtual memory pages order balance work algorithms distinguish pages based device supplied page additionally storage-aware caching algorithms change size partitions dynamically partitioned virtual memory systems change size file system cache virtual memory partitions local global page replacement local page replacement eviction time considers processes isolation global page replacement applies replacement processes storage-aware algorithms make per-partition replacement decisions similar traditional notion local page replacement decisions based cost locality solely locality local page replacement schemes taxonomy aggregate partitioned algorithms work investigate taxonomy aggregate partitioned algorithms show dynamic aggregate partitioning needed taxonomy subsection basic approaches aggregate partitioning static dynamic static scheme ratio partitions selected one-time notion costs knowledge workload resulting miss rates cache size priori determine relative sizes lead balanced work dynamic partitioning needed ratio partition sizes adjusts requests monitored dynamic partitioning benefits dynamic partitioning adjust dynamic performance variations faults common modern devices dynamic partitioning react contention devices due hotspots workloads finally dynamic partitioning compensate fact performance ratios devices portion test performance reduced slightly cleaner begins replicating data write test completes cleaner continues replicate data background rations set significantly performance characteristics relax placement restrictions choosing disks constitute set per-file basis problem fundamental approach file-system management redundancy implementation standpoint file-system managed redundancy problematic vnode layer designed single underlying disk mind recursive invocation technique successful stretched limits current framework additions modifications code straightforward implement support file-system managed redundancy redesign vnode layer beneficial future work number avenues exist future research generally organizations storage protocol stack explored pieces functionality implemented trade-offs natural followon incorporate lower-level information raid main challenge exposing information file system find pieces information file system readily exploit file service today spans client server machines important functionality split machines portion traditional storage protocol stack reside clients portion reside servers researchers distributed file systems opposing points view systems zebra xfs letting clients work frangipani petal system places functionality storage servers cooperative approaches file system storage system found implementing redundancy file system vexing approach shared responsibility redundancy file system storage layer improvement storage layer file system block mirror block file system decide perform replication decide storage interface difficult convince storage vendors move tried-and-true standard scsi interface storage pragmatic approach treat raid layer gray box inferring characteristics exploiting file system modification underlying raid layer tools automatically extract low-level information disk drives dixtrac skippy steps goal extensions needed understand parallel aspects storage systems finally envision optimizations arrangement storage protocol stack exploring notion intelligent reconstruction basic idea simple disk region fails lfs duplicated data disk lfs begin reconstruction process key difference lfs reconstruct live data disk entire disk blindly storage system substantially lowering time perform operation fringe benefit intelligent reconstruction lfs give preference files reconstructing higher-priorityfiles increasing availability files failure imagine optimizations lfs cleaner data laid disk current performance characteristics access patterns meet subsequent potentially non-sequential reads applications similarly disks added cleaner run order lay older data disks cleaner reorganize data drives read performance presence heterogeneity drives similar work neefe generalized operate heterogeneous multi-disk setting conclusions terms abstractions block-level storage systems scsi successful disks hide low-level details file systems exact mechanics arm movement head positioning export simple performance model file systems optimize lampson interface combine simplicity flexibility high performance solving problem leaving rest client early single-disk systems balance struck perfectly storage systems evolved single drive raid multiple disks interface remained simple raid result system full misinformation file system longer accurate model disk behavior now-complex storage system good understanding expect file system raid lfs bridge information gap design presence multiple regions exposed directly file system enabling functionality paper explored implementation on-line expansion dynamic parallelism flexible redundancy lazy mirroring lfs implemented straight-forward manner file system increasing system manageability performance functionality maintaining reasonable level system complexity aspects lfs difficult impossible build traditional storage protocol stack highlighting importance implementing functionality correct layer system chosen single point design space storage protocol stacks arrangements preferable hope explored conclusion research division labor file storage systems proper division arrived design implementation experimentation historical artifact acknowledgements shepherd elizabeth shriver john bent nathan burnett brian forney florentina popovici muthian sivathanu anonymous reviewers excellent feedback work sponsored nsf ccrccr- ngsccr- itrand wisconsin alumni research foundation timothy denehy sponsored ndseg fellowship department defense anderson dahlin neefe patterson wang serverless network file systems proceedings acm symposium change function access patterns dynamic partitioning divided eager partitioning 
lazy partitioning eager partitioning partition sizes desired algorithm immediately reallocates pages cost information algorithm lazy partitioning scheme gradually reallocates pages demand desired size response workload eager partitioning simplifies choosing victim partition location page cost removing pages conversely lazy partitioning scheme removes pages partitions needed partition lazy partitioning block replace block long blocks replaced proper frequency maintain desired partition size ratios replacement explicitly choose victim partition investigate strategy based inverse lottery previously proposed resource allocation idea partition number tickets inverse proportion desired size replacement needed lottery held selecting random ticket partition holding ticket picked victim victim valuable page lazy partitioning algorithm allocates page logical partition selecting partition sizes main challenge partitioned approaches determining relative sizes partitions configured storage-aware caching viewed performing selective filtering requests devices assuming slowest device limits system throughput goal storage-aware caching set partition sizes equal amount work device formally device number cache misses multiplied average cost miss equal algorithmic details basic approach dynamic repartitioning algorithm algorithm storage-aware cache observes amount work performed device fixed interval past predicts relative sizes partitions adjusted work equal algorithm work metric cumulative delay period time delay related total number requests includes request service time variation device devices algorithm streaming accesses fit cache problematic algorithm detect type access algorithm measures time spent waiting device past device requests window size records wait time device wait times approximately equal current partition ratios deemed adequate remain wait times size partitions large wait times increased size partitions small wait times decreased ratio wait times devices considered simultaneously selecting amount increment decrement partition non-trivial search problem change partition size affects future miss rate presence dynamically changing workloads initial approach employ simplest algorithm found meet challenge find algorithm adjusts partition sizes quickly find proportions quickly algorithm overshoots correct proportions meet goals simultaneously approach aggressively increases size partition wait time device increasing reacts conservative manner algorithm makes observations wait time device epoch action based observation epoch begins device requests complete epochs cache repartitioned repartitioning occurs steps algorithm computes per-device wait time wait time devices epoch algorithm computes relative wait time partition dividing per-device wait time wait time algorithm determines partitions page consumers pages give consumer page consumers partitions relative wait time threshold threshold filter normal variations wait time due workload device characteristics page consumers found repartitioning stops epoch begins finally algorithm finds partitions below-average wait times called page suppliers reallocates pages consumers consumers reach desired size repartitioning cache algorithm classifies figure corrective actions repartitioning algorithm figure shows actions algorithm response states graph shows observation per-device wait time trend relative wait time time progresses dotted line shows wait time graph graphs actions state shown fixed clarity threshold constant multiplied wait time partition states corrective action change partition size states shown figure cool wait time threshold wait time normal operating regime corrective action needed cool partitions page suppliers page consumers warming wait time threshold increasing algorithm infers increasing wait time due workload device characteristics initially cache size increased pages base correction amount partition continues warm subsequent epochs increase cache size grows exponentially reclassification partition warming state restarts exponential correction cooling wait time threshold decreasing corrective action set epochs halted increase wait time partition started decline wait time algorithm acts conservatively cooling state change partition size aggressive approach continues increase cache size state over-correct unstable warm wait time threshold constant based experimental evidence partitions classified cool warming cooling constant wait time occur operating systems principles sosp pages copper mountain resort december arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations proceedings acm sigmod conference management data sigmod pages tucson arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common workshop input output parallel distributed systems iopads atlanta arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance eighth workshop hot topics operating systems hotos viii pages schloss elmau germany bray bonnie file system benchmark http textuality bonnie brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference pages san 
diego june comer internetworking tcp vol principles protocols architecture prentice hall london edition cormen kotz integrating theory practice parallel file systems proceedings dags symposium dartmouth institute advanced graduate studies pages hanover june cortes labarta extending heterogeneity raid level proceedings usenix annual technical conference boston june jonge kaashoek hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles sosp pages asheville december engler kaashoek exterminate operating system abstractions workshop hot topics operating systems hotos orcas island english stepanov loge self-organizing disk controller proceedings usenix winter technical conference pages san francisco january gibson nagle amiri chang feinberg gobioff lee ozceri riedel rochberg zelenka file server scaling network-attached secure disks proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle june hartman ousterhout zebra striped network file system proceedings acm symposium operating systems principles sosp pages asheville december hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference berkeley january huber elford reed chien blumenthal ppfs high performance portable parallel file system proceedings acm international conference supercomputing pages barcelona spain july jacobson kill internet ftp ftp lbl gov talks vj-webflame kilburn edwards lanigan summer one-level storage system ire transactions electronic computers ecapril lampson hints computer system design proceedings acm symposium operating system principles pages bretton woods december acm lee thekkath petal distributed virtual disks proceedings seventh conference architectural support programming languages operating systems asplos vii pages cambridge october matthews roselli costello wang anderson improving performance log-structured file systems adaptive methods proceedings acm symposium operating systems principles sosp pages saint-malo france october mckusick joy leffler fabry fast file system unix acm transactionson computer systems august nieuwejaar kotz galley parallel file system proceedings acm international conference supercomputing pages philadelphia acm press norcutt iozone filesystem benchmark http iozone patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago june ritchie thompson unix time-sharing system comm assoc comp mach july roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference pages san diego june roselli matthews anderson file system fingerprinting works-in-progress symposium operating systems design implementation osdi february rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february rosenthal evolving vnode interface proceedings usenix summer technical conference pages anaheim santos muntz performance analysis rio multimedia storage system heterogeneous disk configurations acm multimedia december santry feeley hutchinson veitch carton ofir deciding forget elephant file system proceedings acm symposium operating systems principles sosp pages kiawah island resort december savage wilkes afraid frequently redundant array independent disks proceedings usenix technical conference pages san diego january schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler griffin lumb ganger trackaligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey january seltzer bostic mckusick staelin implementation log-structured file system unix proceedings usenix winter technical conference pages san diego january seltzer smith balakrishnan chang mcmains padmanabhan file system logging versus clustering performance comparison proceedings usenix annual technical conference pages orleans january seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems proceedings usenix annual technical conference pages san diego june stodolsky holland courtright gibson parity-logging disk arrays acm transactions computer systems august sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference san diego january talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley teigland pool driver volume driver sans master thesis minnesota december teigland mauelshagen volume managers linux freenix track usenix annual technical conference boston june thekkath mann lee frangipani scalable distributed file system proceedings acm symposium operating systems principles sosp pages saint-malo france october http fsprogs sourceforge net ext html june van renesse masking overhead protocol layering proceedings acm sigcomm conference pages palo alto veritas http veritas june wang anderson patterson virtual logbased file systems programmable disk proceedings symposium operating systems design implementation osdi orleans february wilkes datamesh research project phase proceedings usenix file systems workshop pages wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february zimmermann ghandeharizadeh hera heterogeneous extension raid technical report usc-cs-tr southern california 
partition moves state small change partition size algorithm increases partition size step algorithm reallocates pages page suppliers page consumers algorithm biases collection pages partitions lowest relative wait times determine number pages removed page supplier algorithm computes inverse relative wait time irwt relative wait time partition sums inverse relative wait times finally number pages partition supply computed irwtj sum irwtsforsuppliers consumed pages note parameters algorithm window size threshold base increment amount set care large smooth wait time variations sample sufficient number requests determine accurately effect corrections found sufficient smoothing feedback small exponential correction found cache works practice threshold large filter normal device performance fluctuation seek time found detects wait time warrant correction fixed algorithm compute threshold dynamically statistical variance wait times sum variance threshold discuss adaptive approach plan investigate future modifying existing replacement algorithms desire cost-aware cache performs easily implemented modern operating systems attention paid make computationally efficient landlord priority queue efficiently find lowest cost object mesh common virtual memory hardware easy combine landlord existing code base modern operating systems including solaris variant clock page replacement policy unified page cache desire algorithm incorporated easily clock structure introduce extension clock takes partitions account partitioned-clock base algorithm partitioned-clock assumes page bit set page referenced victim page needed clock arm successive pages bit set clearing bits sweeps partitioned-clock page tracks partition number belongs page selected victim bit cleared partition number match partition number chosen replacement chosen lottery note additional bits single bit consistent variations clock examine dirty bits history multiple bits optimizations improve performance partitioned-clock approximation lazy partitions searching replacement pages belonging victim partition bits cleared clearing bits pages unnecessarily removes usage history separate clock hand partition improves performance helps maintain usage history partition previously lazy partitions simpler implement eager partitions ratio sizes dynamic focus clock algorithm applied lazy partitions version termed lazy clock inverse lottery scheduling pick victims partitions evaluation environment section describes methodology evaluating storage-aware caching specifically overview simulator describes simulated storage environments simulator developed trace-driven storage-system simulator study behavior storage-aware caching configured simulated environment single client connected sixteen network-attached storage devices simulator explore performance impact client workloads data layout caching algorithms network characteristics disk characteristics storage-system heterogeneity simulator driven workload client trace file trace file represents data block requests striped raidacross full set disks request specifies starting offset length data read write simulate system high demand closed workload model completion disk request immediately triggers request client local cache replacement policies focus investigation model time cache hit small real system dwarfed cost remote-block access time cache miss sum network transit time remote disk service time trace trace trace request distribution uniform exponential exponential disk distribution uniform uniform gaussian locality random random random request size 
working set size requests table characteristics synthetic traces table summarizes synthetic traces set experiments gaussian distribution disk standard deviation storage device model roughly matches ruemmler wilkes model cylinders seek time rotational delay bandwidth calculating transfer time request specifically disk request falls cylinder previous request model sequential seek rotational delay set transfer time determined bandwidth nonsequential requests rotational delay chosen uniformly random full rotation time seek time non-linear model depends cylinder distance current request previous request network model based loggp endpoint contention loggp designed model communication large parallel computers depends parameters message latency network endpoint overhead minimum time message sends seconds byte network number endpoints workloads fully understand impact storage-aware caching algorithms study sets workloads variety synthetic traces web server trace collected analyzed roselli simply refer roselli web server trace roselli trace synthetic workloads control request size distributions working set size locality distributions distribution request disks synthetic traces summarized table traces read traces variety request sizes stress small large read requests trace adds request imbalance disks roselli trace image server california berkeley january age bandwidth seek rotation years avg avg table aging ibm lzx model bandwidth seek rotation time family disks based ibm lzx manufactured progressively older years assume bandwidth improves factor year seek rotation time year image server ran web server postgres database stored images trace alternates large reads files database tables small reads writes storage-system characteristics goals configuring set disks simulated environment goal understand full sensitivity storage-aware caching algorithms device heterogeneity requires diverse range configurations goal understand algorithms perform realistic scenarios requires focused set tests meet goals simultaneously employ device aging performance-fault injection idea device aging choose base device case ibm lzx age performance range years collections disks create configurations key performance base disk scaled fixed amount component bandwidth seek rotation time scaled expected yearly improvement historical data suggests year improvement bandwidth roughly year reduction seek time rotational latency realistic aggressive side table shows performance characteristics aged devices experiments note progressively older disks backwards aging newer disks based current year similar manner forward aging performance-fault injection dynamically throughput age slow disk years slow disk lru clock cache throughput number slow disks multiple slow disks lru clock cache figure performance lru clock caching figures show throughput storage system trace figure left varies age single disk x-axis figure increases number year disks system change performance drive experiment earlier represent disk stuttering absolute failure unexpected network traffic client drive sudden workload imbalance environment configuration section describes details simulator configuration configure network bottleneck system choose parameters similar ethernet set bandwidth future hope investigate network performance caching interact distributed storage systems configured simulator separately synthetic roselli traces synthetic traces choose sufficient number requests mitigate effects cold-start misses set client cache size roselli trace set cache size hit rate hit rate high requests disks heterogeneity device performance issue traces include disk layout file path information created simple layout policy layout policy assumes raidstriping policy lays blocks order access aged disks scenarios scenarios represent cases storage-system incrementally updated newer faster devices added time scenario single heterogeneous disk performance aged entire range years scenario groups heterogeneous disks group age age years relative size groups varied scenarios cover real-world situations provide insight common configurations scenario mimics stuttering disks increased workloads clients scenario closely incremental upgrades disk array incremental upgrades occur due cost constraints prohibit replacement entire array small number disks fail experiments section presents progression experiments demonstrating effectiveness storage-aware caching begin motivating cost-aware caching algorithms heterogeneous devices show partitioned approaches mask performance differences configuring fixed partition ratios correctly difficult static environment demonstrate mask performance heterogeneity adjusting ratio partition sizes on-line observations amount work performed device finally show partitioned approaches easily incorporated operating system replacement policies perform explore performance robustness trace web server motivating set experiments motivates storage-aware caching algorithms storage system heterogeneous devices figure shows throughput obtained trace common replacement policies cost-aware lru throughput age slow disk years static lru cache figure potential partitioned approaches slow disk aged shown x-axis approaches cache lru caching static partitioning cache disk performance trace figure clock caching graph left illustrates disks aged seek rotation time bandwidth decline throughput system drops dramatically performance benefit file cache decreases lru replacement throughput drops disks equally fast single disk performance -year disk similarly graph shows entire storage system runs rate slowest disk system throughput slow disk fast disks poor slow disks contrast storage-aware caching algorithm mask performance slow disks allocating cache slow disks slow disk fewer requests handle harm performance system dramatically configuring partition sizes set experiments show partitioned caching algorithms potential mask heterogeneous performance care selecting ratio partition sizes begin examining static partitioning algorithm simply named static figure show performance static trace experiments ratio partition sizes statically set directly proportional ratio expected service time disk trace request size directly compute expected transfer time disk function seek time rotation delay peak bandwidth graph static throughput partition ratio trace figure sensitivity static partitioning partition ratios workload graph shows performance workloads run disks twoyear disk experiment varies ratio slow disk partition fast disk partitions x-axis lower lines requests vary ratio requests slow disk versus ratio partition strategy significantly improves performance relative cost-oblivious algorithms lru static performs priori request size per-disk miss rate function cache size real world information advance difficulty correctly configuring static partition strategy illustrated figure experiment examine single drives disks deliver sequential reads disk delivered lesser-performing disk times block faults devices author hypothesizes scsi bad-block remappings transparent users file systems culprit bad-block remapping technique early operating systems univac series record tracks disk faulty avoid subsequent writes disk timeouts disks tend exhibit sporadic failures study -disk farm -month period reveals largest source errors system scsi timeouts parity problems scsi timeouts parity errors make errors network errors removed figure rises error instances examining data ascertain timeout parity error occurs roughly times day average errors lead scsi bus resets affecting performance disks degraded scsi chain similarly intermittent disk failures encountered bolosky noticed disks video file server off-line random intervals short periods time apparently due thermal recalibrations geometry previous discussions focus performance fluctuations devices performance differential present single disk documented disks multiple zones performance zones differing factor static examples disks treated identically disks layouts performance characteristics unknown careful research uncover performance problems work external sorting rivera chien encounter disk performance irregularities machines cluster tested revealed slower performance excluded subsequent experiments study ibm vesta parallel file system reveals results shown measurements obtained typically unloaded system cases small variance measurements cases variance significant cases typically cluster measurements gave near-peak results measurements spread widely low peak performance network switches deadlock switches complex internal mechanisms problematic performance behavior author describes recurring network deadlock myrinet switch deadlock results structure communication software waiting long packets form logical message deadlockdetection hardware triggers begins deadlock recovery process halting switch traffic seconds unfairness switches behave unfairly high load load myrinet switch routes receive preference result nodes disfavored links slower sender fully capable receiving data link rate work unfairness resulted slowdown global adaptive data transfer flow control networks internal flowcontrol mechanisms lead unexpected performance problems brewer kuszmaul show effects slow receivers performance all-to-all transposes cmdata network study receiver falls messages accumulate network excessive 
network contention reducing transpose performance factor software unexpected performance arises due hardware peculiars behavior important software agent common culprit operating system management decisions supporting complex abstractions lead unexpected performance surprises manner component exhibit poor performance occurs application time problem acute memory swaps data disk over-extended operating systems virtual machines page mapping chen bershad shown virtual-memory mapping decisions reduce application performance virtually machines today physical addresses cache tag cache small page offset cache tag allocation pages memory affect cache-miss rate file layout simple experiment demonstrates file system layout lead non-identical performance identical disks file systems sequential file read performance aged file systems varies factor file systems empty file systems recreated afresh sequential file read performance identical drives cluster background operations work fault-tolerant distributed hash table gribble find untimely garbage collection node fall mirror replicated update result machine over-saturates bottleneck background operations common systems including cleaners log-structured file systems salvagers heuristically repair inconsistencies databases interference applications memory bank conflicts work scalar-vector memory interference authors show perturbations vector stream reduce memory system efficiency factor memory hogs recent paper brown mowry show effect out-of-core application interactive jobs response time interactive job shown times worse competing memory-intensive process memory resources cpu hogs similarly interference cpu resources leads unexpected slowdowns sorting study performance now-sort sensitive disturbances requires dedicated system achieve peak results node excess cpu load reduces global sorting performance factor summary documented cases components exhibit unexpected performance hardware software components increase complexity perform internal error correction fault masking performance characteristics depending load usage behave nondeterministically note short-term performance fluctuations occur randomly components harmful slowdowns long-lived occur subset components types faults handled traditional methods incorporated model component behavior fail-stutter fault tolerance section discuss topics central fail-stutter model fully formalized model outline number issues resolved order storage configuration slow disk years older disks x-axis graph varies ratio partition sizes slow disk disks system greater slow disk correspondingly larger partition lines graph correspond workloads optimal partition size ratios top line workload examined verify highest throughput workload approximately matches shown figure two-year disk lines distribution requests disks changed slow disk receives ten times requests disks graph shows workloads optimal partition ratio ratio top workload ratio bottom workload performance workload varies greatly partition ratio performance workload varies throughput age slow disk years trace lazy lru eager lru landlord lru cache throughput age slow disk years trace lazy lru eager lru landlord lru cache figure dynamic partitioning algorithms figure shows performance eager lru lazy lru landlord disk aged left graph trace graph trace approach select partition ratios dynamically 
function workload disk performance partitioning balance work set experiments shows dynamically adjusting size partition algorithms balance amount work performed disk effectively hide heterogeneity classes dynamic partitioning eager partitioning lazy partitioning lazy partitioning inverse lottery scheduling pick victim partitions replacement time eager lazy lru partitions simplicity refer approach eager lru approach lazy lru experiments investigate trace trace realistic evaluation continuing understood workload parameters figure compares performance eager lru lazy lru storage-aware algorithms lru landlord left-most graph figure examine workload uniform number requests disks setup throughput lru degrades dramatically performance slow disk aged specifically throughput drops approximately eager lru lazy lru maintain throughput system slow disk aged specifically performance algorithms similar lru disks speed ten-year-old disk mask impact slow gathering disk throughput creating communities grid graph douglas thain figure john shows bent challenges andrea arpaci-dusseau non-uniform remzi number arpaci-dusseau requests miron disks livny computer interestingly sciences department disks wisconsin identical madison west dayton cost-aware street algorithms madison perform abstract lru grid applications workload demanding popular disks suffer schedulers contention bring jobs queueing data delays close make blocks proximity order disks satisfy costly throughput fetch scalability monitoring policy replacement cost requirements cost-aware systems algorithms devote accomplish making cache jobs popular data disks mobile propose balance system load jobs data disks meet binding previous execution workload storage performance sites benefits cost-aware caching communities improve participate disks wide-area aged system -year relationships disk eager participants lru community lru differ expressed factor classad framework comparing extensions performance framework eager lru lazy community lru members express landlord indirect sees relations demonstrate performance implementation algorithms communities similar improving identical performance graph key high-energy shows physics simulation difference eager lru international distributed performance system robust introduction lazy grid lru applications landlord demanding lazy lru devotes applications entire cache fields slow high-energy disk eager physics lru continues high-throughput allocate access small wide amount selection cache data files chosen fast disks repositories measured repartitioning petabytes eager due lru aggravates large efforts number find users good size partition size data trace distances clock-based involved replacement online noted access data section repositories operating scalable systems efficient clock large algorithm numbers clock jobs systems cost-aware solve problems section generally evaluate fallen lazy camps partitioned move algorithm called data lazy clock job practical move virtual memory job page replacement data algorithm approaches traces universally applicable compare lazy suffer clock clock scalability experimental problem results network found storage capacities figures limits figure number lazy replicas clock performs made desired lazy clock number greater jobs proportion cache replica slower propose devices balance devices shown requests figure lazy local clock area execution mask sites band performance differences communities speed share slow research disk degrades supported part significantly nsf contracts imbalanced workload lazy itrand clock begins eiaand nasa throughput arc approximately contract ncc disks identical degrades slow disk full -years older throughput age slow disk years trace lazy clock clock cache throughput age slow disk years trace lazy clock clock cache figure clock-based replacement algorithms figure shows performance lazy clock clock disk aged workloads investigated figure throughput number slow disks trace lazy clock clock cache figure clock-based multiple disks figure shows performance lazy clock clock number disks age years increased trace workload left graph figure throughput compares favorably landlord lazy lru figure figure shows lazy clock gracefully masks increasing number two-year disks clock affected heterogeneity performance lazy clock slowly degrades performance match clock system homogeneous two-year disks performance fairly close experience shown smaller base correction size devices cover discuss potential benefits utilizing fail-stutter model fail-stutter model discuss issues central developing failstutter model focus main differences fail-stop model separation performance faults correctness faults notification components presence performance fault system performance specifications component separation performance faults correctness faults fail-stutter model distinguish classes faults absolute correctness faults performance faults scenarios manner deal correctness faults total disk processor failure utilize fail-stop model schneider considers component faulty behavior longer consistent specification response correctness failure component state permits components detect failure component stops operating addition fail-stutter model incorporate notion performance failure combined completes fail-stutter model component considered performance-faulty absolutely failed defined performance performance specification separation performance correctness faults crucial model gained utilizing performance-faulty components cases devices perform large fraction expected rate components behave treating absolutely failed components leads large waste system resources difficulty addressed occurs component responds arbitrarily slowly request case performance fault blurred correctness fault distinguish cases model include performance threshold definition correctness fault disk request takes longer seconds service absolutely failed performance faults fill rest regime device working notification components major departure fail-stop model components informed performance failures occur reasons erratic performance occur frequently distributing information overly expensive performance failure perspective component manifest failure caused bad network link component persistently performance-faulty system export information component performance state allowing agents system readily learn react performance-faulty constituents performance specifications difficulty arises defining fail-stutter model arriving performance specification components system ideally fail-stutter model present system designer trade-off extreme model component performance simple disk delivers bandwidth simpler model performance faults occur performance deviates expected level assumptions made system designer allowed flexibility drawing attention fact devices perform expected designer good model performance faults occur long environment component specific strongly influence system built react failures sketch fail-stutter model employed simple assumptions performance faults specifically scenarios order increasingly realistic performance assumptions omit details complete designs hope illustrate fail-stutter model utilized enable robust system construction assume workload consists writing data blocks parallel set disks data encoded disks raidfashion pair disks treated raidmirrored pair data blocks striped mirrors raidin scenario fail-stop model assuming naively performance faults occur absolute failures accounted handled absolute failure occurs single disk detected operation continues reconstruction initiated hot spare disks mirror-pair fail operation halted performance faults considered design pair disk number blocks write performance fault occurs pairs time write storage determined slow pair assuming disk-pairs write disk-pair write perceived throughput reduced scenario addition absolute faults performance faults static nature assume performance mirror-pair stable time uniform disks design compensate difference option gauge performance disk installation ratios stripe data proportionally mirror-pairs pair disks perform similarly rate mirror determined rate slowest disk single slow disk system correctly gauges performance write throughput increases disk perform expected time performance tracks slow disk finally scenario general performance faults include disks perform arbitrary rates time design option continually gauge performance write blocks mirrorpairs proportion current rates note approach increases amount bookkeeping proportions change time controller record block written increasing complexity create system robust deliver full bandwidth wide range performance faults benefits fail-stutter important consideration introducing model component behavior effect systems utilized model systems reliable manageable systems built tolerate fail-stop failures manageability manageability fail-stutter fault tolerant system fail-stop system reasons fail-stutter fault tolerance enables true plug-and-play system administrator adds component system performance additional involvement operator true futz system system incrementally grown allowing newer faster components added adding faster components incrementally scale system handled naturally older components simply performance-faulty versions administrators longer stockpile components anticipation discontinuation finally workloads imbalances bring introduced system fear imbalances handled performance-fault tolerance mechanisms cases human intervention reduced increasing manageability van jacobson experience shows configured misconfigured removing intricate tuning problems caused misconfiguration eradicated availability gray reuter define availability fraction offered load processed acceptable response times system utilizes fail-stop model deliver poor performance single performance failure performance meet threshold availability decreases contrast system takes performance failures account deliver consistent high performance increasing availability reliability fail-stutter model improve system reliability ways design diversity desirable property large-scale systems including components makes manufacturers problems occur collection identical components suffer identical design flaw avoided gray reuter state design diversity akin belt suspenders belts suspenders system handles performance faults naturally works parts reliability enhanced detection performance anomalies erratic performance early indicator impending failure related work experience o-intensive application programming clusters convinced erratic performance norm large-scale systems system support building robust programs needed began work river programming environment mechanisms 
enable consistent high performance spite erratic performance underlying components focusing disks river handle absolute correctness faults integrated fashion relying retry-after-failure checkpointrestart package river requires applications completely rewritten enable performance robustness situations researchers realized model fault behavior simple fail-stop earliest aware shasha turek work slow-down failures authors design algorithm runs transactions correctly presence failures simply issuing processes work reconciling properly avoid work replication authors assume behavior occur due network congestion processes slowed workload interference assume fail-stop model disks dewitt gray label periodic performance fluctuations hardware interference characterize nature problems realize potential impact parallel operations birman recent work bimodal multicast addresses issue nodes stutter context multicast-based applications birman solution change semantics multicast absolute delivery requirements probabilistic gracefully degrade nodes begin perform poorly networking literature replete examples adaptation design variable performance prime tcp similar techniques employed development adaptive fail-stutter fault-tolerant algorithms conclusions systems built assuming components identical component behavior static unchanging nature component works assumptions dangerous increasing complexity computer systems hints future components behave differently behave dynamic oft-changing large range normal operation falls binary extremes working working utilizing fail-stutter model systems manageable reliable work deployed real world challenges remain fail-stutter model formalized models component behavior developed requiring measurement existing systems analytical development adaptive algorithms cope difficult class failures designed analyzed implemented tested true costs building system discerned approaches evaluated step direction exploring construction fail-stutter-tolerant storage wisconsin network disks wind project investigating adaptive software techniques central building robust manageable storage systems encourage fail-stutter model endeavors acknowledgements people comments earlier versions paper david patterson jim gray david culler joseph hellerstein eric anderson noah treuhaft john bent tim denehy brian forney florentina popovici muthian sivathanu anonymous reviewers thoughtful suggestions work sponsored nsf ccrand nsf ccrreferences acharya uysal saltz active disks asplos viii san jose oct arpaci dusseau vahdat process management network workstations http berkeley remzi -final arpaci-dusseau arpaci-dusseau wisconsin network disks project http wisc wind arpaci-dusseau arpaci-dusseau bent forney muthukrishnan popovici zaki manageable storage adaptation wind ieee int symposium cluster computing grid ccgrid arpaci-dusseau arpaci-dusseau culler hellerstein patterson searching sorting record experiences tuning now-sort spdt aug arpaci-dusseau performance availability networks workstations phd thesis california berkeley arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common iopads birman hayden ozkasap xiao bidiu minsky bimodal multicast tocs bolosky iii draves fitzgerald gibson jones levi myhrvold rashid tiger video fileserver technical report microsoft research bressoud schneider hypervisor-based fault tolerance sosp dec brewer inktomi web search engine invited talk sigmod brewer kuszmaul good performance cmdata network proceedings international parallel processing symposium cancun mexico april brown mowry taming memory hogs compiler-inserted releases manage physical memory intelligently osdi san diego october chen permission make bershad digital impact hard copies operating system structure part memory work system performance personal proceedings classroom granted acm symposium fee provided operating systems copies principles made pages distributed profit december commercial advantage corbett copies feitelson bear vesta notice parallel file full system citation acm transactions page computer copy systems republish post august servers dewitt redistribute ghandeharizadeh lists requires schneider prior specific bricker permission hsaio fee rasmussen gamma november database denver machine copyright project ieee acm transactions acm knowledge data infn engineering figure march communities data dewitt locally-determined gray physical parallel limits database systems future community high-performance hosts database storage systems appliance communications serve data locally acm june existing wide-area fox replication system gribble scheduler chawathe make brewer number gauthier informed choices cluster-based jobs scalable requesting network services data sosp moved pages communities saint-malo france staged oct data gray staged reuter community transaction processing job concepts techniques morgan balance kaufmann point gribble brewer fixed hellerstein culler homogeneous scalable distributed remove data structures discrepancy internet dynamic service construction performance evaluate osdi san tolerance diego performance faults show october intel intel partitioned pentium caching architecture algorithms product briefing react home page http relative developer intel performance design storage pentium devices prodbref cases january effectively jacobson landlord congestion avoidance experiments control begin proceedings acm cluster sigcomm homogeneous disks pages trace august inject jacobson performance fault kill internet disks ftp disk ftp lbl simulated gov time talks vjwebflame seconds approximately half kushman performance simulation nonmonotonocities performance case fault study effect ultrasparc slowing processor master disk thesis factor massachussets institute technology boston lamport shostak pease byzantine generals problem acm transactions programming languages systems july meter observing effects multi-zone disks proceedings usenix conference jan patterson anderson cardwell fromm keeton kozyrakis thomas yelick intelligent ram iram chips remember compute ieee international solid-state circuits conference san francisco february patterson gibson katz case redundant arrays inexpensive disks raid sigmod pages chicago june acm press raghavan hayes scalar-vector memory interference vector computers international conference parallel processing pages charles august rivera chien high speed disk-to-disk sort windows cluster running hpvm submitted pulication rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february satyanarayanan digest hotos vii http rice conferences hotos digest march schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december schneider personal communication february scott burkhart kumar blumberg ranson four-way superscalar pa-risc processors hewlettpackard journal august shasha turek fail-stop wait-free serializability resiliency presence slow-down failures technical report computer science department nyu september siewiorek swarz reliable computer systems design evaluation peters edition talagala patterson analysis error behaviour large storage system ipps workshop fault tolerance parallel distributed systems 
ratio supportable jobs replicas depends properties application data storage devices networks order structure communities participants express relationships relations direct job require machine cpu indirect job require machine storage device dataset demonstrate classad framework additions indirection express relationships communities traditionally constructed distributed filesystems require special privileges deploy configure present building blocks permit construction communities unprivileged user-level software building blocks communicate state device discovery system discovery system replica short-haul appliance storage long-haul execution site job agent system cpu information placement distributed repository discovery figure model condor distributed batch system places jobs appropriately address matter selecting policies communities policies govern size communities contents storage devices decisions relocate jobs place policies wholly dependent particulars individual applications physical network capacity number cpus hope work mechanisms enable study policies demonstrate feasibility model applying key high-energy physics simulation run international grid constructing communities policies knowledge application demonstrate marked improvement simulation capacity communities building communities community consists cpus gather storage device programs executing cpus encouraged required community device storing retrieving data sharing device similar applications reduce consumption widearea resources communities reflect physical administrative boundaries number cpus effectively served storage device limited connecting network load offered running programs users admitted community depend membership social structures familiar form community distributed file system shared members workgroup sort community semi-permanent require special privileges coordinated software participants contrast services offered grid intended flexible users applications loads change communities set reconfigured torn permit agile deployment communities constructed building blocks applied normal users special privileges accomplish employ storage appliances interposition agents shown figure storage appliance serves meeting place community storage appliance frequently conceived specialized hardware device general-purpose computer equipped software serve equally storage appliance appliance speaks number protocols members community select protocol situation application selecting elements database fine-grained block-access protocol conversely application processing large amounts sequential data choose streaming protocol standard applications rarely speak protocols brave users rewrite applications work systems exists large body programs rewritten problem solved interposition agents agent small piece software inserts application native operating system agent responsible converting program standard operations suitable actions community agent place unmodified applications run grid environment discovering communities computational grid community resources change warning environment programs rich methods finding communities meet determine membership community execution find resources community order bring inside actions form discovery refer cpu discovery device discovery replica discovery figure shows form discovery fits community execution cpu discovery performed find cpu proper architecture operating system execution device discovery performed find membership community execution replica discovery performed locate items job community replica discovery important area research recent grid efforts device discovery closely related subtly frequently employed selfconfiguring systems jini locate storage human-interface devices mobile software hardware briefly comment difference replica discovery answers question data local storage graph figure show eager lru adjusts partition ratios change performance partitions initially equal performance fault window disk requests passed observation algorithm observes waiting time disk significantly higher average waiting time partition disk increased small amount partitions disks decreased number cache entries algorithm continues measuring wait time disk increasing partition size disk wait times approximately equal time-line shows correct partition ratio found quickly graph figure summarize results comparing performance landlord eager lru clock lazy clock plot throughput workload performance fault occurs disk disks disks simultaneously number affected disks small relative total number disks system aggregating replacement-cost information beneficial specifically eager lru achieves throughput approximately performance fault landlord maintains lazy clock performs finally costoblivious clock algorithm performs number faulty disks increases partition ratio simulation time seconds single fault injection disk fault injected disks eager lru landlord lazy clock figure performance dynamic faults cluster initially homogeneous disks performance fault time seconds slows disk factor graph left performance fault occurs single disk show partition sizes chosen eager lru algorithm graph performance fault occurs disks cases trace throughput age slow disk years lru-based algorithms lru landlord eager lru lazy lru cache throughput age slow disk years clock-based algorithms clock lazy clock cache figure web server workload figure shows performance lruand clock-based algorithms run file system trace web server providing art images results lru-based algorithms graph left side clock-based algorithms graph results change eager lru mask performance faults landlord groups disks similar performance characteristics correction algorithm detect severity heterogeneity making smaller corrections needed adaptive threshold real-world performance conclude experiments examination partitioned algorithms web workload web server received modest number requests trace shorter synthetic traces partitioned algorithms partially penalized shortness trace move partition sizes initial state similar experiments investigate aging single disk results shown figure expected cost-oblivious algorithms show sharp drop performance age slow disk increases lru performance falls peak performance age range realistic range falls aged years clock shows similar slightly dramatic decline expected adaptive algorithms show robust performance performance degrades lazy lru eager lru landlord lazy lru performs poorly eager lru landlord lazy lru poor interaction repartitioning algorithm devotes entire cache slow disk disks homogeneous eager lru distributes pages evenly pages gradually distribution disk ages lazy clock shows small decrease performance sharp drop slow disk years drop due significant change partition ratios lazy clock strongly favors slow disk age weak preference age performance landlord decreases age increases trace bimodal distribution request sizes due postgres table reads interleaved small web server reads writes introduce anomalous behavior based experiments shown due space constraints performance dynamic partitioning algorithms sensitive base correction amount experiment figure fixed base correction amount range slow disk ages base correction amount starts small increases disk ages performance improves matches base cost-oblivious algorithm homogeneous system adaptive base correction amount needed performance related work work cost-aware caching occurred web cache database communities web cache community extensively studied cost-aware caching addition document size included algorithms web caching work differs storage-aware caching ways performance wide area varies common storage systems web caching document caching differs fixed-size blocks storage systems finally web caching replacement cost web page strongly correlated replacement costs pages broadcast disks continuously deliver data clients asymmetric link broadcast schedule meet client client met broadcast schedule client cache strives manage cache contents mask non-ideal broadcast schedule knowledge broadcast schedule probability access cache manages contents algorithm generalizes lru storage-aware caching differs ways partitions cache 
pages device broadcast disks page device granularity track replacement costs broadcast disks assume infrequently changing broadcast schedule storage-aware caching react frequent workload device performance recently researchers studied allocation pages classes prefetching compiler-controlled memory management resizeable file buffer caches prefetching page allocation occurs applications hinted unhinted compiler-controlled memory management compiler application memory usage replica management systems track copies datasets spread storage devices grid dataset retrieved replica management system finds suitable remote copy requestor device discovery answers question local storage device executing jobs discover device offer bandwidth storage space inputs outputs temporary files replica discovery system locate remote data device discovery system locate place put incoming data caller members community find device discovery systems complex define communities simply giving execution site neareststorage propery points storage appliance approach simple effective complex systems imagined execution site storage appliances policy restrictions device access members administrative group case device discovery system query devices return nearest device accepts job indirection critical feature discovery system addition querying direct properties devices user request chain relations user request cpu storage device dataset user storage devices request set cpus set situation change user knowledge user submit request expressing chain indirection language needed express relations scheduling policy management systems concrete represent properties requirements preferences involved community classad language uniquely suited task expressing communities classads classads condor system describe properties requirements preferences participants distributed computing system classads named classified advertisements found newspapers multiple parties publish requests service offers serve well-known place single classad list attribute pairs values simple atoms strings integers complex expressions referring potential matches figures shows job machine represented language describes simple properties machine mentions cpu operating system job mentions executable owner requirements potential match machine accept jobs owned user job accept machines running correct operating system unlike newspaper condor central matchmaking service pairs offers requests suitable match found parties informed individually responsible contacting accomplishing work process bi-lateral matchmaking extensively raman build communities add participant match storage appliance shown figure incoming job requests cpu places indirect requirements storage classad representing cpu decides storage referenced figure shows job specifies indirect requirements field states accept job neareststorage hascmsdata true neareststorage evaluated context potential cpu cpu simply point nearest storage device address unique classads schemafree single distinct adad cpujob neareststorage requirements 
lookup matchmaker cpujob storage store match figure matchmaking type job targettype machine cmd sim exe owner thain requirements opsys linux neareststorage hascmsdata figure job classad type machine targettype job raven opsys linux requirements owner thain neareststorage turkey type storage figure machine classad type storage turkey hascmsdata true cmsdatapath cmsdata figure storage classad neareststorage property set constraints identify unique storage classad shown figure rest expression evaluated context referred-to classad contained job evaluates neareststorage hascmsdata true neareststorage cmspath cmsdata neareststorage turkey contents storage appliance change simply sends updated state matchmaker dataset added device jobs require match community dataset removed jobs longer match information matchmaker necessarily stale state cpu storage appliance change match made sides responsibility verifying requirements satisfied claiming protocol successful match policies adding level indirection job storage user freed jobs run user simply state needed order execute jobs state storage devices jobs run user policy policies expressed submit time classad language job boolean requirements expression determines machines suitable execution sites evaluates true execution site accepted rejected integer expression rank potential matches machines requirements evaluates true machine highest rank chosen expressions control jobs move data wait arrive user job move site dataset express requirements neareststorage hascmsdata information operating system global replacement policies hints reintegrating elements local page replacement global page replacement finally nelson work resizeable file buffer caches evaluates tradeoff file buffer caches virtual memory system loaded work areas closely related work directly address storage device heterogeneity future work storage-aware caching increases storage system performance robustness adapting performance differences devices areas improvement application domain addition study real implementation partitioning algorithm presented significant limitations sophisticated informed cost-benefit algorithms limitations linear relationship assumption cache size hit rate reliance proper values window size base increment amount threshold limitation evident considers access patterns locality working set larger cache intuitively algorithm recognize instances increasing cache size decrease wait time general framework storageaware caching existing cost-oblivious policies manage individual partitions studied framework approach modularity primary strength existing non-cache aware policies lru clock mru eelru minimal effort work concentrated non-cooperative client caching combination cooperative caching cost-aware caching lead performance robustness disk arrays individual cache sizes small fourth storage-aware caching applied lowpower environments storage-aware caching extended include power retrieval cost devices higher power accessed frequently stay low-power mode longer frequently finally caching algorithms affected prefetching layout decisions explore advantages tradeoffs integrated prefetching layout caching decisions light device heterogeneity previous caching prefetching work homogeneous environments shown benefits integration benefit extends heterogenous environments conclusions diverse characteristics modern storage devices time ripe re-investigate caching algorithms optimize performance task costaware cache control blocks cached amount work performed storage device roughly equal paper presented family cost-aware caching algorithms based notion explicitly partitioning cache size partition configured directly corresponds relative cost usefulness data partition approaches advantages partitions aggregate replacement-cost information entries cache reducing amount information tracked allowing recent cost information blocks device important virtual partition approach easily implemented clock replacement policy increasing likelihood adoption real systems acknowledgements members wisconsin network disks research group helpful discussions contributions paper comments florentina popovici provided ibm lzx disk profile information simulations leslie cheung wrote layout code berkeley trace experiments omer zaki contributed early versions work comments anonymous reviewers tireless shepherding jeff chase greatly improved quality paper condor distributed execution system run simulations members condor project todd tannenbaum supported condor work sponsored nsf ccrccr- itrand wisconsin alumni research foundation acharya alonso franklin zdonik broadcast disks data management asymmetric communications environments proceedings acm sigmod international conference management data san jose pages acm press acharya franklin zdonik disseminationbased data delivery broadcast disks ieee personal communications december alexandrov ionescu schauser scheiman loggp incorporating long messages logp model step closer realistic model parallel computation papers annual acm symposium parallel algorithms architectures santa barbara june pages acm press apple computer corporation idisk http itools mac arpaci-dusseau arpaci-dusseau bent forney muthukrishnan popovici zaki manageable storage adaptation wind proceedings ieee int symposium cluster computing grid ccgrid pages arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance workshop hot topics operating systems hotos schloss elmau germany babaoglu joy converting swap-based system paging architecture lacking page-referenced bits proceedings acm symposium operating system principles pages pacific grove december bertoni understanding solaris filesystems paging technical report tr- sun microsystems cao felten karlin implementation performance integrated application-controlled file caching prefetching disk scheduling acm transactions computer systems november cao irani cost-aware proxy caching algorithms usenix symposium internet technologies systems proceedings monterey california december pages berkeley usa carley ganger nagle mems-based integrated-circuit mass-storage systems communications acm november chrobak karloff payne vishwanathan results server problems proceedings annual acmsiam symposium discrete algorithms soda pages san francisco usa jan cortes labarta extending heterogeneity raid level usenix annual technical conference june demke brown mowry taming memory hogs compiler-inserted releases manage physical memory intelligently proceedings symposium operating systems design implementation osdipages berkeley october grochowski ibm leadership disk stroage technology ibm corporation howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems february jin bestavros popularity-aware greedydual-size algorithms web access proceedings international conference distributed computing systems icdcs april kelly chan jamin mackie-mason biased replacement policies web caches differential quality-ofservice aggregate user fourth international web caching workshop san diego california march april kubiatowicz bindel eaton chen geels gummadi rhea weimer wells weatherspoon zhao oceanstore architecture global-scale persistent storage proceedings ninth hand user moving job expensive operation require stay community requirements neareststorage turkey wisc simply prefers run local community require ranks local community ten requirements neareststorage hascmsdata rank neareststorage turkey wisc execution site job system pfsmatchmaking attributeattribute chirp nest nearest hascmsdata neareststorage classad figure implementation machines machines user execute remote local copy eliminate requirements statement complicated information included expression set policy migration permitted job required execute community night network traffic lower requirements neareststorage turkey wisc clockhour clockhour implementation built prototype concepts condor distributed batch system condor cpu scheduling system classad framework interposition agent pluggable file system attach jobs local storage appliance implemented software called nest devices sufficiently general purpose put individually systems nest software creating general-purpose storage appliances commodity computers special privileges externally supports variety network protocols allowing applications choose interact storage made gridftp chirp strong authentication high-throughput transfers variety techniques multiple tcp streams native nest protocol simple rpc-like partial-file access single tcp connection gridftp lingua franca communicating grid services long-haul connections chirp short-haul partial-file access require overhead tcp connection data operation pfs interposition agent constructed bypass pfs adapts legacy applications storage systems mounting application view file system special privileges kernel-level required number standard network protocols including gridftp chirp supported pfs loaded unmodified unix programs interact nest running turkey wisc chirp turkey wisc file system cpu selection device parameters application condor classad property job execution site inserted program environment variables arguments run time macro-expanding expressions beginning dollar signs arguments chirp neareststorage input data condor understands executable consist single file submit pfs-enabled application condor resort trick submitting self-extracting archive application pfs script invoke properly level indirection step omitting application archive modifying script fetch executable community pfs technique retrieve common executable local appliance finally noted users classads prepared handle stale match suppose stale information job match community longer needed dataset pfs discover file found error performs nearest nest simply passing error application incorrect exit error message forcing user manually understand error resubmit job correct action pfs application exit abnormally kill process signal condor interprets signal execution aborted re-queue job execution attempt performance demonstrate implementation chosen examine simulation component cms experiment performed cern large experiment documented users italy united states make heavy application condor pools istituto nazionale fisica nucleare infn wisconsin began assuming role scientist infn wishes execute large number instances simulation infn pool equipped fair number cpus competition users pool limits thirty additional cpus leveraged explored deployment communities order solve problem application viewed perspective system cms simulation works reads input file instructions reads variety files database directory database provided application consists mixture input files data files libraries source files user conceivably determine exact set database files needed run simulation experience care citing cost analysis expensive dealing data files needed trivially predictable input sake application assume arbitrary simulation run access entire directory trimmed libraries source database yielding directory directories symbolic links files chose sample run simulation input file reads total input files database directory generates output files executable compresses network transfer mips machine local storage sample runs seconds simulation executable directly submitted system self-extracting archive pfs script submitted execution site script downloaded simulation executable storage appliance invoked arguments sample run representative real cms higher cpu ratio real run typically simulation runs hours minutes chosen shorter run reasons primarily push envelope system open condor applications greater demands secondarily consume excessive amounts resources allocated real simulations progress environment condor pools infn employed running simulations pool configured distinct community infn condor pool consisted cpus time processing power cpus ranged mips memory ranged cpus physically spread country departments institution workstation providing mips memory established bologna storage appliance infn community variety networks ranging connected execution sites storage appliance condor pool consisted cpus reserved reserved cpus provided mips memory identical machine established storage appliance community reserved machines connected appliance dedicated ethernet switch communities connected public internet bandwidth path varied latency measurements began assuming executables data files stored workstation infn workstation installed instance condor submitting jobs instance nest serve input data provide output space moving collection site site reproducing symbolic links important archive size swells jobs completed elapsed time infn local number cpus allocated elapsed time infn local figure infn community figure details execution batch jobs running infn community left-hand graph shows number jobs completed time progresses right-hand graph shows number cpus function time jobs completed elapsed time local stage remote number cpus allocated elapsed time local stage remote figure community figure details execution batch jobs running community left-hand graph shows number jobs completed time progresses right-hand graph shows number cpus function time line represents discipline note anomalies remote discipline elapsed times seconds artifact number machines bandwidth storage appliance shared fairly jobs finish approximately time delay incurred jobs start complete anomalies present disciplines pronounced due decreased time fetch executable input files jobs completed elapsed time infn local local infn local stage infn local remote number cpus allocated elapsed time infn local local infn local stage infn local 
remote figure communities figure details execution batch jobs running simultaneously communities lefthand graph shows number jobs completed time progresses right-hand graph shows number cpus function time line represents discipline anomalous behavior infn local remote lines due phenomenon figure infn local remote stage local infn local remote infn local stage infn local local time secs figure completion time graph shows execution time simulations configuration lower values turnaround time user perspective infn local remote stage local infn local remote infn local stage infn local local time secs figure ninety-five percent completion time graph shows percent completion time simulations configuration lower values response time user perspective infn local remote stage local infn local remote infn local stage infn local local time secs figure average cpu consumption graph shows average cpu time consumed jobs configuration lower values efficient resources system perspective benchmark capacity infn pool submitted simulation jobs completed shown figure run number machines fluctuates inevitable property distributed system involving hundreds users changing minds machines make steady progress roughly job seconds explored feasibility cpus cpus reserved submitted jobs pool configurations results shown figure case labelled remote jobs performed appliance infn larger number cpus run faster jobs constrained small bandwidth address situation deployed nest pool switch reserved cpus cpus updated advertise members community jobs submitted constraint run cms data machines immediately satisfied requirement satisfy manually staged data appliance instructed advertise contents procedure roughly seconds jobs run completed shown stage case accounting time transfer stage case marginally faster remote case future executions advantage already-transferred data run shown local case fourth configuration measured shown figure configuration combined stage remote models performed stage database concurrently allowing jobs execute remote idea staging operation complete jobs access data locally bandwidth remote server shared stage operation jobs jobs finished stage completed additionally due bandwidth contention jobs finished slowly remote configuration note local community executable database files fetched local storage appliance processspecific input files syncronoulsy fetched storage appliance submission site output files delivered finally made communities concert results shown figure infn local remote case jobs run communities performing appliance submitting host number cpus high bandwidth constraints limited performance infn local stage case jobs run infn community data staged stage completed end run whereopon large number cpus finished remaining jobs yielding increase performance finally data community run jobs match community shown infn local local case evaluation evaluate configurations points view users generally concerned completion time workload system operators generally concerned efficient resources consumed user perspective summarized figure graph shows completion time job configuration general applying cpus run yields faster results larger numbers cpus provide marginal improvement remotely localized yields faster results figures show configuration completes large fraction jobs quickly completion delayed small number jobs end execution compare configurations disregarding contributions long tail examining completion time ninety-five percent jobs shown figure examined jobs long tails discovered distinct sources delay cases jobs starved input phase enter computation phase competition decreased jobs late run evicted execution sites owners returning workstations jobs simply longer execution times due competition local users cpu memory network capacity problem starvation suggests examination fairness storage appliance problems difficult address eliminated tightly-controlled environment ever-present feature large-scale grid computing large computation performed resources partially shared receive interference performance users long tail prevented executing multiple copies jobs number outstanding number cpus operator perspective summarized figure graph shows average cpu consumption job configuration figure arrived dividing allocated cpu time number jobs run efficient configurations involve localized remote improvement completion time holds cpus idle waiting yielding poor efficiency performance configuration parameters runs stage cases provide improvement time transfer datasets execution time jobs performing remote details execution surprising figures remote cases incur dramatic delays frequency job completion drops drastically reflected drops cpu allocation cases occurred large numbers jobs previously contending completed examination submitter logfile shows condor start jobs quickly completed due overhead re-transferring self-extracting archive newly started job alleviated cache execution site major obstacle throughput application related work simple distributed systems make centralized server connect jobs data canonical network file system nfs analogue grid computing condor remote system call facility running job performs remote procedure calls back orginating computer central-server models limited scalability number clients limited aggregate bandwidth provided central server performance individual clients limited bandwidth latency network reliability system decreases number networks participants increases systems address difficulties copying data site job execution andrew file system afs scale larger client server ratio nfs analogue grid computing globus gass system files fetched distributed repositories stored locally longer referenced hierarchical data grids expand idea trees servers replicate data production site jobs data moved orthogonal question data located note system matches data jobs data type resource applications require discovery data arbitrary types resources replica management system track data locations storage resource broker srb pulls pieces provide coherent view multiple replication sites arrangement communities similar shared web proxy cache web clients fixed location option choosing proxy run large body research software general resource discovery projects include jini replica catalogs ldap snmp recent peer-to-peer file sharing protocols napster gnutella advantage classad framework condor unique ability integrate resource discovery scheduling classads resource discovery contexts vazhkudai describe classads match jobs storage devices model replica manager consulted discover list replicas matchmaking performed find storage device job submitted execution bound discovered device assumed distribution replicas change lookup basney describe classads execution domains model execution sites bind checkpoint servers jobs write checkpoints nearest server express policy controlling migrate checkpoint image contribution classads introduce indirection model jobs express constraints storage devices execution site declare binding storage storage referred member match promised exclusively requesting job contrast gang-matching classads raman multiple entities exclusively promised arrangement organization limited number licenses run proprietary software case gangmatching match licenses machines jobs ensure licensing agreements violated variety research ventures exploring storage devices names nasd active disks flash ibp buffer servers commercial vendors netapp emc offer storage servers hardware package making nest easily deployable software appliance speaks protocols suitable grid computing run special privilege wide variety mechanisms building interposition agents proposed including system call interception static relinking binary rewriting emulation existing interface making bypass due low overhead ability special privileges conclusion communities natural structures localizing application 
grid binding cpus storage organizations reflect physical reality increase performance applications utilization systems users ability express relations participants community indirect relations user express requirements storage cpu classad framework extensions indirection well-suited describing managing communities employing general-purpose building blocks condor nest pfs demonstrated easy deployment communities special privileges deploying reasonable configuration improved throughput high-energy physics simulation avenues future work configuration communities left human ratio cpus storage appliance depends offered loads physical constraints envision higher-level software reconfigure communities deploying removing storage appliances load static set communities user find difficult choose policy jobs move data vice versa mechanisms admit possibilities select trigger moves current staging mechanism complete transfer data files future investigate caching policies finer granularity data transfer instance files dataset demand fetched cached local storage appliance similar configuration tested jobs execute remotely complete stage dataset case found performance low due bandwidth contention executing jobs stage operation level file sharing sufficiently high demand caching outperform staging data finally concentrated problems delivering input data efforts condor research group kangaroo address problems reliably moving output data distant destination data movement asyncronously remotely executing jobs vacate execute machines quickly combination kangaroo communities address grid applications beginning end acknowledgements vladimir litvin assistance advice cms software paolo mazzanti offering dedicated nest machine generally making infn condor pool albert alexandrov maximilian ibel klaus schauser chris scheiman ufo personal global file system based user-level extensions operating system acm transactions computer systems pages august bill allcock joe bester john bresnahan ann chervenak ian foster carl kesselman sam meder veronika nefedova darcy quesnel steven tuecke secure efficient data transport replica management high-performance data-intensive computing submitted william allcock ann chervenak ian foster carl kesselman charles salisbury steve tuecke data grid architecture distributed management analysis large scientific datasets journal network computer applications anderson yocum chase case buffer servers proceedings ieee workshop hot topics operating systems hotos april baru moore rajasekar wan sdsc storage resource broker proceedings cascon toronto canada jim basney miron livny paolo mazzanti utilizing widely distributed computational resources efficiently execution domains submitted computer physics communications john bent andrea arpaci-dusseau remzi arpaci-dusseau miron livny nest project http nestproject joseph bester ian foster carl kesselman jean tedesco steven tuecke gass data movement access service wide area computing systems proceedings sixth workshop input output parallel distributed systems pages andrew birrell bruce jay nelson implementing remote procedure calls acm transactions computer systems februrary case fedor schoffstall davin simple network management protocol request comments internet engineering task force emc corporation http emc garth gibson david nagle khalil amiri fay chang eugene feinberg howard gobioff chen lee berend ozceri erik riedel david rochberg case network-attached secure disks technical report cmu-cs- carnegie-mellon gnutella http gnutella wego hitz storage networking appliance technical report network appliance howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systmes february galen hunt doug brubacher detours binary interception win functions technical report msr-tr- microsoft research february network appliance http netapp michael jones interposition agents transparently interposing user code system interface proceedings acm symposium operating systems principles pages litzkow remote unix turning idle workstations cycle servers proceedings usenix summer conf pages michael litzkow miron livny matthew mutka condor hunter idle free inode bitmap inode blocks disk segmentsfree data block bitmap superblock structure workstations proceedings international conference distributed computing systems pages napster http napster pai druschel zwaenepoel flash efficient portable web server proceedings usenix technical conference plank beck elwasif moore swany wolski internet backplane protocol storage network proceedings network storage symposium rajesh raman matchmaking frameworks distributed resource management phd thesis wisconsin october rajesh raman miron livny marvin solomon matchmaking distributed resource management high throughput computing proceedings seventh ieee international symposium high performance distributed computing july rajesh raman miron livny marvin solomon resource management multilateral matchmaking proceedings ninth ieee symposium high performance distributed computing pages pittsburgh august riedel gibson active disks remote execution network-attached storage technical report cmu-cs- carnegie-mellon asad samar heinz stockinger grid data management pilot iasted international conference appliaed informatics february russel sandberg david goldberg steve kleiman dan walsh bob lyon design implementation sun network filesystem proceedings summer usenix conferece pages douglas thain jim basney se-chang son miron livny kangaroo approach data movement grid proceedings tenth ieee symposium high performance distributed computing san francisco california august douglas thain miron livny pluggable file system http wisc condor pfs douglas thain miron livny bypass tool building split execution systems proceedings ninth ieee symposium high performance distributed computing pages pittsburg august douglas thain miron livny multiple bypass interposition agents distributed computing journal cluster computing sudharsan vazhkudai steve tuecke ian foster replica selection globus data grid proceedings international workshop data models databases clusters grid sudharshan vazhkudai steven tuecke ian foster replica selection globus data grid jim waldo jini architecture network-centric computing communications acm jia wang survey web caching schemes internet acm computer communication review october white grimshaw nguyen-tuong grid-based file access legion model proceedings ieee symposium high performance distributed systems august yeong howes kille lightweight directory access protocol request comments internet engineering task force march victor zandy barton miller miron livny process hijacking proceedings ieee international symposium high performance distributed computing 
international conference architectural support programming languages operating systems asplos november manasse mcgeoch sleator competitive algorithms on-line problems proceedings twentieth annual acm symposium theory computing chicago illinois pages york usa nelson welch ousterhout caching sprite network file system acm transactions computer systems february nelson virtual memory file system technical report compaq computer corporation nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference patterson gibson ginting stodolsky zelenka informed prefetching caching proceedings fifteenth acm symposium operating systems principles pages copper mountain december acm press pro softnet corporation ibackup http ibackup rashid tevanian young golub baron black bolosky chew machine-independent virtual memory management paged uniprocessor multiprocessor architectures proceedings international conference architectural support programming languages operating systems asplos pages palo alto october association computing machinery ieee rizzo vicisano replacement policies proxy cache ieee acm transactions networking roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference usenixpages berkeley june ruemmler wilkes introduction disk drive modeling ieee computer march sandberg design implementation sun network file system proceedings usenix summer conference pages berkeley usa june usenix association sanford greier yang olyha narayan hoffnagle alt melcher one-megapixel reflective spatial light modulator system holographic storage ibm journal research development smaragdakis kaplan wilson eelru simple effective adaptive page replacement proceedings acm sigmetrics international conference measurement modeling computing systems sigmetricsvolume sigmetrics performance evaluation review pages york acm press sullivan seltzer isolation flexibility resource management framework central servers proceedings usenix annual technical conference san diego california infiniband trade association http infinibandta june tomkins patterson gibson informed multiprocess prefetching caching proceedings acm sigmetrics conference measurement modeling computer systems pages acm press june vahdat anderson dahlin belani culler eastham yoshikawa webos operating system services wide area applications proceedings seventh symposium 
high performance distributed computing july waldspurger weihl lottery scheduling flexible proportional-share resource management proceedings usenix symposium operating systems design implementation nov wooster abrams proxy caching estimates page load delays proceedings international conference april young k-server dual loose competitiveness paging algorithmica june young on-line file caching proceedings ninth annual acm-siam symposium discrete algorithms balitmore january pages acm press 
application baio system calls buffer cache manager device drivers baio slave kernel user-level operation completes baio asynchronously returns control user baio directly interacts device-driver baio initiates operation buffer-cache application initiates operations initiate bypassing buffer-cache device driver intimates completions baio slave baio slave intimates completion application disk random access performance buffered working set size baio read kaio read raw read baio write kaio write raw write sequential access performance buffered working set size baio read kaio read raw read baio write kaio write raw write random access performance buffered working set size baio read ext read kaio read baio write ext write kaio write 
manageable storage adaptation wind andrea arpaci-dusseau remzi arpaci-dusseau john bent brian forney sambavi muthukrishnan florentina popovici omer zaki department computer sciences wisconsin madison abstract key storage manageability adaptation traditional storage systems adaptation performed human administrator assess problems manually adjust knobs levers bring behavior system back acceptable level future storage systems adapt reduce manual intervention paper describe wisconsin network disks project wind seek understand develop key adaptive techniques required build manageable network-attached storage system wind gracefully efficiently adapts environment reducing burden administration increasing flexibility performance storage eclectic range clients wind automatically adapt addition disks system failure erratic performance existing disks client workload access patterns introduction data storage lies core information technology distributed file systems internet services database engines shapes user experience reliability availability performance subsystem basic elements storage device industry midst radical change advent network-attached storage devices fundamentally alter manner data maintained stored accessed combination inexpensive powerful microprocessors inside disk high-speed scalable networks enables storage vendors move disks slow shared-medium busses storage-area network san collectively refer group network-attached disks storage cluster group cmu refers nasd network-attached secure disks refer general simply refer disks network network-attached storage devices storage clusters provide potential advantages traditional storage architectures scalability limited shared-medium interconnect scsi network-attached storage deliver scalable bandwidth multiple clients fault tolerance standard server environment software failure single host lead data unavailability contrast storage cluster accessible host multiple access paths data increasing data availability simplicity file systems built network-attached storage leave low-level layout decisions performance optimizations drives simplifying software increasing maintainability incremental growth compared typical raid array network-attached storage essentially unlimited growth number disks removing fork-lift replacement entire raid array specialization network-attached storage mixed cluster enables specialization system direct response system requires disk bandwidth capacity buy disks system cpu power buy processing nodes problem management storage clusters introduce additional challenges manageability absolute performance goal great number previous systems manageability focus system works consistently human intervention preferred system sporadically delivers near-peak performance requires large amount human attention manageability challenging storage clusters due additional complexity complexity result networking hardware protocols clients disks increasingly sophisticated nature modern disks drives multiple zones scsi bad-block remapping sporadic performance absolute failure result complexity networks disks system unpredictable behavior terms performance norm rare case likelihood unexpected behavior compounded increasing demand largescale systems storage-service provider -disk storage farm spite unpredictability ideal manageable storage system behave upgrade disk added ideal system immediately begins utilizing full capacity migrating data balance load longterm data migration writing data disk increase throughput short-term adaptation ideal system fully utilizes disk capacity performance relative disks failure complexity modern disks introduced range failures binary fail-stop model components work perfectly continuous range component working full capacity ideal system utilizes performance faulty components degree workload delta applications data-sets access patterns presented storage system change previous layout data disks longer satisfactory ideal system reacts migrating data match current conditions frequently accessed data migrated newer faster disks spread disks increased bandwidth shared access network-attached storage multiple clients file systems simultaneously share underlying disks incomplete knowledge activity disk ideal system adapts contention run-time delivers performance clients traditional systems react scenarios assistance human administrator assess problem manually adjust parameters bring performance system back acceptable level adaptation key manageable storage systems future system reacts system behavior automatically adjusts problems reducing manual intervention nestnestnestnestnestnest short-term reactive middlew storm sa-nfs sa-nfs riverfs database parallel query operator user app user app rain ipi clouds server clouds ipi client clouds clouds ipi rain rain storm figure wind system architecture wisconsin network disks paper describe design current status wisconsin network disks wind main focus wind develop key techniques build manageable network-attached storage achieve goal fully exploit potential underlying hardware software develop distributed scalable wind comprised major software components broken groups runtime off-line adaptive elements wind storm gale clouds key pieces supporting infrastructure rain nest system architecture presented figure briefly describe components key piece adaptation software storm performs run-time adaptation data access layout storm adapts short-term workload characteristics disk performance quickly adjusting client reads writes disk short-term adaptation lacks global perspective introduce gale software layer monitors system activity on-line simulation performs global long-term optimizations improve performance reliability gale migrate replicate data improve read performance re-balance workload drives gale place data accordance current climate system access patterns applications gale interacts important ways storm data replication storm flexibility read data block needed gale replicates data provide increased flexibility storm improves adaptivity system storm gale designed adapt data flows disk requests network-attached disk satisfied in-memory caches clouds flexible caching network-attached storage clouds mechanisms policies client-side server-side caches taking variable costs account clouds employ cooperative techniques conglomerate serverside cache resources potentially hide disk variations clients wind key pieces software infrastructure rain information architecture encapsulates acquisition dispersal information storm gale clouds rain information programming interfaces ipis software subsystem hide details information flows greatly simplify system structure maintainability nest flexible efficient single-site storage management finally eventually plan implement evaluate file systems top wind infrastructure build striped adaptive version nfs sanfs adheres nfs file system interface modified stripe data blocks disk adaptive fashion plan construct riverfs parallel record-oriented file system designed support highperformance parallel query operators found database environments riverfs takes advantage relaxed data semantics readily provide robust consistent performance adaptivity wind section discuss major adaptive components wind run-time adaptive layout access storm off-line monitoring adaptation gale adaptive caching clouds pieces software technology work harmony adapt system provide hands networkattached storage storm storm short-term reactive middleware distributed software layer interposes clients servers provide adaptive data access storm file systems adapt volatile disk behavior run-time deliver full bandwidth clients intervention challenges storm two-fold storm adapt data streams moving disk general idea clients interact higher performance disks frequently client access proportionally data faster disks storm achieve goal low overhead extra processing book-keeping metadata adaptation overhead odds seek knee curve adaptivity high overhead acceptable challenges adapting allocating writes versus non-allocating writes reads substantially allocating writes long pay cost book-keeping nonallocating writes reads freedom performed locations stored disk discuss operation allocating writes data written file time space allocation occurs blocks committed physical storage refer writes allocating writes illustrative purposes assume data striped set remote 
disks raid level striping redundancy algorithms allocating data blocks disks classified terms frequently evaluate relative speeds disk adapt placement discuss range algorithms level traditional striping gauge performance storage components allocating data assumes disks run rate strength approach meta-data needed block lookup block size weakness disks treated identically system performance tracks rate slowest disk level primitive adaptive allocation algorithm adjusts disks delivering data rates assumes disk behaves fixed manner time algorithm relative speeds disks calculated amount data striped device made proportional relative speed lookup logical block approach requires additional meta-data striping ratios disks level algorithms periodically determine relative performance disks adjust striping ratios period striping ratio striping interval additional meta-data needed striping ratio size striping interval level adaptive algorithm client continuously gauges performance system writes data block disk believes handle request fastest advantage approaches adapt rapidly performance make small adjustments reflected simple integer striping ratios disadvantage significant amount meta-data recorded target disk block offset logical block written demonstrate benefit versus approach present performance user-level library file striping measurements performed ultrasparc workstation internal rpm seagate hawk disks single fast-narrow scsi external rpm seagate barracuda disks fast-wide scsi measurements produced context single machine multiple disks results general apply environments heterogeneous disks table compares performance striping disks striping data striped blocks disk system table shows striping effective disks speeds achieving peak bandwidth striping gauge relative performance disks simple off-line tool measure achieve writing simultaneously hawk disks barracuda disks peak performance measured isolation determines proper ratio stripe sizes write blocks data slower disks faster disks striping achieve peak bandwidth major research issues storm extend adaptation algorithms raid levels concentrate raid level mirroring excellent performance properties conceptually simple parity calculated straight-forward transformation adaptive striping mirrored adaptive striping treat pairs disks single logical disk perform adaptive striping logical disks major disadvantage approach introduces performance coupling pairs disks pair run rate slow disk storm couple disks similar performance characteristics calling gale hints pairing alternatively mirroring performed lazily files tolerate window potential loss gale run fill mirrors full reliability similar idea proposed afraid redundancy relaxed improve raidperformance small writes non-allocating writes non-allocating writes blocks written previously allocated blocks file result storm choice disk receives non-allocating write lead acceptable performance cases disks scsi bus write write max actual seagate hawks narrow seagate barracudas wide disks striping disks striping table benefits striping table shows write bandwidth achieved level striping column lists disks column applicable scsi buses write max column shows peak aggregate bandwidths disks write actual column shows bandwidth achieved striping library storm initially allocates amount data disk depending observed performance disk leaves performance footprint storage system performance footprint contributing factors produced disks produced workloads accessing drives factor contributed devices simply speed file data sequentially written contention layout blocks disk factor contributed workload includes access pattern single application sequential random contention multiple applications implication formalizing concept performance footprint temporal performance locality exists performance footprint recent past clients access file non-allocating writes reads access data disks optimal performance allocation temporal performance locality exist non-allocating writes vulnerable performance variations completely general flexible solution transform non-allocating writes allocating writes build multi-disk log-structured file system incorporates run-time adaptive techniques akin approach case adaptivity high cost plan evaluate relative strengths weaknesses range allocation algorithms terms adaptivity changing performance footprints amount meta-data required specifically investigate algorithms adaptively determining length striping interval performance footprint rapidly striping interval small obtain bandwidths disks performance footprint slowly striping interval longer amortize cost gauging recording meta-data reads freedom reads depends level replication file simple striping block file written location block read back disk performance lack freedom acceptable temporal performance locality exists performance footprint longer valid assume gale re-organizes replicates data taking current climate account storm adaptively advantage replicated sources data reads earlier work graduated declustering focused distributed adaptive mirrors parallel clients plan generalize handle general-purpose workloads variety replicated layout schemes gale short-term adaptation solve problems encountered dynamic heterogeneous environments short-term adaptations analogous greedy algorithms arrive solution lack global perspective provide long-term view system workload activity optimize system performance ways runtime building additional software structure globally adaptive long-term engine gale gale basic services wind gale performs system monitoring active passive techniques gather workload access patterns device performance characteristics detecting anomalies component behavior gale decides perform global optimization action instantiation gale replicate oft-read file performance reasons finally gale information storm clouds hint generation system monitoring gale inserts lightweight monitors clients servers trace workload access patterns measure response times simple tracing aspect gale on-line simulation generate performance expectations periodically gale set actual disk requests submit disk simulator system compare performance results simulation measured performance note stark differences accurate disk simulators readily initial experiments reveal simulate modern drive real-time comparing real performance simulated performance gale detect system awry action instantiation monitoring system gale choose migrate replicate data match current climate system steps step cost benefit analysis access time hawk access time request size ratio access times random reads seek time rotational delay dominate transfer time dominates figure access size impact figure depicts impact access size performance difference seagate hawk modern ibm drive graph plots ratio performance disks random reads varying size requests x-axis gale compare costs migration replication versus benefit current workloads taking care ignore short-term system secondstep actual migration replication migration moving data disk behaving unexpected ways determined simulation-comparison fail reorganizing oft-read data match access patterns utilize disks higher bandwidth replication important giving storm additional flexibility accessing data replicating block storm adaptively choose site read based current climate writes plan exploring active lazy updating replicas disadvantage leaves opportunity adaptation disadvantage replicas stale data investigating multi-access replication schemes performance differential drives varies workload figure illustrates performance differential drives seagate hawk ibm small requests ibm drive faster due smaller edge seek time rotational delay larger request transfer rate dominates larger performance difference drives gale detects small random large sequential accesses important performance gale creates replicas data layout optimized small requests large transfers challenge simultaneously utilize multiaccess replicas data availability migration replication consume resources system scheduled carefully idle time successfully systems hope times gale activities hint generation gale hints storm clouds access improve decisions examples hints gale pass 
data placement hints disks file allocated ratio adjustment hints initial performance levels expect set disks faultmasking hints data blocks benefit fault masking caching section mirror-matching hints set disks mirrors gale return confidence indications hint allowing storm clouds decide clouds clouds flexible adaptive caching layer wind flexible clients storage forced accesses limited subset files adaptive clouds algorithms fundamentally varying cost block access account blocks replacement costs costs change time clouds divided distinct components clientside clouds server-side clouds explore client clouds server clouds client clouds clients network-attached storage system cache blocks number drives system clouds enhances existing operating-system buffer managers enabling caching algorithms variable cost block access account algorithms lru cost account assume blocks equally costly fetch work variable-cost environment early theoretical work belady established offline optimal cache-replacement algorithm simply replace block referenced furthest future off-line generalization caches variable replacement costs good heuristics recent work web cache caching strategies focuses extending modifying lru work costs replacement clouds extend approaches function local-area environment access costs wide-area change rapidly interesting challenge track replacement cost low overhead expensive track cost accessing block caching algorithm inform rain set candidate blocks rain return costs explore trade-offs managing information versus performance algorithms apply variable-cost caching non-lru algorithms recent work shown sophisticated algorithms behave lru data fits cache avoiding thrashing larger data sets initial simulation results client-side cost-aware caching algorithms promising single disk performing poorly stuttering absolute failure older slower disk disks perform expected level traditional caching algorithms adversely affected lru account cost accessing slower disk cost-aware cooperative approach performance differences masked server clouds server-side clouds cooperative-caching algorithms manage caches disks enables ability disk caches cooperate cache blocks slow disks hide variable cost disk access clients call performance-fault masking assume single disk running slower rest application sequentially accessing data set disk case server-side caches cooperate cache blocks slow disk slowness masked higher levels system creating illusion uniform set disks masking occur reads writes storm writes easier handle server caches cooperate buffer data destined slow disk hide slowness clients large amount data written disk compared total amount buffer space technique successful masking reads behave analogously server-side caches favor blocks cooperatively slower disks potentially hide latency blocks caches technique effective repeated reads prefetching employed fill caches blocks slow disk finally note interaction clouds storm server-side caches cooperate hide variable behavior disks clients interface clouds storm storm informed intention clouds action clients autonomous trust clients plan explore client-side cooperative algorithms core infrastructure identified pieces infrastructure build effective wind system rain thin software layer responsible efficiently gathering dispersing information system nest single-site flexible storage manager rain goal rain rapid access information distribute current climate system remote components behaving enable effective simplified implementations storm gale clouds information layer presents higherlevel layers specialized information programming interfaces ipis interfaces insulate algorithms adaptive components details information gathered stored propagated challenge rain deliver accurate lowcost information current performance axes investigate information gathered explicitly implicitly recognize explicitly querying remote disks approach interface obtaining desired information accessing explicit interface costly sending explicit request consumes shared network resources needed data transfers induces additional work remote disk finally explicit query fail forcing requester handle failure cases investigate implicit sources information rain infers desired characteristic observing operations exist system observing time required recent read disk number outstanding requests disk remote-disk performance inferred overhead central advantage implicit methods provide information free additional communication required ability deduce remote behavior local observation disadvantages methods inferences made subtle information flow restricted path data flow ipis hide method gathering information rain free switch methods run-time find effective explicit implicit switch methods depends variables frequency climate chaotic dynamic system accuracy method good information obtained overhead induces total system resources spent plan evaluate impact information-gathering style higher layers system nest key piece infrastructure nest network storage technology single-site storage manager named original intent provide storage condor nest highly flexible configurable appliance application wind environment main axes flexibility nest client protocol communication wind-specific protocol http nfs simple nest native tongue concurrency architecture threads processes events range locking consistency semantics flexible infrastructure caching configuring nest deploy specialized highly-tuned server well-suited current environment investigating interface nest provide support adaptation starting point object-level interface put national storage industry consortium nsic derived directly cmu nasd project observed interface limits client-side algorithms develop alternatives adaptation friendly subsequently implement nest status wind system developed cluster intel-based machines running linux operating system connected mbit gigabit ethernet ibm ultrastar lzx disks networkattached storage device client basic prototype storm developed rain ipi informationgathering alternatives client-side software plugs linux loadable kernel module talks nest homegrown wind protocol nest running recent experiments show delivers full bandwidth clients process thread models event-based nest work linux machines due limitations select interface gale studied ganger simulator found performance suitable events simulated rate faster real-time initial results simulator accurate per-request granularity sophisticated approaches required finally understand caching algorithms clouds developed detailed simulation environment easily explore algorithmic alternatives understand algorithmic trade-offs proceed implementation prototype environment related work basic architecture storage system strongly influenced network-attached secure disk nasd project carnegie-mellon nasd project introduced drive object model starting point focus cmu nasd support traditional file systems strong security concentrating adaptivity hoping leverage security infrastructure cmu group develops petal storage cluster closely related petal assembled cluster commodity pcs number disks attached petal exports large virtual disk clients high-speed network petal elegance simplicity arises careful separation storage system functionality file system petal limited form run-time adaptation client reading mirror picks mirror shortest queue length wind enforce strong separation storage system file systems exposing disk client-side file system software adaptation disks made station usc-isi explores networkattached peripheral devices advocate stock protocols communication custom-tailored fast networking layers approach advantages providing easy compatibility communicating devices allowing off-the-shelf well-tested software wind tcp reasons projects network-attached storage step running user code form drives active disks approaches lose advantages separation specialization network-limited environments provide performance advantages adaptive techniques applicable environment berkeley i-store project discusses concept introspective system built intelligent disks wind gale component form system introspection on-line simulation recently authors propose set benchmarks evaluate system availability hope apply evaluation wind robust performance long goal storage systems chained 
declustering balances load mirrored storage system failure carefully allocating data disks read traffic avoids hot-spots typical mirrored systems failure seek generalize concept adapting unexpected behavior disks absolute failure excellent adaptive system autoraid storage array autoraid presents standard raid interface clients adaptively migrates data raid levels hot data mirrored storage improved performance cold data moved raidstorage increase effective capacity performance versus capacity optimization gale framework adaptivity explored context parallel file systems study authors explore fuzzy logic adaptively select proper stripe size storage system approaches applicable wind long-term adaptation shown promise single-disk log-structured file systems approach file layout reorganized off-line improve read performance lfs similar techniques employed gale gale generalize task reorganization operate multiple disks finally issue heterogeneous raid strategies studied multimedia literature video servers studies assume static performance differences components dynamic environment expect develop summary complexity growing point manageability storage systems comprised largely autonomous complicated individual components connected complex networking hardware protocols storage systems future exhibit properties problems larger scale widearea systems software programming environments platforms provide mechanisms facilitate robust global behavior spite chaotic dynamic component behavior adaptive mechanisms storage increasingly difficult manage require high amount human involvement end wind developing pieces adaptive software storm reactive run-time data access layout gale performs system monitoring off-line adaptation clouds flexible caching underpinnings layers adaptation development rigorous information architecture called rain flexible general storage manager nest successful development components reduce burden storage administration acharya uysal saltz active disks proceedings conference architectural support programming languages operating systems asplos viii san jose october anderson dahlin neefe patterson wang serverless network file systems proceedings acm symposium operating systems principles pages december arpaci-dusseau arpaci-dusseau culler hellerstein patterson highperformance sorting networks workstations proceedings acm sigmod conference pages arpaci-dusseau performance availability networks workstations phd thesis california berkeley arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common iopads belady study replacement algorithms virtual storage computers ibm systems journal brown oppenheimer keeton thomas kubiatowicz patterson istore introspective storage data-intensive network services proceedings workshop hot topics operating systems hotos-vii rio rico arizona march brown patterson maintainability availability growth benchmarks case study software raid systems usenix san diego june cao irani cost-aware proxy caching algorithms usits pages december fox gribble chawathe brewer gauthier cluster-based scalable network services sosp pages ganger worthington patt disksim simulation environment technical report michigan cse-tr- ghandeharizadeh muntz design implementation scalable continuous media servers parallel computing january gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture asplos viii october gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie-mellon golding bosch staelin sullivan wilkes idleness sloth usenix association editor proceedings usenix technical conference january orleans louisiana usa pages berkeley usa jan usenix hennessy future systems research ieee computer august hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international data engineering conference pages lee thekkath petal distributed virtual disks asplos vii pages cambridge october litzkow livny mutka condor hunter idle workstations proceedings eighth international conference distributed computing systems san jose california june matthews roselli costello wang anderson improving performance logstructured file systems adaptive methods proceedings symposium operating systems principles sospvolume operating systems review pages saint-malo france october acm sigops acm press meter observing effects multi-zone disks proceedings usenix conference jan meter finn hotz visa netstation virtual internet scsi adapter asplos xiii pages oct riedel gibson faloutsos active storage large-scale data mining multimedia proc international conference large databases vldb august rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february satyanarayanan digest seventh ieee workshop hot topics operating systems rice conferences hotos digest digesthtml html march savage wilkes afraid frequently redundant array independent disks proceedings usenix technical conference pages january simitci reed adaptive disk striping parallel input output seventh nasa goddard conference mass storage systems san diego march ieee computer society press smaragdakis kaplan wilson eelru simple effective adaptive page replacement sigmetrics conference measurement modeling computer systems atlanta talagala patterson analysis error behaviour large storage system ipps workshop fault tolerance parallel distributed systems wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february young k-server dual lose competitiveness paging algorithmica june 
information control gray-box systems andrea arpaci-dusseau department computer sciences wisconsin madison dusseau wisc remzi arpaci-dusseau department computer sciences wisconsin madison remzi wisc abstract modern systems developers unable modify underlying operating system build services environment advocate gray-box techniques treating operating system gray-box recognizes changing restricts completely obviate information acquire internal state control impose paper develop investigate gray-box information control layers icls determining contents le-cache controlling layout les local disk limiting process execution based memory gray-box icl sits client combination algorithmic knowledge observations inferences garner information control behavior gray-box system summarize set techniques helpful building gray-box icls begun organize gray toolbox ease construction icls case studies demonstrate utility gray-box techniques implementing os-like services modi cation single line source code introduction modern operating systems large complex bodies code hundreds programmer-years invested result modifying operating system difcult costly impractical endeavor extreme realistic view researchers noted traditional operating systems rigid simply hardware masquerading software viewing operating system immutable object odds bulk operating systems research seeks develop integrate ideas operating systems reduce orts required change large body research investigated operating system restructured extensible systems functionalto symposium operating systems principles sosp october chateau lake louise banff canada permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee copyright acm ity performance improvements easily added tailored desires applications limitation approaches require operating system orts minimize modi cations require altered minor requiring change single line code make deployment innovation commercial operating systems problem obvious non-technical hurdles overcome persuade large company incorporate idea accepted single vendor open-source base wide-spread adoption innovations unused applications run cross-platform existing interfaces systems transactional database manages raw disk obtain high performance implements optimized database-oriented system incentive system single platform complicates database source code rare idea incorporated widely large range good ideas orphaned remaining challenge disseminate research ideas requiring underlying projects distributed computing addressed building system services top unmodi commodity operating systems approach constricting seemingly sti implementation functionality thesis paper surprisingly large class os-like services provided applications modi cation speci cally acquire information state control behavior unexpectedly powerful ways explicit interfaces exist approach treated gray box general characteristics algorithms employed combining knowledge run-time observations reacts commands queries services implemented term software layer interfaces gather information control gray-box system graybox information control layer gray-box icl icl residing clients applications graybox system presents clients traditional enhanced interfaces interfaces icl clients learn state underlying system data cache control behavior place les disk internally obtain information icl observe existing client interactions gray-box system insert probes system case combining observations statistical analyses priori knowledge behaves icl infer current state experienced programmers tend exploit knowledge behavior underlying system knowledge encapsulated icls techniques programmers gray-box systems step combining knowledge measurements observations technique commonly found microbenchmarks exists strong duality microbenchmarks gray-box techniques icls require underlying components benchmarked con gure internal thresholds parameters understanding behavior icls requires understanding behavior icls reveal surprising behavior microbenchmark paper explore challenges building graybox icls developing studying services rst le-cache content detector fccd determines contents cache applications re-order operations rst access data cache service functionality similar proposed modi cations layout detector controller fldc discerns controls layout blocks disk applications schedule accesses reduce seek time memory-based admission controller mac detects amount memory multi-programmed system limits number contending processes cases anticipate modifying applications interfaces provided icls show cases unmodi applications icls obtain bene demonstrate utility icls simple benchmarks real applications cases observe substantial performance improvements relative versions applications information control underlying cases improve performance order magnitude limitations gray-box approach discuss fundamental advantage building services icl library examples middleware distributed environment service receive wide-spread adoption related advantage approach icls easy port demonstrate relative ease icl porting running codes erent unix platforms linux netbsd solaris implementations found overlapping set required functionality gray-box icls begun formalize gray toolbox common repository routines ease construction erent operating systems fast platformspeci timers statistical routines envision toolbox grow icls developed similar spirit interposition toolkit jones rest paper organized begin summarizing techniques building icls section discuss previous gray-box systems section section give overview experimental environment cover icl case studies le-cache content detector layout detector controller memory-based admission controller describe beginnings gray toolbox section cover related work section conclude section gray-box techniques encapsulation helps simplify design large complex systems allowing system designers ignore unnecessary details simpli cation high cost viewing black box make assumptions implementation behavior internal state speci interface internal-state information acquired behavior controlled explicit mechanisms designed implementors interface provided information hidden control prevented practice systems black boxes savvy programmers idea component implemented gray box users knowledge acts speci interface knowledge toe-hold gaining information state key controlling behavior note focus paper treatment operating system gray box component layer module objectbased system treated section discuss techniques found developing case studies existing systems information techniques information internal state optimize system services applications scheduler activations key piece state information passed kernel user-level scheduling library number processors application running information threads library job scheduling active threads summarize ways determine internal state interface exists acquire algorithmic knowledge developers interfacing knowledge algorithms employed developer access source code internal design documentation simply familiarity common implementation techniques lru-like caching algorithmic knowledge exists levels detail extreme designer caching performed system extreme designer full understanding source code cost hitting missing cache algorithmic knowledge gray-box icl interact cient manner determining interact component general knowledge behaves studied extensively theoretical work game theory decision theory practical side exists tension optimizations makes icl portability algorithmic knowledge assumed optimizations make fewer systems assumptions apply monitor outputs algorithmic knowledge icl infer internal state improve quality inferences icl make found combine knowledge observations output observed output speci interface measurable characteristic external interface covert channel examples covert channels gray-box systems include elapsed time power consumed presence dropped messages outputs black-box observed make predictions infer behaves infer internal state powerful aspect gray-box techniques combination observations algorithmic knowledge allowing designers build icls portable cient portable assume high-level algorithmic knowledge cient tuned speci platform observations infer current state algorithmic knowledge simplistic inaccurate icls built robust observations 
verify true state note icl observe inputs infer state models simulations drawback requires participation processes investigate icls assume visibility inputs statistical methods infer internal state icl observe output correlated state interest infer speci code path executed data item cached observe response fast slow draw robust inferences potentially noisy data advocate statistical methods microbenchmarks parameterize system icls system parameters order operate properly speed sequential disk access suite microbenchmarks icls care executing benchmarks require dedicated system time run insert probes cases client icl make cient requests icl observe outputs icl insert probes speci requests generated solely observe resulting output probe icl generate requests desired inputs desired time desired context challenge probes describe case studies presence change state system refer heisenberg ect challenge probes add signi overhead system cases adding probes icl improve application performance prefetching disk blocks control techniques responsibility icl control ways speci existing interfaces assume designer icl level algorithmic knowledge control algorithmic knowledge perform actions side-e ects operations read interface afs icl read single byte prefetch entire server describe techniques exerting control move system state inferring information arbitrary unknown state cult state control technique icls move system simpler state easier gauge contents page cache periodically ushes monitors models subsequent activity reinforce behavior feedback application icl interactions strongly determined behavior icl icl reinforce desired behavior controlling manner behaves contents cache determined order accesses icl direct client interactions make cache contents predictable repeated access icl erent runs erent applications act positive feedback stabilizing system behavior previous gray-box approaches illustrate techniques brie survey literature systems assume exploit gray-box knowledge rst examine microbenchmarks assume knowledge system test examine existing systems gray-box techniques tcp congestion control implicit coscheduling manners note services developed implementors modify existing part system microbenchmarks applications obtain performance detailed characteristics underlying hardware systems interfaces microbenchmarks developed exploit gray-box knowledge user infer characteristics measuring completion time memory accesses erent patterns determine parameters memory hierarchy nding greatest common divisor execution time erent expressions determine processor cycle time measuring access time carefully designed requests low-level characteristics disk geometry inferred gray-box icls bear similarity microbenchmarks number important ways microbenchmarks acquire information control system microbenchmarks gather static information component characteristics current state microbenchmarks run controlled environment microbenchmarks arbitrarily long time run make inferences tcp congestion control goal tcp congestion control algorithm distributed clients send data amounts congestion viewing network gray-box clients combine general knowledge network behaves measurements ongoing communication infer current state network congestion knowledge tcp implicit coscheduling manners knowledge message dropped congestion dest scheduled send msg symmetric performance impact outputs time ack arrives arrival requests reported progress process time response statistics variance linear regression exponential avg paired-sample sign test benchmarks round-trip time probes state required benchmarks slow convergence feedback routers drop msgs signal react observations table summary gray-box techniques existing systems network drops packets congestion clients observe existing communication acknowledged infer congestion exists routers network turn control sending rate clients dropping packets congestion occurs misbehaving clients identi observing unresponsive gray-box control tcp congestion control algorithm labeled black-box scheme due assumption packet loss caused congestion gray-box scheme fact recognizing gray-box knowledge led problems environments wireless setting dropped message longer congestion due simply lossy medium result unmodi tcp congestion control algorithm behave wireless settings correctly identifying gray-box knowledge problems avoided implicit coscheduling time-shared ne-grain parallel jobs achieve acceptable performance communicating processes scheduled simultaneously implicit coscheduling technique achieving coordinated multiprocess scheduling modifying implicit coscheduling combines gray-box knowledge communication interacts scheduling remote nodes observations on-going communication parallel job speci cally hard-wired algorithm process waiting response knowledge receiving message remote process means remote process scheduled recent past likewise receiving prompt response request means remote process scheduled infer scheduling state remote nodes process simply observes message arrivals waiting time manners running low-importance processes idle time feature missing modern operating systems manners functionality suspending low-importance jobs resource contention detected implemented modi cation manners gray-box knowledge process competing degrades progress symmetrically combining knowledge measurements progress low-importance process manners infer low-importance process suspended authors number simple statistical techniques calculating expected level performance uncontended environment required time frame order hours summary summarized table services touch number gray-box techniques revisit case studies services combine algorithmic knowledge observations time required existing operations infer state system services statistical techniques run time priori benchmarking controlled state addition techniques presented case studies demonstrate utility probing case studies section explore icls speci cally develop experiment le-cache content detector fccd layout detector controller fldc memory-based admission controller mac due space limitations describe fccd detail present subset issues fldc mac section discuss basic problem icl addresses gray-box knowledge explain implementation perform experiments show capabilities layer discuss limitations summary gray-box techniques found shown table experiments run machine intel pentium-iii processors physical memory ibm lzx disks machine large amount memory stresses icls determine contents cache amount memory experiments performed top linux evaluate gray-box libraries netbsd solaris fact easily deploy icls platforms illustrates major advantages gray-box approaches file-cache contents knowledge contents cache applications re-order data accesses potentially improve performance application repeatedly accesses set les erent arguments grep arg system total amount data exceeds size cache operating system performs lru-like replacement case performance improves dramatically application rst processes data cache small fraction data fetched disk application access cached data rst operates lru worst-case mode fetching data disk run section describes gray-box file-cache content defile-cache content file layout detector memory-based admission detector fccd controller fldc controller mac knowledge lru-replacement groups directory replacement algorithm locality app groups i-nodes data-blocks outputs time read byte i-number time write byte statistics sort cluster sort discard outliers benchmarks measure seek overhead memory disk threshold probes read byte stat i-number write byte page state refresh directory contents write rst make resident feedback order determines access pattern table summary gray-box techniques case studies tector fccd applications gauge contents cache act pursuit gray-box fccd inspired recent work storage latency estimation descriptors sleds discussed van meter gao work van meter gao propose interface returns predicted access times sections interface determine parts fast access based combination knowledge storage hierarchy static estimates storage device latencies main limitation work requires modi cations linux kernel gather information show great deal utility proposed 
system obtained modi cation operating system gray-box knowledge begin exploring algorithmic knowledge le-cache manager knowledge develop gray-box fccd extreme approach complete algorithmic knowledge le-cache manager access inputs extreme approach basic algorithmic knowledge combined observations outputs complete knowledge behavior le-cache page-replacement algorithm ability observe input model simulate pages cache approach complex inaccurate detailed model page-replacement algorithm order correctly simulate contents cache due interactions memory pages observe accesses memory accesses finally applications interested state le-cache provide inputs simulation single process obey rules knowledge accessed incomplete simulation inaccurate explore infer internal state le-cache observing outputs begin assuming coarsest level algorithmic knowledge cache les full page replaced order page hypothesis predict presence systems provide information contents cache mincore routine interface broadly relied part cache timing carefully selected le-cache probes probe read single byte page read returns quickly conclude probed page cache probe returns slowly page disk probes sparingly reasons probing page memory high cost time probe page essentially identical application access page disk probing page disk destructive state cache heisenberg ect probe page entire page brought cache page evicted selective probes relative cost low avoid changing state system probes accurately ect state cache goals inherently odds order probes successful presence page cache highly correlated presence pages nearby order correlation exist system tend adjacent pages cache evict ect occurs systems applications access les spatial locality page replacement algorithms designed mind operating system approximation lru clock algorithm tend evict pages signi cantly long chunks figure demonstrate relationship plotting correlation presence random page prediction unit contiguous region percentage unit cache line designates erent access unit application random-access sequential-access access unit amount data application reads sequentially randomly picking set test run ush cache run program accesses speci access unit query cache determine contents query contents cache modi linux kernel return bit-map presence bits page graph prediction unit equal access size presence probed page highly correlated presence entire prediction unit prediction unit large relative access unit application corindeed interface existed platforms require gray-box fccd correlation prediction unit size prediction units access size access size access size figure probe correlation graph plots correlation presence single random page prediction unit percentage unit cache size prediction unit increased x-axis correlation plotted y-axis sets points plotted vary access pattern test program running test program explicit probe determines pages present cache accessed roughly size cache measurements times means standard deviations shown relation falls noticeably implementation section discuss fccd prediction unit made smaller access unit desired implementation details envision common usage template applications fccd application speci set les interested reading library returns list offset length pairs data thinks cache based probes performs application information re-order accesses rst accessing pages memory applications examined modi cations straight-forward involved lines code provide method application fccd requiring modi cation users call utility gbp set les returns list les predicted access order implying grep foo replaced grep foo gbp -mem utilize data reordering single option gbp probe read data blocks probe order copying reads stdout gbp -mem -out infile app unmodi application reads stdin utilize intrale re-ordering describe make inferences probes working library simple cient portable manner goal library work operating system performs replacement similar based time access underlying hardware technology parameters speed memory disks implementation address problems erentiate probe times cache cache amount data application access unit number pages state predicted single probe describe issues turn cache-di erentiation threshold conceptually determine probed page memory erentiate time er-cache hit versus er-cache miss approach simple threshold time probe threshold page considered cache greater page considered disk library work variety platforms approach requires priori benchmarking kernel-to-user memory copy time storage subsystem painful erent types disks present arrived solution requires differentiation threshold sorting prediction units time required probe method simple robust erentiates entities multiple-level store memory disk tape case closest items accessed rst closest access unit gray-box interface order accesses application previously read large sequential order read random order amortize seek overhead reading arbitrary sets library return offset length pairs large length elds determine default access unit delivers near-peak performance disk performing simple microbenchmark platform found default access unit works application access units obey boundaries ensure records cross multiple access units advanced applications exact manner data returned passing list offset length pairs prediction unit shown picking prediction unit smaller access unit application cient high prediction accuracy similarly fccd access size access unit simply set prediction unit access unit obtain reasonable predictor found performing probes access unit slightly robust prediction unit gray-box layer probes points default access unit measures time probe sorts access units total time probes overhead probes negligible measurements reveal probe time in-cache data realm microseconds milliseconds probe out-of-cache data amortized entire access time files smaller size probed found method choosing probe point prediction unit important approach select bytes predetermined sets process terminates probe phase access phase processes probe le-cache time set probes return bad information indicating pages cache solution probe random byte prediction unit method robust runs added bene application probe cache repeatedly increased con dence time file size single-file scan traditional gray-box worst-case disk ideal in-cache ideal out-of-cache figure single-file scan graph plots total access time repeated runs warm cache traditional linear scan gray-box scan gray-box scan fccd ascertain parts cache accesses accessing rest data point average runs includes standard deviation bars simple models plotted predicted worst-case time data retrieved disk predicted ideal data cache retrieved memory-copy rates data fetched disk experiments perform experiments demonstrate utility cacy gray-box fccd begin showing software obtains good performance reordering accesses single large reordering accesses les examine bene applications modi interfaces fastsort grep finally demonstrate techniques work erent unix-based operating systems single-file scan perform simple experiment modify scan single utilize library gray-box scan library probe state cache accesses rst pages library predicts in-cache rest result access pattern gray-box scan longer purely sequential scan sequentially accesses segments size directly determined access unit ect running application multiple times control technique positive feedback accessing access-unit sized chunks access-unit sized chunks present cache figure plots time access single varying size gray-box scan traditional linear scan experiment begin ushing cache running application times note graph similar spirit style graphs 
presented van meter gao text gure traditional scan ers large performance decrease size exceeds size cache point entire retrieved disk due lru-like page replacement algorithm gray-box scan consistently perform accesses disk frequently total amount performed proportional size minus size cache multiple-file scan applications easily modi process single arbitrary order exibly process set input les arbitrary order experiments shown due lack space utilize fccd determine ordering group les processing sequentially performance similar shown singlele scan application experiments set experiments incorporate gray-box library real applications rst examine versions grep rst unmodi standard gnu utility searches string set les version gb-grep modify grep internally reorder les speci command line gray-box library change straight-forward transforming lines code roughly lines version output gbp utility input unmodi grep grep foo gbp -mem figure shows time versions grep -mb text les warm cache time application normalized time unmodi version gray-box knowledge repeated runs access les order run rate disk gray-box version gb-grep runs factor faster data cache traditional grep combined gbp exhibits bene slight additional overhead incurred due extra fork exec redundant opens closes application fastsort highly tuned twopass disk-to-disk sort similar agarwal rst pass creates multiple sorted runs records size run determined records memory run reads records sorts keys writes sorted records disk pass reads sorted runs disk merges single sorted list writes nal output disk experiments sort roughly data -byte records report performance rst read phase simulate pipeline creating records sorting refresh cache contents run versions sort unmodi sort sort modi gray-box library unmodi sort gbp -mem -out input transformation traditional sort graybox version slightly involved grep application read parts single input erent order required replacing read code lines code adding probe phase main sorting loop lines note gbp informed -byte alignment restrictions sort returns chunks record-aligned figure shows performance read-phase versions gray-box versions substantially improve performance bene large grep erence occurs sort copies memory gray-box versions grep follow exact semantics grep output ordered erently semantics preserved output grep re-ordered application thrash outputting large number matches grep sort normalized execution time application application performance grep gb-grep grep gbp -mem fastsort gb-fastsort gbp -mem -out infile sort figure application performance performance grep fastsort shown leftmost bar group shows normalized performance unmodi application repeated runs roughly total data grep scans -mb les seconds average fastsort completes read-phase input seconds bar group shows relative improvement gray-box version application finally bar group gray-box command line tool unmodi application advantage gray-box knowledge data item reads eventually writes data disk contention memory read-only application grep pages heap pages write-bu ering purge parts input memory prematurely unmodi sort gbp -out input experiences bene extra copy data required operating system pipe mechanism copy palatable sort cpu acceptable situations multiple-platform tests demonstrate gray-box approach works range operating systems examine fccd linux netbsd solaris experiments compare performance microbenchmarks scan multile search measuring unmodi performance cold warm cache modi gray-box performance warm cache figure plots relative execution times normalized platform time cold-cache run platform actual times caption examining scan results rst linux repeated runs gray-box fccd exhibit signi improvement relative unmodi scan expected slightly surprised performance repeated scans -gb netbsd solaris linux solaris entire physical memory caching throwback early unix implementations netbsd xed amount memory caching case note recent overhaul netbsd netbsd repeated scan -gb runs near-disk rate gray-box knowledge illustrate bestcase gray-box performance netbsd report linux bsd solaris linux bsd solaris normalized execution time scan search multi-platform experiments traditional cold cache traditional warm cache gray-box warm cache solaris re-reads hit file cache figure multi-platform experiments gure plots performance repeated largele scans multile searches bars plotted runs experiment average time cold-cache warm-cache run traditional approach average time warm-cache gray-box run group bars normalized cold-cache time linux bsd solaris scans les seconds cold-cache case searches -mb les -mb les -mb les average seconds performance repeated -mb scans surprisingly solaris repeated scans warmcache perform gray-box knowledge case le-cache manager lrubased replacement algorithm single portion cache repeated accesses hit cache testing revealed portion solaris cache cult dislodge repeated scans erent les approach works benchmark solaris cache manager holds pages rst accessed persistently behavior implementors intended investigation warranted search benchmark demonstrates nonlru replacement policies bene gray-box techniques performing search match string set les match found cached gray-box search nish quickly traditional search mercy ordering speci user scenario similar grep experiment reported experiment set arbitrary manner con gure illustrates maximum bene graybox approach matching string located cached speci command-line figure shows unmodi search advantage cache searches les order nding match gray-box search nds match quickly cache discussion investigating multiple platforms revealed level algorithmic knowledge assumed fccd largely unix-based operating systems relying primarily measurements probes determine state cache requiring detailed knowledge study highlights duality gray-box systems microbenchmarks tend unveil inner-workings systems fccd panacea major limitation techniques limited heisenberg ect gauge presence small page size cache bringing entire cache fccd probe small les returns fake high probe-time analogous heisenberg ect arises distributed system afs reading single byte force fetch entire local disk cache file layout accessing les disk exact layout les strong ect performance section investigate treat le-system layout algorithm gray-box developing layout detector controller fldc fldc layer applications order accesses improved performance based probable layout disk discussed earlier applications re-order accesses time improve disk performance purposes discussion focus smallle accesses scans large les amortize arm-movement overheads obviate re-ordering gray-box knowledge information exact layout inode le-block disk application re-order accesses reduce seek time rotational delay superuser privileges knowledge le-system structures reconstruct exact layout les raw-disk reads information hidden users applications fortunately experienced programmers gray-box knowledge les allocated disk modern unix le-systems direct intellectual descendents berkeley fast file system ffs ffs attempts lay les subsequent read performance optimized basic premise blocks meta-data les directory accessed ffs place cylinder group consecutive cylinders disk based algorithmic knowledge ffs simple heuristic reduce seek time group set les directory access order access order matter directory clean le-system small les created directory creation order matches data-block layout disk determine creation order option creation time resolution creation time cient multiple les created simultaneously option inode number i-number probe stat system 
call account ects le-system aging attempt discover layout predictor arbitrary creations deletions follow control technique gray-box systems moving system state refreshing directory writing les directory order hypothesize system state i-number order highly correlated data-block layout note directory refreshed small les rst small les assigned rst i-nodes directory large les presence lower correlation i-numbering data layout assigned i-nodes data blocks impact implementation approach implementation layout detector controller straight-forward application wishes access set les calls fldc layer desired set les fldc layer performs stat returns les i-number sorted order note sorting i-number essentially obviates sort directory verify low-overhead performing stat measured operation requires milliseconds disk access fact accessing group les single directory rst calling stat accessing data improves performance slightly inodes data blocks located separate regions cylinder group control component fldc layer refresh directory requires steps create temporary directory level hierarchy sort les original directory size user-speci cation copy les original directory sorted order update access modi cation times match original les make time-dependent programs operate correctly delete directory rename temporary directory directory experiments explore bene fldc clean aged le-systems experiments examine simple microbenchmarks ensure data meta-data cache begin reporting performance newly created le-system operating system platforms examine total time read small les evenly divided directories erent access patterns random ordering les les sorted directory les sorted i-number figure shows sorting directory improves performance relative random order sorting i-number leads dramatic improvements factor linux netbsd factor solaris hypothesize solaris pack data blocks small les tightly issues atomicity refresh operation crash occurs delete midst rename envision nightly script directory signature patches problems implemented linux bsd solaris time platform sensitivity file ordering random sort directory sort i-number figure file ordering matters gure plots total access time scan -kb les split equally directories experiment varies platform order access random bar ects access time les random order trial sort directory bar rst groups les directory accesses nally sort i-number bar rst sorts collection les i-number reads bars ect average measurements standard deviations shown spends time rotation measure ects le-system aging fldc create series epochs epoch random les deleted les created experiment les directory compare performance application reads les random order versus i-number ordering figure plots time application function increasing epochs epoch explicitly refresh directory graph shows random ordering performs poorly expected i-number ordering performs excellently fresh directory degrades epoch i-number ordering random epochs performance worse fresh directory performance factor directory refreshed epoch i-number sorting returns performance original level composition applications utilize fldc layer utilize fccd layer modifying grep process les order probable layout disk passing gbp -file commandline unmodi grep improves performance manner similar speed-ups figure shown due space constraints similarity fccd fldc interfaces leads natural question compose icls ordering les application rst access les cache access rest i-number ordering culty approach fccd explicitly identify les cache orders les probe time reliably discern in-cache out-of-cache les apply standard statistical clustering time aging epoch effects aging random sorted i-number refresh brings performance back excellent level figure undoing ects aging gure plots total time access -kb les directory linux platform upper line plots time access les random order lower line sorts les i-number rst x-axis age directory increased epoch random les deleted les created epochs directory refreshed copying contents temporary directory deleting directory renaming temporary directory probe times clustered groups minimizing intragroup variance maximizing intergroup variance form clusters clustering algorithm fast rst group predicted cache group predictions incorrect les disk group sorted i-number incorporated approach grep initial experiments performs rst accessing les cache picking order on-disk accesses provide avenue unmodi applications compose gbp utility discussion limitation gray-box fldc highly unix-centric utilizes i-number approach work platforms expose low-level information current implementation work non-ffs-based systems porting prove cult lfs icl advantage knowledge writes occur time lead proximity space open question refresh directory simple heuristic refresh directory randomly accesses refresh important directories nightly script ideally ascertain i-number ordering performing historical tracking perform refresh additional danger refresh operation applications i-number les directly applications active time refresh cease operate correctly safe refreshes invoked system start-up user guarantee processes running memory-based admission control virtual memory systems provide applications illusion unlimited amount memory memory resources heavily overcommitted illusion breaks system page parts memory swap entire processes disk desire service ensures running processes actively memory physically present section describe gray-box memory-based admission controller mac limits total amount memory allocated service components control mac ensures set processes allocate memory physically present admissioncontrol processes forced wait cient memory information mac noties applications amount memory applications adjust memory usage operating multiple passes mac atomically identi allocates memory avoid race conditions gray-box knowledge investigate approach process independently determines amount memory probing measuring time increasingly large memory accesses approach naturally leverages nition working set employed page-replacement algorithm mac observes memory accessed triggering replacement technique special conditions needed account memory erent competing processes erent purposes heap stack cache directly observe paging activity systems vmstat observe time order explore environments limited interfaces basic algorithm employed mac probe chunk memory page time sequential loops recording time page access note probes write page copy-on-write systems reads force pages allocated rst loop control technique moving system state directly infer accesses amount memory space access time include costs allocating zeroing re-fetching page disk loop access page fast mac infers chunk memory space pages selected replacement accesses slow mac infers amount memory large paged disk probing progressively larger chunks memory mac determine amount space assumption algorithm rate probes access memory approximately matches access rate application stable working set competing processes mac application pages resident probes write single byte page moving page application touches pages slowly result application allocated pages resident active competing process make mac slightly aggressive approach application rate mac probe pages order match rate subsequent access patterns approach cult cumbersome approach mac assume memory resident competing process active working set result cient mac wait access page determine paging occurring mac notices consecutive slow data points rst access loop predicts increasing working set activated page daemon size exceed amount suspicions mac immediately skips loop verify contents memory implementation mac low-level interface applications informed memory speci cally mac 
exports interface dynamic memory allocation alloc takes minimum maximum multiple bytes allocate returns pointer allocated space actual number bytes allocated null pointer returned minimum amount memory application adapt memory requirements speci identical minimum maximum amounts exposing control information low level processes respond application-speci manner lack memory cases anticipate applications simply allocate memory previous invocation fails waiting period time naive interface applications deadlock applications allocate half memory allocate memory releasing initial memory complete classic solutions deadlock prevention allocating required memory releasing memory allocation fails solve problem future plan investigate higher-level interfaces hide complexity provide fair allocation competing processes discuss challenges implementing mac erentiating in-memory out-of-memory probe times incrementing amount memory test iteration memory-di erentiation threshold determine paging memory disk occurring mac erentiate time write memory versus disk platform-independent manner unlike fccd collect probe times sort cluster times erentiate groups mac determine page-by-page basis probing reveals page memory approaches determining thresholds rst method values calculated simple microbenchmark run controlled environment advertised method microbenchmark run rst time mac contacted process measures time repeated accesses pages memory time non-resident page simply considered signi cantly larger erentiate paging scheduling activity found experimentally observing slow data points succession reliable indicator paging increment unit repeatedly accessing large amounts pages time-consuming mac probe substantially larger chunk memory iteration recovering large increment paging costly processes increase aggressive found good compromise initially increment search size conservative amount slowly double increment amount allocated memory found space xed maximum increment unit back completely original increment size problem detected analogous conservative tcp congestion-control scheme experiments linux platform extensively veri mac returns expected amount memory experiments show process allocates data accesses variety patterns mac reliably returns competing application applications repeatedly access allocated memory paging show mac behaves multiple competing processes simultaneously demanding application investigate copies fastsort sorting data sort adapt amount memory reading sorting writing sets records passes size pass memory determining number records memory complicated due fact linux shared virtual memory cache amount devoted virtual memory system records read written disk investigate traditional fastsort size pass speci command-line gb-fastsort modi mac icl gb-fastsort frees chunk memory allocating memory pass meshes interface deadlock minimum allocatable amount ensure arbitrarily small passes performed maximum equal total amount sorted gb-fastsort processes allocate memory simultaneously grab large chunks acquire minimum wait memory freed figure shows sorting extremely sensitive amount memory allocated pass overestimating amount severely increases amount time required complete workload perfect knowledge workload user determine amount correctly machine anticipate sorted pass processes paging passes paging signi slowdown compared runs robustness mac layer illustrated fact gb-fastsort exhibits paging activity read sort write phases experiments gb-fastsort average pass size close best-performing static version gb-fastsort time data sorted run gray-box sort performance write sort read overhead note gray-box version sort overhead component figure performance sort mac execute rst phase competing copies fastsort sorts million -byte records execute merge phase performance sensitive amount memory avoid contention disks process reads writes disk fth disk paging cache ushed test performance mac degrades rapidly memory allocated sorting pass size shown requires average minutes data point represents average experiments performs worse due alloc overhead signi approximately equal overheads present gb-fastsort designated overhead graph rst component time spent mac layer repeatedly probing memory increase number probed pages iteration operation proportional square number pages component time spent waiting memory compute-bound workload waiting time hidden increased throughput processes bound workload fastsort waiting time increases total time workload note performing admission control workload accounts improved performance write phase contention cache discussion mac important limitations implementation sensitive behavior underlying page-replacement process parameters mac tuned work linux interface assumes applications call pairs alloc free memory requirements change inform application memory call-backs cult programmers deal ciently ectively allocate memory application mac allocated memory malloc approach prevent thrashing competing application subsequently calls malloc amount larger memory gray toolbox support gray-box systems assembling collection tools operations performed frequently gray-box layers common operations implemented optimized implemented number utilities current toolbox imagine added time developers gain experience gray-box systems observing systems literature case studies tools building gray-box icls microbenchmarks con guration graybox icls require knowledge performance parameters underlying components amortize overheads set access unit le-cache content detector erentiate erent states determine page memory disk multiple icls parameters share information common repository microbenchmarks report performance numbers expected disk seek time expected disk bandwidth time allocate page time access page memory time access page disk common format persistent storage microbenchmark run performance suspected changed icl search desired information exist icl determine acquire information update common repository measuring output acquire information gray-box component icls studied measure time operations complete overhead obtaining elapsed time added operations important overhead low time operations complete quickly timer resolution issue provide fast timer speci current platform intel machines rdtsc instruction interpreting measurements observations output icl manipulate raw data infer current state found common data manipulations calculating simple statistics standard deviation median maximum minimum performing slightly sophisticated operations correlations clusters discarding outliers nally sorting due frequency important operations performed low time space overhead data collected time results continually monitored operations performed incrementally note douceur bolosky statistical sampler good candidate inclusion related work uential research projects investigated restructure operating systems extensible adopted systems solve problems incorporate functionality commercial operating systems restructured work investigated developers add functionality ways minimal interpositioning developers write code extensions invoked events cross interface boundaries implementing protected interposition agents requires small presence enhances number gray-box techniques applied speci cally interpositioning easily observe inputs outputs model simulate infer current state future plan investigate interpositioning gray-box icls disco virtual machine monitor modify run multiprocessor additional layer software inserted hardware multiple operating systems disco occasionally gray-box knowledge achieve desired information control disco developers irix enters low-power mode idle signal switch virtual processor finally visual proxies treat applications gray boxes changed insight gui-based applications ect internal state visual interface visual proxy snoop gui mirror internal state application visual proxy control application generating synthetic window events simulate user input conclusions systems longer developed isolation building local distributed service utilize interact software components control situation contrast past small group researchers implement operating system include compiler shell tools gray-box techniques 
building systems services live constraints gray-box icls encapsulate knowledge behavior observation statistical methods inference clients gain information state control behavior paper demonstrated utility gray-box techniques os-like services lecache content detector layout detector controller memory-based admission controller applications improve performance substantially icls source-code modi cation gray-box techniques broadly applicable local operating system distributed environments plan develop explore gray-box icls settings services developed gray-box approach revolutionary ideas require parts system remaining challenge determine types services implemented gray-box icl incorporated operating system acknowledgements bent burnett denehy forney popovici sivathanu anonymous reviewers shepherd monica lam excellent feedback work sponsored nsf ccrccr- itrand wisconsin alumni research foundation agarwal super scalar sort algorithm risc processors proceedings acm sigmod conference pages june anderson bershad lazowska levy scheduler activations ective kernel support user-level management parallelism acm transactions computer systems pages february arpaci culler krishnamurthy steinberg yelick empirical evaluation cray-t compiler perspective annual international symposium computer architecture iscapages june arpaci-dusseau implicit coscheduling coordinated scheduling implicit information distributed system acm transactions computer systems tocs aug babaoglu joy converting swap-based system paging architecture lacking pagereferenced bits proceedings acm symposium operating system principles pages paci grove december baker hartman kupfer shirri ousterhout measurements distributed file system proceedings acm symposium operating systems principles pages october balakrishnan padmanabhan seshan katz comparison mechanisms improving tcp performance wireless links ieee acm transactions networking december bershad savage sirer fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles december blackwell girshick theory games statistical decisions john wiley sons york bugnion devine govil rosenblum disco running commodity operating systems scalable multiprocessors acm transactions computer systems november cranor parulkar uvm virtual memory system proceedings usenix annual technical conference monterey california june douceur bolosky progress-based regulating low-importance processes procedings acm symposium operating systems principles december druschel peterson hutchinson micro-kernel design decoupling modularity protection lipto proceedings international conference distributed computing systems pages june engler kaashoek exterminate operating system abstractions proceedings workshop hot topics operating systems engler kaashoek toole exokernel operating system architecture application-level resource management proceedings acm symposium operating systems principles december floyd jacobson random early detection gateways congestion avoidance ieee acm transactions networking aug ghormley petrou rodrigues anderson slic extensibility system commodity operating systems usenix annual technical conference pages june ghormley petrou rodrigues vahdat anderson global layer unix network workstations software practice experience july howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems february jacobson congestion avoidance control sigcomm symposium communications architectures protocols pages aug jain delay-based approach congestion avoidance interconnected heterogeneous computer networks technical report dec-trdigital equipment corporation april jones interposition agents transparently interposing user code system interface proceedings symposium operating systems principles pages december kocher jun erential power analysis advances cryptology crypto annual international cryptology conference lecture notes computer science pages lampson note con nement problem communications acm october lampson hints computer system design operating systems review october litzkow livny mutka condor hunter idle workstations proceedings international conference distributed computing systems pages june mckusick joy fabry fast file system unix acm transactions computer systems aug meter gao latency management storage systems osdi october ousterhout scheduling techniques concurrent systems international conference distributed computing systems pages pang carey livny memory-adaptive external sorting proceedings international conference large data bases aug ritchie thompson unix time-sharing system communications acm july rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saavedra smith measuring cache tlb performance ect benchmark runtimes ieee transactions computers satyanarayanan flinn walker visual proxy exploiting customizations application source code acm operating systems review july seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating system design implementation osdi smaragdakis kaplan wilson eelru simple ective adaptive page replacement sigmetrics conference measurement modeling computer systems atlanta smith seltzer file system aging proceedings sigmetrics conference seattle june smith seltzer comparison ffs disk allocation policies usenix annual technical conference pages staelin mcvoy mhz anatomy microbenchmark proceedings usenix annual technical conference pages berkeley usa june usenix association talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley von neumann morgenstern theory games economic behavior princeton press princeton jersey edition worthington ganger patt wilkes on-line extraction scsi disk drive parameters proceedings acm sigmetrics performance conference measurement modeling computer systems pages zeller gray adaptive hash join algorithm multiuser envronments proceedings international conference vldb pages 
run-time adaptation river remzi arpaci-dusseau wisconsin madison present design implementation evaluation run-time adaptation river dataflow programming environment goal river system provide adaptive mechanisms database query-processing applications cope performance variations common cluster platforms describe system basic mechanisms carefully evaluate mechanisms effectiveness analysis answer previously unanswered important questions core run-time adaptive mechanisms effective compared ideal keys making work applications easily primitives finally situations run-time adaptation sufficient performing study utilize three-pronged approach comparing results idealized models system behavior targeted simulations prototype implementation providing insight positives negatives run-time adaptation specifically river broader context comment interplay modeling simulation implementation system design categories subject descriptors computer-communication networks distributed systems distributed applications database management systems parallel databases general terms design experimentation measurement performance reliability additional key words phrases performance availability performance faults run-time adaptation parallel clusters robust performance introduction successful application domains mapped parallel machines realm database query-processing resulted successful research projects barclay boral dewitt lorie commercially viable products industry baru tandem performance group teradata corporation success attributed relational model describing data natural structure codd relational model affords flexibility implementation dewitt gray state ideally suited parallel execution work funded years darpa -cdarpa -cnsf cda nasa fdnagwthe california state micro program nsf ccrauthor address arpaci-dusseau computer sciences department wisconsin madison dayton madison remzi wisc permission make digital hard copy part work personal classroom granted fee provided copies made distributed profit commercial advantage copyright notice title date notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee acm acm transactions computer systems vol february pages run-time adaptation river past parallel database systems tailored run specialized parallel hardware platforms recent networking technology transformed commodity clusters workstations called networks workstations nows viable platform tightly coupled parallel applications advent switch-based lowlatency high-throughput networks enabled deployment classes applications services modern clustered systems clusters number built-in advantages specialized parallel machines including higher performance lower cost higher performance incorporate recent microprocessors track moore law lower cost due economies scale mass production anderson clusters provide excellent alternative specialized parallel machines introduce range problems system designers difficulty arises complexity modern computer systems basic building blocks clustered systems including processors networks disks software increasingly sophisticated current top-of-the-line microprocessors tens millions transistors amd operating systems typically millions lines code increasing component complexity directly affects component behavior identical complex components behave identically arpaci-dusseau arpaci-dusseau identical disks made manufacturer receiving input stream necessarily deliver performance unexpected dynamic performance heterogeneity arise number factors including fault-masking capabilities modern scsi disk drives remapping bad blocks bad sectors hidden higher levels system altering performance drive disks purveyor heterogeneity similar behavior observed cpus bressoud schneider kushman networks arpaci-dusseau memory systems raghavan hayes software systems chen bershad gribble performance heterogeneity difficult overcome mixed parallelism due performance assumptions made parallel algorithms previous systems made simplifying assumption components system operate rate times assumptions common parallel database systems traditionally static data distribution schemes rangepartitions hash-partitions move data assign work nodes system dewitt graefe strong performance assumptions global operations perform rate slowest member group decreasing performance robustness system attempting prevent performance heterogeneity occurring river parallel programming environment takes variations performance faults account inherent design consideration arpaci-dusseau river basic dataflow programming environment substrate clusters goal enabling common-case acm transactions computer systems vol february arpaci-dusseau robust performance face arbitrary unforeseen performance faults components primary focus river provide support parallel database query-processing primitives dataflow programming environment fairly general broader class applications benefit river infrastructure key delivering robust system performance river run-time adaptation river adaptive data-movement mechanisms continually gauge react performance components system avoiding performance assumptions design specifically river core adaptive mechanisms distributed queue balances data flowing consumers system graduated declustering dynamically adjusts flow data generated producers constructs designed advantage performance characteristics modern high-speed networks moving data location processed river applications utilize tandem deliver consistent high performance spite unanticipated performance variations article present design implementation evaluation second-generation river system ganges make major contributions comparison previous work describe ganges prototype detail incorporates lessons learned implementation evaluation prototype euphrates importantly provide detailed study core river run-time adaptive mechanisms initial results presented iopads workshop promising arpaci-dusseau limited important questions left unanswered core run-time adaptive mechanisms river operate compared ideal keys making work applications readily apply mechanisms fashion robust applications finally limitations mechanisms answering questions generality efficacy river approach adequacy run-time adaptation solution performancefault problem answer questions article three-pronged analysis incorporate modeling simulation study addition extending previous implementation work models quantify ideal performance comparison ideal understand core mechanisms operate simulations study river distributed algorithms depth worry interference difficulties common implementation work implementation bring issues arise accurate simulations validate simulated performance expectations approach enhance ganges prototype improving performance river mechanisms roughly factor scenarios small factors important greatly enhance understanding software adaptation technology central river feel confident core mechanisms river robust acm transactions computer systems vol february run-time adaptation river fig performance comparison implementation versus number performance faults increased x-axis percentage peak performance plotted y-axis results older version compared improved algorithm improvement gained broader evaluation technique previewed figure plot performance versions distributed queue performance faults compare ideal model line labeled original plots data directly original paper performance euphrates implementation track ideal falling sharply high number performance faults ideal model place section easy improvement line presents ganges implementation algorithm tracks ideal perfectly previous fall-off interplay algorithm message-layer flow control study carefully simulation section resulting much-improved distributed queue implementation summarize major findings core run-time adaptive mechanisms river operate compared ideal find cases refined distributed queue graduated declustering algorithms operate gracefully delivering ideal performance number perturbation scenarios quantify limitations due limited replication best-case worst-case situations previous work measured best-case scenarios keys making work find keys effective operation river order run-time adaptive primitives function desired behavior communication layer network hardware critical control flow control implemented communication acm transactions computer systems vol february arpaci-dusseau layer order perform find river mechanisms require high degree parallelism application level excess parallelism current river mechanisms avoid performance faults successfully fortunately data-intensive applications achieve high degrees parallelism readily find local data-processing decisions guided global knowledge progress main difficulty lies obtaining global information distributed scalable manner finally designing river hardware platform recommend amount slack engineered order enable 
consistent high performance spite performance fluctuations applications readily apply mechanisms fashion robust applications question difficult fully answer implementing number database query-processing operators top river find programs employ distributed queue graduated declustering create robust dataflow find applications suitable transformations fundamental reasons multiphase applications automatically utilize stages pay cost data replication many-toone dataflow easily transformed robust counterpart adq finally limitations mechanisms study describe cases localized run-time adaptation river sufficient river adaptive mechanisms tolerate local performance faults hardware software global performance fluctuations network switches difficult impossible avoid network backplane primary avenue adaptation work system behave robustly find cases run-time adaptation short-sighted memoryless speculate long-term adaptation needed note problems general run-time adaptive systems specific river rest article organized section highlights river system motivation design section describes ganges implementation section develop model expected ideal performance describe experimental environment results presented answers questions outlined sections section discuss related work section conclude river system motivation design goal river programming environment enable construction applications exhibit performance availability provide mechanisms data-intensive applications adapt run-time performance fluctuations make high performance consistently end-users stated primary focus database queryprocessing primitives broader class applications programmed river environment section briefly describe design river system detail including motivation acm transactions computer systems vol february run-time adaptation river system description core adaptive mechanisms conclude section discussion technologies river design relies order provide flexible robust programming substrate discussion limitations environment motivation previous work field distributed parallel systems addressed design large-scale systems tolerate correctness faults individual components birman cooper borg englert liskov schneider notion work distributed systems consist multiple hardware software components periodically fail system works continuously top unreliable components operate spite failures raid storage tolerate failure disk continue correct operation patterson understood notion system functions components perform expected refer unexpected low performance component performance fault clusters disk drives main source performance variation scope data-intensive applications reasons disks clustered system exhibit static dynamic performance faults single disk disks include presence multiple zones meter scsi bad-block remapping arpaci-dusseau thermal recalibrations bolosky sporadic performance absolute failure talagala patterson contention due workload imbalance structural heterogeneity due incremental growth brewer documented arpaci-dusseau arpaci-dusseau hardware software components exhibit unexpected performance variations worse faults tend occur subset components system occur tend long-lived current systems support data-intensive applications interact performance faults systems built static techniques exploiting parallelism allocating data standard striping algorithms distributing data requests set disks place amount data disk identified problem static schemes make rigid performance assumptions relative performance components perform identically result small number components deliver peak performance performance entire service reduced slow entities performance availability states performance entire system track aggregate performance components system degrading gracefully performance faults systems provide proper run-time adaptive primitives support performance availability enable delivery excellent sustained performance spite localized component performance failures acm transactions computer systems vol february arpaci-dusseau fig now-sort perturbance best-case performance now-sort versus performance slight disk cpu memory perturbations performance results relative -node run now-sort delivers data near-peak disk rate run motivate global performance problems induced local performance faults report results simple experiment nowsort arpaci-dusseau high-performance parallel external sort clusters experiment sort runs machines run perform slight perturbation sort machines results perturbation experiments shown figure graph perturbations single machine global performance effect single file single machine poor layout tracks versus outer performance drops single disk hot spot competing data stream performance drops factor cpu loads machines decrease performance proportional amount cpu steal excess memory load machine begin thrashing factor performance lost build system avoids situations balancing load system perfectly times meticulously managing resources system difficult system size complexity increase carefully managing system challenging impossible approach problem manner assuming presence performance faults providing substrate enables applications operate spite river environment river generic dataflow programming environment clusters workstations similar basic design previous parallel database environments gamma volcano dewitt graefe applications constructed piecewise fashion modules acm transactions computer systems vol february run-time adaptation river fig module api simple river module module messages upstream performs operation calling user-defined filter conditionally put messages downstream fig flow api simple reader-to-writer flow user calls place add module node dataflow graph attach connect modules finally calling program begins run ufsread module reads collection output input ufswrite module writes disk module logical thread control input output channel inside module called obtain data upstream source put called pass data downstream begin execution application control program constructs flow connects desired modules sources sinks instantiated computation begins continues data processed examples module flow figures enable applications cope performance faults deliver consistent high performance river distributed software constructs distributed queue graduated declustering dynamically adjust data transferred set producer modules processes parallel application set consumer modules set disks run-time goal tolerate performance faults transfer producers consumers constructs river solve problem cope performance-faulty consumers performance-faulty producers tandem flow build performance-robust applications acm transactions computer systems vol february arpaci-dusseau design river explicitly provide mechanisms deal absolute failure handle absolute failure applications restarted advantage checkpointing package barclay litzkow discuss desired behavior distributed queue graduated declustering detail distributed queue transferring data set producers set consumers consumers suffer performance fault proper reaction case move data consumers proportional relative bandwidths faster slower functionality provided distributed queue application writers insert program dataflow tolerate consumer faults arrive design constraint-free transfer data arbitrary number producers consumers envision scenario distributed queue producers consumers distributed queue behavior data queue producers consumers note ordering data guaranteed point-to-point producer places consumer receives receive strictly speaking bag queue terms performance ideal distributed queue deliver data consumers rates proportional rates consumption fixed time interval consumes rate rate ratio data received compared rates consumption consumers change dramatically time subject performance faults quickly adapt figure presents logical structure control dataflow producer data distribute set consumers consumer queue incoming data blocks producers process consist producers consumers data transferred parallel set producers set consumers part queue flow block behavior system determined decisions potentially requires global information producer chooses consumer receive data block show important decision performanceavailable distributed queue algorithm consumer choose order process blocks received graduated declustering corollary problem occurs data transfer producers consumers producer suffers performance fault case alternate data source producers bottleneck transfer acm transactions computer systems vol february run-time adaptation river fig data 
control basic data movement control options distributed queue producer wishes send data block faced decision consumer data decided data block consumer consumer receives blocks producers faced control option order blocks processed consumers block received consumer sends reply producer required acknowledgment-based reliable communications protocol fig data control basic data movement control options graduated declustering consumer requesting block faced decision producer request request selected producer requests consumers producer faces decision queue serviced serviced data back requesting consumer data transfer runs rate single slow producer data source replicated river applications employ graduated declustering data transfer mechanism carefully divides producer bandwidth equally consumers performance availability producer faults parallel reads mirrored on-disk data sets applied replicated in-memory data sets figure presents logical structure dataflow graduated declustering consumer sends request block replicated acm transactions computer systems vol february arpaci-dusseau sources block producer choose order handle consumer requests decisions important respect behavior algorithm formally describe desired behavior graduated declustering assume subset data set replicated copies subset refer n-way graduated declustering assume producers cases physically colocated entities producers spread cluster machines assuming extra machine resources capacity data set replication assume machines producers running machine producers icp icp replicas assume remote machines consumers consumer consumes portion data set produced replicas note bit distributed queue data producer consumer number producers consumers necessarily equivalent assume producer produces data rate producers machine share resource sum machine equals rate bottleneck resource total bandwidth machines max idp number perturbations added system bandwidth perturbation performance fault resource bandwidth assume faults present system total bandwidth consumers avail idp idf goal graduated declustering bandwidth divide equally consumers avail division accomplished consumers proceed rate finish data transfer instant performance faults system tolerated allocate producer bandwidth cope producer-side performance faults shown figure global redistribution accomplished local producer bandwidth adjustments total bandwidth resource divided producers share resource replica producers share piece flexibility apportion bandwidth producers acm transactions computer systems vol february run-time adaptation river fig dataflow alleviates problem producer-side performance faults disks producers data mirrored dataflow shown control flow consumers request data items producers depicted figure omitted sake clarity producer produces data sets consumer consumes data set performs half expected rate producers compensate bandwidth allocations consumers receive fair share aggregate bandwidth enabling technologies application writers ability construct flexible dataflows moving data computation processed current state system river design relies number recent hardware software technologies order deliver desired behavior applications river designed advantage high-throughput system-area networks myrinet boden advent networks altered clusters computers loosely coupled distributed systems tightly coupled parallel systems high-performance interconnect river degenerate system processing performed locally moving data machine costly merit consideration scenario river environment build applications dataflow model flexibility afforded river mechanisms unutilized river design integrates cleanly top active messages substrate von eicken mainwaring culler designed export raw power high-performance networks natural match river aspects river implementation discussed section direct advantage request response nature protocol focus river primarily high-throughput database queryprocessing applications relationships disk performance network performance river relies network move data rate local disk enabling flexible dataflow source sink cluster throughput streaming data disk machine process machine close equal throughput streaming data disk process machine note river rely heavily acm transactions computer systems vol february arpaci-dusseau low-latency aspects modern networks afford applications stream large data sets throughput oriented bisection bandwidth network equal sum disk bandwidth cluster requirements unreasonable hold environment sequential disk bandwidth improving rapidly grochowski network switches links buses pace arpaci-dusseau finally natural question river benefits realized standard tcp ip-based ethernet network type environment complexity software protocol stack hardware switches introducing additional overheads communication efficient global communication key river overheads penalize design freely moves data cluster long technology relationships paragraph hold benefits river approach realized limitations river programming environment strive make river mechanisms general number limitations programming model flexible data-distribution mechanisms river suited parallel applications naturally programmed dataflow style database queryprocessing primitives selects sorts joins natural candidates primary focus application study section wolniewicz graefe shown scientific parallel operators work dataflow model dataflow model ideal clustered services building internet service search engine web proxy specialized environment fox type application differs traditional dataflow model small requests processed concurrently contrast focus single parallel application time building cluster environments concept performance availability considered similar performance problems encountered birman primary mechanisms performance robustness river distributed queue graduated declustering limited applicability applications flexibility manner order process data database query-processing primitives true design advent relational model decouples manner computation performed specification desired result codd parallel applications property shown large body work flexible parallel programming environments studies existing scientific codes poole randall chakrabarti parallel applications readily programmed dataflow model acm transactions computer systems vol february run-time adaptation river case rewritten utilize performance robustness finally obvious limitation data set served fully replicated cost producing replica inhibit envision applications primarily access frequently read on-disk data collections data form redundancy reliability mirroring on-disk data expensive terms space usage disk costs decline simplicity performance advantages mirroring increasingly work favor parity-based schemes save storage space approaches amenable gd-like performance-robust technique requires alternate data source readily serve data performance fault reading block parity-based approach data block read block regenerated reading blocks parity block stripe costly note extended function partial data set replica data records replicated added complexity case performance robustness fall nonrobust mirrored system full replication ganges implementation ganges implementation second-generation prototype river dataflow system section describe ganges focusing implementation distributed queue graduated declustering ganges implemented distributed algorithms provide desired functionality global coordination centralized control algorithms distributed order scale large clusters centralized approach fundamentally provide performance availability single slowdown central master component lead global performance problems ganges implemented cluster sun ultra workstations running solaris workstation consists mhz ultrasparc processor tremblay memory seagate hawk rpm disks attached fast-narrow scsi bus s-bus disk commonly swap space system data bandwidth hawk ranges outer tracks workstations connected high-speed myrinet local-area network boden workstation single myrinet card s-bus cards capable moving data workstation approximately entire system connected collection -port myrinet switches arranged -ary fat tree communication performed active messages mainwaring culler exposes raw performance myrinet integrating features threads blocking communication events multiple independent endpoints acm transactions computer systems vol february arpaci-dusseau distributed queue important 
aspect distributed queue implementation data transfer protocol implementation possibilities producers push data consumers consumers pull data producers information exchanged relative rates execution consumers main concern construction data transfer protocol make performance assumptions producers rely priori performance characteristics consumers describe implementation key ideas combined arrive algorithm randomness picking equivalent consumers feedback flow control critical element attain desired behavior consumer perturbation producer data send consumer calls put routine internally steps producer checks total outstanding messages network outstanding determined threshold producer waits returned credits producer pick consumer send data picking random consumer checking outstanding messages consumer consumer producer picks consumer repeats check producer proceeds performs step algorithm send data chosen consumer receive side implementation straightforward consumer waits message arrive event mechanism active message layer message arrives wakes extracts message network polling message packaged returned consumer sends reply producer indicating received message producer receives reply updates flow-control counters mentioned decision made occurs messages single producer case service producers proportion current rate progress receive higher proportion service details presented section discussion key avoids assumptions performance consumer run-time adaptation producer utilizes feedback consumers gauge consumer data specifically producer sends data consumer form active message request protocol stipulates consumer reply message reason levels credit management simplicity credits desired behavior producer wait message return provide simple check condition reply message required protocols utilize acknowledgments implement reliable message transfers acm transactions computer systems vol february run-time adaptation river reply signal channel implicit information arpaci-dusseau consumer handled data request producer infers remote consumer making progress producer monitors incoming replies infer current performance level consumer algorithm property unifying seemingly diverse implementations push-based pull-based approaches found message-passing layers karamcheti chien producers bottleneck data transfer consumers respond quickly data requests choice destinations degenerates random choice entire set consumers push-based approach random choice shown good similar scenarios brewer kuszmaul consumers bottleneck producer messages outstanding consumer reply returns implicitly pulls data request replying consumer depending relative performance rates producers consumers utilize push-based pull-based approach functionality distributed queue providing simply load balancing studied extensively literature adler blumofe johnson wen effective load-balancing algorithms performanceavailable make performance assumptions form meet demands load-balancing performance availability isomorphic centralized scheme single machine rendezvous point matching producers consumers data item performance assumption algorithm makes centralized match-maker suffer performance faults performance entire system suffers advanced load-balancing schemes proposed adler johnson algorithms utilize probe-then-send model scheme consumers chosen uniformly random queried current queue length replies filter back producer picks consumer data queue sends data performance assumption family algorithms makes probes return timely manner probed consumers exhibits performance deficiencies result producer spends time waiting response query graduated declustering describe ganges implementation graduated declustering start consumer-side algorithm straightforward n-way graduated declustering consumer partition data set choices request data item jumpstart process consumer sends requests fixed number blocks distributing round-robin fashion producers sending requests producers acm transactions computer systems vol february arpaci-dusseau request producer request producer request producer request producer crucial adaptation consumer-side occurs replies back producers reply data block returns producer consumer requests data item producer producers responsive consumerside algorithm adapt responsiveness requesting data active producer aspect consumer-side algorithm important request consumer sends extra information producer progress metric small amount extra knowledge crucial producer determining consumers requests served progress metric total number bytes received consumer previous implementation average bandwidth consumer received experimentation found total number bytes robust reliable metric producer-side algorithm bit complex producer represented components set service threads scheduler thread service threads replica receive requests scheduler thread read data disk data source send data back directly consumers note n-way graduated declustering machine receive requests consumers component scheduler thread thread crucial proper operation graduated declustering examines progress metric consumer biases scheduling requests catch lagging consumer producer serving requests consumers received blocks data received blocks blocks producer caught discussion distributed algorithm implements graduated declustering based intuition producer balance progress metric consumers delivers data producers strive localized balance global balance consumers achieved key success producer-side scheduler directly charge biasing occur choice progress metric central correct operation current implementation consumer piggybacks information consumer requests order producer decide consumer receive proportion service current metric total number bytes received consumer metrics important aspect metric give notion global progress final goal average bandwidth fits definition partition data set roughly identical size future metric exposed applications internal implementation application-specific scheduling acm transactions computer systems vol february run-time adaptation river methods models study behavior river employ simulations implementation compare results simple idealized model system behavior original study arpaci-dusseau based solely results prototype implementation suffered conclude mechanisms river generally compared nonadaptive approach true upper bound performance implementation details difficult separate inherent behavior distributed algorithms remedy problem develop idealized model system behavior performance faults plotting results versus models gauge system absolute sense relative nonadaptive system models difficult satisfied system performance remedy problems utilize simulation simulations explore basic algorithmic behavior wide range system parameters feasible measure prototype measurements implementation gathered confirm simulation results bring systems issues modeled simulations combination techniques improves understanding separating intrinsic properties algorithms implementation details model system behavior develop model behavior ideal system performance faults ideal system adapt instantly performance fault move work data utilize system resources limited underlying hardware resources comparing experimental simulation results model ideal system gauge algorithms implementations functioning note model simple general applied evaluating behavior systems performance faults assume device expected deliver performance rate peak unperturbed time interval interest device suffering performance fault fault amount resources term rate fault fault fault peak resource undergoing performance fault view fault entity utilizes portion resource akin application utilizes resource delivered rate performance performancefaulty device peak fault disk deliver peak bandwidth peak suffer fault takes fault leaving applications characterize strength performance fault define fault utilization fault fault peak fault utilization ranges note absolute failure special case performance fault fault fault peak river concentrate range fault acm transactions computer systems vol february arpaci-dusseau fig performance availability spectrum performance increasing number performance faults x-axis number components experiencing performance fault increased maximum number components system system performance fraction ideal performance setting free performance faults plotted y-axis final definition required developing model behavior faults application utilization resource assume application interest runs rate app resource period interest application o-intensive 
app rate reads data disk read phase peak peak rate resource deliver fault faulty performance level application app app peak fraction resource unperturbed note app precisely resource heaviest demand application cpu computebound calculation phase disk strictly o-bound portion program understand behavior ideal system increasing number performance faults plot graph called performance availability spectrum shown figure x-axis graph increase number components experiencing performance fault simplicity notation assume performance fault occurs faulty component application utilizes fraction resources component peak rate component identical restrictions easily relaxed desired y-axis plots system performance fraction ideal performance nonperturbed perfect conditions application completes phase interest time number faults system graph plots ideal increasing x-axis ideal minimal time algorithm complete operation equal acm transactions computer systems vol february run-time adaptation river examining read phase external one-pass sort ideal time read data disks peak rate comparison point ideal spectrum differentiate poorly performing algorithms perform basic approach developing model ideal behavior simple two-step process derive point system fully utilized faults present application point sum utilization faults system application total amount resources point system duress ideal case application performance loss application resources continue unperturbed derive point components experiencing performance fault points place generate complete piecewise-linear model ideal performance point interest derive labeled loss figure point ideal system begins performance degradation due presence performance faults point perturbed resource cpu network link disk fully utilized components system point ideal system move work data suffer performance loss point performance degrades compared nonperturbed system due lack resources solve loss point resource utilized components sum application resource usage perturbation resource usage components equal app peak loss fault peak solving loss arrive loss peak app fault terms utilization loss app fault present simple make concrete imagine cpu resource interest assume parallel application runs cpus cluster process application utilizes cpu unperturbed system app peak application total cpus full cpus assume performance fault interested fault utilization fault peak fault utilizes cpu substituting values equation arrive loss half cpus perturbed expect performance drop peak make general observations loss degenerates app app peak observation matches intuition application resource unperturbed case perturbation system leads loss performance extreme app fault peak app fault loss greater plainly stated sum application resource utilization performance-fault resource utilization total amount resources components perturbation slowdown experienced ideal case point interest figure components collection experiencing performance fault call y-axis acm transactions computer systems vol february arpaci-dusseau denote performance level ideal system components incur performance faults derive point components perturbation rate node deliver perturbation peak fault calculate directly observing application slowdown resources overtaxed rate delivered divided rate needed application peak fault app terms utilization fault app application utilizes cpu performance fault utilizes nodes perturbed total ideal system performance general application increase decreases similarly fault utilization increases decreases points interest loss expect linear drop performance drop performance linear points amount resources application increases linearly resources correspond directly performance application disk bandwidth write phase slowdown factor ideal case derive slope line connects points express expected ideal system behavior closed form loss loss loss loss piecewise linear model ideal performance set performance faults judge absolute performance system simulations prototype implementation limitations model describing simulation environment briefly discuss primary limitations model address assumption linear slowdown faults model assumption hold depending scaling properties application question model application half bottleneck resource assumed application run half rate full resource holds applications interested general applications easily utilize fewer resources perform worse expected total number resources increased singh model assumes manner perturbations occur components exponentially worsen performance case parallel programs arpaci communicating processes coscheduled ousterhout performance worsen orders magnitude progress acm transactions computer systems vol february run-time adaptation river application depend progress specific process comparing ideal realistic assume program attain ideal uncorrelated faults multiple components model assumes throughput response time performance metric interest result assumption systems deliver throughput identical model response time costs queueing reflected performance analysis practice found assumptions adequate database query-processing applications focused good scaling behavior require coscheduling good performance process large volumes data throughput-oriented limitations found model invaluable tool development system providing performance target ideal model focuses performance tuning letting designer aspects system improved good model lets designer stop tuning performance approximates ideal fine-tune system simulation environment addition models measurements real implementation employ set simulations demonstrate properties distributed algorithms simulator constructed low-level primitives queues consume data user-specified rates sources generate data user-specified rates simulations distributed queue graduated declustering readily constructed components simulator core consists event-based simulator event subsystem written efficiency rest simulation system including queue source abstractions callbacks glue code written tcl ousterhout tcl affords great flexibility assembling arbitrarily complex simulations ousterhout enables visualization animation scenarios effective adaptive mechanisms river sections answer questions posed introduction basic goal understanding run-time adaptation approach robust system design stated utilize results simulations prototype implementation comparing ideal begin exploring general effectiveness river mechanisms presenting results experiments test performance increasing number performance faults experiments utilize simulation implementation results show match closely results extend previous work number axes enhancing understanding simulation showing mechanisms acm transactions computer systems vol february arpaci-dusseau fig distributed queue performance results simulation implementation presented compared ideal performance increasing number performance faults performance static transfer experiments producers generate data nonfaulty consumers consume data performance fault reduces consumer performance factor work ideally compared model displaying sensitivity layout performance faults demonstrating improved performance second-generation implementations distributed queue earlier section distributed queue presents applications high-speed backplane data sharing tolerate consumer-side performance faults application producers place data distributed queue consumers receive data consumer logically receive data block put central challenge design scalable efficient distributed algorithm moves data producers consumers faster consumers receive proportionally higher amount aggregate producer bandwidth figure illustrates performance simple perturbation scenario compares performance ideal static nonadaptive approach experiments producers send data remote consumers total machines producers generate data nonfaulty consumers sink data setup emulates set processes producers writing in-memory records parallel disk disks consumers bottlenecks transfer x-axis increase number consumers undergoing performance faults performance fault reduces rate consumer factor consumer performance reduced leftmost point x-axis shows performance system performance acm transactions computer systems vol february run-time adaptation river faults rightmost point shows performance consumers perform perturbed rate y-axis plots performance fraction 
ideal unperturbed case faults system ideal simply aggregate bandwidth consumers ideal performance line reflects model developed earlier note loss experiments test utilizes consumer bandwidth unperturbed case case ideal system drops performance single fault shown figure static transfer producer static hashing algorithm pick consumer send data performs faults system point throughput high scales machines single fault performance static approach drops immediately slowest consumer transfer complete consumer finished portion global task performance faults added system performance stays low level shown figure utilizing implicit information run-time adaptation simulated performance distributed queue performance faults excellent delivering ideal performance range induced faults figure shows simulation results closely match implementation running experiment implementation ideal datapoints graduated declustering distributed queue tolerates consumer-side performance faults moving data faster consumers suffers producer deliver data expected rate data locale producers bottleneck data transfer solution problem cases data replicated sake reliability mirrored disk system data block locations graduated declustering coordinate access replicated data set dividing aggregate producer bandwidth equally consumers lessening effects producerside performance faults figure shows performance simulation implementation simulations performance-fault layouts presented worst case faults occur adjacent producers immediately affect data sources consumer case distributed producers faults consecutive producers half producers perturbed implementation results shown layout note previous results presented best-layout performance capture range behaviors performance layout faults good ideal due limited amount replication producer contained acm transactions computer systems vol february arpaci-dusseau fig graduated declustering performance results simulation implementation compared ideal performance increasing number performance faults static transfer experiments nonfaulty producers generate data consumers consume data performance fault reduces producer performance factor due limited replication layout performance faults matters implies spread worst occur consecutively producers data set flexibility ideal performance generally level replication data item number producers system algorithm tolerate presence performance faults worst layout potentially tolerate faults layout real system randomly faults delivered performance fall bestand worst-case lines keys effective run-time adaptation discuss keys success run-time adaptation key careful management interaction communication layer communication heart adaptation river managing interaction river distributed algorithms flow control crucial robust performance key presence excess parallelism application level robust performance faults river applications move reasonable number objects fewer objects transferred performance fault unexpected deleterious effect fortunately data-intensive applications trouble meeting requirement key presence globally aware data scheduling entity participates robust data transfer monitor progress transfer compensate laggards ends goal scalable distributed implementation finally fourth key presence slack system system driven full rate acm transactions computer systems vol february run-time adaptation river fig flow control amount flow control varied set perturbation simulations line plots performance perturbation number flow-control credits total number credits producer number perturbations increased x-axis credit consumer tolerate full range perturbations experiments producers consumers rate unperturbed production consumption fault reduces rate consumption performance fault lower performance ideal system system engineered amount extra capacity adaptive mechanisms river utilize unexpected problems occur deliver performance consistently high rate keys led refinements algorithms communication layer crucial aspect implementation distributed queue graduated declustering behavior underlying message layer message layer restrict number outstanding messages mechanisms layer performance perturbation expected message layers including restrict number outstanding messages arbitrary number chosen message-layer developers worse number hidden clients communication layer render construction distributed algorithms impossible investigate number flow-control credits needed mask performance faults figure plots results simulations vary amount flow-control credits results similar shown simulations reveal importance flow control algorithm unperturbed case performance faults system face perturbation acm transactions computer systems vol february arpaci-dusseau unperturbed case outstanding credits producer performance drops ideal surprisingly producer outstanding messages fully pipeline system interestingly number faults increases number credits producer allocated increase cases number flow control credits equal number perturbed consumers performance suffers reason drop-off straightforward producer message credits outstanding slow producers algorithm remember nodes slow nodes sending messages slow nodes simulations conclude distributed queue algorithm outstanding message consumer delivers producer perspective remote performance consumers straightforward simple effective manner perspective prototype underlying message layer restrict number outstanding messages number consumers system current implementation hard limit total outstanding messages prototype tolerate performance faults consumers excess parallelism essential ingredient tolerating performance faults presence excess parallelism system producer consumers items data send perfect distribution distributed queue leads piece data consumer performance dictated rate slowest consumer realistic settings excessively perturbed nodes lead end-of-run effects first-order performance factors explore amount parallelism needed tolerate performance faults figure plots simulated performance graduated declustering single performance fault function total amount data read results identical focus solely experiment graph small amount data perform time perturbed producer deliver remaining blocks finished takes significant proportion run-time consumer send request data producer slowest producer receives messages level replication data set mirrored disk system producer processes messages end effects noticeable work system total amount work small performance faults difficult tolerate current river approach number arises hardware limitation myrinet network-interface card tandem design decisions acm transactions computer systems vol february run-time adaptation river fig excess parallelism performance single perturbation plotted function total amount work pushed simulated blocks block consumption rate perturbed producer unperturbed producer cases small number blocks pass system vulnerable performance faults delivering performance noticeably ideal simulations producers consumers note performance system deliver ideal experiment single fault present view problem weakness run-time methods river data nodes system data requested nodes operate faster problem partly remedied historical methods remember nodes performed poorly past bias actions start node system performing poorly inclusion data transfer offer gain node avoided altogether current mechanisms river memoryless runs relearn run performance characteristics system performance faults lengthy duration past performance predicts future performance historically based techniques effective solution write applications utilize parallelism amortize end-of-run effects found overly burdensome range data-intensive applications addition added complexity including historical approach warrant resultant small performance benefit globally aware local scheduling algorithm producer made aware progress consumers serves biasing scheduling data blocks lagging consumer producer make purely local decisions coerce system common global goal original algorithm acm transactions computer systems vol february arpaci-dusseau fig globally aware local scheduling average producer bandwidth plotted lifetime experiment experiment producers send data consumers consumers receive data varied lines rates producer receives roughly fair share bandwidth finish time producer sends data contention network begins leading unfair bias producers distributing progress information consumer clever local 
scheduling solves unfairness problem believed global awareness demonstrate requires globally aware local scheduling contexts figure plots performance experiments prototype system experiment producers sending data consumers experiments rate consumer sinks data controlled y-axis figure plots relative share aggregate consumer bandwidth producer numbered x-axis receives ideally producer receives equal share bandwidth reflected consumer rate producer receives fair share consumer bandwidth expected producers left graph receive notably higher portion consumer bandwidth receive correspondingly problem inherent original design firstcome first-served algorithm processing blocks consumer interaction myrinet switch fairness properties performing detailed measurements implementation found high loads participating nodes myrinet switches fairly schedule transfers producers receive fair share network bandwidth performance determined rate slowest entity unfairness performance system drop producer finish transfer acm transactions computer systems vol february run-time adaptation river overcome problem piggyback extra information messages specifically sending information rate progress producers consumers data request response consumer schedule service informed blind fcfs manner initial results modified performs consumer-side scheduling shown figure biasing service lagging producers performance level producers unfairness problem solved note behavior discovered simulation simplified simulations model switch behavior detail underscoring importance combined approach slack system finally explore slack system formally excess performance capability components slack single performance fault system reduces performance idealized model real results figure slack present run-time adaptive methods river exploit deliver consistent high performance applications figure shows results simulations unperturbed rate consumer varied graphs experiments producers generate data fixed rate peak rate consumers performs ideally graphs surprisingly consumer rate increases faults required induce performance decrease compared unperturbed system generated similar results omit resulting dilemma designer excess performance capacity engineer system decision scope current work answer question develop stochastic models modern device behavior apply models analyze behavior system enable probabilistic guarantees system behavior similar birman work bimodal multicast plan investigate work fail-stutter fault tolerance arpaci-dusseau arpaci-dusseau designer system consistent performance provided slack system engineered simply provide applications case mechanisms river graceful degradation performance failure river mechanisms applications examine system application level easily applications utilize order create robust applications study concentrate database query-processing primitives present performance results range query-processing primitives built top prototype implementation river comparing performance ideal regard previous results arpaci-dusseau present acm transactions computer systems vol february arpaci-dusseau fig slack results simulations compared ideal performance increasing number performance faults experiments producers generate data nonfaulty consumers consume data performance fault reduces consumer performance factor nonfaulty consumer performance increases faults required reduce performance peak advantage slack system predicted model results greater number applications improved implementations improve performance robustness applications discuss difficulties utilizing basic river mechanisms perfect match application semantics find suitable creating robust programs application performance general develop robust river application programmers insert distributed queues graduated declustering applications form points flexibility places dataflow performance faults tolerated application writers focus constructing flexible flows infrastructure handles rest utilizing core river mechanisms applications potentially withstand performance faults achieve ideal performance range faults study present performance o-intensive parallel database query-processing primitives transformed static parallel application robust adaptive version figure presents results primitive figure performance application increasing number disk performance faults shown acm transactions computer systems vol february run-time adaptation river fig application performance results running database primitives -machine cluster disk performance faults applications parallel scan data set read parallel generation data set write parallel filter read data disk select records based user-defined function write selected records disk top-n selection find top values data set find efficiently parallel hash-join data sets finally parallel external sort application operates roughly data disk node total machines performance fault reduces performance factor datapoint represents single run runs shown point x-axis best-fit line data plotted acm transactions computer systems vol february arpaci-dusseau fault disk utilizes half disk bandwidth applications ideal amount data touched application divided peak rate disks reasonable harsh ideal applications data intensive compute time negligible parallel scan reads total disks run ideal time scan seconds figure applications performance good faults system degrades gracefully faults imposed disks desired trend specifically performance ideal entire range disk faults applications examining leftmost point graph gain insight overheads utilizing river mechanisms provide performance robustness examine simpler applications scan scan simply parallel scan input data case processes read data disks read-only application robust version scan utilizes access data mirrored collection graph noticeable overhead roughly compared ideal application process reads data single disk overhead attributed change workload presented disk static nonrobust scan disks serves single sequential stream data single process robust scan disk serves streams processes due replication serving streams additional seeks induced lowering performance examine generate viewed converse scan parallel data generator processes generates random records writes disk robust version generate distribute load disks tolerate disk performance faults leftmost point generate application delivers ideal performance showing slight acceptable degradation applications exhibit complex combinations basic costs utilizees dqs robust versions exception found sort utilizes coarse-grained manner applications exhibits slightly worse performance characteristics expected application semantics top-n selection examine cases application semantics perfectly mesh mechanisms focus top-n selection top-n selection selects top data items collection based user-specified key small number queries form common databases internet search engines generating large set candidate results order results based quality metric present top results user acm transactions computer systems vol february run-time adaptation river fig top-n data flow static robust versions top-n selection presented top static nonrobust version application data read disk disk-read modules passed sort modules buffer amount data generating sorted run writing disk disk-write modules runs generated data transformed set sorted runs short final phase runs merged produce top data items user desires data merge continued sorted runs bottom diagram depicts disk-robust first-phase top-n selection robust mechanisms employed tolerate read side performance faults disks sort modules tolerate performance faults write side figure presents basic dataflow top-n selection phase sorted runs generated reading entire data set sorting block time read memory writing sorted run disk phase completes merging top records sorted runs final result approach user request items quickly supplied continuing merge challenge programmer make program robust disk performance faults begin phase figure shows diskrobust version phase top-n graduated declustering distributed queue utilized order make phase program robust utilizing replicated data set producer faults easily handled inserting immediately data moved 
faster sorters rates limited local disks fully disk-robust phase generated results figure confirm note ways implement top-n selection present method acm transactions computer systems vol february arpaci-dusseau find adding robustness merge phase program difficult reasons general apply applications order utilize replicate sorted runs written replication highlights general weakness run-time adaptive approach multiphase program pays high cost replicate data writing disk wishes subsequently avoid producer-faults read phase suited oft-read data sets replicated read times addition data merged single merge module prints final output module runs slowly reason application run slowly phase utilized single destination data applications dataflows similar minimize time spent portion dataflow weaknesses mind case application performance highly robust due amdahl law time spent phase large data sets disk greatly slowed phase application performance suffer unduly sorting present challenging operator external diskto-disk sorting general sorting good benchmark clustered systems stresses disk memory interconnect bandwidth section one-pass version sort two-pass sort consists multiple runs one-pass sort merge benefits development precedent set researchers measure performance sort key values uniform distributions assumption implications method distributing keys local buckets processing nodes nonuniform distribution modify implementation perform sampling phase sort blelloch dewitt sampling phase made robust topmost diagram figure presents flow data standard version sort based flow now-sort world-record breaking parallel sorting program clusters arpaci-dusseau data begin unsorted parallel collection number disks data read disk node disk read module passed range-partitioning module partitioning modules perform key-range partitioning data partitioning module reads top bits record determine sorter module record sorter module received input sorts data begins streaming disk write module proceeds write data disk stream preserving order read-sort-write phase repeats data transformed series sorted runs enhance sort disk-robustness utilize graduated declustering distributed queue shown bottommost diagram acm transactions computer systems vol february run-time adaptation river fig sort dataflow data read disk parallel set disk-read modules passed set range partitioners partitioners segment data set sort modules key sort modules present top fourth keys sorter fourth sorter sort modules received large chunk data fill memory independently sort data pass disk-write module output disk multipass sort phase repeat data sorted sorted runs facilitate robustness disk performance faults sort employ graduated declustering distributed queue graduated declustering utilized transparently transforms sort read-robust sort distributed queue complex data sorted sorters place data distributed queue standard data randomly scattered disks essentially undoing work sorting performed slightly distributed queue utilized sorter handing records distributed queue hands distributed queue entire sorted run time load balancing occurs coarser granularity preserving semantics sort figure case previous operators employ graduated declustering disk read provide performance-robust parallel data stream program addition distributed queue complex figure observe queue sort modules diskwrite modules sort modules passed sorted records distributed queue programs application perform expected distributed queue algorithm spread records randomly disks undoing work sort distributed queue sort modules key-range partitioning occurs crucial semantics sort removing acm transactions computer systems vol february arpaci-dusseau key-range partitioning change correctness sort distributed queue position placement function properly slight modification made distributed queue handing records time distributed queue sort module passes large sorted chunks data distributed queue distributed queue adapts rate disks coarser granularity sort module received data sort divide ten chunks note slightly form output one-pass sort n-node sort generates sorted runs n-node sort produces runs number sorted runs sort modules hand distributed queue performance cost extra work sort perform files opened closed standard sort figure presents results perturbation experiment figure performance perturbation stable programs attribute directly coarser granularity load balancing disks runs balance disks single slightly faster disk end noticeably larger amount work general performance degrades gracefully expected absolute performance high primitives due extra amount per-run overhead managing runs extending river application domains concentration database query-processing primitives natural question arises generality river model types applications benefit mechanisms provided river environment application written river model general-purpose programming substrate question types applications easily naturally written river framework readily utilize adaptive mechanisms provided order achieve level performance robustness application domain benefit river environment parallel scientific codes wolniewicz graefe shown common scientific operators fit dataflow environment operators reengineered performance robustness manner similar database primitives common operation types applications matrix transpose poole assuming data begin end disk transpose structurally similar external sort routing data based key record floating-point routed final destination based location final output set slight difference input read memory sort read input stream sequential order transpose slight modifications made standard distributed queue accommodate acm transactions computer systems vol february run-time adaptation river read input stream row column staggered manner balance output load properly examples work river environment out-ofcore matrix vector matrix matrix multiplications applications great deal flexibility order process data elements multiplied amenable transformation performance-robust programs finally applications poole survey o-intensive scientific applications require strict ordering records written disk hinting suitability river environment poole applications excellent candidates river implementation experimentation good match limitations river run-time adaptation finally discuss scenarios run-time adaptive techniques river work situation myrinet switch deadlock showcases reliance river network performance-reliable medium note problem arose implementation highlighting experimentation real system scenario demonstrates inherent weakness run-time adaptation decisions made run-time lack global perspective data written disk current conditions laid properly access potentially conditions global performance faults problem present result peculiar switch behavior demonstrative general problem figure plots performance graduated declustering scale increasing number producers consumers x-axis plotting total throughput percentage peak y-axis case producers throttled produce data fast perturbations system figure performance excellent low scale coming close peak drops unexpectedly producers consumers involved careful instrumentation found poorly performing experiments performance fine period time suffered dramatic systemwide two-second pauses investigation symptom led conclusion myrinet switches deadlocking halting progress detected deadlock recovered fortunately assistance implementors library altered avoid problem experience illustrative library changed fragment messages smaller chunks fragmented switches observe fragment interarrival time erroneously assume delayed fragment implied deadlock deadlock recovery mode implementors installed fix fragmenting messages switches longer reason time-out acm transactions computer systems vol february arpaci-dusseau fig switch deadlock performance graduated declustering plotted scale cases switch deadlock occur graph number producers consumers covaried x-axis percent ideal performance plotted y-axis machines involved experiment deadlock occurs scale higher rates occur performance drops noticeably general point river method run-time 
adaptation relies global characteristics network network performing expected system tolerate producerand consumer-side performance faults global performance faults network global effect avoided mechanisms contrast localized performance faults network link contention link performance-failure naturally handled adaptive mechanisms river faults indicative design diversity gray reuter called architectural heterogeneity type heterogeneity avoids problems occur collection identical components suffers identical design flaw including components makes manufacturers system gray reuter state heterogeneity akin belt suspenders belts suspenders additional network graceful performance fail-over network avoided myrinet deadlock problem solutions costly local versus global perspective finally discuss potential general weakness run-time adaptation river scenario assume application writing records disk disks faster naturally allocate proportional amount data disks application reads acm transactions computer systems vol february run-time adaptation river data back disks obtain peak performance system performance characteristics disks changed previously high-performing disks longer high-performing read performance suffer data written disk performance footprint created time future state system longer matches footprint performance mismatch occurs delivered performance longer approaches ideal method ameliorate potential problem replication replicating data access potentially tolerate unanticipated performance fluctuations left question data sets replicate type adaptation rearranging data offline account current characteristics system broader scale run-time mechanisms river designed handle complementary offline adaptive techniques similar developed neefe matthews required provide complete solution view development techniques main goals future work related work river draws related work areas parallel databases parallel storage systems parallel file systems parallel programming environments discuss related work areas turn parallel databases large-scale operations common parallel database systems number parallel databases found literature including gamma dewitt volcano graefe digital rdb prototype barclay bubba copeland systems based techniques similar dataflow model river parallel queries directed graph connects sequential data operators gamma gamma parallel database system developed wisconsin dewitt initial prototype developed shared-nothing cluster vax processors main memory connected token-ring network machines identical hard drives attached dewitt basic partitioning techniques provided distribute data processors round-robin hash range user-specified key range assuming uniform distribution communication processors performed split table takes tuples sending processor distributes receiving processors aforementioned distribution styles contrast river data distribution techniques gamma make strong performance assumptions partitioning techniques total time completion determined slowest consumer group network connects machines shared medium acm transactions computer systems vol february arpaci-dusseau case token-ring network aggregate network bandwidth scale processors data easily moved cluster remote consumption fundamental tenets river design volcano prominent parallel database system literature volcano graefe wolniewicz graefe volcano construct called exchange operator move data processors similar gamma split table case gamma contrast river volcano makes solely nonrobust distribution techniques hash-partitioning range-partitioning round-robin replication flexible distribution mechanism major difference volcano gamma models parallelism gamma demand-driven approach sinks pull data sources request messages conversely volcano data-driven approach data eagerly consumers consumers explicitly request data message-passing libraries issues arise form pull-based message layers versus push-based karamcheti chien conceptually similar gamma parallel database systems volcano intended primarily shared-memory machine early prototypes ran -processor sequent symmetry shared-memory machine excellent platform river-like system interconnect performance good digital rdb work parallel-load prototype digital rdb project barclay describe dataflow execution environment connections producers consumers data-flow rivers connect streams data stated river partitioning based split-table streams river split table suggests record inserted river river program split table pick destination stream record river extracts field values record compares values values split table pick destination stream split-table rangepartitioning hash partitioning round-robin replication input records sink operators barclay static techniques provide performance availability run rate slowest sink authors state nodes speeds amounts memory longer straight-forward distribute work evenly nodes barclay flexible method distribution found river addition parallel river takes advantage unordered record processing system provide form run-time adaptation ibm smps lindsey system shared data pools accessed multiple threads faster threads acquiring work lindsey refers access style straw model thread slurps data straw potentially rate acm transactions computer systems vol february run-time adaptation river implementing system natural smp simple lock-protected queue suffice modulo performance concerns river viewed distributed implementation concept parsets notion applying operations data set parallel explored parsets dewitt object-oriented database system application developer create function subsequently direct system apply objects collection model computation great flexibility building performance robust system data replicated robust access replicated data storage parsets implementation processes function statically data site dynamically balance load avoid ill-effects performance variations ncr teradata current commercial systems ncr teradata machine exclusively hashing partition work achieve parallelism good hash function effect dividing work equally processors providing consistent performance achieving good scaling properties jim gray teradata system performance bad worse consistency scalability goals system cost underlying hardware contrast river attempts deliver performance current configuration system stable performance river-based applications fluctuate eddies finally eddies adaptive dataflow environment built top river system avnur hellerstein eddies adaptive ideas river step reordering data operators on-thefly order achieve higher levels performance specifically monitoring selection join predicates highly selective eddies adapt dataflow place highly selective operators reduce total amount work performed system storage systems raid redundant arrays inexpensive disks raids popular organize collections multiple disks gibson katz patterson idea simple aggregate set less-expensive disks block-level interface commonly amount storage circumvent failures variety redundancy mechanisms chen excellent survey striping commonly extract full aggregate bandwidth multiple disks striping spreads blocks disks fixed round-robin pattern based logical address block simple striping breaks disks collection runs slower rate expected performance simple striping classified performance fragile entity perform expected global performance match expectations performance assumptions made acm transactions computer systems vol february arpaci-dusseau disk recent work addresses static performance heterogeneity raid systems cortes labarta relative performance rates drives change performance problems occur petal petal distributed system exports block-level interface lee thekkath assembled group workstations pcs multiple disks attached petal presents collection clients highly virtual disk place data main objective petal provide easily administrable high-performance storage scalable switch-based network petal systems provide form run-time adaptation similar spirit petal mirrors data disks set reads client directed multiple locations based load information petal simple dynamic algorithm client track number requests pending server sends read requests server shorter queue length lee thekkath performs similar balancing additional element disk server services requests biased fashion optimizing global application progress petal load-balancing writes global operations striping suffer fate traditional 
storage systems chained declustering chained declustering technique performs naive mirrored system failure present system hsiao dewitt typical mirrored systems replication naive blocks disk mirrored identically failure occurs surviving disk pair overloaded chained declustering avoids problem spreading replica blocks disks balancing load read-intensive workloads ways generalization chained declustering chained declustering works case absolute failure single disk works performance failure single disk active disks recent trend storage systems computation moved disks acharya riedel active disk systems perfect environment river problems encountered clusters encountered acharya stream-based programming model similar river dataflow model extending adaptive mechanisms straightforward additional adaptation techniques required dynamic migration computation host processor disks depending current system load network performance levels amiri parallel file systems turn attention large body work parallel file systems systems focused extracting high performance set uniform disks including ppfs huber bridge dibble acm transactions computer systems vol february run-time adaptation river panda seamons winslett galley nieuwejaar kotz vesta corbett feitelson swift cabrera long cfs nitzberg sfs loverso sio specification bershad spiffi freedman common features include scatter gather transfers asynchronous interfaces layout control prefetching caching support client server parallel file systems stripe data naively set disks subsystem undesirable performance properties shared file pointers interesting feature provided systems notion shared file pointer found cfs nitzberg spiffi freedman shared file pointer multiple processes machines access file concurrently consistent manner sharing local file pointer shared file pointers excellent performance properties group processes reading data collection faster processes read data providing coarse-grained load balancing application similar spirit shared-file pointers provide properties sequentially read files provide support load-balancing writes disk collective advanced parallel file systems higher-level interfaces data collective kotz referred disk-directed similar concept expressed two-phase choudhary original paper kotz found scientific codes show tremendous improvement aggregating requests shipping underlying system nodes schedule requests noticeably increase delivered bandwidth requests made returned specific consumers load balanced consumers dynamically solve performance problems common clustered systems panda systems discussed panda kuo seamons winslett deals explicitly performance heterogeneity solutions limited deals heterogeneity disk writes reads left unbalanced previous write perfectly balanced load disks access pattern approach priori static measurement disk performance calculate lay data disks performance write system properly react round measurement contrast river applications make decisions dynamically state drive performance handle performance run-time parallel distributed programming environments finally parallel programming environments exploited benefits run-time adaptation examples include cilk blumofe lazy threads goldstein multipol acm transactions computer systems vol february arpaci-dusseau chakrabarti systems dynamically balance load consumers order facilitate programming highly irregular finegrained parallel applications cilk cilk randall parallel programming environment designed parallel machines parallelism attained spawning extremely lightweight threads allowing users express arbitrarily complex parallel control constructs load-balancing achieved cilk work stealing processor work examines processor work queue picked uniformly random steals work conceptually stealing work work queue similar load balancing cilk implementation tuned thread-level work stealing aimed high-performance data movement contribution note cilk system authors proven cilk work-stealing scheduler achieves space time communication bounds constant factor optimal multipol multipol run-time support irregular applications distributed data structures focus hiding communication latency asynchrony chakrabarti load balancing provided distributed task queue wen similar design implementation linda linda shared globally addressable tuple-space parallel programs carriero gelernter applications perform atomic actions tuple-space inserting tuples querying space find records attributes tuple space similar general generality model high performance distributed environments shown difficult achieve bal reliable multicast finally birman encountered similar problems performance faulty nodes research reliable multicasting work alter guarantee provided multicast infrastructure absolute guarantee probabilistic avoid ill-effects stuttering node pursue similar goals exploit application flexibility obtain robust performance conclusions heart river system run-time adaptation component system statically trusts performance component node constantly gauges performance data transfers allocates data requests nodes proportion perceived performance built philosophy mind demonstrated article robust data-transfer mechanisms delivering ideal performance range perturbation scenarios acm transactions computer systems vol february run-time adaptation river keys run-time adaptation derived interaction communication layer utmost importance number flow control credits provided scale size system excess parallelism needed order overcome potential problems extremely poorly performing components performance dictated slow component system local data processing guided global knowledge progress property found fourth final slack needed run-time adaptive methods deliver peak performance presence small number performance faults slack system remains open question applications built river framework make primitives order robust disk performance faults suite database query-processing primitives run ideal broad range disk performance faults applications perfectly met system demonstrated top-n query external sort uncovered weaknesses river approach run-time adaptive methods rely strongly network backplane adaptation entire network function properly switches deadlock performance match expectations cases run-time adaptation short-sighted plan investigate complementary long-term adaptation order eventually build fully adaptive system methodological point view combination modeling simulation implementation crucial understanding system behavior simulations study isolation well-controlled setting allowing focus important properties flow control understanding algorithms behave second-generation implementation distributed algorithms proceeded ease implementation find limitations system arise simplified simulations underscoring importance building working prototype simple models gauge absolute performance faults understanding performance complex adaptive system made easier understands potential ideal acknowledgments eric anderson noah treuhaft contributed ideas implementation river system andrea arpaci-dusseau excellent aspects work dave patterson david culler joe hellerstein made substantial contributions ideas presented article jim gray excellent feedback direction advice finally anonymous referees careful feedback greatly improved substance style article acm transactions computer systems vol february arpaci-dusseau acharya uysal saltz active disks proceedings eighth conference architectural support programming languages operating systems asplos viii san jose calif adler chakrabarti mitzenmacher rasmussen parallel randomized load balancing proceedings annual acm symposium theory computing stoc acm york amd amd athlon processor architecture amd amiri petrou ganger gibson dynamic function placement dataintensive cluster computing proceedings usenix annual technical conference san diego anderson culler patterson team case networks workstations ieee micro february arpaci dusseau vahdat liu anderson patterson interaction parallel sequential workloads network workstations proceedings acm sigmetrics international conference measurement modeling computer systems ottawa arpaci-dusseau implicit coscheduling coordinated scheduling implicit information distributed system acm trans comput syst tocs august arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations proceedings acm sigmod conference management data sigmod tucson arpaci-dusseau performance availability networks workstations phd thesis california berkeley arpaci-dusseau arpaci-dusseau fail-stutter fault 
tolerance proceedings eighth workshop hot topics operating systems hotos viii schloss elmau germany arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common proceedings workshop input output parallel distributed systems iopads atlanta arpaci-dusseau arpaci-dusseau culler hellerstein patterson architectural costs streaming comparison workstations clusters smps proceedings high-performance computer architecture hpca las vegas avnur hellerstein eddies continuously adaptive query processing proceedings acm sigmod conference management data sigmod dallas bal kaashoek tanenbaum orca language parallel programming distributed systems ieee trans softw eng march barclay barnes gray sundaresan loading databases dataflow parallelism sigmod record acm sig manage data dec baru fecteau goyal hsiao jhingran padmanabhan copeland wilson parallel edition ibm syst bershad black dewitt gibson peterson snir operating system support high-performance parallel systems tech rep ccsfscalable initiative caltech concurrent supercomputing facilities caltech birman cooper isis project real experience fault-tolerant programming system oper syst rev april birman hayden ozkasap xiao bidiu minsky bimodal multicast acm trans comput syst tocs blelloch leiserson maggs comparison sorting algorithms connection machine cmin proceedings symposium parallel algorithms architectures hilton head blumofe joerg kuszmaul leiserson randall zhou cilk efficient multithreaded runtime system proceedings symposium principles practice parallel programming santa barbara calif acm transactions computer systems vol february run-time adaptation river boden cohen felderman kulawik seitz seizovic myrinet gigabit-per-second local-area network ieee micro feb bolosky iii draves fitzgerald gibson jones levi myhrvold rashid tiger video fileserver tech rep microsoft research boral alexander clay copeland danforth franklin hart smith valduriez prototyping bubba highly parallel database system ieee trans knowl data eng march borg blau graetsch herrmann oberle fault tolerance unix acm trans comput syst feb bressoud schneider hypervisor-based fault tolerance proceedings fifteenth acm symposium operating systems principles sosp copper mountain resort colo brewer inktomi web search engine invited talk proceedings acm sigmod conference brewer kuszmaul good performance cmdata network proceedings international parallel processing symposium cancun cabrera long swift distributed disk striping provide high data rates comput syst fall carriero implementation tuple space phd thesis department computer science yale chakrabarti deprit jones krishnamurthy wen yelick multipol distributed data structure library tech rep csd- california berkeley july chen bershad impact operating system structure memory system performance proceedings fourteenth acm symposium operating systems principles sosp asheville chen lee gibson katz patterson raid highperformance reliable secondary storage acm comput surv june choudhary bordawekar harry krishnaiyer ponnusamy singh thakur passion parallel scalable software input-output tech rep sccsece dept npac case center syracuse september codd relational model data large shared data banks commun acm june copeland alexander boughter keller data placement bubba proceedings acm sigmod international conference management data acm chicago corbett feitelson vesta parallel file system acm trans comput syst august cortes labarta extending heterogeneity raid level proceedings usenix annual technical conference boston dewitt gray parallel database systems future high-performance database systems commun acm june dewitt gerber graefe heytens kumar muralikrishna gamma high performance dataflow database machine tech rep trdept computer science univ wisconsin-madison march dewitt ghandeharizadeh schneider performance analysis gamma database machine sigmod record acm sig manage data september dewitt naughton shafer venkataraman parsets parallelizing oodbms traversals implementation performance proceedings international conference parallel distributed information systems ieee computer society austin texas dewitt naughton schneider parallel sorting shared-nothing architecture probabilistic splitting proceedings international conference parallel distributed information systems miami beach acm transactions computer systems vol february arpaci-dusseau dibble scott ellis bridge high-performance file system parallel processors proceedings eighth international conference distributed computer systems san jose calif englert gray kocher shah benchmark nonstop sql release demonstrating near-linear speedup scaleup large databases proceedings acm sigmetrics conference measurement modeling computer systems boulder colo fox gribble chawathe brewer gauthier cluster-based scalable network services proceedings sixteenth acm symposium operating systems principles sosp saint-malo france freedman burger dewitt spiffi scalable parallel file system intel paragon ieee trans parallel distrib syst nov gelernter carriero chandran chang parallel programming linda proceedings international conference parallel processing icpp charles ill gibson redundant disk arrays reliable parallel secondary storage acm distinguished dissertation mit press cambridge mass goldstein schauser culler lazy threads implementing fast parallel call parallel distrib comput august graefe volcano extensive parallel dataflow query processing system tech rep oregon graudate center june graefe encapsulation parallelism volcano query processing system sigmod record acm sig manage data june gray processors infinitely fast storage free invited talk proceedings iopads gray reuter transaction processing concepts techniques morgan kaufmann san francisco gribble brewer hellerstein culler scalable distributed data structures internet service construction proceedings fourth symposium operating systems design implementation osdi san diego grochowski emerging trends data storage magnetic hard disk drives datatech sept hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings sixth international data engineering conference los angeles huber elford reed chien blumenthal ppfs high performance portable parallel file system proceedings ninth acm international conference supercomputing barcelona johnson designing distributed queue proceedings seventh ieee symposium parallel distributed processing san antonio tex karamcheti chien comparison architectural support messaging tmc cmand cray proceedings annual international symposium computer architecture santa margherita ligure italy katz gibson patterson disk system architectures high performance computing proc ieee dec kotz disk-directed mimd multiprocessors proceedings symposium operating systems design implementation monterey calif kuo winslett cho lee chen efficient input output scientific simulations proceedings sixth workshop input output parallel distributed systems acm press atlanta kushman performance nonmonotonocities case study ultrasparc processor thesis massachusetts institute technology boston lee thekkath petal distributed virtual disks proceedings seventh conference architectural support programming languages operating systems asplos vii cambridge mass lindsey smp intra-query parallelism udb database seminar berkeley acm transactions computer systems vol february run-time adaptation river liskov distributed programming argus commun acm march litzkow tannenbaum basney livny checkpoint migration unix processes condor distributed processing system tech rep wisconsinmadison computer sciences april lorie daudenarde hallmark stamos young adding intra-transaction parallelism existing dbms early experience ieee data eng newslett march loverso isman nanopoulos nesheim milne wheeler sfs parallel file system cmin proceedings summer usenix technical conference cincinnati mainwaring culler active message applications programming interface communication subsystem organization tech rep csd- california berkeley october matthews roselli costello wang anderson improving performance log-structured file systems adaptive methods proceedings sixteenth acm symposium operating systems principles sosp 
saint-malo france meter observing effects multi-zone disks proceedings usenix conference anaheim calif nieuwejaar kotz galley parallel file system proceedings tenth acm international conference supercomputing acm press philadelphia nitzberg performance ipsc concurrent file system tech rep rnd- nas systems division nasa ames december ousterhout scheduling techniques concurrent systems proceedings international conference distributed computing systems miami fort lauderdale ousterhout tcl embedable command language proceedings usenix association winter conference washington ousterhout toolkit based tcl language proceedings usenix association winter conference dallas patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod chicago patterson gibson katz case redundant arrays inexpensive disks raid sigmod record acm sig manage data sept poole preliminary survey intensive applications tech rep ccsfscalable initiative caltech concurrent supercomputing facilities caltech raghavan hayes scalar-vector memory interference vector computers proceedings international conference parallel processing charles ill randall cilk efficient multithreaded computing phd thesis massachusetts institute technology boston riedel gibson faloutsos active storage large-scale data mining multimedia proceedings vldb york schneider implementing fault-tolerant services state machine approach tutorial acm comput surv dec seamons winslett multidimensional array panda supercomput singh weber gupta splash stanford parallel applications sharedmemory comput arch news march talagala patterson analysis error behaviour large storage system proceedings ipps workshop fault tolerance parallel distributed systems san juan puerto rico tandem performance group benchmark nonstop sql debit credit transaction proceedings sigmod international conference management data chicago teradata corporation dbc data base computer system manual release teradata corporation document number acm transactions computer systems vol february arpaci-dusseau tremblay greenley normoyle design microarchitecture ultrasparc-i proc ieee dec von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings fifteenth acm symposium operating systems principles copper mountain resort colo wen portable library support irregular applications phd thesis california berkeley tech rep ucb csd- wolniewicz graefe algebraic optimization computations scientific databases proceedings vldb dublin received october revised june accepted july acm transactions computer systems vol february 
information-based approach distributed systems design andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison abstract propose information-based methodology developing distributed systems core approach understand sources gathering information paper develop information taxonomy explore costs benefits source case study river environment clusters investigation performed targeted simulations implementation measurements focuses distributed algorithms foundation dynamic adaptation river information-centric approach design efficient scalable data-transfer mechanisms turn yield robust application performance introduction key making intelligent decisions parallel distributed system timely access accurate information component information state behavior rest system adapt behavior current conditions placing jobs distributed system scheduler load processor pick load tcp client congestion links network calculate sending window-size cache controller marks entry exclusively owned caches entry methods gather information affect performance reliability properties system clear importance system designers employ hoc approach lead pick non-optimal extremes assume information utilize static algorithms employ explicit queries remote nodes information incurring unnecessary overhead complexity understanding full range methods system architects select algorithms system paper develop generalized information-centric methodology designing distributed systems introduce information taxonomy catalog characterize methods information dispersal information taxonomy ways information gathered null implicit parasitic explicit base case null method information exchanged communication components components information state system rely algorithmic knowledge components behave implicit information components observe contents properties existing communication infer traits remote components parasitic information inserts small amount information existing communication structures propagating additional information natural data flows service finally explicit information components introduce additional communication system query remote entities state behavior investigate approach taxonomy perform in-depth exploration trade-offs sources information distributed system river environment clusters main goal river enable data-intensive applications adapt performance faults components cluster perform spite unexpected variations performance disks workstations components system central run-time adaptation river distributed algorithms applications layer reactive middleware algorithms distributed queue applications utilize distributed queue move data processed ameliorate effects consumers performance faults parallel replicated data transfer graduated declustering graduated declustering applications harness replicated data sources intelligently equally sharing producer bandwidth consumers tolerate performance faults data producers tandem constructs utilized fashion robust applications explore methods information dispersal distributed queue graduated declustering algorithms combination simulation implementation modifying algorithms techniques information dispersal judge relative merits identify important decisions made algorithm determine information needed points foster actions case study determine null methods algorithms adapt performance faults information required avoid components performing expected find implicit parasitic information river sources components adapt current conditions scalable efficient manner finally show requests explicit information carefully added increase complexity decrease robustness system generally find information-based approach facilitates deeper understanding design decisions make distributed algorithm employing methods taxonomy construct robust scalable data-transfer mechanisms turn enable assembly robust data-intensive applications rest paper organized introduce information-based methodology taxonomy gathering information section section motivates river interesting case study describes evaluation environment sections study application information distributed queue graduated declustering examine application performance section conclude section information-based methodology section discuss components obtain information remote components distributed system illustrate obtaining information fits behavior distributed-system service begin describing generic framework designing service introduce taxonomy sources remote information illustrate usage real systems literature design distributed services main challenges designing distributed service algorithm architect flows communication information-centric approach suggest breaking process steps key ingredient separation data transfer flow information design data transfer step building service application distributed system architect flow data messages components define data communication absolutely essential implement service protocol underlying layers data communicated specific service communication reading block data remote machine include request block response includes requested block step produces data architecture system identify control points step designer constructs control architecture identifying control points system control point component makes decision potentially affects behavior components component decide machine send piece data design information flow finally information architecture assembled component make decision control point information state rest system inextricable link information algorithm obtained low-cost stale information preferred up-to-date costly information methods obtaining information focus paper information-gathering taxonomy make informed decision control point component information state behavior components classify fundamental methods obtaining information primary categories null implicit parasitic explicit depicted table describe approaches beginning require amount communication null simplest approach guiding decisions null approach leverage communication gather information cases approach approprimethod gathering definition advantages disadvantages information null communication simple build non-adaptive leveraged simple reason lead poor decisions implicit observed free difficult reason existing communication elegant proper info parasitic embedded additional information information flow restricted existing communication low cost data flow explicit adds information cost additional communication communication interface gathered interface table taxonomy information-gathering methods taxonomy information dispersal gathering techniques presented perceived cost gathering information increases method listed table cost measured terms additional communication ate components information control points information obtained algorithmic knowledge discuss cases turn algorithm requires information guide control points information gathered algorithms static adapt current conditions traditional disk striping raid algorithms make static decisions place disk blocks current load system fact information gathered imply component knowledge remote behavior components algorithmic knowledge remote components act component components implemented algorithm follow defined specification theoretical work investigated problem making decisions general knowledge components behave high level game theory predicts actions adversaries absence communication decision theory optimizes utility decisions based uncertain information quantified probability measure specific lard load-balancing web server presented front-end leverages algorithmic knowledge back-end servers cache web pages direct requests page server efficiently aggregate memory primary advantage null method communication overhead gathering information disadvantage components accurate information state rest system make poor decisions implicit lowest-overhead method obtaining accurate information current state changing system observe existing communication implicit methods fall categories monitor contents messages observe information defined interfaces implicit methods powerful combined algorithmic knowledge allowing component infer current state parts system subsequent actions contents messages guide decisions systems including cache coherence algorithms shared-memory multiprocessors contention-avoidance ethernet networks caches shared bus snoop traffic main memory cache controller detects write data block infers controller owns block responds invalidating copy implicit methods additionally observe information interface observing contents data message time messages form implicit information identical covert channel field security distributed systems leveraged type implicit information include tcp congestion-control algorithm observes packet loss infer congestion coordinated scheduling parallel jobs infers remote scheduling state message arrivals round-trip time central advantage implicit methods provide information free communication requisite data messages 
required ability deduce remote behavior local observation disadvantages methods inferences made subtle lead system difficult reason construct information flow restricted exact path data flow system paths ideal information flow system parasitic parasitic category leverages existing data messages disseminate additional information system parasite host means original intention approach referred piggybacking systems parasitic method reduce number messages protocol requires inserting acknowledgement data message reverse direction paper focus application information dispersal sun nfs version reduce frequency getattr requests attribute information returned results file operations getattrrequest generated attribute information seconds number explicit getattrrequests reduced advantage parasitic methods low cost adding bytes existing messages similar implicit methods information flows restricted data communication channels limitation explicit finally direct manner obtain information explicit query remote entities explicit approaches common randomized load-balancing scheme sends explicit probes randomly chosen nodes determine load places job lesser loaded main benefit explicit methods simple understand require inferences knowledge behavior components correct number disadvantages interface obtaining desired information accessing explicit interface costly sending explicit request consumes shared network resources needed data transfers induces additional work remote node finally explicitly contacting remote node prepared deal kinds remote failure occur discussion designing distributed service recommend simple three-step process design data architecture dictates communication generally flows achieve desired service develop control architecture important decisions made finally layer information architecture specifies information gathered inform control decisions advocate step-wise approach design information architecture designers distributed systems obtain information method incurs lowest overhead sufficiently guiding decisions control points information needed knowledge algorithm designer implicit information existing data communication infer remote state implicit information lead decisions acceptable performance designer adding parasitic information finally parasitic information fails provide sufficient guidance explicit information added process naturally iterative design testing occur stage assess efficacy deployed information methods case study river evaluate range methods gathering information distributed system application case study river programming environment section describe river system interesting case study information describe experimental environment motivation river generic data-flow programming environment clusters workstations similar basic design previous parallel database environments volcano river interesting environment studying information primary reasons river solves important problem enables data-intensive applications perform robustly underlying components performing expected river fundamentally requires information state behavior remote components adapt performance problems river environment multiple distributed algorithms information compared discuss reasons detail solves important problem river supports construction parallel data-intensive applications core database query-processing primitives out-of-core scientific programs internet services worthwhile domain river solves additional problem ensuring applications perform robustly underlying hardware software components perform erratically previous work addressed design large-scale systems tolerate correctness faults individual components work focused design systems tolerate performance faults performance fault unexpected low performance component system clusters performance faults commonplace modern disks reasons static dynamic performance faults single disk disks include presence multiple zones scsi bad-block re-mapping thermal recalibration sporadic performance absolute failure contention due workload imbalance heterogeneity due incremental growth goal river enable performance availability application performance tracks aggregate performance components system degrading gracefully increasing number performance faults fundamentally requires information current systems support data-intensive applications interact performance faults static techniques exploit parallelism distribute data standard striping algorithms distributing data requests set disks place amount data disk problem static schemes make rigid assumptions relative performance components perform identically gathering information actual performance result performance service determined slowest component finishes dynamically adjust performance faults distributed system current state remote components implement high performance service scales large numbers components information ideally accurate up-todate gathered overhead sensitivity performance faults consists multiple distributed algorithms river distributed software constructs distributed queue graduated declustering dynamically adjusts data transfered set producers processes parallel application set consumers set disks parallel data-intensive applications consist transfers performance-available environment provided building performance-available data transfer constructs river solve half problem tandem build performance-available applications transferring data set producers set consumers consumers suffer performance fault proper reaction case move data faster consumers amount data proportional relative speed consumer functionality provided distributed queue difficult problem occurs data transfer producers consumers producer incurs performance fault data source replicated multiple producers river applications employ graduated declustering data transfer mechanism carefully divides producer bandwidth equally consumers performance availability producer faults evaluating sources information algorithms side-by-side interesting algorithms characteristics common vary fundamental aspect amount flexibility adjust slow components avoid slow consumer sending data consumer avoid slow producer gathering data producers replicating data restrictive environment forces gather information achieve robust performance experimental environment study costs benefits information sources distributed queue graduated declustering employ simulations implementation event-based simulator explore basic algorithmic behavior wide range system parameters feasible measure implementation measurements implementation gathered confirm simulation results bring systems issues modeled simulations combination techniques improves understanding separating intrinsic properties implementation details river runs cluster ultra workstations running solaris workstation consists mhz ultrasparc processor memory seagate hawk -rpm disks attached fast-narrow scsi bus disk commonly river data disks deliver peak bandwidth workstations connected highspeed myrinet local-area network workstation single myrinet card cards capable moving data workstation approximately entire system connected series -port myrinet switches -ary fat tree communication performed active messages exposes raw performance myrinet integrating features threads blocking communication events multiple independent endpoints distributed queue apply information-based approach distributed algorithms river beginning distributed queue sections present general interface mechanism study identify control points examine approaches information taxonomy applied interface semantics previous section distributed queue presents applications high-speed backplane data sharing tolerate consumer-side performance faults application producers place data distributed queue calling put data size consumers receive data calling data size consumer logically receive data block put system central challenge designing move data producnote ordering data guaranteed pointto-point producer places consumer receives receive strictly speaking bag queue consumer decision producer data processed producer decision consumer data producer consumer communication send data chosen consumer consumer producer communication acknowledge receipt data consumerx producer process block figure data control diagram depicts basic data movement control options distributed queue producer wishes send data block faced decision consumer data decided data block consumer consumer receives blocks producers faced control option order blocks processed consumers block received consumer sends reply producer required active messages request response protocol ers consumers faster consumers receive proportionally higher amount aggregate producer bandwidth implementation accomplish goal distributed scalable manner ways distributed queue 
reminiscent linda tuple space restricted usage entities allocated storage distributed queue simply passed directly producer consumer designed strictly high-performance data transfers tolerate consumer-side performance faults bears similarity distributed load balancing constructs goal performance-available data transfers consumer performance faults one-to-one mapping traditional load-balancing schemes data control figure presents logical structure data flow producer data distribute set consumers consumer queue incoming data blocks process data transferred parallel set producers set consumers part queue flow block behavior system determined decisions control points potentially requires global information candidates exploring information taxonomy producer chooses consumer receive percent peak performance performance faults alternatives performance faults implicit null simulation implicit null implementation null null simulation null null performance drops immediately fault implicit null performance simulations implementation ideal range performance faults ideal figure performance null implicit results simulation null null implicit null implementation implicit null compared ideal performance increasing number performance faults experiments producers generate data nonfaulty consumers consume data performance fault reduces consumer performance factor data block important decision performance-available distributed queue algorithm consumer choose order process blocks received examine levels information taxonomy applied make decisions distributed queue null information begin study information alternatives simplest alternative information distributed queue consists decisions refer distributed queue information decisions null null decision producer consumer receive data block information-free algorithms sending consumers round-robin random pattern round-robin pattern lead excessive endpoint contention focus random choice consumer decision consumer order blocks processed null alternatives study simple first-come first-served policy figure illustrates performance simulated null null experiments producers send data remote consumers total machines producers generate data non-faulty consumers sink data set-up emulates set processes writing in-memory records parallel disk disks consumers bottlenecks transfer x-axis increase number consumers undergoing performance faults performance fault reduces level consumer performance factor left-most point x-axis shows performance system performance faults right-most point shows performance consumers perform perturbed rate y-axis plots performance fraction peak non-perturbed case faults system peak simply aggregate bandwidth bottleneck transfer case consumers bottleneck ideal performance line shown subsequent graphs computed fraction peak performance number performance faults shown figure performance faults null null performs throughput high scales machines single fault performance drops immediately slowest consumer performance faults added system null information schemes fundamentally react changing characteristics consumers system provide performance availability support desired behavior distributed queue form global information dispersed system allowing producers adapt volatile performance delivered consumers implicit information implicit information elegantly solves basic problem distributed queue lack adaptation varying rates consumption implicit information feedback request response paradigm data messages producer sends data consumer form active message request protocol stipulates consumer replies message null null reply empty message implicit null reply signal consumer handled data request producer infers remote consumer making progress producer monitors incoming replies infer current perthis reply message required protocols utilize acknowledgements implement reliable message transfers formance level consumer leverage implicit information implicit null algorithm producer records unacknowledged message outstanding consumer producer send block data finds consumer unacknowledged message consumer producer chooses uniformly random consumers producer relinquishes processor waits consumer reply consumer receives data block processes blocks first-come firstserved order sends empty active message reply producer producer handles reply records longer outstanding message consumer intuition implicit algorithm consumers respond quickly requests receive subsequent requests producer implicit information producer window consumer performance instant producer trusts consumers adequately responded data requests trust responded algorithm property unifying seemingly diverse implementations push-based pull-based approaches found messagepassing layers producers bottleneck data transfer consumers respond quickly data requests choice destinations degenerates random choice entire set consumers push-based approach consumers bottleneck producer messages outstanding consumer reply returns implicitly pulls data request replying consumer depending relative performance rates producers consumers implicit null characteristics push-based pull-based approach shown figure simulated performance implicit distributed queue performance faults excellent delivering ideal performance range induced faults figure shows implementation results system parameters set equal simulation closely match simulation results scheme easily extended outstanding block consumer found sufficient range experiments reported relative bandwidth achieved producer number bandwidth scale implicit null implementation implicit null implementation implicit null implementation implicit parasitic simulation implicit null approach network unfairness producers receive fair-share bandwidth addition parasitic information solves fairness problem unfair unfair figure fairness graph depicts results experiments implementation implicit null simulation implicit parasitic experiments producers sending data fast remote consumers total machines rate consumer controlled varied spot x-axis producer system y-axis plots proportion total bandwidth producer received experiment fair distribution bandwidth leads producers y-value parasitic information implicit information choosing destination data block implicit null non-informed algorithm first-come first-served processing blocks consumer performing detailed measurements implementation found high loads participating nodes myrinet switches fairly schedule transfers producers receive fair share network bandwidth performance determined rate slowest entity unfairness performance system drop figure plots relative bandwidths producers experiments implementation experiments producers sending data consumers performance faults system per-consumer bandwidth set levels ideally producer obtain fair-share aggregate consumer bandwidth occurs per-consumer bandwidths per-consumer bandwidth producers receive substantially lower proportion aggregate consumer bandwidth parasitic information solution fairness problem data producer sends current bandwidth consumers information schedule data blocks process consumer biases service producers lagging encourages global fairness good performance spite networkswitch unfairness note implicit parasitic ideal respect amount communication performs logically adding bytes information large data transfer verify performance implicit parasitic modified simulator model switch unfairness simulation results version figure implicit parasitic performs desired implemented approach real system confident parasitic information equally aid performance note combination simulation measurements implementation pinpoint fix problem readily explicit information successfully utilized implicit information choose destination block data load-balancing literature propose explicit approaches adler suggest simple probe-n model producer sends explicit probes random subset consumers sending data block probes return load consumer producer pick consumer lowest load intuition algorithm time send probe fast roughly equal roundtrip latency network implemented approach realized explicit queries make dangerous performance assumption probe returns promptly type assumptions seeking avoid distributed queue surprisingly implementation explicit null distributed queue proved unsuccessful performance dropped level slow consumers system identical null null algorithm shown figure probe-n algorithm shown balance load cluster performance-available manner performance assumptions made explicit information queries reour implementation incurs overhead active message fixed set arguments unused sult brittle behavior assumptions invalid consumer respond timely 
manner queries explicit approach disadvantage adding probe messages data block finally probebased approach significantly complicates distributed queue adding substantial amounts code discussion studied range information sources performance-available adaptive distributed queue primary importance adaptive algorithm feedback null methods systems tolerate performance faults conclusion implications systems traditional disk striping schemes parallel database techniques rangeand hashpartitioning null methods adapt components varying performance levels important piece information consumption rate remote consumers needed deciding send data block implicit information adequate decision consumer replies signaling rate remote consumer progress yields highperformance algorithm adequate consumer-side scheduling block processing factor switch unfairness mandates parasitic methods disperse extra information system rate progress producer combination parasitic implicit methods leads general robust distributed queue finally explicit methods information remote entities gathered performance assumptions introduced producer queries number remote sites current load wait queries return yield performance-available explicit methods avoided performance dependencies well-understood graduated declustering distributed queue tolerates consumerside performance faults moving data faster consumers suffers producer deliver data expected rate data locale producers bottleneck data transfer bbb fau figure diagram shows alleviates problem producer-side performance faults disks producers data mirrored producer produces data sets consumer consumes data set performs half expected rate producers compensate bandwidth allocations consumers receive fair-share aggregate bandwidth solution problem cases data replicated sake reliability mirrored disk system data block locations river graduated declustering coordinate access replicated data set dividing aggregate producer bandwidth equally consumers avoid producer-side performance faults work shown figure interface semantics internal interface graduated declustering slightly consumer side simple consumers call data size requests eventually returns requested data sources producer interface complex producers call getrequest request request specification put request data size send results request back requesting consumer graduated declustering similar chained declustering technique improves performance absolute failure mirrored disk system carefully spreading replicated blocks disks extends concept tolerate full range performance faults similar adaptive techniques choosing replica read petal petal coordinate activity parallel clients global manner successful avoiding potentially harsh effects consumer decision producer request producer decision consumer request scheduled producer consumer communication reply request data block consumer producer communication request data block producer consumer producerx process requested blocks process request send reply figure data control diagram depicts basic data movement control options graduated declustering consumer requesting block faced decision producer request request selected producer requests consumers consumer faces decision queue serviced serviced data back requesting consumer performance faults parallel clients data control figure presents logical structure data flow graduated declustering consumer sends request block replicated sources block decision represents control point producer choose order handle consumer requests decision represents control point null information null null makes decisions information consumer chooses random producer data request producer services requests first-come first-served manner performance simulated shown figure consumers requests data producers set producers non-faulty producers generate data consumers sink data scenario emulates parallel read disks disks producers application processes consumers x-axis increase number producers performance faults performance fault reduces level producers performance factor surprisingly null null approach works performance faults system performance degrades single performance fault occurs percent ideal performance performance faults alternatives performance faults implicit parasitic simulation layout implicit parasitic implementation layout implicit parasitic simulation worst layout null implicit null simulation layout implicit parasitic performance depends strongly layout performance faults worst null null implicit null performance poor single fault ideal figure performance results simulations null null implicit null implicit parasitic implementation implicit parasitic compared ideal performance increasing number performance faults experiments non-faulty producers generate data consumers consume data aperformance fault reduces producer performance factor limited replication layout performance faults matters implies spread worst occur consecutively producers implicit information implicit null modeled implicit null consumer sends request producer responded recently intuition approach give requests responsive servers avoid slow producers performance implicit null algorithm identical null method consumers adapt responsive producers producer scheduling first-come first-served policy successful due greater flexibility producers producer send data consumer consumer send request subset producers due limited replication producers bias scheduling cooperatively account slow producers implicit method bring behavior parasitic information arrive performance-available algorithm addition parasitic information analogous consumer inserts rate progress request producer localized view progress consumer serves producer biases scheduling service lagging consumer easily shown producer locally balance bandwidths consumers services consumers receive equal share producer bandwidth implicit null consumers implicit method decision sending requests producer recently responded figure shows performance implicit parasitic simulation implementation simulations performance-fault layouts presented worst case faults occur consecutively immediately affect data sources consumer case evenly distributed occurring consumer producers half producers perturbed implementation results shown layout performance layout faults good ideal due limited amount replication producer contained data set flexibility ideal performance generally level replication data item number producers system implicit parasitic algorithm tolerate presence performance faults potentially tolerate faults explicit information finally examine utility explicit information movement graduated declustering distributed queue adding explicit queries system built performance availability dangerous adding explicit information graduated declustering care avoid performance assumptions situation implicit parasitic algorithm performs explicit information case envision alternative parasitic dispersal progress information implicit parasitic piggy-backs consumer progress request producer benefit approach low cost achieves defined goal potential disadvantage occurs consumer request data producer time case producer up-to-date knowledge consumer progress make optimal scheduling decision explicit approach solves problem continually updating producers proparasitic producer consumer parasitic explicit consumer table parasitic explicit tables show percent requests consumer satisfied producer gathered simulation table shows results implicit parasitic algorithm consumer receives roughly half requests producer table shows results implicit parasitic explicit approach consumer receives requests producer experiments performance faults ducer real request dummy request progress information disadvantage implicit parasitic approach faults system case producer divides bandwidth roughly equal parts consumers serves results slight performance cost additional seeks incurred reading streams disk implicit parasitic explicit algorithmically favor consumer deliver bandwidth consumer informed consumer making sufficient progress explicit approach avoids extraneous seeks table presents simulation results approaches disadvantages costs extra explicit information requests generated environments noticeably affect system performance information-bearing requests make performance assumptions implemented reliable protocol request consumes resources buffers retransmission producer slow repeated requests consume resources eventually halting progress information-dispersing requests built unreliable medium raw u-net active messages discussion design successful graduated declustering algorithm requires information baseline 
distributed queue consumer progress passed consumers producers facilitate producerside scheduling producer rates consumers direct requests serviced efficiently realized parasitic information implicit methods kinds information develop robust algorithm information consumer progress widely dispersed explicit techniques explicit informationbearing request producers slight performance improvement represents class information null implicit parasitic techniques implicit parasitic explicit approach demonstrates hybrid techniques leveraged gather information putting demonstrate utility complete distributed queue graduated declustering mechanisms give summary application experience river solid information-centric understanding algorithm final implementation mechanisms straight-forward result performance applications compared previous efforts present performance o-intensive database query-processing primitives transformed static parallel application robust adaptive version programmers insert distributed queues graduated declustering applications form points flexibility places data-flow performance faults tolerated note application writers knowledge information flow focus constructing flexible flows infrastructure handles rest utilizing core river mechanisms application withstands performance faults disks achieves near-ideal performance figure presents summary results figure performance application increasing number disk performance faults shown fault disk utilizes half disk bandwidth applications ideal amount applications paper percent ideal performance performance faults induced application performance scan generate filter top-n selection hash join sort ideal performance applications performance degrades gracefully faults figure application performance performance database primitives disk performance faults presented run implementation machines applications parallel scan data set read parallel generation data set write parallel filter read data disk select records based user-defined function write selected records disk top-n selection find top values data set find efficiently parallel hash-join data sets finally parallel external sort application operates roughly data disk node total machines performance fault reduces performance factor data point represents average runs variance numbers data touched application divided peak rate disks parallel scan reads total disks run ideal time scan seconds cases performance excellent measured execution time river applications ideal summary carefully applying information-based approach build solid implementations distributed algorithms information-ignorant applications advantage informationaware mechanisms reap benefits excellent behavior performance faults instances implicit parasitic techniques proved elegant low-cost methods gathering information constructs needed adaptation conclusions future work paper introduced explored information-based approach systems design understanding information distributed algorithms iteratively evaluating range approaches information taxonomy substantially improved performance system system designers follow steps equally beneficial results case study river found implicit parasitic information distributed queue graduated declustering information derived methods sufficient enable design implementation robust scalable elegant algorithms distributed systems continue increase complexity information-centric techniques critical method classified null implicit parasitic explicit place depending problem domain evaluating costs benefits approach central successful design future increase ease information distributed systems important step goal develop information programming interfaces ipis similar traditional application programming interfaces apis ipis serve isolate information-gathering method rest algorithm allowing implicit parasitic explicit hybrid methods employed depending environment ipis increase portability standard tcp congestion control run wireless environment dropped message longer congestion due simply lossy medium congestion signal encapsulated ipi porting ipi setting forces designers re-think gather information avoiding incorrect inferences implicit observations finally plan formalize gathering information distributed systems interesting environments feedback components involved hope application control theory prove fruitful adding rigor design distributed algorithms adler chakrabarti mitzenmacher rasmussen parallel randomized load balancing proceedings annual acm symposium theory computing stoc pages york acm arpaci-dusseau arpaci-dusseau culler hellerstein patterson searching sorting record experiences tuning now-sort spdt aug arpaci-dusseau culler mainwaring scheduling implicit information distributed systems proceedings acm sigmetrics international conference measurement modeling computer systems arpaci-dusseau performance availability networks workstations phd thesis california berkeley arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common iopads balakrishnan padmanabhan seshan katz comparison mechanisms improving tcp performance wireless links ieee acm transac-tions networking december birman cooper isis project real experience fault-tolerant programming system operating system review pages april birrell nelson implementing remote procedure calls acm transactions computer systems february blackwell girshick theory games statistical decisions john wiley sons york blumofe joerg kuszmaul leiserson randall zhou cilk efficient multithreaded runtime system proceedings symposium principles practice parallel programming july boden cohen felderman kulawik seitz seizovic myrinet gigabet-per-second local-area network ieee micro february bolosky barrera draves fitzgerald gibson jones levi myhrvold rashid tiger video fileserver http research microsoft research bolosky tiger tiger html brewer inktomi web search engine invited talk sigmod brewer kuszmaul good performance cmdata network proceedings international parallel processing symposium cancun mexico april carriero implementation tuple space phd thesis department computer science yale december culler arpaci-dusseau arpaci-dusseau chun lumetta mainwaring martin yoshikawa wong parallel computing berkeley jspp joint symposium parallel processing kobe japan june englert gray kocher shah benchmark nonstop sql release demonstrating near-linear speedup scaleup large databases proceedings acm sigmetrics conference measurement modeling computer systems pages goodman cache memory reduce processormemory traffic proceedings annual international symposium computer architecture pages stockholm sweden june computer architecture news june graefe encapsulation parallelism volcano query processing system sigmod record acm special interest group management data june hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international data engineering conference pages jacobson congestion avoidance control proceedings acm sigcomm pages august johnson designing distributed queue proceedings seventh ieee symposium parallel distributed processing pages san antonio texas october karamcheti chien comparison architectural support messaging tmc cmand cray annual international symposium computer architecture pages santa margherita ligure italy june lampson note confinement problem communications acm october lee thekkath petal distributed virtual disks asplos vii pages cambridge october mainwaring culler active message applications programming interface communication subsystem organization technical report csd- california berkeley october metcalf boggs ethernet distributed packet switching local computer networks communications acm july meter observing effects multi-zone disks proceedings usenix conference jan microsysytems whitepaper nfs version http sun software whitepapers wp-nfs march neumann morgenstern theory games economic behavior princeton press princeton jersey edition pai aron banga svendsen druschel zwaenepoel nahum locality-aware request distribution cluster-based network servers asplosviii san jose california patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod international conference management data pages chicago june acm press rubin design automatic control systems artech house norwood schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december sobalvarro pakin weihl chien dynamic coscheduling workstation clusters proceedings ipps workshop job scheduling strategies parallel processing talagala patterson analysis error behaviour large storage system ipps workshop fault tolerance 
parallel distributed systems tremblay greenley normoyle design microarchitecture ultrasparc-i proceedings ieee december von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings fifteenth acm symposium operating systems principles pages copper mountain resort usa wen portable library support irregular applications phd thesis california berkeley january techreport ucb csd- 
cluster river making fast case common remzi arpaci-dusseau eric anderson noah treuhaft david culler joseph hellerstein david patterson kathy yelick computer science division california berkeley introduce river data-flow programming environment substrate clusters computers river designed provide maximum performance common case face nonuniformities hardware software workload river based simple design features queue storage redundancy mechanism called graduated declustering implemented number data-intensive applications river validate design near-ideal performance variety non-uniform performance scenarios scalable systems form basis highperformance computing market recent years manufacturers found growth customer appetite capacity outstripping moore law cluster systems key component design today scalable data-intensive architectures frustrating aspect cluster systems common-case performance great deal worse reported peak performance discrepancy arises forms performance heterogeneity clustered components simplest heterogeneity hardware cluster composed machines differing speeds capacities principle problem solved fiat packaged clusters ibm spmore nefarious heterogeneities software performance arise dynamically multitude sources unexpected operating system activity uneven load placement heterogeneous mixture operations machines software heterogeneity hard control quickly time surprisingly hardware heterogeneity non-trivial control cylinders disk bandwidth outer apparently identical disks bandwidths depending locations unused bad disk blocks presented iopads input output parallel distributed systems atlanta georgia attempting prevent performance heterogeneity designed system takes account inherent design consideration paper describe river data-flow programming environment substrate clusters goal river provide common-case maximal performance o-intensive applications achieved basic system mechanisms distributed queue balances work consumers system data layout access mechanism called graduated declustering dynamically adjusts load generated producers center river design high-performance implementation river dqs data flow operators autonomously adaptive rates time producer places data fast consumer takes data fast interposing dqs operators data flow load naturally balanced consumers running rates advantage simplicity lack global coordination required consumers change rate autonomously time communicating clients result full-bandwidth balanced consumption bandwidth naturally utilized times consumers set data complete near-simultaneously important aspect river flexible redundant disk layout access mechanism called graduated declustering generalization mechanism proposed early parallel database systems task data production shared multiple producers flexible fashion mirrors large sequential collections disks producers data flow producer multiplexes bandwidth data sets handling ensure produces share global bandwidth collection result full-bandwidth balanced production bandwidth utilized times producers data set complete near-simultaneously introducing river describe programming model graphical interface composing river programs based traditional data-flow diagrams composed operators similar database query plans scientific data-flow systems intuitive interface programmers focus applicationspecific logic river transparently handles issues high-performance parallelism application demonstrate river interface number dataintensive applications validate perforslowdown now-sort single anomaly case single disk poor layout single disk hot spot single machine light cpu single machine heavy cpu single machine memory load figure now-sort perturbance graph depicts best-case performance now-sort versus performance slight disk cpu memory perturbations performance results relative -node nowsort delivers data near-peak disk rate run mance system cases river near-ideal performance face severe performance perturbations motivate problem performance heterogeneity perform simple experiment now-sort high-performance parallel external sort clusters experiment sort runs machines run perform slight perturbation sort machines results perturbation experiments shown figure graph perturbations single machine global performance effect single file single machine poor layout tracks versus outer performance drops percent single disk hot spot competing data stream performance drops factor cpu loads machines decrease performance proportional amount cpu steal finally memory load pushes machine page factor performance lost build system avoids situations balancing load system perfectly times meticulously managing resources system difficult system size complexity increase carefully managing system near-impossible approaching problem manner assuming presence performance faults providing substrate operate spite rest paper structured section describe design system current implementation euphrates section validate performance properties dynamic infrastructure measurements distributed queues graduated declustering present initial application experience section related work found section section present plans future work section conclude section describes design river environment current implementation euphrates briefly describe hardware software environment present river data model data stored accessed disk continue explaining components river programming model including details typical river program constructed conclude discussion river prototype euphrates runs cluster ultra workstations connected myrinet localarea network workstation mhz ultrasparc processor seagate hawk rpm disks swap space common case memory solaris operating system machine modern multi-threaded unix communication performed active messages generation communication layer designed distributed computing exposes raw performance myrinet providing support threads blocking communication events multiple independent endpoints fast message layers support blocking communication events require polling network interface receive messages boundless polling consumes cpu cycles building infrastructure river single disk data represented group on-disk records collection record set named fields types catalog information meta-data system data accessed disk unordered collection unordered collections provide ordering constraints records collection application reading collection receive records arbitrary order subject optimizations system ordering desired application data accessed stream stream ordered set records application writes collection disk stream write order preserved applications accessing collection directly receive records order euphrates implementation underlying solaris unix file system ufs implement record collections read disk read directio enabled unbuffered read disk mmap interface deliver data raw disk rate sequential read access simple read interface directio leads double-buffering inside file system undesirable applications writes disk write system call directio enabled implemented top ufs layout information optimizations unordered collections implemented disk manager implementation river include disk manager top raw disk order exploit range scheduling optimizations enabled disk disk perturbation perturbation clients perceive imbalance bandwidth balanced clients client client figure graduated declustering diagrams depict scenarios graduated declustering perturbation unperturbeddisks deliver bandwidth perturbed disk delivers half left disk serving partitions clients perturbed half bandwidth application left unchecked result clients receive bandwidth clients bandwidths disk adjusted compensate perturbation case graduated desclustering adjustments client receives equal share bandwidth applications system access data spread multiple disks facilitate provide abstraction parallel collection grouping set single-disk collections single logical entity parallel collection facility tracks parallel meta-data names physical locations single-disk collection form parallel collection desired ordering single-disk collections euphrates parallel collection meta-data stored nfs nfs consistency guarantees concurrent access parallel meta-data operations serialized single process application operations rare occur file opened created performance bottleneck large scale clusters presence data availability strategy important data frequently unavailable due disk machine failures river applications increased data reliability availability choose mirror single-disk collection disks housed machines cluster interested exploiting 
redundant data contained mirrors improve consistency application performance building earlier work authors introduced chained declustering key insight chained declustering failure disk mirrored system read-only load balanced evenly remaining working disks balance achieved carefully-calculated distribution read requests mirror segments remaining disks generalize technique call graduated declustering order solve performance consistency problem common case disks storing mirrored collection functional offer bandwidth time reasons enumerated earlier individual reader traditional approaches mirroring variations unavoidable reader choose mirrored segment copy read entire segment variations lead global slowdown parallel programs slow clients complete fast remedy approach problem differently picking single disk read partition client fetch data data mirrors illustrated figure case data replicated disks disk disk client alternatively send request block disk block disk disk responds request desired block solve problem graduated declustering provide client reading set collections equal portion bandwidth application clients receive expected bandwidth disk mirrors receive bandwidth mirror compensation implementation graduated declustering observe bandwidth differences clients adjust bandwidth allocation appropriately euphrates implementation simple algorithm balance load data sources disk manages segments parallel collection continually receives feedback consumers total bandwidth consumers receiving performance inequity clients detected disk manager biases requests lagging client attempts balance rates readers progress result balancing shown right-side figure disks compensate perturbation disk allocating bandwidth clients resulting bandwidths client properly balanced module loop records process msg null operate message operate msg conditionally pass message downstream put msg completion return null figure module api simple river module module messages upstream performs operation calling user-defined operate conditionally put messages downstream river generic data flow environment applications similar parallel database environments volcano applications constructed component-like fashion set modules module logical thread control input output channel simple filter module record single input channel applies function record function returns true puts data single output channel modules connected machine machine boundaries queues queue connects producers consumers ratematching modules dynamically sending data faster consumers queues essential adjusting work distribution system begin execution application master program constructs flow flow connects desired set modules source sink time single module connected queuemust betweenthem flow instantiated master program computation begins continues data processed termination control returned master program module basic unit programming river modules operate records calling obtain records input channels calling put place output channels convenience refer set records moving system message logically module provided thread control one-input one-output module performs simple loop obtain records upstream channel operate records put pass records downstream illustrated figure complex modules input output case input output number argument put non-blocking versions interfaces ability perform select operation waits set channels ready returns control user euphrates modules written classes current implementation module thread control benefits drawbacks main advantage approach applications naturally overlap computation data movement user freed burden carefully managing thread switches costly amortize cost modules pass data set records large chunks experience complicated modules noticeable fashion felt inclusion complex buffer managementwas worth implementation effort queues connect multiple producers multiple consumers local machine distributed machines cases flow construction queues modules messages transmitted producers consumers modules side local distributed queues oblivious type queue interact messages river move arbitrarily system depending run-time performance characteristics constraints flow dynamic load balancing achieved routing messages faster consumers queues consumer improve performance ordering relaxed queues multi-producer queue consumer receive arbitrary interleaving messages producers ordering guarantee provided queue point-to-point producer places message queue message consumer receives messages receives receives ordering retain ordering disk-resident stream attaching single consumer single producer stream ordered property stream properly maintained implementation local queues data structures shared threads locking signalling protocol euphrates implementation interesting takes flavors general case lightweight randomized credit-based scheme balance load consumers push-based algorithm producer tracks number outstanding messages consumer sends messages randomly consumers messages outstanding threshold desired behavior automatically sending records nodes consuming higher rates implemented efficiently randomized algorithm adds near-zero cpu overhead top normal message transfer costs cases found load balancing provided larger-than-record size units sort module sorted input data pass entire sorted run disk write module order preserved exact section provide functionality implementation handed arbitrarily large set records pullbased algorithm consumers querying producers data balance load randomized push-based algorithm work case single bad decision costly guarantee provided version single consumer receive entire set records order load balancing occurs granularity large potentially unit handed simple copy program flow module instantiate module instances place ufsread file place ufswrite file attach read module write attach execute flow figure flow api simple reader writer flow shown ufsread module reads collection output input ufswrite module writes disk execute program river environment modules connected form flow flow graph data source sink intermediate stages dictated program phases involved instantiating flow construction operation tear-down construction master program specifies global graph describing data flow including modules specific interconnection construction phase complete master program instantiates flow operation phase threads created machines control passed modules flow data begins data sources flows system graph completion flow construction performed programmatically flow api provided graphically flow construction api simple add node graph place routine called module arguments read on-disk collection programmer ufsread module argument filename shown figure place returns module attach modules simpleattach interface interface graph edges figure simple copy flow formed read write module flow attached attaching modules places queue modules input output case user extra arguments attach routine input connect output finally instantiate flow ago interface provided starts threads performs attachments waits completion asynchronous version flow description point restricted single-machine flow specification sake simplicity construct parallel flows multiple machines programmer nodes place modules local distributed queues inserted program run spawned nodes system simple remote execution module internal system user add extra arguments attach routine details remote connections producers consumers single -to-a distributed queue -todistinct queues fully-connected graph euphrates implementation numerous languages program flows interface found overly cumbersome re-compile codes simple change flow provide tcl perl interfaces allowing rapid assembly flows scripting language finally built graphical user interface gui data flow graphs similar spirit tioga gui programmers select modules module library draw data flow graph desired user execute program generate flow construction code re-use gui variables added program enabling user easily construct generic programs simple copy user choose input output collection names variables generate general-purpose copy program 
general found simpler programmatic interface bug-prone conclude section discussion system expect typical programmer writing program spend time programming individual modules bulk application code live modules modules system order construct flow imagine user community interested similar problem areas libraries standard modules share tuned high performance achieving parallelism straight-forward user construct flow script gui tool nodes run system spawn modules multiple nodes easily generate desired connections queues local distributed modules focus river simply enabling construction high-performance parallel o-intensive applications seek provide framework building performance-robust programs system part solution transparently application writers graduated declustering algorithm enabling mirroring applications automatically gain robustness read perturbations component river performance robustness distributed queues inserted application writer cases place dqs depends program semantics difficult automate decision general dqs easily inserted embarassing parallelism cases producers place work queue consumers work queue individual rates addition dqs situations bit difficult requires solid understanding application construction performance-robust applications requires application writer construct optimize sequential modules describe flow connect inserting distributed queues spending programmer effort placement user gain return scalable application runs face variablerate producers consumers well-designed river application run high performance set machines highly varying performance characteristics bandwidth nodes distributed queue scaling ideal figure distributed queue scaling experiment scalability scrutiny run producersread data blocks disk put distributed queue sources pull data ideal line shows aggregate bandwidth disk section perform experiments validate expected performance properties system explore absolute performance adaptability distributed queue performance queue crucial system primary mechanism providing load balancing flow distributed queue effective balancing load consumers moving data faster consumers perform experiments graduated declustering performance enhancement mirrored collections balancing work consumers distributed queues solve problem achieving consistent performance single producer slows performance system drops proportionally case important system avoid producer hot-spot precisely transparently simple distributed algorithm adapt run-time perturbations data sources explore scaling behavior distributed queue experiment set-up data read disks put distributed queue consumed cpu sinks scale results scaling experiment shown figure graph reveals scaling properties ideal disk capable delivering disks expect peak read bandwidth data moving achieve percent peak distributed queue found scaling problems cluster size design aggressive algorithm producer sends data subset consumers performance writing disks shown scales equally percent peak static performance cliff reached single perturbation cpu nodes perturbed cpu consumer perturbation figure read perturbation figure shows percent peak performance achieved consumer perturbations added system balance load unperturbed consumers performance drops single consumer slowed performance unaffected large number nodes perturbed cpu perturbationsteals processor test consists producers separate consumers examine results consumers arbitrarily slower rest type perturbation arise dynamic load imbalance hot spots system due presence cpus disks performance capabilities figure shows effect slowing cpu consumers reading disks work pre-allocated consumers single consumer slows performance bad consumers slowed labeled static static allocation figure inserted producers disks consumers data flows unperturbed consumers flowing hot spots system cpus fully utilized unperturbed case noticeable performance drop-off perturbation consumers perturbed previous experiment form parallel read experiment parallel write experiment place cpu sources generate records disks system results write experiment shown figure static allocation behaves poorly slight perturbation case performance writing disks degrades immediately perturbation gradually falling fact performance slightly worse static application disks perturbation degradation disk bandwidth fully utilized begin unlike cpus read experiment single disk system perturbed total bandwidth reduced difference data unperturbed disks static application adapt demonstrated distributed queue desired properties balancing load data consumers mirroring producerof data unique percent peak static performance cliff reached single perturbation graceful degradation disk nodes perturbed disk consumer perturation figure write perturbation figure shows effect disk perturbation writes dynamically adapts system test consists disks falling performance cliff routes data bandwidth gracefully degrades case perturber continually performs sequential large-block writes local disk stealing roughly half bandwidth collection records complete flow deliver data consumers producers bottleneck system case streaming large data sets slowing single producer lead large global slowdown program complete slow producer finished producer problem exact problem graduated declustering attempts solve describe experimental validation graduated declustering implementation find absolute performance behavior perturbations expected initial implementation performance graduated declustering reads disk perturbation slightly worse non-mirrored case direct result design fetches data mirrors selecting single order ready adapt performance characteristics change multiplexing streams single disk slight cost seek occur streams increasing disk request size amortizes cost seek achieve percent peak nonmirrored bandwidth figure writes disks incur problem real strengths read-intensive workloads decision support data mining cases applications reading non-adaptive mirroring system slow rate slow disk system system shifts bandwidth allocation disk consumer data receives data rate aggregate bandwidth ideal nodes graduated declustering scaling figure graduated declustering scaling graphs shows performance scaling performance loss due fact reads actively mirrors segment seek cost incurred roughly peak performance delivered percent peak static disk nodes perturbed graduated declustering perturbation figure read perturbation graphs shows performance read perturbation performance degrades slowly case typical nonadaptive mirrored system suffers slowdown perturber competing read-stream disk results -machine experiment shown figure scenario half machines serve disk nodes half serve data consumers explained performance mirroring compared mirroring slightly worse unperturbed case single perturbation slows application non-gd system bandwidth slow disk case delivers data roughly half peak rate due single competing stream performance degrades slowly spreading bandwidth evenly consumers disks equally perturbed performance dips non-gd system due overhead seeking multiple streams finally perturbing write stream collection mirror expected effect slowing write speed slower disk sense represents fundamental cost mirroring applications write scratch data data lesser mirroring potential performance cost describe initial application experience begin unmodified sequential program river infrastructure proceed parallel applications written parallel sort parallel hash-join section focuses application writers add robustness applications distributed queues show performance mirroring enabled mirroring transparent user application examine trace-driven simulator generation version file system simulator complex sequential program simulates multiple file system layout policies buffer management includes complete disk simulator application river fast data source simulator modified order access data river system simulator loads data river system simple copy-in script accesses copy-out script constructs flow record collection standard output piped standard input simulator case main benefit river fast switch-based network application disk river simulator accesseddata nfs file server ethernet shared network parallelism application distributed queues provide robust performance set experiments involve 
complicated application external sorting case program written river environment sort good benchmark clustered systems performance largely determined disk memory interconnect bandwidth compare external sort built river framework ideal statically partitioned sort ideal parallel sort reads data disk full disk bandwidth takes time perform in-memory sort writes back disk full bandwidth overhead parallelism sake simplicity single-pass sort records read memory sorted written disk single pass eventually plan extend work include two-pass sorting places severe memory management demands system figure presents flow data simple version external sort river flow similar nowsort data begins unsorted parallel collection number disks data read disk node disk read module passed partitioning module partitioning modules perform key-range partitioning data partitioning module reads top bits record determine sorter module record sorter module received input sorts data begins streaming disk write module proceeds write data disk stream preserving order application proceeds phases read partition sort write simple single-pass external sort figure parallel external sort river figure depicts logical data flow single-pass external sort data proceeds disk set static partitioners split data based key range record set sort modules data read sort modules sort parallel hand data write modules send disk ordered stream markers circled points flow altered add robustness discussed discuss scaling behavior sort figure shows result scaling river sort machines graph compares river sort idealized staticallypartitioned parallel sort performance sort river framework begins peak efficiency drops slightly nodes majority inefficiency attributed poorly tuned in-memory sort contributes total elapsed time un-tuned in-memory sort learn graph easy build high-performance non-trivial application lose efficiency inside framework application write single line code manage qualitatively application writer write partitioning module sort module scaling parallel sort matter constructing proper flow stands simple river parallel sort robust performance perturbations graduated declustering sort tolerate read perturbation focus read partition write phase sort potential performance robustness examine perturb partition modules achieve reasonable performance add level robustnessto partition phase insert distributed queue disk read modules partitioners labeled circled figure sort partitioning data order imposed stage sort inserting distributed queue performance characteristics sort correctness figure shows result perturbing partitioner modules experiment disk modules sort modules set machines partitioners set machines total perturbations applied partitioners partition modules slack system overloaded degrading slowly partition nodes perturbed location flow modified avoid percent peak nodes ideal static sort river sort sort scalability figure parallel external sort scaling figure shows scaling behavior sort built river framework compared idealized statically-partitioned sort river sort scales deficiency under-tuned in-memory sort run-time perturbations sort modules disk write modules labeled figure desire tolerate disks slowing write phase simply move records arbitrarily disks preserve set sorted partitions generated sort module balancing load disks record-level balance load higher level granularity dynamically deciding place sorted partition order balanceload amonga consumers data producers data items produced original form sort allocates single sort module producer consumer remedy load balancing disks allocate sort modules wherea small constant note produces slightly modified output sorted partitions performance load balancing sorted-runs disk perturbation shown figure expected writing runs performance degrades gracefully static allocation full perturbation performance lower expected case overhead current implementation results peak performance roughly lower expected hash-join important database operation extensively decision-support benchmarks tpc-d hash-join takes collections records input outputs pairs equal values join key one-pass two-pass variants exist pass algorithm suitable smaller collection fits aggregate cluster memory simplicity discuss one-pass hash join figure shows flow data phase smaller collection building collection hash table built read disk partitioned hash function nodes internally hashed inside join module labeled diagram prepare join phase phase probing collection read disk partitioned nodes hash function records pass join module matching records building collection found output proceeds immediately percent peak ideal static sort river sort nodes perturbed sort partition perturbation figure perturbing sort partitioner figure shows sort partition modules perturbed disk sort modules run set machines partition modules run set river sort compared perfect sort statically partitioned perturbation steals percent cpu static percent peak disks perturbed sort write perturbation figure perturbing sort writer figure shows performance writers perturbed write phase sort runs set machines writing disks separate set machines case perturbation competing write-stream disk disk phase reading collection writing output operate concurrently addition distributed queues hash join similar sort queue data sources partitioners allowing faster partitioners partition data join performed output relation hash form inserted easily balancing load disks application wishes output records hashed partitions situation similar balancing sorted runs external sort employed interestingly hash-join avoid performance perturbations join modules replication building collection replicated nodes record partitioned probing phase dynamically choose sites functionality limitations current infrastructure performance numbers hash-join scale initial results machines promising simple one-pass hash figure parallel external hash river figure depicts logical data flow single-pass hash-join solid lines path relation disks hash-partition modules hashjoin modules dashed lines path relation similar path relation relation passes hash-join modules join performed output relation generated river relates work number distinct areas file systems programming environments database research section discuss work areas high-performance parallel file systems abundant literature ppfs galley vesta swift cfs sfs sio specification assume performance dictated slowest component system devoid specific programming model applications constructed single-program multiple-data spmd -like fashion parallel file system deliver consistent high-performance wasted inside rigidly-designed program advanced parallel file systems higherlevel interfaces data collective similar concept expressed two-phase original paper kotz found scientific codes show tremendous improvement aggregating requests shipping underlying system nodes schedule requests noticeably increase delivered bandwidth requests made returned specific consumers load balanced consumers dynamically types systems provide flexibility interface solve problems common today clustered systems finally recent file-system work extolling virtue adaptive systems hardware systems increase complexity argued intelligent software systems extract performance underlying machine architecture systems employ off-line reorganization improve global performance goal river balance load on-line run-time long-term adaptation system number popular parallel programming environments support spmd programming style including messaging passing environments mpi pvm explicit parallel languages splitc packages provide simple model parallelism user allowing ready construction parallel applications provide facility avoid run-time perturbations adapt hardware devices differing rates experience writing parallel external sort split-c led realize problems spmd approach run sort now-sort broke world record databaseindustry standard sorting benchmarks 
difficult attain high-level performance consistently parallel programming environments aligned river design philosophy run-time adaptivity examples include cilk lazy threads multipol systems balance load consumers order highly-irregular fine-grained parallel applications main difference river systems granularity communication river limits workloads data pushed interconnect large-sized blocks systems run-times general-purpose parallel programming focus finegrained irregular applications today clusters latency remote memory higher latency local memory orders magnitude microseconds versus nanoseconds forces locality dominant issue systems remote bandwidth worse local bandwidth difficult hide remote memory latency data pushed system cost systems attempt deal problem slow producers important environment similar river environment linda shared globally-addressable tuple-space parallel programs applications perform atomic actions tuple-space inserting tuples querying space find records attributes generality model high performance distributed environments difficult achieve distributed aspects river built top linda suffer performance scaling problems relevant river large body work parallel databases data flow techniques well-known database literature stems naturally relational model system takes advantageof unordered processing records ibm smps system shared data pools accessed multiple threads faster threads acquiring work referred straw model thread slurps data straw potentially rate implementing system natural smp simple lock-protected queue suffice modulo performance concerns river argue type data distribution performed cluster due bandwidth interconnect number parallel databases found literature including gamma volcano bubba systems similar techniques distribute data processes gamma split table volcano exchange operators generalized split table river move data producers consumers distributed memory machine static data partitioning techniques hash partitioning range partitioning round robin functions adapt run-time load variations consumers current commercial systems ncr teradata machine exclusively hashing partition work achieve parallelism good hash function effect dividing work equally processors providing consistent performance achieving good scaling properties jim gray recently teradata system performance bad worse consistency scalability goals system cost underlying hardware future research areas explore enhancements system infrastructure serve move system realm system expert programmers easily process data placement process placement data placement important decisions determined user river ideal system decisions automated higher-level entity compiler query planner process data migration river moves data system effectively initial experience suggests feasibility code migration improve dynamic performance properties system long-term data migration short-term locally optimal placement decisions re-evaluated result data movement optimize current usage application fault tolerance ultimate goal write applications river interface robust performance continue operation machine failure similar work dynamic programming environments form automatic check-pointing solution suggested river well-suited large class external distributed applications including traditional scientific codes multimedia programs evidence exists literature volcano scientific data-intensive applications programmed optimized volcano data-flow environment plan exploring add robust performance features types applications finally developing simple models performance faults affect system welldeveloped analytical models easily compare performance system versus theoretical ideal perturbation scenario hardware software systems spiral size complexity systems designed controlled environments experience performance defects real-world settings long realized area wide-area networking end-to-end argument pervades design methodology protocol stacks tcp systems clear globally-controlled well-behaved environment attainable applications system treat black box adjusting behavior dynamically based feedback system achieve performance current circumstances complexity slowly grown point manageability smaller distributed systems comprised largely autonomous complicated individual components clusters exhibit properties problems larger scale wide-area systems problem exacerbated clusters move serving general-purpose computational infrastructure large organizations resources pooled shared computing machine hundreds thousands jobs users present system difficult impossible system behave orderly fashion address increase complexity decrease predictability introduce river substrate building o-intensive cluster applications river confluence programming environment system extending notion adaptivity flexibility lowest levels system application river programs reliably deliver high performance system resources over-committed performance applications written style degrade gracefully avoiding sudden frustrating prolongations expected run time initial study applications found avoiding perturbations consumers straight-forward distributed queues important issue balancing load granularity ordering required applications fine-grained applications balance load level individual records simplest construct performance-robust manner distributed queues proven excellent load balancers require programmer insert flow avoiding perturbations producers problem solved river graduated declustering dynamically shifting load perturbed producers system delivers proper proportion bandwidth client application high-performance clusters consistent performance easy bad peak performance matter persistence good run goal river environment source code request foremost jim gray advice encouragement alan mainwaring work support active messages sine qua work anonymous reviewers helpful comments finally andrea arpaci-dusseau amin vahdat i-store group berkeley suggestions improved presentation content paper work funded part darpa -cdarpa -cnsf cda nasa fdnagwand california state micro program supercomputers plug play economist november arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations sigmod arpaci-dusseau arpaci-dusseau culler hellerstein patterson searching sorting record experiences tuning now-sort spdt aug bal kaashoek tanenbaum orca language parallel programming distributed systems ieee transactions software engineering mar barclay barnes gray sundaresan loading databases dataflow parallelism sigmod record acm special interest group management data december bershad black dewitt gibson peterson snir operating system support high-performance parallel systems technical report ccsfscalable initiative caltech concurrent supercomputing facilities caltech blumofe joerg kuszmaul leiserson randall zhou cilk efficient multithreaded runtime system proceedings symposium principles practice parallel programming july blumofe lisiecki adaptive reliable parallel computing networks workstations usenix editor annual technical conference january anaheim pages berkeley usa jan usenix boden cohen felderman kulawik seitz myrinet gigabit-per-second local area network ieee micro february cabrera long swift distributed disk striping provide high data rates computing systems fall carriero implementation tuple space phd thesis department computer science yale december chakrabarti deprit jones krishnamurthy wen yelick multipol distributed data structure library technical report csd- california berkeley july choudhary bordawekar harry krishnaiyer ponnusamy singh thakur passion parallel scalable software input-output technical report sccsece dept npac case center syracuse september codd relational model data large shared data banks communicationsof acm june published readings database systems edition stonebraker hellerstein morgan-kaufmann copeland alexander boughter keller data placement bubba sigmod record acm special interest group management data sept corbett feitelson vesta parallel file system acm transactions computer systems august council tpc-d individual results http tpc results tpc results page html culler dusseau goldstein krishnamurthy lumetta von eicken yelick parallel programming split-c proceedings supercomputing pages dewitt gray parallel database systems future high-performance database systems communications acm june dewitt ghandeharizadeh schneider performance analysis gamma database machine sigmod record acm special interest group management data sept geist andv sunderam evolution pvm 
concurrent computing system compcon february gelernter carriero chandran chang parallel programmingin linda degroot editor international conference parallel processing pages goldstein schauser culler lazy threads implementing fast parallel call journal parallel distributed computing aug graefe encapsulation parallelism volcano query processing system sigmod record acm special interest group management data june gray processors infinitely fast storage free invited talk iopads november hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international data engineering conference pages huber elford reed chien blumenthal ppfs high performance portable parallel file system proceedingsof acm international conference supercomputing pages barcelona july acm press kitsuregawa tanaka moto-oka grace relational algebra machine based hash sort design concepts journal information processing society japan kleiman voll eykholt shivalingiah williams smith barton andg skinner symmetric multiprocessing solaris proceedings compcon spring kotz disk-directed mimd multiprocessors proceedings symposium operating systems design implementation pages usenix association november updated dartmouth pcs-tr november kubica robey moorman data parallel programming khoros data services library lecture notes computer science lindsey smp intra-query parallelism udb database seminar berkeley february loverso isman nanopoulos nesheim milne wheeler sfs parallel file system cmin proceedingsof summer usenix technical conference pages mainwaring culler active message applications programming interface communication subsystem organization technical report csd- california berkeley october matthews roselli costello wang anderson improving performance log-structured file systems adaptive methods proceedings symposiumon operatingsystems principles sospvolume operating systems review pages saint-malo france october acm sigops acm press meter observing effects multi-zone disks proceedings usenix conference jan nieuwejaar kotz galley parallel file system proceedings acm international conference supercomputing pages philadelphia acm press nitzberg performance ipsc concurrent file system technical report rnd- nas systems division nasa ames december pakin lauria chien high performance messaging workstations illinois fast messages myrinet proceedings acm ieee supercomputing conference december san diego convention center san diego usa acm press ieee computer society press papadopolous untitled talk winter retreat july ritchie stream input-output system bltj part october saltzer reed clark end-to-end arguments system design acm transactions computer systems pages november scales lam transparent fault tolerance parallel applications networks workstaions proceedings usenix conference jan seltzer small self-monitoring self-adapting systems proceedingsof workshop hot topics operating systems chatham shapiro join processing database systems large main memories acm transactions database systems sept stonebraker chen nathan paxson tioga providing data management support scientific visualization applications international conference large data bases vldb pages san francisco usa aug morgan kaufmann publishers mpi forum mpi messagepassing interface proceedings supercomputing pages november von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings acm symposium operating systems principles pages december von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedings annual international symposium computer architecture pages gold coast australia acm sigarch ieee computer society tcca computer architecture news winter auerbach big time winter vldb survey database programming design wolniewicz graefe algebraic optimization computations scientific databases vldb pages 
searching sorting record experiences tuning now-sort andrea arpaci-dusseau remzi arpaci-dusseau david culler joseph hellerstein david patterson computer science division california berkeley dusseau remzi culler jmh pattrsn berkeley present experiences developing tuning performance now-sort parallel disk-to-disk sorting algorithm now-sort holds world records databaseindustry standard benchmarks critical tuning process setting expectations programmer tune stop found categories tools tools set expectations configure application hardware parameters visualization tools animate performance counters search tools track performance anomalies tools interact layers underlying software operating system applications leverage modern features threads memory-mapped march debate benchmark prototype cluster system decided implement external disk-to-disk parallel sort external sorting qualities desired benchmark external sorting memory intensive sorting stresses aspects local distributed operating system performance well-understood sorting algorithms exist sequential parallel environments finally presence industry-standard benchmarks compare performance large-scale systems time -processor sgi challenge held worldrecords existing benchmarks datamation benchmark sgi sorted million -byte records -byte keys disk disk seconds minutesort -byte records sorted minute goal surpass records cluster ultrasparc workstations connected high-speed network year april april fool day people berkeley project laid claim benchmark records machines reduced time sort million records seconds importantly seconds sorted machines now-sort datamation minutesort records stand today now-sort algorithms performance measurements previous paper relate difficulties scaling memoryand o-intensive application machines learned scalable systems disk performance cache-sensitive algorithms operating system interfaces memory management paper focus methodology tuning now-sort tools found methodology achieving scalable performance relied setting meeting optimistic performance expectations began forming set performance goals phase sort repeatedly characterized implemented progressively larger components sort measured implementation met expectations found discrepancies focused optimization efforts slow phases sort fixing performance bugs application underlying system refining expectations realistic expectations phase algorithm knew start tuning process stop found performance tools distinct purposes setting expectations visualizing measured performance searching anomalies configuration tools helped define expectations resource measuring best-case performance achieved output tools parameterize sort hardware hand allowing sort achieve peak utilization variety machine configurations found simple visualization tools allowed quickly identify performance problems animating machine performance counters notice nature problems arose emphasize importance performance counters levels system cpu bus network switches links easily implement simple tools areas implement hierarchical search tool remove run-time anomalies tool helpful final stages efforts achieve peakperformance undisturbed run now-sort tool forced track anomalies hand isolating problem workstation identifying aspect machine differed paper organized roughly follow chronology experience begin section describing upa data bus sysio asic asic address bus upa processor data bus bmx chips cache processorultrasparc memory simms interface seagate hawk rpm scsi bus fast-wide network myrinet -bit mhz s-bus scsi bus ultra workstation fast-narrow seagate barracuda rpm figure ultra workstation figuredepicts internal architecture ultra workstation bmx crossbar connection memory sysio main controller cluster hardware software section discuss development tuning single-node version algorithm forms core parallel implementation section develops model basic algorithm parallel version now-sort difficulties scaling full-sized system detailed section section relate experience isolating performance problems large systems call finding needle now-stack finally conclude reflections experience berkeley cluster consists commodity ultra workstations ultra workstation single ultrasparc processor on-chip instruction data caches unified second-level cache base memory hierarchy dram diagram internal architecture ultra workstation shown figure system ultra centers s-bus bus peak theoretical bandwidth bus workstation houses internal rpm seagate hawk disks single fast-narrow scsi bus peak experiment disk bandwidth node added additional rpm seagate barracudas disk extra fast-wide scsi card peak configurations cluster generally viewed homogeneous collection workstations extra disks run faster rates providing challenge conscious programmer addition usual connection world ethernet workstation single myrinet network card attached s-bus myrinet switchbased high-speed local-area network links capable bi-directional transfer rates myrinet switch ports -node cluster comprised -node clusters switches connected -ary tree-like structure shown figure -bit s-bus higher peak rate devices bits bus peak wsws figure berkeley cluster figure shows workstations connected eight-port myrinet switches groups comprise entire -node cluster tuning user-level application performance now-sort directly affected number important software sub-systems including multi-layer operating system communication layer machine cluster runs copy solaris multi-threaded version unix strengths solaris include efficient support kernel-level threads well-developed unix file system sophisticated memory management machines running solaris cluster make present illusion single large-scale system end user cluster employs glunix prototype distributed operating system berkeley glunix monitors nodes system load-balancing coschedules parallel programs job control redirection measurements place dedicated environment primarily glunix parallel program launcher parallel versions now-sort written support split-c library split-c parallel extension supports efficient access global address space distributed memory machines split-c library group synchronization communication primitives including barriers reductions communication utilized active messages communication layer designed low latency high bandwidth switch-based networks active message essentially restricted lightweight remote procedure call process sends active message specifies handler executed remote node message received handler executes atomically respect message arrivals version active messages gam supports single communicating process workstation time gam myrinet round-trip latency roughly uni-directional bandwidth node sending receiving tuning application programmers set performance expectations hope achieve mflops rating absolute execution time level speed-up expectations programmer determine point reached sufficient performance times expectations vaguely defined worse omitted experience shown explicitly defining expectations important components system greatly simplify tuning complex applications section describe experience developing meeting expectations primary building-block nowsort single-node algorithm path contained simple steps carefully chose initial expectations based models measurements system operating ideal circumstances implemented straight-forward algorithm potential achieving goals finally measured performance phases implementation focused efforts portions meet expectations sorting benchmark stresses memory sub-systems machine expectations focused memory system performance designing algorithm enumerated goals single-node sort perform minimum amount required avoid paging virtual memory ensure dominates total run-time transfer data disk peak sequential rates expectation basic disks slowest component system minimized general performance achieved performing extra computation additional memory copies accessing disk sorting minimum amount required determined amount memory system records sorted fit memory one-pass sort record read written disk two-pass sort required record read written disk memory explicitly managed paging virtual memory disk occur paging damaging environment disks holds records sorting swap space paging hurt memory performance interferes sequential access patterns disk transfers expectation motivated previous work diskto-disk sorting previous researchers found clever ways hide cost internal sorting overlapping disk goal implementation finally transfers disk proceed 
maximum rate achieve maximum throughput disk seeks optimization performed two-pass model sorted run pass temporarily written disk read back minimized achieved accessing disk sequentially sufficiently large transfer sizes sorting accesses data disk regular deterministic fashion goal attainable expectations predict desired time sort function number records size record expected time amount memory peak read write disk bandwidths determining amount memory peak disk bandwidth system required implement simple configuration tools memory configuration tool expectations require now-sort aware amount memory ideal case amount reported simple system call operating system solaris give accurate assessment free memory developed user-level tool accessing amounts memory monitoring paging activity memory configuration tool accurately estimates amount user program safely allocate machine physical memory configuration tool determined memory user applications tool number limitations notably memory remain constant tool application execute sufficient forming expectations determines passes required records disk specifies number records sorted pass paging seagate disks scsi bus rpm hawk narrow rpm hawk narrow rpm barracuda wide rpm barracuda wide hawks barracudas table disk bandwidth expectations read write columns show peak bandwidth reported disk configuration tool measurements system required determine scsi-bus limits peak performance disk configuration tool concretely set expectations peak disk bandwidth developed simple disk configuration tool program serves purposes automatically adjust amount data striped disk handle disks differing performance characteristics reports maximum achievable bandwidth processor performing work tool unaware bottlenecks system set unrealistically high expectations achievable bandwidth multiple disks read rpm disk disk configuration tool revealed read disks due saturation fast-narrow scsi bus naive model extrapolated expectation difference full set expectations disk performance determined disk configuration tool shown table expectations well-defined began developing now-sort code single workstation implementation distinct code paths depending passes required sort records memory describe experience tuning one-pass sort two-pass sort initial version one-pass single-node sort contained simple steps read program begins opening input file read system call transfers -byte records disk main memory internal sort quicksort performed keys records memory quicksort -byte key separated -byte record pointer set full record separation sort operate efficiently swapping keys pointers full records quicksort write list sorted keys pointers traversed order gather sorted records output buffer output buffer fills written disk write system call finally fsync called ensure data safely disk file closed measurements revealed initial implementation failed meet expectations ways fewer records expected sorted paging disk time internal sort noticeable percentage total execution time briefly discuss located fixed performance problems track unexpected paging activity monitored disk traffic increasing number records sorted visually observed traffic implementing small performance meter diskbar detail tool found sorting records machine memory small amount paging traffic occurred increased input size traffic steadily worsened machine thrashing sort write phases readily diagnosed classic problem read system call file system unnecessarily buffering input file size file reached half memory operating system forced discard memory pages replacement policy chose discard running sort program buffered useless input file switching mmap interface madvise inform file system sequential access pattern found sort records paging desired evaluate shorten internal sorting time monitored cpu utilization cpubar home-grown visualization tool executing now-sort low cpu utilization observed read phase implied overlap part internal sort read phase suggested performed bucket sort reading records disk partial-radix sort optimization reduced in-memory sort time onemillion records disks total execution time cache configuration tool reduce time spent stalled memory keys touched partial-radix sort made fit second-level cache optimal number keys bucket determined size cache current implementation optimized cache ultrasparc-i running now-sort cache architecture require size second-level cache determined look-up hardware parameter table cache configuration tool similar microbenchmarks sort trivially modified accept cache size parameter hard-coded constant two-pass version now-sort leverages code optimized one-pass sort highestlevel phases create runs application repeatedly reads portion input file sorts writes sorted run disk kernel-threads overlap reading writing disk effect one-pass sort called multiple times time half memory merge runs sorted runs read disk merged single sorted output file run created phase mapped memory records written disk record lowest-valued key picked top run copied output buffer output buffer fills written disk measurements initial two-pass implementation revealed creating runs expected amount time merging slower desired observing delivered read disk bandwidth simple diskbar tool quickly illuminated problem reading sorted run sequentially layout multiple runs disk resulted disk seek read significantly reducing achieved bandwidth achieve near-peak bandwidth disks now-sort amortizes disk seek-time carefully managing merge large amount data explicitly prefetched run reading empirically found prefetch-buffers sufficiently large achieve near-sequential performance kernel threads required successfully prefetch data writing disk reader thread begins copying run merge buffer merger thread waits set buffers full copies record lowest-valued key output buffer output buffer fills writer thread writes buffer disk marking buffer empty finished seek configuration tool general buffer size merge phase calculated automatically function disk characteristics disk configuration tool return disk seek time peak disk bandwidth application non-sequential transfers simple calculation reveals required size transfer obtain desired fraction peak disk bandwidth ability focus single-node performance essential tool debugging performance analysis parallel machines now-sort optimize cache memory processor performance single-node algorithm behavior hold parallel algorithm found automatic configuration high-performance applications adapt wide variety system parameters adapting now-sort system parameters simplified concentrating behavior single node end developed memory capacity disk bandwidth configuration tools cache disk seek time tools dynamic environment tools served dual purpose adapting sort varying hardware parameters differing speed disks differing amounts memory crucial setting expectations expectations determine achieving acceptable transfer rates disks making effective memory traditional tools report code spending time report stop tuning profiling final code inform code bound nowsort implies tuning completed applications imply tuning needed identifying performance problems single node simplified feedback small set visualization tools implemented cpubar tool displays current cpu utilization divided user system wait idle time diskbar displays delivered bandwidth local disk including swap partition current text-based tools top iostat report similar information update statistics found update rates order ten times capture bursts resource usage cpu tool textual counterpart enabled visualize summation multiple components simultaneously anomalies simple tools found incredibly giving instantaneous continuous view system behavior today rarely run applications concurrent monitoring tuning in-memory behavior sort easier expected largely due development cache-sensitive algorithms previous 
researchers early experience ran shade instruction set simulator evaluate implementation largely becausewe easily match reported statistics back specific lines user-level code information tuning shade trace kernel code important interactions now-sort algorithm internal sort consumed larger fraction total execution time tools specifically aimed finding memory bottlenecks cprof memspy fine-tuning codes interact underlying software systems communication layer crucial tools give information user-level code modifying tools work tandem complete machine simulator simos focusing single-node performance developed excellent building block scalable sorting algorithm step large-scale parallelism define set expectations parallel performance defining expectations implemented algorithm small number machines small cluster configured full set disks found implementation meet initial expectations single-node version now-sort found discrepancies expectations measured performance found removed inefficiencies implementation parallel version differences found due unrealistic expectations based overly simplistic model system refining expectations measuring small cluster determine hardware configuration entire cluster machines ideal case process executing parallel application sorts number records amount time single-node version scalability now-sort perfect problem size increased linearly number workstations microbenchmarks active message communication performance transferred nodes implication level bandwidth sending set records node requires noticeably time reading records disk add parallel expectation enumerated single-node sort overlap communication phases sort sections present algorithm attempts hide cost communication read phase sort discuss expectation met amount disk bandwidth node increased reduce complexity tuning parallel sort chose programming model explicit control operations performed node single-program-multipledata spmd spmd programming model active messages one-pass two-pass parallel sorts direct extensions single-node algorithms ease design due largely active messagesparadigm communication integrated on-going computation natural fashion change single-node versions occurs records initially read disk record read disk range-partitioned set workstations machine receives roughly equivalent fraction data workstation receiving lowest-valued keys workstation lowest records temporarily buffered transmission multiple records single active message achieving communication bandwidth receipt message active message handler splits keys buckets copies record record array internal sort write steps entire merge phase two-pass sort identical single-node versions require communication nodes evaluation oneand two-pass parallel sorts began measurements four-node cluster ultrasparc workstations disk initial experiments revealed desired properties algorithm communication costs completely hidden wait time unnecessary paging data transferred disk peak bandwidth negligible internal sorting time added disks cluster found performance increase expected found direct method isolating performance problem run application hardware performance counters measure behavior program difficulty methodology environment ultra workstation cpu performance counters capture memory traffic bus traffic solution employ sophisticated machine ultraenterprise array performance counters memory bus bus machine -processor smp shut processors utilized machine ordinary workstation extraordinary counters running sorting code counters produced performance profile revealing disks workstation cpu bus saturated cpu reaches peak utilization due cost initiating reads disk simultaneously sending records remote nodes copying keys buckets bus saturates handling disk network send network receive traffic byte read disk roughly bytes travel bus architectural bottlenecks reached benefit overlapping reading disk communication communication costs hidden disks workstation performance expectations mitigated previous models based naive understanding machine architecture actual measurements foresee bottlenecks fact originally projected disks machine ideal purchase entire cluster running small number well-configured machines understand architectural limitations hardware environment correctly shape future hardware purchases entire -node cluster sort keys cost one-pass sort disk system fill memory half desired sort time minutesort seconds disks give cost-performance workstation system read write approximately machine memory roughly records accounting memory requirements operating system application workstation physical memory result calculations hardware entire cluster upgraded disks workstation memory workstation due budget donation limitations insufficient memory one-pass attempt minutesort benchmark measurements continued rely two-pass sort accurate models predictions performance correctly purchase hardware large systems modeling memoryand o-intensive parallel program non-trivial task accurately predict resource usage running now-sort small well-configured cluster gauge worthwhile hardware purchases entire system models easily constructed comprehensive set hardware counters levels machine modern processors reasonable set performance counters shown detailed performance profiling components machine researchers shown network packet counters extremely monitoring in-coming out-going packets minimally -bit counters interconnection system memory bus workstation switches network counters track number bytes pass monitored part system resource utilization specific information device helpful disks report seeks transfers include tags differentiate traffic pid counters tracing short periods activity facilitate detailed studies portions code collecting data techniques feeding results back user standard performance monitoring tools cpubar diskbar step allowing user bottlenecks system advanced systems counters pinpoint troublesome sections program interest expectations set fulfilled small number machines ready scale now-sort full cluster size section discuss difficulties encountered scaling processors general difficulty debugging scaling algorithmic aspects sort challenge arose scaling characteristics underlying sub-systems distributed operating system communication layer sub-systems found performance slowly degrade workstations fell cliff radical drop performance occurred workstations cluster size system developers stopped evaluations datamation minutesort benchmarks start-up time application included measurements essential task glunix bottleneck large cluster sizes encountered start-up times seconds medium-sized clusters nodes starting job nodes required seconds glunix developers tracked interesting performance bug set group job glunix daemon performing nis request centralized nis server large parallel job meant clients simultaneously requesting data nis server shared ethernet responds client sequentially removing call nis server trivial local system call set group found thought performance problem removing group-id bug slowed start-up time manual instrumentation code revealed bottleneck member parallel program set tcp connections start-up process redirect standard input output error glunix client -way parallel job opens socket connections home node large parallel jobs burst overflowed default socketaccept queue entries overflow forced clients timeout retry connection multiple times drastically increasing start-up time large applications time-out retry occur previously nis request serialized portion code serialization removed bottleneck appeared fortunately size solaris tcp accept-queue readily modified removing bottlenecks glunix improved start-up time acceptable ideal range seconds running now-sort application large configurations stressed communication sub-system distinct ways basic issues arose errors communication hardware insufficient bisection bandwidth general problems communication layer difficult isolate indistinguishable performance problems application myrinet local-area network designed provide mpp-like backplane local area network setting goal export low-overhead high-throughput communication layer design generic active messages gam myrinet assumed complete hardware messaging reliability medium-sized clusters assumption reasonable bit errors rarely encountered processors largest existing myrinet cluster day bit errors occurred runs now-sort application errors manifested crc errors communication layer recognized recover terminating application 
current version active messages performs reliable message transfer top reliable medium temporary solution gam reduce number outstanding messages allowed flow-control layer limiting load network solution desired effect reducing chanceof bit errors disastrous side-effect halving achievable bandwidth worse change made knowledge shortly april deadline sorting results theoretically sorted seconds sorted seconds stands current minutesort record large configurations bisection bandwidth network insufficient all-to-all communication pattern now-sort development reasons earlier phases project distinct clusters roughly machines network topology designed bandwidth clusters sufficient insufficient links clusters tracking bandwidth deficiency network difficult anticipated due deadline pressures sufficiently micro-benchmark network running now-sort assumed larger network continued meet expectations developed smaller cluster observe utilization links switches internal network performance counters readily differentiate poor performance due insufficient bandwidth poor performance due algorithmic characteristics found adding links bandwidth solved problem difficulties learned number valuable lessons application writers debug programs running large number nodes interactive environment realistic assume programmers track bugs smaller number nodes now-sort algorithm scalable cases underlying system large-scale systems system designers access tools restricted application-level programs support system-level performance tuning hoc laborious process parallel profiling tools quartz modified cluster environment developers glunix tool paradyn helpful avoiding lengthy recompiles allowing dynamic instrumentation interesting pieces code providing visual feedback isolating performance problems frustrating dynamic environment libraries daemons system services change altering application binary case one-line change active message library lead devastating performance results tools understand complete dependence tree application notify user occur helpful developmental systems configuration management tools good match problem difficulty approach chain dependencies environment applications link glunix library access glunix glunix library changed glunix user-level daemon contacted library finally large-scale systems commonly tuned target system size research environment largest common platform testing machines due contention resources developers result system behaved size quickly found system scale desired full cluster size implication system development groups investigating build systems consisting workstations entire system workstations greater system scaled final step chasing sorting record successful unperturbed run sort performance now-sort sensitive disturbances requires dedicated system achieve peak results section discuss solved set run-time performance problems foreign agent competing process inadequate memory full disk machines slowed entire application general hierarchical process similar search process step finding needle now-stack slow workstations cluster identified step removing needle specific problem slow workstations identified solved process parallel sort performs amount work machine disturbance slows entire application now-sort exhibited lower expected performance isolated nodes disturbances ways ran glunix status tool glustat reports load average virtual memory workstation cluster nodes anomalous values easily singled nodes appeared homogeneous glunix ran now-sort additional timing statistics displayed workstation phase workstations noticeably slower phase pointing machine needle slow machine isolated key find machine compared cluster common needles found machines faulty hardware extra processes competing cpu memory extra data disks faulty hardware needle encountered consisted machines faulty misbehaving hardware beginning aware machines -node cluster unavailable due faulty power supplies motherboards machines adequate running compute-intensive sequential processes network-interface disk problems cluster machines found usable now-sort cpu stealing trivial common needle foreign job running workstations competing process slows sorting process workstation slow workstation identified standard process monitoring tools top identified culprit killed memory hog common performance problem caused difference free memory machines notable needle scenario glunix master daemon workstations running sort separate administrative machine cpu requirements master negligible memory footprint result sort process running master began page interfering sequential activity tracking aberrant glunix process proved difficult expected due small differences involved disk layout performance now-sort sensitive layout files local disk multizone disks give significantly higher bandwidth data allocated outer tracks full disks lead expected data rates disk configuration tool verified read performance varies significantly dependingon disk layout times tracking slow node quick inspection revealed scratch disk full user temporary files removed files sort performance returned expected level hierarchical methodology isolating problematic workstations identifying specific disturbances required performance isolation workstations hard boundarybetween workstations allowed drill-down focus performance component cpu memory system disk large-scale systems resources aggregated single interface smps raid-style disk systems lend naturally hierarchical approach strategy allowed successfully solve performance problems required fair amount intelligent user intervention automatic approach desirable users familiar components system problematic workstations flagged sophisticated tool performance assertion checking tool simple assertions performance expectations code flagged succeed standard parallel tools search process limitations prevented limitations arise due advanced features solaris leverage kernel threads memory-mapped files paradyn work threaded applications eliminates single-node one-pass sort codes make heavy mmap access files tools track read write system calls reporting behavior system pablo tracks read write calls give breakdowns requests sizes times access mmap difficult requiring instrumentation loads stores mapped file regions identifying specific problems simplified visualization tools developed section problem feedback run meters nodes due large amount window traffic generate advanced system collect data locally node collate present animation likewise specific problem automatically identified constantly monitoring processor memory disk utilization machine reporting anomalies user found fixing problems easy reasonable understanding components system users familiar berkeley cluster difficult time isolating anomalies fixing problems required interfering rights users killing jobs removing files users permission perform actions process safely automated paper presented experiences writing tuning now-sort current world-record holder external sorting crucial attaining peak performance setting performance expectations allowed find fix performance problems sort tools users set expectations great case simpler main expectation achieve peak disk bandwidth process made number tools attain peak performance configuration tools helped set expectations automate parameterization sort disk systems memory sizes visualization tools gave instant visual feedback performance sort allowing quick deductions location performance problems finally hierarchical search tools helpful automating search performance anomalies end mix simple tools hoc practices achieved performance desired challenging external sorting benchmark minutesort sorted total minute nodes passes delivered sustained performance sort application roughly peak rate delivered time final record achieved minute nodes time pass delivered bandwidth application high case due glunix start-up difficulties one-line change underlying communication layer final imperfect total serves reminder nature journey now-sort combined effort members berkeley project contributed success rich martin doug ghormley extendspecial jim gray chris nyberg sparking interest sorting encouragement sage advice finally shepherd margaret martonosi anonymous reviewers pieces feedback work funded part darpa -cdarpa -cnsf cda nasa fdnagwand california state micro 
program remzi arpacidusseau supported intel graduate fellowship acharya uysal bennett mendelson beynon hollingsworth saltz sussman tuning performance intensive parallel applications proceedings fourth workshopon input output parallel distributed systems pages philadelphia acm press agarwal super scalar sort algorithm risc processors acm sigmod conference pages june anderson culler patterson team case networks workstations ieee micro february anderson lazowska quartz tool tuning parallel program performance proceedings acm sigmetrics performance conference measurement modeling computer systems pages arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations sigmod arpaci-dusseau arpaci-dusseau culler hellerstein patterson architectural costs streaming comparison workstations clusters smps hpca february boden cohen felderman kulawik seitz myrinet gigabit-per-second local area network ieee micro february chapin herrod rosenblum gupta memory system performance unix cc-numa multiprocessors acm sigmetrics performance conference pages cmelik keppel shade fast instruction-set simulator execution profiling proceedings acm sigmetrics conference pages culler dusseau goldstein krishnamurthy lumetta von eicken yelick parallel programming split-c supercomputing culler liu martin yoshikawa logp performanceassessment fast network interfaces ieee micro dec decchip risc microprocessor preliminary data sheet technical report digital equipment corporation measure transaction processing power datamation readings database systems stonebraker morgan kaufmann san mateo ghormley petrou rodrigues vahdat anderson global layer unix network workstations software practice experience hollingsworth finding bottlenecks large-scale parallel programs phd thesis wisconsin aug hollingsworth miller content-derived names configuration management acm symposium software reusibility boston kleiman voll eykholt shivalingiah williams smith barton andg skinner symmetric multiprocessing solaris proceedings compcon spring lebeck wood cache profiling spec benchmarks case study ieee computer pages october mainwaring active message application programming interface communicationsubsystem organization master thesis california berkeley martonosi clark mesarina shrimp hardware performance monitor design applications proceedings sigmetrics symposium parallel distributed tools spdt february martonosi gupta anderson memspy analyzing memory system bottlenecks programs proceedings acm sigmetrics performance conference measurementand modelingof computer systems pages mathisen pentium secrets byte pages july meter observing effects multi-zone disks proceedings usenix conference jan miller callaghan cargille hollingsworth irvin karavanic kunchithapadam newhall paradyn parallel performance measurement tools ieee computer nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference perl weihl performance assertion checking proceedings acm symposium operating systems principles pages december perl sites studies windows performance dynamic execution traces osdi pages october reed elford madhyastha scullin aydt smirni performance analysis performance data immersion proceedingsof mascots pages february rosenblum bugnion devine herrod simos machine simulator study complex computer systems acm transactionson modelling andcomputer simulation tomacs january saavedra-barrera cpu performance evaluation execution time prediction narrow spectrum benchmarking phd thesis berkeley computer science division february stonebraker operating system support database management communications acm july sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference jan tremblay greenley normoyle design microarchitecture ultrasparc-i proceedings ieee december von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedingsof annual symposium computer architecture gold coast australia 
architectural costs streaming comparison workstations clusters smps remzi arpaci-dusseau andrea arpaci-dusseau david culler joseph hellerstein david patterson computer science division california berkeley remzi dusseau culler jmh patterson berkeley investigate resource usage performing streaming contrasting architectures single workstation cluster smp benchmarks derive analytical empiricallybased models resource usage data transfer examining bus memory bus network processor system investigating resource detail assess comprises wellbalanced system workloads find architectures study balanced streaming applications platforms main limitation attaining peak performance cpu due lack data locality increasing processorperformance improvedblock operation performance great aid workloads future cluster workstation bus major system bottleneck increased load network communication well-balanced cluster workstation copious bus bandwidth multiple busses smp suffers poor memory-system performance true parallelism benchmark contention shared-memory system leads reduced performance result clustered workstations provide higher absolute performance streaming workloads keywords clusters smps balance balanced computer system main memory capacity mbit bandwidth mips cpu performance -amdahl case rule thumb well-known tenet computer architecture suggests systems balanced terms memory capacity disk bandwidth processing power design principle reminds architects focus engineering effort small subset system performance gains sub-system obviated lack similar gains amdahl case rule thumb guideline building balanced systems rule originated performance increased orders magnitude fact amdahl made balancedsystem estimations experience ibm timeshared single-processor mainframe rule thumb apply today vastly altered environment term balance interpretations contexts realm scientific computing balance defined number peak floating-point operations cycle divided sustained memory operations cycle o-based workloads clear definition constitutes well-balanced architecture applications large demands streaming o-intensive workloads define well-balanced system resources simultaneously reach nearpeak utilization input output phases assess architecture analyze resource demands set streaming sequential workloads beginning disks moving memory system processor develop models resource usage applications compare models measured usage study questions mind data moving peak rate demand resources system efficient specific architectures moving data approach questions technique disk scaling adding disks system monitoring resource usage understand resources taxed byte transferred disk discover resource bottleneck range hardware platforms prevalent today restrict class machine examine costs data movement diverse architectures platform simplest common desktop workstation focus sun ultra workstation forms basis comparison systems platform cluster ultra workstations instance larger-scale systems comprised commodity workstations high-speed networks atm myrinet providing low-latency highbandwidth interconnection switches potential fusing workstations cohesive implicitly evaluate underlying assumption clustered systems workstation current form good building block assumption optimistic machine wellbalanced stand-alone case properly architected tightly-integrated cluster environment platform small-scale symmetric multiprocessor smp specifically ultra enterprise built components ultra workstation smp lends direct comparison cluster architecture main difference systems processor-to-processor interconnect cluster architecture local-area network myrinet connects machines smp main memory bus gigaplane high-bandwidth cache-coherent channel communication processors order drive architectures employ set kernels simple scan reads data disk sequentially selects matching records writes records disk benchmark external sort sorting longtime database-industry standard benchmark recently garnered interest addition classical database environment sorting typical workload systems performing decision support paper single-node cluster versions now-sort world fastest disk-to-disk sorting program benchmark external transpose benchmarks fit database domain transpose found external scientific codes benchmarks hand optimized platform set benchmarks find systems balanced specifically introduce system processor bottleneck system resource due lack locality streaming workloads spend time moving data operating increasing ability processor move data memory system increased support block operations greatly aid types workloads platforms find memory traffic high higher inherent applications part discrepancy attributed behavior extra copies buffer cache zeroing heap pages part mismatch grain size benchmarks block size cache cluster scenario part communication layer large amount memory traffic memory interconnect stand-alone clustered workstation capable handling bandwidth demands clustered workstations bus bandwidth crucial times bus bandwidth needed compared stand-alone workstation due aggregation network disk traffic ideal clustered workstation provide bandwidth network disk multiple busses sufficient bus bandwidth clusters improve aggregate disk performance increasing number workstations system smp well-balanced architectures cost lower absolute performance partly attributed extra copy avoid lock contention false sharing factor memory system filled bank time full memory configuration memory system performance smp study significantly decreased rest paper outlined begin explaining methodology section hardware environment section benchmarks modeled section section presents experimental results section conclude study main objectives provide general characterization resource usage set benchmarks benchmarks representative applications perform large amounts streaming models show resources architecture provide efficiently execute programs models based understanding program code confirmed empirically range machines objective evaluate architectures hand study stand-alone workstation small cluster workstations small-scale smp architectures execute benchmarks balanced systems types applications approach dual objectives unified method disk scaling system begins single disk processor run benchmark configuration monitor resources hardware software counters add disks system introducing potential run benchmarks measuring resource usage process empirically determine benchmark behaves compare experimental results application models separate inherent data movement demands actual realization hardware measuring application behavior systems recognize system bottlenecks criticize architectures data presented forms form presents general relationships resource usage benchmark platforms cluster architecture find amount crossing bus read phase sort roughly times amount data read disk algorithmic property sort derived model section confirmed empirically section comparing modeled versus actual usage aids understanding parameters architecture affect resource usage cache block size match natural grain size benchmarks leading extra memory traffic form presents actual utilization resource question utilized bus read phase sort disks added system data gauge effectiveness system test estimate resource question reach maximum utilization measure resource utilization platforms combination software hardware counters ultrasparcs on-chip counters track firstand second-level cache access statistics smp study enterprise counters track memory bus traffic obtain information single workstation cluster environments configure enterprise emulate single-processor system shutting processors board resulting machine similar single workstation memory system interconnect capable higher data transfer rates cluster measurements attach single network card smp connect ultrasparc workstations noted configuration evaluate memory bus workstation employed gather information program behavior amount traffic generated benchmark small modifications made solaris kernel enable counters non-idle periods crucial tracing kernel activity counter-based measurements run benchmarks times graphs present results runs standard deviations upa data bus sysio asic asic address bus upa processor data bus bmx chips cache processorultrasparc memory simms interface seagate hawk rpm seagate hawk rpm scsi bus fast-wide fast-wide network myrinet -bit mhz s-bus scsi bus ultra workstation sbus sbus controllercontroller fastwide 
scsifast-wide scsi sbus board cache processorultrasparc address cache processorultrasparc sysio sysio data -bit address bus data bus -bit gigaplane address controller cpu memory board memory data controller ultraenterprise figure hardware diagram figure depicts internals ultra workstation ultra enterprise workstation inside workstation left bmx crossbar chips asic controller provide switched-based access memory smp enterprise cpu board houses cpus share address data bus board consists s-bus busses boards connected main memory interconnect gigaplane memory physically distributed access time uniform study compare platforms represent important distinct data interconnection architectures hold hardware characteristics common platforms based ultrasparc processor cpu caches memory bus peripheral busses disks platform single processor cluster workstations eight-way smp simplest platform single processor ultra model workstation shown figure machine single mhz ultrasparc processor off-chip secondlevel cache upa crossbar-like connection processor caches main memory ultrasparc systems sustain bandwidths single processor drive rate main bus ultra s-bus ethernet single fast-narrow scsi bus connect s-bus motherboard s-bus slots additional devices experimental setup ultra workstation single internal seagate hawk -rpm disk attached narrow scsi bus paging activity extend disk capacity system fastwide scsi controllers connected external disks platforms runs solaris modern multi-threaded operating system presenting study architectural characteristics operating system behavior dictates usage patterns underlying hardware architectural requirements scalability nas parallel benchmarks frederick wong richard martin remzi arpaci-dusseau anddavide culler computer science division department electrical engineering computer science california berkeley fredwong rmartin remzi culler berkeley abstract present study architectural requirements scalability nas parallel benchmarks direct measurements simulations identify factors affect scalability benchmark codes relevant distinct platforms cluster workstations ccnuma sgi origin find benefit increased global cache size pronounced applications offsets communication cost constructing working set profile benchmarks visualize improvement computational efficiency constant-problem-size scaling find origin mpi point-to-point performance cluster mpi layer scalable communication load communication performance applications lower achieved microbenchmarks show communication protocols mpi runtime library influential communication performance applications benchmark codes wide spectrum communication requirements introduction nas parallel benchmarks npb widely evaluate parallel machines date vendor large parallel machines presented npb results original paper pencil version reports provide comparison execution time function number processors execution rate speedup efficiency easily computed extremely valuable results provide understanding delivered performance fixed algorithm standard message passing interface mpi programming model nas parallel benchmarks version make benchmarks basis indepth comparative analysis parallel architectures current reports provide crude performance comparison reported result total execution time performance metrics derived execution time measured nas benchmarks berkeley network workstations pleasantly surprised find speedup good cray per-node performance ibm spalthough lesser performance processor raw speed mpi active messages ratio processor performance message performance provided adequate accounting differences lack clear explanation motivated develop set tools analyze architectural requirements npb detail single pass class benchmarks roughly trillion instructions traditional simulation techniques intractable ruled employed hybrid method combining direct measurements real machine parallel trace-driven simulations understand performance characteristics actual platform shows architectural parameters affect scaling paper detailed analysis architectural factors determine scalability nas parallel benchmarks parallel machines berkeley cluster sgi origin relevant distinct platforms basis study starting base performance speedup curves break benchmarks terms computation communication costs isolate factors determine speedup analysis shows machines scalable communication performance improvements memory system performance due increasing cache effectiveness compensate time spent communication extra computational work applications exhibit perfect super-linear speedup machine sizes typically class data set processors class behavior inherent constant problem size cps scaling benchmarks characterized precisely constructing working set graphs input size main contributions work characterization complex interactions software hardware effects speedup methodology understanding speedup cps domain quantitative analysis architectural requirements nas parallel benchmarks suite version including detailed study nas benchmarks working set behavior scaling evaluation communication efficiency applicationswith mpi communication protocols experimental environment methodology understanding performance scaling large parallel codes difficult problem ideally run benchmarks real machines option precludes detailed study hardware characteristics cache size parameters simulations problematic limit size problem potentially miss longterm effects remedy situation apply hybrid approach instrumented communication library tandem hardware counters measure execution characteristics benchmarks real machine trace simulate benchmarks vary architectural parameters details methods direct measurement study executions nas benchmarks performed cluster ultrasparc model workstations sgi origin system runtime measurements present paper average runs excluding outliers cluster node single myricom network interface card attached s-bus bus machines interconnected ten -port myrinet switches fattree topology node cluster running solaris cache main memory ultrasparc main memory obtain single processor runtime origin machine consists processors running mhz processor cache nodes machine node processors main memory running irix mpi initial study origin shown care operating system interfaces data movement ultrasparc performed special block copy hardware part vis instruction set block loads stores move data directly double-precision floating point registers polluting caches hardware feature accessible library routines memcpy copy rates roughly moving traffic memory bus s-bus -bits wide devices -bit mode including network interface cards reducing potential bandwidth study examine -bit s-bus products cluster consists ultra workstations identical workstations connected myrinet switched-based network machine single myrinet card s-bus attached cable eight-port switch multiple switches linked form large arbitrary topologies parallel applications cluster communicate active messages high-performance communication layer designed low latency high bandwidth switch-based networks active message restricted lightweight remote procedure call communication layer studying base architectural costs data movement unnecessary copying buffering data avoided paper active messages gam myrinet round-trip latency roughly bi-directional sustained bandwidth sending receiving primary unit data transfer benchmark applications ultra enterprise small-scale symmetric multiprocessor uniform memory access main hardware resource distinguishes system system based mhz system numerical aerospace simulation gigaplane -bit wide packet-switched memory bus connects cpu boards shown figure system -cpu processor boards boards total memory distributed processor boards board s-busses s-bus built-in fast ethernet fast-wide scsi bus extra slot devices s-bus slots experiments add fast-wide scsi controller card fast-wide scsi disks attached communication enterprise performed loads stores shared memory primitives mutexes condition variables safely access shared data barriers synchronize threads processors copy rates slightly higher single ultra workstation bus memory bus read write read write scan workstation cluster smp sort workstation cluster smp transpose workstation cluster smp figure benchmark resource models table presents models resource usage benchmarks workstation cluster smp platforms read phase resource usage relative rate data read disk similarly write phase resource usage relative rate data expected cross bus read phase single workstation sort equal rate data disk read phase cluster sort times rate disk cross bus grows large section give overview kernels study develop models resource usage benchmarks primarily perform sequential future studies plan examine non-sequential access patterns benchmarks scan sort typical benchmark transpose commonly found scientific codes models presented section memory bus usage read write phases benchmarks derived understanding code models resource usage presented ratios rate data read written disk data read disk single workstation platform expect cross bus benchmarks read data disk memory-mapped files usemmap benchmarks alternative read results extra copy buffer cache operating system problematic applications stream data usingmadvise sequential accesspattern pages prefetched pages discarded appropriately writing benchmarks repeatedly call write large buffer avoid high cost repeated traps kernel mmap natural match writing extend length files finally benchmarks capability access multiple disks concurrently simple user-level striping library similar library spreads disk blocks disk sub-system user-specified block size minimal cpu overhead benchmark hand optimized platform question benchmarks versions code cluster smp present models perspective single processor cpu performs identical tasks describe benchmark sequential scan modeled scan selection database simplest benchmarks input set scan sort derived datamation minutesort sorting benchmarks byte records -byte keys terminology size key size record scan selects writes disk records match user-specified set criteria fraction matching records benchmark keys data range selected roughly half data written back disk model model bus usage scan platforms simple traffic crossing bus traffic disks reading disk cross bus likewise write phase writing disk cross bus workstation memory model read phase scan performs steps records memory-mapped file transferred memory accountingfor memory bus key portion record examined determine key matches criteria logical crosses bus memory processor caches key matches copied separate buffer bus write phase writes buffer disk accounting cluster memory model cluster version identical single-node scan processes started nodes cluster node executes single-workstation scan situation best-case scenario cluster explicit data exchangeoccurs embarrassingly parallel complete parallelism models resource usage cluster scan match single node models smp memory model smp version forks threads read select records independent portion data file write disk logically sharing benchmark resource usage models identical single workstation complex benchmark study external sort detail sorting chosen database experts excellent test memory communication sub-systems machine scan -byte keys -byte records basic sorting algorithm similar platforms step records converted layout disk format suitable efficient sorting records read disk key pointer full record buckets based top bits key improves cache behavior sort ways sort operates partial key pointer pairs copying -bytes -byte records keys compared swapped number keys bucket matches size second-level cache step sorts keys bucket algorithm step accounts small fraction total execution time performs discuss finally write phase scans bucket array gathering sorted records writing disk model bus usage single workstation smp sort identical scan disk traffic travels bus cluster bus handle network communication record read local disk processor determines destination workstation responsible record final sorted order assuming initial data randomly data remote processors equivalent amount received processors reading disk cluster sends receives grows large cross bus workstation memory model memory bus model captures extra complexity sort read phase input file mapped read user address space accounting memory bus 
ensure pages discarded operating system memory manager records copied input buffer set sort phase keys buckets based top bits key examined top bytes key pointer record written bucket array size partial key pointer pair bytes write phase includes scan bucket array copy write buffer transfer memory disk cluster memory model difference cluster version basic algorithm occurs read phase simply placing keys pointers local bucket entire record workstation hold final sorted-order processor mapped input file records copied send buffers buffer fills destination processor buffering efficient communication -byte messages achieve peak transfer rates messages received traffic cross memory bus receipt records copied record buffer key examined partial key pointer written bucket array single workstation sort processors synchronize complete phase independently sort write set records identical single workstation sort smp memory model model smp version sort account communication processors global array buckets record buffers allocated processor begins reading separate file parallel records accessed processor simply acquire lock copy key pointer pair global bucket array copy record global record array release lock leads high lock contention poor performance avoid problem processor small record bucket buffer copies records key pointer pairs buffer fills processor grabs proper lock copies keys records global arrays read phase complete processors synchronize divide global bucket buffer array sort keys parallel finally write phase processor gathers records writes disk single workstation sort final benchmark transpose similar operation found external scientific codes out-of-core fft basic operation reads blocks row-major order writes disk column-major order blocks benchmark departure -byte records previous benchmarks model bus model phases single workstation smp write phase cluster match benchmarks model read phase cluster corresponds sort workstation memory model single node transpose reads input set writes transpose blocks disk reading blocks disk copies column-ordered buffers program transposes blocks array block read buffer location location write phase writes blocks sequentially operation complete moving bus cluster memory model cluster mapping input disk memory node repeatedly sends block local memory-mapped input file selected destination node receiving message processor copies proper buffer location based processor block input data read memory proper destinations processor writes data local disk phase communication identical single-node case smp memory model smp version begins processor sequentially reading data disk copying buffer transpose performed shared memory processor claiming required portion processors data writing disk moving memory bus write models resource usage summarized figure parallelizable workloads scan models identical platforms cluster bus usage read phases sort transpose roughly times stand-alone workstation due addition network communication finally memory bus usage cluster smp increases communicating cluster extra copy performed aggregate small messages larger smp extra copy avoid false sharing lock contention scan scan sort sort trans trans bandwidth delivered disk benchmark phase workstation cpu cpu scan scan sort sort trans trans benchmark phase cluster cpu bus scan scan sort sort trans trans benchmark phase smp cpu figure absolute performance figures plot incremental achieved disk bandwidth benchmarks group bars scales number disks processor measured rate match expectations denote bottleneck cpu bus section main experimental results paper begin presenting absolute performance benchmark platforms shown amount delivered disk bandwidth disks added system establishes facts workloads well-tuned effectively multiple disks point benchmarks reach system bottleneck proceed examining resource path disk cpu bus memory bus processor interconnect processor show absolute performance attained benchmark platforms disks added system ideal system achieved bandwidth matches peak disk rate multiplied number disks system system disk deliver reading disk writing disk figure shows increase bandwidth achieved phase benchmarks platforms read phase sort single workstation bar group data read disk bars show adding additional disks yields expected benefit disk adding roughly sum bars shown total data rate achieved disks fourth disk added gain read bandwidth expected indicating reached bottleneck system case cpu bottleneck graph summarize disks workstation single workstation sort reaches cpu bottleneck cluster performance falls read phases sort transpose due cpu bottleneck occur disks workstation finally smp performance degrades write phase sort disks processor due cpu proceed exploring resource detail begin exploration system resources bus find measurements bus traffic platforms match predictions models measurements utilization sun bus reveal single workstation smp sufficient bandwidth performance cluster limited workstation bus leftmost graph figure shows measured ratio data crossing bus relative data coming disk disks scaled phase workloads predicted applications single workstation bus usage matches disk traffic disks moving disk amount crossing bus dividing amount crossing bus amount read disk ratio bus bandwidth disk bandwidth figure modeled traffic shown horizontal black line models match experimental results precisely cluster bus cluster communication phases read phases sort transpose interest benchmarks expected move bus read disk -node cluster shown middle graph figure experimental results confirm behavior large cluster bus handle roughly times traffic stand-alone workstation smp bus single workstation data disk crosses bus communication traffic crosses memory bus data figure confirms bandwidth move bus read written disk established demands bus models empirical measurement apply knowledge understand architectures study support streaming-i applications figure plots bus utilization applications architectures number disks increased workstation s-bus single-processor workstation s-bus sufficient bandwidth support benchmarks high disk transfer rate peak data bandwidth s-bus operating -bit mode theoretically attained moving maximal -word -cycle data burst bus -cycle arbitration phase due control information form programmed peak utilization occurs figure shows utilization s-bus disks added system disks bus utilized linear extrapolation scan scan sort sort trans trans ratio bus disk bandwidth benchmark phase workstation network disk inherent scan scan sort sort trans trans benchmark phase cluster network disk inherent scan scan sort sort trans trans benchmark phase smp network disk inherent figure bus data movement figures plot ratio bus bandwidth disk bandwidth workloads platforms models shown horizontal black lines set bars represents phase benchmarks group number disks increased showing load system increased scan scan sort sort trans trans bus utilization benchmark phase workstation theoretical peak scan scan sort sort trans trans benchmark phase cluster theoretical peak scan scan sort sort trans trans benchmark phase smp theoretical peak figure bus utilization figures plot bus utilization workloads architectures disks added system cluster reaches peak utilization read phase transpose bottlenecks system s-bus support approximately disk bandwidth hitting peak utilization s-bus ultra 
workstation sufficient bandwidth support disk subsystem internal scsi bus workstation house internal disks fast-narrow scsi bus peak bandwidth connects disks motherboard configuration limits performance modern disks attached avoided cluster s-bus extra load bus phases concurrent disk network communication s-bus suffers high contention peaks quickly single workstation scenario figure utilization s-bus read phase sort transpose high disks workstation show section sort reach peak s-bus utilization severe cpu bottleneck figure s-bus limits performance transpose s-bus targeted meet bandwidth requirements slower devices today high-speed disks networks tandem sun microsystems words s-bus optimized technologies expected dominate late early evident bus needed support o-intensive applications cluster -bit s-bus partially solves widespread availability -bit cards smp s-bus stated smp s-bus utilization identical single-node utilization standards s-bus meet demands enterprise system examine architecture enterprise filling board slots enterprise symmetrically cpu boards boards machine total s-busses peak achievable bandwidth busses single s-bus slot built-in fast-wide scsi slot s-bus slots comparison single ultra workstation slots internal scsi attach fast-wide scsi cards slots drive scsi disk bandwidth s-bus transfer roughly half potential data rate lack bus slots implies modern disk technologies ultrascsi full advantage bandwidth stand-alone workstations problems today bus technology workstations grow faster future standards pci provide bandwidth clusters situation bus handle times bandwidth standalone machines current s-bus technology struggles aggregate demand disk network traffic straightforward solution provide separate paths disk network traffic multiple busses recent machine sun ultra workstation good pci busses make machine ideal cluster computing radical solutions suggest placing network interface memory bus solves problem removing traffic bus placing additional load scan scan sort sort trans trans ratio memory bus disk bandwidth benchmark phase workstation processor-memory network disk inherent scan scan sort sort trans trans benchmark phase cluster processor-memory network disk inherent scan scan sort sort trans trans benchmark phase smp processor-memory network disk inherent figure memory bus data movement figures plot ratio memory bus bandwidth disk bandwidth modeled amount data movement shown horizontal black lines under-predicts actual usage noticeable amount due interactions cache block size operating system zeroing copying case cluster copy communication layer memory bus workloads question bandwidth latency sensitive solution justify costs finally smp systems standard busses provide plenty disk bandwidth foreseeable future architecture provide adequate number slots attaching disks bus behavior benchmarks move memory bus memory traffic difficult model find models consistently underpredict memory bus usage show underlying cache architecture operating system communication layer responsible differences balance platforms ultra workstation memory bus bottleneck streaming plenty bandwidth benchmarks workstation cluster extra copies abound due communication memory interconnect suffices workstation memory bus figure shows measured memory bus traffic ratio disk bandwidth inherent traffic designated black horizontal line group bars main reasons models underestimate memory traffic mismatch natural grain size -byte cache blocks operating system behavior effects account memory traffic explained extra traffic occurs read phase scan due distinct interactions cache scan examines -byte key -byte record -byte block fetched cache transferred memory bus predicted copying selected records output buffer -byte record lies cache blocks worst-case record matches scan criteria generate generates similar granularity mismatches occur sort difficult analytically determine memory traffic repeated accesses data cached worst case examining -byte keys -byte records full -byte cache block accessed scattering -byte partial key pointer pairs random buckets entire cache line read written generate empirical results show worst case realized work including simulations user code operating system required understand behavior detail part future work operating system behavior represented models generates significant traffic benchmarks read phase benchmarks perform work required bring blocks disk pages allocated user process sort transpose entire data set copied user-allocated buffer zeroed demand half zeroing takes place scan records copied found empirically total traffic memory bus read phase sort transpose scan cluster memory bus figure shows memory traffic under-predicted cluster models read phases sort transpose phases perform communication differ single-workstation implementation extra traffic captured models rest arises behavior message layer sort transpose benchmarks send buffer communication layer copy buffer pre-pinned pre-mapped region address space dma operations performed network device result significantly traffic passes memory bus cluster single-workstation benchmarks sort cross bus read disk smp memory bus memory traffic smp benchmarks read phase sort identical single node measurements figure show extra generated transferred disk extra traffic facility due nasa contention ames research file center system buffers investigation avoid lock hardware contention configuration false origin sharing system application upgraded mhz avoid mhz contention origin system underneath measurements sort based transpose benchmarks mhz system smp input section buffers starting performance nas measurements benchmarks results processor memory speeds traffic discussed read nas phase benchmarks explicitly communicate performing mpi operation performance mpich suffered extent implementation fully adi utilize calls mapped disks active cpu messages operations layer highly tuned performance break performance benchmarks add instrumentation code mpi present layer evaluation mpi call ultra record memory systems desired measure memory bus utilization workstation estimate utilization based specifications workstation upa interconnect upa designed support small-scale smp systems over-engineered simple case single processor inside workstation -mhz ultrasparc processor drive memory system memcpy interconnect theoretically capable sustaining roughly sort memory-bus intensive applications moves memory bus transferred disk move peak memory interconnect read disk sub-system cluster upa interconnect due high capability upa memory interconnect amount memory traffic generated sort overload memory system sort moves memory interconnect part system bottleneck reaches system components handle ultraenterprise memory system gigaplane main system interconnect enterprise systems bandwidth interconnect high capable delivering memory banks compare cluster interconnect defer discussion section interestingly memory bandwidth ultraenterprise scales memory capacity system memory banks bank serves increase total memory bandwidth adding dram bank bank filled small capacity systems small performance capabilities early experiments server configured memory banks limiting sustainable performance processors running copy micro-benchmark similar subtleties program interaction cache architecture operating system network communication lead excess memory traffic mismatches grain size applications block size cache architecture difficult avoid requiring application awareness underlying machine architecture reducing number copies performed operating system communication layer tractable modern operating systems avoid extra copies providing set statistics end execution write results disk cases instrumentation adds execution time cases manner gather information message sizes destinations time stamps communication events occur measure instruction count cpi performance counters ultrasparc processors beginning run configure counter count cycles count instructions start end mpi event record values end run de-construct amount time spent number instructions executed inside mpi routines measurements run full number iterations nas benchmarks class problem sizes simulation instrumented mpi layer give usage characteristics breakdowns time spent code give working sets benchmarks counters provide miss rates cache design machine cluster origin miss rate cache size tosolvethisproblem weemploytheshadesimulation environment cluster shade sparc-based virtual machine application program shade manner running independent instances simulator workstations cluster inside virtual cluster communication processes parallel program takes place real myrinet network written shade analyzer outputs data cache address trace process simulated machine benchmarks traced single time-step experiments revealed behavior time steps identical benchmarks trace produced dinero cache simulator simulate desired cache configurations speedup figure table show speedup single processor execution time cluster origin class problem size nas figure speedup nas parallel benchmarks figures present speedup curves berkeley cluster sgi origin class problem size nas direct non-buffered applications bypass file system buffer cache version solaris includes functionality zeroing pages protection difficult avoid continue play important role streaming workloads cluster communication results extra copies current environment potentially avoided cluster sort copy explicit program -byte records copied larger blocks amortize overhead sending message copy avoided tight integration network interface processor lowering overheads allowing applications send small messages peak rates copy occurs message layer copy buffers portions address space setup dma transfers copy avoided exposing communication 
buffers application largest reduction memory traffic cluster direct disk-to-network transfers completely avoiding memory bus application sort dynamically determine destination -byte record requiring application-specific code disk controller examine demands communication backplane cluster smp find interconnects significantly demands bottleneck real systems parallel benchmarks version cluster achieves perfect slightly cluster perfect speedup interconnect analyze communication programs rates origin attains links superlinear attaching speedup machines network benchmarks total wide amount spread traffic speedup generated behavior origin nodes established section workstation sends receives roughly speedup machine size processors spe dup read phase ideal cluster sort transpose link connects machine speedup switch machine size support processors spe dup ideal traffic table single processor direction execution time processors table sending presents aggregate single bandwidth processor moving runtimes berkeley network cluster read phase sgi origin class problem size smp nas interconnect benchmarks smp interconnect version benchmark support cluster memory seconds origin seconds traffic surprising resource heavy contention figure figure time phases breakdown scan nas benchmarks processor places figures roughly break total execution memory time bus summed read processors disk nas benchmarks scan places cluster roughly origin communication shared computation time interconnect shown sort places separate non-cumulative lines total time bus sum communication transpose roughly computation time presented scan difference aggregate bandwidth smp interconnect cluster interconnect cluster roughly machine factor size processors bandwidth eco requirements total mediums comp scale comm linearly ideal processors origin machine size processors eco total comp comm ideal myrinet network analyze ability myrinet hardware origin machine support size traffic processors -node cluster eco assuming total single comp -port comm switch ideal myrinet links sustain direction cluster machine size sufficient processors bandwidth eco reaches total comp comm ideal ability myrinet switches handle total communication bandwidth origin switch machine size perfect processors crossbar support eco aggregate total comp bandwidth comm ideal port contention myrinet switch bottleneck cluster total machine network size traffic processors long eco total comp comm s-bus ideal handle myrinet links switches cluster provide ample machine size bandwidth processors capacity relative eco today disk total speeds comp comm current ideal networks proper performance regime cluster machine mbps size processors fast ethernet connection eco total link comp bandwidth comm support ideal disk bandwidth workstation gigaplane origin machine enterprise size gigaplane processors support roughly eco data total transfer comp comm ideal -processor system established origin machine size transferred processors bus eco benchmarks place total comp comm ideal gigaplane achieve peak cluster utilization machine bus size worst processors case limit eco amount per-processor total disk comp traffic comm ideal figure time breakdown nas benchmarks continued figures break total execution time summed processors nas benchmarks cluster main origin concern communication system computation designers time conclude shown separate streaming non-cumulative lines applications total time small-scale sum parallel systems communication computation time presented bottleneck cluster interconnect provide bandwidth scales linearly number origin disks machine size workstation processors 
eco total comp comm ideal benchmarks run class class problem size basis study class run single processor parallel machines observe cluster obtains perfect speedup slope benchmarks obtains efficiency benchmarks speedup slightly super-linear machine sizes behavior complex origin performance benchmarks substantially super-linear range performance falls efficiency reason cache effects communication effects behavior systems generation ago occurs large second-level caches present today machines time step understanding nas benchmarks behavior isolate components application execution time parallel programs work time spent processors curves labeled total figure show sum execution time processors nas benchmarks machines function number processors metric perfect speedup corresponds horizontal line labeled ideal intercept single processor execution time cluster follow ideal closely drop super-linear speedup moderate machine sizes rise sub-linear speedup origin curves show greater variation understand behavior isolate components execution time instrumenting portions program obtained detailed breakdowns time spent inside mpi library hard practice distinguish inherent load imbalance parallel program synchronization cost communication event actual communication cost curves labeled communication figure show sum time spent mpi communication synchronization including send receive wait time curves labeled computation showthe sum time spent mpi library processor count increases general communication time grows processor count compensated improvements computational efficiency super-linear speedup observed decrease total computation time increase communication cost computation bound communication total execution time processors configuration benchmark experiences modest linear improvement computational efficiency larger machine size hand roughly constant efficiency processors change relative processor opposite machines communication computation cost roughly balance communication time occupies execution time benchmarks offset decrease computation time communication bound shows gain computational efficiency run processors fact computation time increases slightly increase communication time significant dominant factor speedup benchmark computation efficiency benchmark improves increases communication time dominate performance benchmark cps scaling rule total amount work total number computational operations solve fixed problem remains number processors computation time processors remain constant computational efficiency unchanged hand moreprocessors added solve problem communication time increases number computational operations due redundant work nonetheless benchmarks show perfect super-linear speedup extra time spent communicating synchronizing compensated improvement computational efficiency hardware counters concluded reduction computation time corresponds reduction miss rateandincpi sections investigate factors govern speedup nas benchmarks section examines change computational efficiency caused effective memory system total amount cache increases section examine communication behavior nas benchmarks working sets gain insight memory access characteristics benchmarks scaling obtained per-processor memory address trace application figure working set profiles nas benchmarks working sets class problem size top middle andlu bottom presented curve corresponds machine size curve labeled corresponds -processor machine cases large cache sizes increasing number processors working problem decreases per-processor miss rate noticeable amount smaller cache size scaling makes difference cache performance increases miss rate processor cache size processor cache size rate processor cache size rate processor cache size processor cache size rate processor cache size rate figure working set profiles nas benchmarks continued working sets class problem size top middle bottom presented per-processor miss rate change significantly size cache scales slight improvement cache miss rate large cache sizes miss rate per-processor cache size increases scaled machine size interest ran trace processor cache simulator range cache size simulations caches fully associative lru replacement -byte blocks write back write allocate curves labeled offigure showthedata cache miss rate -processor machine function cache size size demonstrate effect scaling miss rates concentrate effect pronounced smooth decrease miss rate general rule doubling cache eliminates constant fraction misses roughly miss rate flat drops levels knees working set correspond shared address space programs measured splashthe key observation cps scaling working set curve machine size processors knee starts processors processors cases sharp drop occurs amount global cache sum local caches reaches mbforthebenchmarklu memory access requirements cluster drawing vertical line per-processor cache size miss rate drops significantly -processor system cluster flattens larger configurations change reflected change total computation time figure ontheoriginsystemwith mbofl cache working set knee captured cache single processor increase computation processors due factors decrease large configurations working set fits global cache algorithms tuned cache friendly phenomenon pronounced benchmarks experience levels boost global cache size increases benchmark moderate improvements efficiency significant change interestingly small local cache size typical early parallel machines miss rate benchmarks increases number processors improvement computational efficiency benchmarks machines small caches increase miss rate scaling benchmarks amount work increases processors added figure shows percentage increase computational instructionson cluster relative single processor case benchmarks experience moderate growth instructions scales significant increase extra work load memory system expected increase instruction load important point examining cache effects significant influence scalability benchmarks cps scaling result increase memory system efficiency due figure extra work figure shows percentage increase computational instructions relative single processor nas benchmarks cluster scales extra work machine size processors crea cache effects overlooked case nas benchmarks dismissed assumed working sets exceed cache size work demonstrates combination large caches processor cache-friendly codes cache effects play significant role scalability machine cps scaling rules case nas benchmarks cache boost mask poor performance areas communication communication performance breakdowns execution times figure benchmarks spend significant amount time communication benchmarks increase communication time primarily determines scalability benchmarks section investigate communication load benchmarks place architecture sensitivity benchmarks underlying communication protocol mpi communication scaling table shows baseline communication characteristics nas benchmarks processors communication scaling relative base case table shows total number messages processors collective operation performs processors counted message total volume total number bytes processors processors benchmarks send messages size messages large benchmarks send messages megabytes size non-blocking mpi isend primitive traditional blocking mpi send number collective communication operations low surprisingly benchmark streaming barriers benchmarked portion code benchmarks reduce operations reduce operations contribute performance significantly communicate primarily collective communication series all-to-all exchanges figure shows communication characteristics nas benchmarks change machine sizes figure top plots change total message count processor function machine sizes normalized message count -processor figure middle shows byte count processor finally figure bottom shows resulting average message size processor realm interest order magnitude difference average size message benchmark interestingly smallest messages benchmarks order bytes substantially larger grain found parallel benchmarks all-to-all pattern normalized per-processor message count growth linear machine size total number pointto-point messages increases square number processors total byte volume remains constant bytes processor decreases 
message size decreases range processors studied absotable nas benchmarks baseline communication characteristics table shows baseline communication characteristics nas benchmarks codes class problem sizes -processor table shows number messages breakdown type total number bytes results -processor case reduce allreduce benchmark isend send alltoall allreduce barrier total volume figure message scaling figure shows number messages normalized processor case top number bytes middle average size message bottom scale function processors machine size processors ssa sso bytesperprocessor machine size processors lum averagemessagesize machine size processors age age lute number messages remains small squaring message count resulting decrease message size important implications machine designers machines processors efficient transfer mechanisms exist messages ranging hundred bytes megabytes size total amount communication surface volume ratio scale number processors unlike benchmarks finer-grained communication message sizes benchmarks span order-ofmagnitude range paticular sends messages ranging bytes range processors interest scaling communication lines unduly limit speedup spatial decomposition communication nearest-neighbor regime benchmarks summary nas benchmarks place wide variety communication loads system ranging nearest-neighbor point-to-point exchange coarse-grained all-to-all communication general communication load increases scale cluster sensitivity communication protocol message characteristics imply total communication costs increase cps scale machine size figure shows total communication costs rise sizable differences platform handles increased communication load figure plots mpi one-way latency bandwidth platforms dongarra echo test one-way latency half message round-trip time one-way bandwidth reciprocal latency startup cost maximum bandwidth sec sec cluster origin micro-benchmarks results message characteristics nas benchmarks construct expected communication cost message accumulate micro-benchmark latency message size predicted communication time nas benchmarks communication efficiency ratio predicted measured communication figure mpi performance figures show performance mpi platforms dongarra echo test top figures show one-way latency small messages bottom figures show one-way bandwidth message size mpi one-way bandwidth cluster message size bytes mpione-way latency onorigin message size bytes mpi one-way latency cluster message size bytes mpione-way bandwidthonorigin message size bytes sec sec cluster rendevous machinesize processors cien cluster eager machinesize processors cien origin machinesize processors cien figure communication efficiency figures show percentage predicted bandwidth delivered benchmarks switch essentially capability handle sum disk traffic larger systems consist switches interconnect scale mediumand large-scale clusters smp interconnect handle aggregate memory data movement small-scale systems modern aggressive busses gigaplane sufficient section examine processing requirements streaming begin analyzing number instructions executed byte read written disk study sparc-based systems relationships coarse estimates risc-based machines understand ultrasparc processor executes workloads measure processor utilization cpi instructions implementation independent utilization cpi directly determined number architecture-specific factors including branch behavior memory latency cpi platforms benchmarks high result processor bottleneck workloads note characterizations section preliminary exact breakdown instructions executed time spent part on-going research mpi layer total time spent sends workstation processor waiting figure shows application number function instructions number millions processors top executed figure system shows user communication mode efficiency rendevous delivered protocol disk bandwidth mpi layer number cluster instructions middle executed figure shows system mode communication efficiency constant eager benchmarks protocol writing bottom figure write shows requires communication system efficiency instructions reading origin mmap machine time due figure graphs extra memory communication copy efficiency buffer cache platforms notice main difference cluster benchmarks rendevous arises benchmarks work efficiency user-level half performed drop transpose slightly requires scale amount work hand starts read high phase user efficiency code falls performs sharply single copy scale communication record efficiency read phase benchmarks scan origin executes instructions figures show key compared cluster platform selection handles criteria load copies origin performed show fine granularity delivered write performance phases transpose micro-benchmark scan performance perform essentially work anomaly implementation userlevel mpi repeatedly layer calling write move interacts data disk underlying sort architecture evaluation processor nas intensive due benchmarks drove mismatch development disk mpi layer representation records cluster form total time internal predicted sorting combining disk micro-benchmark records performance comprise linear message array profile data internal sort significantly executes time -byte spent keys separated communication investigation larger records cluster revealed groups keys source fit second-level problem cache internal records protocol read mpi layer disk keys initial implementation scattered mpi buckets conservative pointers rendevous set protocol full records low-latency active write phase messages building reverse block operation rendevous gathering keys protocol simplified records receive linear code array writing message format disk cluster short processor round trip major time increases rtt easily instruction rates amortized relative large single impending bulk workstation occur transfer sending micro-benchmarking receiving conditions messages design read deliver phases optimal sort performance workstation practice cluster smp queueing sys delays user sys user source sys mpi-to-network user interface scan exacerbate rtt real read applications resulting write efficiency sort shown figure read rendevous figure plots write trans histogram measured round-trip times read protocol message write run figure processor benchmark instruction rate -processor table shows micro-benchmark mips tests rtt disk bandwidth transpose actual benchmarks cost communication performed variance user level high increase indicating prediction user instruction round-trip times counts transpose difficult minimal large instruction average cost message size perform bulk-message nas network benchmarks communication protocol messages immediately experience sends long queueing block delay surface read application disk level increase form low transpose read communication phase efficiency revised workstation mpi cluster implementation shows slightly aggressive eager mips protocol required significantly send increased receive complexity data re-sequence out-of-order work messages combined slightly system worse user micro-benchmark cost performance reading communication writing performance disk cost context real communicating applications greater improved sort benchmark -byte benchmarks records shown copied figure eager buffers amortize eager protocol startup cost increases communication utilization smp processor outgoing similar channel reducing cluster queueing sort delays smp figure versions plots benchmarks histogram perform additional measured work communication copy time data buffering message performed running avoid false-sharing -processor lock eager contention protocol figure shows sort copying cost keys message temporary buffer reduced significantly inserting smaller variance global bucket array benchmarks increases instruction achieve costs higher mips communication efficiency higher eager instruction costs protocol benchmarks read phase -processor achieve scan full write phases efficiency communication programs efficiency falls due larger lock configurations contention network kernel saturated benchmarks communication efficiency limited point-to-point communication primarily mpi send mpi isend improvement communication efficiency eager protocol effective benchmarks communication efficiency benchmark increases efficiency improves unchanged efficiency benchmark caused inherent load imbalance program investigation shows benchmark experiences approximately load imbalance processors suggests improvement point-to-point communication hidden large amount synchronization time origin sensitivity processor speed initial study origin system based mhz processor origin system nasa ames upgraded mhz investigation change behavior illuminating section study differences performance nas benchmarks systems difference systems increase processor speed on-chip cache latency memory bus speed closely tied processor speed performance memory system mpi change micro-benchmarks capture differences systems experience user applications figure shows performance memory systems memory stride benchmark benchmark shows approximately decrease latency cache approximately improvement latency main memory figure shows one-way point-to-point bandwidth mpi mhz system dongarra echo test improvement processor speed memory system micro-benchmark obtains roughly maximum bandwidth decrease latency small messages sec predicted std node isend cost sec coun figure message cost mpi internal protocols figures show change message costs mpi layers figure plots histogram observed round-trip times initial mpi layer set-up message node benchmark -processor figure plots time mpi isend eager protocol message standard deviation shown graphs tails distributions omitted clarity sec sec std sec ount node set-up message cost figure memory system characteristics figures show memory system characteristics origin machines mhz mhz processors origin mhz stride origin mhz stride mhz system one-way latency achieves maximum bandwidth sec message size mhz system figure one-way latency obtains maximum bandwidth sec message size nas benchmarks performance figure shows speedup nas benchmarks older origin single processor performance benchmarks newer origin increased improvement observed benchmarks improvement larger configurations runtimes processor higher mhz system benchmarks roughly speedup speedup benchmarks drops efficiency super-linear efficiency sensitivity workload measurements present paper dedicated environment program run time origin machine meant running odd hours load low found time-sharing workloads sec sec figure mpi performance figures show mpi point-to-point performance mhz origin system dongarra echo test mpione-waylatency mhz system message size bytes mpione-waybandwidth mhz workstation ultrasparc-i figure shows cpu utilization cpi benchmark graphs ascertain sustainable disk bandwidth platform cpu limited single workstation sort places load processor reaching cpu utilization disks roughly explains drop absolute performance figure earlier section benchmarks fair extrapolating cpu utilization predicts cpu bottleneck roughly transferred disk benchmarks cpu expected primary bottleneck predicted memory busses handle saturating cluster ultrasparc-i cluster measurements show fast communication layer places heavy demand ultrasparc processor processor utilized read phase sort disks cpu bottleneck adding disks workstation improve performance sorting transpose read fairs reach utilization disks workstation reader notice number system level instructions read phase transpose decreases cluster smp implementations cluster repeatedly small buffers large buffer saving operating system cost zeroing pages smp versions pages read phase begins result reducing smp instructions relative platforms scan scan sort sort trans trans cpu utilization benchmark phase workstation system user cpi scan scan sort sort trans trans benchmark phase cluster cpi scan scan sort sort trans trans benchmark phase smp figure processor utilization figures plot processor utilization bars left y-axis label cpi line y-axis label workloads platforms workloads processor main bottleneck system moving disk sort transpose cpu s-bus reach peak utilization simultaneously read phase transpose high cpi reasons low data locality accessing blocks data leads high cache miss rates effect measured single workstation accessing network interface card s-bus expensive cpi grows number disks contention bus increases smp ultrasparc-i cpu utilization shows sort smp leverage disks processor general cpu utilization smp higher single workstation number instructions executed similar utilization graphs platforms illustrate importance operating system performance streamingi workloads benchmark platform time spent kernel dominates spent user-level number disks increases interestingly number instructions executed biased high cpi poor kernel cache performance processor main bottleneck workloads platforms situations cluster environment bus limits performance approaches lowering processor utilization workloads reducing number instructions communication layers active messages provide minimal cost primitives network access time revisit instruction costs disk high instruction cost disk access exacerbated solaris operating system generality modularity support threads increase common code path disk instruction costs reduced additional support large block operations instruction set approach focuses lowering cpi easiest solution hardware tighter integration memory system software lesson feasible current systems operations cache conscious crucial group data accesses secondlevel cache-sized objects avoiding high cost repeated access main memory style programming extensively plan investigate support applications achieve locality presented measurements 
resource costs data movement machine architectures platforms developed models benchmark resource usage validated models empirically measured utilization resource amount disk bandwidth system scaled order evaluate architecture perspective set kernels summarize results present graphical representation means balanced system figure plots balance platforms phase workloads question machine resources machine shown x-axis y-axis plots predicted per-processor disk bandwidth added machine resource reach saturation workloads based linear extrapolation usage characteristics found earlier sections set bars height balanced system set higher bars system figure machines balanced platforms resource bottleneck processor due lack locality streaming benchmarks fact benchmarks cpi lower processor potentially execute instructions cycle single workstation memory interconnect stand-alone workstation over-engineered bottleneck types workloads cluster bus limits performance phases network communication surprisingly benchmarks communication balance cluster workstation defaults stand-alone case memory bus plenty bandwidth memoryintensive benchmarks fairly utilized network backplane myrinet ample bandwidth easily handling traffic rates proportional disk finally smp absolute performance result appears balanced architectures communication backplane gigaplane handles high load streaming application set adequate small-scale parallel systems performance cpu weak link chain surprisingly high cpis processor utilization peaks rapidly end result cluster platform performing workloads dominated streaming obus membus disk bandwidth saturation read write workstation scan sort transpose obus membus backplane read write cluster workstation scan sort transpose obus backplane read write smp scan sort transpose figure balanced systems figures reflect balance architecture x-axis shows resources system y-axis amount per-processor disk bandwidth introduce system resource reaches peak utilization flat set bars balanced system higher set bars system authors jim gray eric anderson kim keeton rich martin amin vahdat valuable comments feedback suggestions ashok singhal sun performance counters katherine hartsell sun information sun enterprise servers sharad mehrotra sun generous equipment donation special satoshi asami disks finally shephard jean-loup baer direction setting work context work sponsored darpa contract -cand california state micro program remzi arpaci-dusseau funded intel graduate fellowship agarwal super scalar sort algorithm risc processors acm sigmod conference pages june amdahl storage parameters system potential ieee computer groupconference pages june anderson culler patterson case networks workstations ieee micro february arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations sigmod boden cohen felderman kulawik seitz myrinet gigabit-per-second local area network ieee micro february chen bershad impact operating system structure memory system performance proceedings annual symposium operating systems pages december culler liu martin yoshikawa logp performance assessmentof fast networkinterfaces ieee micro measure transaction processing power datamation readings system message size bytes origin speedup mhz system machinesize processors spe dup ideal figure nas benchmarks performance mhz origin machine figure shows speedup nas benchmarks origin mhz machine execution time benchmarks significantly higher multi-workload environment dedicated environment average runtime benchmark -processor seconds average runtime benchmark -processor seconds dedicated mode benchmarks run processors execution time benchmarks increases seconds seconds execution time profile shows communication time increase 
average seconds processor communication time increases factor unexpected results suggest communication benchmarks interfere leads higher synchronization cost conclusion detailed analysis architectural requirements nas benchmarks shows benchmarks perform non-trivial amount communication scalable communication system principle handle communication load dominant factor base performance scalability sequential node architecture including cpu caches local memory system important node architecture interacts application requirements cps scaling communication found applications carefully designed perform coarsegrained communication efficiency communication lower expected interestingly origin spite availability fine-grained shared memory data transport achieves fairly low communication efficiency cases spending time communication cluster result work word caution common assumptions machine architecture scalability tempted judge communication ability machine based speedup nas benchmarks good speedup implies good communication conversely poor speedup implies poor communication nas benchmarks necessarily defined scalability communication system origin superlinear speedups poor communication scalability examine computation communication scaling parallel machine order judge machine effectiveness areas understanding scaling performance characteristics large parallel machines difficult problem nas parallel benchmarks critical step goal providing set common benchmarks comparison platforms current output benchmarks execution time scaling plotted time speedup efficiency reveal complexities benchmarks processor counts lightweight instrumentation added standard mpi libraries minimally report time spent computation versus communication simple breakdown give users insight nature processorversus networkperformancefora machine acknowledgments william saphir ernest orlando lawrence berkeley national laboratory mary hultquist nasa ames research center obtaining account origin numerical aerospace simulation facility nasa ames research center dan lenoski sgi mark straka ncsa helping understanding issues performance counters finally shirley chiu alan mainwaring david helpful comments discussions work research supported part darpa nsf cda doe asci djb california micro program agarwal horowitz hennessy analytical cache model acm trans comp sys vol anderson culler patterson team case networks workstations ieee micro february david bailey harris rob van der wigngaart william saphir alex woo maurice yarrow nas parallel benchmarks technical report nas- nasa ames research center nanette boden danny cohen robert felderman alan kulawik charles seitz jakov seizovic wen-king myrinet gigabet-per-second local-area network ieee micro volume number feb bob cmelik doug keppel shade fast instruction-set simulator execution profiling proceedings signetrics leonardo dagum david bailey eric barszcz horst simon nas parallel benchmarks results technical report rnr- nasa ames research center dongarra dunnigan message passing performance computers tennessee technical report cs- von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedings international symposium computer architecture gold coast qld australia gropp lusk doss skjellum high-performance portable implementation mpi message passing interface standard parallel computing september mark hill dinero cache simulator aug http wisc larus warts html mainwaring active message application programming interface communication subsystem organization california berkeley computer science department technical report ucb csd- october richard martin amin vahdat david culler thomas anderson effects latency overhead bandwidth cluster architecture proceedings international symposium computer architecture june message passing interface forum mpi message passing interface standard technical report tennessee knoxville april nasa ames research center npb detailed results http science nas nasa gov software npb npb results steven reinhardt mark hill james larus alvin lebeck james lewis david wood wisconsin wind tunnel virtual prototyping parallel computers sigmetrics edward rothberg jaswinder pal singh anoop gupta working sets cache sizes node granularity issues large scale multiprocessors proceedings international symposium computer architecture pages saavedra-barrera cpu performance evaluation execution time prediction narrow spectrum benchmarking thesis berkeley technical report ucb csd february william saphir alex woo maurice yarrow nas parallel benchmark results technical report nas- nasa ames research center elisabeth wechsler nas parallel benchmarks set industry standard mpp performance nas news jan feb volume number http science nas nasa gov pubs nanews benchmark html steven cameron woo moriwoshi ohara evan torrie jaswinder pal singh anoop gupta splashprograms characterization methodological considerations proceedings international symposium computer architecture pages june maurice yarrow rob van der wijngaart communication improvement nas parallel benchmark model efficient parallel relaxation schemes technical report nas- nasa ames research center november 
database systems stonebraker morgan kaufmann san mateo gray personal communication june hill larus reinhardt wood cooperative-shared memory software hardware scalable multiprocessors acm transactions computer systems kleiman voll eykholt shivalingiah williams smith barton andg skinner symmetric multiprocessing solaris proceedings compcon spring kuszmaul out-of-core ffts parallel application environment technical report nas technical report rnd- mccalpin sustainable memory bandwidth current high-performance computers white paper mukherjee hill case making network interfaces peripheral hot interconnects aug nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference perl sites studies windows performance dynamic execution traces osdi pages october stonebraker case shared database engineering sunmicrosystems sbus specification rev white paper torrellas gupta hennessy characterizing caching synchronization performance multiprocessor operating system asplos-v pages october von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings acm symposium operating systems principles pages december von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedingsof annual symposium computer architecture gold coast australia 
deconstructing storage arrays timothy denehy john bent florentina popovici andrea arpaci dusseau remzi arpaci dusseau department computer sciences wisconsin madison abstract introduce shear user-level software tool characterizes raid storage arrays shear employs set controlled algorithms combined statistical techniques automatically determine important properties raid system including number disks chunk size level redundancy layout scheme illustrate correctness shear running numerous simulated con gurations verify real-world applicability running shear software-based hardware-based raid systems finally demonstrate utility shear case studies show shear storage management environment verify raid construction detect failures demonstrate shear extract detailed characteristics individual disks array show operating system shear automatically tune storage subsystems speci raid con gurations categories subject descriptors storage management storage hierarchies general terms measurement performance keywords storage raid introduction modern storage systems complex highend storage array tens processors hundreds disks array con gured ways commonly raidraid- raidhowever internal complexity raid arrays expose simple interface consisting linear array blocks internal complexity hidden large array exports interface single disk encapsulation advantages important transparent operation unmodi systems top storage device transparency cost users applications easily obtain information storage system storage systems reveal data blocks mapped underlying disks permission make digital hard copies part work personal classroom granted fee provided copies made distributed pro commercial advantage copies bear notice full citation rst page copy republish post servers redistribute lists requires prior speci permission fee asplos october boston massachusetts usa copyright acm raid con guration large impact performance reliability fact con guring modern array dif cult error-prone administrators verifying correctness setup paper describe shear user-level software tool automatically identi important properties raid tool characterize raid developers higher-level software including systems database management systems tailor implementations speci array run administrators shear understand details arrays verifying con gured raid expected observing disk failure occurred common microbenchmarking general approach shear generate controlled request patterns disk measure time requests complete applied generally similar techniques single-disk storage systems carefully constructing patterns shear derive broad range raid array characteristics including details block layout strategy redundancy scheme building shear applied number general techniques critical successful realization important application randomness generating random requests disk shear control experimental environment avoiding multitude optimizations common storage systems crucial shear inclusion variety statistical clustering techniques techniques shear automatically conclusions avoid human interpretation demonstrate effectiveness shear running simulated real raid con gurations simulation demonstrate breadth shear running variety con gurations verifying correct behavior show shear discover interesting properties real systems running shear linux software raid driver uncover poor method parity updates raidmode running shear adaptec raid controller card unusual left-asymmetric parity scheme finally demonstrate utility shear tool case studies rst show administrators shear verify correctness con guration determine disk failure occurred raid array demonstrate shear enables existing tools extract detailed information individual disks array show system knowledge underlying raid improve performance speci cally show modi linux ext system performs stripe-aware writes improves sequential performance hardware raid factor pppp striping raid stripe size pattern size pppp ppp pppp pppp parity raid left symmetric stripe size pattern size striping zig zag stripe size pattern size mirroring raid stripe size pattern size mirroring chained declustering stripe pattern pppp parity raid stripe size pattern size pppp pppp pppp parity raid left asymmetric stripe pattern pppp pppp pppp pppp qqqqpppp qqqpppp qqqqpppp qqqqpppp parity stripe size pattern size figure examples terminology picture displays number disk arrays layout patterns discussed paper numbers represent blocks parity blocks redundant data denoted italics case chunk size blocks stripe size pattern size blocks listed array depicts full patterns layout scheme rst shaded gray rest paper organized section describe shear illustrating output variety simulated congurations redundancy schemes section show results running shear software hardware raid systems section show shear improve storage administration system performance case studies finally discuss related work section conclude section shear describe shear software identifying characteristics storage system multiple disks begin describing assumptions underlying storage system present details raid simulator verify shear give intuition behavior finally describe algorithms compose shear assumptions paper focus characterizing block-based storage systems composed multiple disks speci cally assumptions shear determine mapping logical block numbers individual disks disks mirrored copies parity blocks model storage system captures common raid levels variants chained declustering assume storage system properties data allocated disks block level block minimal unit data system reads writes storage system chunk set blocks allocated contiguously disk assume constant chunk size stripe set chunks data disks shear assumes mapping logical blocks individual disks repeatable unknown pattern pattern minimum sequence data blocks block offset pattern located disk likewise pattern mirror parity blocks disks note con gurations pattern size identical stripe size raidand raidleft-symmetric pattern size larger raidleft-asymmetric based assumption shear detect complex schemes autoraid migrate logical blocks physical locations redundancy levels figure illustrates number layout con gurations analyze paper con guration disks chunk size blocks vary layout algorithm level redundancy raid systems typically signi amounts memory caching shear high-performance sorting attempt identify amount storage memory policy replacement techniques developed applicable due random accesses steady-state behavior shear operates correctly presence cache long cache small relative storage array assumption shear initiate read requests cached perform writes overwhelm capacity cache framework makes additional assumptions assume disks homogeneous performance capacity random accesses makes shear robust heterogeneity detail assume shear access raw device access blocks directly storage system bypassing system buffer cache finally assume traf processes system found shear robust small perturbations techniques basic idea shear accessing sets disk blocks timing accesses detect blocks located disks infer basic properties block layout intuitively sets reads slow assumed located disk sets reads fast assumed located disks basic approach shear employs number techniques key operation randomness key insight employed shear random accesses storage device random accesses important number reasons random accesses increase likelihood request disk cached prefetched raid performance random access dominated number disk heads servicing requests shear easily identify number disks involved random accesses saturate interconnects hide performance differences finally random accesses tend homogenize performance slightly heterogeneous disks historical data disk bandwidth improves year seek time rotational latency improve year result disks generations similar terms random performance sequential performance steady-state shear measures steady-state performance storage system issuing large number random reads 
writes approximately outstanding requests examining steadystate performance ensures storage system prefetch cache requests important write operations temporarily buffered writeback raid cache statistical inferences shear automatically identi parameters storage system statistical techniques shear graphical presentations results veri cation human user required interpret results automatic identi cation performed clustering observed access times k-means x-means clustering shear determine access times similar blocks correlated safe operations operations shear performs storage system safe accesses read operations writes performed rst reading existing data memory writing data result shear run storage systems live data shear inspect raids disk failures performance anomalies time simulation framework demonstrate correct operation shear developed storage system simulator simulate storage arrays variety striping mirroring parity con gurations simulate raidraid- raidraid- left-symmetric left-asymmetric right-symmetric rightasymmetric layouts redundancy chained declustering con gure number disks chunk size disk storage array include cache disks storage array con gured perform similarly ibm lzx disk simulation disk storage array fairly detailed accurately modeling seek time rotation latency track cylinder skewing simple segmented cache con gured disk simulator combination methods issuing scsi commands measuring elapsed time directly querying disk values provided manufacturer speci cally simulate rotation time head switch time cylinder switch time track skew sectors cylinder skew sectors sectors track disk heads seek time curve modeled two-function equation proposed ruemmler wilkes short seek distances cylinders seek time proportional square root cylinder distance endpoints longer distances seek time proportional cylinder distance endpoints algorithm shear steps step parameter storage system identi shear determines pattern size shear identi boundaries disks chunk size shear extracts detailed information actual layout blocks disks finally shear identi level redundancy shear behaves correctly striping mirroring parity examples section begin assuming storage system redundancy show shear operates redundancy additional simulations section describe algorithmic steps detail pattern size rst step shear identi pattern size pattern size ned minimum distance blocks located disk shear operates testing assumed pattern size varying assumed size single block prede ned maximum slight unimplemented nement simply continue desired output results shear divides storage device series non-overlapping consecutive segments size shear selects random segment offset random segments issues parallel reads offset segment workload random requests repeated times completion times averaged increasing effect concurrently examining segments disk increasing conducts trials random offsets intuition algorithm nition match actual pattern size requests disks equal requests disk requests serviced parallel disks response time storage system expected requests serviced disk illustrate behavior disk raidarray block size chunk size blocks blocks blocks blocks blocks blocks blocks blocks blocks figure pattern size detection sample execution disks chunk size blocks shaded blocks read shear increments assumed pattern size compactness gure starts assumed pattern size blocks increases time blocks gure highlights blocks stride reality random blocks read pattern size detection raid disks chunks pattern size assumed time pattern size detection raid disks chunks pattern size assumed time pattern size detection raid disks chunks pattern size assumed time figure pattern size detection simulations graphs show results running pattern size detection algorithm raidwith chunks disks actual pattern size blocks figure shows location reads assumed pattern size increased sample execution top graph figure shows timings workload run simulator sample execution shows assumed pattern blocks requests disks result timings stride minimum sample execution shows assumed pattern blocks requests disks result timing slightly higher finally assumed pattern size blocks requests disk stride results highest time detect pattern size automatically shear clusters observed completion times variant x-means cluster algorithm clustering algorithm require number clusters priori shear selects cluster greatest completion time correct pattern size calculated greatest common divisor pattern size assumptions cluster demonstrate shear detect pattern sizes con gure simulator disks remaining graphs figure desired blocks stride disks disks located disk mark length pattern boundaries chunk size step shear identi data boundaries disks chunk size data boundary occurs blocks block allocated disk block chunk size ned amount data allocated contiguously single disk shear operates assuming data boundary occurs offset pattern shear varies pattern size determined previous step shear selects patterns random creates read request offset pattern shear selects random patterns creates read request offset mod requests issued parallel completion times recorded workload repeated trials times averaged intuition correspond disk boundary requests disk workload completes slowly correspond disk boundary requests split disks complete quickly due parallelism illustrate disk raidarray figure shows portion sample execution chunk size detection algorithm top graph figure shows timings sample execution shows equal requests disks values requests disk timing data validates result requests offset faster shear automatically determines chunk size dividing observed completion times clusters k-means algorithm selecting cluster smallest completion time data points cluster correspond disk boundaries raid chunk size calculated difference boundaries show shear detect chunk sizes striping variants begin raidand constant pattern size examine disks chunks disks chunks graphs figure desired accesses slow intervals stress boundary detection zigzag striping alternating stripes allocated reverse direction scheme shown figure graph shows rst chunks stripe large expected layout previous steps shear determine pattern size chunk size step shear infers chunks repeating pattern fall disk determine chunks allocated disk shear block block block block block block block block figure chunk size detection sample execution disks block chunks shaded blocks read shear increments offset pattern requests shown accessing pattern selected random chunk size detection raid disks chunks boundary offset assumed time chunk size detection raid disks chunks boundary offset assumed time chunk size detection raid disks chunks boundary offset assumed time chunk size detection zig zag disks chunks boundary offset assumed time figure chunk size detection simulations rst graphs raidcon gurations disks chunks disks chunks disks chunks graph zig-zag striping con guration alternating stripes allocated reverse direction disks chunks examines turn pair chunks pattern shear randomly selects patterns creates read requests chunk pattern shear selects patterns creates read requests pattern requests pair issued parallel completion times recorded workload repeated trials results averaged shear examines pair figure shows results visualized interesting experiments con gure simulator model raidand zig-zag disks chunks point graph corresponds pair light points slow access times fall disk diagonal line graph corresponds pairs fall disk raidno chunks pattern allocated disk pairs shown chunk chunk chunk chunk figure read 
layout detection simulations rst graph raidthe graph zig-zag con gurations disks chunks points graph correspond pairs chunks pattern accessed simultaneously lighter points workload nished slowly chunks reside disk con ict zig-zag half pattern con icts blocks rst half shown upper-left lower-right diagonal line automatically determine chunks disk shear divides completion times clusters k-means selects cluster largest completion time shear infers chunk pairs cluster physical disk dividing chunks associative sets shear infer number primary data disks system algorithm elicits read dependencies pairs chunks running algorithm writes reads shear identify write dependencies occur due rotating mirrors chained declustering shared parity block raidor raidfor raidleft-asymmetric array figure writing blocks time result short response time operations spread disks writing blocks result longer response time share parity disk similarly writing blocks longer parity block block resides disk block write layout results reinforce conclusions networks workstations andrea arpaci-dusseau computer science division california berkeley dusseau berkeley remzi arpaci-dusseau computer science division california berkeley remzi berkeley david culler computer science division california berkeley culler berkeley joseph hellerstein computer science division california berkeley jmh berkeley david patterson computer science division california berkeley patterson berkeley report performance now-sort collection sorting implementations network workstations find parallel sorting competitive sorting large-scale smps traditionally held performance records -node cluster sort minute -node cluster finishes datamation benchmark seconds implementations applied variety disk memory processor configurations highlight salient issues tuning component system evaluate commodity operating systems hardware parallel sorting find existing primitives memory management file access adequate due aggregate communication disk bandwidth requirements bottleneck system workstation bus read layout results distinguish raidraid- raidand chained declustering discuss write layouts provide results section redundancy fourth step shear identi redundancy managed array generally ratio random read bandwidth random write bandwidth determined disk array manages redundancy detect redundancy managed shear compares bandwidth random reads writes shear creates block-sized random reads issues parallel times pattern size detection raid pattern size assumed time chunk size detection raid boundary offset assumed time figure pattern size chunk size detection raidwe simulate raidwith disks chunks rst graph con rms pattern size graph con rms chunk size completion shear times random writes issued parallel writes performed safely needed rst reading data storage system writing values extra intervening traf ush caches ratio read write bandwidth compared expectations determine amount type redundancy storage arrays redundancy raidthe read write bandwidths expected approximately equal storage systems single mirror raidthe read bandwidth expected write bandwidth reads balanced mirrored disks writes propagate disks generally ratio read bandwidth write bandwidth exposes number mirrors systems raidparity write bandwidth roughly fourth read bandwidth small write requires reading existing disk contents parity writing values back disk raidarrays bandwidth ratio varies number disks single parity disk bottleneck makes raidmore dif cult identify discuss section problem arises redundancy detection algorithm solely reads shear writes writes conjunction reads essential shear observe difference case block read case block parity mirrors committed disk depending speci storage system test writes buffered time written stable storage systems risk data loss desktop drive past reporting years enabled dramatic higher-end improvements arrays speed amount sorting algorithms non-volatile largely ram due increased attention safely delay issues writes computer architecture acknowledged sorting case results shear date avoid produced effects industrial buffering researchers move working steady-state expensive domain well-endowed inducing versions disk shared-memory writes parallel issued computers smps manner produced shear parent achieves companies simple paper adaptive technique describe basic achieved idea records sorting redundancy performance detection algorithm shear modest monitors shared-nothing write network bandwidth general-purpose write unix phase workstations write set performance minutesort record fast previously observed bytes read performance -node shear cluster concludes ultrasparcs pattern size datamation detection benchmark raid record left symmetric seconds pattern size -node assumed cluster time network workstations sorting number benefits nows provide high-degree performance isolation pattern size analysis detection behavior raid left node-by-node asymmetric pattern factor-by-factor size basis assumed contrast time smp resources pooled difficult achieve equivalent fine-grained analysis tuning single-processor performance carefully deliver roughly disk bandwidth -processor sorting application nows provide incremental scalability hardware resources additional workstations added pattern size well-tuned programs detection raid easily scaled symmetric pattern large size configurations assumed time contrast smps hard limit scalability imposed size box expensive complex scale limit paper note lessons research community made largely researchers operating systems computer architecture principle nows similar shared-nothing architectures pattern size detection raid databases asymmetric pattern size typically assumed analyzed time tuned context computeintensive applications demonstrate nows state-of-the-art platform data-intensive applications ease assembling configurations motivated investigate family solutions sorting single algorithm tuned machine investigation exposes range options overlapping figure steps pattern size sorting detection algorithm raidwe simulate implications raidwith left-symmetric develop left-asymmetric tools right-symmetric characterize existing right-asymmetric hardware layouts explore con relationship guration parallelism disks bandwidth chunk size constraints characterize pattern number size tradeoffs pipelining raidleft-symmetric costs memory rest utilization additional writes contribution buffered work written evaluate disk commodity software round writes hardware initiated sorting eventually extension writes ood database write applications cache find induce threads storage system current interface desired memory steadystate management behavior modern writing unix operating systems data disk shear developing detects efficient transition implementations observing writes cheering update longer faster reads wellknown criticisms earlier versions slower unix explore database issue applications experimentation hand section demonstrate identifying important layouts facilities finally shear missing modern pattern size workstations chunk size handling read data layout striped write layout heterogeneous redundancy disks information determining attempt match memory machine observations architecture find schemes clustered ultrasparc raidleft-asym- workstations metric limited match insufficient found bus shear rst bandwidth re-evaluates left number unimproved disks prevent system data-intensive instance applications number enjoying future disks gains cpu doubled memory network raidand disk incremented speed organization raidshear completes paper matches reporting development total number sorting disks implementations array briefly chunk reviewing size related work layout section observed match experimental platform found section shear reports cover discovered versions chunk size now-sort number disks sections reports figure speci depicts algorithm derivation unknown sorting assuming algorithms one-pass chunks parallel allocated two-pass sequentially parallel disks section shear section produce section suspected layout section based one-pass single-node observations two-pass figure single-node read figure write development layout sorting detection raidwe simulate left raidleft-symmetric left-asymmetric right-symmetric right-asymmetric disks rst row displays read layouts row shows write layout graphs pattern size detection raid pattern size assumed time read layout chunk size detection raid boundary offset assumed time write layout figure pattern size chunk size layout detection raidwe simulate raidwith disks chunks rst graph con rms pattern size detected graph shows chunk size detected read layout graph resembles raidbut write layout graph uniquely distinguishes raidfrom parity-based schemes redundancy simulations section describe shear handles storage systems redundancy begin showing results systems parity speci cally raidraid- mirroring variants raidand chained declustering simulations storage array disks chunk size purpose comparison present base case raidin figure parity shear handles storage systems parity blocks form redundancy demonstrate variants raidraid- redundancy raidraid- calculates parity block stripe data location parity block rotated disks raidcan number layouts data parity blocks disks left-symmetric left-asymmetric right-symmetric right-asymmetric left-symmetric deliver bandwidth layout pattern size equal stripe size raidin raidlayouts pattern size times stripe size pattern size detection pattern size assumed time read layout chunk size detection boundary offset assumed time write layout figure pattern size chunk size layout detection present simulated results redundancy disks chunk size rst graph con rms pattern size detected graph shows chunk size detected read layout graph resembles raidbut write layout graph distinguishes schemes pattern size results raidsystems shown figure rst graph shows pattern size leftsymmetric identical raidthe graphs show left-asymmetric right-symmetric right-asymmetric pattern sizes chunks expected note apparent noise graphs x-means clustering algorithm correctly identify pattern sizes chunk size algorithm behave differently raidversus raidtherefore omit results figure shows read layout write layout graphs raidnote raidvariants leads distinct visual image light points correspond dependent chunk pairs slow points dark correspond independent chunk pairs offer fast concurrent access read dependence occurs chunks located disk write dependencies occur chunks reside disk share parity disk interference parity disk instances result overburdened disk longer response time graph depicts pattern-sized grid accounts pairs chunks raidleft-asymmetric read layout graph chunk chunk grid points pair chunk chunks light color chunks located disk knowledge shear identify storage system standard raidvariants calculate number disks raidraid- calculates single parity block stripe data parity blocks reside single disk pattern size chunk size read layout write layout results raidare shown figure pattern size parity disk invisible read-based workload read layout graph resembles raidresult pattern size equal stripe size disk occurs pattern hand write layout graph raidis unique parity disk bottleneck writes pairs chunks limited single disk exhibit similar pattern 
size detection raid pattern size assumed time read layout chunk size detection raid boundary offset assumed time write layout figure pattern size chunk size layout detection raidwe present simulated results raidwith disks chunk size rst graph con rms pattern size detected graph shows chunk size detected read layout write layout graphs resemble raidcompletion times bottleneck produces raidwrite layout graph allowing distinguish raidfrom parity schemes demonstrate shear handles parity schemes show results detecting pattern size chunk size redundancy raidin parity scheme stripe algorithm sections paper follow development progression sorting algorithms simplest complex complex versions build simpler show understanding tuning application performance simple configurations build highly-scalable parallel versions effort datamation benchmark time one-pass parallel version section minutesort results two-pass parallel sort section present conclusions section datamation sorting benchmark introduced group database experts test processor subsystem operating system performance metric benchmark elapsed time sort million records disk disk records begin disk -bytes -bytes key million -byte records data read written disk elapsed time includes time launch application open create close files ensure output resides disk terminate program priceperformance hardware software computed pro-rating five-year cost time sort previous record-holder benchmark processor sgi challenge disks main memory seconds single processor ibm disks memory impressive time seconds price performance raw disk allowed benchmark recognizing datamation benchmark outdated test startup shutdown time performance authors alphasort introduced minutesort key record specifications identical datamation performance metric amount data sorted minute elapsed time priceperformance calculated list price hardware operating system depreciated years sgi system previous record-holder minutesort benchmark sorting alphasort achieved processors disks memory price performance years numerous authors reported performance sorting algorithms implementations leverage implementation algorithmic lessons describe difference work provide measurements range system configurations varying number processors number disks machine amount memory difference environment parallel configurations node complete system virtual memory system disks file system cluster environments form experimental testbed consists commodity ultrasparc workstations memory measurements extend nodes due time constraints workstation houses internal rpm seagate hawk disks single fast-narrow scsi bus note disks machine afford dedicate spare disk paging activity ultra enterprise model rpm disk main memory internal rpm seagate hawk external rpm seagate barracuda enclosure external disks fast-wide scsi card myrinet card myrinet f-sw switch -node system rpm disks rpm disks scsi cards disk enclosures myrinet cards switch -node system rpm disks myrinet cards switches table hardware list prices october list prices cluster connects fully-equipped ultrasparc model workstations main memory extra fast-wide scsi card rpm seagate barracuda external disks attached cluster eighth processors configuration quarter number disks amount memory main lesson taught authors alphasort large sorting problems performed single pass half amount disk performed price memory low previous recordholders datamation minutesort benchmarks sort records single-pass configuration memory-starved perform minutesort benchmark passes addition usual connection world ethernet workstation single myrinet network card myrinet switch-based high-speed localarea network links capable bi-directional transfer rates myrinet switch ports -node cluster constructed connecting switches -ary tree machine cluster runs solaris modern multithreaded version unix disparate resources cluster unified glunix prototype distributed operating system glunix monitors nodes system load-balancing co-schedule parallel programs full job control redirection experiments primarily glunix parallel program launcher parallel versions now-sort written splitc split-c parallel extension supports efficient access global address space distributed memory machines split-c built top active messages communication layer designed advantage low latency high bandwidth switch-based networks active message essentially restricted lightweight version remote procedure call process sends active message specifies handler executed remote node message received handler executes atomically respect message arrivals active messages myrinet performance characteristics round-trip latency roughly layer sustain uni-directional bandwidth node sending receiving study make number simplifying assumptions distribution key values layout records processors disks allowing focus architectural issues involved sorting implementations sorting data sets real world robust precedent set researchers measure performance now-sort key values uniform distributions assumption implications method distributing keys local buckets processing nodes non-uniform distribution modify implementations perform sample sort adding early phase sample data determine range keys targeted processor ensure processor receives similar amount records plan investigating future assume initial number records workstation equal performance parallel implementations greatly affected small imbalances records located subset workstations current 
read phase utilize processors attached disks severe performance implications restructuring algorithms deal situation scope paper section describe basic one-pass version nowsort single workstation records fit main memory read disk discuss impact interfaces performing disk buffer management in-memory sorting algorithm figure one-pass single-node version forms basis sorting algorithms components single-node sort sorts worth understanding tuning detail highest level one-pass single-node sort steps detail shortly read read -byte records disk main memory keys partially sorted simultaneously distributing buckets sort sort -byte keys memory keys buckets bucket sorted individually quicksort partial-radix sort cleanup write gather write sorted records disk implementation steps overlapped synchronous investigate benefits overlapping sorting reading copying keys bucketswhile reading majority execution time spent phases performing begin describing approach reading disk parity blocks calculated reed-solomon codes layout left-symmetric raidin figure rst graph shows pattern size detected graph shows chunk size figure shows read layout write layout graphs read layout graph resembles raidthe write layout graph exhibits distinct performance regions slowest time occurs requests chunk disk repeating pattern fastest time occurs requests parity updates spread evenly disks instance pairing chunks middle performance region occurs parity blocks chunk con ict data blocks chunk testing chunks half parity updates chunk fall disk chunk unique write layout distinguish parity-based schemes mirroring algorithms shear handle storage systems mirrors impact mirrors greater parity blocks read traf directed mirrors key assumption make reads balanced mirrors reads primary copy shear detect presence mirrored copies demonstrate shear handles mirroring simple raidand chained declustering raidthe results running shear disk raidsys- tem shown figure note pattern size raidis half raidgiven chunk size number disks rst graph shows raidpattern size inferred shear reads offsets pattern requests mirrors desired worst performance occurs request offset equal real pattern size case requests serviced pattern size detection chained declustering pattern size assumed time read layout chunk size detection chained declustering boundary offset assumed time write layout figure pattern size chunk size layout detection chained declustering present simulated results chained declustering disks chunk size rst graph con rms pattern size now-sort work variety cluster configurations differing numbers disks amounts memory order application configure performance environment gather relevant information provide software exist stripe files multiple local disks speeds fully utilize aggregate bandwidth multiple disks machine implemented user-level library file striping top local solaris file system building top raw disk similar approachdescribedin striped file characterized stripe definition file specifies size base stripe bytes names files disks stripe multiplicative factor associatedwith file disk determine proper ratio stripe sizes disks developeddiskconf tool list scsi buses list disks buses creates large data file disks reads file independently reads simultaneously files bus measuring achieved bandwidths determining bus saturated tool calculates multiplicative factor base stripe disk achieve maximum transfer rate tool performs analogous chore writes section disk configurations disks fast-narrow scsi bus additional disks fast-wide scsi verified striping library degrade achievable bandwidth disk table shows performance striped file system simple complex disk configurations rows rpm disks saturate fast-narrow scsi bus fast-narrow scsi bus peak bandwidth measure disks capable total potential performance lost due architectural oversight rows fast-wide scsi bus adequately handles faster disks finally rows show superiority diskconf configure striping library compared naively striping equal-sized blocks disks seagate disks scsi bus read write rpm hawk narrow rpm hawk narrow rpm barracuda wide rpm barracuda wide naive striping disk tool peak aggregate table bandwidths disk configurations read write columns show measuredbandwidth striping library rows give performance naive striping same-sized blocks disk disk tool peak aggregate calculated sum maximum bandwidths scsi buses speeds reading writing blocks data slower disk fast-narrow scsi fast-wide scsi achieve peak aggregate bandwidth disks depending system interface read data disk application graph effectively shows control chunk memory size usage prevent detected double-buffering wider bands operating read system layout section write layout compare graphs show approaches reading neighboring records chunks disk mirrored read mmap total mmap disks madvise uniquely identi demonstration chained purposes declustering disks simple implementation sorting illustrated algorithm fact performs steps worstcase sequentially time quicksorts workload keys raidis half memory left-most graph raidi figure shows seconds application graph read figure system shows call read chunk records size memory disk inferred total sort shear time increasesseverely boundary disks records requests sorted mirrors shear physical memory automatically detects disk performance boundary degradation occurs due workload time thrashing increases virtual requests memory system read disks file system disks performs mapping buffering chunks disks user program single unable pattern control total amount con icts memory read layout avoid write layout double-buffering graphs read figure resemble leveraging convenience raidchained declustering file chained system declustering investigate mmap redundancy interface scheme applications disks memory mapped exact files mirrors opening desired file calling disk mmap bind primary instance file block memory segment copy addressspace block accessingthe memory neighbor region results desired running shown shear middle disk graph system performance chained mmap declustering degrades shown figure records rst due graph shows page replacement pattern policy size virtual detected memory subsystem desired lru replacement raideach throws read request soon-to-be-needed sort-buffers serviced read disks phase pattern size ensuing identi thrashing degrades performance fortunately requests mmap auxiliary system disks call system madvise note informs chained operating declustering system pattern size intended access pattern raidsince disk region memory unique set data call blocks madvise notifies graph kernel figure shows region block accessed chunks sequentially allowing detected fetch ratio ahead current worstcase page performance throw differs pages case raidand accessed raidin right-most chained graph declustering figure ratio shows mmap raidand madvise raidthe sorting ratio program linear chained performance declustering roughly adjacent requests located disk memory boundary requests serviced disks raidwhen requests located chunk requests serviced disks mapping con icts amount chained memory declustering workstation determines interesting number shown records remaining graphs sorted figure one-pass chained sort declustering number pair records chunks run located multi-pass sort previous disks work results addressed issue distinct performance adapting sorting regimes algorithms case memory constraints shared disks occurs run-time chunks cyclically adjacent amount free chunks memory resulting sorting wider application bands read existing write solaris layout interface graphs accurate estimate raid-ls developed memconf memory raidraid- tool allocates operations buffer millions shear fetches overhead redundancy main read memory pattern write pattern chunk methods size writing pattern size arbitrary total word raid-la page figure copying shear overhead words graph shows memory-mapped number sequentially-advised input file generated prefetching phase tool shear touches simulated page redundancy schemes buffer shown recording cpu raidraid- utilization raidleft- buffer symmetric fits main memory raidleft-asymmetric cpu utilization numbers high disks current definition chunks larger bar buffer buffer fit cpu utilization low due paging activity tool backs smaller buffer binary search refine estimate usable memory running basic version memconf showed machines real memory user applications operating system daemon processes factored applications mmap madvise now-sort approximately memory finally verified predictions memory matched number records sort memory thrashing sharp increase sort time occurs sort records fit memory conservative scale estimate memory predictions comfortable distance memory wall wasting memory section quantifies performance main-memory sorting techniques performed correctly in-core sort disk-to-disk single-node sort comprises small portion total execution time plots in-core sort number consumes phase seconds required shear sort million records rightmost bar ibm shows total disks raidleft-asymmetric results total time plotted log section scale investigate y-axis programming overhead complexity needed examine achieve overhead range shear performance showing measuring scales in-core disks sorting added algorithms quicksort system figure in-core plots sort total simple number quicksort shear generates keys simulation memory variety previous disk work con gurations swapping x-axis vary key con pointer guration full record y-axis faster plot swapping number entire generated -byte record tool note extra memory work raidleft-asymmetric results required set shown log pointers scale comparisons y-axis keys begin graphs make most-significant word observations examine remaining words total previous number issued identical simple top schemes line left-side graph raidraid- figure shows raidleft-symmetric time low incore millions quicksort scales function slowly number disks keys added system system disks raid schemes approximately seconds required read shear scales write million larger records arrays quicksort run execution time raidwith spent leftasymmetric in-core layout sort shear bucket generates quicksort in-core sort performs redundancy quicksort schemes keys total number distributed buckets scale keys reason poor scaling buckets behavior based read layout write layout detection hightime bars seconds account size read total time traf write time illustrated sort time figure read raidleft- asymmetric pattern size grows time time square seconds size number mmap disks total time write layout time algorithms sort issue time requests read pairs chunks pattern large time patterns time lead seconds large size numbers mmap requests madvise total time write serviced time sort parallel time read time figure read versus mmap graphs show cumulative numbers ultrasparc memory seagate rpm disk graphs show read mmap total sort time increases super-linearly nearing memory limits machine right-most graph shows memory mapping advising memory effectively order bits key placing keys raidleft-asymmetric represents bucket extreme easily case overlapped shear read phase current due form shear simple interface provided roughly mmap days complete degradation read read layout bandwidth write layout overlapping detection cpu raidleft-asymmetric underutilized disks implementation bucket reduce factor significant -bits ten issuing key fewer disk removing topa pairwise -bits trial pointer reducing run full time record decreasing con common dence case layout -bits results real key platforms examined section ties present results keys occur applying shear random accesses memory real platforms number rst buckets linux determined software run-time raid device driver average number keys bucket adaptec fits hardware raid second-level controller cache understand behavior shear ultrasparc real quicksort systems performed ran in-place large partial variety key software pointer requires hardware bytes con urations varying keys number bucket disks note chunk size approach redundancy highly scheme dependenton uniform results distribution expected keys revealed skewed slightly surprising distribution properties buckets systems test keys fit cache raidmode degrading hardware performance controller shown employs middle left-asymmetric line figure performing quicksort buckets parity read fit write ratio region second-level size cache faster effect region quicksort size quantum memory atlas ibm approach ultrastar lzx seagate total cheetah time sort figure sensitivity million records region size spent gure in-core plots sort bucket bandwidth ratio partial-radix series in-core random sort read performs requests partial-radix compared sort series clean-up random write keys requests bucket x-axis varies suggested size region most-significant -bits experiment run key removing run top sector-sized read -bits write pointer requests 
issued lines full record plotted disks bucket radix quantum sort atlas relies wls representation ibm keys lzx -bit seagate numbers cheetah perform passes keys radix size bandwidth examining total amount written -bits refer effect write buffering partial-radix sort figure avoiding write radix sort buffer gure plots bits performance pass writes top keys histogram raidhardware constructed writebuffering enabled count x-axis varies digits number writes histogram issued scanned y-axis plots lowest entry achieved bandwidth highest placement calculate due rank space constraints key concentrate sorted-order finally challenging keys aspect permuted shear redundancy destinations detection experimenting rank redundancy detection histogram uncovered partial issues radix sort clean-up addressed phase produce performed robust keys algorithm ties rst top size bits region bubble-sorted quicksort test number run buckets figure plots selected read write ratio average number single keys disk bucket size fits region secondlevel varied cache radix sort gure requires size source region destination buffer half test conducted keys fit strongly cache uence compared outcome quicksort tests keys bottom line quantum left-side disk graph desired figure ratio shows roughly radix sort achieved bucket small greatly region superior sizes quicksort ratio grows disk time reason seconds keys quicksort undesirable quicksort ation buckets large radix settling time buckets figure quantum comparison disk in-core conclude sorting algorithms redundancy graph compares detection algorithm sorting time run quicksort small keys portion memory disk quicksort odds desire run small portion disk issue presence writeback cache raid adaptec card congured perform write buffering host writes complete quickly disk time note presence buffer affect data integrity buffer non-volatile redundancy detection algorithm independently issue write bucket requests disk radix sort compare read bucket request graph timings shear cumulative total time sort raid-la million records raidraid- spent in-core sort combination raidr distributing ratio keys read write buckets bandwidth performing ratios radix software sort raid hardware keys raid figure bucket redundancy detection gure plots ratio read write bandwidth variety disk con gurations x-axis varies number disks con guration raidraid- raidor raidleft-asymmetric software hardware raid circumvent caching effects recall shear simple adaptive scheme detect bypass buffering issuing successive rounds write requests monitoring performance point write bandwidth decreases indicating raid system moved steady-state writing data disk memory reliable result generated figure demonstrates technique 
adaptec hardware raid adapter write caching enabled enhancements place study redundancy detection software hardware raid systems figure plots read bandwidth write bandwidth ratio number con gurations recall read write ratio key differentiating redundancy scheme ratio redundancy ratio mirrored scheme ratio raidstyle parity encoding note hardware raid card support raidand con gure raidon disks gure shows shear redundancy detection good job identifying scheme expected read write ratios approximately raidnear raidand raidthere points make bandwidth ratios raidscale number disks due parity disk bottleneck makes dif cult identify raidarrays rely write layout test previously exhibits bottleneck write performance unique results write layout test distinguish raidfrom parity-based schemes note performance software raidon disks expected read write ratio measure ratio tracing disk activity inspecting source code revealed linux software raid controller utilize usual raidsmall write optimization reading block parity writing block parity read entire stripe blocks write block parity finally graph shows raidwith disks -disk mirrored system distinguishable disks raidand mirroring converge shear applications section illustrate bene shear begin showing shear detect raid conguration errors disk failures show shear discover information individual disks array finally present storage system paramefigure detecting miscon gured layouts raidleft-symmetric left-asymmetric right-symmetric rightasymmetric upper graph shows read layout graph raid ibm disks correctly con gured lower graphs show read layout logical partitions miscon ured physical device chunk size detection raid boundary offset assumed time figure detecting heterogeneity rst graph shows output chunk size detection algorithm run array single heterogeneous fast rotating disk row gures shows results read layout algorithm simulated disk con gurations con guration single disk capability fast rotating slow rotating fast seeking slow seeking disk depicted gures ters uncovered shear tune system speci cally show system improve sequential bandwidth writing data full stripes shear management intended shear administrative utility discover con guration performance safety problems figure shows failure identify scheme suggest storage miscon guration upper set graphs expected read layout graphs common raidlevels lower resulting read layout graphs disk array miscon gured logical partitions reside physical disk graphs generated disk arrays comprised logical disks built linux software raid ibm disks visualization makes obvious manual inspection shear automatically determines results match existing schemes shear detect unexpected performance heterogeneity disks experiment run shear range simulated heterogeneous disk con gurations exchunk size detection raid left symmetric boundary offset assumed time chunk size detection raid left symmetric boundary offset assumed time figure detecting failure chunk size detection algorithm shear discover failed devices raid system upper graph shows initial chunk size detection results collected building disk software raid system ibm disks lower graph system fth disk removed periments disk slower faster rest figure shows results run heterogeneous con gurations gure faster slower disk makes presence obvious ways read layout graphs chunk size detection output pattern size detection unaffected administrator view outputs observe unexpected performance differential disks action correct problem finally chunk size detection algorithm shear identify safety hazards determining redundant array operating degraded mode figure shows chunk size detection results ten disk software raid system ibm disks upper graph shows chunk size detection correctly working array rst built lower graph shows chunk size detection changed physically remove fth disk array recall chunk size detection works guessing boundaries timing sets requests sides boundary vertical downward spikes half height plateaus guessed boundary correct requests serviced parallel disks plateaus false boundaries requests sides guessed boundary incurred disk lower graph identi array operating degraded mode boundary points missing disk disappear plateau higher due extra overhead performing on-they reconstruction shear disk characterization related projects concentrated extracting speci properties individual disk drives techniques built top characteristic knowledge aligning les track boundaries free-block scheduling shear enables optimizations context storage arrays shear expose boundaries disks existing tools determine speci properties individual disks demonstrate ability skippy disk characterization tool skippy sequence write operations inwrite time request number skippy disk write time request number skippy raiddisks write time request number skippy raiddisks figure skippy gures plot results running skippy disk characterization tool single quantum disk disk raidarray disk raidarray creasing strides determine disk sector track ratio rotation time head positioning time head switch time cylinder switch time number recording surfaces rst graph figure shows pattern generated skippy single quantum disk graph figure shows results running modi version skippy raidarray disks version skippy array information provided shear map block stream logical blocks residing rst disk array results pattern identical running single disk allowing extract individual disk parameters nal graph figure shows results technique applied disk raidarray results identical single disk pattern small perturbations affect analysis limitations approach case raidthe skippy write workload performs expected read workload produces spurious results due fact reads balanced disks conversely reads work raidwhereas writes due update parity information additionally parity blocks raidcannot directly accessed characterization tools obtain incomplete set data limitations tested read-based version skippy raidand successfully extracted parameters individual disks shear performance stripe size disk array large impact performance effect important raidstorage writes complete stripe require additional previous work focused selecting optimal stripe size workload show system adapt size alignment writes function stripe size bandwidth average file size effects stripe-alignment stripe-aligned default figure bene stripe alignment gure plots bandwidth series creations average size varied x-axis variants shown system generates stripe-sized writes default linux workload consists creating les x-axis size les uniformly distributed basic idea system adjust writes stripe aligned optimization occur multiple places modi linux device scheduler properly coalesces divides individual requests raid stripe-sized units modi cation straight-forward lines code added kernel simple change make system stripe-aware leads tremendous performance improvements experiments shown figure run hardware raidcon guration quantum disks chunk size results show stripe-aware system noticeably improves bandwidth moderately-sized les improves bandwidth larger les factor related work idea providing software automatically uncover behavior underlying software hardware layers explored number domains earliest work area targeted memory subsystem measuring time reads amounts strides saavedra smith reveal interesting aspects memory hierarchy including details caches tlbs similar techniques applied identify aspects tcp protocol stack determine processor cycle time cpu scheduling policies work related targeted characterizing single disk storage system worthington identify characteristics disks mapping logical block numbers physical locations costs low-level operations 
size prefetch window prefetching algorithm caching policy schindler talagala build similar portable tools achieve similar ends shown shear conjunction low-level tools discover properties single disks inside arrays evaluations storage systems focused measuring performance workload uncovering underlying properties interesting synthetic benchmark adapts behavior underlying storage system benchmark examines sensitivity parameters size requests read write ratio amount concurrency finally idea detailed storage-systems knowledge system storage client investigated schindler investigate concept track-aligned placement single disk systems work modi system allocates medium-sized les track boundaries avoid head switches deliver low-latency access les systems lfs atropos augment array interface provide information individual disks lfs knowledge disk boundaries dynamically allocate writes based performance control redundancy perle basis atropos volume manager extends storage interface expose disk boundary track information provide cient semi-sequential access two-dimensional data structures shear enables information multiple disk systems worth added complexity note implemented optimizations found including restricting number buckets number tlb entries overlapping sorting writing small optimization clean-up phase significant gain methods summarize figure shows total time returned unix time command one-pass single-node version now-sort systems disks timing run flush file cache unmounting re-mounting striped disks expected incore sorting time negligible configurations compared time reading writing records disk read bandwidth roughly balanced write bandwidth matches transfer times found disk configuration tool finally performance linear number records records fit main memory time seconds keys amout data datamation disks total write sort read time seconds keys amout data datamation disks total write sort read figure single-node single-pass sort time graphs cumulative time sort shown function number records graph left depicts system disks connected fast-narrow scsi bus graph system additional disks fast-wide scsi systems real memory optimizing single-node single-pass sort shed light number machine architecture operating system issues disk configuration tool shown ultrasparc internal fast-narrow scsi bus saturates rpm disk fast-wide scsi bus enabled achieve internal disks loss performance found striping disks bandwidths requires stripe sizes disks evaluation file system interfaces shown mmap simple efficient fine-grained access files mmap madvise interfaces copying keys buckets completely hidden disk transfer time obtaining performance read system call require programming complexity cost issuing read high users threads prefetch data large chunks file system buffering occurs withreadwastes inordinate amount memory acceptable data-intensive applications simple memory configuration tool application effectively determine memory memory tool discovered buffer requirements ofmmap roughly tax applied free memory interface gave programs accurate run-time estimate free memory preferable detailed intricacies single-node sorting records fit memory specifically disk striping memory management in-core sorting algorithm examine one-pass parallel sort issues arise communication added present performance datamation benchmark assuming records begin evenly distributed workstations numbered steps single-node sort extend naturally four-step parallel algorithm read processor reads records local disk main memory communicate key values examined records local remote buckets sort processor sorts local keys write processor gathers writes records local disk workstations memory-maps input files calculates processor remote bucket key current implementation determines destination processor simple bucket function top bits key copies key input file sendbuffer allocated destination processor approach bucketizing assumes key values uniform distribution specifics records send-buffer vary implementations detail message records arrives processor active message handler executed handler moves records sort-buffer copies partial keys pointers correct local buckets operations directly analogous distributing partial keys buckets single-node sort significant difference single-node version distributing records local buckets processor distributes records remote buckets computation naturally decoupled active messages sending processor determines destination processor owning range buckets receiving processor determines final bucket performing calculations identical single-node version message handler synchronizing processors ensure records received node performs incore sort records writes local portion local disk sort write steps identical single-node version end data sorted disks processors lowest-valued valued keys processor highest-valued keys processor note number records node approximately equal depends distribution key values simultaneously reading records local disk sending records processors potential overlap diskwait time network latency overlapping operations shared resources cpu narrow narrow wide hybrid time seconds disks synch interleaved threaded write sort distribute read figure comparison algorithms left-most bar group shows performance breakdown synchronous implementation middle-bar interleaved implementation right-most bar threaded enhanced interface conclusions presented shear tool automatically detects important characteristics modern storage arrays including number disks chunk size level redundancy layout scheme keys shear randomness extract steady-state performance statistical techniques deliver automated reliable detection veri shear works desired series simulations variety layout redundancy schemes subsequently applied shear software hardware raid systems revealing properties speci cally found linux software raid exhibits poor performance raidparity updates adaptec raid adapter implements raidleft-asymmetric layout shown shear case studies storage administrators shear verify properties storage arrays monitor performance detect disk failures shear extract individual parameters disks array enabling performance enhancements previously limited single disk systems finally shown factor improvement performance system tuning writes stripe size raid storage storage systems computer systems general complex layers interacting components remain concealed veil simplicity hope techniques developed shear reveal true power future systems subsequently make manageable composable cient acknowledgements bradford beckmann nathan burnett vijayan prabhakaran muthian sivathanu anonymous microbenchmark-based extraction local global disk characteristics nisha talagala remzi arpaci-dusseau david patterson computer science division california berkeley abstract obtaining timely accurate information low-level characteristics disk drives presents problem system design implementation alike paper presents collection disk microbenchmarks combine empirically extract relevant subset disk geometry performance parameters efficient accurate manner requiring priori information drive measured benchmarks utilization linearly-increased stride glean spectrum low-level details including head-switch cylinderswitch times factoring rotational effects bandwidth benchmark extracts zone profile disks revealing previously preferred linear model zone bandwidth accurate quadratic model seek profile generated completing trio benchmarks data collected broad class modern disks including scsi ide simulated drives introduction theories fundamental data remains mary leakey sustained innovation hard-drive industry spurred incredible advances disk technology performance capacity benefited bandwidth increasing sixty percent year capacity growing rate disk drive industry moves quickly drive appears market twelve months due rapid evolution clients modern disks left quandary obtain accurate detailed information inner-workings recently manufactured disks system implementors knowledge low-level performance characteristics lead improved policy decisions system researchers simulations parameterized latest disk attributes facilitating timely relevant research straight-forward methods obtaining performance characteristics prove successful detailed specifications complete accurate solution put literature employ microbenchmarks characterize hardware software systems alike carefully crafted microbenchmarks utilized wide range environments accurately describe performance uniprocessor multiprocessor memory systems discover cost communication mechanisms parallel machines measure performance operating system primitives evaluate file systems extract parameters scsi disk drives calculate megahertz rating processors applying microbenchmarks disk drives vexing problem complex drive mechanism involving cooperating mechanical electronic parts benchmarks adequate domains translate disk drives rotational factor affects measurement results renders current position drive unpredictable seek develop microbenchmarks suitable extracting performance parameters modern disk drives ideally disk microbenchmarks exhibit properties general runs vast array systems specialized specific kind disks ideal benchmark requires priori information drive measured complete extracts relevant parameters including disk geometry performance parameters low-level parameters including head cylinder switch times overlooked accurate extracts parameters excellent precision fast runs quickly giving information seconds minutes hours days paper introduce microbenchmarks designed extract performance parameters hard disk drives sum total microbenchmarks approach ideal microbenchmark axes general running scsi ide drive raw-device interface run quickly extracting parameters seconds completely characterize physical properties disk drive finally produce accurate drive geometry performance parameters percent manufacturer-reported values contributions reviewers paper three-fold simple method based linearly increased step-size extracting localized disk parameters including platter count sectors track rotational delay head switch time cylinder switch time minimum time media slowly ramping step-size factor rotational effects unveil host drive performance characteristics empirical characterization large collection modern disk drives including scsi ide drives previous work focused solely scsi-drive extraction update results zoned nature modern disks including correction proposed linear model accurate quadratic model present results executing microbenchmarks diverse collection modern drives including scsi ide simulated drives study uncovered numerous interesting results discovered minimum overhead write disk media widely varies drives generation found family drives manufacturer exhibited similar strengths weaknesses seagate drives tend excellent switching times multi-zoned nature modern disks pronounced outer tracks delivering bandwidth tracks surprisingly scsi disks performance characteristics ide disks measured scsi bandwidth switching characteristics programmed dma renders ide drive overhead lower rest paper organized section give background disk terminology related work section section presents overview collection microbenchmarks results range disks presented section conclusions section background explaining functionality disk characterization tool give overview modern disk drives in-depth excellent summaries modern disk drive behavior basic internal structure disk drive rotating disks coated sides magnetic media rotating disk called platter side disk called recording surface data stored recording surface concentric circles called tracks track divided sectors sector minimum unit data accessed disk media typical modern disks -byte sectors tracks surface equidistant center form cylinder disks zoned bit recording zbr outer tracks disk higher sectors track ratio tracks read write heads surface ganged disk arm time move arm proper cylinder called seek time time required sector rotate head referred rotational latency time transfer data media called transfer time modern disks head active time sector track track cylinder access spans tracks disk complete portion track switch heads continue track sector mappings consecutive tracks skewed head switch time switching heads requires short repositioning time skew prevents request crosses track boundaries missing logical block wait full rotation similarly access spans cylinders disk arm seek forward cylinder consecutive cylinders skewed cylinder switch time related work inspired separate works literature saavedra presents microbenchmarking technique memory systems paper worthington describes extract performance geometry parameters scsi disks seek combine simplicity speed accuracy saavedra introduces simple powerful method extract performance characteristics multi-level memory hierarchy benchmark repeatedly performs basic loop reading memory locations fixed-size array stride surprisingly characteristics memory hierarchy including number caches capacity associativity block size access times extracted simply changing size array length stride technique applied disk yield results desired disk subsystems regular memory hierarchies complex interaction rotation seek time leave direct application saavedra disk infeasible study worthington describes partially automated tools extracting parameters scsi disk drives twofold approach interrogative empirical extraction interrogative extraction library scsi access functions read mode pages disk mode pages describe disk parameters sectors track ratio prefetch buffer size information extracted mode pages construct test vectors empirical extraction process measure minimum time requests mtbrc kinds comparing mtbrcs test vectors calculate switching times parameters main disadvantage approach reliance user send low-level scsi commands disks highly non-portable requires user trust disk manufacturer information parameter extracted requires separate group test vectors algorithms outlined worthington minutes hours extract parameters contrast benchmarks version interleaved threaded versions read distribution time collapsed read category hybrid implementation reads fast-wide disks writes disks bus under-utilized subsection evaluate versions one-pass parallel sort vary degree overlap disk communication synchronous synchronous version processor reads communicates sorts writes overlap steps buffer records wherea expansion factor related distribution key values drawback straight-forward implementation requires memory records sorted buffer needed records read processor buffer needed records received implementation reserves extra buffer communication buffers requires extra synchronization processors iteration processor sends processor mod minimize contention sending complete buffer processor processors synchronize waiting records received steps continue records distributed interleaved implementation alternates reading communicating thread send-buffer destination small current implementation waiting records read records sendbuffer full alternating reading sending advantage overlapping memory robust distribution key values synchronousversion algorithm synchronizes processors ensuring records received beginning in-core sort threaded final implementation overlaps reading sending threads reader-thread reads records moves local send-buffers communicator-thread sends receives messages sets send-buffers exist destination reader finishes filling buffers set signals communicator sends records current set measurements shown figure found disks node interleaved versions time seconds processors secs secs disks sort glunix disks sort glunix disks sort glunix disks sort glunix figure scalability one-passparallel sort records processor sorted cluster disks memory machine records processor sorted disks memory machine single dual threaded outperform synchronous version roughly threads perform slightly stagger read send requests smoothly time disks difference synchronous overlapped versions diminishes disks system algorithms perform identically interestingly interleaved versions read slower rate disks rpm disks reduction occurs ultrasparc bus bus saturated long theoretical peak find read disks simultaneously sending receiving approximately aggregate s-bus make good disks capable transferring s-bus sustain roughly aggregate due limited bandwidth s-bus found hybrid system performance interleaving reading communication hybrid system reads disks fast-wide bus writes write phase profits disks concurrent communication bus devoted solely disk activity threaded parallel sort equals outperforms implementations focus exclusively remainder experiments one-pass parallel now-sort perfectly scalable number processors increased number records processor constant shown figure words sort keys amount time simply doubling number processors slight increase time due overhead glunix distributed 
operation system remote process start-up increases processors taking approximately seconds nodes seconds performance datamation benchmark shown figure processor sorts equal portion million records processors added time seconds processors sgi record disks sort glunix disks sort glunix disks sort glunix disks sort glunix figure performance datamation benchmark graph shows time sort million records disks processorand disks processor data points shown million records fit memory node sorts fewer records resulting small problem sizes remote process start-up significant portion total sorting time fact processors total time equally divided process start-up application function lack maturity cluster fundamental costs distributed operating system focus optimization interestingly process start-up time-consuming smps bzero ing address-space significant dominant cost onepass external sorts smp parallelize process nows aspect process creation problem local address space initialized parallel parallel single-pass sort revealed o-system bottleneck ultrasparc architecture s-bus s-bus achieve workstation effectively disks simultaneously communicating network remain viable alternative smps ultrasparc bandwidth improve dramatically radically provide communication memory bus coherence corollary suggests system disks fast-wide scsi bus sweet-spot cost performance curve conclusion found developing parallel single-pass sort natural extension single-node version lessons previous section disk striping memory management in-core sorting directly applicable environment isolate performance effects per-processor basis difference single-node version additional communication traffic s-bus contrast smp system pooling resources obscures type analysis finally active message interface provided high bandwidth low overhead communication facilitated algorithm differed slightly single-nodeversion copyingkeys local buckets processor copies keys buckets spread processors one-pass versions now-sort detail extensions needed single-node insufficient memory passes made records section memory configuration tool determine amount memory choose one-pass two-pass algorithms create runs one-pass sort read sort write repeated create multiple sorted runs disk merge phase sorted runs merged single sorted file create sorted runs phase threads created reader-thread writer-thread memory divided buffers equals depending reader writer synchronous overlapped discussed section run containsa records requiring runs number records reader copies records disk moves keys pointers buckets signals buffer full reader free fill empty buffer run writer waits buffer filled sorts bucket writes records disk signals buffer empty process repeated runs sorted written disk separate files merge sorted runs phase multiple runs merged single output file implementation threads reader merger writer readerthread memory-maps thea run files reads chunk records run sets merge buffers depending reading merging synchronous overlapped discussed section prefetching records large chunks amortizes seek time runs obtains read bandwidths sequential accesses reading thea buffers reader signals merger buffers full continues prefetch set empty merge buffers merger selects lowest-valued key top run copies record write buffer write buffer full writer signaled writes records disk note merge phase instance found simple implementation usingmmap andmadvise attain sequential disk performance accessing multiple read streams disk mmap prefetch data sufficiently large blocks amortize seek costs merge phase explicitly manage prefetching multiple threads buffers phases reading disk overlapped writing computation one-pass parallel sort overlapping phases pipelining beneficial resources cpu bus under-utilized overlapping additional implications layout records disks memory usage pipelining phase implies pipelining phases output phase input investigate impact run length layout records disks phases time seconds number runs run size create runs merge runs total time figure effect run size non-cumulative time phases two-phase sort shown total time keys ultrasparc memory disks creatingruns phaseone run fit memory merging runs phase number runs run length synchronous version phase memory create run generates half runs pipelined version runs adversely affect performance merge phase reading disk streams raise seek time hide prefetching large chunks number runs constrained exists sufficient memory prefetch buffers run merge phase disk layout synchronous pipelined versions manage layout records disks differently maintain sequential access disk dedicated reading writing time pipelined implementations reading writing times half disks reading excellent feedback work sponsored nsf ccrccr- ccrngs- itritr- require half writing synchronous implementations ibm emc wisconsin alumni research foundation bray bonnie file system benchmark http textuality bonnie burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey california june chen lee striping raid level disk array proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages ottawa canada chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june chen patterson maximizing performance striped disk array proceedings annual international symposium computer architecture isca pages seattle washington chen patterson approach performance evaluation self-scaling benchmarks predicted performance proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages santa clara california denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june emc corporation symmetrix enterprise information storage systems http emc glaser tcp stack fingerprinting principles http sans newlook resources idfaq tcp ngerprinting htm october grochowski emerging trends data storage magnetic hard disk drives datatech september hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international conference data engineering icde pages los angeles california february katcher postmark file system benchmark technical report trnetwork appliance october lee katz performance consequences parity placement disk arrays proceedings international conference architectural support programming languages operating systems asplos pages santa clara california april lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october norcutt iozone filesystem benchmark http iozone padhye floyd identifying tcp behavior web servers proceedings sigcomm san diego california august patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june pelleg moore x-means extending k-means cient estimation number clusters proceedings international conference machine learning june regehr inferring scheduling behavior hourglass proceedings usenix annual technical conference freenix track monterey california june ruemmler wilkes introduction disk drive modeling ieee computer march saavedra smith measuring cache tlb performance effect benchmark runtimes ieee transactions computers savage wilkes afraid frequently redundant array independent disks proceedings usenix annual technical conference usenix pages san diego california january schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november schindler grif lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix symposium file storage technologies fast monterey california january schindler schlosser shao ailamaki ganger atropos disk array volume manager orchestrated disks proceedings usenix symposium file storage technologies fast san francisco california april staelin mcvoy mhz anatomy micro-benchmark proceedings usenix annual technical conference usenix pages orleans louisiana june talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley varki merchant qiu issues challenges performance analysis real disk arrays ieee transactions parallel distributed systems june wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february worthington low-level access disk interface sense closer true black box microbenchmarks drive parameters extracted skippy benchmark single fast experiment benchmarks section present collection disk characterization tools table summarizes constituent benchmarks microbenchmark extracts skippy linearly increases step distance platter count sectors track writes sector-sized block rotational delay head switch time cylinder switch time minimum time media writes zoned streams entire disk bandwidth function location reading large blocks seeker repeatedly writes sector-sized blocks seek cost function distance start disk locations table microbenchmarks table describes collection microbenchmarks paper skippy extract parameters runs small portion disk zoned produces bandwidth versus location characterization extracting zone profile disk finally seeker generates seek profile function 
distance standard techniques innovative component benchmark suite skippy utilizing technique linearly increasing stride writing disk factor rotational effects extract surprising amount information disk including sectors track ratio rotation time minimum time access media disk head positioning time head switch time cylinder switch time number recording surfaces impressive run-time characterization completes roughly skippy completely characterize behavior modern disk nature local benchmark crucial pieces global information missing cost seeks function distance effect zones function location derive final pieces information utilize microbenchmarks seeker zoned constructed benchmarks similar found due global nature benchmarks time consuming skippy taking minutes complete benchmarks rely raw device interface order bypass file system optimization activities caching buffering read ahead access raw interface benchmarks difficult impossible construct skippy skippy microbenchmark implements approach disk measurement linearly increasing strides counteract disk natural rotation figure shows pseudocode algorithm benchmark writes sector disk forwards file pointer writes iteration distance increases single sector resulting latency versus step size curve distinctive sawtooth shape extract parameters sectors track ratio rotation time minimum time access media disk head positioning ganger time head switch patt time cylinder switch wilkes time on-line extraction number scsi disk recording drive surfaces parameters proceedings gathered results acm read sigmetrics variant conference skippy measurement modeling sake computer space systems present sigmetrics write pages version ottawa canada benchmark read 
utilize disks reading disks writing phases synchronous input file temporary sorted runs output file striped disks phases pipelined file striped half disks phase synchronous synchronous phase suffers half disks employed pipelined phase measurements two-disk four-disk systems reveal two-pass single-node version occurs phases pipelined pipelined merge phase exhibits dramatic improvement synchronous version effectively prefetches large chunks merge run hides cost seeking pipelined merge improves slightly phase synchronous fewer runs merge penalty phase executing synchronously writing half disks outweighs improvement pipelined algorithm length run impacts performance phase number runs impacts phase section determine compromise number runs lengths results good performance phases tension phases sorting fixed amount data changing number runs shown figure runs created time phase large size run greater amount memory memory manager pages data disk point run fits memory time phase continues drop slightly run size decreases due lower cost filling draining read write pipeline data run time writer wait reader read run decreases time writer write run decreases conversely number runs increases time phase increases merge phase reads records disk chunks amortize cost seeking independent runs creating similar startup effect start-up time increases number runs becausethe reader prefetches fixed chunk runs net result memory wall runs increasing number runs problem set effect performance slightly decreasing time generate runs increasing time merge runs runs performance degrades sharply merge time increasing percent behavior suspect related cache performance performance drop-off mirrored increase user-cpu time disk-wait time conclude summarizing minutesort performance two-pass single-node sort disks table time merge phase slightly larger time create runs records divided twelve runs concurs data figure results comparison case two-pass parallel sort section disks disks create merge total table single-node minutesort results amount data sorted minute single node shown ultrasparc disks memory versus disks memory corresponds records records section now-sort accurate estimate memory choose one-pass two-pass algorithm number records determine maximum run size two-pass algorithm examining two-pass single-node sort found pipelined versions phases performed synchronous implementations number disks found mmap madvise prefetch sufficiently large blocks amortize seek costs multiple streams forcing explicitly prefetching thread merge phase prefetching large blocks sequential performance maintained merge phase read runs greater number disks system finally observed importance carefully managing layout records disks dedicating half disks reading half writing disrupt sequential access pattern generated stream section describe two-pass parallel algorithm results minutesort benchmark final sorting algorithm extendsnaturally one-passparallel twopass single-node algorithms create runs processors create sorted runs nodes cluster 
runs created repeating read send sort write steps one-pass parallel sort merge processor merges local sorted runs single local file phase identical merge single-node previous section discussed creating multiple sorted runs processors phase opportunities overlap onepass parallel algorithm reading sending overlapped two-pass single-node sort reading writing overlapped two-pass parallel sort overlap pairs operations understand tradeoffs describe actions threads detail reader communicator writer read thread responsible mapping input file copyingkeys records per-destination send-buffers send-buffers fill reader signals communicator one-pass parallel sort reader communicator overlapped sets send buffers synchronous set signaled communicator-thread picks entire set send-buffers sends destination processor message arrives active message handler invoked copies records sort-buffer partial keys bucket communicator records current run processors synchronize wait receive records run received writer-thread signaled writer sorts keys bucket gathers writes records disk similar two-pass single-node sort reading run overlapped writing previous run overlapping implies sets sort-buffers buckets exist set consuming approximately half memory measurements number disks system determine operations overlapped disks implementation overlaps operations disks overlaps reading sending performs reading writing synchronously operations overlapped cpu saturated utilization performing read write operations synchronously minimizes cpu bus contention creating run time stripes input file disks creates runs merge phase amount sorted processors sgi record perfect scaling perfect scaling disks disks figure parallel minutesort results graph shows minutesort results cluster configurations -node cluster disks processor -node cluster disks processor perfect scaling relative single-node performance performance two-pass parallel now-sort minutesort benchmark shown figure machines disks machine implementation sorts minute experience large drop number keys sorted move single-node sort parallel version running processor reading writing performed synchronously overlapped compared parallel version processor algorithm scales perfectly linearly processors cluster disks machine parallel version sort results analysis presented full version paper intuition analytical foundation traditionally extracting parameters head switch time disk drive difficult request incurs unpredictable rotational latency intuition linearly increasing stride method write accesses drive rotates distance forward requests incur latencies time successive requests reaching disk roughly step size requests linearly increasing eventually match distance disk rotates successive requests point observable requests prior incur extra rotation requests afterward basically access pattern designed advantage rotational mechanism separate rotational latency request contributing latencies result disk characteristics including head cylinder switch times observable describe skippy works simple analytical model model terms time full rotation rotational latency hand time request spends waitingfor required sector rotate head rotationallatency vary time transfer data media byte cost including overheads minimum time media minimum time access data disk surface disk request completes incurs rotational seek latency number sectors disk rotates time mtm equation defines terms note equation assumes linear relationship latency number sectors rotated seek time increase linearly seek distance stated earlier step sizes generate arm movement delay purely rotational disk rotates fixed speed delay increases linearly number sectors rotated figure shows expected sequence events single sector writes machine sorts keys single-node algorithm implementations overlap phases processors sort data ties previous minutesort record fully-loaded sgi system processors disks memory processors sort seconds nodes sort seconds algorithm scale linearly number processors drop largely due start-up bottlenecks costing seconds nodes due relative dearth memory workstations minutesort two-pass algorithm cluster previous record-holding systems minutesort performed single pass sort data performing sgi xfs system amount time paper presented now-sort collection configurable sorting algorithms networks workstations shown nows well-suited o-intensive applications sorting cluster environment ideal development efficient parallel applications system year procs disks mem datamation minutesort time price perf cents data price perf dec alpha axp dec alpha axp dec alpha axp sgi challenge ibm ultrasparc ultrasparc ultrasparc ultrasparc ideal ultrasparc table summary datamation minutesort results note price sgi challenge estimated ibm raw disk allowed datamation specification note bytes performance isolation monitoring behavior application node program-developer isolate performance bottlenecks clusters enable incremental scalability hardware processors disks memory added system effective manner work enabled key pieces software cluster active messages high-speed communication layer providing low latency high throughput parallel programs key software component glunix distributed operating system nows studying disk-to-disk sorting qualitatively quantitatively assessed workstation operating system machine architecture underlying goals project utilize existing commodity software hardware assumption general-purpose workstations form solid building blocks clusters connected high-speed network paper demonstrated strengths weaknesses assumption now-sort important function operating system support efficient file access find mmap madvise solaris fast finegrained access files file system providing adequate prefetching buffer management consumes memory older read interface track figure shows stages write disk open raw disk device measurements time sequence output time lseek single sector seek cur write buffer single sector close figure skippy algorithm basic algorithm skips disk increasing distance seek sector write outputs distance time write raw device interface order avoid file system optimizations single sector size single sector case bytes seek cur argument lseek moves file pointer amount relative current pointer start atdisk atsurface underhead rotationaldelay end start atdisk atsurface rotationaldelay underheadw end step size latency distance disk rotates requests figure behavior figure shows expected sequence events sector writes disk media writes labeled rotates sectors stage illustration show focusing single sector accesses transfer time negligible starts time time scsi subsystem processed request command reached disk time disk positioned head track time required sector disk head difference rotational latency write system call returned short time write begins disk rotated distance forward illustration step size greater distance required sector ahead request served rotation time time start loop iteration execute lseek call time negligible compared disk access times system microseconds average entire write takes assume time negligible make interesting observations rotational delay approaches rotational delay eliminated figure disk rotates approximately time sectors requests words extra rotation complete rotation logic model latency write request access track prior access request satisfied current rotation latency latency minimum time media time rotate remaining sectors substituting equation simpler term latency equation shows latency linear function step size request satisfied rotation latency equation equation simplified substituting substitution equation incurs double-buffering problem programs large memory requirements find mmap madvise sufficient reading multiple independent streams merge phase two-pass sort needed multiple threads manage prefetching found existing solaris support memoryintensive latency applications linear lacking sectors key trackwith areas offset equal efficiently employing rotational multiple latency disks differing step speeds puts single request workstation track straight-forward request incurs extra reasonable head switch determine delay memory tracks skewed application head switch disk wait full rotation case latencies calculated over-extend equations resources thrash solve problems developed small configuration tools evaluates speed disks system feeds information striping library allowing maximal performance disks tool measures memory node cluster tools combine enable sorting application tune cluster attain peak efficiency found bus structure ultrasparc workstation inadequate support disk communication sorting found disks implementation synchronous reading writing superior pipelined version surprised find machine disks connection high-speed network effective building block adding disks leads benefit solutions pending problem improved infrastructure bandwidth support disks network communication facilities function memory bus coherence leaving disks consume precious bus bandwidth found internal fast-narrow scsi bus equations prevents full cylinder utilization switch similar bandwidth internal disks performance loss performance system relative previous recordholders place datamation minutesort benchmarks shown table best-performing configuration note processors sorts equations million assume records one-pass long seconds distance seeks high model cost skippy processor system intended step sizes disks node seeks greater slightly slower single cylinder seconds point significantly cost significant arm performance movement ibm latency achieves scale linearly cost step performance size results illustrate expected raw graphical disk result allowed mock disk benchmark mock disk specification minutesort rpm results recording obtained surfaces machine sectors configurations track memory minimum time required access two-pass media external algorithms previous head record-holders cylinder contained switch times main memory hold data performing benchmark amount create long distance seeks sort seek bytes profile figure minute shows processors expected graphical configuration result accompanying disks illustrations shown figures deliver roughly reveal sorting points application previous graph minutessort illustration recordholder shows needed writes -disks deliver write shows request pattern price performance marked point project graph track processorswith shown disks concentric circles memory rotational delay fit records memory marked outer circle sorting rotational one-pass delay hardware marked obtain slightly circle performance visit home page find now-sort increases linearly project latency http sawtooth berkeley pattern point figure now-sort combined effort causing members large rotational delay berkeley project making richard martin implementing equal supporting split-c active messages myrinet work finished doug ghormley david petrou increases efforts latency making increases glunix linearly fast reliable equation system members tertiary disk group approaches lending barracuda disks equation shows ken lutz latency eric approaches fraser making clusters working reality extend special jim gray chris nyberg comments point early figure drafts helpful conversationsabout sorting finally anonymous reviewers time providing disk head feedback lowered work funded track part required sector darpa missed -cdarpa full rotation takes place -cnsf cda latency nasa fdnagwand california state micro program andrea arpacidusseau supported intel graduate overhead fellowship steps reach point figure agarwal super scalar slightly sort larger algorithm risc processors proceedings case disk head acm sigmod lowered conference time pages june rotational latency anderson latency culler patterson case networks workstations ieee micro time distance feb sectors mock arpaci disk base dusseau cylinder switch vahdat head liu switch base anderson heads patterson minimal time interaction media parallel transfer sequential time workloads rotational network latency workstations cylinder proceedings switch sigmetrics head performance switch pages figure baru mock disk fecteau output goyal hsiao mock disk jhnigran shown graph padmanabhan constructed wilson strictly overview models developed parallel text edition illustrate proceedings output sigmod international benchmark conference management startw data atsurface san jose rotationaldelay underhead underhead basu endw buch start vogels end atsurface von eicken rotationaldelay u-net figure user-level point network interface startw atsurface parallel rotationaldelay distributed underhead computing end proceedings underhead end acm atsurface symposium start operating rotational systems delay principles figure copper mountain point colorado rotationaldelay min dec timeto media baugsto start atsurface greipsland underhead kamerbeek end sorting start large data underhead files atsurface poma end proceedingsof figure point comparvappiv underhead end pages prior sept start springer verlag atsurface lecture rotationaldelay end start atsurface track end start ofnew track track underhead end track cylskew figure point notes beck increases bitton latency increases linearly wilkinson sorting equation large files graph backend sawtooth multiprocessor shape technical transition report department computer science cornell mar blelloch graph leiserson shows series maggs upward comparison spikes sorting correspond algorithms head connection cylinder machine switches cmin point symposium figure illustrates parallel head algorithms switch architectures case rotational july latency blumrich increased alpert dubnicki felten sandberg virtual memory mapped network interface equation shrimp extracting multicomputer parameters figure proceedings exposes international disk symposium details computer coordinate architecture point pages apr boden cohen felderman kulawik seitz myrinet gigabit-per-second local area network 
ieee micro feb boral alexander clay copeland prototyping bubba highly parallel database system ieee transactions knowledge data engineering mar culler dusseau goldstein krishnamurthy lumetta von eicken yelick parallel programming split-c supercomputing culler liu martin yoshikawa logp performance assessment fast network interfaces ieee micro feb dewitt ghandeharizadeh schneider bricker gamma database machine project ieee transactions knowledge data engineering mar dewitt naughton schneider parallel sorting shared-nothing architecture probabilistic splitting proceedings international conference parallel distributed information systmes dusseau culler schauser martin fast parallel sorting logp experience cmieee transactionson parallel distributed systems aug measure transaction processing power datamation readings database systems stonebraker morgan kaufmann san mateo gerber informix online xps proceedings sigmod international conference management data san jose ghormley petrou vahdat anderson glunix global layer unix http berkeley glunix glunix html graefe volcano extensible parallel coordinate transfer time small single sector coordinate point good estimate difference coordinates points latency step size height transition point information calculate number sectors track point reached reverse equation calculate note calculate sectors track ratio region written modern disks employ zone bit recording zbr outer cylinders packed sectors track cylinders due circular nature disks sectors track ratio regions disk run benchmark regions increases latencies form distinct lines 
slope offsets figure shows lines labeled conforms equation conforms equation taking difference offsets lines calculate slope line extract slope point represents head switch latencies conform equation vertical offset point corresponds cylinder switch vertical offset line finally step size number sectors track number recording surfaces counting number head switches cylinder switches larger number steps successive head cylinder switches decreases figure shows eventually step results head switch sample result prior section showed mock disk parameters extracted figure apply techniques ibm ultrastar disk drive manufacturer specifications learn disk rpm rotational latency recording surfaces recording zones outermost sectors track head cylinder switch times figure shows result running benchmark disk figure similar model result figure result behavior predicted equations equation completely explain result head cylinder switches figure head switches upward latency spikes consistent figure shows upward spikes small downward spikes approaches variation affect ability extract parameters require refinement analytical model refine model subsection focus extracting parameters figure error rates calculated comparing extracted values manufacturer values parameter extraction techniques earlier measured values coordinate point actual latency error height sawtooth wave estimate case error techniques yield extremely accurate results coordinate point coordinate equation states offset writing bytes transfer time small subtracting coordinate point estimate fact transfer time small effect virtually indistinguishable measurement noise coordinate good estimate hand estimate difference time distance sectors ultrastar skippy base base head switch cylinder switch heads minimal time media transfer time rotational latency cylinder switch head switch figure ibm ultrastar sample write result ibm ultrastar disk drive values points counterpart specification represents important estimate system overhead ratio actual sectors track error measured error compared specification similarly measured error compared specification finally counting number head switches cylinder switches find disk recording surfaces matches disk specification disk drive extracted values close actual values cases error rate refined analytical model writes figure showed simple model inadequate describing parts benchmark behavior graph shows downward spikes region explained equation section present refinement initial model explain effects figure downward spikes point happen head switch occurs close normal circumstances head switch mechanics equation apply time position head service write rotation prior write head switch occurs track skew disk head slightly extra time enabling disk service writes waiting extra rotation writes complete latencies close figure shows downward spikes extend head switch line left point observation confirms hypothesis spikes caused head switches adjusting model account effects create model graph identical sample result extra downward spikes reveal interaction disk head positioning head switch property estimate disk head positioning time full paper details refinement limitations limitation write skippy technique work disks delayed write optimizations limitation applies microbenchmarks attempt measure write latencies disks measurable read skippy variant open raw disk device read buffer large size large size transfer large size transfer report size output location bandwidth achieved region transfer close figure zoned algorithm benchmark simply reads disk sequentially blocks size large size threshold amount read report size benchmark outputs location bandwidth achieved region full paper show read result slightly write result interaction read-ahead mechanism smaller cases cases parameters extractable read benchmark slight variant basic read benchmark backwards read benchmark strides disk reverse direction measures parameters basic read write versions avoiding read-ahead optimizations tend obscure results plan investigate utility future work developed tool automatically extract parameter values graphical result extraction algorithm utilizes work statistics image-processing communities process latency versus data extract parameters listed table details final version paper extraction tool made online benchmark tool set zoned subsection briefly describes zoned microbenchmark designed extract bandwidth profile recording zones disk basic algorithm depicted figure straight-forward figure shows algorithm result ultrastar disk drive manufacturer specification learn disk recording zones sectors track ranging outermost zone innermost zone graph shows recording zones earlier demonstrated skippy extract sectors track local area run running skippy zone defined figure extract sectors track zone drive technique learn largest zone average sectors track sectors track values subsequent zones values match specifications observe large difference delivered bandwidth zones drive outermost zone bandwidth roughly tracks deliver roughly increase outer tracks seeker global disk characteristic missing seek profile fortunately seek delays based solely mechanical movements disk arm explored prior studies limit discussion seeks present variant skippy technique make seek experiments easier factoring rotational latency component measured time present seek curves function sector distance cylinder distance skippy local benchmark directly measure seek distances utilize slight variant figure measurements algorithm writes fixed location beginning disk variant disk space reused creates similar identical sawtooth wave minimum estimate seek time rotational latency bandwidth location ibm ultrastar zones largest size inner-most zone covers delivers roughly bandwidth figure ibm ultrastar zoned benchmark run ibm ultrastar figure shows seek latency versus distance sector seagate barracuda shape curve slightly seek curves papers textbooks seek time versus sectors versus cylinders note minimal time media included values reported true seek values obtained subtracting derived skippy benchmark basic algorithm suffers limitations similar skippy work drive immediately write data disk disk area reused read version work nearby disk sectors found buffer cache results section presents results range modern scsi ide drives simulated drives table lists drives measured real drive measured pentium memory running freebsd version scsi drives connected fast-wide scsibus figures show dataflow query processing system technical report oregon graudate center june graefe parallel external sorting volcano technical report cu-cscomputer science colorado boulder june hill larus reinhardt wood cooperative-shared memory software hardware scalable multiprocessors acm transactions computer systems hoare quicksort computer journal kleiman voll eykholt shivalingiah williams smith barton andg skinner symmetric multiprocessing solaris proceedings compcon spring linoff smith stanfill thearling practical external sort shared disk mpps proceedings supercomputing pages nov nyberg barclay cvetanovic gray lomet alphasort risc machine sort proceedings acm sigmod conference salzberg tsukerman gray stewart uren vaughna fastsort distributed single-input single-output external sort sigmod record june stonebraker operating system support database management communications acm july stonebraker case shared database engineering sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference jan tandem performance group benchmark nonstop sql debit-credit transactions proceedings sigmod international conference managament data chicago june teradata corporation dbc data base computer system manual release edition nov document number von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedingsof annual symposium computer architecture gold coast australia young swami parameterized round-robin partitioned algorithm parallel external sort proceedings international parallel processing symposium pages santa barbara apr zagha blelloch radix sort vector multiprocessors supercomputing zhang larson memory-adaptive sort masort database systems proceedings cascon toronto nov 
results skippy seeker zoned drive table summarizes extracted numbers benchmarks disk skippy scsi disk drives scsi disk cases clear height sawtooth wave rpm hawk wave high error rate rpm barracuda disk error rate estimated micropolis disk error rate finally rotation time rpm giving error measurements show vary disks rpm generation hawk average rpm disks ranged lowest seagate barracuda highest micropolis drive finally latest disk rpm lowest disks measured testbed open raw disk device base base disk size base large size measurements lseek seek set write single sector time sequence output location time lseek base single sector seek set write buffer single sector close figure seeker algorithm pseudocode seek algorithm presented benchmark jumps betweeen beginning disk target locale writing single sector time time write timed performed repeatedly parts disk shown loop base seek set argument moves file pointer absolute relative location call seek time distance seagate barracuda seeker figure seagate barracuda seek curve seek profile seagate barracuda note seek 
time non-linear small seeks linear model long-distance seeks note seek time reported includes time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure seagate hawk results presented seagate referred hawk note disk older generation head cylinder switch times good disk large number zones typical seagate disks finally seek curve standard time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure seagate barracuda results presented seagate referred barracuda note excellent head cylinder switch times skippy curve large number platters seagate devices odd number platters hypothesis extra platter position-sensing information zoned curve shows large number zones small indistinguishable finally seeker curve ranges time distance sectors skippy base cylinder switch head switch base heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure micropolis micropolis disk worst performers scsi class poor switch times exceptionally high zone profile odd zone delivers notably higher performance explanation behavior point time distance sectors skippy base basehead switchcylinder switch heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure ibm ultrastar disk presents typical output curve reasonable switch times zones disk outermost zones occupy half disk seek numbers noisy repetition explanation effect time distance sectors skippy base base head switch cylinder switch heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure ibm modern disks study fastest rotating disk revolution rpm low rotational latencies low small-seek costs head cylinder switch times prominent zoning ibm disks distinct zoning found seagate disks time distance sectors skippy base cylinder switchhead switch base heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned axis axis data figure quantum fireball ide ide disks study low profile disk recording surfaces recent drives high sectors track ratio single zone definition suggests drive manufacturers chose simplicity performance unable generate seek profile figure full paper time distance sectors skippy base cylinder switch head switch base heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure ibm ide ide drive study recent scsi drives considerably lower bandwidth higher switching times time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure simulated dartmouth simulator accurately simulates disk generation high rotational latency switch times minimal time media true disks era zone entire disk finally seek profile regular matches formula utilized authors note data simulator cleaner real-world disks time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure dec simulated data disksim michigan disk simulator presented disk modern dartmouth simulator show realistic behavior text zone profile uninteresting seek curve expected model year interface capacity dimensions rpm rotational recording latency surfaces seagate scsi hawk seagate scsi barracuda micropolis scsi ibm ultrastar scsi ibm scsi quantum fireball ide ibm-dttaide simulated dec simulated table disks disks years range rpm rpm detailed specifications ibm ultrastar disk drive table relevant information gather disk drives on-line specification sheets drives excepting simulated disks inch half height low profile table describes simulated disk drives drive represents trial benchmarks dartmouth disk drive simulator dec represents trial disksim simulator developed michigan assume operating system scsi overheads similar results show ibm drive lowest overhead access media seagate hawk barracuda drives interestingly seagate hawk considerably older rpm drives ibm ultrastar micropolis drive measured drives employ zbr extract sectors track ratio outermost zone drive hawk roughly sectors track barracuda ultrastar micropolis finally compare head cylinder switch times graphs show seagate barracuda drive lowest head cylinder switch times hawk cylinder switch time comparable ultrastar hawk older drive counting number head switches cylinder learn hawk recording surfaces barracuda micropolis match specification data table seagate drives odd number recording surfaces suggesting dedicate surface track mentioned ide disk drives figures show write behavior quantum ibm ide disks graphs show caching activity lower step sizes fact appears drives write buffer cache requests empties cache additional request reached behavior entire result graph shift graphs slightly shifted measure rotational latency height transition point measured quantum fireball error specification drive recording surfaces consistent disk specifications table quantum drive head switch time cylinder switch time measured rotation time ibm ide disk error compared specification sectors track ratio disk recording surfaces head switch time cylinder switch time disk low values bit scsi disks reflects programmed disk rotation mtm sectors heads head cylinder bandwidth seek time switch switch outer max min track time time seagate hawk seagate barracuda micropolis ibm ultrastar ibm quantum fireball ibm-dtta sim dec sim table extracted values table lists extracted parameters disk drive including ranges bandwidth seek curves note values seek curve adjusted minimum time media reflect actual seek characteristics dma common ide drives improves overhead cost discussing achieved bandwidth ide simulated drives figures show results dartmouth disk simulator disksim experiments verify skippy technique matches disks expected work disk simulators values extracted measurements match simulator disk specifications simulated results noticeably cleaner measurement results comparing simulation results disksim result shows downward spikes sawtooth transition real disks dartmouth result suggesting newer disksim simulates drive closely older dartmouth simulator zoned drive skippy result accompanied zoned result make general observations zoned results older simulated drives newer ide drives show recording zone ide drives implies drive manufacturers sacrifice performance simplicity achieved bandwidth ide drives low reflect programmed dma scsi drives seagate drives noticeably finely zoned ibm micropolis drives finally disks multiple zones difference outer-track inner-track bandwidth ranges recent comprehensive discussion disk drive zoning behavior observed relationship transfer rate disk position linear function single examining zone results curve closer parabolic linear quadratic function form fit zone 
graph linear function fact fitting linear quadratic functions data standard linear regression techniques learned quadratic function factor factor error simple linear fit linear fit explored extra advantage required highest lowest bandwidth values drive found quadratic fit values factor linear fit values fact cases quadratic fit values linear fit values factor model employed recommend usage quadratic fit simple construct linear model requiring data points matches profiles linear fit disks zones exact step function utilized disksim simulator makes exact characterization seeker figures table show seek latency start drive areas function seek distance sectors seeks tenth disk seek latency appears increase linearly sector distance seek latency increases larger numbers cylinders close examination data reveals seeks reaching innermost zones latency increase higher linear observable ibm seek result seek time increases rapidly sectors track ratio decreases rapidly area requiring arm movement sector distance conclusions paper presents disk benchmarks skippy zoned seeker extract range parameters modern disk drives skippy illustrates approach measuring disks linearly increasing stride patterns technique extensions filter rotational effect kinds disk measurements knowledge present benchmark utilizes disk rotational mechanism characterizing disk defeat benchmarks run scsi drives ide drives disk simulators revealing numerous results modern disk drives find minimum time access drive media vary widely drives generation seagate drives show excellent switching time characteristics ibm drives bandwidth results show similarities drives made manufacturer odd number recording surfaces present seagate drives scsi drives older show performance ide drives switching times bandwidth overhead ide reads writes lower improvements linear areal density reflected sectors track number recording surfaces measured drives modern drives higher sectors track ratio older drives number recording surfaces concurrently decreasing rotational latency seek times decrease head-switch cylinder-switch times important hawk switch times roughly rotation time percentages ibm simple models disk behavior characterize disks seek rotation time longer full paper provide details skippy read write benchmarks future work includes exploration backwards read variant retains benefits read benchmark avoiding interaction read-ahead mechanism benchmarks extraction tool measured data made public website hope run benchmark contribute data active archive disk characteristics remzi arpaci david culler arvind krishnamurthy steve steinberg kathy yelick empirical evaluation cray-t compiler perspective annual international symposium computer architecture iscapages june peter chen david patterson approach performance evaluation self-scaling benchmarks predicted performance proceedings acm sigmetrics conference pages david culler lok tin liu richard martin chad owen yoshikawa logp performance assessment fast network interfaces ieee micro gregory ganger bruce worthington yale patt disksim simulation environment version manual technical report cse-tr- department electrical engineering computer science michigan february cristina hristea daniel lenoski john keen measuring memory hierarchy performance cache-coherent multiprocessors micro benchmarks supercomputing san jose november ibm ultrastar hardware functional specification models rpm version document number ibm storage products division june ibm ibm disk drive specifications http storage ibm david kotz song bac toh sriram radhakrishnan detailed simulation model disk drive technical report pcs-tr department computer science dartmouth college july larry mcvoy carl staelin lmbench portable tools performance analysis proceedings winter usenix january rodney van meter observing effects multi-zone disks proceedings usenix conference january micropolis micropolis disk drive specifications http procom homepage tech quantum quantum disk drive specifications http quantum chris ruemmler john wilkes introduction disk drive modeling ieee computer march rafael saavedra stockton gaines michael carlton characterizing performance space shared-memory machines micro-benchmarks hot interconnects san jose august rafael saavedra-barrera cpu performance evaluation execution time prediction narrow spectrum benchmarking phd thesis berkeley computer science division february david schwarderer andrew wilson understanding subsystems adaptec press edition january seagate seagate disk drive specifications http seagate carl staelin larry mcvoy mhz anatomy micro-benchmark proceedings usenix annual technical conference pages berkeley usa june usenix association bruce worthington greg ganger yale patt john wilkes on-line extraction scsi disk drive parameters proceedings acm sigmetrics conference pages 
time nanoseconds stride bytes cray alpha local memory hierarchy cache main memory off-page access off-page access bank time nanoseconds stride bytes dec workstation memory hierarchy cache cache main memory main memory tlb cache tlb time nanoseconds stride bytes local write performance write merging merging off-page access alpha shell bytepageoffsetindexseg index dtb annex address translation offset page byte offset page byte noon percent jobs submitted hour time day mpp submission times percent jobs finish minutes length job minutes mpp job length distribution frequency weighted frequency time nanoseconds stride bytes uncached remote read split-c time nanoseconds stride bytes cached read latency time nanoseconds stride bytes blocking remote write latency split-c slowdown number competing parallel jobs local scheduling cholesky column connect sample skew skew skew slowdown coscheduling skew cholesky column connect sample slowdown frequency seconds duration column connect cholesky sample slowdown frequency seconds duration column connect cholesky sample time nanoseconds prefetch group size prefetch latency pop queue memory barrier prefetch issue split-c prefetch slowdown percent parallel non-idle column connect cholesky sample slow time nanoseconds stride bytes non-blocking remote write latency split-c noon percent time day workstation availability aggressive peak condor peak condor aggressive slowdown migration time seconds migration cost workstations workstations workstations slowdown recruitment threshold seconds recruitment threshold workstations workstations workstations day percent time idle time minutes idle time distribution minutes minutes frequency weighted bandwidth transfer size bytes bulk read bandwidth bulk transfer engine prefetch queue cached read uncached read split-c bandwidth transfer size bytes bulk write bandwidth write cache write memory bulk transfer engine split-c memory appears proceedings annual conference file storage technology fast march ata versus scsi question addressed paper phrased terms ata drives versus scsi drives accurate ata versus scsi debate groups drives interface interface significant difference differences mechanics materials electronics firmware make real distinctions drive considerable number employees directly impact business operations normal operation faster drives service requests employees supported productive workers key requirements key requirements manifested drives market interface scsi ata dave anderson jim dykes erik riedel seagate technology abstract paper sets clear misconception prominent storage community today scsi disc drives ide ata disc drives technology internally differ external interface suggested retail price classes drives represent product lines aimed markets fact classes range products address variety features usage patterns simply interface talk device target market final product specification account earliest design decision manufacturing testing process paper attempts clarify differences illuminating design choices consequences final device characteristics community build storage systems knowledge trade-offs made performance characteristics result introduction manufacturer product families aimed customer segments smart city coupe daimlerchrysler mercedes e-class sedan apparent technology gasoline engine round wheels similar disc drives traditionally sold personal computer systems distinct appearance performance cost sold larger computer systems refer personal storage enterprise storage classes disc drives portable computers consumer electronics devices disc drives differ important ways classes discuss leave future work comparing unique features drives larger cousins families product lines choosing drive application system designers underlying factors assume interface distinction sufficient interface difference categorize drives correctly fact instances drives equipped scsi interface drives high-end personal computers inherent reason drive ata interface personal storage important quality drives drive cost commensurate cost system installed cost pressure personal computer market gave rise low-cost hard discs continued put pressure drive pricing discuss drives back point repeatedly low cost dominates design drives personal computers appeared hard drive drives day big expensive customer demand hard drive based personal computer drove development small-sized low cost drive enterprise storage invention disc drives large computer systems time systems tended big expensive employed access large quantities data cost support users simultaneously environment gave rise essential properties drives tend configured groups aggregation opposed drives drive system randomly access small portions large data spaces reliability performance critical characteristics failure idle cost constant pressure reduce drive costs drives complex build due resulting demands encoding schemes error correction servo processing takes considerably logic control basic reading writing areal density improvement requires greater precision lower tolerances noise interference kind component drive complex order deliver state-of-the-art capacity time pushed costly build seek performance improving seek performance continuous struggle head move location faster previous generation product involves expensive components higher performance magnetic circuits faster microprocessors lower-mass actuator assemblies process designing drive involves sophisticated modeling analysis optimize structures seek movements vibrational modes structure negatively affect seek performance fast seeks depend ability rapidly follow servo patterns media predictable design preclude drive seeking throttled obscure resonance head disc assembly ibm rotational latency latency improved spinning media faster drives slower adopt performance improvements introduced drives performance enhancements made incur marginal cost capability drives years practical move models cost penalty development cost eliminated volume market figure shows history rpm adoption mainstream products years fact history illustrates general characteristic relationship drives drives tend drive costly innovation achieving levels performance reliability function drives adopt technology cheap model puts drives difficult pricing position compared drives growth market depends added capabilities innovation drives terms cost savings making rpm motor cheaper building rpm motor time drive cost form higher cost materials larger research development investment aggregation notable difference operating environment drives drives groups simply interface issue electrically interconnect multiple drives property fibre channel scsi serial attached scsi sas efficiently attach drives host drive limit traditional ide controller aggregation drives housed time interactions occur dramatically decrease performance compensation included drive seek simply stay track nearby drives spinning energy transfer rotational vibration seeking drive drives cabinet reliability reliability varies significantly usage patterns operating environment personal computers designed active hours day enterprise systems active hours day day means design choices made drives cost reasons make perform operational stresses designed history interfaces traditionally difference interface based work host drive years ago ide controllers programmed main system processor responsible interactions disc drive interrupts direct memory access dma offload data transfer scsi external control chip drive handled independent operation drive standards group adding command queuing function similar scsi serial ata sata protocol ata historically added major features scsi multiple cpu support failover simultaneous operation variable block size support figure adoption higher rotational speeds data control data product guides seagate product guides year introduction spindle speed rpm rpm ability format drive nonbyte block sector dual porting note type drive functionality accretes drives complexity implementation increases technology differences differences drives far-reaching start earliest design choices diagram figure illustrates basic components modern disc drive section items turn note market disc drives cost-sensitive drive designers spend extra penny material assembly cost target device specifications mechanics basic component choices mechanical portion drive affect reliability seek time acoustics resistance temperature shock vibration environmental variations head disc assembly head disc assembly hda consists base casting heads actuator spindle discs air handling system top cover drive operates higher rpm maintaining higher tolerance external disturbance external disturbances influence neighboring drives rotational vibration environmental factors temperature complicated fact higher rpm faster seeking drives put energy drive cabinet creating disturbance time drives required affected requires rigidity mechanical structure drive mass higher bandwidth servos cases special support circuitry offset effects decimate drive performance higher rpm drives require power operate creating heat affect drive neighbors cabinet achieving million hour mtbf drive easy thing failure mode addressed drives tighter tolerances design rules control externally internally generated particles outgassing rules include things avoiding holes greater environmental control higher quality sealing drive typically environmental protection drive filter particles desiccant control humidity active carbon absorbent eliminating organic substances inside hda spindle motors o-ring seals drive cover gasketing things adds cost improves reliability individually addresses minor failure mode achieve hour mtbf drives designed reliability tend compromise components eliminated save percent users delayed times number delays user delays cpu condor cost o-rings desiccant eliminated drives drive shrouding air control devices manage air flow inside hda eliminates air turbulence make harder 
head track optimize seek performance directs air actuator cool adds cost size stiffness base casting top cover impact acoustic characteristics drives susceptibility rotational vibration problems acute higher spindle speeds actuator larger magnets key achieving faster seek times bring additional requirements higher cost order seek performance stay tight power budget actuator coils resistance requires thicker coil material fewer windings mentioned special hda design features promote cooling actuator prevent overheating interesting complex interactions arises latch inside drive latch hold actuator power common method latching involves magnetic circuit latch magnetic field affect seek performance actuator operating latch drive compensation seek performance critical achieve optimum seek performance drives bi-stable latch affect performance expensive solution performance coil bearing cartridge independently bonded arm special epoxy drive drive coil attached arm single molded connector expensive technique makes rigid structure achieving maximum seek performance figure diagram major components disc drive head disc assembly cover discs actuator e-block flex circuit flex circuit connector power connector printed circuit board base casting spindle read write heads drive seek performance high priority drive typically drive design achieve cost targets seek performance opposite priority holds drives actuator design prevent bending modes resonances impacting seek settle time performance presence rotational vibration spindle years drives spun faster rpm drives sped rpm rpm rpm recently rpm spinning faster tremendous engineering challenge read write head track increasingly difficult rpm off-track head reading mis-read rotational miss requiring full rotation read re-tried off-track head writing mis-write introduces noise overwrites adjacent tracks higher rpm requires expensive motors tracks inch tpi increases motor bigger challenge disturbances windage air movement disk arm vibration increase rpm time drives affected order random performance cost reasons drives cantilever motor design motor shaft captured base deck end motor shaft captured ends attachment top cover today tpi fluid bearing motors preferred minimize runout acoustical noise discussion runout years thought impossible fluid dynamic bearing motor captured ends seagate solved unique conical design drives benefits fluid bearings motor supported ends expensive design performance electronics on-drive electronics integrated improvements processor technology matsumoto means fewer components required provide basic functionality control processor drive servo system read write head track moves track drive determines position reading small fields information interspersed data blocks track servo bursts time head crosses servo burst microprocessor suspends takes task identifying head wandering track slightly move head direction distance back middle track seeks actuator constantly reads servo bursts crosses tracks information determine close actuator target location close decelerate actuator servo processor tpi higher servo processing needed head neighboring tracks hard tracks perfect repeatable circles motor variation platter waviness circumferentially radially stacking tolerances factors give rise repeatable non-repeatable runout runout variation radius circumference track occurs head unable follow current track stay position repeatable runout inherent track rotation making easier compensate non-repeatable runout due external influences vibration varies time servo processor adjust head follow track wandering underneath servo capability higher capacities require servo bursts requires processing works minimizing cost increasing capacity drive constant balancing act minimizing cost including processing power tracking higher tpi achieve maximum capacity interface significantly silicon products comparison study asic gate count drive embedded sram space program code permanent flash memory program code data sram cache sram space complexity scsi interface compared ide ata interface shows due part complex system architectures drives find interfaces support multiple initiators hosts drive track separate sets information host attached maintaining processor pointer sets multiple initiators tagged commands capability scsi efficiently process commands tasks parallel resulted higher overhead kernel structure firmware complexities richer command set result expensive pcb carry electronics drive processor busy servo work read write tasks interface work order drive offer maximum performance equipped processors dedicated servo interface read write handling maximizing random access performance rotational vibration depend dedicated servo processor drive single processor handle basic processor tasks drive run interface support reading writing data servo processing memory firmware scsi command set large ata requiring permanent flash code increased sram runtime complex command set larger command queues require additional memory space scsi command set vendor-specific extensions require additional code space allowing greater flexibility configuration magnetics magnetic componentry similarity drives strive stretch areal density boundary differences stem performance goals drives higher rpm drives delivers higher data rates heads magneto-resistive head technology made profound change data read drive writing inductive process sensitive linear velocity higher rpm improves latency data rate reason drives tend stretch writing capability demand constant innovation high rpm higher areal density drives adopt writer technology proven previous generations drives reading opposite read data rate generally insensitive linear velocity cases adversely affected higher rotational speed signal amplitude increase inductive heads noise means drives higher rpm data rate targets difficult magnetic environment read data key property system read write reliably signal noise ratio snr harder reach snr high rpm drive makes task extracting data read signal significantly difficult referred recording stress pronounced drive drives expensive read write electronics cope difficult magnetic environment higher data rate materials traditional substrate material media aluminum layer magnetic material deposited recent glass substrates greater uniformity magnetic surface greater stiffness ibm magnetic layer harder deposit glass making difficult expensive achieve read densities walker shock tolerance glass traded lower density data rate addition glass textured actuator removed media land load unload ramp landing media contact start stop ibm requires landing zone outer edge disc case contact heads leave platters precisely area highest density data rate recent change media structure anti-ferromagnetically coupled media magnetic layer oriented opposite primary layer reinforce magnetic orientation ibm achieve higher densities cost increased complexity materials manufacturing process walker diagram figure illustrates layering manufacturing build test times drives considerably longer drives increased test time make drive reliable time drives undergo detailed characterization learning precisely irregular individual tracks heads track normal operation time spent analyzing media flaws results lower probabilities flaws causing unrecoverable read errors field performance differences outlined design choices designing disc drive target market choices affect performance attempt quantify impact specific choices capacity basic media structures drive types highest areal density time choice number disc platters size platters capacity figure diagram media layers base substrate consists aluminum glass topped layer magnetic material anti-ferromagnetically coupled 
afc media additional layer magnetic material layer ruthenium added layers reinforcing magnetic stability higher density substrate aluminum glass rpm drives platters support faster spindle speeds rpm drives platters size platters drives spin faster performance power increases cube rpm smaller diameter platters drive power acceptable level cost drive platters achieve capacity drive areal density smaller platters brings performance advantages ability spin faster faster seeking average seek times head traverse smaller recording band greater investment actuator capability discussed earlier makes drives perform random access faster counterparts equivalent areal densities larger diameter platters lower rpm give drive clear advantage delivering capacity consistent primary market requirement lowest cost combination minimizing parts cost delivering highest capacity yields lowest dollar gigabyte data figure compares drive capacity date introduction years number platters drives manufactured fewer platters performance matters capacity chart figure illustrates trend depopulated drives recent depopulated heads platter surface save cost additional read write head fewer platters translates faster seeks heads actuator lower total mass move fraction millisecond faster significant subms average seek times matches marketplace users requirement performance buy drives lower capacity spread data actuators data rate fastest drives higher data rate contemporary drive naneseconds link percent non-local edges performance simple bundle unoptimized bundle optimized put bulk due large part higher rpm explained earlier drive advantage media size typically drives platters compared rpm rpm drives larger media size helps drives follow closely data rate factor favoring drive models tend frequently drives introduction drive generation double capacity previous generation successive models years drives hand deliver appreciable increase capacity doubling introduced platter higher frequency enables drives stay closer drives data rate jumps happen drives data table compares data rates drives shows underlying spindle speed areal density platter size figure comparison capacities capacity introduction date years seagate drives seagate maysep- janjun- octmar- juldec- aprsep- date introduction drive capacity capacity capacity figure trend depopulated drives users choosing drives platters trading capacity performance calendar year fraction drives sold platters platters platters platters heads single platter single head cap speed density dia int ext rpm calc spec atlas wls deskstar cheetah cheetah cheetah table comparison drives increasing data rates capacities speeds densities published spec sheets diameters typical spindle speeds internal bandwidths calculated speed diameter tpi shown spec sheets external bandwidths measured linuxhardware augustus table shows components sequential data rate rotational speed areal density diameter platters higher speed drives smaller platters lower energy consumption faster seeks resulting lower data rates drives table arranged order externally-measured sequential throughput rpm deskstar faster rpm atlas due higher areal density larger platter diameter deskstar cheetah close data rate increased rpm cheetah overcome density disadvantage smaller platter diameter cheetah rpm gains data rate loses due reduced platter diameter finally generation cheetah increases areal density outperforms smallest diameter platter diagram figure illustrates trade-off data rate capacity seek time power consumption choosing platter size random performance random performance describes ability drive location unpredicted address service request components performance movement seek performance controller overhead rotational latency seek times mechanical items mentioned section directly affect ability drive seek quickly stay servo track response environmental factors data figure compares seek time drives date introduction seek performance drives lags drives improves slower rate drives expected squeeze gain generation drives entire mechanical design drive focused achieving highest random access performance critical target market table shows seek performance drive workload comparison barracuda cheetah drive system table details experimental setup mechanical details drives close higher density barracuda compensates higher spindle speed cheetah higher spindle speed account improvement random performance drive figure diagram basic drive parameters smaller media lower sacrifices bandwidth capacity shorter seeks lower power additional platter adds capacity power consumption seek dis cetan data rate rate density rpm power power rpm drag drag platter spindle capacity cap density figure comparison seek times seagate maysep- janjun- octmar- juldec- aprsep- data introduction aver age seek seek seek queue depth read write requests req req req req requests req req req req requests req req req req requests req req req req requests req req req req requests req req req req table comparison random request rates increasing queue depth request stream drives drives run write caches enabled write cache drive disabled improvement larger queue depth larger observed previous study white server dell poweredge operating system windows pro scsi controller adaptec ata controller promise ultra drive seagate cheetah specs rpm seek drive seagate barracuda specs rpm seek benchmark iometer jan code release table experimental drive testbed seek scheduling queue depths seek sorting impacts performance drives generally shorter queues fact shorter queue lengths ata interface direct impact drive mechanics fact seeks aggressively scheduled drives average seek distance closer theoretical average disc radius drives aggressive scheduling bring low radius average means mechanical duty cycle total amount time spent seeking stressing mechanical components drives higher similar request stream data table compares random performance drive drive queue depth drive increases queue pending requests drive achieve random performance single queued request drive improves throughput reads barely writes similar results earlier study white performance increase performance improvement seeks smaller leads performance reliability additional scheduling sophistication included drive require additional electronics discussed earlier controller overhead controller overhead optimized processor performance interpret schedule commands arrive recently augmented custom hardware assist provide performance economically realized simply greater investments software hardware ensures data moved interface rates close internal drive data rate rotational vibration drive seek simply stay track nearby drives spinning energy transfer seeking drive drives cabinet excite drives rotate center mass throwing actuator track drive designed mitigate effect writes abort seeks fail find desired track cases manifest decrease performance aborted writes rotational misses accumulate extreme effect bad drive cease function simply stay track long complete operation key understand rotational vibration present server environment design drive withstand drives built single drive systems rotational vibration important factor cd-rom drive create amount vibration slight infrequent effect sufficient produce noticeable performance problem responsiveness measured single user retries create problem cases drives hand explicitly designed operate cabinets full spinning drives requires designing drive maintain operation presence considerable rotational vibration tracks inch tpi increases rotational vibration problem worse difficult stay track ideal conditions external vibrations difficult compensate abramovitch recent drives added rotational vibration sensor detect external 
rotation compensate servo processing earlier mentioned performance degradation due rotational vibration attempt quantify chart figure shows performance single drive test stand varying rotational vibration performance drive affected drive drive essentially stops radians external vibration drive degrades smoothly operate radians multiple drives cabinet rotation induced adjacent discs system components affects performance design cabinet mountings determines bad effect system studies drive enclosures machine designs variety manufacturers show wide range vibration characteristics designs subject drives radians minor performance consequences cabinets inducing radians hall reliability trickiest drive characteristics measure reliability arises wide range factors considfigure externally applied rotational vibration major negative impact performance individual drive cabinets vary widely amount rotational vibration transfer measured rad hall data seagate cheetah barracuda iii applied rotational vibration rad operat ions ond ops iops iops erations design manufacturing operational environment kaczeus yang elerath significant difference reliability specification drives expected power-on hours poh drive type mtbf calculation assumes poh hours day days year specification assumes hours day days year longer drive expected running lower mtbf higher annual failure rate afr chart figure shows expected increase afr due higher power-on-hours moving drive expected poh year poh year increase failure rate two-fold compensation design duty cycle addition obvious increase increased power-on hours amount mechanical work drive affected basic structure workload asked larger number platters drive increases capacity increases mechanical stresses chart figure shows increase expected afr higher duty cycle increase higher drive larger number platters platter disk duty cycle reduce failure rate seek scheduling leads shorter seeks average lower effective duty cycle set user requests preliminary measurements testbed mechanical duty cycle approximately drive drive set requests adding platters heads increases afr due additional mechanical stresses due increased internal heat generation additional head disc interfaces release particles lead negative interactions head crashes temperature reliability decreases increases ambient temperature drive temperature affected temperature components system high-density server rack disc drives grouped close experience higher temperatures single drive mounted desktop computer chart figure shows increased afr increased temperature fifteen degree temperature rise expected increase failure rate factor increase size common assumption high-density server racks patel order prevent data corruption failure elevated temperature drives temperature sensors provide warnings temperature specification range herbst reliability factors makes individual contribution drive failure rate magnify capacity-focussed drive platters sophisticated seek scheduling higher base duty cycle workloads subject temperature variation specification seagate manufacturers varying similar assumptions power-on hours vilsbeck figure reliability reduction increased power hours ranging hours day operation cole yearly power-on-hours poh afr multiplier figure reliability decreased higher duty cycle effect greater drives larger numbers platters cole duty cycle mul single platter platters figure reliability decrease due ambient temperature variation cole temperature afr multiplier past work comparing reliability drives reported failure rate ide drives scsi drives month period talagala numbers treated controlled study due small sample size drives study data collected design phase drives reported scsi ata drives shows annual failure rate larger hughes shows design choices significant impact final drive failure rates related work previous comparison scsi ide white concluded ide slightly sequential performance lagged significantly random performance authors study compare mechanical details drives leading conclusion usenix association proceedings usenix conference file storage technologies san francisco usa march april usenix associationall rights reservedfor information generalized scsi ide drives data table compares set basic characteristics drives considered study slight advantage ata drive sequential performance due density advantage ata drive higher larger platter diameter larger overcome rotational speed advantage scsi drive higher scsi drive comparable density perform significantly discussed section data table shows improvement generation drives manufacturer newer scsi drive comparing ultrastar deskstar areal density rotational speed advantage smaller diameter platters push scsi drive higher data rates higher advantage scsi drive ata drive random performance partly due smaller platters additional differences mechanics explained earlier sections note generations scsi drives seek performance improved seek performance ata drives remained constant performance comparison windows chung shows ide drive slower scsi drive sequential throughput slower random performance shown table performance difference due mechanical differences higher density larger platters ide drive compensate faster spindle speed scsi drive seeks latencies significantly lower higher rpm drive drives introduced density higher rpm drive larger sequential throughput advantage comparison scsi ata end users dominguez makes high-level points disiface cap price speed seek density kbpi ktpi internal dia ext dsks cap spec raw ultrastar lzx scsi rpm deskstar ata rpm table comparison drives ibm white deskstar drive slight advantage sequential bandwidth ultrastar higher rpm authors previous study attribute overhead scsi interface fact closer physical discs shows explanation smaller platter size ultrastar normal reduces seek time expense lower sequential bandwidth outer tracks ultrastar lower areal density make capacity difference additional platters estimated based internal transfer rate raw capacity differences iface cap price speed seek density kbpi ktpi int dia ext disks cap calc spec raw ultrastar scsi rpm deskstar ata rpm table comparison newer generation drives ibm case ultrastar increases sequential performance deskstar due higher spindle speed areal density lower harddrive august published specification measured number version deskstar disks heads surface remains unused cap seek speed density dia int ext rpm calc spec fireball lct atlas scsi table comparison ata scsi windows chung cussed ata drives optimized simplicity low cost scsi drives optimized performance reliability ability connect multiple hosts trends speed sophistication interfaces bringing ata scsi closer ata gaining complexity moves closer scsi comparison ata scsi reliability end user perspective covering factors mentioned discussed extensively recent online article vilsbeck trends recent innovation disc drive technology details specific scsi drive design recently published disc drive maker miura aruga design disc drives complex multi-faceted process students understand engineering cost trade-offs richkus summary discussion compare individual drive models capacity point detailed device specifications impact aspect drive design determine drive performance factors turn comparing impacts capacity markets highest affordable density determined largely areal density trends variation numbers platters drive build drive chosen capacity market data rate proportional spindle speed areal density platter size data rate enterprise market higher personal storage higher spindle speeds cost interface fast seeks cost target enterprise market includes larger magnets bearings stiffer actuators challenge rapidly find target track seek stay track servo spite harsh electrical magnetic environment protection rotational vibration costs extra targets markets multiple drives sit includes motors top 
covers stiffer actuators additional mass scheduling costs extra requiring code space memory re-order queues algorithms easier scsi interface traditionally queueing mature implementation complexity exist interface fancier interface electronics cost extra scsi richer complex customer-modifiable options host connectivity takes electronics memory space difference arises solely choice interface finally high reliability costs extra considered component material choice design account duty cycle targets expected workload expected environment conclusions differences enterprise personal storage disc drives significant derive requirements respective markets offer range choices system designers simply separating products external interface ata scsi misses internal details design choices affect system performance shown external interface chosen smallest contributors performance performance reliability characteristics drive determined drive designed smallest mechanical materials choices head-disc assembly seek scheduling algorithms interface processing order find features design points application underlying trade-offs account continuum specific choices acknowledgements albertine flora performance testing benchmarking details drive internals reported due discussions zip cotter neal gunderson jim weispfenning kevin gomez mark lutwyche explaining terminology analysis comparison builds previous work led heath miller skalko contribution anonymous reviewers shepherd john wilkes detailed helpful comments abramovitch abramovitch rejecting rotational disturbances small disk drives rotational accelerometers ifac world congress july aruga aruga -inch high-performance disk drives enterprise applications alseries fujitsu scientific technical journal december augustus augustus seagate cheetah review linuxhardware features shtml linuxhardware september blount blount fluid dynamic bearing spindle motors ibm storage systems group san jose february chung chung gray worthington horst windows disk performance technical report msr-tr- microsoft research june cnet cnet hardware reviews cole cole estimating drive reliability desktop computers consumer electronics systems technology paper tpseagate technology november dominguez dominguez coligan scsi ata interface comparison technology dell computer december elerath elerath reliability disk drive industry mtbf ieee annual reliability maintainability symposium january hall hall seagate advanced multidrive system sams rotational vibration feature technology paper tpd seagate technology february herbst herbst ibm drive temperature indicator processor drive-tip helps ensure high drive reliability white paper international business machines corp october hughes hughes murray kreutz-delgado elkan improved disk drive failure warnings ieee transactions reliability september ibm ibm ibm hard disk drive load unload technology ibm storage systems division ibm ibm higher reliability ibm glass substrate disks ibm storage systems division july ibm ibm advanced servo-mechanical design facilitates improved performance reliability ibm storage systems division november ibm ibm research news ibm magnetic hard-disk-drive media delays superparamagnetic effects research ibm resources news whitepaper shtml february kaczeus kaczeus disk reliability function design manufacture computer technology review summer mason mason scsi industry workhorse working hard ieee computer december matsumoto matsumoto diskcon abuzz single-chip drives home markets times eetimes sys news oeg september miura miura information storage broadband network era fujitsu challenge hard disk drive technology fujitsu scientific technical journal december patel patel bash belady stahl sullivan computational fluid dynamics modeling high compute density data centers assure system inlet air specifications asme international electronic packaging technical conference exhibition ipack july reinsel reinsel gigabytes drive seagate barracuda idc bulletin november richkus richkus agogino tang virtual disk drive design game links math physics dissection activities asee ieee frontiers education conference san juan puerto rico november online bits berkeley mmcs disk disk html seagate seagate disk drive acoustics technology paper tpseagate technology april seagate seagate support disc drive encyclopedia seagate support disc seagate technology august talagala talagala patterson analysis error behavior large storage system technical report ucb csd- california berkeley february vilsbeck vilsbeck gefahr ide-festplatten dauereinsatz tecchannel hardware index html tecchannel june walker walker performance media tweaking magnetic capabilities technology paper tpseagate technologies august white white hillyer performance comparison ide scsi disks technical report bell labs lucent technologies january worthington worthington ganger patt wilkes on-line extraction scsi disk drive parameters sigmetrics yang yang sun comprehensive review hard-disk drive reliability ieee annual reliability maintainability symposium january appendix data table shows multiple generations drives manufacturers including ata scsi interfaces numbers serve comparisons made paper detailed data provided time drives replace discussed seconds amount text memory mentioned replaced previous cost studies replacing memory recently released dec drives iface ultrix intro sun cap ipx price speed seek sunos density kbpi ktpi dia int ext disks cache rpm calc spec quantum atlas scsi maxtor fireball lct ata ibm ultrastar lzx scsi seagate cheetah scsi quantum atlas scsi ibm ultrastar scsi ibm deskstar gxp ata ibm ultrastar lxz scsi seagate barracuda scsi fujitsu allx scsi seagate cheetah scsi seagate cheetah scsi fujitsu alle scsi maxtor diamondmax ata ibm deskstar gxp ata ibm deskstar gxp ata seagate barracuda ata seagate cheetah scsi seagate cheetah scsi western digital caviar ata seagate barracuda ata western digital caviar ata seagate barracuda ata table comparison multiple drive generations manufacturers numbers manufacturer specifications product manuals noted prices drives sold august dirtcheapdrives seek times average seek values density bandwidth maximums outer diameter internal bandwidth calculated rpm kbpi disc diameter values provided comparison published values version deskstar disks heads side remains unused measured linuxhardware augustus measured bell labs white measured windows chung published specifications measured numbers measured cnet hardware cnet option default cache size 
usenix association office usenix orgwww http usenix rights individual papers remain author author employer permission granted noncommercial reproduction work educational research purposes copyright notice included reproduced paper usenix acknowledges trademarks row-diagonal parity double disk failure correction peter corbett bob english atul goel tomislav grcanac steven kleiman james leong sunitha sankar network appliance abstract row-diagonal parity rdp algorithm protecting double disk failures stores data unencoded exclusive-or operations compute parity rdp provably optimal computational complexity construction reconstruction algorithms optimal amount redundant information stored accessed rdp works single stripe blocks sizes file systems databases disk arrays utilized fixed raidor rotated raidparity placement style extend algorithm encompass multiple raidor raiddisk arrays single rdp disk array add disks existing rdp array recalculating parity moving data implementation results show rdp performance made equal single parity raidand raidperformance introduction disk striping techniques decades reduce data loss due disk failure improving performance commonly raid techniques raidand raidprotect single disk failure standard raid techniques mirrored stripes raidraid- provide protection multiple failures protect double disk failures opposing disks mirror mirrored raidand raidprotect higher order failures efficiency array measured data capacity divided total disk space reduced increasing redundancy small increments stripe cost effective adding redundancy replicating entire array dramatic increase disk sizes slower growth disk bandwidth construction disk arrays larger numbers disks reliable performant varieties disk ata combines increase rate double disk failures discussed section requires algorithms protect double disk failures ensure adequate data integrity algorithms meet information theory singleton bound protect disk failures adding disks redundancy number disks required store unprotected data good algorithms meet bound store data unencoded read directly disk multiple orders magnitude improvement reliability storage system simplify design parts system robustness improving system reliability motivates data protection algorithm protects double disk failures time desirable maintain simplicity performance raidand raidsingle parity protection paper describes algorithm called row-diagonal parity rdp protection double failures rdp applies multiple device storage system communication systems paper focus application rdp disk array storage systems raid rdp optimal computation stores user data clear requires parity disks utilizes exclusive-or operations parity construction reconstruction failures implemented easily dedicated hardware standard microprocessors simple implement compared previous algorithms difficult measure benefit implement algorithm integrate existing raid framework short product development cycle paper make case double disk failure protection increasing describe rdp algorithm proving correctness analysing performance present simple extensions algorithm showing add disks existing array protect multiple raidor raidarrays double failures single extra parity disk finally present observations experience implementing rdp give performance results implementation related work algorithms protect data disk failures array disks evenodd reed solomon erasure codes datum rdp similar evenodd distributes parity disks single stripe equivalently adds stripes parity data interspersed data stripes evenodd datum reedsolomon share property redundant information stored separately data stripe implementations dedicated redundant disks leaving disks hold data analogous raidal- parity disks call raidstyle parity placement alternatively placement redundant information rotated stripe stripe improving read write performance call raidstyle parity placement evenodd reed-solomon encoding compute normal row parity parity disk employ techniques encoding disk redundant data exclusive-or operations reed-solomon encoding computationally intensive evenodd datum encodings generate number redundant information blocks higher order failure tolerance similar reed-solomon encoding case protection disk failures rdp shares properties evenodd datum reed-solomon encoding stores redundant data parity separately disks data stored clear disks previously reported algorithms evenodd lowest computational cost protection disk failures rdp improves evenodd reducing computational complexity complexity rdp provably optimal construction reconstruction optimality construction important normal failure free operational mode optimality reconstruction important maximizes array performance degraded failure conditions double disk failure modes analysis double disk failures result combination types single disk failure individual disks fail wholedisk failure data disk temporarily permanently inaccessible media failure small portion data disk temporarily permanently inaccessible wholedisk failures result problem disk channel network connecting disk system mode duration failures vary class failures make data disk unaccessible categorized failure type purposes recovery whole-disk failures require complete reconstruction lost disk portions wanted data stresses system controller adding cpu load refer unit performs construction parity reconstruction data parity controller maintain uninterrupted service controller serve requests lost disk reconstructing requested data demand time reconstruct lost data desirable reconstruction low response time on-demand reconstruction individual blocks required service reads time exhibiting high throughput total disk reconstruction whole-disk failure rates measured arrival rate usage pattern disk assumption disk bad time failure noticed disk failure rates reciprocal time failure numbers quoted manufacturers typically range hours media failures qualitatively quantitatively whole-disk failures media failures encountered disk reads writes media failures write handled immediately disk controller relocating bad block good area disk media failures read result data loss media failure affects small amount data loss single sector critical data compromise entire system handling media failures read requires short duration recovery small amount missing data emphasis recovery phase response time reconstruction throughput generally issue disks protect media errors relocating bad blocks undergoing elaborate retry sequences extract data sector difficult read precautions typical media error rate disks manufacturers bit error bits read corresponds approximately uncorrectable error tbytes tbytes transferred actual rate depends disk construction static dynamic aspect rate represents rate unreadable sectors encountered normal read activity sectors degrade time writable readable state unreadable state failure occur reconstruction single whole-disk failure point array degraded mode reads blocks failed disk satisfied reconstructing data surviving disks commonly contents failed disk reconstructed spare space disks protect disk failure complete disk failure make reconstruction portion lost disks impossible portion failed disk reconstructed media failure reconstruction make reconstruction missing sectors blocks stripe impossible process reconstruction requires surviving disks read entirety stresses array exposing latent media failures surviving disks double disk failure combinations whole-disk whole-disk wholedisk media media media properly implemented double failure protection algorithm protects categories double failures analysis failure rates discount media media failures rare relative double failure modes whole-disk whole-disk whole-disk media failures encountered reconstruction identified whole-disk failure raid systems protect double failures due media failures periodically scrubbing disks read sector reconstructing relocating data sector unreadable single whole-disk failure occurs preempt potential whole-disk media failures 
cleansing disks accumulated media errors whole-disk failure occurs preventive techniques precaution arrays current large capacity disks media whole-disk failure rates assume uniform failure arrivals lifetime disk uniform failure arrival rates population similar disks actual whole-disk failure rates conform bathtub curve function disk service time higher failure rate encountered beginning-of-life burn-in end-of-life wear-out periods higher rate periods affect double disk failure rate disks array typically age subject usage pattern increase correlation whole-disk failures disks array disks array manufacturing batch subject variations manufacturing increase likelihood individual disk failing disks array subject temperature humidity mechanical vibration conditions subjected mechanical shocks transport result clustering failures increases double failure rate expected individual disk failures uncorrelated single disk fails period vulnerability whole-disk failure determined reconstruction time contrast vulnerability media failure fixed disk fails reconstruction require complete read surviving disks probability encountering media failure scans largely independent time reconstruction failures independent wide sense stationary derive rate occurance whole-disk failures trcn reconstruction time failed disk total number disks array whole-disk failure rate disk term reflecting correlation disk failures whole-disk failures correlated correction factor experience wholedisk failures stationary depend service time disk positively correlated factors increase rate consideration reconstruction time slowdown social contract social contract workstations workstations workstations function total data processed reconstruction linearly related disk size related number disks total data processed product size disks small bandwidths individual disks dominate reconstruction time large aggregate bandwidth disks great saturate processing capacity controller performing reconstruction assert braceleftbigg ceilingleftbiggb ceilingrightbigg maximum rate reconstruction failed disk governed disk write bandwidth maximum rate reconstruction disk array result disk arrays larger whole-disk whole-disk failure rate cubic dependency number disks array linear dependency size disks double failure rate related square whole-disk failure rate employ disks higher failure rates ata drives expect double failure rate increase proportionally square increase single disk failure rate primary failure rate hours correlation factor reconstruction rate ten disk array gbyte disks whole-disk whole-disk failure rate approximately failures hour size disks bandwidth increasing trend years disk size increasing faster disk media rate time takes read write entire disk lower bound disk recovery result recovery time disk increasing aggravating double disk failure rate rate whole-disk media failures related disk size number disks array essentially rate single whole-disk failures multiplied probability failures result double failure due inability read sectors surviving disks single whole-disk failure rate proportional number disks array media failure rate roughly proportional total number bits surviving disks array probability bits readable probability individual bit unreadable number bits read priori rate whole-disk media double failures size disk measured bits primary failure rate hours disk array disks bit error rate whole-disk media double failure rate failures hour typical numbers rate whole-disk media failures dominates rate whole-disk whole-disk failures incidence media failures wholedisk failure uncomfortably high scrubbing disks reduce rate remains significant source double disk failures combination double failure rates time data loss mttdl hours converts annual rate data loss events disk array year due double failures type compare dominant triple failure mode media failures discovered recovery double whole-disk failures rate approximated analog equation substituting equation dominant component tertiary failure rate approximately failures hour reduction orders magnitude compared double failure rate expensive disks ata disks arrays high data integrity required increasing disks performant reliable scsi fcp disks increases reconstruction time individual disk failure rates turn increasing double failure rate arrays size row-diagonal parity algorithm rdp algorithm based simple parity encoding scheme exclusiveor operations data block belongs row parity set diagonal parity set normal configuration row parity block diagonal parity block stripe build raidor raidstyle arrays rdp locating parity blocks disks rotating parity disk disk stripes rdp array defined controlling parameter prime number greater simplest construction rdp array disks data data data data row diag disk disk disk disk parity parity figure diagonal parity set assignments disk rdp array define stripes array consist block disk stripe block holds diagonal parity block holds row parity blocks hold data bulk remainder paper describes grouping stripes includes complete set row diagonal parity sets multiple stripe groupings concatenated form raidstyle raidstyle array extension multiple row parity sets discussed section figure shows stripes disk rdp array number block diagonal parity set block belongs row parity block parity data blocks row including diagonal parity block diagonal parity block parity data row parity blocks diagonal note diagonals store parity diagonals selection diagonals store parity completely arbitrary refer diagonal store parity missing diagonal paper select diagonal missing diagonal store parity missing diagonal compute operation algorithm assume data disks failed array figure reconstruct remaining data parity disks row parity useless step lost members row parity set diagonal misses disk diagonals miss disk diagonal parity sets missing block diagonal parity sets stored parity block missing block diagonal parity sets reconstruct missing blocks reconstructed blocks row parity reconstruct missing blocks rows reconstructed diagonal blocks block diagonal data disk block diagonal data disk blocks turn diagonals diagonals diagonal reconstruction compute store parity diagonal diagonal reconstruct block diagonal data disk step reconstruct block diagonal data disk row parity block diagonal data disk finally block diagonal data disk row parity important observation compute parity diagonal require parity diagonal complete reconstruction missing blocks turns true pairs failed disks parity missing diagonal complete reconstruction safely ignore diagonal parity construction proof correctness formalize construction array construct array disks divided blocks prime number greater group blocks position device stripe groups stripes group stripes assign blocks diagonal parity sets disks numbered blocks numbered disk disk block belongs diagonal parity set mod disk special diagonal parity disk construct row parity sets disks involving disk lost block disks reconstructed row parity normal ensure store single row parity block blocks stripe loss generality disk store row parity key observation diagonal parity disk store diagonal parity diagonals array rows store diagonal parity blocks group stripes select diagonal parity blocks leave loss generality choose store parity diagonal parity set conform numbering scheme roles disks diagonal parity disk mathematically identical contribute symmetrically diagonal parity disk contribute make row parity sums stripe nondiagonal parity disks row parity require reconstruct lost block stripe diagonal parity block row parity diagonal parity block start proof correctness rdp algorithm lemma lemma sequence numbers mod 
prime endpoints equal numbers occur sequence proof number sequence definition number sequence mod mod lemma true endpoints subsequence numbers begins numbers values modulus operation repeating number sequence true mod mod means divisible prime multiple factors equal numbers sequence beginning unique numbers represented number sequence diamondmath complete proof correctness rdp theorem array constructed formal description rdp reconstructed loss disks proof classes double failures include diagonal parity disk failures include diagonal parity disk disk failed row parity section array disk reconstructed row parity row parity sets involve diagonal parity disk completion reconstruction failed disks row parity diagonal parity disk reconstructed definition diagonal parity sets leaves failures disks diagonal parity disk construction array disk intersects diagonals diagonal mod mod disk misses diagonal combination failed disks diagonals intersected disks mod mod substituting mod diagonals missing member stored diagonal parity diagonal reconstruct missing element diagonal diagonals diagonal reconstruct block missing disks diagonal parity step reconstruction failed disks reconstruct block diagonal parity diagonal parity set disk reconstruct block disk diagonal parity set mod row parity similarly reconstruct block diagonal parity disk reconstruct block disk diagonal parity set mod row parity pair diagonals potentially reconstructable failure disks reconstructable reconstruct blocks diagonal mod mod alternating row parity diagonal parity reconstructions similarly reconstructable reconstruct blocks diagonal mod mod alternating row parity diagonal parity reconstructions adjacent points sequence generated lemma reach diagonals reconstruction missing block diagonal parity set block reconstructed row parity end reconstruction chain beginning negationslash negationslash reconstruction proceeds reaching missing blocks diagonal end chain blocks reconstructed row parity diagonals reached reconstruction missing blocks diagonal reconstructed diamondmath store generate parity diagonal complete reconstruction performance analysis performance disk arrays function disk cpu memory bandwidth required construct parity normal operation reconstruct lost data parity failures section analyse rdp terms efficiency compute efficiency rdp stores data clear read performance unaffected algorithm extent disk reads writes data writes interfere data read traffic write case rdp stripes contained single stripe disk blocks section implementation optimizes write preserves property stripe disk blocks written independently stripes data writes require writing parity blocks stripe full stripe writes cost additional disk compared full stripe writes single disk parity arrays partial stripe writes computed addition recomputing parity entire stripe subtraction computing delta parity blocks change data blocks written depending number blocks written stripe writes subtraction method commonly referred small writes writing disk blocks subtraction method requires reads writes addition method requires reads writes write disk blocks reads writes cost addition method requires number disks array subtraction method requires breakpoint addition subtraction method number disk rdp minimal double failure protection algorithm writing data block requires updating parity blocks data block contribute parity blocks determine computational cost rdp total number exclusive xor operations needed construct parity data block contributes row parity block array size rows disks data blocks row xor operations required reduce blocks parity block row parity requires xors compute diagonal parity blocks diagonal total data row parity blocks requiring xors reduce diagonal parity block diagonal parity construction requires number xors row parity construction total number xors required construction theorem array data disks ratio xors block minimum number xors provide protection failures proof assume construct parity disk array groups rows minimum parity blocks row data blocks singleton bound data block contribute parity blocks parity disk ensure recover data block parity block lost pair data blocks contributes parity blocks additional information losing data blocks make parity sets contribute ambiguous construct parity blocks equations minimal formulation common pairs data blocks common subterms equations minimal formulation minimum number separately xored input terms required construct parity blocks set equations reduces terms results xors requires xors minimum number xors data block achieve double parity protection diamondmath rdp protects data blocks xors setting xors protect data blocks meets optimal ratio data rdp evenodd difference disks table row xor counts parity construction compare rdp computationally efficient previously algorithm evenodd array data disks data blocks evenodd requires xors compute row parity xors compute parity diagonals evenodd requires xors add parity distinguished diagonal parity diagonals complete calculation stored diagonal parity results total xors construct parity evenodd block array evenodd requires xors block algorithms asymptotic cost xors block difference computational cost significant small values typical disk arrays shown table ignoring fact algorithms function correctly array sizes rdp computational cost reconstruction failure optimal reconstruction single disk failure requires xors lost row parity sets diagonal parity sets size reconstruct lost block xoring surviving blocks xor operations setting recovering blocks xors xors parity block shown minimum number xors construction array double protects parity fewer operations result evenodd paper overcounts number xors required compute parity diagonal data rdp evenodd difference disks table row xor counts data reconstruction parity sets size cost repair lost disk minimum make individual parity sets smaller protect double failures reconstructing block parity set true disk lose reconstructing double failure includes diagonal parity disk cost parity construction reconstruct lost data row parity disk row parity reconstruct diagonal parity disk reconstructing data disks row parity cost contructing row parity cost reconstructing combination data row parity disks determined reconstruct blocks parity set size cost reconstruct block xors computational cost construction reconstruction cases xors optimal comparing evenodd data reconstruction algorithm evenodd paper advantage rdp shown table numbers disks typical disk arrays performance rdp evenodd construction reconstruction significantly lower compute cost reed-solomon coding rdp optimal compute efficiency efficiency construction normal operation reconstruction failure algorithm extensions single stripe implementation selecting primes meets condition define diagonal parity sets group stripes define block size rdp purposes usual system block size divided disk block sizes powers define self-contained rdp parity set single stripe blocks system disk block size select giving rdp blocks stripe rdp block bytes construct array existing software techniques reading writing single stripe adding disk diagonal parity multiple row parity groups rdp requires recover single lost block stripe row parity case block diagonal parity disk raidand raidstyle configurations row parity set stripe row parity block means portion array include diagonal disk single disk reconstruction technique including concatenation raidor raidarray declustered parity techniques define diagonal parity sets disks construct diagonal parity sets individual blocks stored parity data blocks 
cost-performance tradeoff minimizing number parity disks making reconstruction single failures faster protecting double failures wider array array double failures affect disks row parity sets repaired directly row parity technique expanding diagonal parity cover row parity group imagine rdp arrays file system block size necessarily xor diagonal parity blocks single diagonal parity block store single diagonal parity disk combination diagonal parity disks constituent arrays storing sufficient reconstruction double disk loss array failures occur subarrays recovered local row parity failures occur subarray recovered diagonal parity diagonal parity block subarray reconstructed constructing diagonal parity intact subarrays subtracting stored merged diagonal parity disk reconstructed needed diagonal parity contents normal rdp reconstruction rebuild lost blocks subarray reconstructing expandable arrays discussion implied number disks array fixed selection case underpopulate rdp array putting fewer data disks maximum allowed array simply sets maximum array size underpopulate array taking advantage fact fewer data disks fill remainder array unused disks zeros zero-filled disks contribute parity blocks change contents parity block remove array imputing zero-filled contents parity calculations expand array adding zero-filled disk adjusting parity write data disk reasoning allowable disks sizes array diagonal parity disk largest disks array rows row parity block contributions smaller disks stripes blocks disks counted zeros selecting rdp array grow data disks sufficiently large number accomodate expected disk array size implementation experience rdp implemented feature network appliance data storage system software data ontap version data ontap complete software system including operating system kernel networking storage file system file system protocols raid code raid layer manages final layout data parity raid groups volume consists raid groups raid group independently recoverable section observations made improved implementation algorithm parity construction subblocks data disk contribute row parity subblock diagonal parity subblock note contributions subblocks data disk diagonal parity disk ordered sequence perform parity construction memory efficient manner modern microprocessors process data block pass xoring contents row parity diagonal parity destination blocks properly tuning code microprocessor work blocks top level cpu cache work data block time incrementally constructing target parity blocks remain cache latency memory operations budget completing xor operations bit field pentium optimize ensuring data bytes xored destinations loaded processor registers implementation instructions cache line load complete xors data integrity checksum calculation data cache line significant loss performance overlap cpu execution memory operations greatly reduced effective cost computing redundant parity block rdp observation protected array double failures remaining vulnerability triple higher order failures whole-disk media failures corrected encountered resorting rdp reconstruction stripes missing blocks remainder missing disk reconstructed row parity diagonal parity disk case whole-disk whole-disk failures reconstruction disk fail typically underway disk fails implementation reconstruction single disk failure starts block number proceeds sequentially disk block failure occurs stripes array missing block completed reconstruction single failure stripes run rdp reconstruction remaining stripes array stripes missing blocks reconstructed missing block reducing window vulnerability disk failure combinations disk failures handled including involving reconstructing disks existing raidand raidarrays easily upgraded rdp constructing diagonal parity disk code reconstructing single diagonal disk failures downgrading rdp single parity protection scheme simple removing diagonal disk measured performance data ontap version runs variety hardware platforms current highest performing platform fas includes ghz intel pentium cpus file server filer time full cpu running raid code including xor calculations rdp ran performance benchmarks implementation rdp data ontap set experiments xortest synthetic benchmark testing raid xor checksum code checksum modified adler checksum bits wide computed input block input stripe block deep blocks wide blocks rdp prime divide block sixteen byte subblocks rdp calculations xortest experiment run cold caches random data generated memory disk access test simply test memory cpu processing speed ran sets tests computed parity input blocks computed checksum input block output parity blocks computed checksums parity set tests computed single parity raiddouble parity rdp performed rdp reconstruction randomly selected missing data blocks repeated experiment times results eliminate effects activity operating system generally results repeatable bad outliers represent experiments interfered activity affecting processor cache figures present results experiments note graphs linear small offset due fixed overhead algorithm case single parity calculation raidis fastest table shows measured calculation rates operations note rdp reconstruction rate close rdp construction rate difference timings due primarily completion step reconstruction requires series operations byte rdp blocks step required number blocks stripe block computations rdp construction reconstruction basically implementation reconstruction completion step accounted overhead operation determined time hypothetical calculation stripe data blocks overhead rdp reconstruction significantly higher due completion step cases table construction reconstruction rates close obtained raidcon- number disks raidconstruction diamondmath diamondmath diamondmath diamondmath diamondmath diamondmath diamondmath diamondmath rdp construction rdp reconstruction figure xortest raidand rdp performance checksum computations block disk struction raidconstruction reconstruction identical computations difference rates reflects inability completely overlap computation cache loads stores main memory theoretical memory bandwidth processor achieving percent rate stalling cache line loads saturating processor calculation instruction counts cache line consuming processing budget cache line checksum cases aggwrite test filer aggregate write performance workload supplied array nfs clients performus number disks raidconstruction diamondmath diamondmath diamondmath diamondmath diamondmath diamondmath diamondmath diamondmath rdp construction rdp reconstruction figure xortest raidand rdp performance checksum computations block disk rate overhead raidchecksum rdp cons rdp recons raidchecksum rdp cons rdp recons table xortest derived results algorithm config rate raidrdp rdp rdp table aggwrite results ing write operations tests performed fas dual ghz intel pentium processors filer runs entire data path network nfs protocol file system raid storage compared raidwith configurations rdp data disks case measured achievable write bandwidth table aggwrite results configuration column table presents number separate raid groups connected 
filer number data disks raid group number parity disks raid group wafl file system uniformly distributes writes data disks table cases rdp performance percent raidwith rdp increase size raid groups realizing increase data protection achieving comparable write performance rdp raid groups disks data parity achieve performance equivalent raidwith total number data parity disks improved data protection conclusions rdp optimally efficient double disk failure protection algorithm combination single whole-disk failure media failures troublesome disks large relative expected bit error rate utilizing rdp significantly reduce data loss due types double failures fact rdp optimal computational complexity proved valuable 
achieving performance close single parity raidimplementation simplicity flexibility rdp allowed implement existing raid framework interesting open problem algorithm extended cover concurrent failures acknowledgements authors loellyn cassell ratnesh gupta sharad gupta sanjay kumar kip macy steve rogridues divyesh shah manpreet singh steve strange rajesh sundaram tom theaker huan significant contributions project salem garcia-molina disk striping proceedings internation conference data engineering pgs feb patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod international conference management data pgs burkhard menon disk array storage system reliability proceedings international symposium fault-tolerant computing pgs qin xin miller schwarz long brandt litwin reliability mechanisms large storage systems ieee nasa boddard conference mass storage systems technologies san diego pgs apr blaum brady bruck menon evenodd efficient scheme tolerating double disk failures raid architectures proc annual international symposium computer architecture pgs macwilliams sloane theory error-corrrecting codes north-holland alvarez burkhard christian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual symposium computer architecture pgs park efficient placement parity data tolerate disk failures disk array systems ieee transactions parallel distributed systems nov muntz lui performance analysis disk arrays failure proceedings vldb conference pgs brisbane june anderson dykes riedel interface scsi ata usenix conference file storage technologies san francisco pgs march holland gibson parity deculstering continuous operation redundant disk arrays proceedings international conference architectural support programming languages operating systems pgs papoulis probability random variables stochastic processes edition mcgraw-hill york 

introduction disk drive modeling chris ruemmler john wilkes hewlett-packard laboratories palo alto research systems based disk drive simulation models good accurate simulation model emphasize performancecritical areas paper published ieee computer march supersedes labs technical reports hpl rev hpl osr copyright ieee internal personal material permitted permission reprint republish material advertising promotional purposes creating collective works resale redistribution obtained ieee receive information obtaining permission send blank message info pub permission ieee note file obtained scanning performing ocr ieee published copy result typographic errors published version minor clarifications updates made bibliography modern microprocessor technology advancing incredible rate speedups percent compounded annually norm disk storage densities improving impressively percent compounded annually performance improvements occurring percent compounded annually decade result disk system performance fast dominant factor system behavior naturally researchers improve performance large component performance disk drive research involves analytical simulation models compare alternative approaches quality models determines quality conclusions wrong modeling assumptions lead erroneous conclusions work develop describe accurate disk drive models explain commonplace simple inaccurate models room improvement article demonstrates describes calibrated highquality disk drive model error factor times smaller simple first-order model describe disk drive performance components separately show inclusion improves simulation model enables informed trade-off effort accuracy addition provide detailed characteristics disk drives description simulation environment disk drive model characteristics modern disk drives model disk drives understand behave begin overview current state art nonremovable magnetic disk drives embedded scsi small computer systems interconnect controllers widely disk drives mechanism controller mechanism made recording components rotating disks heads access positioning components arm assembly moves heads correct position track-following system place disk controller microprocessor buffer memory interface scsi bus controller manages storage retrieval data mechanism performs mappings incoming logical addresses physical disk sectors store information closely elements emphasizing features considered creating disk drive model clear features equally important model accuracy recording components modern disks range size inches diameter inches common sizes today smaller disks surface area store data larger counterparts consume power spin faster smaller seek distances historically storage densities increased gigabytes fit single disk next-smaller diameter series cost-effective preferred storage device increased storage density results improvements linear recording density determined maximum rate flux recorded read back current values bits inch approximately double end decade packing separate tracks data closely improvements occurring current values tracks inch rising tpi end decade product factors sustain growth rate percent year end decade single disk dozen platters shown figure stack platters rotates lockstep central spindle rpm facto standard years spindle rotation speed increased recently rpm median rotation speed increasing compound rate percent year higher spin speed increases transfer rates shortens rotation latencies time data rotate head power consumption increases bearings required spindle spin speed typically quoted accurate percent practice disk speeds vary slowly nominal rate perfectly reasonable disk operation makes impossible model disk rotational position revolutions operation fortunately operations occur bursts uncertainty applies request burst platter surface disk head responsible recording writing sensing reading magnetic flux variations platter surface disk drive single read-write data channel switched heads channel responsible encoding decoding data stream series magnetic phase stored disk significant fractions encoded data stream dedicated error correction application digital signal processing increase channel speeds current megabits multichannel disks support read write operation time making higher data transfer rates disks costly technical difficulties controlling cross talk concurrently active channels keeping multiple heads aligned platters simultaneously difficult track densities increase figure mechanical components disk drive top view side view arm assembly arm head spindle sector track arm head arm pivot platter cylinder positioning components data surface set store data series concentric circles tracks single stack tracks common distance spindle called cylinder today typical -inch disk cylinders track densities increase notion vertical alignment cylinders relevant track alignment tolerances simply fine essentially tracks platter independently access data stored track disk head moved attaching head disk arm lever pivoted end rotation bearing disk arms attached rotation pivot moving head move rotation pivot immune linear shocks older scheme mounting head linear slider positioning system task ensure head desired track quickly remains face external vibration shocks disk flaws nonconcentric noncircular tracks seeking speed head movement seeking limited power pivot motor halving seek time requires quadrupling power arm stiffness accelerations required achieve good seek times flexible arm twist bring head contact platter surface smaller diameter disks correspondingly reduced distances head move disks smaller lighter arms easier stiffen flexing contributing shorter seek times seek composed aspeedup arm accelerated reaches half seek distance fixed maximum velocity acoast long seeks arm moves maximum velocity aslowdown arm brought rest close desired track asettle disk controller adjusts head access desired location short seeks cylinders dominated settle time milliseconds fact seek occur head resettle position track short seeks cylinders spend time constant-acceleration phase time proportional square root seek distance settle time long seeks spend time moving constant speed taking time proportional distance constant overhead disks smaller track densities increase fraction total seek time attributed settle phase increases average seek times commonly figure merit disk drives misleading averages calculated ways situation complicated fact independent seeks rare practice shorter seeks common frequency function workload operating system driving disk disk requests completely independent average seek distance full stroke sources quote one-third-stroke seek time average simply quote full-stroke time divided sum times needed perform seek size divide sum number seek sizes commonly techniques weight seek time number seeks size single-track seeks disk cylinders full-stroke seek emphasizes shorter seeks providing approximation measured seek-distance profiles matters people building models profile encourage manufacturers include disk specifications alternative determine experimentally information required determine power apply pivot motor long seek encoded tabular form disk controller subset total stored interpolation intermediate seek distances resulting fine-grained seek-time profile sawtooth thermal expansion arm pivot-bearing stickiness factors occasionally make recalibrate tables milliseconds recalibrations triggered temperature timers occur frequently disk drive powered steady-state conditions recalibration occurs minutes difficulties real-time guaranteed-bandwidth systems multimedia file servers disk drives appearing modified controller firmware avoids visible recalibrations completely host schedule execution track fine-tuning head position end seek keeping head desired track function track-following system system positioning information recorded disk manufacturing time determine disk head correctly aligned information embedded target surface 
recorded separate dedicated surface maximizes capacity frequently disks small number platters track density increases form embedded positioning data essential fine-grained control combined dedicated surface coarse positioning data embedded-data method good coping shock vibration feedback information intermittently data sectors track-following system perform head switch controller switches data channel surface cylinder head repositioning accommodate small differences alignment tracks surfaces time switch typically half time settle end seek similarly track switch cylinder switch occurs arm moved track cylinder track takes time end-of-seek settling process settling time increases track density increases tracks platters aligned head-switching times approaching track switching nowadays disk drives aggressive optimistic approach head settling read operation means attempt read head track data unreadable settle completed lost error correction identification data misread sector ensure data wrongly interpreted hand data save entire revolution delay obvious reasons approach settle immediately precedes write difference settle times reads writes data layout scsi disk appears client computer linear vector addressable blocks typically bytes size blocks mapped physical sectors disk fixed-size data-layout units platters separating logical physical views disk means disk hide bad sectors low-level performance optimizations complicates task higher level software second-guess controller bsd unix fast file system zoning tracks longer platter inside maximize storage capacity linear density remain maximum drive support amount data stored track scale length accomplished disks technique called zoning adjacent disk cylinders grouped zones zones outer edge sectors track zones inside typically zones number double end decade data transfer rate proportional rate media passes head outer zones higher data transfer rates hewlett-packard -inch disk drive burst transfer rate intertrack head switches varies megabytes zone mbps outermost zone track skewing faster sequential access track cylinder boundaries obtained skewing logical sector track amount time required cope worst-case heador track-switch times means data read written full media speed zone track cylinder skew factors sparing prohibitively expensive manufacture perfect surfaces disks invariably flawed sectors flaws found extensive testing manufacturing list built recorded disk controller flawed sectors remapped portions disk process sparing granularity single sectors tracks simplest technique remap bad sector track alternate location alternatively slip sparing logical block map bad sector slipped sector track combinations techniques disk drive designers make complex trade-off involving performance expected bad-sector rate space utilization concrete disk drive forms track-level sparing slip-track sparing disk format time single-track remapping defects discovered operation disk controller disk controller mediates access mechanism runs track-following system transfers data disk drive client cases manages embedded cache controllers built specially designed microprocessors digital signal processing capability special interfaces control hardware directly trend powerful controllers handling increasingly sophisticated interfaces reducing costs replacing previously dedicated electronic components firmware interpreting scsi requests performing computations takes time controller microprocessor speed increasing fast stay ahead additional functions controller asked perform controller head slowly declining typically range bus interface important aspects disk drive host channel topology transfer rate overhead scsi defined bus alternative versions discussed encapsulations higher levels scsi protocol transmission media fibre channel disk drives scsi bus operation synchronous mode run maximum bus speed mbps early scsi buses differential drivers fast scsi specification increased mbps couple years ago disks appearing drive bus mbps fast wide standard defined mbps maximum bus transfer rate negotiated host computer scsi interface disk drive appears serial channel fibre channel popular transmission medium higher speeds partly fewer wires require smaller connector scsi bus device attached scsi initially supported addresses figure recently doubled wide scsi number devices bus increases contention bus occur leading delays executing data transfers matters disk drives large transfers controller overheads high addition time attributed transfer rate scsi bus interfaces host disk require time establish connections decipher commands scsi cost low-level protocol acquiring control bus order microseconds bus idle scsi protocol disk drive disconnect bus reconnect data transfer cycle devices access bus disconnected device processes data resulting higher throughput older channel architectures buffering disk drive result disk ready transfer data host interface ready disk wait entire revolution data head retry transfer scsi disk drive expected speed-matching buffer avoid delay masking asynchrony bus mechanism scsi drives data media slowly send bus drive partially fills buffer attempting commence bus data transfer amount data read buffer transfer initiated called fence size property disk controller modern scsi disk drives control command write requests data transfer disk buffer overlap head repositioning limit permitted buffer size interactions illustrated figure caching requests functions speed-matching buffer disk drive readily extended include form caching reads writes caches disk drives tend small kilobytes megabyte space limitations high cost dualported static ram needed disk mechanism bus interface read-ahead read hits cache satisfied immediately time needed controller detect hit send data back bus quicker seeking data reading disk modern scsi disks provide form read caching common form read-ahead actively retrieving caching data disk expects host request momentarily show read caching turns important modeling disk drive areas disk system behavior read partially hits cache partially serviced cache noncached portion read disk simply bypass cache altogether large read requests bypass cache block read cache controllers discard case subsequent read directed block early disk drives caches on-arrival read-ahead minimize rotation latency wholetrack transfers head arrived relevant track drive started reading cache end revolution full track worth data read host waiting data logical start point reread called zero-latency read disk cache memory called track buffer tracks longer request sizes on-arrival caching brings benefit -kbyte accesses disk -kbyte tracks maximum benefit percent rotation time on-arrival caching largely supplanted simple read-ahead disk continues read host request left proves optimal sequential reads proceed full disk bandwidth readahead back-to-back reads delayed full revolution disk host processing time initiating read request larger inter-sector gap policy choice read-ahead aggressive crossing track cylinder boundaries stop end track reached aggressive read-ahead optimal sequential access degrades random accesses head track switches typically aborted initiated unrelated request arrives switch progress delayed figure overlap bus phases mechanism activity low-level details bus arbitration selection elided simplicity data transfer mechanism head switchseek host sends command controller disconnects bus starts seek scsi bus data transfers host status message host rotation latency controller decodes data transfer mechanism head switchseek host sends command controller starts seek scsi bus data transfer host 
status message host rotation latency controller decodes read write scsi bus disk mechanism scsi bus disk mechanism single read-ahead cache provide effective support single sequential read stream sequential read streams interleaved result benefit remedied segmenting cache unrelated data items cached -kbyte cache split separate -kbyte cache segments configuration commands disk controller write caching disk drives cache volatile losing contents power drive lost perform write caching prevent data loss kind cache managed carefully technique reporting hp-ux file system back-to-back writes user data selected writes disk reported complete written disk cache individual writes flagged immediate-reported write immediately reported write read sequential extension write technique optimizes common case large writes file system split consecutive blocks protect power failures file system disables reporting writes metadata describing disk layout combining reporting readahead means sequential data written read adjacent disk blocks disk full throughput volatile write-cache problems disk cache memory made nonvolatile technique battery-backed ram lithium cell provide -year retention equipped disk drive free accept write requests fit buffer acknowledge immediately addition reduced latency write requests throughput benefits result data write buffer overwritten place reducing amount data written mechanism large number stored writes makes controller schedule near-optimal fashion takes time perform issues discussed detail read caching policies handling write requests hit data previously written disk cache nonvolatile memory safest solution delay writes copy written disk data write cache scanned read hits case buffered copy treated primary disk written command queuing scsi support multiple outstanding requests time provided mechanism called command queuing host give disk controller requests controller determine execution order subject additional constraints provided host letting disk drive perform sequencing potential job detailed knowledge disk rotation position modeling disk drives understanding disk drive performance factors ready model behavior drives describe models sufficient detail quantify relative importance components conscious choice made detail disk drive performance model application selectively enabling features arrive model accurately imitates behavior real drive related work disk drive models disk drives storage devices nonlinear state-dependent behavior disk drives modeled analytically accuracy work area simulation nonetheless simplest models assume fixed time select times uniform distribution elaborate models acknowledge disk separate seek rotation transfer times fail model components carefully seek times modeled linear function seek distance producing poor results smaller seeks common uniform distributions rotational latency inappropriate nonindependent requests frequent media transfer times modeled fixed constant dependent transfer size bus contention multiple devices connected bus previously work detailed models avoided limitations models simulated axial rotational head positions allowing seek rotation transfer times computed drawn distribution article extension simulation work earlier simulator built event based simulator version tasking library modified locally support time double type long type tasking library simple effective simulation environment tasks represent independent units activity call delay time simulated time advances task wait low-level events easy construct variety synchronization mechanisms top primitives basic ideas readily applicable simulation environments model disk drive tasks additional control structures figure task models mechanism including head platter rotation positions task accepts requests form read seek executes time handles data layout mapping logical blocks physical sectors task direct memory access engine dma engine models scsi bus interface transfer engine task accepts requests form transfer request host disk handles time cache object buffers requests tasks classic producer-consumer style manage asynchronous interactions bus interface disk mechanism tasks disk drive model fits larger system items representing scsi bus semaphore device bus time host interface synthetic trace-driven workload generator tasks range statistics-gathering -reporting tools disk-related portions simulation system consist lines commented code lines infrastructure simulator process series model system -mhz pa-risc processor million requests serviced approximately minutes traces study selected representative week-long samples longer trace series hp-ux unix computer systems systems traces greater detail request traces included data start finish times granularity microsecond disk address transfer length flags read write request marked synchronous file system start time corresponds moment disk driver request disk finish time corresponds request completed interrupt fires results present include time spent queued disk driver table describes disks singled analysis purpose show components disk drive model contribute accuracy selected noncaching disk drive cache interfere analysis disk mechanism disk driven show effects adding caching hp-ib ieee bus scsi interface modeling perspective major difference hp-ib bus slower disk drive mechanism scsi buses faster emphasize importance bus-related effects table characteristics disk drives analyzed article disk type formatted capacity cylinders size rotational speed average access host interconnect type max speed rpm hp-ib rpm scsimb figure simulation model structure single disk disk mechanism task dmaengine task buffer cache scsi bus disk mechanism work dmaengine work disk controller data structures code internally queued requests state disk data structures disk mechanism task request perform seek settle rotation cache memory data transfer blocking buffer full data ready dmaengine task request controller cache memory data transfer scsi bus blocking releasing bus buffer full data ready disk controller code select request process queue dmaengine disk mechanism generate readahead request requests waiting evaluation comparison metric evaluate models simple execution time request calibrating model real world differentiation models plot time distribution curves real drive model output root square horizontal distance curves metric call demerit figure model present absolute terms difference milliseconds relative terms percentage time real trace demerit figure matches encourage researchers disk drive models publish demerit figures preferably calibration curves important test workload similar kind data wishes analyze synthetic random load calibrating model workloads great sequential data accesses obtained parameters models manufacturer specifications performing curve fitting traces direct measurement disk drives modeling simplest model constant fixed time figure plots typical values literature actual time week traced data model good time fixed estimate results demerit factor percent average time simple model requires remembering state information requests modeling effect length straightforward model combination features seek time linear distance single-cylinder full-stroke seek times published disk drive specification figure head-settle effects head-switching costs rotational delay drawn uniform distribution interval rotation time fixed controller overhead figure graph displays measured curve linear interpolation manufacturer published single-cylinder full-stroke seek times accompanying table shows formula model real curve seek distance cylinders seek time milliseconds linear real table seek distance seek time cylinders cylinders transfer time linear length request asymmetry transfer 
rates hp-ib bus reads run mbps writes mbps media transfer rate mbps faster hp-ib bus bus speed dominates figure shows model fares demerit percent time demerit times larger effects system designers investigate modeling head-positioning effects previous model seek time linear function distance good match figure shows difference linear seek model real percent error table figure figure time distributions models real disk time fraction time real fixed fixed fixed trivial model constant fixed time demerit fixed fixed fraction time real model transfer time proportional size seek-time linear distance random rotation time interval rotation-time demerit simulation fraction time real model adds measured seek-time profile includes headswitch time demerit simulation fraction time real model final model includes rotational position modelling detailed disk data layout demerit simulation describes model approximate measured seek-time profile disk drive computing model trivial six-line single-line calculation improving positioning calculations opportunity model costs head track switching achieved determining track cylinder request started ended adding fixed cost head track switch needed start request end figure shows demerit figure halved percent time modeling rotation position important performance components left model detailed rotational latency spare-sector placement keeping track rotational position disk explicitly calculate rotational latency drawing uniform distribution calculating times disk revolved start simulation assuming spinning nominally rated speed track cylinder skewing sector-based sparing spare sector track accounted mapping logical blocks physical sectors adding factors results data shown figure good match model fitting real disk drive percent table lists parameters final model modeling data caching discussion disk drive buffer cache cache added disk drive complications arise shown figure model incorporating features simulate scsi disk drive read-ahead reporting large disparity small completion times due caching percent requests completed caching modeled results closely matching real disk drive demerit percent acceptable figure models real disk time fraction time real model basic model includes features model demerit simulation fraction time real model adding caching readahead reporting demerit simulation added aggressive read-ahead reporting model section caching requests gave results shown figure good match demerit percent time half absolute error comparable major remaining components modeled accurately actual bus speeds achieved system drive rated speed host controller imposes lower rate detailed disk drive controller overheads frequently combination interactions previous request current overheads depend size request modeling level detail requires heroic efforts applying logic analyzers scsi buses bruce worthington greg ganger michigan approach managed fine-tune controller-overhead bus-transfer components model similar achieved demerit figures percent disk drive model summary table summarizes models bottom include line michigan model full model good match required onerous implement encourage adopt full model includes details parameters provided table host device driver cpu costs executing queuing strategy scsi bus including bus contention effects table final model parameters parameter sector size bytes bytes cylinders tracks cylinder data sectors track number zones track skew sectors sectors cylinder skew sectors sectors revolution speed rpm rpm controller interface hp-ib scsi-ii controller overhead reads writes seek time short long boundary track switch time read fence size sparing type sector track disk buffer cache size track sparing spare regions beginning end data region effect simulation performance spare sector end track giving sectors track track sparing dedicated sparing regions embedded data area table shows data regions located physically disk format cylinder track boundaries physical sector space disk disk physical cylinders store data rest spares region start end disk controller effects fixed controller overhead scsi bus disconnects mechanism delays overlapped bus transfers mechanism activity disk buffer cache including read ahead write-behind reporting producer-consumer interlocks mechanism bus transfers data layout model reserved sparing areas including sectorand track-based models zoning track cylinder skew head movement effects seek time curve derived measurements real disks settle time values read write head-switch time rotation latency model chose ignore things worthwhile model soft-error retries effects individual spared sectors tracks likewise features disk drive sparing policy important accurate understanding layout effects sparing model rotational positioning effects accurate model disk drive essential obtaining good simulation results studies failure model disk drive behavior result quantitative extreme cases qualitative errors analysis careful modeling difficult costly provided data enables designers quantitatively determine benefits gained investing effort disk drive model important feature model data-caching characteristics disk percent relative demerit important features data transfer model including overlaps mechanism activity bus transfers percent demerit seektime head-switching costs percent demerit evaluation transfer model greater effect positioning model relative importance reversed scsi drives bus generally faster disk mechanism finally modeling rotational position detailed data layout improved model accuracy factor modeling rotational position accurately important systems emphasize sequential transfers modern file systems increasingly adept good model careful calibration tuning values good fit models differ manufacturer published specifications addition space present quantitative effects modeling zoning model handles features important workload large data transfers table performance figures models disk drives show greater accuracy features added model feature demerit disk type constant time basic model add head positioning add rotation position caching add caching controller costs plan refined disk drive simulation model explore variety designs policy choices host disk drive levels hope make source code model interested researchers year calibrated model parameters longer list disk drive types space describe acknowledgments pei cao contributed greatly simulator disk model part marvin keshner provided information underlying storage technology trends tim sullivan patricia jacobson provided helpful feedback earlier drafts article work performed part datamesh research project hewlett-packard laboratories patterson hennessy computer architecture quantitative approach morgan kaufmann san mateo calif ruemmler wilkes unix disk access patterns proc winter usenix conf usenix sunset beach cali jan hewlett-packard boise idaho series -lnch scsidisk drive technical manual part number apr seltzer chen ousterhout disk scheduling revisited proc winter usenix conf usenix sunset beach calif jan jacobson wilkes disk scheduling algorithms based rotational position tech report hpl csp hewlett-packard laboratories palo alto calif feb thekkath wilkes lazowska techniques file system simulation published simultaneously tech reports hpl- hewlett-packard laboratories palo alto calif dept computer science eng univ washington seattle wash oct published software practice experience nov ruemmler wilkes disk shuffling tech report hpl- hewlett-packard laboratories palo alto calif oct holland gibson parity declustering continuous operation redundant disk arrays proc int conf architectural support programming languages operating 
systems published special issue computer architecture news vol unix system language system release selected readings select code hewlett-packard boise idaho series disk storage systems owner manual models part number feb hewlett-packard boise idaho -lnch scsi disk drives technical manual part number june worthington ganger patt scheduling algorithms modern disk drives proc acm slgmetrics conf chris ruemmler software engineer hewlett-packard works area performance analysis technical interests include architectural design system performance operating systems graduated degrees computer science california berkeley john wilkes worked researcher project manager hewlettpackard laboratories current research interest high-performance high-availability storage systems interested performance modeling interconnects resource management scalable systems enjoys interacting academic research community wilkes graduated cambridge degrees physics diploma phd computer science wilkes contacted hewlett-packard laboratories page mill palo alto e-mail wilkes hpl ruemmler address hewlett-packard pruneridge ave cupertino e-mail ruemmler cup 
communication behavior distributed operating system remzi arpaci department electrical engineering computer science computer science division california berkeley abstract present measurements communication behavior prototype distributed operating system solaris employ server workloads drive study build solaris source tree synthetic web server parallel database measurements reveal number facts implications design solaris prototype implementation solaris design message layer find message traffic centered nodes house disks potential bottleneck file system striped data cluster avoid problem messages medium sized range bytes indicating message-layer support messages crucial messages structured chain buffers suggesting gather interface avoid additional buffer allocation memory copies finally request-response time high due overhead current message layer fact indicative prototype status system case low-overhead message layer substantially improve performance contents introduction background experimental setup solaris experimental method hardware tracing workloads workload characterization performance cpu utilization system calls context switches summary message traffic message destinations message sizes message rates implications message dependencies request-response workload implications anatomy message buffer chains protocol overheads implications conclusions future work acknowledgements people making report advisor dave patterson insight guidance friendship insistence completed report reader david culler advice sort facets arbitrary design decisions fundamental remember long time extended members group learned years berkeley difficult single bunch amin vahdat rich martin alan mainwaring making soda hall slightly tolerable place extend extra note alan owning excellent recording beethoven work time effort entire solaris group sun labs yousef khalidi giving great opportunity willingness lend ear moti thadani excellent explanations vagaries streams enjoyable discussions assistance jose bernebeu patience gave vlada matena expert knowledge parts solaris finally ken shirriff time friendship single member group time extend special double co-worker keith vetter helped gather measurements presented keith introducing magic bob greenberg music lectures parents support love encouragement years graduate studies direction advice invaluable achieved finally fiance andrea dusseau person indebted levels academically excellence profoundly influenced co-worker learned uncountable lessons rigorous careful pursuit truth sounding board expert reader follow personally love support terrific friendship alive happy past years lucky found wonderful person forward years chapter introduction finite unknown infinite intellectually stand islet midst illimitable ocean inexplicability business generation reclaim land -thomas huxley distributed systems long active area research distributed system comprised components including process management networking file systems requirements services eclectic common communicate lines early work achieving high network performance paramount attaining good distributed system performance surprisingly researchers focused efforts design implementation fast communication protocols recent arrival high-speed switch-based local-area networks improved communication performance order magnitude bandwidth networks range making lightweight communication protocols one-way end-to-end times range microseconds quantum leap shared-medium ethernet previous systems designed radical change underway important understand characterize modern distributed systems make communication technologies performance characteristics underlying network functionality message layer provide measurements fact implication message traffic centered disks striping file system valuable medium-sized messages account traffic message-layer support medium-sized messages request-response time high low-overhead message layer messages chain buffers message-layer support gathering interface table column present main findings measurements communication behavior solaris column shows potential implications design implementation system message layer structure distributed system begin answer questions instrumented communication layer distributed operating system system scrutiny solaris prototype cluster operating system number ways extends real commercial kernel solaris distributed system building top distributed object system based corba extends file system process management networking subsystems unix provide users single-system image interesting measurement testbed early stages development aspects system optimized directly affects measurements timing-sensitive load message subsytem high measurements including sizes destinations messages affected bearing mind attempt separate results poor implementation design decisions fundamental drive traces communication behavior employ server workloads build solaris source tree web server responding synthetic stream http requests database performing series debit credits analysis consists progressive steps trace resource usage statistics characterize workloads trace aggregate communication nodes cluster data derive message distributions rates communication solaris based requests subsequent responses instrument higher level system unveil communication dependencies conclude examining structure individual message understand type interface message layer provide measurements reveal number facts highlighted table discuss results implications design solaris prototype implementation design message layer find message traffic cluster centered disks striped file system avoid potential disk bottlenecks global services network port name-space manager distributed suffer similar bottlenecks find messages roughly workloads range bytes support message sizes critical design message layer surprisingly data larger messages frequently avoiding memory copies actions function message size beneficial instrumenting object subsystem find prototype implementation solaris suffers unusually high request-response times simple request-response attributed high-overhead streams-based transport implication case obvious system desperate low-overhead transport layer current cost object infrastructure high accounting roughly ideal case transport layer fast overhead object system significant finally find messages formed chain buffers gathering interface benefit saving cost memory-to-memory copy buffer allocation messages chains by-product design object subsystem attaches header separate buffer message implementation effort potentially remedy situation outline rest paper section overview solaris operating system describes hardware configuration methodology study section describe workloads drive study section begin measurements aggregate summaries communication patterns section understand higher level viewing communication request-response pairs finally section detail structure message implications layers conclude give future directions section chapter background experimental setup section describe solaris system give high-level overview concepts philosophy system outline particulars experimental environment solaris solaris prototype multi-computer operating system multi-computer cluster homogeneous computers connected high-speed interconnect solaris single-system image constructing illusion single machine users applications external network existing solaris api abi preserved existing solaris applications device drivers run modification finally support high availability solaris comprised major subsystems file system process management networking extensions made components normal solaris kernel order attain aforementioned goals briefly explain component underlying object substrate solaris file system pxfs proxy file system extends local unix file system distributed environment pxfs file access location transparent process system open file located disk system pxfs accomplishes interposing vnode layer pxfs intercept file operations forward correct physical node nodes single path hierarchy accessible files ensure unix file system semantics coherency protocols employed performance pxfs makes extensive caching based techniques found spring eventually provide zero-copy bulk large data objects major subsystem process management solaris process management globalized location process transparent user threads process restricted physical node process system system makes remote execution facilities run jobs nodes system process management implemented virtual 
layer vproc top existing solaris process management code tracking state parent children processes process groups sessions supports posix process semantics access information processes solaris extends proc file system debuggers global proc covers processes system solaris provide migration facilities remote fork capability global networking subsystem solaris network applications creates single system image respect network devices system process node system network connectivity process location key components involved achieving end global management network space distributed multiplexing distributed de-multiplexing distributed program sap-server manages global network space sapserver prevents simultaneous allocation tcp ports processes spread nodes cluster outgoing packets processed node application forwarded network interface similarly packet filter intercepts incoming packets directs correct node final subsystem solaris global subsystem extends solaris cross-node device access accomodate cross-node functionality made device configuration loading unloading kernel modules device naming providing process context drivers subsystems built top runtime environment distributed objects object request broker orb orb viewed object communication backplane performs work support remote object invocations solaris orb features counting objects support one-way communication full description orb experimental method section describes experimental method describe hardware software platform experiments explain instrumented solaris performance effects arose instrumentation finally give details workloads drive simulations hardware cluster machines consists sparcs sparcs sparcs running copy modified version solaris includes support solaris experiments performed -cpu -node cluster sparcs memory machine ethernet connection world connection fast intra-cluster network myrinet local-area network possibly extra disk acts part global file system section cluster configuration slightly explained tracing trace communication behavior solaris make extensive tnf tracing facility solaris utility events kernel time-stamped logged kernel buffer easily extracted analyzed call log event takes roughly microseconds insertion calls critical path alter results performing timing measurements judiciously inserted logging statements measuring events long timing overhead insignificant cases care subtract timing overhead workloads paper utilize workloads drive system illustrated figure workloads slightly hardware configurations workload make performs large parallel make solaris source tree shell ona starts make rexec system call distribute jobs nodes system disk holding source tree attached workload stresses remote execution facilities pxfs workload web based spec benchmark benchmark external stream http requests generated cluster node running copy ncsa httpd server requests connected ethernet network interface request received networking subsystem redirects traffic connection nodes system similar mach packet filter responses forwarded responding cluster node back original source disk http files connected workload heavily utilizes networking sub-system potentially caching features file system lastly database workload performs series transactions oracle parallel databasea simple debit credit transactions performed parallel nodes disk database attached workload stresses aspects global file system make rexec rexec rexec web tcp traffictcp traffic tcp traffic external http requests database transactions transactions transactions figure workload setup upper-left diagram shows experimental setup make workload note workstation processors circled symbol parallel make started remotely executes jobs workstations disk solaris source tree attached upper-right diagram depicts web workload setup external http requests streaming traffic distributed machines round-robin fashion responses proceed back source disk requested file attached lower diagram shows database workload setup disk attached experiment machines perform transactions fixed time period chapter workload characterization process gathering workloads study distributed system important concentrate finding programs stress system services section characterize workloads drive study show make operating system services present performance workloads give insight nature workloads give breakdowns cpu usage measures system system calls context switches performance section shows workloads perform tracing activated table run time rate operation workloads run-time workloads chosen experimenter run-time make workloads determined fixed amount work rate make reveals number compilations case web workload external clients perform http operations fetch file rate measures number http operations database workload clients system perform simple debit credit operations due legal issues rate workload revealed prototype system performance system spectacular tuned performance section main reason workloads run fixed minute period workload time rate make mins secs compiles sec web mins secs http ops sec database mins secs table benchmark performance total performance time rate benchmark cpu utilization figure shows cpu utilization workload lifetime experiment collect data user-level daemon awakens collects statistics cpu usage consists distinct parts user percent time spent running program system percent time spent operating system wait percent time spent waiting disk return idle remaining difference note graphs cumulative system line sum system user time cpu percentage time seconds cpu profile make wait system user cpu percentage time seconds cpu profile wait system user cpu percentage time seconds cpu profile database wait system user figure cumulative cpu utilization cumulative cpu utilization split user system wait time displayed time workload rest idle maximum cpu percentage moment cpus peak utilization note graphs cumulative system time sum system user time wait time sum white space top-most line wait time idle time make workload utilization time makefile runs sequential portion utilization seconds low roughly processors worth cpu parallel portion begins constant utilization processors spikes peak finally make ends long link phase roughly seconds average workload spends time user mode time system mode total cpu usage machine cpu number time seconds individual cpu traces make machine cpu number time seconds individual cpu traces machine cpu number time seconds individual cpu traces database figure individual cpu utilization cpu utilization sum user system time time displayed cpu cluster axis shows cpu machines impliesa cpu web workload displays significantly behavior workload starts -second mark ends minutes quarters cpu utilized times run item interest dips graph seconds times transport layer lost message hiccup message times retransmitted mishap system continues web workload sensitive type failure ifa stops forwarding http requests nodes system nodes work time remain idle main point interest large percentage time spent system roughly total time non-idle time finally figure reveals database workload makes cpu small fraction utilized spent system time total time non-idle time user time accounts total time workload spends time waiting disk requests show load balanced cluster figure presents cpu utilization processor set graphs view sum user system time make workload load fairly balanced nodes finish sooner note busiest node houses disk source tree located diagram web workload shows processors utilized benchmark note dips cpu graphs packet lost lastly database workload note low constant cpu usage machines case busiest 
node due presence disk system calls time seconds system call profile make system calls time seconds system call profile web system calls time seconds system call profile database figure system calls aggregate system calls workstations context switches time seconds context switch profile make context switches time seconds context switch profile context switches time seconds context switch profile database figure context switches aggregate context switches workstations system calls context switches cpu utilizations workloads making operating system show operating system duress show number system calls context switches figure aggregate number system calls performed workloads workloads make fairly heavy operating system services workloads make performs fewest average calls file system related web workload intense performing aggregate average syscalls workload makes file system networking subsystem frequently crossing user-space kernel boundary finally database shows constant system call rates cluster stressing file system sparcrunning solaris trapping returning kernel takes roughly workload performs system calls rate spent crossing user-kernel boundary time spent kernel call overhead small noticeable fraction examine total number context switches workstations cluster counts number times low-level context switch routine called inside kernel process sleep system call force context switch figure correlated figure data figure evidence operating system strain execution workloads general trends previous figure make web database workloads average context switches interesting point context switch rate intensity context switch takes switches implies time spent switching milliseconds switch time time summary section number measures system usage cpu utilization system call rates context switch rates workloads operating-system intensive non-idle time spent operating system make web database workloads workloads perform heavy number system calls spending time trapping kernel system switches contexts frequently workloads section gain insight frequency switches characterization workloads order purposes study mind leave future work chapter message traffic section introduced workloads drive study section trace messages nodes cluster workloads trace destinations messages cluster bytes node node give cumulative breakdowns message sizes finally rate node sends messages presented measurements kernel-kernel traffic messages intra-cluster interconnect kernel-user traffic server implemented user space amount traffic negligible note nodes workers performing fraction work complete workload hand addition nodes serve special purpose workload message destinations figure shows pictorial representation message destinations width arrow proportional total number messages workstation note data found table discern number facts diagrams foremost number messages node node equal number messages conservation communication holds due request-response nature traffic request node met response node note one-way communication time measurements case communication traffic make imc web database figure message destinations width bar proportional number messages node node make diagram maximal arrow nodes messages exchanged web nodes exchange messages finally database nodes send messages figures scaled base number readily comparable centered disks attached discussed make workload skewed traffic pattern nodes nodes communicate nodes messages disk source tree attached node message traffic non-uniform web workload figure nodes special interest presence external network interface node reveals traffic heavy serves cluster gateway internet http request routed througha round-robin selection nodes determining node handles request fetching requested file node routes message back back requester hot-spot web workload service access point server sap-server manages global port space connection opened closed communication sap-server node occur configuration sap-server runs single node practice distributed program node managing portion space co-locating sap-server nodes external connections lessen message traffic reducing local inter-process communication finally database workload traffic centered disk database resides realistic environment spread disks nodes lead evenly balanced traffic patterns balance achieved access patterns naturally balanced solaris takes explicit action balance file system load cluster previous figures show message communication spread cluster show bytes node figure weights previous graph total bytes width arrow proportional total number bytes workstation total includes user payload orb headers fundamental operation system transport headers implementation specific make workload traffic reason node plays role file server experiment nodes request file blocks modified file blocks eventually return node workload find incoming traffic node totals workstations ethernet connections experiment traffic routed alternative configuration tcp messages return network interface nodes make imc web database figure weighted message destinations width bar proportional number bytes node node number messages make largest arrow node represents communication traffic web workload arrow represents traffic database workload highest node highest widths scaled factor figures comparable out-bound communication indicating bytes read written web workload byte-weighted diagram reveals data flows toa nodes nodes node heavy byte flow mentioned sap-server traffic note total bytes workload significantly make msgs msgs msgs make table messages table presents raw data figures number requests doesn equal number responses difference number messages dropped finally pictorial view database workload byte transfer reveals traffic uniform frequency capacity message sizes section shown data aggregate communication cluster section exact distribution message sizes workloads differences workstations cluster reiterate message size sum user payload orb headers figures show node distribution message sizes node nodes workloads graph depicts cumulative distribution message sizes node experiment line graph weights distribution size message messages size size percent message size bytes message size breakdown make cumulative message count cumulative message size percent message size bytes message size breakdown make cumulative message count cumulative message size percent message size bytes message size breakdown make cumulative message count cumulative message size percent message size bytes message size breakdown make cumulative message count cumulative message size figure message size distribution make workload lines count number messages node message smaller bytes messages smaller bytes data transferred big chunks data shipped units packets transferred data figure shows breakdown make workload observing cumulative distribution message sizes diagrams upper-most line messages small fact workstations messages bytes request-response traffic encourages extent direction request-response transfer large chunk data note message smaller bytes specific support messages size domain lower line graphs weights distribution message size messages small data fact transferred large messages truths computer science data transferred large objects objects small nodes percent message size bytes message size breakdown cumulative message count cumulative message size percent message size bytes message size breakdown cumulative message count cumulative message size percent message size bytes message size breakdown cumulative message count cumulative message size percent message size bytes message size breakdown cumulative message count cumulative message size figure message size distribution web workload lines count number messages 
node message smaller bytes messages smaller bytes machines messages considered large case larger attributed synthetic nature workload roughly data transferred blocks larger node number message sizes case directly influenced file sizes previous studies file sizes unix environments shown similar files small data large files distribution figure message sizes web workload regular make workload message sizes lifetime experiment nodes identical distributions predicted graphs roughly messages bytes difference bytes transferred considered large messages case note highly dependent http request experiment requests -byte file diverse percent message size bytes message size breakdown database cumulative message count cumulative message size percent message size bytes message size breakdown database cumulative message count cumulative message size percent message size bytes message size breakdown database cumulative message count cumulative message size percent message size bytes message size breakdown database cumulative message count cumulative message size figure message size distribution database workload lines count number messages node message smaller bytes messages smaller bytes data transferred big chunks data shipped units non-synthetic workload directly correlate diverse message pattern note message smaller bytes traffic coming comprised small messages node relays requests nodes replies carry larger payload traffic slightly rest attribute sap-server presence leads small message exchanges manage global port space database workload figure additional data point spectrum message distributions bears similarity make workload roughly messages small bytes large messages move data data transferred blocks hand distinct message sizes similar web workload case roughly message sizes realistic database workload run disks significantly alter message size distributions node perform request-responses simply serve balance traffic nodes equally message rates examining message size distributions observed size burstiness traffic small protocol messages large data packet section establish time burstiness exists figures depict number messages node life experiment set graphs workload diagrams workstation make workload node communicates bursty manner workstations periods hundreds messages quarter send fewest messages time average rate messages noted nodes workers nodes partake make special responsibilities differs make master executing additional messages rexec system calls distribute work nodes averages message experiment finally file server strain respond requests files readily graph averages messages notably nodes web workload shows workload-sensitive traffic patterns figure observe nodes constantly sending messages time seconds onward experiment began qualitative difference quantity sends messages routes traffic sap-server sends worker nodes averaging messages database workload proves similar web workload nodes communicating constant rates attributed nature workload node continually performs transactions nodes send messages file server node sends times node responding requests nodes messages time seconds messages time make messages time seconds messages time make messages time seconds messages time make messages time seconds messages time make figure message rate make workload graph shows number messages workstation sends experiment busiest serves source files nodes parallel make master nodes workers environment equally busy peak end due link stage compilation messages time seconds messages time messages time seconds messages time messages time seconds messages time messages time seconds messages time figure message rate web workload graph shows number messages workstation sends experiment gateway internet busiest machine bya home sap-server nodes serve workers messages time seconds messages time database messages time seconds messages time database messages time seconds messages time database messages time seconds messages time database figure message rate database workload graph shows number messages workstation sends experiment rate machines machine housing disk workload machine surprisingly times busy conservation communication implications section workload highly influences message traffic cluster development type environment modeled make workload message traffic highly non-uniform message traffic hot-spots disks system message sizes environment highly variable strongly influenced size files relevant workload system anticipate breakdown small messages large messages data web server traffic patterns dependent number users web service web workload steady stream requests models constantly busy server type workload message traffic spread evenly nodes exception node running sap-server sap-server contacted time port number bound potential bottleneck small messages payload message delivered ratio partly attributed tcp ineffective support http simple http requires tcp open connection send message receive message close connection dependent design networking subsystem database workload regular strongly correlated fact workstation running simple debit credit script node constantly performing transactions message traffic clustered disks implication message layer support medium-sized messages sizes bytes important note -byte message size supported early active message layers environment difficult deem workload typical workloads made extensive medium-sized messages specifically make web database workloads messages bytes traffic request-response latency messaging important reducing latency message round-trip times directly lead improved performance investigate section studying simple request-response final observation arises experiments pertains distributed system design benefits advanced striping file system apparent solaris nodes easily hot-spots performance bottlenecks simply serve files current workload system stripes blocks set nodes load balance system automatically addition nodes serve file servers send large blocks data important optimize large message sends nodes support zero-copy message layers sense important path optimize disk network direct dma disk device network device pose difficulty current hardware technologies chapter message dependencies previous section examined nature message traffic number workloads leaves questions unanswered long simple request-response cost due transport request-response workload workloads excellent driving system aggregate measurements instrument system finer granularity ran workloads produce mountain data hard differentiate queuing delays fundamental costs reason drive system simple test program -ef hardware set-up simplified workstations involved system-v based unix including solaris command proc file system obtain information processes system process created assigned unique process identifier pid find information process resource utilization status opening file proc pid calling correct ioctl command -ef lists information processes system opens directory proc find files processes system opens file performs ioctl finally closes file solaris proc combining global file system sum local proc file systems cluster open ioctl close generate message traffic orb invoke method handle request orb unpack message prepare act orb results prepare return msg send message msg process ack msg receive message send ack context switch msg send message msg process ack msg receive message send ack orb process response user invocation complete orb prepare send user invoke request initiator request handler context switch figure cumulative request handling costs user refers solaris programmer orb object substrate msg transport layer leftmost graph depicts requesting-side costs simple request-response rightmost 
side handling-side costs note handling costs show requesting side graph time waiting response return order fetch global information experiment focus messages discuss details experiment explain steps involved simple requestresponse mentioned solaris built object substrate orb object request broker orb turn built reliable transport layer communication solaris performed layers form object invocations invocation orb marshals arguments request hands transport reliably delivers message handling node handling side transport receives message hands orb orb receives message interprets invokes method request results gathered finally shipped back request initiator orb initiating side unmarshals response puts results place returns control user case solaris programmer process diagrammed figure viewed object-oriented functional equivalent remote procedure call beneath orb transport layer transport reliable send arbitrary buffer chain node system implemented streams module reliable message transport top streams-based device driver mbit ethernet myrinet date context switch context switch request initiator request handler msg receive message send ack orb prepare message orb unpack message prepare act user invoke method handle request orb results prepare send msg send message msg process ack msg receive message send ack orb process response msg process ack msg send message driver receive message pass transport driver receive message pass transport figure request handling costs orb refers time spent orb msg time spent transport layer driver time spent device driver user time spent handler invoked transport costs shown green lighter shading black white orb costs red darker shading cost object invocation white note context switch time folded transport costs experiment machines system requests generated froma handled ona experiment begins enabling trace buffers kernel machines command -ef run finally dump kernel trace buffers log files machine total request-response pairs traced figure shows results requesting side entitled request initiator handling side request handler box scaled lengthwise represent total cost microseconds note orb costs red darker shading black white transport costs green lighter shading conclusion draw simple request-response takes roughly milliseconds fast network ethernet myrinet time wire negligible time examine request-initiating node messages orb spends roughly preparing request point orb hands message transport layer cost message wire costly average note high cost due expensive streams-based utilities locking protocols request side waits acknowledgment receives waits response request request-handle side takes place request-initiator waiting packet received driver passed transport layer cost estimate time accounted fairly accurate transport layer sends acknowledgment back requesting side strips transport headers decides packet destined orb passes message orb takes average costly sum context switch occurs orb message interpretation set orb finally performs requested invocation cost invocation application-specific case note request response work absolutely request-response times expensive minimal cost invocation orb prepares response analogous request side prepare sends response back incurring large transport overhead finally request side response back driver estimated transport finally orb interprets message places results correct place finishes request item discuss latency network experiment -mbit ethernet employed time-on-the-wire -byte packet modern switch-based network latency wire insignificant overhead matters domain implications micro-experiments section cost request-response high culprits high-latency hijinks transport layer orb transport layer surprisingly large tcp-like overheads total send receive overhead drivers altogether accounts roughly total involved simple request-response total time orb costs total time substantial compare real cost remote invocation infinitely fast network protocol place orb responsible time optimization work desperately needed areas chapter anatomy message previous data discussion paper focused message sizes destinations request-response timings section brings structure message scrutiny resume workloads make web database examining construction message detail hope shed light potential areas communication layer network interface optimize attempt understand overhead object system requires terms bytes buffer chains figure shows structure prototypical message message solaris comprised list buffers similar structure mbufs found older unix systems mblks streams vernacular message arbitrarily large number buffers measurement shows case table shows percent messages buffer chains consisting buffers workloads note message consisting buffers message head figure generic message message list potentially non-contiguous buffers buffer buffers buffers make web database table buffer chains percent messages buffers chain workloads messages buffers chain row totals make workload messages single buffer messages consist buffers buffers imbalance increases workloads web workload database workload protocol overheads give give buffer size distributions workloads figure buffer chain bigger bytes half buffers bytes size buffer underlying object system buffer size distribution pattern similar message size distributions figures buffer holds user payload surprisingly shows greatest variation size finally buffer occasionally object system send object protocol information node workloads buffer smaller buffer exceeding bytes leads obvious question overhead percent total bytes object system require table shows breakdown workloads make database workloads send large messages payloads large object percent messages message size bytes buffer chain distribution make buffer buffer buffer percent messages message size bytes buffer chain distribution buffer buffer buffer percent messages message size bytes buffer chain distribution database buffer buffer buffer figure buffer chain sizes figures plot distribution sizes buffer message note bulk data buffer chain buffer buffer buffer make web database table object protocol overheads table shows percentage bytes buffer buffer chain row totals overheads small web workload overheads substantial figure absolutely larger workload largest payloads fairly small bytes small messages object overhead noticeable fraction implications section inspected structure solaris message buffers object system protocol information payload solaris programmer messages chain non-contiguous buffers size distribution distinct buffers radically buffer small buffer ranges small hundred bytes large make workload existence buffer chains implies message layer support gather interface perform extra copy buffer allocation sending data gathering interface present today fast messaging layers alternative significantly restructure code allocate extra space object anticipation object system demands gather cost directly proportional memory copy costs cost buffer allocation substantial restructuring code worth pursuing made buffer chains finally message requires bytes object header information web workload bytes transferred orb chapter conclusions future work paper measured performance workloads prototype cluster operating system solaris measurements revealed interesting results message-layer design distributed system design perspective systems perspective key solaris design decisions adversely affect performance non-striped file system lead bottlenecks occurring disks system node data nodes system global services network port name-space manager suffer similar bottlenecks distributed lastly nodes house disks sending large messages optimizing path disk network prove worthwhile message layer design mind find 
message traffic highly workload dependent cluster compute server database engine web server scenarios system behave differently performance optimization system depends intended clear support medium-sized messages important workloads messages workloads range bytes instrumenting object subsystem prototype implementation solaris suffers unusually high request-response times attributed slow streamsbased transport contributes high overhead high latency current cost object infrastructure high ideal case transport layer fast overhead object system significant finally support gathering interface limited benefit object system attaches header request separate buffer implementation effort level potentially remove future remains examined perspective workloads web database workloads realistic web workload trace-driven usage patterns popular web-site make web server netscape server database workload disks standard database benchmarks tpc-c tpc-d system evolving repeating measurements yield insight problems solved persist clear fast lightweight transport great avenue exploration place tuning orb costs spread orb code path early measurements memory allocation improved cases low overhead significantly improve system performance light view report prototype continued series refined evaluations bibliography anderson dahlin neefe patterson wang serverless network file systems fifteenth acm symposium operating systems principles pages copper mountain resort usa dec anderson owicki saxe thacker high-speed switch scheduling local-area networks acm transactions computer systems nov baker hartman kupfer shirriff ousterhout measurements distributed file system proceedings thirteenth symposium operating systems principles pages pacific grove usa oct baskett howard task communicaton demos proceedings acm symposium operating system principles pages reading usa bernebeu matena khalidi extending traditional object-oriented techniques coots boden cohen felderman kulawik seitz myrinet gigabit-per-second local area network ieee micro feb cheriton vmtp transport protocol generation communication systems proceedings acm sigcomm pages stowe aug cheriton distributed system communications acm mar cheriton williamson network measurement vmtp request-response protocol distributed system proceedings acm sigcomm pages stowe aug chu zero-copy tcp solaris proceedings usenix technical conference pages san diego usa jan douglis ousterhout kaashoek tanenbaum comparison distributed systems amoeba sprite computing systems fall finkel scott experience charlotte simplicity function distributed operating system ieee transactions software engineering june hartman ousterhout zebra striped network file system proceedings fourteenth symposium operating systems principles pages ashville usa dec kaashoek van renesse van staveren tanenbaum flip internetwork protocol supporting distributed systems technical report department mathematics computer science vrije universiteit july khalidi bernebeu matena shirriff thadani solaris multi-computer winter usenix pages usenix jan khalidi nelson extensible file systems spring proceedings fourteenth symposium operating systems principles ashville usa dec mainwaring active message application programming interface communication subsystem organization master thesis california berkeley martin hpam active message layer network workstations proceedings hot interconnects july matena bernabeu khalidi high availability support solaris technical report sun microsystem laboratories moutain view mcvoy staelin lmbench portable tools performance analysis proceedings winter usenix jan mogul rashid accetta packet filter efficient mechanism user-level network code proceedings eleventh symposium operating systems principles pages austin usa nov mullender rossum tanenbaum renesse van staveren amoeba distributed operating system ieee computer magazine ncsa httpd development team ncsa httpd http hoohoo ncsa uiuc apr ousterhout cherenson douglis nelson welch sprite network operating system ieee computer feb popek walker editors locus distributed system architecture pages computer systems series mit press ritchie thompson unix time-sharing system communications acm july schmidt suda transport system architecture services high-performance communications systems ieee journal selected areas communications spec design committee specweb http specbench osg web apr sunsoft technical writers tnf manual page solaris answerbook tibbitts corba common touch distributed applications data communications international van rennesse tanenbaum amoeba world fastest operating system operating systems review dec von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings fifteenth acm symposium operating systems principles pages copper mountain resort usa von eicken culler goldstein schauser active messages mechanism integrated communication computation proc international symposium computer architecture pages gold coast australia welch comparison distributed file system architectures vnode sprite plan computing systems spring 
repeatlocal computation load imbalance processes communication optional time granularity computation variation end opening barrier begin opening barrier closing barrier optional compute comm sync barrier switch idle fractionoftime compute comm sync news switch idle fractionoftime compute comm sync transpose switch idle fractionoftime compute comm sync barrier switch idle slowdown compute comm sync news switch idle slowdown compute comm sync transpose switch idle slowdown barrier imbalance imbalanceimbalance latency latency high low medium context-switch computation granularity low low high coarsefine highlowhighlowhighlow high news imbalance imbalanceimbalance latency latency high low medium context-switch computation granularity low low high coarsefine highlowhighlowhighlow high transpose imbalance imbalanceimbalance latency latency high low medium context-switch computation granularity low low high coarsefine highlowhighlowhighlow high appeared usenix symposium operating systems design implementation november metadata update performance file systems gregory ganger yale patt department eecs michigan ganger eecs umich abstract structural file creation block allocation consistently identified file system performance problems user environments compare implementations maintain metadata integrity event system failure require on-disk structures set schemes file system asynchronous writes passes ordering requirements disk scheduler schedulerenforced ordering schemes outperform conventional approach synchronous writes percent metadata update intensive benchmarks suboptimal due inability safely delayed writes ordering required introduce soft updates implementation asymptotically approaches memory-based file system performance percent providing stronger integrity security guarantees unix file systems metadata update intensive benchmarks improves performance factor compared conventional approach introduction file system metadata updates traditionally proceed disk speeds processor memory speeds ousterhout mcvoy seltzer synchronous writes properly order stable storage update sequencing needed maintain integrity event system failure power loss rename operation file adding link removing link system directory block written link removed written file exist system complete integrity individual update atomic partially written disk achieved forcing critical structure fully contained single disk sector disk sector protected error correcting codes flag partially written sector unrecoverable result loss structures loss integrity addition disks start laying sector sufficient power finish restarted protect metadata consistency directory entry reach stable storage directory block refer ordering requirement update dependency writing directory block depends writing block ordering constraints essentially map simple rules reset pointer resource pointer set moving objects re-use resource nullifying previous pointers point structure initialized synchronous writes metadata update ordering variants original unix file system ritchie berkeley fast file system ffs mckusick performance degradation dramatic implementations choose ignore update dependencies pointer newly allocated block added file inode block initialized stable storage ordering enforced system failure result file data previously deleted file presenting integrity weakness security hole allocation initialization synchronous writes degrade performance significantly result unix file system implementations including ffs derivatives force initialization force initialization newly allocated directory blocks order protect integrity directory hierarchy investigate performance cost allocation initializationin comparison ordering schemes previous schemes address performance penalty update ordering generally entail form logging hagmann chutani journal shadowpaging chamberlin ston chao seltzer approaches successfully applied types unix file system writes synchronous asynchronous delayed write synchronousif process issues sends device driver immediately waits complete write asynchronousif process issues immediately wait complete delayed write issued immediately affected buffer cache blocks marked dirty issued background process cache runs clean blocks exploring implementations require on-disk structures large installed base remainder paper organized section describes experimental setup measurement tools base operating system sections describe approaches safe metadata updates including implementations section describes schemes file system asynchronous writes passes ordering restrictions disk scheduler request section describes soft updates file system implementation safely performs metadata updates delayed writes section compares performance schemes section compares important nonperformance characteristics user-interface semantics implementation complexity section draws conclusions discusses avenues future research appendix describes low-level details soft updates implementation experimental apparatus experiments performed ncr mhz intel machine equipped main memory system trace buffer disk drive experiments high performance -inch scsi storage device base operating system unix svr gis production operating system symmetric multiprocessing ufs file system experiments based berkeley fast file system mckusick virtual memory system similar sunos gingell moran file system caching integrated virtual memory system scheduling code device driver concatenates sequential requests disk prefetches sequentially on-board cache command queueing disk utilized important aspect file system reliability performance syncer daemon background process executes regular intervals writing dirty buffer cache blocks syncer daemon unix svr operates differently conventional sync awakens sweeps fraction buffer cache marking dirty block encountered asynchronous write initiated dirty block marked previous pass approach reduce burstiness conventional approach run experiments network disconnected non-essential activity obtain measurements sources unix time utility total execution times cpu times instrumented device driver collect traces including per-request queue service delays traces collected trace buffer mentioned copied separate disk experiment timing resolution approximately nanoseconds tracing alters performance percent assuming trace buffer section benchmarks compare performance metadata update schemes sections concisely quantify performance impacts implementation decisions provide small amounts measurement data descriptions purpose results metadata update intensive benchmarks n-user copy benchmark user concurrently performs recursive copy separate directory tree files totaling storage author home directory n-user remove benchmark user deletes newly copied directory tree datum average independent executions coefficient variation standard deviation scheduler-enforced ordering scheduler-enforced ordering responsibility properly sequencing disk writes shifted disk scheduler generally part device driver file system asynchronous writes augments request ordering requirements examine levels ordering information simple flag list specific dependencies ordering flag straight-forward implementation scheduler-enforced ordering attaches one-bit flag disk request suggested mcvoy write requests previously synchronous ordering purposes writes require ordering respect subsequent updates issued asynchronously ordering flags set disk scheduler modified appropriately sequence flagged requests significant implementation issue semantic meaning flag represents contract file system disk scheduler ordering semantics determine subsequent requests scheduled flagged request previous requests scheduled flagged request restrictive semantics flagged request acts barrier restrictive meanings offer disk scheduler freedom require file system set flag frequently reducing scheduling flexibility general find restrictive flag semantics result improved performance allowing read requests bypass list writes waiting ordering constraints improve performance significantly endangering metadata integrity read requests locations written compare meanings ordering flag full back part ignore infull flagged request acts full barrier previous requests complete scheduled subsequent requests bypass back prevents requests issued flagged request scheduled previous request flagged request re-ordered freely previous non-flagged requests scheme restrictive full requests ordered respect request request issued flag set part relaxes constraints requiring requests issued flagged request scheduled previous non-flagged requests re-ordered freely subsequent requests flagged request flag meaning requests require ordering respect subsequent request flag set addition -nr scheme disk scheduler nonconflicting read requests bypass list writes waiting ordering restrictions finally ignore re-orders requests freely ignoring flag scheme protect metadata integrity include comparison -user copy benchmark figure performance improves reduction flag restrictiveness reducing number requests flag set improve performance increasing disk scheduler freedom re-order cases occur infrequently counter increased restrictiveness flag meaning found allowing reads bypass flagpending writes improves performance significantly disk access times figure directly display impact allowing scheduler greater freedom elapsed times figure 
show translates performance trend restrictive flag semantics results higher performance holds benchmarks comparisons section utilize part-nr -user remove benchmark figure exception rule elapsed times figure system response times observed benchmark user write requests remove directory tree issued large queue builds scheduler effect evident average driver response times times requests issued device driver complete including queue times disk access times seconds shown figure read requests bypass queue user process wastes time waiting benchmark completes waiting driver queue empty writes fit intomain memory -nr option restrictive flag semantics result lower user-observed response times fewer requests interfere read requests note viewed implementation decision part flag semantics ordering required metadata integrity pertains writes full back part part-nr ignore meaning ordering flag elapsed time seconds elapsed time seconds full back part part-nr ignore meaning ordering flag average disk access time average disk access time figure performance impact ordering flag semantics -user copy benchmark service demonstrates performance advantage sub-saturation bursts activity giving preference requests block processes ganger tying disk scheduler hands poor realizing performance improvement behave system activity exceeds memory shown scheduler chains restrictive semantics ordering flag requests constrained unnecessarily flagged writes tagging disk request unique identifier list requests depends list requests complete scheduled false dependencies avoided refer approach scheduler chains similar approaches exist systems mpe-xl file system support ordered sequences user writes writepart full-nr back-nr part-nr ignore meaning ordering flag elapsed time seconds elapsed time seconds part full-nr back-nr part-nr ignore meaning ordering flag average driver response time average driver response time figure performance impact ordering flag semantics -user remove benchmark ahead loggingprotects metadata busch kondoff cao describes method supporting request dependencies intelligent storage controller device driver dealing multiple hosts interconnection network disk scheduler support complete requests depend future requests request depend previously issued requests addition increased scheduler complexity file system maintain information dirty blocks depend outstanding requests cases straight-forward newly updated blocks depend just-issued requests exception rule block de-allocation deallocated block re-used pointer re-initialized stable storage generally de-allocation independent subsequent re-use occur separate system calls examined approaches maintaining required ordering falls back flag-based approach asynchronous write inode indirect block issued part-nr barrier subsequent write request scheduled completes approach maintains information recently freed blocks re-initializedpointer reaches disk blocks reallocated time owner inode indirect block dependent write owner fact make newly allocated block dependent owner prevents data added file due untimely system failure barrier approach simpler implement unnecessary dependencies reduce performance comparison flag meanings find restrictive approach superior performance percent -user remove benchmark approach scheduler chains data reported section avoiding write locks initial implementation scheduler-enforced ordering revealed metadata writes longer synchronous processes wait cases occurs multiple updates file system metadata occur short period time write request issued device driver source memory block write-lockeduntil request completes prevents subsequent updates occurring subsystem hardware accessing data result update wait reach stable storage solution make temporary in-memory copy memory block issuing request copy source write request obviating write-lock original cost block copy operation avoid unnecessary overhead special case allocation initialization reserve zerofilled memory block system booted block source initialization writes figure compares implementations part ordering flag scheme options -nr option block copying -cb part-nr user processes spend time waiting disk requests failing include enhancement greatly reduces benefit critical source data file blocks virtual memory pages bypass safety precaution file blocks difficult performance reliability trade-off judgment call made system implementors copy-on-write superior approach plan investigate alternative expect substantially improvedthroughput increase cpu usage caused memory copying small fraction total time copy-on-write approach memory-friendly approach utilize erase operation write scsi command scsi initializing disk sectors wasting block memory transferring block zeroes host memory disk part part-nr part-cb part-nr ordering flag implementation elapsed time seconds elapsed time seconds part part-nr part-cb part-nr ordering flag implementation average driver response time average driver response time figure implementation improvements ordering flags -user copy benchmark part-nr reads bypass writes waiting due ordering restrictions part-cb block copy scheme avoid write locks part-nr combines enhancements dark region elapsed time bar represents total cpu time charged benchmark processes reason performance comparisons earlier subsections block copying implementation cpu time increases caused block copying small tend time idle processes waiting disk figure shows driver response times implementations find queue grows large processes generate requests quickly disk service -user remove benchmark figure general trends performance differences substantial queueing delays larger seconds part-nr observe general behavior scheduler chains block copying -nr holds meaning part part-nr part-cb part-nr ordering flag implementation elapsed time seconds elapsed time seconds part part-nr part-cb part-nr ordering flag implementation average driver response time average driver response time figure implementation improvements ordering flags -user remove benchmark part-nr reads bypass writes waiting due ordering restrictions part-cb block copy scheme avoid write locks part-nr combines enhancements dark region elapsed time bar represents total cpu time charged benchmark processes scheduler chains reduces elapsed time percent -user copy compute comm sync barrier switch idle slowdown compute comm sync news switch idle slowdown compute comm sync transpose switch idle slowdown benchmark percent -user remove benchmark delayed metadata writes delayed metadata writes associate additional information in-memory metadata detailing ordering constraints stable storage updates complete structural change file system modifies in-memory copies affected metadata delayed writes updates dependency information dirty metadata blocks flushed syncer daemon section ordering constraints upheld process delayed metadata writes substantially improve performance combining multiple updates smaller quantity background disk writes savings sources multiple updates metadata component removal recently added directory entry significantly multiple independent updates block metadata files added single directory subsection briefly describes original approach flawed subsection describes current implementation cycles aging problems began work envisioned dynamically managed dag directed acyclic graph dirty blocks write requests issued writes depend complete practice found difficult model maintain susceptible cyclic dependencies aging problems blocks consistently dependencies written stable storage difficulties relate granularity dependency information blocks read written disk multiple metadata structures inodes directory fragments generally multiple dependency causing components block pointers directory entries result originally independent metadata easily dependency cycles excessive aging detecting handling problems increases complexity reduces performance soft updates identified coarse-grain dependency information main source cycles aging recent implementation refer soft updates maintains dependency information fine granularity information individual metadata update indicating update depends block dirty metadata written time long updates block pending dependencies temporarily undone 
rolled back block written disk consistent respect current on-disk state disk write completes undone updates re-established in-memory block accessed approach aging problems occur dependencies added existing update sequences dependency cycles occur single sequence dependent updates cyclic fact sequences original synchronous write approach main structural requiring sequenced metadata updates block allocation direct indirect block de-allocation link addition file creation link removal implement block allocation link addition undo redo approach outlined block de-allocation link removal defer freeing resources newly reset pointer written stable storage sense deferred updates undone disk writes depend complete disk write completes processing needed update remove dependency structures redo undone deal deferred work block de-allocation file removal implementation soft updates requires method performing small tasks background simple made disk completion interrupt service routine isr calls pre-defined procedure higher-level module issued request task block wait resource lock worse uncached disk block handled task handled isr preferably background process syncer daemon section purpose tasks require non-trivial processing appended single workitem queue syncer daemon awakens services workitem queue normal activities appendix describes soft updates implementation detail including main structural supported performance comparison subsections compare performance ordering schemes baseline goal ignore ordering constraints order delayed writes metadata updates baseline performance lack reliability delayed mount option ohta similar memory-based file system mckusick conventional scheme synchronous writes sequence metadata updates scheduler flag data represent part-nr scheduler-enforced ordering flag scheme scheduler chains data represent performing scheme section schedulerenforced ordering schemes block copying enhancement section soft updates data current implementation metadata throughput figure compares metadata update throughput supported implementations function number concurrent users user works separate directory result create throughput improves number users cpu time spent checking directory contents conflicts scheduler concurrent users throughput files order soft updates scheduler chains scheduler flag conventional file creates concurrent users throughput files order soft updates scheduler chains scheduler flag conventional file removes concurrent users throughput files order soft updates scheduler chains scheduler flag conventional file create removes figure metadata update throughput files data point files split users average independent executions coefficients variation allocation initialization enforced soft updates flag reduces metadata update response times compared conventional substantially improve throughput scheduler chains doubling file removal throughput users order soft updates outperform schemes differences increase level concurrency power delayed metadata writes figure created file immediately removed order soft updates proceed memory speeds achieving times throughput schemes cases soft updates performance percent order metadata intensive benchmarks table shows performance data -user copy benchmark order decreases elapsed times percent number disk requests percent compared conventional allocation initialization scheduler flag scheduler chains decrease elapsed times percent affect number disk requests performance soft updates percent order elapsed time number disk requests performance cost allocation initialization benchmark ranges percent soft updates percent conventional performance differences extreme file removal table consists metadata updates note soft updates elapsed times lower order benchmark due deferred removal approach soft updates order magnitude decrease disk activity soft updates verses scheduler chains demonstrates power delayed metadata writes lengthy response times scheduler-enforced ordering schemes caused large queues dependent background writes form device driver andrew benchmark table compares schemes original andrew file system benchmark howard consists phases create directory tree copy data files examine status file read byte file compile files expected significant differences metadata update intensive phases read-only phases practically indistinguishable compute intensive compile phase marginally improved percent non-conventional schemes compile phase dominates total benchmark time aggressive time-consuming compilation techniques slow cpu standards ordering alloc elapsed time percent cpu time disk response scheme init seconds order seconds requests time avg conventional scheduler flag scheduler chains soft updates order table scheme comparison -user copy datum average independent executions elapsed times averages users coefficients variation cpu times sums users coefficients variation disk system statistics system-wide coefficients variation ordering elapsed time percent cpu time disk response scheme seconds order seconds requests time avg compute comm sync news spin switch idle slowdown compute comm sync news spin switch idle slowdown compute comm sync news spin switch idle slowdown conventional scheduler flag scheduler chains soft updates order table scheme comparison -user remove datum average independent executions elapsed times averages users coefficients variation cpu times sums users coefficients variation disk system statistics system-wide coefficients variation ordering create copy read read compile total scheme directories files inodes files conventional scheduler flag scheduler chains soft updates order table scheme comparison andrew benchmark seconds represents average independent executions values parens standard deviations allocation initialization enforced soft updates concurrent scripts throughput scripts hour order soft updates scheduler chains scheduler flag conventional figure scheme comparison sdet data point average independent executions coefficients variation allocation initialization enforced soft updates sdet figure compares schemes sdet spec sdm suite benchmarks benchmark gaede gaede concurrently executes scripts user commands designed emulate typical software-development environment editing compiling file creation unix utilities scripts generated randomly predetermined mix functions reported metric scripts hour function script concurrency scheduler flag outperforms conventional percent scheduler chains additional percent improvement order outperforms conventional percent soft updates throughput percent order non-performance comparisons file system semantics synchronous writes sequence metadata updates imply synchronous file system semantics general write series metadata updates asynchronous delayed cases file system call returns control caller guarantee change persistent link addition block allocation update adds pointer directory block inode indirect block requested change permanent system call returns link removal block de-allocation update modifies free maps system call returns link permanently removed blocks freed re-use scheduler-enforced ordering schemes freed resources immediately re-use links pointers permanently removed system call returns soft updates true freed resources re-use re-initialized inode indirect block reaches stable storage calls syncio flag tells file system guarantee permanent returning schemes support interface scheduler-enforced ordering schemes encounter lengthy delays long list dependent writes formed augment additional file system calls link addition link removal flag order support lock files implementation complexity schemes compare conventional synchronous write approach straight-forwardto implement moving ordering flag scheme straightforward synchronous writes asynchronous flag set device driver required lines code scheduler-enforced ordering specific request dependencies considerably difficult implementation required lines code device driver support lines file system support specific remove dependencies adds additional lines code blockcopy enhancement section required additional lines code implementation soft updates consists lines code restricted file system buffer cache modules learned key lessons initial implementation author completed soft updates implementation weeks including 
debugging conclusions future work synchronous writes order metadata updates identified file system performance problem ousterhout mcvoy seltzer direct measurement compared alternative implementations schemes file system relies disk scheduler appropriately order disk writes outperform conventional approach percent cases maximum observed difference percent withthis improvement schemes software locking schemes lock files encounter surprises fail achieve performance levels delayed writes introduced mechanism soft updates approaches memory-based file system performance percent providing stronger integrity security guarantees allocation initialization unix file systems translates performance improvement factor cases maximum observed difference factor implementations compared paper prevent loss structural integrity requires assistance provided fsck utility unix systems recovering system failure power loss file system time-consuming process reducing data system availability investigating soft updates extended provide faster recovery experiments performed unix system results applicable wider range operating environments file system operating system address issue integrity maintenance mpe-xl cms windows database techniques logging shadow-paging vms rely carefully ordered synchronous writes directly results soft updates mechanism appears promising plan compare popular methods protecting metadata integrity non-volatile ram nvram logging shadow-paging nvram greatly increase data persistence provide slight performance improvements compared soft updates reducing syncer daemon activity expensive write-ahead logging protection soft updates delayed group commit achieve performance levels shadow-paging maintain integrity difficult delayed writes combined soft updates late binding disk addresses logical blocks chao provide high performance log-structured file system seltzer special case shadow-paging integrity grouping writes atomically checksum enforce atomicity large writes resulting log-structuring utilize disk bandwidth required cleaning activity reduces performance significantly hope make non-proprietary components implementations term interested contact authors acknowledgements jay lepreau shepherd paper john wilkes bruce worthington anonymous reviewers directly helping improve quality paper remote hands summer months carlos fuentes finally research group fortunate financial technical support industrialpartners including gis hewlett-packard intel motorola ses hal mti dec gis enabled research extremely generous equipment gifts allowing generate experimental kernels source code high performance disk drives experiments donated hewlett-packard busch busch kondoff disc caching system processing units family computers journal february cao cao lim venkataraman wilkes tickertaip parallel raid architecture acm isca proceedings chamberlin chamberlin astrahan history evaluation system communications acm chao chao english jacobson stepanov wilkes mime high-performance parallel storage device strong recovery guarantees hewlett-packard laboratories report hpl-csp- rev november chutani chutani anderson kazar leverett mason sidebotham episode file system winter usenix proceedings january gaede gaede tools research computer workload characterization experimental computer performance evaluation ferrari spadoni gaede gaede scaling technique comparing interactive system capacities international conference management performance evaluation computer systems ganger ganger patt process-flow model examining performance system point view acm sigmetrics proceedings gingell gingell moran shannon virtual memory architecture sunos summer usenix proceedings june hagmann hagmann reimplementing cedar file system logging group commit acm sosp proceedings published acm operating systems review november hewlett-packard company inch scsidisk drive technical manual edition september part howard howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system ieee transactions computer systems february journal ncr corporation journaling file system administrator guide release ncr document april kondoff kondoff mpe data management system exploiting precision architecture generation commercial computer systems ieee compcon proceedings mckusick mckusick joy leffler fabry fast file system unix acm transactions computer systems august mckusick mckusick karels bostic pageable memory based filesystem ukuug summer conference pub united kingdom unix systems user group buntingford herts july mcvoy mcvoy kleiman extent-like performance unix file system winter usenix proceedings january moran moran sunos virtual memory implementation euug conference proceedings spring ohta ohta tezuka fast tmp file system delay mount option summer usenix proceedings june ousterhout ousterhout aren operating systems faster fast hardware summer usenix proceedings june ritchie ritchie thompson unix time-sharing system bell system technical journal july august ruemmler ruemmler wilkes unix disk access patterns winter usenix proceedings january scsi small computer system interfaceansi draft revision march seltzer seltzer bostic mckusick staelin implementation log-structured file system unix winter usenix proceedings january ston stonebraker design postgres storage system large database conference september soft updates implementation details appendix low-level details soft updates implementation assumes information section read basic structure storing dependency information sets previous pointers internal indexing purposes unique identifier type types space additional type-specific values found organizational dependency structures inodes general file blocks pages system separate actual file system metadata file blocks in-core inode structures re-used losing corrupting dependency information directory block inode accessed check outstanding dependency structure stored hash table make undone updates reflected copy visible users dependency structure types serve purpose type-specific values identify owner pointer virtual file system vfs structure inode number logical block number file inode organizational structure heads lists dependency structures simply waiting metadata written disk support undo redo portions owner block allocation block fragment direct indirect allocated file block pointer written stable storage block initialized allocation initializationdependency introduction time allocation update metadata normal fashion allocate allocsafe dependency structure block initialized memory allocated alloc dependency structure allocdirect allocindirect metadata block pointer inode indirect block allocsafe-specific pointer companion alloc structure alloc-specific values exact location block pointer metadata structure pointer pointer null fragment extended size newly allocated block fragment extension pointer allocsafe structure state state dependency outstanding in-memory copy up-to-date true allocation dependencies point implementations direct indirect pointers differ describe support pointers located inode initialized block written disk allocdirect state modified allocsafe structure freed time inode written allocdirect structure freed inode written newly allocated block initialized allocation undone accomplished replacing block pointer case fragment extension reducing file length appropriately values changed buffer cache block modifyingthe in-core inode structure redo operations in-core structure holding inode pending allocdirect dependencies re-used file inode number uniquely identifies inode file system simple fast management policy dependency structures needed allocate page kernel memory break list structures freeing structure consists adding list allocation simply takes structure list file system copies inode contents buffer cached in-core internal inode structure accessing inode structure manipulated file system separate source block disk writes system inode brought in-core values replace inode marked dirty happen seconds inode dependency structure added workitem queue service function consists simply bringing inode in-core pointers indirect blocks implement things differently indirect block block pointers making inefficient traverse list per-pointer structures undo redo updates associate 
indirdep dependency structure indirect block pending allocation dependencies block memory equal size indirect block allocated structure initialized copy safe contents writing indirect block disk safe block source newly allocated disk block initialized allocsafe structure freed allocindirect structure update safe copy freed remaining dependencies indirect block written indirdep structure safe block freed indirect blocks generally represent small fraction cache contents force stay resident dirty pending dependencies avoid additional undo redo embellishments block de-allocation de-allocated block re-used previous pointer permanently reset achieve freeing block setting bits free map reset block pointer reaches stable storage block de-allocation required block pointer values freeblocks dependency structure size fragment outstanding alloc allocsafe dependencies de-allocated blocks freed point longer serve purpose block pointers reset inode indirect block release blocks modified metadata written disk freeblocks structure added workitem queue special case extending fragment moving data block de-allocating original fragment inode appropriately modified allocdirect dependency clears blocks freed syncer daemon code paths original file system dependency structures owned blocks considered complete point handled applies directory blocks link addition link added directory block written disk pointed-to inode possibly reached stable storage link count incremented undo redo approach allocation initialization provide protection in-memory copies directory block inode modified normal fashion addition addsafe dependency structure allocated inode add structure allocated directory block addsafe-specific pointer add structure add-specific values offset directory job opening barrier job job spin processor processor processor processor job job job job spin read spin spin barrier barrier closing barrier completescompletes load imbalance entry block pointed-to inode number pointer addsafe structure state state serves purposes allocation initialization inode written addsafe structure freed state modified appropriately directory block accessed add structure freed directory block written inode reaches stable storage link addition undone replacing inode number directory entry indicating entry unused directory block write completes correct inode number replaces directory block contents out-of-date inhibit accesses reads writes disk write mark directory block dirty immediately disk write completes system re-use cache block page directory block accessed make directory entries up-to-date accessed seconds dependency structure added workitem queue service function simply accesses directory block marks dirty link removal link removed link count inode decremented modified directory block reaches disk provide protection deferred approach block de-allocation directory entry removed normal fashion directory entry pending link addition dependency add addsafe structures removed link removal proceeds unhindered add remove serviced disk writes remove dependency structure allocated directory block remove-specific values inode number pointer vfs structure allowing previously pointed-to inode identified directory block written disk remove structure added workitem queue service function consists decrementing link count link count inode freed normal code paths blocks de-allocated link count identifies number directory entries pointing inode 
compute comm sync news switch idle slowdown compute comm sync news switch idle slowdown compute comm sync barrier switch idle slowdown compute comm sync news switch idle slowdown compute comm sync transpose switch idle slowdown compute comm sync barrier switch idle slowdown compute comm sync news switch idle slowdown compute comm sync transpose switch idle slowdown compute comm sync barrier switch idle slowdown compute comm sync news switch idle slowdown compute comm sync transpose switch idle slowdown barrier imbalance imbalanceimbalance latency latency high low medium context-switch computation granularity low low high coarsefine highlowhighlowhighlow high news imbalance imbalanceimbalance latency latency high low medium context-switch computation granularity low low high coarsefine highlowhighlowhighlow high transpose imbalance imbalanceimbalance latency latency high low medium context-switch computation granularity low low high coarsefine highlowhighlowhighlow high 
laboratories technical report sufficient processor power disk scheduling based rotational position disk arm position shown provide improved performance taxonomy algorithms based shortest access time satf developed explored simulations access-time based algorithms match outperform seek-time studied aged shortest access time asatf forms continuum fcfs satf equal superior response time variance entire range authors notes writing paper work margo seltzer peter chen john ousterhout seltzer set similar ideas published year wrote report people citing work cited original document physical cut paste graphics electronic version includes scanned images original document marked vertical bar left margin copyright hewlett-packard company rights reserved disk scheduling algorithms based rotational position david jacobson john wilkes concurrent systems project palo alto technical report hpl csp rev february revised march issued electronically introduction widely algorithm scheduling disk requests scan algorithm disk arm alternatively moved edge disk back arm crosses cylinder requests serviced goal disk scheduling algorithms reduce disk arm motion makes sense seek times dominant performance factor impossible rotational position processing power interrupt structure host computer limits amount responsiveness computation disk driver conditions require reconsidering design principle years rotational speed disks increased slightly full stroke seek time shortened significantly table shows sample values late vintage drive modern hewlett-packard smaller diameter date introduction processor speed increased tremendously availability abundant cycles enables scheduling algorithms finally work initially motivated datamesh project hewlettpackard laboratories research effort developing parallel storage server smart storage surface array closely-coupled disk processor modules connected fast reliable mesh module disk mips processor primary ram close coupling cpu disk means processor rotational position disk time request executed channel contention introduce uncertainty conditions make practical design disk scheduling algorithm based rotational position primary consideration begun context datamesh ideas directly applicable systems including device drivers smart disk controllers previous work simplest algorithm service requests order arrive served fcfs algorithm poor performance lightest loads wastes lot time moving areas disk relative time spent transferring data seek time data ibm inferred seek time versus distance graph frank disk diameter rotation speed full-stroke seek time rotations ibm rpm rpm rpm rpm rpm scan request queue request nearest head positioned process traditionally nearest calculated difference cylinder numbers technique shortest seek time sstf periods high load requests arriving area disk arm stick region leaving requests waiting long time starvation problem absence complete starvation phenomenon increases service time variance scan algorithm proposed denning denning sweeps back disk stopping cylinder pending requests variation scan sweep direction end disk reached seek back beginning wong shown bidirectional scan good unidirectional independent arrivals wong bsd version unix one-way algorithm gave behavior readahead leffler scan suffers starvation lesser degree sstf authors proposed adaptations overcome problem approach scan delay recent arrivals sweep gotlieb purported unix system implementations control starvation limiting number requests cylinder serviced moving performance fcfs sstf scan extensively studied literature denning coffman gotlieb oney wilhelm hofri coffman geist daniel proposed continuum algorithms called parameter geist idea pick request sstf add penalty times total number cylinders reversing direction sstf scan suggest good compromise performs scan avoids high variance starvation difficulties sstf time requests arriving disk queue presence large queue lengths surprisingly common figure plots queue lengths experienced ten minute trace time sharing system disk model analysis algorithms presented parameterized performance scsi disk drive drive platters data surfaces physical cylinders sectors bytes track total formatted capacity platters rotate rpm settling time switching tracks cylinder notation algorithms confused two-way algorithm called scan word elevator leffler elevator one-way algorithm unix registered trademark united states countries licensed exclusively open company limited figures presented derived measuring sample drive agree published product specifications long seeks modern disks accelerate disk arm maximum velocity coast constant velocity decelerate back finally settle desired track short seeks arm accelerates halfway point decelerates settles giving seek time form seek distance cylinders long seeks arm reaches terminal velocity seek time form functions form fit measured performance data results seek time milliseconds shown graphically figure rotationally-sensitive scheduling algorithms total access time combines seek rotational latencies traditionally represented rotational delay experienced head arrived correct cylinder modern disks seek time rotational latency natural formula shows combined cost rotational latency basis convenient scale access times time disk rotate sector transfers start stop sector boundaries units equation access time repositioning cost head movement figure frequency queue lengths traverses cylinders sectors number sectors track current head position seek time shown demonstrate graphically significant effect rotation latency behavior access time figure plots access time function cylinder horizontal sector vertical offsets notice large area disk accessible rotational latency suggests strongly rotational delay considered equal footing seek time disk scheduling decision remainder section introduces scheduling policy subsequent sections discuss variations present result simulations performance optimal access time request disk optimal access time oat algorithm considers combinations requests queue performs optimal scheduling decision requests efficient algorithm optimal scheduling result oat theoretical upper bound scheduling performance fortunately greedy algorithms make choice choice remains good approximations fully optimal calculation test experimentally issued clumps random requests greedy scheduler optimal scheduler compared average execution time resulting schedules clumps length greedy algorithm resulted schedules longer optimal problem special case well-known traveling salesman problem decision problem exist schedule completed stated bound np-complete special cases found np-complete garey conjecture figure seek time versus seek distance cylinders extended simple experiment short-sighted oat algorithm optimally scheduled batches requests head queue compared full greedy algorithm remaining requests queue short-sighted greedy algorithm looked requests queue sliding window oat algorithm pulled request queue scheduled dispatched table shows results relative full greedy algorithm examining entire queue enormous win compared optimal scheduling mid-sized batches requests reason oat pick distant request order gain time servicing requests occur isolation requests arriving service batching algorithm taking advantage produce schedule results pursue oat algorithms shortest access time shortest access time satf algorithm approximation oat greedy scheduler access time scheduling parameter seek distance direct analogue sstf algorithm relative performance short-sighted oat sliding window oat short-sighted greedy full greedy figure access time function cylinder sector sector left cylinder middle cylinder times uniform horizontally region implementation satf closely sstf queue pending requests scanned access time calculated request head position disk idle calculated head position end current request 
easily implemented assistance small static array holding seek times seek distance reduce number requests scanned dividing disk number bins figure encompasses connected region access-time graph shown figure request arrives put bin target portion disk time calculate schedule bins searched order depends current head position order pre-computed stored small array indexed head position figure shows bin search order head sector middle cylinder cells lie sides head trajectory requests accessed revolution cope threshold worst cell calculated cell worst time good request cell request split cell met limit search continues split cells numbered good requests bad avoid scan keeping track request bad side split cell request replaces search terminates cell encountered worst cell assume simplicity disk command queueing worse reordering ignore effects spared sectors tracks figure disk divided cells ordering happen cell scan list bin orderings worst values small held -element grid algorithm figure illustrates detail taccess time access request head position reducing starvation starvation requests continue arrive chosen servicing queue avoid refuse temporarily admit requests queue accumulated side future time considered scheduling pool requests scheduled decreases size likelihood disk optimally utilized declines population candidates small selection requests fit neatly holes schedule maintaining high disk efficiency requires pool candidates scheduling large avoiding starvation requires requests allowed indefinitely delay reduces efficient disk result compromise required sstf satf subject starvation conditions engender complex nearness dynamic function rotational position effect section introduces number variations satf address starvation problem limiting time request remain unserved reduce variance service time eliminate starvation expense maximum throughput approach presented responds starvation forcibly scheduling oldest request variations theme mollify tendency approach degenerate fcfs highload conditions starvation approach avoids starvation batching requests preventing considered batch processed variations theme requests scheduled long delay execution current batch bestsofar nil current head position cell order current head position request cell taccess taccess bestsofar bestsofar endfor taccess bestsofar tworst cell exit returning bestsofar endfor exit returning bestsofar figure efficient satf algorithm finally approach avoids starvation modifying criteria select deserving request processed adding age-related amount calculated access time selecting selected forcibly scheduling oldest request shortest access time urgent forcing satfuf algorithm shown figure forcibly schedules oldest request executed immediately aged minimize effect disk throughput requests scheduled front scheduled doesn delay oldest request symbols addition length length request sectors simulations showed satfuf good algorithm requests spliced front oldest request starvation behavior triggered performance approximates fcfs typically arrival rate exceeded capacity fcfs clear backlog response time recovers result dropped pure satfuf set simulated algorithms modification satfuf forcibly select oldest requests schedule slotting additional requests accommodated delaying schedule called shortest access time urgent forcing satfuf satfuf family batch algorithms discussed section approach improving satfuf starts selecting oldest request aged inserts whole-rotations worth slack executed requests scheduled intervening time gap requiredreq nil current head position oldest oldest request nil bestsofar cell order current head position request cell nil taccess length taccess taccess make reach losing revolution taccess taccess bestsofar bestsofar endfor bestsofar worsttime cell exit returning bestsofar endfor exit returning bestsofar figure satfuf algorithm number requests swallowed parameterized algorithm called shortest access time delayed forcing satfdf variations schedule inserted requests aren forcibly scheduled perfect optimization rotation times delay entire request queue eligible consideration factorial behavior renders approach computationally intractable approximate optimal assignment greedy algorithm greedy algorithm deadline forced request begin determine cutoff point knowledge end ideal head positioned close access time start forcible request practice uniformlydistributed random repositioning delay introduced greedy algorithms working forward current head position backwards end point computed portion forward algorithm spliced portion backwards point minimizes resulting dead time coax head forced request heuristic guide greedy algorithm selection algorithm called shortest access time guided delayed forcing satfgdf note variation require examining entire queue scheduling step pseudocode figure satfgdf static variable deadline survives request number revolutions oldest scheduled future typically hueristic function larger returns desirable choice static deadline initially nil oldest request current time sectors current head position deadline nil deadline taccess bestsofar request taccess length taccess deadline make reach oldest deadline deadline-t bestsofar deadline-t bestsofar endfor bestsofar deadline nil exit returning bestsofar figure satfgdf algorithm possibilities heuristic function function maximized yield good schedule number additional requests reached end request reaching remaining time make complexity selection request quadratic number remaining requests linear complex simply returning taccess virtually identical satfdf coax head oldest request heuristic appealing area disk reached remaining time figure illustrates idea plot area heuristic curves distance deadline plotted starting cylinder number family parabolas area metric directly moderately expensive compute store approximation developed effect edge disk quadratic function distance area heuristic computed quadratic function fit figure coefficients stored arrays indexed remaining time sectors linear term quadratics centered batch algorithms batch algorithms prevent starvation temporarily preventing requests joining queue delaying indefinitely batch algorithms continuously two-mode fashion invoked starvation observed bring check prevent occurrences two-mode behavior attempts benefit high throughput satf limiting damage caused starvation batch technique figure area accessible disk space simplest algorithm batched shortest access time bsatf operates processing requests queue completion admitting greedy algorithm optimization variant relaxes requests rule called leaky batched shortest access time lbsatf bsatf batch acquired scheduled greedy algorithm projected end time remembered deadline request arrives added batch schedule found complete existing requests existing deadline request put batch bsatf forward progress guaranteed requests accommodated satfuf variety two-moded batch algorithm modified selection criteria avoid starvation needed oldest requests removed strictly positive rate queue choice rate influences long request sit queue higher rate shorter time request stuck weight scheduling algorithm choice deserving request account age aged shortest access time asatf algorithm calculates merit request queue age calculated access time modified proportional time age request waiting queue units sectors weighting factor varied pure satf infinity pure fcfs sense similar algorithms geist covers spectrum sstf scan figure approximations area heuristic times deadline performance simulator constructed model behavior disk including seek time rotational latency transfer time run warmup requests generated real requests indefinite number warm requests real requests tracked measured simulation terminated real requests completed data point response time curves corresponds results twenty experiments requests uniformly distributed disk exponentially distributed interarrival times requests uniform size transfer size common unix file 
systems simulation carried range arrival rates low capacity scheduling algorithm handle arrival rate service time percentile service time distribution collected -bin histogram response times collected percentile arrived interpolation histogram simulations fcfs scan sstf satf satfuf satfdf satfgdf quadratic heuristic asatf conducted results shown figure method replications pawlikowski estimate confidence intervals cases narrow figure shows graphically plotting error bars representing confidence intervals response time arrival rate curve asatf algorithm results algorithms perform equally low arrival rates queue lengths short fcfs good algorithm algorithms seek time basis distinctly fcfs ranked metrics performances figure service time distributions asatf range arrival rates showing confidence intervals throughput scan sstf starvation resistance scan sstf rotational-sensitive algorithms performed seek-time based algorithms increasing order goodness metrics shown algorithms performed throughput satfdf satfgdf asatf satf starvation resistance satfdf satfgdf satf asatf asatf shows performance chose exploring performance asatf algorithm function high-load case arrival rate requests figure plots standard deviation percentile rankings obtained comparing request rates sustainable fixed high response times means percentiles figure upper percentile lower service times scheduling algorithms range arrival rates response time range values performance measures good values range conservatively choose conclusion disk scheduling algorithms based access time proposed promising studied well-known algorithms simulation algorithms perform low arrival rates higher arrival rates access-time algorithms match outperform seek-time studied algorithm asatf sustain higher throughput sstf response time terms request rates percentile response times asatf outperforms sstf request rate asatf worse satf response time percentile response time finally asatf sustain higher throughput rate scan maintaining percentile response time seconds percentile response time seconds asatf sustain throughput higher scan heretofore goal disk scheduling algorithms minimize disk arm motion work shows goal minimizing access time taking account rotational position improve maximum disk throughput modern disks simulation results reported based load exponential interarrival times uniform distribution requests disk couple months plan test algorithms traces real system activity coffman coffman klimko ryan analysis scanning policies reducing disk seek times siam journal computing september coffman coffman hofri expected performance scanning disks siam journal computing february denning denning effects scheduling file memory operations proceedings afips spring joint computer conference atlantic city jersey april pages april frank frank analysis optimization disk storage devices time-sharing systems journal acm october garey garey johnson computers intractability guide theory npcompleteness freeman geist geist daniel continuum disk scheduling algorithms acm transactions computer systems february gotlieb gotlieb macewen performance movable-head disk storage systems journal acm october hofri hofri disk scheduling fcfs sstf revisited communications acm november figure standard deviation percentile response time versus asatf arrivals leffler leffler mckusick karels quarterman design implementation bsd unix operating system addison-wesley oney oney queueing analysis scan policy moving-head disks journal acm july pawlikowski pawlikowski steady-state simulation queueing processes survey problems solutions computing surveys june seltzer margo seltzer peter chen john ousterhout disk scheduling revisited proceedings winter usenix technical conference washington january pages wilhelm wilhelm anomaly disk scheduling comparison fcfs sstf seek scheduling empirical model disk accesses communications acm january wong wong algorithmic studies mass storage systems computer science press taft court rockville 
technical report rev file system design nfs file server appliance dave hitz james lau michael malcolm networkappliance presented january usenix winter san francisco california copyright usenix association reproduced permission file system design nfs file server appliance rev network appliance corporation-printed usa north bemardo avenue mountain view rights reserved part covered copyright reproduced form means-graphic electronic mechanical including photocopying recording taping storage electronic retrieval system-without prior written permission copyright owner network appliance reserves change products time notice network appliance assumes responsibility liability arising products expressly agreed writing network appliance purchase product convey license user patent rights trademark rights intellectual property network appliance product protected patents foreign patents pending applications restricted rights legend duplication disclosure government subject restrictions set subparagraph rights technical data computer software clause dfars october june trademark acknowledgment faserver wafl snapshot trademarks network appliance sunos nfs pc-nfs registered trademarks sun microsystems unix registered trademark united states countries licensed exclusively open company limited ethernet registered trademark xerox corporation intel registered trademark intel corporation products services mentioned document identified trademarks service marks product names designated companies market products inquiries trademarks made directly companies file system design nfs file server appliance rev table contents abstract introduction introduction snapshots user access snapshots snapshot administration wafl implementation overview meta-data lives files tree blocks snapshots file system consistency non-volatile ram write allocation snapshot data structures algorithms block-map file creating snapshot performance conclusion bibliography biographies file system design nfs file server appliance rev abstract network appliance recently began shipping kind network server called nfs file server appliance dedicated server sole function provide nfs file service file system requirements nfs appliance general-purpose unix system nfs appliance optimized network file access appliance easy paper describes wafl write file layout file system designed specifically work nfs appliance primary focus algorithms data structures wafl implement snapshots read-only clones active file system wafl copy-on-write technique minimize disk space snapshots consume paper describes wafl snapshots eliminate file system consistency checking unclean shutdown file system design nfs file server appliance rev introduction appliance device designed perform function recent trend networking provide common services appliances generalpurpose computers instance special-purpose routers companies cisco bay networks replaced general-purpose computers packet routing general purpose computers originally handled routing examples network appliances include network terminal concentrators network servers network printers type network appliance nfs file server appliance requirements file system operating nfs appliance general purpose file system nfs access patterns local access patterns special-purpose nature appliance affects design wafl write file layout file system network appliance corporation faserver nfs appliance wafl designed meet primary requirements provide fast nfs service support large file systems tens grow dynamically disks added provide high performance supporting raid redundant array independent disks restart quickly unclean shutdown due power failure system crash requirement fast nfs service obvious wafl intended nfs appliance support large file systems simplifies system administration allowing disk space belong single large partition large file systems make raid desirable probability disk failure increases number disks large file systems require special techniques fast restart file system consistency checks normal unix file systems unacceptably slow file systems grow nfs raid strain write performance nfs servers store data safely replying nfs requests raid read-modify-write sequence maintain parity patterson led non-volatile ram reduce nfs response time write-anywhere design wafl write disk locations minimize raid write performance penalty write-anywhere design enables snapshots turn eliminate requirement time-consuming consistency checks power loss system failure file system design nfs file server appliance rev introduction snapshots wafl primary distinguishing characteristic snapshots read-only copies entire file system wafl creates deletes snapshots automatically prescheduled times snapshots on-line provide easy access versions files snapshots copy-on-write technique avoid duplicating disk blocks snapshot active file system blocks active file system modified removed snapshots blocks begin consume disk space users access snapshots nfs recover files accidentally changed removed system administrators snapshots create backups safely running system addition wafl snapshots internally restart quickly unclean system shutdown user access snapshots directory file system hidden sub-directory named snapshot users access contents snapshots nfs suppose user accidentally removed file named todo recover shows list versions todo saved snapshots spike -lut snapshot todo -rw-r--r-hitz oct snapshot nightly todo -rw-r--r-hitz oct snapshot hourly todo -rw-r--r-hitz oct snapshot hourly todo -rw-r--r-hitz oct snapshot nightly todo -rw-r--r-hitz oct snapshot nightly todo option shows todo access time set time snapshot created user recover recent version todo copying back current directory spike snapshot hourly todo file system design nfs file server appliance rev snapshot directories hidden sense show directory listings snapshot visible commands find report files expected commands -rf fail files snapshots read-only removed snapshot administration faserver commands system administrators create delete snapshots creates deletes snapshots automatically default faserver creates hourly snapshots times day nightly snapshot night midnight hourly snapshots days nightly snapshots week creates weekly snapshot midnight sunday weeks file systems change quickly schedule consume disk space snapshots deleted sooner snapshots hours users notice immediately removed important file file systems change slowly make sense snapshots on-line longer typical environments keeping snapshots week consumes percent disk space file system design nfs file server appliance rev wafl implementation overview wafl unix compatible file system optimized network file access ways wafl similar unix file systems berkeley fast file system ffs mckusick transarc episode file system chutani wafl block-based file system inodes describe files blocks fragments wafl inode block pointers blocks belong file unlike ffs block pointers wafl inode refer blocks level inodes files smaller block pointers point data blocks inodes files smaller point indirect blocks point actual file data inodes larger files point doubly indirect blocks small files data stored inode place block pointers meta-data lives files episode wafl stores meta-data files wafl meta-data files inode file inodes file system block-map file identifies free blocks inode-map file identifies free inodes term map bit map files bit entry block-map file format detail root inode inode file files block map file inode map file files file system figure wafl file system tree blocks root inode describes inode file top meta-data files regular files underneath file system design nfs file server appliance rev keeping meta-data files wafl write meta-data blocks disk origin 
wafl stands write file layout write-anywhere design wafl operate efficiently raid scheduling multiple writes raid stripe avoid -towrite penalty raid incurs updates block stripe keeping meta-data files makes easy increase size file system fly disk added faserver automatically increases sizes meta-data files system administrator increase number inodes file system manually default small finally write-anywhere design enables copy-on-write technique snapshots snapshots work wafl write data including meta-data locations disk overwriting data wafl stored meta-data fixed locations disk tree blocks wafl file system thought tree blocks root tree root inode shown figure root inode special inode describes inode file inode file inodes describe rest files file system including block-map inode-map files leaves tree data blocks files figure detailed version figure shows files made individual blocks large files additional layers indirection inode actual data blocks order wafl boot find root tree exception wafl write-anywhere rule block root inode live fixed location disk wafl find file system design nfs file server appliance rev root inode inode file indirect blocks inode file data blocks regular file indirect blocks regular file data blocks block map file inode map file random small file random large file figure detailed view wafl tree blocks snapshots understanding wafl file system tree blocks rooted root inode key understanding snapshots create virtual copy tree blocks wafl simply duplicates root inode figure shows works figure simplified diagram file system figure leaves internal nodes tree inodes indirect blocks figure shows wafl creates snapshot making duplicate copy root inode duplicate inode root tree blocks representing snapshot root inode represents active file system snapshot inode created points disk blocks root inode brand snapshot consumes disk space snapshot inode figure shows user modifies data block wafl writes data block disk active file system point block snapshot original block unmodified disk time files active file system modified deleted snapshot blocks longer active file system rate files change determines long snapshots line consume unacceptable amount disk space file system design nfs file server appliance rev snapshot root inode abcde snapshot root inode abcde snapshot block update root inode abcde snapshot figure wafl creates snapshot duplicating root inode describes inode file wafl avoids changing blocks snapshot writing data locations disk interesting compare wafl snapshots episode fileset clones duplicating root inode episode creates clone copying entire inode file indirect blocks file system generates considerable disk consumes lot disk space instance file system inode disk space inodes file system creating snapshot duplicating inodes generate disk consume disk space creating snapshots consume one-third file system space data blocks modified duplicating root inode wafl creates snapshots quickly disk snapshot performance important wafl creates snapshot seconds quick recovery unclean system shutdowns figure shows transition figure detail disk block modified contents written location block parent modified reflect location parent parent turn written location root tree inode file indirect block inode file block regular file indirect block regular file data block block update block update snapshot inode root inode snapshot inode root inode ddd figure write block location pointers block ancestors updated requires written locations file system design nfs file server appliance rev wafl inefficient wrote blocks nfs write request wafl gathers hundreds nfs requests scheduling write episode write episode wafl allocates disk space dirty data cache schedules required disk result commonly modified blocks indirect blocks blocks inode file written write episode nfs request file system consistency non-volatile ram wafl avoids file system consistency checking unclean shutdown creating special snapshot called consistency point seconds unlike snapshots consistency point accessible nfs snapshots consistency point completely consistent image entire file system wafl restarts simply reverts recent consistency point faserver reboot minute data single partition consistency points wafl write data disk writes blocks tree blocks representing recent consistency point remains completely unchanged wafl processes hundreds thousands nfs requests consistency points on-disk image file system remains seconds wafl writes consistency point time on-disk image advances atomically state reflects made requests technique unusual unix file system databases instance astrahan describes shadow paging technique system databases unusual write operations time wafl consistency points wafl non-volatile ram nvram log nfs requests processed consistency point nvram special memory batteries store data system power unclean shutdown wafl replays requests log prevent lost faserver shuts creates consistency point suspending nfs service clean shutdown nvram doesn unprocessed nfs requests turned increase battery life wafl divides nvram separate logs log full wafl switches log starts writing consistency point store log safely disk wafl schedules file system design nfs file server appliance rev consistency point seconds log full prevent on-disk image file system date logging nfs requests nvram advantages common technique nvram cache writes disk driver layer lyon sandberg describe nvram write cache technique legato prestoserve nfs accelerator lyon processing nfs request caching resulting disk writes generally takes nvram simply logging information required replay request instance move file directory file system update contents inodes source target directories ffs blocks cache space wafl bytes log information needed replay rename operation rename factor difference nvram usage extreme case simple write caching disk blocks consume data inode update large files indirect block wafl logs data bytes header information typical mix nfs operations wafl store operations megabyte nvram nvram cache unwritten disk blocks turns integral part disk subsystem nvram failure corrupt file system ways fsck detect repair wrong wafl nvram wafl lose nfs requests on-disk image file system remains completely consistent important nvram reliable reliable raid disk array final advantage logging nfs requests improves nfs response times reply nfs request file system nvram update in-memory data structures allocate disk space data wait modified data reach disk file system nvram write cache steps copies modified data nvram waiting data reach disk wafl reply nfs request quickly update in-memory data structures log request allocate disk space data copy modified data nvram write allocation write performance important network file servers ousterhout observed read caches larger client server writes file system design nfs file server appliance rev begin dominate subsystem ousterhout effect pronounced nfs client-side write caching result disks nfs server times write operations reads wafl design motivated largely desire maximize flexibility write allocation policies flexibility takes forms 
wafl write file system block root inode location disk ffs meta-data inodes bit maps fixed locations disk prevents ffs optimizing writes putting data newly updated file inode disk wafl write meta-data disk optimize writes creatively wafl write blocks disk order ffs writes blocks disk carefully determined order fsck restore file system consistency unclean shutdown wafl write blocks order on-disk image file system wafl writes consistency point constraint wafl write blocks consistency point writes root inode consistency point file system design nfs file server appliance rev wafl allocate disk space nfs operations single write episode ffs allocates disk space processes nfs request wafl gathers hundreds nfs requests scheduling consistency point time allocates blocks requests consistency point deferring write allocation improves latency nfs operations removing disk allocation processing path reply avoids wasting time allocating space blocks removed reach disk features give wafl extraordinary flexibility write allocation policies ability schedule writes requests enables intelligent allocation policies fact blocks written location order semantically-smart disk systems muthian sivathanu vijayan prabhakaran florentina popovici timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract propose evaluate concept semantically-smart disk system sds opposed traditional smart disk sds detailed knowledge system disk system including information on-disk data structures system sds exploits knowledge transparently improve performance enhance functionality beneath standard block read write interface automatically acquire knowledge introduce tool eof discover le-system structure types systems show sds exploit knowledge on-line understand le-system behavior quantify space time overheads common sds showing excessive study issues surrounding sds construction designing implementing number prototypes case studies case study exploits knowledge aspect system implement powerful functionality beneath standard scsi interface surprising amount functionality embedded wide variety strategies easy block allocation strategies change wafl on-disk data structures details wafl write allocation policies scope paper short wafl improves raid performance writing multiple blocks stripe wafl reduces seek time writing blocks locations disk wafl reduces headcontention reading large files placing sequential blocks file single disk raid array optimizing write allocation difficult goals conflict file system design nfs file server appliance rev snapshot data structures algorithms block-map file file systems track free blocks bit map bit disk block bit set block technique work wafl snapshots block time wafl block-map file -bit entry disk block bit set active file system block bit set snapshot block block bits block-map entry set figure shows life cycle typical block-map entry time block-map entry completely clear indicating block time wafl allocates block stores file data snapshots created times wafl copies active file system bit bit indicating membership snapshot block deleted active file system time occur file block removed contents block updated contents written location disk block reused snapshot figure occurs time snapshots block removed time block-map entry description block unused block allocated active snapshot created snapshot created block deleted active snapshot created snapshot deleted snapshot deleted block unused bit set active file system bit set snapshot bit set snapshot bit set snapshot figure life cycle block-map file entry creating snapshot challenge writing snapshot disk avoid locking incoming nfs requests problem nfs requests change cached data part snapshot remain unchanged reaches disk easy create snapshot suspend nfs processing write snapshot resume nfs processing writing snapshot long file system design nfs file server appliance rev nfs server stop responding remember wafl creates consistency point snapshot seconds performance critical wafl technique keeping snapshot data consistent mark dirty data cache snapshot rule snapshot creation data marked snapshot modified data marked snapshot flushed disk nfs requests read file system data modify data isn snapshot processing requests modify snapshot data deferred avoid locking nfs requests wafl flush snapshot data quickly wafl performs steps allocate disk space files snapshot blocks wafl caches inode data places special cache in-core inodes disk buffers belonging inode file finishes write allocating file wafl copies newly updated inode information inode cache inode file disk buffer clears snapshot bit in-core inode step complete inodes regular files marked snapshot nfs operations continue blocking fortunately step quickly requires disk update block-map file block-map entry wafl copies bit active file system bit snapshot write snapshot disk buffers cache newly-allocated locations disk buffer flushed wafl restarts nfs requests waiting modify duplicate root inode create inode represents snapshot turn root inode snapshot bit snapshot inode reach disk blocks snapshot written rule unexpected system shutdown leave snapshot inconsistent state snapshot inode written snapshot data exists cache nfs requests suspended processed normal loads wafl performs steps step generally hundredths wafl completes nfs operations delayed deleting snapshot trivial wafl simply zeros root inode representing snapshot clears bit representing snapshot block-map entry file system design nfs file server appliance rev performance difficult compare wafl performance file systems directly wafl runs nfs appliance benchmarked file systems context nfs nfs benchmark today spec sfs system file server benchmark laddis laddis stands group companies originally developed benchmark legato auspex digital data general interphase sun laddis tests nfs performance measuring server response time throughput levels servers typically handle requests quickly low load levels load increases response time figure compares faserver laddis performance well-known nfs servers system level benchmark laddis compare file system performance misleading argue instance figure underestimates wafl performance faserver cluster file systems servers dozens file system basis wafl outperforms file systems faserver raid typically degrades file system performance substantially small request sizes characteristic nfs servers raid hand argue benchmark overestimates wafl performance entire faserver designed specifically nfs performance nfs-specific tuning system wafl wafl special purpose nature fair compare performance general purpose file systems satisfies design goal performing nfs raid file system design nfs file server appliance rev conclusion wafl developed stable surprisingly quickly file system production file system year case lost user data attribute stability part wafl consistency points processing file system requests simple wafl updates in-memory data structures nvram log consistency points eliminate ordering constraints disk writes significant source bugs file systems code writes consistency points concentrated single file interacts rest wafl executes infrequently nfs operations average response time msec faserver cluster auspex sun sparcenter sun sparccluster sun sparcserver figure graph specnfs operations clusters graph shows specnfs cluster operations importantly easier develop high quality high performance system software appliance general purpose operating system compared general purpose file system wafl handles regular simple set requests general purpose file system receives requests thousands applications wide variety access patterns applications added frequently contrast wafl receives requests nfs client code systems nfs client implementations implementations rare applications ultimate source nfs requests nfs client code converts file system requests regular pattern network requests filters error cases reach server small number operations wafl supports makes define test entire range inputs expected handle file system design nfs file server appliance rev advantages apply appliance file server appliances network appliance makes sense protocols defined widely protocols appliance provide important advantages general purpose computer file system design 
nfs file server appliance rev bibliography astrahan astrahan blasgen chamberlain eswaran gravy griffiths king traiger wade watson system relational approach database management acm transactions database systems chutani sailesh chutani episode file system proceedings winter usenix conference san francisco january hitz dave hitz nfs file server appliance network appliance corporation tasman drive suite santa clara lyon bob lyon russel sandberg breaking nfs performance barrier suntech journal autumn mckusick marshall mckusick fast file system unix acm transactions computer systems august ousterhout john ousterhout fred douglis beating bottleneck case log-structured file systems acm sigops january patterson patterson gibson katz case redundant arrays inexpensive disks raid acm sigmod chicago june sandberg russel sandberg david goldberg steve kleiman dan walsh bob lyon design implementation sun network file system proceedings summer usenix conference portland june file system design nfs file server appliance rev biographies dave hitz dave hitz co-founder system architect network appliance builds nfs file server appliances network appliance dave focused designing implementing network appliance file system design network appliance file server worked auspex systems file system group mips system kernel group jobs hobbies included herding castrating slaughtering cattle pen-based computer programming typing names blue shield insurance cards dropping high school attended george washington swarthmore college deep springs college finally princeton received computer science bse author reached mail hitz netapp james lau james lau co-founder director engineering network appliance james spent years auspex systems recently director software engineering instrumental defining product requirements high level architecture auspex high performance nfs file server joining auspex james spent years bridge communications implemented variety protocols xns tcp ethernet hdlc spent year bridge group manager products james received masters degree computer engineering stanford bachelors degrees computer science applied mathematics berkeley author reached e-mail jlau netapp sds hinting future disk manufacturers compete enhanced functionality simply cost-per-byte performance introduction true knowledge confucius microprocessors memory chips smaller faster cheaper embedding processing memory peripheral devices increasingly attractive proposition placing processing power memory capacity smart disk system functionality migrated system disk raid providing number potential advantages traditional system computation takes place data improve performance reducing traf host processor disk disk system exploit low-level information typically le-system level including exact head position blockmapping information finally unmodi systems leverage optimizations enabling deployment broad range systems smart disk systems great promise realizing full potential proven dif cult causative reason shortfall narrow interface systems disks disk subsystem receives series block read write requests inherent meaning data structures system bitmaps tracking free space inodes data blocks directories indirect blocks exposed research efforts limited applying disk-system intelligence manner oblivious nature meaning system traf improving write performance writing blocks closest free space disk ful potential retain utility smart disk systems smarter interface storage remains system acquire knowledge system exploit understanding order enhance functionality increase performance storage system understands blocks constitute perform intelligent prefetching perle basis storage system blocks unused system utilize space additional copies blocks improved performance reliability storage system detailed knowledge system structures policies semantically-smart disk system sds understands meaning operations enacted important problem solved sds information discovery disk learn details system on-disk data structures straight-forward approach assume disk exact white-box knowledge system structures access relevant header les cases information unavailable cumbersome maintain paper explore gray-box approach attempting proceedings usenix conference file storage technologies fast san francisco california automatically obtain le-system speci knowledge storage system develop present ngerprinting tool eof automatically discovers le-system layout probes observations show eof smart disk system automatically discover layout class systems similar berkeley fast file system ffs show exploit layout information infer higher-level le-system behavior processes classi cation association operation inferencing refer ability categorize disk block data inode bitmaps superblock detect precise type data block directory indirect pointer associate data block inode relevant information identify higher-level operations creation deletion sds techniques implement desired functionality prototype smart disk system software infrastructure in-kernel driver interposes read write requests system disk prototype environment explore challenges adding functionality sds adhering existing interfaces running underneath stock system paper focus linux ext ext systems netbsd ffs understand performance characteristics sds study overheads involved ngerprinting classi cation association operation inferencing microbenchmarks quantify costs terms space time demonstrating common overheads excessive finally illustrate potential semantically-smart storage systems implemented number case studies sds framework aligning les track boundaries increase performance smallle operations information lesystem structures implement effective secondlevel caching schemes volatile non-volatile memory secure-deleting disk system ensures non-recoverability deleted les journaling storage system improve crash recovery time case studies demonstrate broad range functionality implemented semantically-smart disk system cases demonstrate sds tolerate imperfect information system key building robust semantically-smart disk systems rest paper organized section discuss related work discuss lesystem ngerprinting section classi cation association section operation inferencing section evaluate system section present case studies section conclude section related work related work smart disks grouped categories rst group assumes interface storage systems xed changed category sds belongs research group proposes storage interface requiring systems modi leverage interface finally group proposes interface programming model applications fixed interfaces focus paper integration smart disks traditional system environment environment system narrow scsi-like interface storage disk persistent store data structures early smart disk controller loge harnessed processing capabilities improve performance writing blocks current disk-head position wang log-based programmable disk extended approach number ways quick crash-recovery free-space compaction systems assume require knowledge system structures storage system interfaces developed provided local setting opportunities functionality network packet lter slice virtual service slice interpose nfs traf clients implement range optimizations preferential treatment small les interposing nfs traf stream simpler scsi-disk michael malcolm michael malcolm co-founder senior vice president strategic development network appliance previously ran successful management consulting practice clients focused distributed computing networking file storage technology founder ceo waterloo microsystems canadian developer network operating system software past associate professor computer science waterloo taught hundreds students program real-time systems control electric model trains research spanned areas network operating systems portable operating systems interprocess communication compiler design numerical mathematics led development major operating systems thoth waterloo port received mechanical engineering denver computer file system design nfs file server appliance rev science stanford author reached e-mail malcolm netapp 
block stream contents nfs packets well-de ned high-end raid products perfect place semantic smartness typical enterprise storage system substantial processing capabilities memory capacity emc symmetrix server eighty mhz motorola microprocessors con gured memory high-end raid systems leverage resources perform bare minimum semanticallysmart behavior storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically paper explore acquisition exploitation detailed knowledge system behavior expressive interfaces primary factors limits addition functionality smart disk narrow interface systems storage surprising research investigates changing interface brie highlight projects mime investigates enhanced interface context intelligent raid controller speci cally mime adds primitives proceedings usenix conference file storage technologies fast san francisco california clients control updates storage visible traf streams commit order operations logical disks expand interface allowing system express grouping preferences lists systems simpli maintain information raid exposes per-disk information informed system lfs providing performance optimizations control redundancy improved manageability storage finally ganger suggests reevaluation interface needed outlines relevant case studies track-aligned extents explore paper freeblock scheduling recent work storage community suggests evolution storage place disks general-purpose network standard scsi bus suggested network disks export higher-level object-like interface moving responsibilities low-level storage management system drives speci challenges context xed object-based interface systems storage provide interesting avenue research utility semantic awareness programming environments contrast integration underneath traditional system work focused incorporating active storage parallel programming environments recent work active disks includes acharya riedel amiri research focuses partition applications host disk cpus minimize data transferred system busses inferring on-disk structures fingerprinting file system semantically smart disk implement interesting functionality interpret types blocks read written disk speci characteristics blocks sds practical information obtained robust manner require human 
involvement alternatives obtaining information rst approach directly embeds knowledge system sds onus understanding target system developer sds obvious drawbacks sds rmware updated system upgraded sds robust target system approach target system informs sds data structures run-time case responsibilities target system numerous disadvantages approach importantly target system changed system process access information directly communicate sds communication channel existing protocols added target system sds finally dif cult ensure speci cation communicated sds matches actual system implementation approach sds automatically infers system data structures bene approach speci knowledge target system required sds developed assumptions made sds target system checked deployed additional work required con gure sds installed sds deployed environments difculty approach potential semantically-smart storage system explore sds automatically acquire layout information ngerprinting software automatically inferring system structures bears similarity research efforts reverseengineering researchers shown bit-level machine instruction encodings semantic meaning assembly instructions deduced developed techniques identify parameters tcp protocol extract lowlevel characteristics disks determine buffer-cache policies understand behavior real-time cpu scheduler assumptions automatically inferring layout information arbitrary system challenging problem important rst step developed utility called eof extraction filesystems extract layout information ffs-like systems journaling capabilities veri eof identify data structures employed linux ext ext netbsd ffs eof understand future ffs-like system adheres assumptions layout data structures disk general disk blocks statically exclusively assigned categories data inodes bitmaps free allocated data blocks inodes summary information superblock group descriptors log data eof identi block addresses disk allocated category data blocks data block dynamically data directory listings pointers data blocks indirect block data blocks shared les eof identi structure directory data proceedings usenix conference file storage technologies fast san francisco california eof assumes record directory data block length record entry length entry inode number entry eld directory entry assumed multiple bits eof assumes indirect blocks -bit pointers inode blocks inode block inodes inode consumes n-th block eof assumes nition inode eld static time eof identi location absence elds inode size blocks number data blocks allocated inode ctime time inode changed mtime time data changed dtime deletion time links number links inode generation number data pointers number combination direct pointers single double triple indirect pointers dir bits bits change directory inodes exception dir bits elds identify default assumed multiple bits multiple elds identi bits blocks links elds size eld assumed largest multiple bits lead overlapping elds bitmap blocks bitmaps data inodes share single block separate blocks bits data inode bitmap blocks one-to-one linear mapping data blocks inodes bitmap block valid log data log data journaling system managed circular contiguous buffer make assumptions contents log future feasibility inferring on-disk data structures depends assumption production systems change slowly time assumption hold system developers strong motivation on-disk structures legacy systems continue operate examining systems past present corroborates belief on-disk structure ffs system changed years linux ext system layout conception ext journaling system backward compatible ext extensions freebsd ffs designed avoid on-disk algorithm overview eof software system made sds partition eof run partition sds understands context deployed basic structure eof user-level probe process performs operations system generating controlled traf streams disk sds high-level operations performed disk traf result observing blocks written bytes blocks change sds infers blocks type system data structures offsets block type eld sds knowledge con gure simultaneously verifying target system behaves expected sds correlate traf observes system operations performed probe process correlation requires pieces functionality probe process ensure blocks operation ushed system cache written sds ensure probe process unmounts system unmounting re-mounting sparingly increases running time eof probe process occasionally inform sds speci operation ended probe process communicates sds writing distinct pattern fencepost sds pattern resulting traf message probe process general techniques eof identify blocks inode elds identify data blocks sds pattern probe process writes test les classify blocks elds sds attempts isolate unique unclassied block written operation set operations operations algorithm phases eof composed phases eof isolates summary blocks log eof identi data blocks data bitmaps eof inodes inode bitmaps blocks classi eof isolates inode elds finally eof identi elds directory entries bootstrapping phase goal bootstrapping isolate blocks frequently written phases tered blocks interest phase isolates summary blocks log inode data blocks fencepost test directory test les probe process creates fencepost number test les test directory sds identies data blocks searching patterns eof identi blocks belonging log exists step probe process synchronously appends data pattern proceedings usenix conference file storage technologies fast san francisco california test les sds observes blocks meta-data blocks written circular pattern belong log block traf matches pattern eof infers system perform journaling eof identi summary blocks probe process unmounts system written blocks classi log data identi summary blocks isolate inode blocks repeatedly written probe process performs chmod fencepost test directory test les case inode written allowing classied data blocks belonging test directory identi changing test blocks previously unidenti blocks written finally determine separate bitmap blocks data inode blocks linux ext ext single bitmap block shared netbsd ffs eof creates sds observes unclassi blocks determine bitmap blocks shared separate data inodes simplify presentation remainder discussion case data inode bitmaps separate blocks eof correctly handles shared case case eof isolates speci bits shared bitmap block devoted inode data block state data data-bitmap blocks phase eof continues identifying blocks disk data data bitmaps isolate blocks probe process appends blocks data pattern test les blocks match pattern classi assumed data-bitmap blocks indirect-pointer blocks eof differentiates inferring blocks written les data-bitmap blocks care create small les single lls bitmap block bitmap block special case smaller expected completely cleanup phase test les deleted inodes inode-bitmap blocks phase identifying inodes bitmaps requires creating les distinct steps required probe process creates les inodes inode bitmaps modi probe process performs chmod les inodes inode bitmaps written inodes inode bitmaps distinguished phase calculates size inode performed recording number times block identi inode dividing block size observed number inodes block inode fields phase point eof classi blocks disk belonging categories data structures phase identi elds inodes observing elds change operations brevity describe eof infers blocks links generation number elds rst inode elds eof identi size times requires steps probe process creates sds 
stores inode data compare inode data written steps probe process overwrites data inode elds change related time probe process appends small amount data data pointer added point size eld identi data changed step step fourth probe process performs operation change inode changing data adding link changing permissions sds isolate mtime changed step ctime changed finally deleted deletion-time eld observed eof identi location level data pointers inode probe process repeatedly appends sds observes bytes inode change changed previous step eof infers location indirect pointers observing additional data block written additional pointer updated inode improve performance write block probe process seeks progressively larger amount seek distance starts block increases size handled detected indirection level finally eof isolates inode bit elds designate directories probe process alternately creates les directories sds histograms directory inodes histogram eof records count times bit inode type determine directory elds eof isolates bits les directories vice versa bits values considered identify les versus directories soft link bits identi similar manner directory entries phase nal phase eof identi structure entries directory eof infers offsets entry length probe proproceedings usenix conference file storage technologies fast san francisco california cess creates sds searches directory data block eld designating length validation deleted step repeated numerous times lenames lengths eof nds location record length assumption length record remaining space directory data block length reduced record added probe process creates additional les sds simply records offsets change previous entries finally offset inode number found assumption directory entry step probe process creates empty directories sds isolates inode offset recording differences data blocks directories assertion assumptions major challenge automatic inferencing ensure sds correctly identi behavior target system robust system meeting assumptions eof mechanisms detect assumption fails case system identi non-supported sds operates correctly semantic knowledge blocks expected written speci step speci blocks observed eof detects violation veri violations identi appropriately eof run non-ffs systems msdos vfat reiserfs additional bene eof con gure sds system bugs identi running eof ext linux isolated bugs sds observed incomplete traf key steps problem tracked back ext bug data written seconds prior unmount ushed disk probe process noted error inodes allocated case ext incorrectly marks system dirty eof enables checks system easily obtained methods exploiting structural knowledge classi cation association key advantage sds ability identify utilize important properties block disk properties determined direct indirect classi cation association direct classi cation blocks easily identi location disk indirect classi cation blocks identi additional information identify directory data indirect blocks inode examined finally association data block inode connected cases sds requires functionality identify change occurred block functionality implemented block differencing infer data block allocated single-bit change data bitmap observed change detection potentially costly operations sds reasons compare current block version block sds fetch version block disk avoid overhead cache blocks employed comparison expensive location difference byte block compared byte block quantify costs section direct classi cation direct classi cation simplest cient form on-line block identi cation sds sds determines type block performing simple bounds check calculate set block ranges block falls ffs-like system superblock bitmaps inodes data blocks identi technique indirect classi cation indirect classi cation required type block vary dynamically simple direct classi cation precisely determine type block ffs-like systems indirect classi cation determine data block data directory data form indirect pointer block single double triple indirect block illustrate concepts focus directory data differentiated data steps identifying indirect blocks versus pure data similar identifying directory data basic challenge identifying data block belongs directory track inode points data check type directory perform tracking sds snoops inode traf disk directory inode observed data block numbers inserted hash table sds removes data blocks hash table observing blocks freed block differencing bitmaps sds identify block directory block presence table directory data discuss complications approach proceedings usenix conference file storage technologies fast san francisco california sds guarantee correctly identify blocks les directories speci cally data block present hash table sds infers data corresponds cases directory inode sds result hash table situation occur directory created blocks allocated existing directories system guarantee inode blocks written data blocks sds incorrectly classify newly written data blocks problem occur classifying data blocks read case system read inode block data block data block number sds inode rst correctly identify subsequent data blocks transient misclassi cation problem depends functionality provided sds instance sds simply caches directory blocks performance tolerate temporary inaccuracy sds requires accurate information correctness ways ensured rst option guarantee system writes inode blocks data blocks true default ffs soft updates linux ext mounted synchronous mode option buffer writes time classi cation made deferred classi cation occurs inode written disk data block freed inferred monitoring data bitmap traf sds perform excess work obliviously inserts data blocks hash table directory inode read written inode recently passed sds causing hash table updated optimize performance sds infer block added modi deleted time directory inode observed ensure blocks added deleted hash table process operation inferencing detail section identifying indirect blocks process identifying indirect blocks identical identifying directory data blocks case sds tracks indirect block pointers inodes read written maintaining hash table single double triple indirect block addresses sds determine data block indirect block association association connect data blocks inodes size creation date sds association achieved simple space-consuming approach similar indirect classi cation sds snoops inode traf inserts data pointers addressto-inode hash table concern table size accurate association table grows proportion number unique data blocks read written storage system system booted approximate information tolerated sds size table bounded detecting high-level behavior operation inferencing block classi cation association provide sds cient identifying special kinds blocks operation inferencing understand semantic meaning observed blocks outline sds identify system operations observing key challenge operation inferencing sds distinguish blocks valid version instance newly allocated directory block written compared contents block block contained arbitrary data identify versions sds simple insight metadata block written read contents block relevant detect situation sds maintains hash table meta-data block addresses read past meta-data block read added list block freed block bitmap reset removed list block allocated data freed reallocated directory block address present hash table sds contents illustrative purposes section 
examine sds infer create delete operations discussion speci ext similar techniques applied ffs-like systems file creates deletes steps identifying creates deletes rst actual detection create delete determining inode affected describe detection mechanisms logic determining inode rst detection mechanism involves inode block inode block written sds examines determine inode created deleted valid inode non-zero modi cation time proceedings usenix conference file storage technologies fast san francisco california deletion time modi cation time non-zero deletion time non-zero means inode newly made valid created similarly reverse change newly freed inode deleted indication change version number valid inode delete create occurred cases inode number calculated physical position inode disk on-disk inodes inode numbers detection mechanism involves inode bitmap block bit set inode bitmap created inode number represented bit position similarly newly reset bit deleted update directory block indication newly created deleted directory data block written sds examines block previous version directory entry dentry added inode number obtained dentry case removed dentry contents dentry inode number deleted newly created deleted choice mechanism combinations thereof depends functionality implemented sds sds identify deletion immediately creation inode number inode bitmap mechanism sds observe change bitmap operations grouped due delayed write system case modi cation times version numbers similarly newly created deleted directory block-based solution cient file system operations general technique inferring logical operations observing blocks versions detect system operations note cases conclusive inference speci logical operation sds observe correlated multiple meta-data blocks semantically-smart disk system infer renamed observes change directory block entry inode number stays note version number inode stay similarly distinguish creation hard link normal directory entry inode examined evaluation section answer important questions sds framework cost ngerprinting system time overheads classi cation association operation inferencing space overheads proceeding evaluation rst describe experimental environment platform prototype sds employ software-based infrastructure implementation inserts pseudo-device driver kernel interpose traf system disk similar software raid prototype appears systems device system mounted primary advantage prototype observes information traf stream actual sds system current infrastructure differs important ways true sds importantly prototype direct access low-level drive internals information made dif cult sds runs system host interference due competition resources initial case studies prime importance performance characteristics microprocessor memory system actual sds high-end storage arrays signi processing power processing capacity trickle lower-end storage systems experimented prototype sds linux linux netbsd operating systems underneath ext ext ffs systems experiments paper performed processor slow modern standards mhz pentium iii processor k-rpm ibm lzx k-rpm quantum atlas iii disk experiments employ fast system comprised ghz pentium k-rpm seagate cheetah disk gauge effects technology trends off-line layout discovery subsection show time run ngerprinting tool eof reasonable modern disks eof run system runtime eof determine common case performance sds runtime eof prohibitive disks larger potential solution parallelism proceedings usenix conference file storage technologies fast san francisco california time partition size costs fingerprinting eof phase phase slow system fast system figure costs fingerprinting gure presents time breakdown ngerprinting slow system ibm disk fast system running underneath linux ext x-axis vary size partition ngerprinted y-axis shows time phase time-consuming components eof parallelizable reduce run-time disk arrays figure presents graph time run eof single-disk partition size partition increased show performance results slow system ibm disk fast system graph shows phase determines locations data blocks data bitmaps phase determines locations inode blocks inode bitmaps dominate total cost ngerprinting time phases increases linearly size partition requiring approximately seconds slow system seconds fast system comparing performance systems conclude increases sequential disk performance directly improve eof ngerprinting time phases require small amount time partition size on-line time overheads classi cation association operation inferencing potentially costly operations sds subsection employ series microbenchmarks illustrate costs actions results experiments sds underneath linux ext presented table action microbenchmark cases rst case system mounted synchronously ensuring meta-data operations reach sds order allowing sds guarantee correct classi cation additional effort synchronous mounting linux ext similar traditional ffs handling meta-data updates case system mounted asynchronously case guarantee correct classi cation association sds perform operation inferencing microbenchmarks perform basic system operations including directory creates deletes report perle per-directory overhead action test experiments make number observations operations tend cost order tens microseconds directory operations require complete cost due per-block cost operation inferencing synchronous mode create workload takes roughly corresponds base cost create workload cost approximately block costs rise size increases sds incurs small per-block overhead compared actual disk writes number milliseconds complete cases overheads ext system run asynchronous mode lower run synchronous mode asynchronous mode numerous updates meta-data blocks batched costs block differencing amortized synchronous mode meta-data operation ected disk system incurring higher overhead sds observe synchronous mode classi cation expensive association expensive inferencing sds care employ actions needed implement desired functionality on-line space overheads sds require additional memory perform classi cation association operation inferencing speci cally hash tables required track mappings data blocks inodes caches needed implement cient block differencing quantify memory overheads variety workloads table presents number bytes hash table support classi cation association operation inferencing sizes maximum reached run workload netnews postmark modi andrew benchmark netnews postmark vary workload size caption table dominant memory overhead occurs sds performing block-inode association classi cation operation inferencing require table sizes proportional number unique meta-data blocks pass sds association requires information unique data block passes worst case entry required proceedings usenix conference file storage technologies fast san francisco california indirect block-inode operation classi cation association inferencing sync async sync async sync async create create delete delete mkdir rmdir table sds time overheads table breaks costs indirect classi cation block-inode association operation inferencing microbenchmarks row stress aspects action create benchmark creates les size delete benchmark similarly deletes les mkdir rmdir benchmarks create remove directories result presents average overhead operation extra time sds takes perform classi cation association inferencing experiments run slow system ibm lzx disk linux ext mounted synchronously sync asynchronously async data block disk memory disk space space costs tracking association information high prohibitive memory resources scarce sds choose tolerate imperfect information swap portions table disk addition hash tables needed perform classi cation association operation inferencing cache data blocks perform block differencing effectively recall differencing 
observe pointers allocated freed inode indirect block check time elds inode changed detect bitwise bitmap monitor directory data creations deletions performance system sensitive size cache cache small difference calculation rst fetch version block disk avoid extra size cache roughly proportional active meta-data working set postmark workload found sds cache approximately blocks hold working set cache smaller block differencing operations disk retrieve older copy block increasing run-time benchmark roughly case studies section describe case studies implementing functionality sds implement drive raid semantic knowledge case studies indirect block-inode operation classi cation association inferencing netnews netnews netnews postmark postmark postmark andrew table sds space overheads table presents space overheads structures performing classi cation association operation inferencing workloads netnews postmark modi andrew benchmark workloads netnews postmark run amounts input correspond roughly number transactions generates netnews implies transactions run number table represents maximum number bytes stored requisite hash table benchmark run hash entry bytes size experiment run slow system linux ext asynchronous mode ibm lzx disk built system proper implementing le-system functionality storage system advantages semantic intelligence storage-system manufacturers augment products broader range capabilities due space limitations fully describe case studies paper highlight functionality case study implements present performance evaluation conclude analyzing complexity implementing functionality sds performance evaluation included demonstrate interesting functionality implemented effectively sds leave detailed performance studies future work theme explore section usage approximate information scenarios sds wrong understanding system case studies track-aligned extents proposed schindler track-aligned extents traxtents improve disk access times placing medium-sized les tracks avoiding track-switch costs detailed level knowledge traxtents-enabled system requires underlying disk mapping logical block numbers physical tracks traxtents natural candidate implementation sds information readily obtained fundamental challenge implementing traxtents sds system adapting policies system system speci cally traxtent sds uence system allocation prefetching mid-sized les proceedings usenix conference file storage technologies fast san francisco california prefetching prefetching ext traxtent sds table track-aligned extents table shows bandwidth obtained reading les randomized order roughly size track case examine default track-aligned allocation varying track-sized prefetching enabled sds experiment run slow system running linux ext system mounted asynchronously quantum atlas iii disk allocated consecutive data blocks span track boundaries accesses track-sized units components interest traxtent sds implementation bitmap blocks rst read system sds marks bitmap block track allocated similar technique schindler wastes small portion disk fake allocation uences system allocate les span tracks system decides allocate tracks sds dynamically remaps blocks track-aligned locale similar block remapping loge smart disks major difference sds remaps blocks part mid-sized les benet track-alignment non-semantically aware disks make distinction traxtent sds performs additional prefetching ensure accesses smaller track linux ext ffs prefetches blocks initially read traxtent sds observes read rst block track-aligned requests remainder track places data blocks cache traxtent sds relies piece exact information correctness location bitmap blocks marks trick system track-aligned allocation information static obtained reliably eof performance cost runtime indirect classi cation data belonging medium-sized les occasionally incorrect remapping performance correctness table shows traxtent sds prefetching results roughly improvement bandwidth medium-sized les structural caching discuss semantic information caching sds simple lru management disk cache duplicate contents system cache wastes memory storage system waste onerous storage arrays due large amounts memory contrast sds structural undertpc-b tpc-b ffs lru sds file-aware caching sds table file-aware caching table shows time seconds takes execute tpc-b transactions experiments transactions rst run warm system large scan run series transactions timed table compares netbsd ffs standard disk sds lru-managed cache sds le-aware cache experiments run slow system ibm lzx disk standing system cache blocks intelligently avoid wasteful replication explore caching blocks volatile memory dram non-volatile memory nvram presents unique opportunities optimization rst examine simple optimization avoids worst-case lru behavior file-aware caching sds fac sds exploits knowledge size selectively cache blocks les small cache les accessed sequentially strategy avoids caching blocks large les scanned ush cache blocks implement le-aware caching fac sds identi cacheable blocks indirect classi cation association case hash table holds block addresses correspond les meet caching criteria previously sds misclassify blocks cases inode written disk data blocks fac sds small amount state active order detect sequential access patterns table shows performance fac sds database workload scenario run tpc-b transactions periodically intersperse large scans system emulating system running mixed interactive batch transactions large scan ushes contents traditional lru-managed cache degrades performance subsequent transactions le-aware cache cache blocks large scans keeping transactional tables sds memory improving performance examine sds semantic knowledge store important structures non-volatile memory explore possibilities rst exploit semantic knowledge store ext journal nvram implement journal caching sds sds sds recognize traf journal redirect nvram straightforward eof tool determines blocks belong journal classifying caching data reads proceedings usenix conference file storage technologies fast san francisco california create create sync ext lru sds lru sds journal caching sds table journal caching table shows time create -kb les ext sds sds performs lru nvram cache management cache journal caching sds storing journal nvram create benchmark performs single sync les created create sync benchmark performs sync creation inducing journaling-intensive workload experiments run slow system running linux utilizing ibm lzx disk writes journal sds implement desired functionality place meta-data bitmaps inodes indirect blocks directories netbsd ffs nvram inodes bitmaps identi location disk pointer blocks directory data blocks identi indirect classi cation occasionally miss blocks exploit fact approximate information adequate sds writes unclassi blocks disk nvram observes inode track meta-data blocks meta-data caching sds mdc sds additional map record in-core location tables show performance sds mdc sds cases simple nvram caching structures journal system meta-data effective reducing run times dramatically greatly reducing time write blocks stable storage lru-managed cache effective case cache large working set main bene structural caching nvram size cached structures sds guarantees effective cache utilization hybrid combine worlds storing important structures journal meta-data nvram managing rest cache space lru fashion future plan investigate ways semantic information improve storagesystem cache management sds types meta-data updates last-accessedtime updates inode order ascertain les system cache prefetching sds intelligent system awareness make guess block read finally blocks deleted removed cache freeing space live blocks create read delete postmark ffs lru sds lru sds 
mdc sds table meta-data caching left columns table show time seconds complete phase lfs microbenchmark experiment lfs benchmark creates reads deletes -kb les column shows total time seconds postmark benchmark run les transactions directories rows compare performance netbsd ffs slow system ibm disk sds sds performs lru nvram cache management cache mdc sds strategy secure deletion advanced magnetic force scanning tunneling microscopy stm person physical access drive lot time potentially extract sensitive data user deleted case study explore secure-deleting sds disk guarantees data deleted les unrecoverable previous approaches incorrectly functionality system over-writing deleted blocks multiple times patterns guarantee data removed disk copies data blocks exist due bad-block remapping storage system optimizations multiple consecutive le-system writes reach disk media due nvram buffering sds locale secure delete implemented ensure stray copies data exist over-writes performed disk nature case study approximate incorrect information blocks deleted acceptable secure-deleting sds recognizes deleted blocks operation inferencing overwrites blocks data patterns speci number times system reallocate blocks possibly write block fresh contents meantime sds tracks deleted blocks queues writes blocks overwrite nished note mount ext system synchronously secure deletion operate correctly investigating techniques relax requirement part future work table shows overhead incurred sds function number over-writes overwrites performed data recoverable noticeable price paid securedelete functionality loss acceptable highlysensitive applications requiring security performance improved delaying secureproceedings usenix conference file storage technologies fast san francisco california delete postmark ext secure-deleting sds secure-deleting sds secure-deleting sds table secure deletion table shows time seconds complete delete microbenchmark postmark benchmark secure-deleting sds delete benchmark deletes -kb les postmark benchmark performs transactions row secure-deleting sds shows performance number over-writes experiment place slow system running linux ext mounted synchronously ibm lzx disk overwrite disk idle performing immediately freeblock scheduling minimize performance impact journaling nal case study complex sds implements journaling underneath unsuspecting system view journaling sds extreme case helps understand amount information obtain disk level unlike case studies journaling sds requires great deal precise information system due space limitations present summary implementation fundamental dif culty implementing journaling sds arises fact disk transaction boundaries blurred instance system create system inode block parent directory block inode bitmap block updated part single logical create operation block writes grouped single transaction straight-forward fashion sds sees stream meta-data writes potentially interleaved logical system operations challenge lies identifying dependencies blocks handling updates atomic transactions result journaling sds maintains transactions coarser granularity journaling system basic approach buffer meta-data writes memory write disk in-memory state meta-data blocks constitute consistent metadata state logically equivalent performing incremental in-memory fsck current set dirty meta-data blocks writing disk check succeeds current set dirty meta-data blocks form consistent state treated single atomic transaction ensuring on-disk meta-data contents remain previous consistent state fully updated consistent state bene coarse-grained transactions batching commits performance improved traditional journaling systems create read delete ext sync ext async ext ext sync journaling sds table journaling table shows time complete phase lfs microbenchmark seconds -kb les con gurations compared ext linux mounted synchronously mounted asynchronously journaling ext linux journaling sds synchronously mounted ext linux experiment place slow system ibm lzx disk guarantee bounded loss data crash journaling sds limits time elapse successive journal transaction commits journaling daemon wakes periodically con gurable interval takes copy-on-write snapshot dirty blocks cache dependency information point subsequent meta-data operations update copy cache introduce additional dependencies current epoch similar secure-deleting sds current journaling sds implementation assumes system mounted synchronously robust sds requires verify assumption holds turn journaling meta-data state written disk journaling sds consistent synchronous asynchronous mount problem imposed asynchronous mount sds miss operations reversed create delete lead dependencies resolved inde nite delays journal transaction commit process avoid problem journaling sds suspicious sequence meta-data blocks single change expected multiple inode bitmap bits change part single write turns journaling cases fall-back journaling sds monitors elapsed time commit dependencies prolong commit time threshold suspects asynchronous mount aborts evaluate correctness performance journaling sds check correctness crashed system numerous times ran fsck verify inconsistencies reported performance journaling sds summarized table sds requires system mounted synchronously performance similar asynchronous versions semantically-smart disk system delays writing meta-data disk read test sds similar performance base system ext delete test similar performance journalproceedings usenix conference file storage technologies fast san francisco california eof fingerprinting probe process sds sds infrastructure case studies initialization traxtents hash table cache file-aware cache direct classi cation journal cache indirect classi cation meta-data cache association secure delete operation inferencing journaling table code complexity number lines code required implement aspects sds presented sds component eof tool lines code roughly lines shared system types rest le-system speci ing system ext creation sds pays signi cost relative ext overhead block differencing hash table operations noticeable impact purpose case study demonstrate sds implement complex functionality small overhead acceptable complexity analysis brie explore complexity implementing software sds table shows number lines code components system case studies table complexity found eof tool basic cache hash tables operation inferencing code case studies trivial implement top base infrastructure traxtent sds journaling sds require thousand lines code conclude including type functionality sds pragmatic conclusions beware false knowledge dangerous ignorance george bernard shaw recent article wise drives gordon hughes associate director center magnetic recording research writes favor smarter drives stressing great potential improving storage system performance functionality believes interface systems storage required widespread drive input output command requirements interface speci cation short industry consensus task general interest offers market opportunities multiple computer drive companies hughes comments illustrate dif culty interfaces require wide-scale industry agreement eventually limits creativity inventions existing interface framework information system disk low-level knowledge drive internals sds sits ideal location implement powerful pieces functionality disk system implement enabling innovations existing interfaces storage system manufacturers embed optimizations previously relegated domain systems enabling vendors compete axes cost performance paper demonstrated underneath class ffs-like systems le-system information automatically gathered exploited implement functionality drives heretofore implemented system implemented shown costs reverse-engineering system structure behavior reasonable challenges remain including understanding generality robustness semantic inference broader range systems sophisticated systems wider range platforms probed reveal workings approximate information exploited implement interesting functionality techniques tools developed assure correct operation 
semantic technology answer questions research experimentation nal answer elicited acknowledgments members wind research group feedback ideas presented paper keith smith excellent shepherding anonymous reviewers thoughtful suggestions greatly improved content paper finally computer systems lab tireless assistance providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation timothy denehy sponsored ndseg fellowship department defense proceedings usenix conference file storage technologies fast san francisco california acharya uysal saltz active disks proceedings conference architectural support programming languages operating systems asplos viii san jose october amiri petrou ganger gibson dynamic function placement dataintensive cluster computing proceedings usenix annual technical conference pages june anderson chase vahdat interposed request routing scalable network storage transactions computer systems tocs february arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october baker asami deprit ousterhout seltzer non-volatile memory fast reliable file systems proceedings international conference architectural support programming languages operating systems pages boston massachusetts october acm sigarch sigops sigplan bauer priyantha secure data deletion linux file systems tenth usenix security symposium washington august brown yamaguchi oracle hardware assisted resilient data oracle technical bulletin note burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey june chao english jacobson stepanov wilkes mime high performance parallel storage device strong recovery guarantees technical report hpl-csp- rev laboratories november collberg reverse interpretation mutation analysis automatic retargeting conference programming language design implementation pldi las vegas nevada june jonge kaashoek hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles pages asheville december denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey june dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey california june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge selforganizing disk controller proceedings usenix winter technical conference pages san francisco january ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings conference architectural support programming languages operating systems asplos viii october gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon gray storage bricks arrived invited talk usenix conference file storage technologies fast gutmann secure deletion data magnetic solid-state memory sixth usenix security symposium san jose california july hagmann reimplementing cedar file system logging group commit proceedings acm symposium operating systems principles november proceedings usenix conference file storage technologies fast san francisco california hsieh engler back reverseengineering instruction encodings proceedings usenix annual technical conference usenix boston massachusetts june hughes wise drives ieee spectrum august katcher postmark file system benchmark technical report trnetwork appliance october king dirty lesystem bug ext https listman redhat pipermail ext users -april html march lumb schindler ganger freeblock scheduling disk firmware proceedings usenix conference file storage technologies fast monterey january mckusick joy lef fabry fast file system unix acm transactions computer systems august morton data corrupting bug ext http uwsg hypermail linux kernel html dec ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference anaheim june padhye floyd inferring tcp behavior sigcomm san deigo august regehr inferring scheduling behavior hourglass proceedings usenix annual technical conference freenix track monterey june riedel gibson faloutsos active storage large-scale data mining multimedia vldb york august rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler grif lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey january seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems proceedings usenix annual technical conference pages san diego june swartz brave toaster meets usenet lisa pages chicago illinois october talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans february wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey june gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings fourth symposium operating systems design implementation osdi san diego october zhou philbin multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference boston june 
petal distributed virtual disks edward lee chandramohan thekkath systems research center digital equipment corporation lytton ave palo alto abstract ideal storage system globally accessible unlimited performance capacity large number clients requires management paper describes design implementation performance petal system attempts approximate ideal practice combination features petal consists collection networkconnected servers cooperatively manage pool physical disks petal client collection appears highly block-level storage system large abstract containers called virtual disks virtual disk globally accessible petal clients network client create virtual disk demand tap entire capacity performance underlying physical resources additional resources servers disks automatically incorporated petal initial petal prototype consisting mhz dec workstations running digital unix connected mbit atm network prototype clients virtual disks tolerate recover disk server network failures latency comparable locally attached disk throughput scales number servers prototype achieve rates requests sec bandwidth mbytes sec introduction managing large storage systems expensive complicated process single component failure halt entire system requires considerable time effort resume operation capacity performance individual components system periodically monitored balanced reduce fragmentation eliminate hot spots requires manually moving partitioning replicating files directories paper describes design implementation performance petal easy-to-manage distributed storage system clients file systems databases view petal collection virtual disks shown figure petal virtual disk permission make digital hard copy part work personal classroom granted fee provided copies made distributed profit commercial advantage copyright notice title date notice copying perm ssion acm copy republish post servers redistribute lists requires prior specific permission fee asplos vii usa acm lfs petal client ntfs petal client petal client scalable network petal --a virtual disk bsd ffs petal client figure client view container sparse -bit byte storage space ordinary magnetic disks data read written petal virtual disks blocks addition combination characteristics reduce complexity managing large storage systems tolerate recover single component failure disk server network geographically distributed tolerate site failures power outages natural disasters transparently reconfigures expand performance capacity servers disks added uniformly balances load capacity servers system fast efficient support backup recovery environments multiple types clients file servers databases petal virtual disks cleanly separate client view storage physical resources implement share physical resources flexibly clients offer important services snapshots incremental expandability efficient manner disk-like interface offered petal lower-level service distributed file system distributed file system efficiently implemented top petal resulting system cost scalable network figure physical view disk storage disk storage disk storage disk storage figure petal server modules effective comparable distributed file system implementation accesses local disks directly separating system cleanly block-level storage system file system handling distributed systems problems block-level storage system system easier model design implement tune simplicity important design expected scale large size provide reliable data storage long period time additional benefit block-level interface supporting heterogeneous clients client applications easily support types file systems databases implemented petal servers alpha workstations running digital unix connected digital atm network petal client interface exists digital unix implemented kernel device driver allowing standard unix applications utilities file systems run unmodified petal implementation exhibits graceful scaling performance comparable local disks providing significant functionality design petal shown figure petal consists pool distributed storage servers cooperatively implement single block-level storage system clients view storage system collection virtual disks access petal services remote procedure call rpc interface basic principle design petal rpc interface maintain state needed ensuring integrity storage system servers maintain hints clients clients maintain small amount high-level mapping information route read write requests server request inappropriate server server returns error code causing client update hints retry request figure illustrates software structure petal ovals represents software module arrows module modules liveness module global state module manage distributed system aspect petal liveness module ensures servers system agree operational status running crashed service modules notably global state manager guarantee continuous consistent operation system face server communication failures operation liveness module based majority tfs consensus periodic exchange alive alive messages servers message exchanges timely manner ensure progress arbitrarily delayed reordered affecting correctness petal maintains information describes current members storage system supported virtual disks information replicated petal servers system global state manager responsible consistently maintaining information megabyte current implementation algorithm maintaining global state based leslie lamport paxos part-time parliament algorithm implementing distributed replicated state machines algorithm assumes servers fail ceasing operate networks reorder lose messages algorithm ensures correctness face arbitrary combinations server communication failures recoveries guarantees progress long majority servers communicate ensures management operations petal creating deleting snapshotting virtual disks adding deleting servers fault tolerant modules deal servicing read write requests issued petal clients data access recovery modules control client data distributed stored petal storage system set data access recovery modules exists type redundancy scheme supported system support simple data striping redundancy replication-based redundancy scheme called chaineddeclustering desired redundancy scheme virtual disk virtual disk created subsequently redundancy scheme attributes transparently changed process called virtual disk reconfiguration virtual-to-physical address translation module common routines data access recovery modules routines translate virtual disk offsets physical disk addresses rest section examine specific aspects system greater detail virtual physical translation section describes petal translates virtual disk addresses clients physical disk addresses basic problem translate virtual addresses form virtualdisk-identifier offset physical addresses form serveridentifier disk-identifier disk-offset translation consistently efficiently distributed system events alter virtual disk address translation server vdiskld offset serverld diskld dlskoffset vdiskld offset diskld diskoffset figure virtual physical mapping failure recovery occur unexpectedly figure illustrates basic data structures steps translation procedure important data structures virtual disk directory vdir global map gmap physical map pmap dotted lines virtual disk directory global map global data structures replicated consistently updated servers global state manager server physical map local server translating client-supplied virtual disk identifier offset disk offset occurs steps shown figure virtual disk directory translates client-supplied virtual disk identifier global map identifier global map determines server responsible translating offset physical map server translates global map identifier offset physical disk offset disk minimize communication cases server performs translation step server performs translation step client initially request server server perform steps translation locally communicating server global map virtual disk specifies tuple servers spanned virtual disk redundancy scheme protect client data stored virtual disk tolerate server failures secondary server assigned responsibility mapping offset primary global 
maps immutable change virtual disk tuple servers redundancy scheme virtual disk assigned global map section describing reconfiguration details process physical map actual data structure translate offset virtual disk physical disk offset disk similar page table virtual memory system physical map entry translates kbyte region physical disk server performs translation perform disk operations needed service original client request separation translation data structures global local physical maps bulk mapping information local minimizes amount information global data structures replicated expensive update support backup petal attempts simplify client backup procedure providing common mechanism applied clients automate backup recovery data stored system mechanism petal fast efficient snapshots virtual disks copy-on-write techniques petal quickly create exact copy virtual disk point time client treats snapshot virtual disk modified supporting snapshots requires slightly complicated virtual-to-physical translation procedure previous section virtual disk directory translate virtual disk identifier global map identifier tuple global-map-identifier epoch-number epoch-number monotonically increasing version number distinguishes data stored virtual disk offset points time tuple global-map-identifier epoch-number physical map step translation system creates snapshot virtual disk tuple epoch number created virtual disk directory accesses original virtual disk made epoch number older epoch number newly created snapshot ensures data written original virtual disk create entries epoch overwriting data previous epoch read requests find data recently written offset recent epoch creating snapshot consistent client application level requires pausing application time takes create petal snapshot alternative approach require pausing application create crash-consistent snapshot snapshot similar disk image left application crashed snapshots made consistent application level running application-dependent recovery program sck case unix file systems implementing crash-consistent snapshots supported snapshots on-line facilitate recovery accidentally deleted files snapshot behaves read-only local disk petal client create consistent archives data utilities ear incremental reconfiguration occasionally desirable change virtual disk redundancy scheme set servers mapped change precipitated addition removal disks servers section describes petal incorporates disks servers existing virtual disks reconfigured advantage resources processes point view adding resources easily generalized removal resources process referred virtual disk reconfiguration primary focus section addition adisk server handled locally server subsequent storage allocation requests automatically disk consideration load balance desirable redistribute previously allocated storage disk redistribution easily accomplished part local background process periodically moves data disks implemented background process petal nonetheless existing data redistributed newly added disks side-effect virtual disk reconfiguration addition petal server global operation composed steps involving global state management module liveness module server added membership petal storage system server participate future global operations sets servers liveness module determining server adjusted incorporate server finally existing virtual disks reconfigured advantage server process virtual-to-physical translation procedure section absence activity system virtual disk reconfiguration trivially implemented create global map desired redundancy scheme server mapping change virtual disk directory entries refer global map refer redistribute data servers translations global map data distribution potentially require substantial amounts network disk traffic challenge perform reconfiguration incrementally concurrently processing normal client requests find acceptable procedure takes hours degrade performance system significantly virtual disk reconfigured server added performance virtual disk gradually increase reconfiguration level reconfiguration level reconfiguration describe reconfiguration algorithm steps describe basic algorithm refinement algorithm refined algorithm implemented system basic algorithm steps executed starting translations recent epoch moved data transferred collection servers global map amount data moved reconfiguration long time complete meantime clients read write data virtual disk reconfigured accommodate requests read write procedures designed function client read request serviced global map translation found global map ensures translations moved found global map client write requests access global map move data starting recent epoch ensure read requests return data older epoch requested client main limitation basic algorithm server mappings entire virtual disk changed data moved means client read request submitted based global map miss global virtual disk arver server server server figure chained-declustering map forwarded require additional communication servers potential degrade performance system refined algorithm solves limitation basic algorithm relocating small portions virtual disk time basic idea break virtual disk address range regions fenced requests regions simply global maps requests fenced region basic algorithm relocated fenced region region fence part region repeat moved data region region keeping relative size fenced region small roughly ten percent entire range minimize forwarding overhead guard fencing heavily subrange virtual disk construct fenced region collecting small non-contiguous ranges distributed virtual disk single contiguous region data access recovery section describes petal chained-declustered data access recovery modules modules give clients highly access data automatically bypassing failed components dynamic load balancing eliminates system bottlenecks ensuring uniform load distribution face component failures start describing basic idea chained-declustering move detailed descriptions read write operation figure illustrates chained-declustered data placement scheme dotted rectangle emphasizes data storage servers single virtual disk clients sequence letters represents block data stored storage system note copies block data stored neighboring servers pair neighboring servers data blocks common arrangement server fails servers automatically share server read load server experience load increase performing dynamic load balancing server copies data servers servers offload normal read load server achieve uniform load balancing chaining data placement server offload read load server immediately preceding server cascading offloading multiple servers uniform load maintained surviving servers contrast simple mirrored redundancy scheme replicates data stored servers failure result load increase opportunities dynamic load balancing system stripes mirrored servers load increase single server reduce system throughput current prototype implements simple dynamic load balancing scheme client track number requests pending server sends read requests server shorter queue length works requests generated clients work requests generated clients occasionally issue requests choice load balancing algorithm active area research petal project additional advantage chained-declustering placing even-numbered servers site oddnumbered servers site tolerate site failures disadvantage chained-declustering relative simple mirroring reliable simple mirroring server failed failure mirror server result data unavailable chained-declustering ifa server falls failure neighboring servers result data unavailable implementation chalned-declustering copies data block denoted primary denoted secondary read requests serviced primary secondary copy servicing write requests start primary server primary case start secondary lock copies data blocks reading writing guarantee consistency ordering guarantee avoid deadlocks read request server receives request attempts read requested data successful server returns requested data returns error code client server request times due network congestion server client alternately retry primary 
secondary servers request succeeds servers return error codes indicating satisfy request disks copies requested data destroyed write request server receives request checks primary data element primary marks data element busy stable storage simultaneously sends write requests local copy secondary copy requests complete busy bit cleared client issued request status code indicating success failure operation primary crashes performing update busy bits crash recovery ensure primary secondary copies consistent write-ahead-logging group commits makes updating busy bits efficient optimization cleating busy bits lazily maintain cache recently set busy bits write requests display locality busy bit set disk require additional server received write request secondary data element service request determine server primary copy lio -iio- digital atm network figure petal prototype case secondary marks data element stale stable storage writing local disk server primary copy eventually bring data elements marked stale up-to-date recovery process similar procedure primary secondary dies implementation performance petal prototype illustrated figure mhz dec running digital unix act server machines runs single petal server user-level process accesses physical disks unix raw disk interface network udp unix sockets server machine configured digital disks inch scsi device gbyte capacity machine disks write-ahead logging remaining store client data disks connected server machine mbyte fast scsi strings digital pmzaa-c host bus adapter additional machines running digital unix configured petal clients generate load servers client kernel loaded petal device driver accessing petal virtual disks clients access petal virtual disks local disks servers clients connected mbit atm links digital atm network entire petal rpc interface calls calls devoted management functions creating deleting virtual disks making snapshots reconfiguring virtual disk adding deleting servers calls typically user-level utilities perform tasks virtual disk creation monitoring physical resource pools system determine additional servers disk added petal rpc calls implement management functions infrequently executed generally complete create snapshot operations milliseconds delete reconfiguration milliseconds initiate total execution time dependent actual amount physical storage virtual disk remainder section report performance accessing petal virtual disk behavior file systems built petal primary performance goals provide latency roughly comparable locally attached disk throughclient request latency local disk petal request byte read log nvram log kbyte read kbyte read byte write kbyte write kbyte write table latency chained-declustered virtual disk put scales number servers performance gracefully degrades servers fail petal performance section examines read write performance petal chain-declustered virtual disk read request client makes rpc petal server simply returns data local disk server receives write request writes small log entry recover consistent state server crash server simultaneously writes data local disk disk mirror server disk writes complete primary server replies client read write procedures petal greater detail section table compares read write latency chaineddeclustered petal virtual disk local disk experiment single client generates requests size random disk offsets show petal performance kinds write-ahead-logging devices disk nvram device simulated ram log device service write requests affect read performance logging nvram improves write latency approximately read requests bytes kbytes petal latency slightly worse kbyte reads latency gap widens increased latency due additional delay transmitting data network includes unix socket udp atm hardware overheads accounts petal server software client interface overheads negligible overlapped reading data disks transfer data network eliminate overhead nvram log device petal write performance worse local disk addition network delay sending data primary server additional delay primary send data mirror server wait acknowledgment returning client latencies due network transmissions approximately byte kbyte kbyte write requests arms spindles primary secondary disks unsynchronized lack synchronization write requests wait slower primary secondary disk writes column table shows peak throughput chained-declustered petal virtual disk log device peak write throughput higher nvram log device small request sizes express throughput number requests larger request sizes shown megabytes measure aggregate throughput log request normal failed normal byte read req req kbyte read mbytes mbytes kbyte read mbytes mbytes byte write req kbyte write mbytes kbyte write mbytes req mbytes mbytes table normal failed throughput chained-declustered virtual disk byte read kbyte read kbyte read byte write kbyte write kbyte write number servers figure scaling increased servers peak throughput petal clients shown figure make random requests single petal virtual disk throughput limited cpu overheads cases server cpu approximately utilized significant fraction time spent copying checksumming data network access petal servers run user-level standard unix socket interface udp protocol stacks techniques streamlining network accesses understood experiment eliminated copying checksums network layer large read requests kbyte read requests optimization reduced cpu utilization increased throughput mbytes mbytes case throughput limited disk controller column table shows performance chaindeclustered petal disk servers crashed read requests performance normal three-quarters servers three-quarters normal performance data placement dynamic load balancing schemes working effectively redistribute load write performance failure normal case servers fail virtual disk addresses managed servers longer mirrored reduces number disk writes system fraction failed servers load surviving server server failure figure shows effect scaling petal servers throughput request type normalized respect maximum throughput request type elapsed time seconds phase petal petal create directories copy files directory status scan files compile table modified andrew benchmark system configurations measured large determine scaling remain linear observed scaling promising file system performance petal clients large virtual disk clients network cluster file systems xfs parallel databases oracle parallel server advantage fact concurrently accessing single virtual disk multiple machines systems widely restrict attention digital unix file system ufs advanced file system advfs table compares performance modified andrew benchmark configurations ufs locally attached disk ufs petal virtual disk advfs collection locally attached disks advfs petal virtual disk petal virtual disk configured chain-declustered data placement disk logging modified andrew benchmark phases phase recursively creates subdirectories phase measures file system data transfer capabilities phase recursively examines status directories files contained fourth phase scans contents data stored file final phase indicative program development phase computationally intensive cases file system level performance petal virtual disk comparable locally attached disks exception phase benchmark ufs generates synchronous writes mentioned earlier writes chalned-declustered petal virtual disk incur logging overheads increase synchronous write latency advfs journals meta-data updates reduce number synchronous writes suffer overheads running petal achieves higher performance ufs phase benchmark local disk measurements ufs single disk 
advfs disks achieve similar performance modified andrew benchmark primarily stresses latency throughput storage system case compilation phase performance primarily limited speed cpu discussion availability cost-effective scalable networks driving force work thinking network primary system-level interconnect build incrementally expandable distributed storage systems availability capacity performance current centralized storage systems distributed storage systems pose difficult management consistency problems petal experiment address problems petal virtual disks hide distributed nature system clients independent applications share performance capacity physical storage resources system transparently incorporate storage components provide convenient management features snapshots provide special support protecting client data clients difficult provide security virtual disk basis petal virtual disk abstraction adds additional level overhead prevent application-specific disk optimizations rely careful placement data problem reasonable tradeoff benefits petal provide view virtualization current trend sophisticated disk array controllers scsi disks obscure physical disk geometry fact petal server approximately complexity raid controller similar hardware resource requirements petal disk-like interface clients read write blocks data chose interface easily integrated existing computer system transparently support existing file systems databases alternative petal design distributed storage richer interface file system cmu nasd project potentially result system efficient simpler petal interface adequate higher level services efficiently built top petal framework sufficiently general incorporate classes redundancyschemes based parity chosen concentrate replication-based redundancy schemes chained-declustering impose higher capacity overhead readily applicable tolerating site failures present opportunities dynamic load balancing easier implement efficiently distributed systems related work section describes work related petal terms primary characteristics type abstraction block-level file-systemlevel degree distribution level fault tolerance support incremental expandability related block-level storage systems include raid-ii tickertaip logical disk loge mime autoraid swift systems support simple algorithmic mappings address space client underlying physical disks mapping completely system configured contrast autoraid logical disk loge mime petal support flexible mappings index data structures autoraid petal systems support creation multiple virtual disks block-level systems including autoraid support distribution multiple nodes geographically distributed sites exceptions tickertaip swift provide support distributing data multiple nodes assume communication interconnect reliable deal full range distributed systems issues addressed petal systems tolerate disk failures tickertaip tolerate node failures contrast petal supports wider distribution tolerate node network failures closely related file systems include xfs zebra echo afs systems xfs single meta-data server partial subtree file system space ultimately limiting scalability xfs distribute management meta-data multiple nodes object-by-object basis file systems suffer problem file disk systems considered incrementally expandable sense data dumped tape restored adding extra components reconfiguring system systems step zebra autoraid disks incorporated system dynamically transparently respect clients afs nodes added volumes partial subtrees file system space moved nodes transparently afs volume span single node contrast petal virtual disk span multiple nodes goal xfs design change management node file dynamically load balancing response node additions deletions functionality implemented petal supports addition deletion nodes system face arbitrary node network failures petal virtual disk span multiple nodes transparently reconfigured advantage additional nodes reconfiguration transparent petal clients knowledge petal distributed block-level storage system supports virtual containers managing physical resources difficult storage system larger distributed found distribution virtual containers powerful combined distribution system scale large sizes virtual containers make easier allocate physical resources efficiently large-scale systems petal storage system supports transparent addition deletion nodes existing storage containers face arbitrary component network failures system-level performance single container scale gracefully additional nodes added summary conclusions petal distributed block-level storage system tolerates recovers single component failure dynamically balances load servers transparently expands performance capacity principal goal design storage system heterogeneous environments easy manage scale gracefully capacity performance significantly increasing cost managing system found combination features achieve goal actual system significant period time conclusively prove assertion designing petal decided distributed software solutions hardware solutions applicable software hardware tradeoff petal strategy fault tolerance distributed mirroring providing redundant hardware paths disk approach makes easier geographically distribute system scale larger system sizes tradeoff distributed algorithms determine servers failed generally achieve consensus reliable communication hardware specialized hardware synchronization petal block-level file-level interface petal handle heterogeneous client file systems gracefully choice block-level interface greatly simplified work adversely limiting functionality provide opens possibility encapsulating petal server software disk array controller raid software encapsulated disk array controllers today petal virtual disks proved invaluable separating client view storage physical resources system virtualization makes easier allocate physical resources heterogeneous clients enabled features snapshots transparent incremental expandability generally satisfied performance prototype read write latencies chain-declustered petal virtual disk larger locally attached disk achieve rates requests sec small read requests bandwidth mbytes sec large read requests throughput write request understand improve performance significantly performance petal degrades gracefully fraction number failed servers throughput system scales number servers measured sufficiently large system determine performance scaling linear feel confident prototype running past months working building larger production system deployment day-to-day laboratory acknowledgments authors roger needham mike schroeder bill weihl anonymous referees comments earlier drafts paper cynthia hibbard provided valuable editorial assistance thomas anderson michael dahlin jeanna neefe david patterson drew roselli randolph wang serverless network file systems acm transactions computer systems february thomas anderson susan owicki james saxe charles thacker high-speed switch scheduling localarea networks acm transactions computer systems november andrew birrell bruce jay nelson implementing remote procedure calls acm transactions computer systems february luis-felipe cabrera darrel long swift distributed disk striping provide high data rates acm computing systems fall pei cao swee boon lim shivakumar venkataraman john wilkes tickertaip parallel raid architecture acm transactions computer systems august chao english jacobson stepanov wilkes mime high performance parallel storage device strong recovery guarantees technical report hpl-csp- hewlett-packard laboratories november peter chen edward lee ann drapeau ken lutz ethan miller srinivasan seshan ken shirriff david patterson randy katz performance design evaluation raid-ii storage server journal distributed parallel databases july wiebren jonge frans kaashoek wilson hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles pages december peter druschel larry peterson bruce davie experiences high-speed network adaptor software perspective proceedings sigcomm symposium communications architectures protocols applications pages august english stepanov loge self-organizing disk controller proceedings winter usenix conference pages january garth gibson david nagle khalil amiri fay chang eugene feinberg howard gobioff chen lee berend ozceri erik riedel david rochberg case network-attached 
secure disks technical report cmu-cs- department electrical computer engineering carnegie-mellon june john hartman john ousterhout zebra striped network file system acm transactions computer systems august hui-i hsiao david dewitt chained declustering availability strategy multiprocessor database machines technical report wisconsin madison june leslie lamport part-time parliament technical report digital equipment corporation systems research center lytton ave palo alto september timothy mann andrew birrell andy hisgen chuck jerian garret swart coherent distributed file cache directory write-behind transactions computer systems satyanarayanan scalable secure highly distributed file access ieee computer daniel stodolsky mark holland william courtright garth gibson parity-logging disk arrays acm transactions computer systems august chandramohan thekkath henry levy limits low-latency communication high-speed networks acm transactions computer systems john wilkes richard golding carl staelin tim sullivan autoraid hierarchical storage system proceedings acm symposium operating systems principles pages december 
mapreduce simpli data processing large clusters jeffrey dean sanjay ghemawat jeff google sanjay google google abstract mapreduce programming model implementation processing generating large data sets users map function processes key pair generate set intermediate key pairs reduce function merges intermediate values intermediate key real world tasks expressible model shown paper programs written functional style automatically parallelized executed large cluster commodity machines run-time system takes care details partitioning input data scheduling program execution set machines handling machine failures managing required inter-machine communication programmers experience parallel distributed systems easily utilize resources large distributed system implementation mapreduce runs large cluster commodity machines highly scalable typical mapreduce computation processes terabytes data thousands machines programmers system easy hundreds mapreduce programs implemented upwards thousand mapreduce jobs executed google clusters day introduction past years authors google implemented hundreds special-purpose computations process large amounts raw data crawled documents web request logs compute kinds derived data inverted indices representations graph structure web documents summaries number pages crawled host set frequent queries day computations conceptually straightforward input data large computations distributed hundreds thousands machines order nish reasonable amount time issues parallelize computation distribute data handle failures conspire obscure original simple computation large amounts complex code deal issues reaction complexity designed abstraction express simple computations perform hides messy details parallelization fault-tolerance data distribution load balancing library abstraction inspired map reduce primitives present lisp functional languages realized computations involved applying map operation logical record input order compute set intermediate key pairs applying reduce operation values shared key order combine derived data appropriately functional model userspeci map reduce operations parallelize large computations easily re-execution primary mechanism fault tolerance major contributions work simple powerful interface enables automatic parallelization distribution large-scale computations combined implementation interface achieves high performance large clusters commodity pcs section describes basic programming model examples section describes implementation mapreduce interface tailored cluster-based computing environment section describes nements programming model found section performance measurements implementation variety tasks section explores mapreduce google including experiences basis osdi rewrite production indexing system section discusses related future work programming model computation takes set input key pairs produces set output key pairs user mapreduce library expresses computation functions map reduce map written user takes input pair produces set intermediate key pairs mapreduce library groups intermediate values intermediate key passes reduce function reduce function written user accepts intermediate key set values key merges values form possibly smaller set values typically output produced reduce invocation intermediate values supplied user reduce function iterator handle lists values large memory problem counting number occurrences word large collection documents user write code similar pseudo-code map string key string key document document contents word emitintermediate reduce string key iterator values key word values list counts int result values result parseint emit asstring result map function emits word count occurrences simple reduce function sums counts emitted word addition user writes code mapreduce speci cation object names input output les optional tuning parameters user invokes mapreduce function passing speci cation object user code linked mapreduce library implemented appendix full program text types previous pseudo-code written terms string inputs outputs conceptually map reduce functions supplied user types map list reduce list list input keys values drawn domain output keys values intermediate keys values domain output keys values implementation passes strings user-de ned functions leaves user code convert strings types examples simple examples interesting programs easily expressed mapreduce computations distributed grep map function emits line matches supplied pattern reduce function identity function copies supplied intermediate data output count url access frequency map function processes logs web page requests outputs hurl reduce function adds values url emits hurl total counti pair reverse web-link graph map function outputs htarget sourcei pairs link target url found page named source reduce function concatenates list source urls target url emits pair htarget list source term-vector host term vector summarizes important words occur document set documents list hword frequencyi pairs map function emits hhostname term vectori pair input document hostname extracted url document reduce function passed per-document term vectors host adds term vectors throwing infrequent terms emits nal hhostname term vectori pair osdi user program master fork worker fork worker fork assign map assign reduce split split split split split output file write worker read worker local write map phase intermediate files local disks worker outputfile input files remote read reduce phase output files figure execution overview inverted index map function parses document emits sequence hword document idi pairs reduce function accepts pairs word sorts document ids emits hword list document pair set output pairs forms simple inverted index easy augment computation track word positions distributed sort map function extracts key record emits hkey recordi pair reduce function emits pairs unchanged computation depends partitioning facilities section ordering properties section implementation implementations mapreduce interface choice depends environment implementation suitable small shared-memory machine large numa multi-processor larger collection networked machines section describes implementation targeted computing environment wide google large clusters commodity pcs connected switched ethernet environment machines typically dual-processor processors running linux memory machine commodity networking hardware typically megabits gigabit machine level averaging considerably bisection bandwidth cluster consists hundreds thousands machines machine failures common storage provided inexpensive ide disks attached directly individual machines distributed system developed in-house manage data stored disks system replication provide availability reliability top unreliable hardware users submit jobs scheduling system job consists set tasks mapped scheduler set machines cluster execution overview map invocations distributed multiple machines automatically partitioning input data osdi set splits input splits processed parallel machines reduce invocations distributed partitioning intermediate key space pieces partitioning function hash key mod number partitions partitioning function speci user figure shows mapreduce operation implementation user program calls mapreduce function sequence actions occurs numbered labels figure correspond numbers list mapreduce library user program rst splits input les pieces typically megabytes megabytes piece controllable user optional parameter starts copies program cluster machines copies program special master rest workers assigned work master map tasks reduce tasks assign master picks idle workers assigns map task reduce task worker assigned map task reads contents input split parses key pairs input data passes pair user-de ned map function intermediate key pairs 
produced map function buffered memory periodically buffered pairs written local disk partitioned regions partitioning function locations buffered pairs local disk passed back master responsible forwarding locations reduce workers reduce worker noti master locations remote procedure calls read buffered data local disks map workers reduce worker read intermediate data sorts intermediate keys occurrences key grouped sorting needed typically keys map reduce task amount intermediate data large memory external sort reduce worker iterates sorted intermediate data unique intermediate key encountered passes key set intermediate values user reduce function output reduce function appended nal output reduce partition map tasks reduce tasks completed master wakes user program point mapreduce call user program returns back user code successful completion output mapreduce execution output les reduce task names speci user typically users combine output les pass les input mapreduce call distributed application deal input partitioned multiple les master data structures master data structures map task reduce task stores state idle in-progress completed identity worker machine non-idle tasks master conduit location intermediate regions propagated map tasks reduce tasks completed map task master stores locations sizes intermediate regions produced map task updates location size information received map tasks completed information pushed incrementally workers in-progress reduce tasks fault tolerance mapreduce library designed process large amounts data hundreds thousands machines library tolerate machine failures gracefully worker failure master pings worker periodically response received worker amount time master marks worker failed map tasks completed worker reset back initial idle state eligible scheduling workers similarly map task reduce task progress failed worker reset idle eligible rescheduling completed map tasks re-executed failure output stored local disk failed machine inaccessible completed reduce tasks re-executed output stored global system map task executed rst worker executed worker failed osdi workers executing reduce tasks noti reexecution reduce task read data worker read data worker mapreduce resilient large-scale worker failures mapreduce operation network maintenance running cluster causing groups machines time unreachable minutes mapreduce master simply re-executed work unreachable worker machines continued make forward progress eventually completing mapreduce operation master failure easy make master write periodic checkpoints master data structures master task dies copy started checkpointed state single master failure current implementation aborts mapreduce computation master fails clients check condition retry mapreduce operation desire semantics presence failures user-supplied map reduce operators deterministic functions input values distributed implementation produces output produced non-faulting sequential execution entire program rely atomic commits map reduce task outputs achieve property in-progress task writes output private temporary les reduce task produces map task produces les reduce task map task completes worker sends message master includes names temporary les message master receives completion message completed map task ignores message records names les master data structure reduce task completes reduce worker atomically renames temporary output nal output reduce task executed multiple machines multiple rename calls executed nal output rely atomic rename operation provided underlying system guarantee nal system state data produced execution reduce task vast majority map reduce operators deterministic fact semantics equivalent sequential execution case makes easy programmers reason program behavior map reduce operators nondeterministic provide weaker reasonable semantics presence non-deterministic operators output reduce task equivalent output produced sequential execution non-deterministic program output reduce task correspond output produced sequential execution non-deterministic program map task reduce tasks execution committed execution weaker semantics arise read output produced execution read output produced execution locality network bandwidth scarce resource computing environment conserve network bandwidth taking advantage fact input data managed gfs stored local disks machines make cluster gfs divides blocks stores copies block typically copies machines mapreduce master takes location information input les account attempts schedule map task machine replica input data failing attempts schedule map task replica task input data worker machine network switch machine data running large mapreduce operations signi fraction workers cluster input data read locally consumes network bandwidth task granularity subdivide map phase pieces reduce phase pieces ideally larger number worker machines worker perform tasks improves dynamic load balancing speeds recovery worker fails map tasks completed spread worker machines practical bounds large implementation master make scheduling decisions state memory constant factors memory usage small piece state consists approximately byte data map task reduce task pair osdi constrained users output reduce task ends separate output practice tend choose individual task roughly input data locality optimization effective make small multiple number worker machines expect perform mapreduce computations worker machines backup tasks common lengthens total time mapreduce operation straggler machine takes unusually long time complete map reduce tasks computation stragglers arise host reasons machine bad disk experience frequent correctable errors slow read performance cluster scheduling system scheduled tasks machine causing execute mapreduce code slowly due competition cpu memory local disk network bandwidth recent problem experienced bug machine initialization code caused processor caches disabled computations affected machines slowed factor hundred general mechanism alleviate problem stragglers mapreduce operation close completion master schedules backup executions remaining in-progress tasks task marked completed primary backup execution completes tuned mechanism typically increases computational resources operation percent found signi cantly reduces time complete large mapreduce operations sort program section takes longer complete backup task mechanism disabled nements basic functionality provided simply writing map reduce functions suf cient found extensions section partitioning function users mapreduce number reduce tasks output les desire data partitioned tasks partitioning function intermediate key default partitioning function provided hashing hash key mod result fairly well-balanced partitions cases partition data function key output keys urls entries single host end output support situations user mapreduce library provide special partitioning function hash hostname urlkey mod partitioning function urls host end output ordering guarantees guarantee partition intermediate key pairs processed increasing key order ordering guarantee makes easy generate sorted output partition output format support cient random access lookups key users output convenient data sorted combiner function cases signi repetition intermediate keys produced map task userspeci reduce function commutative associative good word counting section word frequencies tend follow zipf distribution map task produce hundreds thousands records form counts network single reduce task added reduce function produce number user optional combiner function partial merging data network combiner function executed machine performs map task typically code implement combiner reduce functions difference 
reduce function combiner function mapreduce library handles output function output reduce function written nal output output combiner function written intermediate reduce task partial combining signi cantly speeds classes mapreduce operations appendix combiner input output types mapreduce library support reading input data formats text osdi mode input treats line key pair key offset contents line common supported format stores sequence key pairs sorted key input type implementation split meaningful ranges processing separate map tasks text mode range splitting ensures range splits occur line boundaries users add support input type providing implementation simple reader interface users small number prede ned input types reader necessarily provide data read easy reader reads records database data structures mapped memory similar fashion support set output types producing data formats easy user code add support output types side-effects cases users mapreduce found convenient produce auxiliary les additional outputs map reduce operators rely application writer make side-effects atomic idempotent typically application writes temporary atomically renames fully generated provide support atomic two-phase commits multiple output les produced single task tasks produce multiple output les crossle consistency requirements deterministic restriction issue practice skipping bad records bugs user code map reduce functions crash deterministically records bugs prevent mapreduce operation completing usual action bug feasible bug third-party library source code unavailable acceptable ignore records statistical analysis large data set provide optional mode execution mapreduce library detects records deterministic crashes skips records order make forward progress worker process installs signal handler catches segmentation violations bus errors invoking user map reduce operation mapreduce library stores sequence number argument global variable user code generates signal signal handler sends gasp udp packet sequence number mapreduce master master failure record record skipped issues re-execution map reduce task local execution debugging problems map reduce functions tricky actual computation distributed system thousand machines work assignment decisions made dynamically master facilitate debugging pro ling small-scale testing developed alternative implementation mapreduce library sequentially executes work mapreduce operation local machine controls provided user computation limited map tasks users invoke program special easily debugging testing tools gdb status information master runs internal http server exports set status pages human consumption status pages show progress computation tasks completed progress bytes input bytes intermediate data bytes output processing rates pages links standard error standard output les generated task user data predict long computation resources added computation pages gure computation slower expected addition top-level status page shows workers failed map reduce tasks processing failed information attempting diagnose bugs user code counters mapreduce library counter facility count occurrences events user code count total number words processed number german documents indexed facility user code creates named counter object increments counter appropriately map reduce function osdi counter uppercase uppercase getcounter uppercase map string string contents word contents iscapitalized uppercaseincrement emitintermediate counter values individual worker machines periodically propagated master piggybacked ping response master aggregates counter values successful map reduce tasks returns user code mapreduce operation completed current counter values displayed master status page human watch progress live computation aggregating counter values master eliminates effects duplicate executions map reduce task avoid double counting duplicate executions arise backup tasks re-execution tasks due failures counter values automatically maintained mapreduce library number input key pairs processed number output key pairs produced users found counter facility sanity checking behavior mapreduce operations mapreduce operations user code ensure number output pairs produced equals number input pairs processed fraction german documents processed tolerable fraction total number documents processed performance section measure performance mapreduce computations running large cluster machines computation searches approximately terabyte data pattern computation sorts approximately terabyte data programs representative large subset real programs written users mapreduce class programs shuf data representation class extracts small amount interesting data large data set cluster con guration programs executed cluster consisted approximately machines machine ghz intel xeon processors hyperthreading enabled memory ide seconds input figure data transfer rate time disks gigabit ethernet link machines arranged two-level tree-shaped switched network approximately gbps aggregate bandwidth root machines hosting facility round-trip time pair machines millisecond memory approximately reserved tasks running cluster programs executed weekend afternoon cpus disks network idle grep grep program scans -byte records searching rare three-character pattern pattern occurs records input split approximately pieces entire output figure shows progress computation time y-axis shows rate input data scanned rate gradually picks machines assigned mapreduce computation peaks workers assigned map tasks nish rate starts dropping hits seconds computation entire computation takes approximately seconds start nish includes minute startup overhead overhead due propagation program worker machines delays interacting gfs open set input les information needed locality optimization sort sort program sorts -byte records approximately terabyte data program modeled terasort benchmark sorting program consists lines user code three-line map function extracts -byte sorting key text line emits key osdi input shuffle seconds output normal execution input shuffle seconds output backup tasks input shuffle seconds output tasks killed figure data transfer rates time executions sort program original text line intermediate key pair built-in identity function reduce operator functions passes intermediate key pair unchanged output key pair nal sorted output written set -way replicated gfs les terabytes written output program input data split pieces partition sorted output les partitioning function initial bytes key segregate pieces partitioning function benchmark builtin knowledge distribution keys general sorting program add pre-pass mapreduce operation collect sample keys distribution sampled keys compute splitpoints nal sorting pass figure shows progress normal execution sort program top-left graph shows rate input read rate peaks dies fairly quickly map tasks nish seconds elapsed note input rate grep sort map tasks spend half time bandwidth writing intermediate output local disks intermediate output grep negligible size middle-left graph shows rate data network map tasks reduce tasks shuf ing starts rst map task completes rst hump graph rst batch approximately reduce tasks entire mapreduce assigned machines machine executes reduce task time roughly seconds computation rst batch reduce tasks nish start shuf ing data remaining reduce tasks shuf ing seconds computation bottom-left graph shows rate sorted data written nal output les reduce tasks delay end rst shuf ing period start writing period machines busy sorting intermediate data writes continue rate writes 
nish seconds computation including startup overhead entire computation takes seconds similar current reported result seconds terasort benchmark things note input rate higher shuf rate output rate locality optimization data read local disk bypasses bandwidth constrained network shuf rate higher output rate output phase writes copies sorted data make replicas output reliability availability reasons write replicas mechanism reliability availability provided underlying system network bandwidth requirements writing data reduced underlying system erasure coding replication osdi effect backup tasks figure show execution sort program backup tasks disabled execution similar shown figure long tail write activity occurs seconds reduce tasks completed stragglers don ish seconds entire computation takes seconds increase elapsed time machine failures figure show execution sort program intentionally killed worker processes minutes computation underlying cluster scheduler immediately restarted worker processes machines processes killed machines functioning properly worker deaths show negative input rate previously completed map work disappears map workers killed redone re-execution map work quickly entire computation ishes seconds including startup overhead increase normal execution time experience wrote rst version mapreduce library february made signi enhancements august including locality optimization dynamic load balancing task execution worker machines time pleasantly surprised broadly applicable mapreduce library kinds problems work wide range domains google including large-scale machine learning problems clustering problems google news froogle products extraction data produce reports popular queries google zeitgeist extraction properties web pages experiments products extraction geographical locations large corpus web pages localized search large-scale graph computations number instances source tree figure mapreduce instances time number jobs average job completion time secs machine days days input data read intermediate data produced output data written average worker machines job average worker deaths job average map tasks job average reduce tasks job unique map implementations unique reduce implementations unique map reduce combinations table mapreduce jobs run august figure shows signi growth number separate mapreduce programs checked primary source code management system time early separate instances late september mapreduce successful makes write simple program run ciently thousand machines half hour greatly speeding development prototyping cycle programmers experience distributed parallel systems exploit large amounts resources easily end job mapreduce library logs statistics computational resources job table show statistics subset mapreduce jobs run google august large-scale indexing signi mapreduce date complete rewrite production indexto osdi ing system produces data structures google web search service indexing system takes input large set documents retrieved crawling system stored set gfs les raw contents documents terabytes data indexing process runs sequence ten mapreduce operations mapreduce ad-hoc distributed passes prior version indexing system provided benets indexing code simpler smaller easier understand code deals fault tolerance distribution parallelization hidden mapreduce library size phase computation dropped approximately lines code approximately lines expressed mapreduce performance mapreduce library good conceptually unrelated computations separate mixing avoid extra passes data makes easy change indexing process change months make indexing system days implement system indexing process easier operate problems caused machine failures slow machines networking hiccups dealt automatically mapreduce library operator intervention easy improve performance indexing process adding machines indexing cluster related work systems provided restricted programming models restrictions parallelize computation automatically associative function computed pre xes element array log time processors parallel pre computations mapreduce considered simpli cation distillation models based experience large real-world computations signi cantly provide fault-tolerant implementation scales thousands processors contrast parallel processing systems implemented smaller scales leave details handling machine failures programmer bulk synchronous programming mpi primitives provide higher-level abstractions make easier programmers write parallel programs key difference systems mapreduce mapreduce exploits restricted programming model parallelize user program automatically provide transparent fault-tolerance locality optimization draws inspiration techniques active disks computation pushed processing elements close local disks reduce amount data subsystems network run commodity processors small number disks directly connected running directly disk controller processors general approach similar backup task mechanism similar eager scheduling mechanism employed charlotte system shortcomings simple eager scheduling task repeated failures entire computation fails complete instances problem mechanism skipping bad records mapreduce implementation relies in-house cluster management system responsible distributing running user tasks large collection shared machines focus paper cluster management system similar spirit systems condor sorting facility part mapreduce library similar operation now-sort source machines map workers partition data sorted send reduce workers reduce worker sorts data locally memory now-sort user-de nable map reduce functions make library widely applicable river programming model processes communicate sending data distributed queues mapreduce river system provide good average case performance presence non-uniformities introduced heterogeneous hardware system perturbations river achieves careful scheduling disk network transfers achieve balanced completion times mapreduce approach restricting programming model mapreduce framework partition problem large number negrained tasks tasks dynamically scheduled workers faster workers process tasks restricted programming model schedule redundant executions tasks end job greatly reduces completion time presence non-uniformities slow stuck workers bad-fs programming model mapreduce unlike mapreduce targeted osdi execution jobs wide-area network fundamental similarities systems redundant execution recover data loss caused failures locality-aware scheduling reduce amount data congested network links tacc system designed simplify construction highly-available networked services mapreduce relies re-execution mechanism implementing fault-tolerance conclusions mapreduce programming model successfully google purposes attribute success reasons model easy programmers experience parallel distributed systems hides details parallelization fault-tolerance locality optimization load balancing large variety problems easily expressible mapreduce computations mapreduce generation data google production web search service sorting data mining machine learning systems developed implementation mapreduce scales large clusters machines comprising thousands machines implementation makes cient machine resources suitable large computational problems encountered google learned things work restricting programming model makes easy parallelize distribute computations make computations fault-tolerant network bandwidth scarce resource number optimizations system targeted reducing amount data network locality optimization read data local disks writing single copy intermediate data local disk saves network bandwidth redundant execution reduce impact slow machines handle machine failures data loss acknowledgements josh levenberg instrumental revising extending user-level mapreduce api number features based experience mapreduce people suggestions enhancements mapreduce reads input writes output google file system mohit aron howard gobioff markus gutschke david kramer shun-tak leung josh redstone work developing gfs percy liang olcan sercinoglu work developing cluster management system mapreduce mike burrows wilson hsieh josh levenberg sharon perl rob pike debby wallach 
provided helpful comments earlier drafts paper anonymous osdi reviewers shepherd eric brewer provided suggestions areas paper improved finally users mapreduce google engineering organization providing helpful feedback suggestions bug reports andrea arpaci-dusseau remzi arpaci-dusseau david culler joseph hellerstein david patterson high-performance sorting networks workstations proceedings acm sigmod international conference management data tucson arizona remzi arpaci-dusseau eric anderson noah treuhaft david culler joseph hellerstein david patterson kathy yelick cluster river making fast case common proceedings sixth workshop input output parallel distributed systems iopads pages atlanta georgia arash baratloo mehmet karaul zvi kedem peter wyckoff charlotte metacomputing web proceedings international conference parallel distributed computing systems luiz barroso jeffrey dean urs hcurrency olzle web search planet google cluster architecture ieee micro april john bent douglas thain andrea arpaci-dusseau remzi arpaci-dusseau miron livny explicit control batch-aware distributed system proceedings usenix symposium networked systems design implementation nsdi march guy blelloch scans primitive parallel operations ieee transactions computers november armando fox steven gribble yatin chawathe eric brewer paul gauthier cluster-based scalable network services proceedings acm symposium operating system principles pages saint-malo france sanjay ghemawat howard gobioff shun-tak leung google system symposium operating systems principles pages lake george york osdi gorlatch systematic cient parallelization scan list homomorphisms bouge fraigniaud mignotte robert editors euro-par parallel processing lecture notes computer science pages springer-verlag jim gray sort benchmark home page http research microsoft barc sortbenchmark william gropp ewing lusk anthony skjellum mpi portable parallel programming message-passing interface mit press cambridge huston sukthankar wickremesinghe satyanarayanan ganger riedel ailamaki diamond storage architecture early discard interactive search proceedings usenix file storage technologies fast conference april richard ladner michael fischer parallel pre computation journal acm michael rabin cient dispersal information security load balancing fault tolerance journal acm erik riedel christos faloutsos garth gibson david nagle active disks large-scale data processing ieee computer pages june douglas thain todd tannenbaum miron livny distributed computing practice condor experience concurrency computation practice experience valiant bridging model parallel computation communications acm jim wyllie spsort sort terabyte quickly http alme almaden ibm spsort pdf word frequency section program counts number occurrences unique word set input les speci command line include mapreduce mapreduce user map function class wordcounter public mapper public virtual void map const mapinput input const string text input const int text size int skip past leading whitespace isspace text find word end int start isspace text start emit text substr start i-start register mapper wordcounter user reduce function class adder public reducer virtual void reduce reduceinput input iterate entries key add values int inputdone stringtoint inputvalue inputnextvalue emit sum inputkey emit inttostring register reducer adder int main int argc char argv parsecommandlineflags argc argv mapreducespecification spec store list input files spec int argc mapreduceinput input spec add input inputset format text inputset filepattern argv inputset mapper class wordcounter output files gfs test freq-of- gfs test freq-of- mapreduceoutput spec output outset filebase gfs test freq outset num tasks outset format text outset reducer class adder optional partial sums map tasks save network bandwidth outset combiner class adder tuning parameters machines memory task spec set machines spec set map megabytes spec set reduce megabytes run mapreduceresult result mapreduce spec result abort result structure info counters time number machines return osdi 
autoraid hierarchical storage system john wilkes richard golding carl staelin tim sullivan hewlett-packard laboratories con uring redundant disk arrays black art configure array properly system administrator understand details array workload support incorrect understanding workload time lead poor performance present solution problem two-level storage hierarchy implemented inside single disk-array controller upper level hierarchy copies active data stored provide full redundancy excellent performance lower level raid parity protection provide excellent storage cost inactive data lower performance technology describe article autoraid automatically transparently manages migration data blocks levels access patterns change result fully redundant storage system extremely easy suitable wide variety workloads largely insensitive dynamic workload performs disk arrays comparable numbers spindles larger amounts front-end ram cache implementation autoraid technology software additional hardware cost benefits small describe autoraid technology detail provide performance data embodiment storage array summarize results simulation studies choose algorithms implemented array categories subject descriptors input output data communication input output devices-channels controllers input output data communications reliability testing fault-tolerance redundant design operating systems storage management secondary storage general terms algorithms design performance reliability additional key words phrases disk array raid storage hierarchy introduction modern businesses increasing number individuals depend information stared computer systems modern disk drives mean-time-to-failure mitf values measured hundreds years storage increased enormous rate sufficiently large collection devices experience inconveniently frequent authors address hewlett-packard laboratories page mill road palo alto wilkes gelding staelin sullivan hpl corn permission make digital hard copy part work personal classroom granted fee provided copies made distnbu profit commercial advantage copyright notice title date notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee acm acm transactions computer systems vol february psges autoraid failures worse completely reloading large storage system backup tapes hours days resulting costly downtime small numbers disks preferred method providing fault protection duplicate mirror data disks independent failure modes solution simple performs total number disks large cost effective employ array controller form partial redundancy parity protect data stores raids redundant arrays independent disks early lawlor park balasubramanian popularized work group berkeley patterson storing partial redundancy data incremental cost desired high availability reduced total storage-capacity cost number disks array cost array controller berkeley raid terminology number raid levels representing amount redundancy placement rule redundant data disk array products implement raid level raid level host data blocks bitor byte-interleaved set data disks parity stored dedicated data disk figure raid level host data blocks block-interleaved disks disk parity block stored rotates round-robin fashion stripes figure hardware software raid products vendors current disk arrays difficult chen lee raid levels performance characteristics perform narrow range workloads accommodate raid systems typically offer great configuration parameters dataand parity-layout choice stripe depth stripe width cache sizes write-back policies setting correctly difficult requires knowledge workload characteristics people unable unwilling acquire result setting raid array daunting task requires skilled expensive people cases painful process trial error making wrong choice costs resulting system perform poorly changing layout inevitably requires copying data device reformatting array reloading step process hours opportunity inadvertent data loss operator error-one commonest sources problems modern computer systems gray adding capacity existing array essentially problem taking full advantage disk requires reformat data reload raid arrays suffer reduced performance degraded mode drives failed include provision spare disks pressed service active disk fails redundancy reconstruction commence immediately reducacm transactions computer systems vol february john wilkes data parity data atity raid raid fig data parity layout raid levels ing window vulnerability data loss device failure minimizing duration performance degradation normal case spare disks contribute performance system secondary problem assuming spare disk working spare idle array controller find failed late solution managed storage hierarchy fortunately solution problems great applications disk arrays redundancy-level storage hierarchy basic idea combine performance advantages mirroring cost-capacity benefits raid mirroring active data storing inactive read-only data raid make solution work part data active part inactive cost performance reduce mirrored data active subset change slowly time array work move data levels fortunately studies access patterns disk shuffling file system restructuring shown conditions met practice akyurek salem deshpandee bunt floyd schlattir ellis geist majumdar mcdonald bunt mcnutt ruemmler wilkes smith storage hierarchy implemented number ways manually system administrator large mainframes run decades gelb discusses slightly refined version basic idea advantage approach human intelligence brought bear problem knowledge lower levels operating systems error prone wrong choices made mistakes made moving data level adapt rapidly changing access patterns requires highly skilled people resources disk drives added system easily file system per-file basis place terms good balance knowledge file system track access patterns per-file basis implementation freedom acm lyansaetions computer systems vol february autoraid file system implementations customers hands deployment major problem smart array controller block-level device interface small systems computer interface scsi standard scsi level disadvantage knowledge files lost enormous compensating advantage easily deployable strict adherence standard means array approach regular disk array set plain disk drives surprisingly describing array-controller-based solution auto refer collection technology developed make embodiment array controller summary features autoraid summarize features autoraid mapping host block addresses internally mapped physical locations transparent migration individual blocks mirroring write-active data mirrored performance provide single-disk failure redundancy raid write-inactive data stored raid cost capacity retaining good read performance single-disk failure redundancy addition large sequential writes directly raid advantage high bandwidth access pattern adaptation amount data stored initially array starts empty data added internal space allocated mirrored storage data stored storage space automatically reallocated raid storage class data migrated mirrored storage class raid layout compact data representation data stored array reapportionment allowed proceed capacity mirrored storage shrunk total usable space exact number policy choice made implementors autoraid firmware maintain good performance space apportioned coarse-granularity lmb units adaptation workload active set data newly active data promoted mirrored storage data active demoted raid order amount mirrored data roughly constant data movements background affect performance array promotions demotions occur completely automatically fine-granularity units hot-pluggable disks fans power supplies controllers failed component removed inserted system continues operate commonplace features acm transactions computer systems vol february john wilkes higher-end disk arrays important enabling features on-line storage capacity expansion disk added array time maximum allowed physical 
packaging-currently disks system automatically takes advantage additional space allocating mirrored storage time workload permit active data rebalanced drives workload newcomer previous disks maximum performance system easy disk upgrades unlike conventional arrays disks capacity advantages drive purchased optimal capacity cost performance point regard prior selections entire array upgraded disk type capacity interrupting operation removing disk time inserting replacement disk waiting automatic data reconstruction rebalancing complete eliminate reconstruction data drained disk replaced advantage retaining continuous protection disk failures process require spare capacity system controller fail-over single array controllers capable running entire subsystem failure primary operations rolled failed controller replaced system active concurrently active controllers supported active hot spare spare space needed perform reconstruction spread disks increase amount space mirrored data array performance-rather simply left idle disk fails mirrored data demoted raid provide space reconstruct desired redundancy process complete disk failure tolerated-and physical capacity filled data raid storage class simple administration setup system administrator divide storage space array logical units luns scsi terminology correspond logical groupings data stored creating lun changing size existing lun trivial takes seconds front-panel menus select size confirm request array formatted traditional sense creation lun require pass newly allocated space initialize parity operation hours regular array needed controller data structures updated log-structured raid writes well-known problem raid disk arrays so-called small-write problem update-in-place part stripe takes data parity read parity calculated data parity written back autoraid acm transactions computer systems vol february autoraid avoids overhead cases writing raid storage log-structured fashion empty areas disk written old-data old-parity reads required related work papers published raid reliability performance design variations parity placement recovery schemes chen annotated bibliography autoraid work builds studies concentrate architectural issues multiple raid levels specifically single array controller storage technology corporation iceberg ewing stk similar indirection scheme map logical ibm mainframe disks count-keydata format array -inch scsi disk drives art rudeseal private communication nov iceberg handle variable-sized records autoraid scsi interface handle indirection fixed-size blocks emphasis iceberg project achieving extraordinarily high levels availability emphasis autoraid performance single-component failure model regular raid arrays achieved iceberg include multiple raid storage levels simply single-level modified raid storage class dunphy ewing team ibm almaden extensive work improving raid array controller performance reliability ideas application ibm mainframe storage controllers floating-parity scheme menon kasson indirection table parity data written nearby slot necessarily original location reduce small-write penalty raid arrays distributed sparing concept menon mattson spreads spare space disks array allowing spindles hold data autor aid data parity relocated distributed spare capacity increase fraction data held mirrored form improving performance schemes menon courtney dual-controller version autoraid array handle controller failures loge disk drive controller english stepanov followons mime chao logical disk jonge scheme keeping indirection table fixed-sized blocks held secondary storage supported multiple storage levels targeted raid arrays work extended function controller disk divisions looked issues progress awaited development suitable controller technologies make approach adopted autoraid cost effective log-structured writing scheme autoraid owes intellectual debt body work log-structured file systems lfs carson setia ousterhout douglis rosenblum ousterhout acmtransactions computer systems vol february john wilkes seltzer cleaning garbage collection policies blackwell mcnutt mogi kiteuregawa large body literature hierarchical storage systems commercial products domain chen cohen dec deshpandee bunt epoch systems gelb henderson poston katz miller misra sienknecht smith proceedings ieee symposia mass storage systems work concerned wider performance disparities levels exist autoraid systems disk robotic tertiary storage tape magneto-optical disk levels hierarchical storage systems front-end dieks act cache data tertiary storage autoraid mirrored storage cache data moved storage classes residing precisely class time method maximizes storage capacity number disks highlight system kohl extended lfs two-level storage hierarchies disk tape fixed-size segments highlight segments lmb size suited tertiary-storage mappings secondary-etorage levels schemes inactive data compressed burrows cate taunton exhibit similarities storage-hierarchy component autoraid operate file system level block-based device interface finally modern array controllers autoraid takes advantage kind optimization noted baker ruemmler wilkes nonvolatile memory roadmap remainder article remainder article organized begin overview technology autoraid array controller works sets performance studies set measurements product prototype set simulation studies evaluate algorithm choices autoraid finally conclude article summary benefits technology technology section introduces basic technologies autoraid etarts overview hardware discusses layout data disks array including structures ueed mapping data blocks locations disk overview normal read write operations illustrate flow data system descriptions series operations performed background eneure performance system remaine high long periods time acm transactions computer systems vol february autoraid sscsl scsln host processor fig overview autoraid hardware autoraid array controller hardware autor array fundamentally similar regular raid array set disks intelligent controller incorporates microprocessor mechanisms calculating parity caches staging data nonvolatile connection host computers speed-matching buffers figure overview hardware hardware prototype provide performance data back-end scsi buses connect disks fast-wide scsi buses front-end host connection alternatives exist packaging technology scope article array presents scsi logical units luns hosts treated virtual device inside array controller storage freely intermingled lun size increased time subject capacity constraints block lun valid data stored address array controller allocate physical space data layout intelligence autoraid controller devoted managing data placement disks two-level allocation scheme physical data layout pegs pexes segments data space disks broken large-granularity objects called physical extents pexes shown figure pexes typically lmb size acm transactions computer systems vol february john wilkes disk waddreties egs --------dk ka--------fig mapping pegs pexes disks adapted burkes table summary autqraid data layout terminology term meaning size pex physical extent unit sicaiapaceallocation lmb peg physical extent group group pexss assigned storage class stripe row parity dats segments raid storage class segment stripe unit raid half mirroring unit relocation block unit data migration lun logical unit host-visible virtual disk user settable depends number disks pexes combined make physical extent group peg order provide redundancy make usable mirrored raid storage class peg includes pexes disks time peg assigned mirrored storage class raid storage class unassigned speak mirrored raid free pegs terminology summarized table pexes allocated pegs manner balances amount data disks hopefhlly load disks retaining redundancy guarantees pexes disk stripe beeause diska autoraid array acm transactions computsr syatema vol february autoraid disko disk diak diak diek mirrored peg 
mirroradz pair segme strip raid peg fig layout pegs mirrored raid peg spread disks raid peg segments disks assemble strip-es mirrored peg segments disks form mirrored pairs sizes allocation process leave uneven amounts free space disks segments units contiguous space disk included stripe mirrored pair pex divided set segments figure shows mirrored raid pegs divided segments segments logically grouped storage classes ways raid segment stripe unit mirrored storage class segment unit duplication logical data layout rbs logical space provided array visible clients divided small units called relocation blocks rbs basic units migration system lun created increased size address space mapped set rbs assigned space peg host issues write lun address maps size compromise data layout data migration data access costs smaller rbs require mapping information record put increase time spent disk seek rotational delays larger rbs increase migration costs small amounts data updated report exploration relationship size performance section acm transactions compuix systems vol february john wilkes vktld vico tabtes tie owlun rss tinters tothepme wmchthey reside peg mbles peg holdsfistof rss pegand listof xesused store pex mbles physicaldiskdrive fig structure tables map addresses virtual volumes pegs pexes physical disk addresses simplified peg hold rbs exact number fimction peg size storage class unused slots peg marked free data allocated mapping structures subset mapping structures shown figure data structures optimized physical disk address logical lun-relative address common operation addition data held access times history amount free space peg cleaning garbage collection purposes statistics shown back pointers additional scans normal operations start host-initiated read write operation host sends scsi command descriptor block cdb autoraid array parsed controller cdbs active time additional cdbs held fifo queue waiting serviced limit requesta queued host long requests broken pieces handled sequentially method limits amount controller resources single consume minimal performance cost request read data completely controller cache memories data transferred host speed-matching btier command completes statistics acm transactionson computer systems vol february autoraid updated space allocated front-end buffer cache read requests dispatched back-end storage classes writes handled slightly differently nonvolatile front-end write buffer nvram host request complete copy data made memory check made cached data invalidating space allocated nvram allocation wait space trigger flush existing dirty data back-end storage class data transferred nvram host host told request complete depending nvram cache-flushing policy back-end write initiated point hope subsequent write coalesced increase efllciency flushing data back-end storage class simply back-end write data mirrored storage class flush trigger promotion raid mirrored exceptions describe promotion calling migration code allocates space mirrored storage class copies raid space mirrored storage class background daemons chance run turn provoke demotion mirrored data raid tricky details involved ensuring turn fail free-space management policies anticipate worst-case sequence events arise practice mirrored reads writes reads writes mirrored storage class straightforward read call picks copies issues request disk write call writes disks returns copies updated note back-end write call issued flush data nvram synchronous host write raid reads writes back-end reads raid storage class simple mirrored storage class normal case read issued disk holds data recovery case data reconstructed blocks stripe usual raid recovery algorithms case discuss failure case article implemented current system techniques parity declustering holland gibson improve recovery-mode performance back-end raid writes complicated raid storage laid log freshly demoted rbs appended end current raid write peg overwriting virgin storage writes ways per-rb writes batched writes simpler efficient per-rb writes ready written flushed disk copy contents flow past parityacm transactions computer systems vol february john wilkes calculation logic xors previous contents-the parity stripe data written parity written prior contents parity block stored nonvolatile memory process protect power failure scheme data-rb write disk writes data parity scheme advantage simplicity cost slightly worse performance batched writes parity written data rbs stripe written end batch beginning batched write valid data peg written prior contents parity block copied nonvolatile memory index highest-numbered peg valid data panty calculated xoring rbs indices equal rbs written data portion stripe end stripe reached batch completes point parity written parity computed on-the-fly parity-calculation logic data written batched write fails complete reason system returned prebatch state restoring parity index write retried per-rb method batched writes require bit coordination per-rb writes require additional parity write full stripe data written raid writes batched writes addition logging write methods method typically nonlogging raid implementations read-modify-write caees method reads data parity modifies rewrites disk forward progress rare cases peg logging write processes update data holes section place raid migrate mirrored storage background migrations array idle background operations addition foreground activities autoraid array controller executes background activities garbage collection layout balancing background algorithms attempt provide slack resources needed foreground operations foreground trigger synchronous version background tasks dramatically reduce performance background operations triggered array idle period time idleness defined algorithm current past device activity-the array completely devoid activity idle period detected array performs set background operations subsequent idle period continuation current triggers set operations acmtransactions computer systems vol february autoraid long period array activity current algorithm moderate amount time detect array idle hope apply results gelding improve idle-period detection prediction accuracy turn aggressive executing background algorithms compaction cleaning hole-plugging mirrored storage class acquires holes empty slots rbs demoted raid storage class updates mirrored rbs written place generate holes holes added free list mirrored storage class subsequently promoted newly created rbs peg needed raid storage class free pexes mirrored peg chosen cleaning data migrated fill holes mirrored pegs peg reclaimed reallocated raid storage class similarly raid storage class acquires holes rbs promoted mirrored storage class rbs updated normal raid write process logging holes reused directly call garbage array perform periodic garbage collection eliminate raid peg holes full array performs hole-plugging garbage collection rbs copied peg small number rbs fill holes full peg minimizes data movement spread fullness pegs case peg holes empty holes plugged array peg cleaning appends remaining valid rbs current end raid write log reclaims complete peg unit migration moving rbs levels background migration policy run move rbs mirrored storage raid primarily provide empty slots mirrored storage class handle future write burst ruemmler wilkes showed bursts common rbs selected migration approximate 
recently written algorithm migrations performed background number free slots mirrored storage class free pegs exceeds high-water mark chosen system handle burst incoming data threshold set provide burst-handling cost slightly lower out-of-burst performance current autoraid firmware fixed determined dynamically balancing adjusting data layout drives drives added array data contribute system performance balancing process migrating pexes disks equalize amount data stored disk request load imposed disk access histories acm transactions computer systems vol february john wilkes balance disk load precisely balancing background activity performed system type imbalance results drive added array newly created raid pegs drives system provide maximum performance previously created raid pegs continue original disks imbalance corrected low-priority background process copies valid data pegs full-width pegs workload logging uncertainties faced developing autoraid design lack broad range real system workloads disk level measured accurately evaluating performance remedy future aut aid array incorporates workload logging tool system presented specially formatted disk tool records start stop times externally issued request events recorded desired overhead small event logs buffered controller ram written large blocks result faithfid record unit asked drive simulation design studies kind describe article management tool aut dlaid controller maintains set internal statistics cache utilization times disk utilizations statistics cheap acquire store provide significant insight operation system product team developed off-line inference-based management tool statistics suggest configuration choices tool determine period high load performance improved adding cache memory array controller short read cache information administrators maximize array performance environment autoraid performance results combination prototyping event-driven simulation development autoraid technology autoraid embedded algorithms policies manage storage hierarchy result hardware firmware prototypes developed concurrently event-driven simulations studied design choices algorithms policies parameters algorithms primary development team based product division designed built tested prototype hardware firmware supported group laboratories built detailed simulator acm lhmactions computer systems vol february autoraid hardware firmware model alternative algorithm policy choices depth organization allowed teams incorporate technology products time fully investigating alternative design choices section present measured results laboratory prototype disk array product embodies autoraid technology section present set comparative performance analyses algorithm policy choices guide implementation real thing experimental setup baseline autoraid configuration report -disk system controller controller data cache connected fast-wide differential scsi adapters system processor main memory running release hp- operating system clegg drives ogb rpm seagate barracudas write reporting turned calibrate autoraid results external systems measurements disk subsystems measurements host hardware days host configurations number disks type disks data general clariion series disk-array storage system deskside model front-end cache refer system raid array array chosen recommended third-party raid array solution primary customers autoraid product clariion supports connection host fast-wide differential scsi channels single channel bottleneck system array configured raid results raid raid set directly connected individual disk drives solution data redundancy hp-ux logical volume manager lvm stripe data disks chunks unlike autoraid raid array disks central controller controller-level cache refer configuration jbod-lvm bunch disks performance results begin presenting database macrobenchmarks order demonstrate autoraid excellent performance real-world workloads workloads exhibit behaviors burstiness present simple rate tests relying provide misleading impression system behave real acm transactions computer systsms vol february john wilkes macrobenchmarks oltp database workload made medium-weight transactions run autoraid array regular raid array jbod-lvm database test allowed fit mirrored storage autoraid working-set sizes larger mirrored space discussed benchmark raid array disks spread evenly scsi channels cache enabled cache page size set optimal workload default stripe-unit size figure shows result autoraid significantly outperforms raid array performance threefourths jbod-lvm results suggest autoraid performing expected keeping data mirrored storage means writes faster raid array fast jbod-lvm reads handled equally cases figure shows autoraid performance data migrated mirrored storage raid working set large contained mirrored storage class type oltp database workload database size set lgb fit -drive autoraid system started -drive system baseline mirrored storage accommodate one-third database case two-thirds -drive system -drive system larger systems differences performance -drive systems due primarily differences number migrations performed differences larger systems result spindles spread amount mirrored data -drive configuration limited host cpu speed performed n-drive system data database workload fairly random access pattern large data set autoraid performs factor optimum one-third data held mirrored storage threefourths optimum two-thirds data mirrored microbenchmarks addition database macrobenchmark ran microbenchmarks synthetic workload generation program drive arrays saturation working-set size random tests measurements slightly conditions reported section autoraid contained controller data cache host tests single fast-wide differential scsi channel autoraid raid array tests jbod case lvm striping nature workload immaterial addition jbod disks match amount space data configurations finally jbod test acm transactions computer systems vol february autoraid raid array autofiaid jbod-lvm fig oltp macrobenchmark results comparison autoraid non-raid drives regular raid array system drives entire database tit mirrored storage autqraid performance autoraid numbers drives fraction lgb database held mirrored storage -drive system -drive system -drive system larger systems numberof drives fast-wide single-ended scsi card required host cpu cycles affect microbenchmarks cpu limited raid array cache pages cache noted data microbenchmarks provided figure shows relative performance arrays jbod random sequential reads writes random read-throughput testis primarily measure controller overheads autoraid performance roughly midway raid array cache disabled jbod cachesearching algorithm raid array significantly limiting performance cache hit rate close tests random write-throughput test primarily test low-level storage system systems driven disk-limited acmtransactionson computersystems vol february john wilkes l-l autoraid raid raid jbod cache random reads autoraid raid fiaid jbod nocache sequential reads aijioraid raid raid jbod nocache random writes autoraid raid raid jbod nocache sequential writes fig microbenchmark companions autoraid aregular raid array non-raid drives behavior benchmark expected ratio raid small update autoraid mirrored storage jbod write place sequential read-bandwidth test shows mirrored storage autoraid largely compensate controller overhead deliver performance comparable jbod finally sequential write-bandwidth test illustrates autor aid ability stream data disk nvram cache performance pure jbod solution good explanation poor performance raid array cases results shown obtained acmtransactions computer systems vol february autoraid number array configurations results demonstrated difficulties involved properly conf ring raid array parameters adjusted caching cache granularity stripe depth data layout single combination performed range workloads examined thrashing noted section performance autoraid depends working-set size applied workload working set size mirrored 
space performance good shown figure figure figure shows good performance obtained entire working set fit mirrored storage active write working set exceeds size mirrored storage long periods time drive autoraid array thrashing mode update target promoted mirrored storage class demoted raid autoraid array configured avoid adding disks write-active data mirrored storage ail data write active cost-performance advantages technology reduced fortunately fairly easy predict detect environments large write working set avoid thrashing occur autoraid detects reverts mode writes directly raid automatically adjusts behavior performance worse raid simulation studies section illustrate design choices made inside autoraid implementation trace-driven simulation study simulator built pantheon cao gelding simulation framework detailed trace-driven simulation environment written individual simulations configured set simulation objects scripts written tcl language ousterhout configuration techniques gelding disk models simulation improved versions detailed calibrated models ruemmler wilkes traces drive simulations variety systems including cello time-sharing series hp-ux system snake series hp-ux cluster file server oltp series hp-ux system running database benchmark made medium-weight transactions system section hplajw personal workstation netware server subsets traces usr disk cello subset database disks oltp oltp log disk long time periods months lthe simulator called tickertaip changed avoid confusion parallel raid array project cao acm transactions computer systems vol february john wilkes simulation runs two-day subsets traces netware trace contained detailed timing information resolution considerable detail ruemmler wilkes modeled hardware autoraid pantheon components caches buses disks wrote detailed models basic firmware alternative algorithms policies design experiments pantheon simulation core comprises lines lines tel hp-autoraid-specific portions simulator added lines lines tel complexity model number parameters algorithms policies examining impossible explore combinations experimental variables reasonable amount time chose organize experiments baseline runs runs related baseline allowed observe performance effects individual closely related changee perform wide range experiments quickly cluster high-performance workstations run simulations executing experiments week elapsed time performed additional experiments combine individual suspected strongly interact positively negatively test aggregate effect set algorithms proposing product development team hardware implementation aut aid early simulation study initially unable calibrate simulator disk models high level detail simulation confident relative performance differences predicted simulator valid absolute performance numbers calibrated relative performance differences observed simulation experiments suggest improvements team implementing product firmware present turn updated baseline model correspond made implementation individual results report chosen describe highlight behaviors autor aid system disk speed experiments measured sensitivity design size performance components wanted understand faster disks cost effective baseline disks held spun rpm evaluated variations disk spinning rpm rpm keeping data density bite inch transfer rate bits constant expected increasing back-end disk performance generally improves performance shown figure results suggest improving transfer rate important improving rotational latency acm transactions computer systems vol february autoraid snake const density const bit rate const density const bit rate oltpdb const density corrstbit rate crest density mnst bit rate onp-log const density const bit rate const density const bit rate cello-usr const density const bit rate const density const bit rate percent improvement versus rpm disks disk spin speed snake -db oltp-log cello-usr percent fmprovemenf versus size fig effects disk spin speed size performance acm transactions computer systems vol february john wilkes size standard autoraid system rbs basic storage unit looked effect smaller larger sizes workloads figure size balance seek rotational overheads versus data movement costs surprising disks track sizes transfer sizes range tend benefit fewer mechanical delays data layout system blocks remapped blocks host system lay sequentially physically discontinuous bad problem compared performance system host lun address spaces initially laid completely linearly disk case completely randomly worst case figure shows difference layouts modest improvement performance linear case compared random suggests size large limit impact seek delays sequential accesses mirrored storage class read selection algorithm front-end read cache misses stored mirrored storage class array choose read stored copies baseline system selects copy random attempt avoid making disk bottleneck possibilities strictly alternating disks alternate attempting heads disks outer edge keeping inside outer disk shortest queue shortest queue disk reach block determined shortestpositioning-time algorithm jacobson wilkes seltzer shortest seek policies stacked aggressive policy falling back break tie experiments random final fallback policy figure shows results investigations possibilities shortest queue simple load-balancing heuristic performance improved average random workloads shortest seek performed random average complex implement requires detailed knowledge disk head position seek timing static algorithms alternate innertouter perform random interact unfavorably patterns workload decrease system performance acm kanaactions computer systems vol february autoraid snake oltp-db oltp-log cello-usr percent improvement sequerttjal ayout snake altarnate innerlouter shortest queue shortast seek shortest queue alternate innarloutar shortastqueue shorlest seek shorleat seek queue lemaite innerlwter shcftast queue shortest seek shortestw rqueue alternate outer shortest quaue shortest seek shortest seek queue versus random layout data layout percent improvement vecsusrandom read disk selection policy fig effects data layout mirrored storage clasa read diek selection policy performance acm transactions computer systems vol february john wilkes snake fig effect ofallowing writ ecache overwrites performance note passing differences show microbenchmarks type reported figure disks typically driven saturation effects show write cache overwrites investigated policy choices managing nvram write cache baseline system instance write operation overwrite dirty data cache write block previous dirty data cache flushed disk figure shows allowing overwrites noticeable impact workloads huge impact oltp-log workload improving performance factor omitted workload graph scaling reasons hole-plugging demotion rbs typically written raid reasons demotion mirrored storage garbage collection normal operation system creates holes raid promoting rbs mirrored storage class order space consumption constant system demotes rbs raid default configuration autdtaid logging writes demote rbs raid quickly demotion idle time demotions fill holes left promoted rbs reduce work raid cleaner allowed rbs demoted idle periods written raid hole-plugging optimization reduced number rbs moved raid cleaner acmtransactionson computer systems vol february autoraid cello-usr workload snake improved time user summary autoraid technology works extremely providing performance close nonredundant disk array workloads time full data redundancy tolerate failures single array component easy authors article delivered system manuals day demonstration running trial benchmark minutes afl connected completely unmodified workstation product team experiences demonstrating system potential customers autoraid technology panacea storage problems workloads suit algorithms environments variability response time unacceptable nonetheless adapt 
great environments encountered real life outstanding general-purpose storage solution availability matters product based technology xlr advanced disk array acknowledgments colleagues hps storage systems division developed autoraid system architecture product version controller customers performance algorithm studies people put enormous amounts effort making pro success possibly acknowledge directly chris ruemmler wrote benchmark results section article dedicated memory late colleague kondoff helped establish collaboration produced body work akyurek salem adaptive block rearrangement tech rep cs-trdept computer science univ maryland college park bakkr asami deprit ousterhout andseltzer non-volatile memory fast reliable file systems proceedings international conference architectural support programming languages operating systems cornput arch news oct blackwell harris andseltzer heuristic cleaning algorithms log-structured tile systems proceedings usenix technical conference unix aduanced computing systems usenix assoc berkeley calif burkes dlamond andvoigt adaptive hierarchical raid solution raid write problem part hewlett-packard storage systems division boise idaho acm transactions computer systems vol february john wilkes burrows jerjan lampson iklwni on-line data compression log-structured file syetem proceedings zntemationnl conference architectuml support progmmming languages opemting systems comput arch news oct cao lim veiwwtaraman andwilkes tickertaip parallel raid architecture acm tmns comput syst aug carson seiy optimal write batch size log-structured file systems usenzx workshop file systems usendc assoc berkeley calif cate levels file system hierarchy disk tech rep cmu-cs- dept computer science carnegie-mellon univ pittsburgh chao english jacobson stepanov wilkes mime high performance storage device strong recovery guarantees tech rep hpl- hewlettpackard laboratories palo alto calif chen optimal file allocation multi-level storage hierarchies proceedings national computer conference exposition afips conference promedings vol afips press montvale chen andlee striping raid leveldisk array tech rep cse-tr- univ michigan ann arbor mich chen lee gibson katz pattrrson raid high-performance reliable secondary storage acm comput surv june clegg kusmer andsontag hp-ux operating system precision architecture computers hewlett-packard dec cohen king andbrady storage hierarchies ibm syst dec polycenter storage management openvms vax systems digital equipment corp maynard mass jonge kaashoek andhsieh logical disk approach improving file systems proceedings tlw acm symposium opemting systems principles acm york deshpande andbunt dynamic file management techniques proceedings ieee phoenix conference computers communication ieee york dunphy wmsh bowers andrudeseal disk drive memory patent patent office washington english andstepanov loge self-organizing storage device proceedingsof lysl lnfxwinter technical conference usenik assoc berkeley calif epochsystems mass storage server puts optical discs line workstations electronics nov ewing raid overview part storage technology corp louisville colo http stortek storagetek raid htrnl floyd andschlatterellis directory patterns hierarchical file systems ieee trms knqwl data eng june geist reynolds andsuggs minimizing seek distance mirrored disk systems cylinder remapping perf eual gels system managed storage ibm syst gcmnng staelin sullivan andwilkes tel cures simulation configuration problems claims astonished researcher proceedings tcl workshop tech rep hpl-ccd- concurrent computing dept hewlettpackard laboratories palo alto calif golding bosch staelin sullivan andwilkss idleness sloth proceedings usenix technical conference unix advanced computing systems usenix assoc berkeley calif gray census tandem system availability tech rep tandem computers cupertino calif acm transactions computer systems vol february autoraid hen erson powon mssand rash mainframe unix based mass storage system rapid access storage hierarchy tile management system proceedings usenix winter conference usenix assoc berkeley calif holmnd andgibson parity declustering continuous operation redundant disk arrays proceedings international conference architectural support programming atgua ges operating systems comput arch news oct jacobson andwilkes disk scheduling algorithms based rotational position tech rep hpl-csp- hewiett-packard laboratories palo alto calif katz andeiwdn ousterhout patterson robo-line storage low-latency high capacity storage systems geographically distributed networks csd computer science div dept electrical engineering computer science univ california berkeley berkeley calif kohl staelin anrrstonebraicsr highlight log-structured file system tertiary storage management proceedings winter usenly usenfx assoc berkeley calif lawlor efficient mass storage panty recovery mechanism ibm tech discl bull july majumdar locality file referencing behaviour principles applications thesis tech rep dept computer science univ saskatchewan saskatmn saskatchewan canada mcdonald bunt improving tile system performance dynamically restructuring disk space proceedings phoenix conference computers communica tion ieee york mcnu background data movement log-structured disk subsystem ibm res devel menon courtney architecture fault-tolerant cached raid controller proceedings international symposium computer architecture acm york menon kassori methods improved update performance disk arrays tech rep ibm almaden research center san jose calif declassified nov menon andkasson methods improved update performance disk arrays proceedings international conference system sciences vol ieee york menon mattson comparison sparing alternatives disk arrays proceedings international symposium computer architecture acm york mill file migration cray y-mp national center atmospheric research ucb csd computer science div dept electrical engineering computer science univ california berkeley berkeley calif mwra capacity analysis mass storage system ibm syst kitsuregawa dynamic parity stripe reorganizations raid disk arrays proceedings parallel distributed information systems international conference ieee york ousterhout doum beating bottleneck case log-structured tile systems oper syst reu jan ousterhol tcl toolkit addison-wesley reading mass park balasumi anmnmn providing fault tolerance parallel seconda storage systems tech rep cs-tr- dept computer science princeton univ princeton pati rson chen gibson katz introduction redundant arrays inexpensive disks raid spring compcon ieee york pattkrson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod international conference managemen data acm york acm transactions computer systems vol february john wilkes rosenblum andousterhout design implementation log-stmctured file system acm trans comput syst feb ruemmler andwilkes disk shuffling tech rep hpl- hewlett-packard laboratories palo alto calif ruemmler andwilkes unix disk access patterns proceedings winter usenix conference usenix assoc berkeley calif ruemmler andwilkes introduction disk drive modeling ieee comput mar scsi draft proposed american national standard information systems-small computer system interfacescsi- lhft ansi standard revision secretariat computer business equipment manufacturers association seltzer bostic mckusick staelin implementation iog-stnctured tile system unix proceedings winter usenix conference usentx assoc berkeley calif seltzer chen andousterhout disk scheduling revisited proceedings winter usenix conference usenix asscw berkeley calif seltzer smith balakrishnan chang mcmains andpadmanarhan file system logging versus clustering performance comparison conference proceedings usenix technical conference unix advanced computing systems usenix assoc berkeley calif sienknecht friedrich martinka friedenrach implications distributed data commercial environment design hierarchical storage management per-f eual smith optimization systems cache disks file migration summary perf eval 
stk iceberg disk array subsystem storage technology corp misville colo http stortek storagetek iceberg html taunton compressed executable exercise thinking small proceedings summer usenzx usenix assoc berkeley calif received september revised october accepted october acm transactions computer systems vol february 
frangipani scalable distributed file system chandramohan thekkath timothy mann edward lee systems research center digital equipment corporation lytton ave palo alto abstract ideal distributed file system provide users coherent shared access set files arbitrarily scalable provide storage space higher performance growing user community highly spite component failures require minimal human administration administration complex components added frangipani file system approximates ideal easy build two-layer structure lower layer petal earlier paper distributed storage service incrementally scalable highly automatically managed virtual disks upper layer multiple machines run frangipani file system code top shared petal virtual disk distributed lock service ensure coherence frangipaniis meant run cluster machines common administration communicate securely machines trust shared virtual disk approach practical frangipani file system exported untrusted machines ordinary network file access protocols implemented frangipani collection alphas running digital unix initial measurements frangipani excellent single-server performance scales servers added introduction file system administration large growing computer installation built today technology laborious task hold files serve users add disks attached machines components requires human administration groups files manually assigned disks manually moved replicated components fill fail performance hot spots joining multiple disk drives unit raid technology partial solution administration problems arise system grows large require multiple raids multiple server machines permission make digital hard copy part work personal classroom granted fee provided copies made distributed profit commercial advantage copyright notice title date notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee frangipani scalable distributed file system manages collection disks multiple machines single shared pool storage machines assumed common administration communicate securely earlier attempts building distributed file systems scale throughput capacity distinguishing feature frangipani simple internal structure set cooperating machines common store synchronize access store locks simple structure enables handle system recovery reconfiguration load balancing machinery key aspect frangipani combines set features makes easier administer frangipani existing file systems users consistent view set files servers easily added existing frangipani installation increase storage capacity throughput changing configuration existing servers interrupting operation servers viewed bricks stacked incrementally build large file system needed system administrator add users concern machines manage data disks store systemadministrator make full consistentbackup entire file system bringing backups optionally online allowing users quick access accidentally deleted files file system tolerates recovers machine network disk failures operator intervention frangipani layered top petal easy-to-administer distributed storage system virtual disks clients physical disk petal virtual disk storage read written blocks unlike physical disk virtual disk sparse byte address space physical storage allocated demand petal optionally replicates data high availability petal efficient snapshots support consistent backup frangipani inherits scalability fault tolerance easy administration underlying storage system careful design required extend properties file system level section describes structure frangipani relationship petal greater detail user program user program user program frangipani file server frangipani file server distributed lock service petal distributed virtual disk service physical disks figure frangipani layering interchangeable frangipani servers provide access set files petal virtual disk figure illustrates layering frangipani system multiple interchangeable frangipani servers provide access files running top shared petal virtual disk coordinating actions locks ensure coherence file system layer scaled adding frangipani servers achieves fault tolerance recovering automatically server failures continuing operate servers survive improved load balancing centralized network file server splitting file system load shifting machines files petal lock service distributed scalability fault tolerance load balancing frangipani servers trust petal servers lock service frangipani designed run cluster workstations single administrative domain frangipani file system exported domains frangipani viewed cluster file system implemented frangipani digital unix due frangipani clean layering atop existing petal service implement working system months frangipani targeted environments program development engineering workloads tests workloads frangipani excellent performance scales limits imposed network system structure figure depicts typical assignment functions machines machines shown top run user programs frangipani file server module diskless shown bottom run petal distributed lock service components frangipani assigned machines shown figure petal frangipani servers separate machines make sense petal machine run frangipani installation petal machines heavily loaded distributed lock service independent rest system show lock server running petal server machine run frangipani hosts machines components shown figure user programs access frangipani standard operating system call interface programs running machines files views coherent changesmade file directory machine petal server lock server petal server lock server petal server lock server petal virtual disk network user programs file system switch frangipani file server module petal device driver user programs file system switch frangipani file server module petal device driver figure frangipani structure typical frangipani configuration machines run user programs frangipani file server module run petal distributed lock service configurations machines play roles immediately visible programs essentially semantic guarantees local unix file system file contents staged local kernel buffer pool guaranteed reach nonvolatile storage applicable fsync sync system call metadata logged optionally guaranteed non-volatile time system call returns small departure local file system semantics frangipani maintains file last-accessed time approximately avoid metadata write data read frangipani file server module machine runs operating system kernel registers kernel file system switch file system implementations file server module kernel buffer pool cache data recently files reads writes petal virtual disks local petal device driver file servers read write file system data structures shared petal disk server redo log pending distinct section petal disk logs petal frangipani server crashes server access log run recovery frangipani servers communicate directly communicate petal lock service server addition deletion recovery simple petal device driver hides distributed nature petal making petal ordinary local disk higher layers operating system driver responsible contacting define metadata on-disk data structure contents ordinary file correct petal server failing digital unix file system run top petal frangipani coherent accessto files multiple machines petal servers run cooperatively provide frangipani large scalable fault-tolerant virtual disks implemented top ordinary physical disks connected server petal tolerate disk server failures long majority petal servers remain communication copy data block remains physically accessible additional details petal separate paper lock service general-purpose service multiple-reader single-writer locks clients network implementation distributed fault tolerance scalable performance frangipani lock service coordinate access virtual disk buffer caches coherent multiple servers security client server configuration configuration shown figure machine 
hosts user programs hosts frangipani file server module configuration potential good load balancing scaling poses security concerns frangipani machine read write block shared petal virtual disk frangipani run machines trusted operating systems sufficient frangipani machine authenticate petal acting behalf user remote file access protocols nfs full security requires petal servers lock servers run trusted operating systems types components authenticate finally ensure file data private users prevented eavesdropping network interconnecting petal frangipani machines fully solve problems placing machines environment prevents users booting modified operating system kernels interconnecting private network user processes granted access necessarily machines locked room private physical network cryptographic techniques secure booting authentication encrypted links applications partial solutions acceptable typical existing nfs installations secure network eavesdropping data modification user boots modified kernel workstation implemented security measures date reach roughly nfs level security petal servers accept requests list network addresses belonging trusted frangipani server machines frangipani file systems exported untrusted machines administrative domain configuration illustrated figure distinguish frangipani client server machines trusted frangipani servers communicate petal lock service located restricted environment interconnected private network discussed remote untrusted clients talk frangipani servers separate network direct access petal servers clients talk frangipani server file access protocol supported host operating system dce dfs nfs smb becausefrangipani local file system machine running frangipani server prouser programs file system switch nfs dfs client file system switch frangipani file server module nfs dfs server petal device driver network lock service petal frangipani client machine frangipani server machine figure client server configuration frangipani server provide file access local machine remote client machines connect standard network file system protocols tocol supports coherent access dce dfs frangipani coherence multiple servers thrown level ideally protocol support failover frangipani server protocols mentioned support failover directly technique machine address failed machine systems applied security reason client server configuration frangipani runs kernel quickly portable operating systems versions unix clients frangipani unsupported system accessing supported remotely discussion idea building file system layers lower level providing storage repository higher level providing names directories files unique frangipani earliest universal file server storage facility provided petal substantially earlier systems leading higher level structure section detailed comparisons previous systems frangipani designed work storage abstraction provided petal fully considered design needed exploit alternative storage abstractions nasd petal highly storage scale throughput capacity resources added petal provision coordination sharing storage multiple clients applications directly petal client interface disk-like file-like frangipani file system layer makes petal applications retaining extending good properties strength frangipani transparent server addition deletion failure recovery easily combining write-ahead logging locks uniformly accessible highly store strength frangipani ability create consistent backups system running frangipani backup param eters logs allocation bitmaps inodes small blocks large blocks figure disk layout frangipani takes advantage petal large sparse disk address space simplify data structures server log blocks allocation bitmap space mechanism discussed section aspects frangipani design problematic frangipani replicated petal virtual disk implies logging occurs frangipani log petal frangipanidoes disk location information placing data petal virtualizes disks finally frangipani locks entire files directories individual blocks usage experience evaluate aspects design general setting frangipani measured performance engineering workloads tested good disk layout frangipani large sparse disk address space petal simplify data structures general idea reminiscent past work programming computers large memory address spaces address space parcelled generously petal virtual disk bytes address space petal commits physical disk space virtual addresses written petal decommit primitive frees physical space backing range virtual disk addresses internal data structures small petal commits decommits space fairly large chunks range addresses data written decommitted physical disk space allocated petal clients afford make data structures sparse physical disk space wasted fragmentation figure shows frangipani divides virtual disk space region stores shared configuration parameters housekeeping information terabyte virtual space region fact kilobytes region stores logs frangipani server obtains portion space hold private log reserved bytes region partitioned logs choice limits current implementation servers easily adjusted region allocation bitmaps describe blocks remaining regions free frangipani server locks portion bitmap space exclusive server bitmap space fills finds locks unused portion bitmap region long fourth region holds inodes file inode hold metadata timestamps pointers location data symbolic links store data directly inode made inodes bytes long size disk block avoiding unnecessary contention false sharing servers occur servers needed access inodes block allocate inode space allowing room inodes mapping bits allocation bitmap inodes fixed frangipani server allocates inodes files portions inode space corresponds portions allocation bitmap frangipani server read write free existing file inode region holds small data blocks bytes size blocks file stored small blocks file grows rest stored large block allocate bytes small blocks allowing times maximum number inodes remainder petal address spaceholds large data blocks address space reserved large block disk layout policy blocks suffer fragmentation policy carefully husbands disk space allocating bytes inode wasteful space alleviate problems storing small files inode gain design simplicity reasonable tradeoff cost extra physical disk space current scheme limits frangipani slightly million large files large file file bigger file larger small blocks large block limits prove small easily reduce size large blocks making larger number permit large files span large block raising maximum file size byte address space limit prove inadequate single frangipani server support multiple frangipani file systems multiple virtual disks chosen file system parameters based usage experience earlier file systems choices serve time usage confirm design frangipani flexible experiment layouts cost backup restore file system logging recovery frangipani write-ahead redo logging metadata simplify failure recovery improve performance user data logged section word file includes directories symbolic links frangipani server private log petal frangipani file server make metadata update creates record describing update appends log memory log records periodically written petal order updates describe requested optionally log records written synchronously offers slightly failure semantics cost increased latency metadata operations log record written petal server modify actual metadata permanent locations permanent locations updated periodically roughly seconds unix update demon logs bounded size current implementation petal allocation policy log composed fragments distinct physical disks space allocated log managed circular buffer log fills frangipani reclaims oldest log space log entries ordinarily entries reclaimed area refer metadata blocks written 
petal previous sync operation case additional petal writes metadata blocks written work completed log reclaimed size log typical sizes frangipani log records bytes log fill periodic sync operations operations modify metadata interval frangipani server crashes system eventually detects failure runs recovery server log failure detected client failed server lock service asks failed server return lock holding reply recovery demon implicitly ownership failed server log locks demon finds log start end examines record order carrying update complete log processing finished recovery demon releases locks frees log frangipani servers proceed unobstructed failed server failed server optionally restarted empty log long underlying petal volume remains system tolerates unlimited number frangipani server failures ensure recovery find end log disk controllers write data order attach monotonically increasing log sequence number -byte block log end log reliably detected finding sequence number lower preceding frangipani ensures logging recovery work correctly presence multiple logs requires attention details frangipani locking protocol section ensures updates requested data servers serialized write lock covers dirty data change owners dirty data written petal original lock holder recovery demon running behalf implies log hold uncompleted update block frangipani ensures recovery applies updates logged server acquired locks cover holds locks needed ensure serialization imposed locking protocol violated make guarantee enforcing stronger condition recovery replays log record describing update completed accomplish version number -byte metadata block metadata directories span multiple blocks multiple version numbers block log record updates record description version number recovery block applied block version number record version number user data updates logged metadata blocks space reserved version numbers creates complication block metadata freed reused user data log records referring block skipped properly version number overwritten arbitrary user data frangipani avoids problem reusing freed metadata blocks hold metadata finally frangipani ensures time recovery lock service guarantees granting active recovery demon exclusive lock log frangipani logging recovery schemes assume disk write failure leaves contents single sector state state combination sector damaged reading returns crc error petal built-in replication ordinarily recover copies sector lost frangipani data structures corrupted software bug metadata consistency check repair tool unix fsck needed implemented tool date frangipani logging intended provide high-level semantic guarantees users purpose improve performance metadata updates speed failure recovery avoiding run programs fsck time server fails metadata logged user data user guarantee file system state consistent point view failure claim semantics ideal standard local unix file systems provide local unix file systems frangipani user consistency semantics calling fsync suitable checkpoints frangipani logging application techniques developed databases log-based file systems frangipani log-structured file system data log maintaining conventional on-disk data structures small log adjunct provide improved performance failure atomicity unlike log-based file systems cited log-structured file systems zebra xfs frangipani multiple logs synchronization cache coherence multiple frangipani servers modifying shared on-disk data structures careful synchronization needed give server consistent view data concurrency scale performance load increased servers added frangipani multiple-reader single-writer locks implement synchronization lock service detects conflicting lock requests current holder lock asked release downgrade remove conflict read lock server read data disk cache server asked release read lock invalidate cache entry complying write lock server read write data cache server cached copy disk block on-disk version holds relevant write lock server asked release write lock downgrade read lock write dirty data disk complying retain cache entry downgrading lock invalidate releasing lock flushing dirty data disk write lock released downgraded chosen bypass disk forward dirty data directly requester reasons simplicity design frangipani servers communicate communicate petal lock server design ensures server crashes process log server dirty buffers directly forwarded destination server dirty buffer crashed log entries referring dirty buffer spread machines pose problem recovery reclaiming log space fills divided on-disk structures logical segments locks segment avoid false sharing ensure single disk sector hold data structure shared division on-disk data structures lockable segments designed number locks small avoid lock contention common case lock service bottleneck system log single lockable segment logs private bitmap space divided segments locked exclusively contention files allocated data block inode allocated file protected lock segment allocation bitmap holds bit marking free finally file directory symbolic link segment lock protects inode file data points per-file lock granularity engineering workloads files rarely undergo concurrent write-sharing workloads require finer granularity locking operations require atomically updating on-disk data structures covered locks avoid deadlock globally ordering locks acquiring phases server determines locks involve acquiring releasing locks names directory sorts locks inode address acquires lock turn server checks objects examined phase modified locks released releases locks loops back repeat phase performs operation dirtying blocks cache writing log record retains lock dirty blocks covers written back disk cache coherence protocol similar protocols client file caches echo andrew file system dce dfs sprite deadlock avoidance technique similar echo frangipani oracle data base oracle parallel server writes dirty data disk cache-to-cache transfers successive owners write lock lock service frangipani requires small generic set functions lock service expect service performance bottleneck normal operation implementations fill requirements lock service implementations frangipani project existing lock services provide functionality thin layer additional code top lock service multiple-reader single-writer locks locks sticky client generally retain lock client conflicting recall clients lock service frangipani servers lock service deals client failure leases client contacts lock service obtains lease locks client acquires lease lease expiration time set seconds creation renewal client renew lease expiration time service failed network failures prevent frangipani server renewing lease crashed server discards locks data cache cache dirty frangipani turns internal flag subsequent requests user programs return error file system unmounted clear error condition chosen drastic reporting error make difficult ignore inadvertently initial lock service implementation single centralized server lock state volatile memory server adequate frangipani becausethe frangipaniservers logs hold state information permit recovery lock service loses state crash lock service failure large performance glitch implementation stored lock state petal virtual disk writing lock state change petal returning client primary lock server crashed backup server read current state petal provide continued service scheme failure recovery transparent performance common case poorer centralized in-memory approach fully implement automatic recovery failure modes implementation final lock service implementation fully distributed fault tolerance 
scalable performance consists set mutually cooperating lock servers clerk module linked frangipani server lock service organizes locks tables namedbyascii strings individual locks tables named -bit integers recall single frangipani file system petal virtual disk multiple frangipani file systems mounted machine file system table frangipani file system mounted frangipani server calls clerk opens lock table file system lock server clerk lease identifier successful open subsequent communication file system unmounted clerk closes lock table clerks lock servers communicatevia asynchronousmessages rpc minimize amount memory achieve good flexibility performance basic message types operate locks request grant revoke release requestand release message types clerk lock server grant revoke message types lock server clerk lock upgrade downgrade operations handled message types lock service fault-tolerant distributed failure detection mechanism detect crash lock servers mechanism petal based timely exchange heartbeat messages sets servers majority consensus tolerate network partitions locks consume memory server clerk current implementation server allocates block bytes lock addition bytes clerk outstanding granted lock request client bytes lock avoid consuming memory sticky locks clerks discard locks long time hour small amount global state information change consistently replicated lock servers lamport paxos algorithm lock service reuses implementation paxos originally written petal global state information consists list lock servers list locks responsible serving list clerks opened closed lock table information achieve consensus reassign locks lock servers recover lock state clerks lock server crash facilitate recovery frangipani servers efficiency locks partitioned hundred distinct lock groups assigned servers group individually locks occasionally reassigned lock servers compensate crashed lock server advantage newly recovered lock server similar reassignment occurs lock server permanently added removed system cases locks reassigned number locks served server balanced number reassignments minimized lock served lock server reassignment occurs phases phase lock servers lose locks discard internal state phase lock servers gain locks contact clerks relevant lock tables open servers recover state locks clerks clerks informed servers locks frangipani server crashes locks owns released recovery actions performed specifically crashed frangipani server log processed pending updates written petal frangipani server lease expires lock service clerk frangipani machine perform recovery release locks belonging crashed frangipani server clerk granted lock ensure exclusive access log lock covered lease lock service start recovery process fail general frangipani system tolerates network partitions continuing operate shutting cleanly specifically petal continue operation face network partitions long majority petal servers remain communication parts petal virtual disk inaccessible replica majority partition lock service continues operation long majority lock servers communication frangipani server partitioned lock service unable renew lease lock service declare frangipani server dead initiate recovery log petal frangipani server partitioned petal unable read write virtual disk cases server disallow user access affected file system partition heals file system remounted small hazard frangipaniserver lease expires server crash contact lock service due network problems access petal lease expired frangipani server checks lease valid valid margin seconds attempting write petal petal checking write requestarrives sufficienttime delay frangipani lease check arrival subsequent write request petal problem lease expired lock server large error margin margin seconds normal circumstances problem occur rule absolutely future eliminate hazard method work add expiration timestamp write request petal timestamp set current lease expiration time moment write request generated minus margin petal ignore write request timestamp current time method reliably rejects writes expired leases provided clocks petal frangipani servers synchronized margin method required synchronized clocks integrate lock server petal include lease identifier obtained lock server write request petal petal reject write request expired lease identifier adding removing servers frangipani installation grows system administrator occasionally add remove server machines frangipani designed make task easy adding frangipani server running system requires minimal amount administrative work server told petal virtual disk find lock service server contacts lock service obtain lease determines portion log space lease identifier operation administrator touch servers adapt presence automatically removing frangipani server easier adequate simply shut server preferable server flush dirty data release locks halting strictly needed server halts abruptly recovery run log time locks needed bringing shared disk consistent state administrator touch servers petal servers added removed transparently petal paper lock servers added removedinasimilarmanner backup petal snapshotfeature convenientway make consistent full dumps frangipani file system petal client create exact copy virtual disk point time snapshot copy appears identical ordinary virtual disk modified implementation copy-on-write techniques efficiency snapshots crashconsistent snapshot reflects coherent state petal virtual disk left frangipani servers crash backup frangipani file system simply taking petal snapshot copying tape snapshot include logs restored copying back petal virtual disk running recovery log due crash-consistency restoring snapshot reduces problem recovering system-wide power failure improve scheme minor change frangipani creating snapshots consistent file system level require recovery accomplish backup program force frangipani servers barrier implemented ordinary global lock supplied lock service frangipani servers acquire lock shared mode modification operation backup process requests exclusive mode frangipani server receives request release barrier lock enters barrier blocking file systemcalls modify data cleaning dirty data cache releasing lock frangipani servers entered barrier backup program acquire exclusive lock makes petal snapshot releases lock point servers reacquire lock shared mode normal operation resumes scheme snapshot mounted frangipani volume recovery volume accessed on-line retrieve individual files dumped tape conventional backup format require frangipani restoration volume mounted read-only petal snapshots readonly future extend petal support writable snapshots implement thin layer top petal simulate performance frangipani layered structure made easier build monolithic system expect layering exact cost performance section show frangipani performance good spite layering file systems latency problems frangipani solved straightforwardly adding non-volatile memory nvram buffer front disks effective place put nvram system directly physical disks petal server software ordinary prestoserve cards drivers suffice purpose petal frangipani needed failure nvram petal server treated petal equivalent server failure severalaspects frangipaniand petal combine provide good scaling throughput parallelism layers system multiple frangipani servers multiple petal servers multiple disk arms working parallel clients system parallelism increases aggregate throughput compared centralized network file server frangipani difficulty dealing hot spots file system processing split shifted machines files frangipani petal logs commit updates clients log write group commit providing improved log throughput load individual clients large writes benefit parallelism due petal striping data multiple 
disks servers experimental setup planning build large storage testbed petal nodes attached hundred disks frangipani servers petal nodes small array controllers attached off-the-shelf disks network frangipani servers typical workstations testbed study performance frangipani large configuration ready report numbers smaller configuration measurements reported mhz dec alpha machines petal servers machine stores data digital disks inch fast scsi drives storing average seek time sustained transfer rate machine connected port atm switch mbit point-to-point link prestoserve cards nvram servers petal servers supply data aggregate rate replicated virtual disks petal servers sink data aggregate rate single machine performance subsection compares frangipani code path compares unix vnode file system digital advanced file system advfs advfs comparison familiar bsd-derived ufs file system advfs significantly faster ufs advfs stripe files multiple disks achieving double throughput ufs test machines unlike ufs synchronouslyupdates metadata advfs write-ahead log frangipani significantly reduces latency operations file creation advfs ufs similar performance reading small files directories ran advfs frangipani file systems identical machines storage subsystems comparable performance machine mhz dec alpha cpu ram managed unified buffer cache ubc connected atm switch point-to-point link frangipani file system local disks accesses replicated petal virtual disk petal device driver accessed raw device interface block sizes petal driver read write data saturating atm link petal server cpu utilization read latency petal disk advfs file system storage subsystem performance roughly equivalent petal configuration consists digital disks connected fast scsi strings backplane controllers accessedthrough raw device interface controllers disks supply data cpu utilization read latency connected advfs file system petal virtual disk ensure file systems identical storage subsystems previous experiments shown advfs slower run petal present advfs light chose intention compare petal cost performance locally attached disks hardware resources required provide storage subsystemsfor frangipani advfs vastly goal demonstrate frangipani code path efficient compared existing well-tuned commercial file system hardware resources petal non-trivial resources amortized multiple frangipani servers tables compare performance systems standard benchmarks table columns advfs raw column benchmark run advfs directly accessing local disks advfs nvr column benchmark rerun nvram interposed front local disks frangipani raw column benchmark run frangipani accessing petal device interface frangipani nvr column frangipani configuration retested addition nvram buffer petal disks numbers averaged ten runs benchmarks standard deviation cases advfs frangipani phase description raw nvr raw nvr create directories copy files directory status scan files compile table modified andrew benchmark unmount operations compare performance file system configurations local access nvram digital unix advanced file system advfs frangipani frangipani nvram buffer added petal disks unmount file system end phase table entry average elapsed time seconds smaller numbers table results modified andrew benchmark widely file system benchmark phase benchmark creates tree directories phase copies collection source files tree phase traverses tree examines status file directory fourth phase reads file tree phase compiles links files difficult make comparative measurements modified andrew benchmark standard form becausethe benchmarkdoes accountfor work deferred file system implementation work deferred phase benchmark performed phase inappropriately charged phase work deferred past end benchmark accounted traditional unix file systems advfs frangipani defer cost writing dirty file data sync operation explicitly requested user triggered background periodic update demon unlike traditional unix file systems advfs frangipani log-based write metadata updates synchronously disk metadata updates deferred sync log wraps order account properly sources deferred work changed benchmark unmount file system phase choseto unmount async call digital unix sync queues dirty data writing guarantee reached disk returning results shown table frangipani comparable advfs phases table shows results running connectathon benchmark connectathon benchmark tests individual operations small groups related operations providing insight sources differences visible andrew benchmark andrew benchmark benchmark account deferred work unmounted file system end phase frangipani latencies nvram roughly comparable advfs frangipani test description raw nvr raw nvr file directory creation creates files directories file directory removal removes files directories lookup mount point getwd stat calls setattr getattr lookup chmods stats files write writes byte file times read reads byte file times readdir reads directory entries files link rename renames links files symlink readlink symlinks readlinks files statfs statfs calls table connectathon benchmark unmount operations run connectathon benchmark unmount operation included end test table entry average elapsed time seconds smaller numbers test anomalous due bug advfs advfs notable exceptions tests creating files setting attributes reading directories significantly longer frangipani practice latencies small users hard optimize file creation takes longer frangipani partly log fills times test double log size times reduce seconds frangipani slower file read test advfs file read test peculiar artifact implementation iteration read test benchmark makes system call invalidate file buffer cache reading current advfs implementation appears ignore invalidation directive read test measures performance advfs reading cache disk redid test cold advfs file cache performance similar frangipani seconds nvram report throughput achieved single frangipani server reading writing large files file reader sits loop reading set files iteration loop flushes contents files buffer cache file writer sits loop repeatedly writing large private file file large steady stream write traffic disk read write tests run minutes observed significant variation throughput time-averaged steady state results summarized table presence absence nvram effect timing single frangipani machine write data throughput cpu utilization frangipani advfs frangipani advfs write read table frangipani throughput cpu utilization show performance frangipani reading writing large files limit imposed atm link udp software machine frangipani achieves good performance clustering writes petal naturally aligned blocks difficult make frangipani occasionally sync write part data smaller blocks smaller block sizes reduces maximum throughput udp stack frangipani server cpu utilization petal server cpus bottleneck single frangipani machine read data cpu utilization performance improved changing read-ahead algorithm frangipani frangipani read-ahead algorithm borrowed bsd-derived file system ufs effective advfs comparison advfs write data accessing large files striped disks connected controllers cpu utilization advfs read performance cpu utilization cpu controllers bottlenecked advfs performance improved bit tuning interesting note frangipani simple policy lay data latency write throughput comparable conventional file systems elaborate policies frangipani good write latency latency-critical metadata updates logged asynchronously performed synchronously place file systems ufs synchronously update metadata careful data placement separate experiments found frangipani updates logs synchronously performance good log allocated 
large physically contiguous blocks nvram absorbs write latency frangipani achieves good write throughput large files physically striped contiguous units disks machines frangipani exploit parallelism inherent structure frangipani good read throughput large files reason recall section individual blocks files smaller allocated contiguously disk frangipani read-ahead small files hide disk seek access times frangipani bad read performance small files quantify small read performance ran experiment processes single frangipani machine read separate files invalidating buffer cache frangipani throughput cpu bottleneck petal accessed raw device interface blocks deliver frangipani maximum throughput achievable case scaling section studies scaling characteristics frangipani ideally operational latencies unchanged throughput scales linearly servers added compile scan files directory status copy files create directories frangipani machines elapsed time secs figure frangipani scaling modified andrew benchmark frangipani servers simultaneously run modified andrew benchmark independent data sets y-axis averageelapsed time frangipani machine complete benchmark figure shows effect scaling frangipani running modified andrew benchmark experiment measure average time frangipani machine complete benchmark number machines increased experiment simulates behavior users program development shared data pool notice minimal negative impact latency frangipani machines added fact single machine machine experiment average latency increased surprising benchmark exhibits write sharing expectlatencies remain unaffected increasedservers read file uncached linear scaling frangipani machines throughput figure frangipani scaling uncached read frangipani servers simultaneously read set files dotted line shows linear speedup curve comparison figure illustrates frangipani read throughput uncached data test replicate reader single-server experiment multiple servers test runs minutes observe negligible variation steady-state throughput figure frangipani shows excellent scaling test process installing frangipani machines expect aggregate read performance increase saturates petal servers capacity figure illustrates frangipani write throughput writer single-server experiment replicated multiple servers server distinct large file experiment write file linear scaling frangipani machines throughput figure frangipani scaling write frangipani server writes large private file dotted line shows linear speedup curve comparison performance tapers early atm links petal servers saturated runs minutes observe variation steady-state throughput interval lock contention experiment performance scale atm links petal servers saturated virtual disk replicated write frangipani server turns writes petal servers effects lock contention frangipani coarse-grained locking entire files important study effect lock contention performance report experiments experiment measures effect read write sharing files readers compete single writer large file initially file cached readers writer readers read file sequentially writer rewrites entire file result writer repeatedly acquires write lock callback downgrade readers read lock callback writer flush data disk time reader repeatedly acquires read lock callback release writer write lock callback reader invalidate cache read reacquiring lock fetch data disk read-ahead read-ahead number readers read throughput figure frangipani reader writer contention frangipani servers read shared file single frangipani server writes file show effect read-ahead performance results observed experiment unexpected distributed lock manager designed fair granting locks simulations show true implementation single writer readers make lock requests uniform rate serviced round-robin fashion successive grants write lock writer separated grants read lock readers interval downgrade callbacks expect number read requests aggregate read throughput increase readers added limit large scaling linear observe behavior experiment read throughput flattens readers running shown dashed line figure earlier figure frangipani servers achieve lock contention conjectured anomalous behavior caused read-ahead repeated experiment read-ahead check read-ahead disadvantageous presence heavy read write contention reader called back release lock invalidate cache read-ahead data cache delivered client discarded work read turns wasted readers extra work make lock requests rate writer redoing experiment read-ahead disabled yielded expected scaling result shown solid line figure make performance improvement users letting explicitly disable read-ahead specific files devising heuristic recognize case disable read-ahead automatically trivial implement affect parts operating system kernel frangipani making inconvenient support future releases kernel approach devised tested heuristics triangle diamond number readers read throughput triangle triangle triangle triangle triangle diamond diamond diamond diamond diamond figure effect data size reader writer contention frangipani readers share varying amounts data frangipani writer readahead disabled experiment experiment variation readers run writer modifies amounts file data frangipani locks entire files readers invalidate entire cache irrespective writer behavior readers acquire lock faster writer updating fewer blocks data writer flush smaller amounts data disk figure shows performance frangipani read-ahead disabled readers writer concurrently share differing amounts data expected shared data smaller performance experiment measures effects write write sharing files base case frangipani server writes file isolation added frangipani servers wrote file measured degradation performance writers modify file data blocks frangipani whole-file locking offsets writers irrelevant test found aggregate bandwidth writers dropped single-writer case writers surprising multiple writers modify file write system call lock revocation request revocation request lock holder flush dirty data petal locks revoked write system call call dirties data throughput limited smaller block sizes throughput smaller experience workloads exhibit concurrent write sharing straightforward extend frangipani implement byte-range locking block locking improve performance workloads read write parts file making similar performance writing files current system workloads multiple machines concurrently read write blocks file filesystem interprocess communication channel perform frangipani simply targeted workloads related work frangipani cambridge universal file server takes two-layered approach building file system split layers cfs lower layer clients abstractions files indices file systems built cfs abstractions implement files directories major difference cfs petal cfs single machine manages storage nfs file system simply remote file access protocol nfs protocol weak notion cache coherence stateless design requires clients access servers frequently maintain level coherence frangipani strongly coherent single system view protocol maintains state eliminates unnecessary accesses servers andrew file system afs offshoot dce dfs provide cache performance coherence nfs afs designed kind scalability frangipani frangipani unified cluster file system draws single pool storage scaled span disk drives machines common administration contrast afs global space security architecture plug separate file servers clients wide area afs frangipani approaches scaling complementary make good sense frangipani servers export file system wide-area clients afs dce dfs space access protocol frangipani echo file system logbased replicates data reliability access paths availability permits volumes span multiple disks coherent caching echo share frangipani scalability echo volume managed server time failover designated backup volume span disks connected single machine internal layering file service atop disk service echo implementation requires layers run 
address space machine experience echo showed server cpu bottleneck vms cluster file system offloads file system processing individual machines members cluster frangipani cluster member runs instance file systemcode top shared physical disk synchronization provided distributed lock service shared physical disk accessed special-purpose cluster interconnect disk controller directly connected ordinary network ethernet machine acting disk server frangipani improves design ways shared physical disk replaced shared scalable virtual disk provided petal frangipani file system log-based quick failure recovery frangipani extensive caching data metadata performance spiralog file system offloads file system processing individual cluster members run shared storage system layer interface layers spiralog differs original vms cluster file system petal lower layer file-like simply disk-like array stably-stored bytes permits atomic actions update arbitrarily scattered sets bytes array spiralog split layers simplifies file system complicates storage system considerably time spiralog storage system share petal scalability fault tolerance spiralog volume span disks connected machine unavailable machine crashes designed cluster file system calypso similar echo vms clusters frangipani echo calypso stores files multiported disks machines directly connected disk acts file server data stored disk machine fails takes members calypso cluster access current server file system clients frangipani echo clients caches coherent multiple-reader single-writer locking protocol comparison purposes authors calypso built file system shared-disk style called pjfs calypso performed pjfs leading abandon shareddisk approach pjfs differs frangipani main respects lower layer centralized disk server distributed virtual disk petal file server machines pjfs share common log shared log proved performance bottleneck frangipani pjfs locks shared disk wholefile granularity granularity caused performance problems workloads large files concurrently write-shared multiple nodes expect present frangipani implementation similar problems workloads noted section adopt byte-range locking shillner felten built distributed file system top shared logical disk layering system similar lower layer multiple machines cooperate implement single logical disk upper layer multiple independent machines run file system code top logical disk providing access files unlike petal logical disk layer provide redundancy system recover node fails restarts dynamically configure failed nodes configure additional nodes file system careful ordering metadata writes logging frangipani logging technique avoids full metadata scan fsck restore consistency server crash unlike logging lose track free blocks crash necessitating occasional garbage collection scan find unable compare performance system present performance numbers file system layer xfs file system closest spirit frangipani fact goals systems essentially distribute management responsibility files multiple machines provide good availability performance frangipani effectively serverless sense xfs service distributed machines configured frangipani server petal server machine frangipani locking coarser-grained xfs supports block-level locking work differs xfs principal ways internal organization file system interface storage system significantly xfs unlike frangipani xfs predesignated manager file storage server log-structured contrast frangipani organized set cooperating machines petal shared store separate lock service concurrency control simpler model reminiscent multithreaded shared memory programs communicate common store locks synchronization model deal file system recovery server addition deletion machinery xfs requires made system easier implement test addressed file system recovery reconfiguration issues left open problems xfs work date compare frangipani performance xfs considerable performance work remains completed current xfs prototype comparison systems time premature unfair xfs conclusions frangipani file system users coherent shared access set files scalable provide storage space higher performance load balancing user community grows remains spite component failures requires human administration administration complex components added growing installation frangipani feasible build two-layer structure consisting multiple file servers running simple file system code top shared petal virtual disk petal lower layer provided benefits petal implements data replication high availability obviating frangipani petal virtual disk uniformly accessible frangipani servers server serve file machine run recovery server fails petal large sparse address space allowed simplify frangipani on-disk data structures frangipani simple data layout allocation policy coarse-grained locking happy performance initial performance measurements frangipani comparable production digital unix file system expect improvement tuning frangipani shown good scaling properties size testbed configuration petal nodes frangipani nodes results leave optimistic system continue scale nodes future plans include deploying frangipani dayto-day hope gain experience prototype load validate scalability testing larger configurations experiment finer-grained locking complete work backup finally ideas frangipani make commercial products acknowledgments mike burrows mike schroeder puneet kumar helpful advice frangipani design comments paper fay chang implemented early prototype frangipani summer project anonymous referees shepherd john wilkes suggested improvements paper cynthia hibbard provided editorial assistance thomas anderson michael dahlin jeanna neefe david patterson drew roselli randolph wang serverless network file systems acm transactions computer systems february philip bernstein vassos hadzilacos nathan goodman concurrency control recovery database systems addison-wesley anupam bhide elmootazbellah elnozahy stephen morgan highly network file server proceedings winter usenix conference pages january birrell needham universal file server ieee transactions software engineering seseptember andrew birrell andy hisgen charles jerian timothy mann garret swart echo distributed file system research report systems research center digital equipment corporation september michael burrows efficient data sharing phd thesis cambridge september chao english jacobson stepanov wilkes mime high performance parallel storage device strong recovery guarantees technical report hpl-csp- hewlett-packard laboratories november jeffrey chase henry levy michael feeley edward lazowska sharing protection singleaddress-spaceoperating system acm transactions computer systems november sailesh chutani owen anderson michael kazar bruce leverett anthony mason robert sidebotham episode file system proceedings winter usenix conference pages january wiebren jonge frans kaashoek wilson hsieh logical disk approach improving file systems proc symp operating systems principles pages december murthy devarakonda bill kish ajay mohindra recovery calypso file system acm transactions computer systems august murthy devarakonda ajay mohindra jill simoneaux william tetzlaff evaluation design alternatives cluster file system proceedings winter usenix conference pages january garth gibson david nagle khalil amiri fay chang eugene feinberg howard gobioff chen lee berend ozceri erik riedel david rochberg jim zelenka file server scaling network-attached secure disks proceedings acm international conference measurements modeling computer systems sigmetrics pages june andrew goldstein design implementation distributed file system digital technical journal september digital equipment corporation nagog park acton cary gray david cheriton leases efficient faulttolerant mechanism distributed file cache consistency proc symp operating systems principles pages december robert hagmann reimplementing cedar file system logging group commit proc symp operating systems principles pages november john hartman john 
ousterhout zebra striped network file system acm transactions computer systems august andy hisgen andrew birrell charles jerian timothy mann garret swart new-value logging echo replicated file system research report systems research center digital equipment corporation june john howard michael kazar sherri menees david nichols satyanarayanan robert sidebotham michael west scale performance distributed file system acm transactions computer systems february james johnson william laing overview spiralog file system digital technical journal digital equipment corporation nagog park acton michael kazar bruce leverett owen anderson vasilis apostolides ben bottos sailesh chutani craig everhart antony mason shu-tsui edward zayas decorum file system architectural overview proceedings summer usenix conference pages june nancy kronenberg henry levy william strecker vaxclusters closely-coupled distributed system acm transactions computer systems leslie lamport part-time parliament research report systems research center digital equipment corporation september edward lee chandramohan thekkath petal distributed virtual disks proc intl conf architectural support programming languages operating systems pages october barbara liskov sanjay ghemawat robert gruber paul johnson liuba shrira michael williams replication harp file system proc symp operating systems principles pages october timothy mann andrew birrell andy hisgen charles jerian garret swart coherent distributed file cache directory write-behind acm transactions computer systems marshal kirk mckusick william joy samuel leffler robert fabry fast file system unix acm transactions computer systems august james mitchell jeremy dion comparison network-based file servers communications acm april sape mullender andrew tanenbaum files software practice experience april michael nelson brent welch john ousterhout caching sprite network file system acm transactions computer systems february brian pawlowski chet juszczak peter staubach carl smith diane lebel david hitz nfs version design implementation proceedings summer usenix conference pages june mendel rosenblum john ousterhout design analysis evolution journaling file systems vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison vijayan dusseau remzi wisc abstract develop apply methods analyzing file system behavior evaluating file system semantic block-level analysis sba combines knowledge on-disk data structures trace disk traffic infer file system behavior contrast standard benchmarking approaches sba enables users understand file system behaves semantic trace playback stp enables traces disk traffic easily modified represent file system implementation contrast directly modifying file system stp enables users rapidly gauge benefits policies sba analyze linux ext reiserfs jfs windows ntfs process uncover strengths weaknesses journaling file systems apply stp evaluate modifications ext demonstrating benefits optimizations incurring costs real implementation introduction modern file systems journaling file systems writing information pending updates write-ahead log committing updates disk journaling enables fast file system recovery crash basic techniques existed years cedar episode journaling increased popularity importance recent years due ever-increasing disk capacities scan-based recovery fsck prohibitively slow modern drives raid volumes popularity importance journaling file systems ext reiserfs jfs ntfs internal policies understanding file systems behave important developers administrators application writers time perform detailed analysis journaling file systems previous work analyzed file systems writing userlevel programs measuring time file system operations elicit salient aspects file system performance difficult discover underlying reasons observed performance approach paper employ benchmarking methodology called semantic block-level analysis sba trace analyze file systems sba induce controlled workload patterns file system focus analysis time operations resulting stream read write requests file system analysis semantic leverage information block type block request journal inode analysis block-level interposes block interface storage analyzing low-level block stream semantically meaningful understand file system behaves analysis hints file system improved reveal change worth implementing traditionally potential improvement file system implement change measure performance workloads change improvement implementation effort wasted paper introduce apply complementary technique sba called semantic trace playback stp stp enables rapidly suggest evaluate file system modifications large implementation simulation effort real workloads traces show stp effectively applied detailed analysis linux ext reiserfs preliminary analysis linux jfs windows ntfs case focus journaling aspects file system determine events data metadata written journal fixed locations examine characteristics workload configuration parameters size journal values commit timers impact behavior analysis uncovered design flaws performance problems correctness bugs file systems ext reiserfs make design decision group unrelated traffic compound transaction result tangled synchrony single disk-intensive process forces write traffic disk affecting performance asynchronous writers find ext reiserfs artificially limit parallelism preventing overlap pre-commit journal writes fixed-place updates analysis reveals ordered data journaling modes ext exhibits eager writing forcing data blocks disk sooner typical -second delay addition find jfs proceedings usenix annual technical conference april anaheim infinite write delay utilize commit timers indefinitely postpones journal writes trigger forces writes occur memory pressure finally identify previously unknown bugs reiserfs fixed subsequent releases main contributions paper methodology semantic block analysis sba understanding internal behavior file systems methodology semantic trace playback stp rapidly gauging benefits file system modifications heavy implementation effort detailed analysis sba important journaling file systems ext reiserfs preliminary analysis jfs ntfs evaluation stp design implementation alternatives ext rest paper organized describe techniques sba stp apply techniques ext reiserfs jfs ntfs discuss related work conclude methodology introduce techniques evaluating file systems semantic block analysis sba enables users understand internal behavior policies file system semantic trace playback stp users quantify changing file system impact performance real workloads semantic block-level analysis file systems traditionally evaluated approaches applies synthetic real workloads measures resulting file system performance collects traces understand file systems performing isolation misses interesting opportunity correlating observed disk traffic running workload performance answer workload behaves block-level tracing disk traffic analyze number interesting properties file system workload coarsest granularity record quantity disk traffic divided reads writes information understanding file system caching write buffering affect performance detailed level track block number block read written analyzing block numbers extent traffic sequential random finally analyze timing block timing information understand file system initiates burst traffic combining block-level analysis semantic information blocks infer ext reiserfs jfs ntfs sba generic sba specific sba total table code size sba drivers number statements counted number semicolons needed implement sba ext reiserfs preliminary sba jfs ntfs behavior file system main difference semantic block analysis sba standard block-level tracing sba analysis understands on-disk format file system test sba enables understand properties file system sba distinguish traffic journal versus in-place data track individual transactions journal implementation infrastructure performing sba straightforward places pseudo-device driver kernel associates underlying disk mounts file system interest ext pseudo device refer sba driver runs controlled microbenchmarks generate disk traffic sba driver passes traffic disk efficiently tracks request response storing small record fixed-sized circular buffer note tracking ordering requests responses pseudo-device driver infer order requests scheduled lower levels system sba requires interpret contents disk block traffic interpret contents journal infer type journal block descriptor commit block interpret journal descriptor block data blocks journaled result efficient semantically interpret block-level traces on-line performing analysis off-line require exporting contents blocks greatly inflating size trace sba driver customized file system test concern amount information embedded sba driver file system focus paper understanding journaling file systems sba drivers embedded information interpret placement contents journal blocks metadata data blocks analyze complexity sba driver journaling file systems ext reiserfs jfs ntfs journaling file systems journal transactions temporarily recorded fixed-location data structures data permanently reside sba driver distinguishes traffic journal fixed-location data structures traffic simple distinguish reiserfs jfs ntfs journal set contiguous blocks separate rest file system backward proceedings usenix annual technical conference april anaheim compatible ext ext treat journal regular file determine blocks belong journal sba knowledge inodes indirect blocks journal change location created classification remains efficient run-time sba classify types journal blocks descriptor block journal data block commit block perform analysis journaling file systems sba driver understand details file system driver understand directory blocks superblock ext tree structure reiserfs jfs wishes infer additional file system properties embed sba driver knowledge sba driver policies parameters file system fact sba infer policies parameters table reports number statements required implement sba driver numbers show code sba driver statements general infrastructure approximately 
statements needed support journaling file systems ext specific code file systems ext journal created file span multiple block groups order find blocks belonging journal file parse journal inode journal indirect blocks reiserfs jfs ntfs journal contiguous finding blocks trivial journal file ntfs small journals contiguously allocated workloads sba analysis gather information workload focus paper understanding internal policies behavior file system result construct synthetic workloads uncover decisions made file system realistic workloads considered apply semantic trace playback constructing synthetic workloads stress file system previous research revealed range parameters impact performance created synthetic workloads varying parameters amount data written sequential versus random accesses interval calls fsync amount concurrency focus exclusively write-based workloads reads directed fixed-place location impact journal analyze file system report results workloads revealed file system policies parameters overhead sba processing memory overheads sba minimal workloads ran generate high rates request sba driver performs operations collect detailed traces gettimeofday call start end block number comparison block journal fixed-location block check magic number journal blocks distinguish journal metadata journal data sba stores trace records details read write block number block type time issue completion internal circular buffer operations performed detailed traces analyses sufficient cumulative statistics total number journal writes fixedlocation writes numbers easy collect require processing sba driver alternative approaches directly instrumenting file system obtain timing information disk traces equivalent superior performing sba analysis case reasons directly instrument file system source code file system re-instrument versions released contrast sba analysis require file system source sba driver code reused file systems versions directly instrumenting file system accidentally miss conditions disk blocks written sba driver guaranteed disk traffic finally instrumenting existing code accidentally change behavior code efficient sba driver impact file system behavior semantic trace playback section describe semantic trace playback stp stp rapidly evaluate kinds file system designs heavy implementation investment detailed file system simulator describe stp functions stp built user-level process takes input trace parses issues requests disk raw disk interface multiple threads employed concurrency ideally stp function taking blocklevel trace input generated sba driver sufficient types file system modifications straightforward model layout schemes simply mapping blocks on-disk locations desire enable powerful emulations stp issue explore effect byte differences journal storing entire blocks complication arises changing contents journal proceedings usenix annual technical conference april anaheim timing block thresholds initiate triggered time handle emulations alter timing disk information needed readily low-level block trace specifically stp observe high-level activities stp observe file-system level operations create dirty buffers memory reason requirement found number uncommitted buffers reaches threshold ext journal size commit enacted similarly interval timers expires blocks flushed disk stp observe application-level calls fsync stp understand operation sba trace due fsync call due normal file system behavior thresholds crossed timers differentiation stp emulate behaviors timing sensitive requirements met giving filesystem level trace input stp addition sbagenerated block-level trace library-level interpositioning trace application interest qualitatively compare stp standard approaches file system evolution approach idea improving file system simply implements idea file system measures performance real system approach attractive reliable answer idea real improvement assuming workload applied relevant time consuming modification file system non-trivial approach builds accurate simulation file system evaluates idea domain file system migrating real system approach attractive avoid details building real implementation quickly understand idea good requires detailed accurate simulator construction maintenance challenging endeavor stp avoids difficulties approaches low-level traces truth file system behaves modifying file system output block stream based simple internal models file system behavior models based empirical analysis found advantages traditional implementation simulation stp limited important ways stp suited evaluating design alternatives simpler benchmarks workload exhibits complex virtual memory behavior interactions file system modeled results meaningful stp limited evaluating file system radical basic operation file system remain intact finally stp provide means evaluate implement change understand modification improves performance environment measurements machine running linux mhz pentium iii processor main memory file system test created single ibm lzx disk separate root disk data point reports average trials cases variance low ext file system section analyze popular linux filesystem ext begin giving overview ext apply semantic block-level analysis semantic trace playback understand internal behavior background linux ext journaling file system built extension ext file system ext data metadata eventually standard ext structures fixed-location structures organization loosely based ffs disk split number block groups block group bitmaps inode blocks data blocks ext journal log commonly stored file file system stored separate device partition figure depicts ext on-disk layout information pending file system updates written journal forcing journal updates disk updating complex file system structures writeahead logging technique enables efficient crash recovery simple scan journal redo incomplete committed operations bring file system consistent state normal operation journal treated circular buffer information propagated fixed location ext structures journal space reclaimed journaling modes linux ext includes flavors journaling writeback mode ordered mode data journaling mode figure illustrates differences modes choice mode made mount time changed remount writeback mode file system metadata journaled data blocks written directly fixed location mode enforce ordering journal fixed-location data writes writeback mode weakest consistency semantics modes guarantees consistent file system metadata provide guarantee consistency data blocks ordered journaling mode metadata writes journaled data writes fixed location ordered journal writes metadata proceedings usenix annual technical conference april anaheim jcib inode groupscylinder group journal descriptor block inode bitmap journal commit blockdb data bitmap journal superblock figure ext on-disk layout picture shows layout ext file system disk address space broken series block groups akin ffs cylinder groups bitmaps track allocations regions inodes data blocks ext journal depicted file block group file system superblock descriptor blocks describe contents commit blocks denote ends transactions journal commit journal inode sync fixed data fixed data fixed inode fixed data journal inode sync journal commit fixed data sync fixed inode sync writeback mode data write happen time writeback ordered journal inode data fixed inode data journal commit data journal write journal commit checkpoint write figure ext journaling modes diagram depicts journaling modes ext writeback ordered data diagram time flows downward boxes represent updates file system journal inode implies write inode journal destination writes labeled fixed write fixed in-place ext structures 
arrow labeled sync implies blocks written succession synchronously guaranteeing completes curved arrow ordering succession write happen time finally writeback mode dashed box fixed data block happen time sequence data block write inode updates propagated file system diagrams show data flow ext journaling modes contrast writeback mode mode consistency semantics data metadata guaranteed consistent recovery full data journaling mode ext logs metadata data journal decision implies process writes data block typically written disk journal fixed ext location data journaling mode strong consistency guarantees ordered journaling mode performance characteristics cases worse surprisingly cases explore topic transactions file system update separate transaction ext groups updates single compound transaction periodically committed disk approach simple implement compound transactions performance fine-grained transactions structure frequently updated short period time free space bitmap inode file constantly extended journal structure ext additional metadata structures track list journaled blocks journal superblock tracks summary information journal block size head tail pointers journal descriptor block marks beginning transaction describes subsequent journaled blocks including final fixed on-disk location data journaling mode descriptor block data metadata blocks ordered writeback mode descriptor block metadata blocks modes ext logs full blocks opposed differences versions single bit change bitmap results entire bitmap block logged depending size transaction multiple descriptor blocks data metadata blocks logged finally journal commit block written journal end implementation log-structured file system acm transactions computer systems february russel sandberg david goldberg steve kleiman dan walsh bob lyon design implemention sun network filesystem proceedings summer usenix conference pages june robert shillner edward transaction commit block written journaled data recovered loss checkpointing process writing journaled metadata data fixed-locations checkpointing checkpointing triggered thresholds crossed file system buffer space low free space left journal timer expires crash recovery crash recovery straightforward ext journaling file systems basic form redo logging updates data metadata written log process restoring in-place file system structures easy recovery file system scans log committed complete transactions incomplete transactions discarded update completed transaction simply replayed fixed-place ext structures analysis ext sba perform detailed analysis ext sba framework analysis divided categories analyze basic behavior ext function workload journaling modes isolate factors control data committed journal isolate factors control data checkpointed fixed-place location proceedings usenix annual technical conference april anaheim bandwidth amount data written bandwidth data ordered writeback ext journal data amount data written amount journal writes data ordered writeback ext fixed-location data amount data written amount fixed-location writes data ordered writeback ext figure basic behavior sequential workloads ext graph evaluate ext ext journaling modes increase size written file x-axis workload writes single file sequentially performs fsync graph examines metric top graph shows achieved bandwidth middle graph sba report amount journal traffic bottom graph sba report amount fixed-location traffic journal size set basic behavior modes workload begin analyzing basic behavior ext function workload journaling mode writeback ordered full data journaling goal understand workload conditions trigger ext write data metadata journal fixed locations explored range workloads varying amount data written sequentiality writes synchronization interval writes number concurrent writers sequential random workloads begin showing results basic workloads workload writes single file sequentially performs fsync flush data disk figure workload issues writes random locations single file calls fsync writes figure workload issues random writes calls fsync write figure workload increase total amount data bandwidth amount data written random write bandwidth data ordered writeback ext journal data amount data written amount journal writes data ordered writeback ext fixed-location data amount data written amount fixed-location writes data ordered writeback ext figure basic behavior random workloads ext figure similar figure workload issues writes random locations single file calls fsynconce writes top graph shows bandwidth middle graph shows journal traffic bottom graph reports fixed-location traffic journal size set writes observe behavior ext top graphs figures plot achieved bandwidth workloads graph compare journaling modes ext bandwidth graphs make observations achieved bandwidth extremely sensitive workload expected sequential workload achieves higher throughput random workload calling fsync frequently reduces throughput random workloads sequential traffic ext performs slightly highest performing ext mode small noticeable cost journaling sequential streams workloads ordered mode writeback mode achieve bandwidths similar ext finally performance data journaling irregular varying sawtooth pattern amount data written graphs file system throughput compare performance workloads journaling modes enable infer differences infer internal behavior file system apply semantic analysis underlying block stream proceedings usenix annual technical conference april anaheim bandwidth amount data written random write bandwidth data ordered writeback ext journal data amount data written amount journal writes data ordered writeback ext fixed-location data amount data written amount fixed-location writes data ordered writeback ext figure basic behavior random workloads ext figure similar figure workload issues random writes calls fsync write bandwidth shown graph journal writes fixed-location writes reported graph sba journal size set record amount journal fixedlocation traffic accounting shown bottom graphs figures row graphs figures quantify amount traffic flushed journal infer events traffic data journaling mode total amount data written journal high proportional amount data written application expected data metadata journaled modes metadata journaled amount traffic journal small row figures shows traffic fixed location writeback ordered mode amount traffic written fixed location equal amount data written application data journaling mode observe stair-stepped pattern amount data written fixed location file size process called fsync force data disk data written fixed location time application terminates data logged expected consistency semantics preserved consistency application writes data checkpointing occur regular intervals extra traffic leads sawtooth bandwidth measured graph experiment sequential traffic journal size checkpoint occurs data written explore relationship checkpoints journal size carefully sba graphs reveal data journaling mode performs modes asynchronous random writes data journaling mode data written log random writes logically sequential achieve sequential bandwidth journal filled checkpointing extra disk traffic reduces bandwidth experiment checkpointing occurs finally sba analysis reveals synchronous writes perform data journaling mode forcing small write log logical sequence incurs delay sequential writes shown write incurs disk rotation concurrency report results running workloads multiple processes construct workload diverse classes traffic asynchronous foreground process competition background process foreground process writes file calling fsync background process repeatedly writes block random location optionally calls fsync sleeps period time sync interval focus data journaling mode effect holds ordered journaling mode shown figure show impact varying sync interval background process performance foreground process graph plots bandwidth achieved foreground asynchronous process depending competes asynchronous synchronous background process expected 
foreground process runs asynchronous background process bandwidth uniformly high matches in-memory speeds foreground process competes synchronous background process bandwidth drops disk speeds sba analysis graph reports amount journal data revealing frequently background process calls fsync traffic journal fact amount journal traffic equal sum foreground background process traffic written interval background process effect due implementation compound transactions ext file system updates add global transaction eventually committed disk workload reveals potentially disastrous consequences grouping unrelated updates comproceedings usenix annual technical conference april anaheim bandwidth sync interval milliseconds bandwidth background process call fsync background process calling fsync journal data sync interval milliseconds amount journal writes background process call fsync background process calls fsync figure basic behavior concurrent writes ext processes compete workload foreground process writing sequential file size background process writing optionally calling fsync sleeping sync interval repeating x-axis increase sync interval top graph plot bandwidth achieved foreground process scenarios background process calling calling fsync write bottom graph amount data written disk sets experiments shown pound transaction traffic committed disk rate asynchronous traffic wait synchronous updates complete refer negative effect tangled synchrony explore benefits untangling transactions stp journal commit policy explore conditions ext commits transactions on-disk journal factors influence event size journal settings commit timers experiments focus data journaling mode mode writes metadata data journal traffic journal easily mode writeback ordered modes commit transactions policies exercise log commits examine workloads data explicitly forced disk application process call fsync minimize amount metadata overhead write single file impact journal size size journal configurable parameter ext contributes updates committed varying size journal amount data written workload infer amount data triggers log commit figure shows resulting bandwidth amount journal traffic function file size journal size graph shows amount data writbandwidth amount data written bandwidth google file system sanjay ghemawat howard gobioff shun-tak leung google abstract designed implemented google file system scalable distributed system large distributed providesfault tolerance anditdelivers high aggregate performance large number clients sharing goals previous distributed systems design driven observations application workloads technological environment thatre ectamarked departure earlier system assumptions led reexamine traditional choices explore radically erent design points system successfully met storage widely deployed google storage platform generation processing data service research development orts require large data sets largest cluster date hundreds terabytes storage thousands disks thousand machines concurrently accessed hundreds clients paper present system interface extensions designed support distributed applications discuss aspects design report measurements micro-benchmarks real world categories subject descriptors distributed systems general terms design reliability performance measurement keywords fault tolerance scalability data storage clustered storage authors reached addresses sanjay hgobio shuntak google permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee sosp october bolton landing york usa copyright acm introduction designed implemented google file system gfs tomeet therapidlygrowing demandsofgoogle data processing gfs shares goals previous distributed systems performance scalability reliability availability design hasbeen drivenbykeyobservations ofourapplication workloads technological environment current anticipated ect marked departure earlier system design assumptions reexamined traditional choices explored radically erent points design space component failures norm exception system consists hundreds thousands storage machines built inexpensive commodity parts accessed comparable number client machines quantity quality components virtually guarantee functional time recover current failures problems caused application bugs operating system bugs human errors failures disks memory connectors networking power supplies constant monitoring error detection fault tolerance automatic recovery integral system les huge traditional standards multi-gb les common typically application objects web documents regularly workingwithfast growing billions objects unwieldy manage billions approximately kb-sized les system support result design assumptions parameters operation blocksizes revisited les mutated appending data randomwriteswithin practically non-existent written les read sequentially variety data share characteristics constitute large data streams continuously generated running applications archival data intermediate results produced machine processed simultaneously time access pattern huge les appending focus performance optimization atomicity guarantees caching data blocks client loses appeal fourth co-designing applications system api bene system increasing exibility relaxed gfs consistency model vastly simplify system imposing onerous burden applications introduced atomicappendoperation concurrently toa lewithout extrasynchronization discussed details paper multiple gfs clusters deployed erent purposes largest storage nodes diskstorage heavily accessed hundreds clients distinct machines continuous basis design overview assumptions designing system guided assumptions challenges opportunities alluded key observations earlier lay assumptions details thesystem isbuilt manyinexpensivecommodity componentsthatoftenfail itmustconstantlymonitor detect tolerate recover promptly component failures routine basis system stores modest number large les expect million les typically larger size multi-gb les common case managed ciently small les supported optimize workloads primarily consist kinds reads large streaming reads small journal size journal size journal size journal size journal data amount data written amount journal writes journal size journal size figure impact journal size commit policy ext topmost figure plots bandwidth data journaling mode different-sized file writes lines plotted representing journal sizes graph shows amount log traffic generated experiments clarity journal sizes shown ten application precise number dirty uncommitted buffers includes data metadata reaches size journal bandwidth drops considerably fact performance regime observed bandwidth equal in-memory speeds semantic analysis shown graph reports amount traffic journal graph reveals metadata data forced journal equal journal size inspection linux ext code confirms threshold note threshold ordered writeback modes shown triggered frequently metadata logged impact timers linux ext timers control data written metadata commit timer data commit timer managed kupdate daemon commit timer managed kjournal daemon system-wide kupdate daemon responsible flushing dirty buffers disk kjournal daemon specialized ext responsible committing ext transactions strategy ext flush metadata frequently seconds delaying data writes longer time seconds flushing metadata frequently advantage file system approach ffs-like consistency severe performance penalty delaying data writes advantage files deleted quickly tax disk mapping ext goals ext timers leads default values seconds kupdate metadata timer seconds kjournal timer proceedings usenix annual technical conference april anaheim journal write time seconds kupdated metadata timer seconds sensitivity kupdated metadata timer journal write time seconds kupdated data timer seconds sensitivity kupdated data timer journal write time seconds kjournald timer seconds sensitivity kjournald timer figure impact timers commit policy ext graph timer varied x-axis time write journal recorded y-axis measuring impact timer set timers seconds journal size affect measurements seconds kupdate data timer measure timers affect transactions committed journal ensure specific timer influences journal commits set journal size sufficiently large set timers large analysis observe write appears journal figure plots results varying timers x-axis plotting time log write occurs y-axis graph graph show kupdate daemon metadata commit timer kjournal daemon commit timer control timing log writes data points log write occurred precisely timer expired traffic log minimum timers graph shows kupdate daemon data timer influence timing log writes data points correlated x-axis timer influences data written fixed location interaction journal fixed-location traffic timing writes journal fixedrequest queue blocks time seconds write ordering ext fixed location journal figure interaction journal fixed-location traffic ext figure plots number outstanding writes journal fixed-location disks experiment run processes issues random synchronous writes file system journal running ordered mode journal configured run separate disk location data managed carefully consistency fact difference writeback ordered mode timing writeback mode enforce ordering ordered mode ensures data written fixed location commit block transaction written journal performed sba analysis found performance deficiency ordered mode implemented workload synchronously writes large number random blocks sba driver separate journal fixed-location data figure plots number concurrent writes data type time figure shows writes journal fixed-place data overlap specifically ext issues data writes fixed location waits completion issues journal writes journal waits completion finally issues final commit block waits completion observe behavior irrespective journal separate device device file system inspection ext code confirms observation wait needed correctness cases journal configured separate device extra wait severely limit concurrency performance ext falsely limited parallelism stp fix timing problem checkpoint policy turn attention checkpointing process writing data fixed location ext structures show checkpointing ext function journal size commit timers synchronization interval workload focus data journaling mode sensitive journal size understand checkpointing occurs construct workloads periodically force data journal call fsync observe data subsequently written fixed location impact journal size figure shows sba results proceedings usenix annual technical conference april anaheim random reads large streaming reads individual operations typically read hundreds kbs commonly successive operations client read contiguous region small random read typically reads kbs arbitrary set performance-conscious applications batch sort small reads advance steadily backand workloads large sequential writes append data les typical operation sizes similar reads written les seldom modi small writes arbitrary positions supported cient systemmust ciently implement well-de ned semantics multiple clients concurrently append les fixed location data amount data written amount fixed location writes felten simplifying distributed file systems shared logical disk technical report tr- dept computer science princeton garret swart andrew birrell andy hisgen charles jerian timothy mann availability echo file system research report systems research center digital equipment corporation september randy wang tom anderson mike dahlin experience distributed file system implementation technical report california berkeley computer science division june edward wobber martin abadi michael burrows butler lampson authentication taos operating system acm transactions computer systems february 
producerconsumer queues many-way merging hundreds producers running machine concurrently append atomicity minimal synchronization overhead essential read consumer reading simultaneously high sustained bandwidth important low latency target applications place premium processing data bulkat high rate stringent response time requirements individual read write interface gfs familiar system interface filesare edbypathnames support usual operations create delete open close read andwrite les gfs snapshot record append operations snapshot creates copy directory tree low cost record append multiple clients append data concurrently guaranteeing atomicity individual client append implementing multi-way merge results producerconsumer queues clients simultaneously append additional locking found types les invaluable building large distributed applications discussedfurther sections architecture gfs cluster consists single master multiple chunkservers accessed multiple clients asshown figure typically commodity linux itiseasy torun aslong machine resources permit lower reliability caused running possibly aky application code acceptable files divided xed-size chunks chunkis identi immutable globally unique bitchunk handleassigned themaster thetime chunkcreation chunkservers store chunks local disks linux les read write chunkdata speci chunkhandle byterange forreliability eachchunkisreplicated onmultiplechunkservers bydefault westorethreereplicas users designate erent replication levels erent regions namespace master maintains system metadata includes namespace access control information mappingfrom lestochunks controls system-wide activities chunklease management garbage collection orphaned chunks chunkmigration chunkservers master periodically communicates chunkserver heartbeat messages give instructions collect state gfs client code linked application implements system api communicates master chunkservers read write data behalf application clients interact master metadata operations data-bearing communication directly chunkservers provide posix api hookinto linux vnode layer client chunkserver caches data client caches bene applications stream huge les working sets large cached simpli client system eliminating cache coherence issues clients cache metadata chunkservers cache data chunks stored local les andsolinux sbu data memory single master single master vastly simpli design enables master make sophisticated chunk placement legend data messages control messages application file chunk index chunk handle chunk locations gfs master file namespace foo bar instructions chunkserver chunkserver state gfs chunkservergfs chunkserver chunk handle byte range chunk data chunk linux file system linux file system gfs client figure gfs architecture replication decisions global knowledge minimize involvement reads writes bottleneck clients read andwrite ledatathroughthemaster aclientasks master chunkservers contact caches information limited time interacts chunkservers directly subsequent operations ence figure xed chunksize client translates byte set speci application chunkindex sends master request chunk index master replies chunk handle locations replicas client caches information chunkindex key client sends request replicas closest request speci chunk handle byte range chunk reads chunkrequire client-master interaction cached information expires reopened fact client typically asks multiple chunks request master include information chunks immediately requested extra information sidesteps future client-master interactions practically extra cost chunk size chunksize key design parameters chosen larger typical system blocksizes chunkreplica stored plain linux chunkserver extended needed lazy space allocation avoids wasting space due internal fragmentation greatest objection large chunksize large chunksize ers important advantages reduces clients interact master 
reads writes chunkrequire initial request master chunklocation information reduction signi workloads applications read write large les sequentially small random reads client comfortably cache chunklocation information multi-tb working set large chunk sync size sync size sync size free space amount data written checkpointing sync size sync size sync size journal size journal size figure impact journal size checkpoint policy ext workload amount data x-axis written sequentially fsync issued graph sba plot amount fixed-location traffic graph sba plot amount free space journal function file size synchronization interval single journal size graph shows amount data written fixed ext location end experiment point checkpointing occurs varies sync intervals sync interval data forced disk worth writes checkpoints occur approximately committed log sync interval checkpoints occur illustrate triggers checkpoint graph plot amount journal free space immediately preceding checkpoint correlating graphs checkpointing occurs amount free space -th -th journal size precise fractiondepends synchronization interval smaller sync 
amounts checkpointing postponed free space journal confirmed relationship journal sizes shown impact timers examine system timers impact timing checkpoint writes fixed locathe exact amount free space triggers checkpoint straightforward derive reasons ext reserves amount journal space overhead descriptor commit blocks ext reserves space journal committing transaction synchronization interval derived free space function precisely feel detailed information enlightening simply checkpointing occurs free space -th -th journal size write time seconds kupdated data timer seconds sensitivity kupdated data timer log writes fixed-location writes figure impact timers checkpoint policy ext figure plots relationship time data written log checkpointed dependent kupdate data timer scatter plot shows results multiple runs process running writes data fsync data journaling mode timers set seconds journal size tions workload vary kupdate data timer setting timers seconds figure shows kupdate data timer impacts data written fixed location previously figure log updated timers expire checkpoint write occurs amount kupdate data timer granularity experiments shown reveal granularity controlled kupdate metadata timer analysis reveals ext timers lead timing data metadata traffic ext ordered data journaling modes force data disk time metadata writes data metadata flushed disk frequently timing behavior largest potential performance differentiator ordered writeback modes interestingly frequent flushing potential advantage forcing data disk timely manner large disk queues avoided performance improved disadvantage early flushing temporary files written disk subsequent deletion increasing load system summary ext sba isolated number features ext strong impact performance journaling mode delivers performance depends strongly workload random workloads perform logging relationship size journal amount data written application larger impact performance ext implements compound transactions unrelated concurrent updates transaction result tangled synchrony traffic transaction committed disk rate results disastrous performance asynchronous traffic combined synchronous traffic proceedings usenix annual technical conference april anaheim bandwidth file number bandwidth ordered journaling mode default ext journal beginning modified ext journal middle stp journal middle figure improved journal placement stp compare placements journal beginning partition ext default modeled middle file system stp middle file system files created file system file chosen number x-axis workload issues synchronous writes file ordered mode ext overlap writes journal fixed-place data specifically ext issues data writes fixed location waits completion issues journal writes journal waits completion finally issues final commit block waits completion wait needed correctness journal separate device falsely limited parallelism harm performance ordered data journaling modes timer flushes meta-data disk data flushed disadvantage eager writing temporary files written disk increasing load evolving ext stp section apply stp wider range workloads traces evaluate modifications ext demonstrate accuracy stp approach begin simple modification varies placement journal sba analysis pointed number improvements ext quantify stp journaling modes depending workload separate transactions update overlapping pre-commit journal writes data updates ordered mode finally stp evaluate differential journaling block differences written journal journal location experiment stp quantifies impact changing simple client perform operations chunk reduce network overhead keeping persistent tcp connection chunkserver extended period time reduces size metadata stored master metadata memory turn brings advantages discuss section ontheotherhand alargechunksize evenwithlazyspace allocation disadvantages small consists small numberofchunks perhapsjustone thechunkservers storing chunks hot spots clients accessing practice hot spots major issue applications read large multi-chunk les sequentially hot spots develop gfs rst bya batch-queuesystem executable written gfs single-chunk started hundreds machines time chunkservers storing executable overloaded hundreds simultaneous requests xed problem storing executables higher replication factor making batchqueue system stagger application start times potential long-term solution toallow clients read data clients situations metadata master stores major types metadata chunknamespaces mapping les chunks locations chunk replicas metadata master memory rst types namespaces le-to-chunkmapping persistent logging mutations operation log stored master local diskand replicated remote machines log update master state simply reliably risking inconsistencies event master crash master store chunklocation information persistently asks chunkserver chunks master startup chunkserver joins cluster in-memory data structures masteroperationsare fast easy cient master periodically scan entire state background periodic scanning implement chunkgarbage collection re-replication presence chunkserver failures chunkmigration balance load diskspace usage chunkservers sections discuss activities potential concern memory-only approach number chunks capacity system limited memory master limitation practice master maintains bytesof metadata chunk lescontainmany chunks partially lled similarly namespace data typically requires bytes stores names compactly pre compression support larger systems cost adding extramemory tothemaster small price topay simplicity reliability performance exibility gain storing metadata memory chunk locations master persistent record chunkservers replica policy placement chunk simply polls chunkservers information startup master up-to-date controls chunkplacement monitors chunkserver status regular heartbeat messages weinitially information persistently master decided simpler request data chunkservers startup periodically eliminated problem join leave cluster change names fail restart cluster hundreds servers events happen understand design decision realize chunkserver nal word chunks disks point maintain consistent view information master errors chunkserver chunks vanish spontaneously disk bad disabled operator rename chunkserver operation log operation log historical record critical metadata central gfs persistent record metadata serves logical time line nes order concurrent operations files chunks versions section uniquely eternally identi logical times created operation log critical store reliably make visible clients metadata made persistent ectively lose system recent client operations chunks survive replicate multiple remote machines respond client operation ushing log record disk locally remotely master batches log recordstogetherbefore ushing replication system throughput master recovers system state replaying operation log minimize startup time log small themaster checkpointsitsstate wheneverthelog grows size recover loading latest checkpoint local disk replaying write record append serial defined defined success interspersed concurrent consistent inconsistent successes undefined failure inconsistent table file region state mutation limited number log records checkpoint compact b-tree form directly mapped memory namespace lookup extra parsing speeds recovery improves availability building checkpoint master internal state structured checkpoint created delaying incoming mutations master switches log creates checkpoint separate thread checkpoint includes mutations switch created minute cluster million les completed written diskboth locally remotely recovery latest complete checkpoint subsequent log les older checkpoints log les freely deleted guard catastrophes failure checkpointing ect correctness recovery code detects skips incomplete checkpoints consistency model gfs relaxed consistency model supports highly distributed applications remains simple cient implement discuss gfs guarantees applications highlight gfs maintains guarantees butleave details parts paper guarantees gfs file namespace mutations creation atomic handled exclusively master namespace locking guarantees atomicity correctness section master operation log nes global total order operations section state region data mutation depends type mutation succeeds fails concurrent mutations table summarizes result region consistent clients data replicas read region ned data mutation consistent clients mutation writes entirety mutation succeeds interference concurrent writers ected region ned implication consistent clients mutation written concurrent successful mutations leave region unde ned consistent clients data ect mutation haswritten typically consistsof mingledfragments multiple mutations failed mutation makes region inconsistent unde ned erent clients erent data erent times describe applications distinguish ned regions unde ned regions applications distinguish erent kinds unde ned regions data mutations writes record appends awrite data written application-speci set record append data record appended atomically presence concurrent mutations set gfs choosing section contrast regular append write set client believes current end set returned client marks beginning ned region record addition gfs insert padding record duplicates occupy regions considered inconsistent typically dwarfed amount user data sequence successful mutations mutated region isguaranteedtobede nedandcontain thedatawritten mutation gfs achieves applying mutations chunkin order replicas section replica stale missed mutations chunkserver section stale replicas involved mutation clients master chunk locations garbage collected earliest opportunity clients cache chunklocations read stale replica information refreshed window limited cache entry timeout open purges cache chunkinformation les append-only stale replica returns premature end chunkrather outdated data reader retries contacts master immediately current chunklocations long successful mutation component failures corrupt destroy data gfs identi failed chunkservers regular handshakes master chunkservers detects data corruption checksumming section problem surfaces data restored section achunk lost irreversibly replicas lost gfs react typically minutes case unavailable corrupted applications receive clear errors corrupt data implications applications gfs applications accommodate relaxed consistencymodelwith afew simpletechniquesalready neededfor purposes relying appends overwrites checkpointing writing self-validating self-identifying records practically applications mutate les appending overwriting typical writer generates beginning end atomically renames permanent writing data periodically checkpoints successfully written checkpoints include application-level checksums readers verify process region checkpoint ned state ofconsistency concurrencyissues approach served appending cient resilient application failures random writes checkpointing writers restart incrementally readers processing successfully written data incomplete application perspective typical writers concurrently append merged results producer-consumer queue recordappend serves writer output readers deal occasional padding duplicates record prepared writer extra information checksums validity veri reader identify discard extra padding record fragments checksums tolerate occasional duplicates trigger non-idempotent operations lter unique identi ers records needed application entities webdocuments functionalities record duplicate removal library code shared applications applicable interface implementations google sequence records rare duplicates delivered record reader system interactions designed system minimize master involvement operations background describe client master chunkservers interact implementdatamutations atomicrecordappend andsnapshot leases mutation order 
mutation operation contents metadata chunksuch write append operation mutation performed chunk sreplicas weuseleases tomaintain aconsistent mutation orderacross replicas themaster grantsachunklease tooneof thereplicas call primary primary picks serial order mutations chunk replicas follow order applying mutations global mutation order ned rst lease grant order chosen master lease serial numbers assigned primary lease mechanism designed minimize management overhead master lease initial timeout seconds long chunkis mutated primary request typically receive extensions master inde nitely extension requests grants piggybacked heartbeat messages regularly exchanged master chunkservers master revoke lease expires master disable mutations renamed master loses communication primary safely grant lease replica lease expires figure illustrate process control write numbered steps client asks master chunkserver holds current lease chunkand locations replicas lease master grants replica chooses shown locations secondary replicas client caches data future mutations contact master primary primary replica secondary replica secondary replica master legend control data client step journal default ext creates journal regular file beginning partition start policy validate stp results obtain stp similar implement change ext construct workload stresses placement journal partition filled files benchmark process issues random synchronous bandwidth sync interval milliseconds bandwidth untangled standard figure untangling transaction groups stp experiment identical figure addition show performance foreground process untangled transactions emulated stp writes chosen file figure vary file chosen x-axis line graph shows performance ordered mode default ext bandwidth drops file located journal sba analysis shown confirms performance drop occurs seek distance increases writes file journal evaluate benefit placing journal middle disk stp remap blocks validation coerce ext allocate journal middle disk compare results figure shows stp predicted performance identical version ext worst-case behavior avoided placing journal middle file system beginning longest seeks entire volume avoided synchronous workloads workloads frequently seek journal ext structures journaling mode shown workloads perform journaling modes random writes perform data journaling mode random writes written sequentially journal large sequential writes perform ordered mode avoids extra traffic generated data journaling mode journaling mode ext set mount time remains fixed mount stp evaluate adaptive journaling mode chooses journaling mode transaction writes transaction transaction sequential ordered journaling data journaling demonstrate potential performance benefits adaptive journaling run portion trace labs removing inter-arrival times calls compare ordered mode data journaling mode adaptive approach trace completes seconds seconds ordered data journaling modes stp adaptive journaling trace completes seconds trace sequential random write proceedings usenix annual technical conference april anaheim request queue blocks time seconds modified write ordering fixed location journal figure changing interaction journal fixedlocation traffic stp experiment run figure run stp issue precommit journal writes data writes concurrently plot stp emulated performance made change ext directly obtaining resultant performance phases adaptive journaling performs single-mode approach transaction grouping linux ext groups updates system-wide compound transactions commits disk periodically shown single update stream synchronous dramatic impact performance asynchronous streams transforming in-memory updates disk-bound stp show performance file system untangles traffic streams forcing process issues fsync commit data disk figure plots performance asynchronous sequential stream presence random synchronous stream vary interval updates synchronous process graph segregated transaction grouping effective asynchronous stream unaffected synchronous traffic timing show stp quantify cost falsely limited parallelism discovered pre-commit journal writes overlapped data updates ordered mode stp modify timing journal fixed-location writes initiated simultaneously commit transaction written previous writes complete workload processes issuing random synchronous writes journal separate disk figure shows stp model implementation change modifying timing requests workload stp predicts improvement prediction matches achieve ext changed directly expected increasing amount concurrency improves performance journal separate device journal contents ext physical logging writes blocks entirety log blocks journaled irrespective bytes changed block journal space fills quickly increasing commit checkpoint frequency stp investigate differential journaling file system writes block differences journal blocks entirety approach potentially reduce disk traffic noticeably dirty blocks substantially previous versions focus data journaling mode generates journal traffic differential journaling modes evaluate differential journaling matters real workloads analyze sba traces underneath database workloads modeled tpc-b tpcc simple application-level implementation debit-credit benchmark realistic implementation order-entry built top postgres data journaling mode amount data written journal reduced factor tpc-b factor tpc-c contrast ordered writeback modes difference minimal modes metadata written log applying differential journaling metadata blocks makes difference total volume reiserfs focus linux journaling 
filesystem reiserfs section focus chief differences ext reiserfs due time constraints stp explore reiserfs background general behavior reiserfs similar ext file systems journaling modes compound transactions reiserfs differs ext primary ways file systems on-disk structures track fixed-location data ext structures ext improved scalability reiserfs tree data stored leaves tree metadata stored internal nodes impact fixed-location data structures focus paper difference largely irrelevant format journal slightly ext journal file partition contiguous reiserfs journal file contiguous sequence blocks beginning file system ext reiserfs journal put device reiserfs limits journal maximum ext reiserfs differ slightly journal contents reiserfs fixed locations blocks transaction stored descriptor block commit block unlike ext reiserfs descriptor block compound proceedings usenix annual technical conference april anaheim bandwidth amount data written bandwidth data ordered writeback journal data amount data written amount journal writes data ordered writeback fixed-location data amount data written amount fixed-location writes data ordered writeback figure basic behavior sequential workloads reiserfs graph evaluate reiserfs journaling modes single workload size sequentially written file increased x-axis graph examines metric hows achieved bandwidth sba report amount journal traffic sba report amount fixed-location traffic journal size set transaction limits number blocks grouped transaction semantic analysis reiserfs performed identical experiments reiserfs ext due space constraints present results reveal significantly behavior file systems basic behavior modes workload qualitatively performance journaling modes reiserfs similar ext random workloads infrequent synchronization perform data journaling sequential workloads generally perform random writeback ordered modes generally perform data journaling reiserfs groups concurrent transactions single compound transaction ext primary difference file systems occurs sequential workloads data journaling shown graph figure fixed-location data amount data written amount fixed-location writes sync size sync size sync size sync size fixed location data number transactions amount fixed-location writes sync size sync size sync size figure impact journal size transactions checkpoint policy reiserfs workloads data sequentially written anfsyncis issued amount data sba report amount fixedlocation traffic graph vary amount data written graph vary number transactions defined number calls fsync throughput data journaling mode reiserfs follow sawtooth pattern initial reason found sba analysis graphs figure data written journal checkpointed inplace location reiserfs appears checkpoint data aggressively ext explore journal commit policy explore factors impact reiserfs commits transactions log focus data journaling sensitive postpone exploring impact timers previously ext commits data log approximately log filled timer expires running workload force data disk call fsync reiserfs performing sba analysis find reiserfs threshold depending journal size reiserfs commits data blocks blocks written reiserfs limits journal size fixed thresholds sufficient finally note reiserfs falsely limited parallelism ordered mode ext reiserfs forces data flushed fixed location issues writes journal proceedings usenix annual technical conference april anaheim write time seconds kreiserfsd timer seconds sensitivity kreiserfsd journal timer log writes fixed-location writes figure impact timers reiserfs figure plots relationship time data written kreiserfs timer scatter plot shows results multiple runs process running writes data fsync data journaling mode timers set seconds journal size checkpoint policy investigate conditions trigger reiserfs checkpoint data fixed-place location policy complex reiserfs ext found data checkpointed journal full reiserfs point data checkpointed depends free space journal number concurrent transactions workloads periodically force data journal calling fsync intervals results shown figure graph shows amount data checkpointed function amount data written cases data checkpointed journal filled graph shows amount data checkpointed function number transactions graph shows data checkpointed intervals transactions running similar workload ext reveals relationship number transactions checkpointing reiserfs checkpoints data journal free space drops transactions journal ext timers control data written journal fixed locations differences ext kjournal daemon responsible committing transactions reiserfs kreiserfs daemon role figure shows time data written journal fixed location kreiserfs timer increased make conclusions log writes occur seconds data write application timer fixed-location writes occur elapsed time greater seconds multiple kreiserfs timer reiserfs timer policy simpler ext finding bugs sba analysis inferring policies filesystems finding cases implemented correctly sba analysis found number problems reiserfs implementation reported case identified problem sba driver observe disk traffic expected verify problems examined code find suggested fixes reiserfs developers transaction mount fsync call returns data written tracked aberrant behavior incorrect initialization file block overwritten writeback mode stat information updated error occurs due failure update inode transaction information committing transactions dirty data flushed tracked erroneously applying condition prevent data flushing journal replay irrespective changing journal thread wake interval dirty data flushed problem occurs due simple coding error ibm journaled file system section describe experience performing preliminary sba analysis journaled file system jfs began rudimentary understanding jfs obtain documentation knew journal located default end partition treated contiguous sequence blocks journaling mode due fact knew file system began found needed apply analysis technique cases filtered traffic rebooted system infer filtered traffic consistency technique understand journaling mode jfs basic starting point examining jfs code learn number interesting properties jfs inferred jfs ordered journaling mode due small amount traffic journal obvious employing data journaling differentiate writeback ordered modes observed ordering writes matched ordered mode data block written application jfs orders write data block written successfully metadata writes issued determined jfs logging record level inode index tree directory tree structure structure logged entire block structure result jfs writes fewer journal blocks ext reiserfs operations jfs default group concurrent updates single compound transaction running experiment performed figure proceedings usenix annual technical conference april anaheim bandwidth asynchronous traffic high irrespective synchronous traffic background circumstances transactions grouped write commit records log page finally commit timers jfs fixedlocation writes happen kupdate daemon timer expires journal writes triggered timer journal writes indefinitely postponed trigger memory pressure unmount operation infinite write delay limits reliability crash result data loss data written minutes hours windows ntfs section explain analysis ntfs ntfs journaling file system default file system windows operating systems source code documentation ntfs publicly tools finding ntfs file layout exist ran windows operating system top vmware linux machine pseudo device driver exported scsi disk windows ntfs file system constructed top pseudo device ran simple workloads ntfs observed traffic sba driver analysis object ntfs file metadata stored terms files journal file located center file system ntfsprogs tools discover journal file boundaries journal boundaries distinguish journal 
traffic fixed-location traffic analysis found ntfs data journaling easily verified amount data traffic observed sba driver found ntfs similar jfs block-level journaling journals metadata terms records verified blocks journaled ntfs matching contents fixed-location traffic contents journal traffic inferred ntfs performs ordered journaling data writes ntfs waits data block writes fixed-location complete writing metadata blocks journal confirmed ordering sba driver delay data block writes upto seconds found metadata writes journal delayed amount related work journaling studies journaling file systems studied detail notably seltzer compare variants journaling ffs soft updates technique managing metadata consistency file systems authors present direct observation low-level traffic familiar systems implementors explain behavior make semantic inferences explain journaling performance drops delete benchmark authors report file system forced read indirect block order reclaim disk blocks section tool sba makes expert observations readily recent study compares range linux file systems including ext ext reiserfs xfs jfs work evaluates file systems fastest benchmarks explanation workload file system benchmarks popular file system benchmarks iozone bonnie lmbench modified andrew benchmark postmark iozone bonnie lmbench perform synthetic read write tests determine throughput andrew postmark intended model realistic application workloads uniformly measure throughput runtime draw high-level conclusions file system contrast sba intended yield low-level insights internal policies file system related work chen patterson self-scaling benchmark work benchmarking framework conducts search space workload parameters sequentiality request size total workload size concurrency hones interesting parts workload space interestingly conclusions file system behavior drawn resultant figure write control data flow unreachable replies longer holds alease client pushes data replicas client order chunkserver store data internal lru cache data aged decoupling data control improve performance scheduling expensive data based networktopology chunkserver primary section discusses replicas acknowledged receiving data client sends write request primary request identi data pushed earlier replicas primary assigns consecutive serial numbers mutations receives possibly multiple clients serialization applies mutation local state serial number order primary forwards write request secondary replicas secondary replica applies mutations serial number order assigned primary secondaries reply primary indicating completed operation primary replies client errors encountered replicas reported client case errors write succeeded cas failed primary assigned serial number forwarded client request considered failed modi region left inconsistent state client code handles errors retrying failed mutation make attempts steps falling backto retry beginning write write application large straddles chunk boundary gfs client code breaks multiple write operations follow control interleaved overwritten concurrent operations clients shared region end fragments erent clients althoughthereplicaswill beidenticalbecausetheindividual operations completed successfully order replicas leaves region consistent unde ned state noted section data flow decouple data control networke ciently control ows client primary secondaries data pipelined fashion goals fully utilize machine networkbandwidth avoid networkbottlenecks high-latency links minimize latency push data fully utilize machine networkbandwidth data pushed linearly chain chunkservers distributed topology tree machine full outbound bandwidth transfer data fast divided multiple recipients avoid network bottlenecks high-latency links inter-switch links machine forwards data closest machine networktopology received suppose client pushing data chunkservers sends data closest chunkserver forwards closest chunkserver closest similarly forwards whichever closer networktopology simple distances accurately estimated addresses finally minimize latency pipelining data transferovertcpconnections data starts forwarding immediately pipelining helpful switched networkwith full-duplex links sending data immediately reduce receive rate networkcongestion ideal elapsed time transferring bytes replicas networkthroughput latency transfer bytes machines network links typically mbps ideally distributed atomic record appends gfs atomic append operation called record append traditional write client speci set data written concurrent writes region serializable region end data fragments multiple clients record append client speci data gfs appends atomically continuous sequence bytes set gfs choosing returns set client similar writing opened append mode unix race conditions multiple writers concurrently record append heavily distributed applications clients erent machines append concurrently clients additional complicated expensive synchronization distributed lockmanager traditional writes workloads les serve multiple-producer single-consumer queues merged results erent clients record append kind mutation control section extra logic primary client pushes data replicas chunkof sends request primary primary checks appending record current chunkwould chunkto exceed maximum size pads chunkto maximum size tells secondaries replies client indicating operation retried chunk record append restricted one-fourth maximum chunksize worstcase fragmentation acceptable level record maximum size common case primary appends data replica tells secondaries write data exact set nally replies success client theclientretriesthe operation result replicas chunkmay erent data possibly including duplicates record part gfs guarantee replicas bytewise identical guarantees data written atomic unit property readily simple observation operation toreportsuccess set replicas chunk replicas long end record future record assigned higher set erent chunkeven erent replica laterbecomestheprimary intermsofourconsistency guarantees output size file cache approach automated construct benchmarks exercise file system behaviors controlled manner file system tracing previous studies traced file system activity zhou ousterhout baker roselli record file system operations deduce file-level access patterns vogels performs similar study inside file system driver framework information mapped missed studies recent tracing infrastructure tracefs traces file systems vfs layer tracefs enable low-level tracing sba finally blaze ellard show low-level packet tracing nfs environment recording network-level protocol activity network file system behavior carefully analyzed type packet analysis analogous sba positioned low level reconstruct higher-level behaviors obtain complete view proceedings usenix annual technical conference april anaheim conclusions systems grow complexity techniques approaches enable users system architects understand detail systems operate presented semantic block-level analysis sba methodology file system benchmarking block-level tracing provide insight internal behavior file system block stream annotated semantic information block belongs journal data structure excellent source information paper focused behavior journaling file systems understood sba case sba straightforward user journal allocated disk sba analyzed detail linux journaling file systems ext reiserfs performed preliminary analysis linux jfs windows ntfs cases uncovered behaviors difficult discover conventional approaches developed presented semantic trace playback stp enables rapid evaluation ideas file systems stp demonstrated potential benefits numerous modifications current ext implementation real workloads traces modifications transaction grouping mechanism ext reevaluated untangled approach enables asynchronous processes obtain in-memory bandwidth presence synchronous streams system acknowledgments theodore jiri schindler members adsl research group insightful comments mustafa uysal excellent shepherding anonymous reviewers thoughtful suggestions work sponsored nsf ccrccr- ccrngs- itritr- ibm emc aranya wright zadok tracefs file system trace fast san francisco april baker hartman kupfer shirriff ousterhout measurements distributed file system sosp pages pacific grove october jfs log journaled file system performs logging proceedings annual linux showcase conference pages atlanta jfs overview ibm developerworks library l-jfs html blaze nfs tracing passive network monitoring usenix winter pages san francisco january bray bonnie file system benchmark http textuality bonnie bryant forester hawkes filesystem performance scalability linux freenix monterey june chen patterson approach performance evaluation self-scaling benchmarks predicted performance sigmetrics pages santa clara chutani anderson kazar leverett mason sidebotham episode file system usenix winter pages san francisco january ellard seltzer nfs tracing tools techniques system analysis lisa pages san diego california october ganger patt metadata update performance file systems osdi pages monterey november gray reuter transaction processing concepts techniques morgan kaufmann hagmann reimplementing cedar file system logging group commit sosp austin texas november katcher postmark file system benchmark technical report trnetwork appliance october mckusick joy leffler fabry fast file system unix acm transactions computer systems august mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april mcvoy staelin lmbench portable tools performance analysis usenix san diego january mogul update policy usenix summer boston june norcutt iozone filesystem benchmark http iozone ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference anaheim june ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system sosp pages orcas island december reiser reiserfs namesys riedel kallahalla swaminathan framework evaluating storage system security fast pages monterey january roselli lorch anderson comparison file system workloads usenix pages san diego california june rosenblum ousterhout design implementation logstructured file system acm transactions computer systems february seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems usenix pages san diego california june solomon inside windows microsoft programming series microsoft press sourceforge linux ntfs project http linux-ntfs net sweeney doucette anderson nishimoto peck scalability xfs file system usenix san diego january transaction processing council tpc benchmark standard specification revision technical report transaction processing council tpc benchmark standard specification revision technical report tweedie future directions ext filesystem freenix monterey june tweedie journaling linux ext file system fourth annual linux expo durham north carolina tweedie ext journaling file system olstrans sourceforge net release ols -ext ols -ext html july vogels file system usage windows sosp pages kiawah island resort december yang twohey engler musuvathi model checking find file system errors osdi san francisco december zhou costa smith file system tracing package berkeley unix usenix summer pages salt lake city june 
regions successful record append operations written data ned consistent intervening regions inconsistent undened applications deal inconsistent regions discussed section snapshot snapshot operation makes copy directory tree source instantaneously minimizing interruptions ongoing mutations users quickly create branch copies huge data sets copies copies recursively checkpoint current state experimenting committed rolled backeasily likeafs implement snapshots master receives snapshot request onthechunks les snapshot ensures subsequentwrites tothesechunkswill requireaninteraction master lease holder give master opportunity create copy chunk rst leases revoked expired master logs operation disk applies log record in-memory state duplicating metadata thesource directory tree newly created snapshot les point chunks source les rst time client write chunkc snapshot operation sends request master current lease holder master notices count chunkc greater defers replying client request picks chunk handle asks chunkserver current replica create chunkcalled creating chunkon chunkservers original ensure data copied locally network disks times fast ethernet links point request handling chunk master grants replicas aleaseon thenewchunkc andrepliestotheclient canwrite thechunknormally notknowing thatithas created existing chunk master operation master executes namespace operations addition manages chunkreplicas system makes placement decisions creates chunks replicas coordinates system-wide activities chunks fully replicated balance load chunkservers reclaim unused storage discuss topics namespace management locking master operations long time wedonotwanttodelay master operations running multiple operations active locks regions namespace ensure proper serialization unlike traditional systems gfs per-directory data structure lists les directory support aliases directory hard symbolic links unix terms gfs logically full pathnames metadata pre compression table ciently represented memory node namespace tree absolute absolute directory read-write lock master operation acquires set locks runs typically involves leaf acquire read-locks directory names read lockor write lockon fullpathname leaf notethatleaf directory depending operation home user foo created home user snapshotted save user snapshot operationacquiresreadlockson home save andwrite locks home user save user creation acquires read locks home home user andawrite lockon home user foo operations serialized properly obtain con icting locks home user file creation require write lock parent directory directory inode-like data structureto beprotected modi cation read lockon cient protect parent directory deletion nice property locking scheme concurrent mutations directory multiple creations executed concurrently directory acquires read lockon directory write lockon read lockon 
directory ces prevent directory deleted renamed snapshotted write locks names serialize attempts create read-writelock objects allocated lazily deleted locks acquired consistent total order prevent deadlock rst ordered level namespace tree lexicographically level replica placement gfs cluster highly distributed levels typically hundreds chunkserversspread machine racks chunkservers turn accessed hundreds clients erent racks communication machines erent racks cross network switches additionally bandwidth rackmay aggregate bandwidth machines rack multi-level distribution presents unique challenge distribute data scalability reliability availability chunkreplica placement policy serves purposes andmaximizenetworkbandwidth utilization spread replicas machines guards snetworkbandwidth spread chunkreplicas racks ensures replicas chunk survive remain entire rackis damaged ine due failure shared resource network switch power circuit means tra reads chunkcan exploit aggregate bandwidth multiple racks hand write tra chasto owthroughmultipleracks atradeo wemake willingly creation re-replication rebalancing chunkreplicas created reasons chunkcreation re-replication rebalancing master creates chunk chooses place initially empty replicas considers factors below-average diskspace utilization time equalizediskutilization acrosschunkservers wewant limit number recent creations chunkserver creation cheap reliably predicts imminentheavywritetra mandedbywrites load typically practically read-only completely written discussed spread replicas chunkacross racks master re-replicates chunkas number replicas falls user-speci goal happen reasons chunkserver unavailable itreports thatitsreplica becorrupted disks disabled errors replication goal increased chunkthat re-replicated prioritized based factors replication goal give higher prioritytoachunkthat haslost tworeplicas thantoachunkthat lost addition prefer rst re-replicate chunks live les opposed chunks belong recently deleted les section finally minimize impact failures running applications boost priority chunkthat blocking client progress master picks highest priority chunk clones instructing chunkserver copy chunk data directly existing valid replica replica goals similar creation equalizing diskspace utilization limiting active clone operations single chunkserver spreading replicas racks cloning tra overwhelming client tra master limits numbers active clone operations cluster chunkserver additionally chunkserver limits amount bandwidth spends clone operation throttling read requests source chunkserver finally master rebalances replicas periodically examines current replica distribution moves replicas diskspace load balancing process master gradually lls chunkserver instantly swamps chunks heavy write tra placement criteria replica similar discussed addition master choose existing replica remove general prefers remove chunkservers below-average free space equalize diskspace usage garbage collection deleted gfs immediately reclaim physical storage lazily regular garbage collection chunklevels approach makes system simpler reliable mechanism deleted application master logs deletion immediately reclaiming resources immediately renamed hidden includes deletion timestamp master regular scan system namespace removes hidden les existed days interval con gurable read special undeleted renaming backto normal hidden removed namespace inmemory metadata erased ectively severs links chunks similar regular scan chunknamespace master identi orphaned chunks reachable erases metadata chunks heartbeat message regularly exchanged master chunkserver reports subset chunks master replies identity chunksthat longer present master metadata chunkserver free delete replicas chunks discussion distributed garbage collection hard problem demands complicated solutions context programming languages simple case easily identify chunks leto-chunkmappings maintained exclusively master easily identify chunkreplicas linux oneachchunkserver replica master garbage garbage collection approach storage reclamation ers advantages eager deletion simple reliable large-scale distributed system component failures common chunkcreation succeed chunkservers leaving replicas master exist replica deletion messages maybelost failures chunkserver garbage collection uniform dependable clean replicas merges storage reclamation regular background activities master regular scans namespaces handshakes chunkservers batches cost amortized master free master respond promptly client requests demand timely attention delay reclaiming storage safety net accidental irreversible deletion experience themain disadvantage thedelay hinders user ort tune usage storage tight applications repeatedly create delete temporary les reuse storage address issues expediting storage reclamation deleted explicitly deleted users apply erent replication reclamation policies erent parts namespace users chunks les directory tree stored replication deleted les immediately irrevocably removed system state stale replica detection chunkreplicas stale chunkserver fails misses mutations chunkwhile chunk master maintains chunk version number distinguish up-to-date stale replicas master grants lease chunk increases chunkversion number informs up-todate replicas master replicas record version number persistent state occurs client noti start writing chunk replica unavailable chunkversion number advanced master detect chunkserver stale replica chunkserver restarts reports set chunks version numbers master sees version number greater records master assumes failed granting lease takes higher version up-to-date master removes stale replicas regular garbage collection ectively considers astale replica exist replies client requests chunk information safeguard master includes chunkversion number informs clients chunkserver holds lease chunk instructs chunkserver read chunk chunkserver cloning operation client thechunkserververi version number performs operation accessing up-to-date data fault tolerance diagnosis greatest challenges designing system dealing frequent component failures quality quantity components make problems norm exception completely trust machines completely trust disks componentfailures worse corrupted data discuss meet challenges lems inevitably occur high availability hundreds servers gfs cluster bound unavailable time system highly simple ective strategies fast recovery replication fast recovery master chunkserver designed restore state start seconds matter terminated fact distinguish normal abnormal termination servers routinely shut killing process clients servers experience minor hiccup time outstanding requests reconnect restarted server retry section reports observed startup times chunk replication discussed earlier chunkis replicated multiple chunkservers erent racks users erent replication levels erent parts namespace default master clones existing replicas needed chunk fully replicated chunkservers goo throughchecksumveri cation section replication served exploring forms cross-server redundancysuchasparityorerasurecodesforourincreasingread- storage requirements expect challenging manageable implement complicated redundancy schemes loosely coupled system tra dominated appends reads small random writes master replication master state replicated reliability operation log checkpoints replicated multiple machines mutation state considered committed log record ushed disklocally master replicas simplicity master process remains charge mutations background activities asgarbage collection thatchange thesystem internally fails restart instantly machine diskfails monitoring infrastructure gfs starts master process replicated operation log clients canonical master gfs-test dns alias changed master relocated machine shadow masters provide read-only access lesystemevenwhen theprimary master isdown shadows mirrors lag primary slightly typically fractions enhance read availability les actively mutated applications mind slightly stale results fact content read chunkservers applications observe stale content stale short windows metadata directory contents access control information tokeepitself informed growing operation log applies sequence data structures primary theprimary polls chunkserversat startup infrequently locate chunkreplicas exchanges frequent handshake messages monitor status depends primary master replica location updates resulting primary decisions create delete replicas data integrity ofstoreddata disks hundreds machines regularly experiences diskfailures data corruption loss read write paths section recover corruption 
chunkreplicas impractical detect corruption comparing replicas chunkservers divergent replicas legal semantics gfs mutations atomic record append discussed earlier guarantee identical replicas chunkserver independently verify integrity copy maintaining checksums achunkisbrokenupinto kbblocks eachhasacorresponding bit checksum metadata checksums memory stored persistently logging separate user data reads chunkserver veri checksum data blocks thatoverlap theread range returningany data requester client chunkserver chunkservers propagate corruptions machines blockdoes match recorded checksum chunkserver returns error requestor reports mismatch master response requestor read replicas master clone chunkfrom replica valid replica place master instructs chunkserverthat reported mismatch delete replica checksumming ect read performance reasons reads span blocks read checksum small amount extra data veri cation gfs client code reduces overhead align reads checksum block boundaries checksum lookups comparison chunkserver checksum calculation overlapped checksum computation heavily optimized writes append end chunk opposed writes overwrite existing data dominant workloads incrementally update checksum partial checksum block compute checksums brand checksum blocks lled append partial checksum block corrupted fail detect checksum valuewill match thestored data thecorruption detected usual blockis read contrast write overwrites existing range chunk read verify rst blocks range overwritten perform write nally compute record checksums verify rst blocks overwriting partially regions overwritten idle periods chunkservers scan verify contents inactive chunks detect corruption chunks rarely read corruption detected master create uncorrupted replica delete corrupted replica prevents inactive corrupted chunkreplica fooling master thinking valid replicas chunk diagnostic tools extensive detailed diagnostic logging helped immeasurably problem isolation debugging performance analysis incurring minimal cost logs hard understand transient non-repeatable interactions machines gfs servers generate diagnostic logs record signi events chunkservers rpc requests replies diagnostic logs freely deleted ecting correctness system logs space permits rpc logs include exact requests responses wire data read written matching requests replies collating rpc records erent machines reconstruct entire interaction history diagnose problem logs serve traces load testing performance analysis performance impact logging minimal outweighed bene logs written sequentially asynchronously recent events memory continuous online monitoring measurements trate bottlenecks inherent gfs architecture implementation numbers real clusters google micro-benchmarks measured performance gfs cluster consisting master master replicas chunkservers clients note con guration set ease testing typical clusters hundreds chunkservers hundreds clients machines con gured dual ghz piii processors gbofmemory rpmdisks mbps full-duplex ethernet connection switch gfs server machines connected switch client machines switches connected gbps link reads clients read simultaneously system client reads randomly selected region set repeated times client ends reading data chunkservers memory expect hit rate linux cache results close cold cache results figure shows aggregate read rate clients theoretical limit limit peaks aggregate gbps linkbetween switches saturated client mbps networkinterface saturated whichever applies observed read rate per-client limit client reading aggregate read ratereaches ofthe linklimit readers client ciency drops probability multiple readers simultaneously read chunkserver writes clients write simultaneously distinct les client writes data series writes aggregate write rate theoretical limit shown figure limit plateaus becauseweneedtowriteeachbyteto ofthe chunkservers input connection thewrite rate client halfof limit main culprit thisis networkstack interact pipelining scheme pushing data chunkreplicas delays propagating data replica reduce write rate aggregate write rate reaches clients client half theoretical limit thecaseofreads write concurrently chunkserver number clients increases collision writers readers write involves erent replicas writes slower practice major problem increases latencies individual clients signi cantly ect aggregate write bandwidth delivered system large number clients record appends figure shows record append performance clients append simultaneously single performance limited networkbandwidth chunkservers store chunkof independent number clients starts client drops clients due congestion variances networktransfer rates erent clients applications tend produce multiple les concurrently words clients append shared les simultaneously dozens hundreds experiment signi issue practice client make progress writing chunkservers busy real world clusters examine clusters google representative cluster regularly research development hundred engineers typical taskis initiated human user runs hours reads mbs tbs data transforms analyzes data writes results backto cluster cluster primarily production data processing tasks cluster chunkservers disk space disk space number files number dead files number chunks metadata chunkservers metadata master table characteristics gfs clusters longer setswith onlyoccasional humanintervention bothcases asingle task consistsofmanyprocesses onmanymachines reading writing les simultaneously storage asshownbythe rst veentriesinthetable bothclusters hundreds chunkservers support tbs disk space fairly completely full space includes chunkreplicas virtually les replicated times clusters store data clusters similar numbers les larger proportion dead les les deleted replaced bya version storage reclaimed chunks les tend larger metadata chunkservers aggregate store tens gbs metadata checksums blocks user data metadata chunkservers chunkversion number discussed section metadata master smaller tens mbs bytes average agrees assumption size master memory limit system capacity practice perle metadata names stored pre x-compressed form metadata includes ownership permissions mapping les chunks chunk scurrentversion inaddition chunkwe store current replica locations count implementing copy-on-write eachindividualserver metadata recovery fast takes seconds read metadata master hobbled period typically seconds fetched chunklocation information chunkservers read write rates table shows read write rates time periods clusters weekwhen measurements clusters restarted recently upgrade version gfs average write rate restart tookthese measurements data produced networkload writes propagated replicas number clients read rate network limit aggregate read rate reads number clients write rate network limit aggregate write rate writes number clients append rate network limit aggregate append rate record appends figure aggregate throughputs top curves show theoretical limits imposed networktopology bottom curves show measured throughputs error bars show con dence intervals illegible cases low variance measurements cluster read rate minute read rate hour read rate restart write rate minute write rate hour writerate sincerestart master ops minute ops ops master ops hour ops ops master ops restart ops ops table performance metrics gfs clusters read rates higher write rates total workload consists reads writes assumed clusters middle heavy read activity sustaining read rate preceding week network conguration support resources ciently cluster support peakread rates applications master load table shows rate operations master operations master easily rate bottleneckfor workloads earlier version gfs master occasionally bottleneckfor workloads spent time sequentially scanning large directories contained hundreds thousands les les changed master data structures cient binary searches namespace easily support thousands accesses speed upfurther placing lookup caches front namespace data structures recovery time chunkserverfails chunkswill underreplicated cloned restore replication levels time takes 
restore chunks depends amount resources experiment killed single chunkserver cluster chunkserver chunks data limit impactonrunningapplicationsandprovideleewayforschedul- ing decisions default parameters limit cluster concurrent clonings number chunkservers clone operation allowed consume mbps allchunkswere restored minutes ective replication rate experiment killed chunkservers withroughly chunksand gbofdata thisdouble failure reduced chunks single replica chunks cloned higher priority restored replication minutes putting cluster state tolerate chunkserver failure data loss workload breakdown section present detailed breakdown workloads gfs clusters comparable identical section cluster research development cluster production data processing methodology caveats results include client originated requests ect workload generated applications system include interserver requests carry client requests internal background activities forwarded writes rebalancing statistics operations based information heuristically reconstructed actual rpcrequestslogged gfs servers gfs client code breaka read multiple rpcs increase parallelism infer original read access patterns highly stylized expect error noise explicit logging applications provided slightly accurate data logistically impossible recompile restart thousands running clients cumbersome collect results machines careful overly generalize workload google completely controls gfs applications applications tend tuned gfs conversely gfs designed applications mutualin operation read write record append cluster inf table operations breakdown size reads andtransferred amount requested systems ect pronounced case chunkserver workload table shows thedistributionof operations bysize read sizes exhibit bimodal distribution small reads seek-intensive clients small pieces data huge les large reads long sequential reads entire les asigni numberof reads returnnodata atall incluster applications production systems les producer-consumer queues producers append concurrently consumer reads end occasionally data returned consumer outpaces producers cluster shows short-lived data analysis tasks long-lived distributed applications write sizes exhibit bimodal distribution large writes typically result signi ering writers writers data checkpointorsynchronizemoreoften orsimplygeneratelessdata account smaller writes record appends cluster sees higher percentage large record appends cluster production systems cluster aggressively tuned gfs table shows total amount data transferred operations sizes kinds operations larger operations generally account bytes transferred small reads transfer small signi portion read data random seekworkload appends versus writes record appends heavily production systems cluster ratio writes record appends bytes transferred operation counts cluster production systems ratios ratios suggest clusters record appends tend larger writes cluster usage record append measured period fairly low results skewed applications size choices expected data mutation workload dominated appending overwriting measured amount data overwritten primary replicas apoperation read write record append cluster inf table bytes transferred breakdown operation size reads size amount data read transferred amount requested read attempts read end design uncommon workloads cluster open delete findlocation findleaseholder findmatchingfiles combined table master requests breakdown type proximates case client deliberately overwrites previous written data appends data cluster overwriting accounts bytes mutated mutation operations clustery theratiosareboth althoughthisisminute higher expected turns overwrites client retries due errors timeouts part workload consequence retry mechanism master workload table shows breakdown type requests master requests askfor chunklocations findlocation reads lease holder information findleaselocker data mutations clusters signi cantly erent numbers delete requests cluster stores production data sets regularly regenerated replaced newer versions erence hidden erence open requests version implicitly deleted opened write scratch mode unix open terminology ports similar system operations unlike requests master process large part namespace expensive cluster sees automated dataprocessing taskstendto examine parts system understand global application state contrast cluster applications explicit user control names needed les advance experiences process building deploying gfs experienced variety issues operational technical initially gfs conceived backend system production systems time usage evolved include research development tasks started support thingslike permissions quotasbutnow includes rudimentary forms production systems disciplined controlled users infrastructure required users interfering someofourbiggest disks claimed linux driver supported range ide protocol versions fact respondedreliablyonlytothemorerecentones sincetheprotocol versions similar drives worked occasionally mismatches drive kernel disagree drive state corrupt data silently due problems kernel problem motivated checksums detect data corruption whileconcurrentlywemodi edthekerneltohandle protocol mismatches earlier problems linux kernels due cost fsync cost proportional size size modi portion problem large operation logs implemented checkpointing worked time synchronous writes eventually migrated linux linux problem single reader-writer lock thread address space hold pages disk reader lock modi address space mmap call writer lock transient timeouts system light load looked hard resource bottlenecks sporadic hardware failures eventually found single lockblocked primary networkthread mapping data memory diskthreads paging previously mapped data limited networkinterface memory copy bandwidth worked replacing mmap pread cost extra copy helped time explore understand system behavior improve kernel share open source community related work large distributed systems afs gfs location independent namespace enables data moved transparently load balance fault tolerance unlikeafs gfsspreadsa sdataacross storage serversin moreakin toxfs swift order deliver aggregate performance increased fault tolerance disks cheap replication simpler sophisticated raid approaches gfs replication redundancyand consumes raw storage xfs swift contrast systemslike afs xfs frangipani intermezzo gfs provide caching system interface target workloads reuse single application run stream large data set randomly seekwithin read small amounts data time distributed systems frangipani xfs minnesota sgfs andgpfs rely distributed algorithms consistency management opt centralized approach order simplify design increase reliability gain exibility centralized master makes easier toimplement sophisticated chunkplacement andreplication policies master relevant information controls address fault tolerance keeping master state small fully replicated machines scalability high availability reads provided shadow master mechanism updates master state made persistent appending write-ahead log adapt primary-copyscheme liketheone harp provide high availability stronger consistency guarantees current scheme weare addressing aproblem similar tolustre interms delivering aggregate performance large number clients simpli problem signi cantly focusing applications building posix-compliant system additionally gfs assumes large number unreliable components fault tolerance central design gfs closely resembles nasd architecture nasd architecture based network-attached diskdrives gfs commodity machines chunkservers nasd prototype unlike nasd work xed-sizechunksrather variable-length objects additionally gfs implements features rebalancing replication recovery required production environment unlike minnesota gfs nasd seek alter model storage device focus addressing day-to-day data processing complicated distributed systems existing commodity components producer-consumer queues enabled atomic record appendsaddress similar problemas thedistributedqueues river river memory-based queues distributed machines careful data control gfs persistent appended concurrently producers river model supports m-to-n distributedqueuesbutlacksthefaulttolerancethatcomeswith persistent storage gfs supports m-toqueues ciently multiple consumers read coordinate partition incoming load conclusions google file system demonstrates qualities essential supporting large-scale data processing workloads commodity hardware design decisions speci unique setting apply data processing tasks 
similar magnitude cost consciousness lesystemassumptions light current anticipated application workloads technological environment observations led radically erent points design space treat component failures norm exception optimize huge les appended concurrently read sequentially extend relax standard system interface improve system system fault tolerance constant monitoring replicating crucial data fast automatic recovery chunkreplication tolerate chunkserver failures frequency failures motivated pairs damage compensates lost replicas additionally checksumming detect data corruption diskor ide subsystem level common number disks system design delivers high aggregate throughput achieve separating system control passes master data transfer passes directly chunkservers clients master involvement common operations minimized large chunk size chunkleases delegates authority primary replicas data mutations makes simple centralized master bottleneck improvements networking stack lift current limitation write throughput individual client gfs successfully met storage widely google storage platform research development production data processing important tool enables continue innovate attackproblems scale entire web acknowledgments system paper brain bershad shepherd anonymous reviewers gave valuable comments andsuggestions anuragacharya dean anddaviddesjardins contributed early design fay chang worked comparison replicas chunkservers guy edjlali worked storage quota markus gutschke worked testing frameworkand security enhancements david kramer worked performance enhancements fay chang urs hoelzle max ibel sharon perl rob pike debby wallach commented earlier drafts paper colleagues google bravely trusted data system gave feedback yoshka helped early testing thomas anderson michael dahlin jeanna neefe david patterson drew roselli randolph wang serverless network systems proceedings acm symposium operating system principles pages copper mountain resort colorado december remzi arpaci-dusseau eric anderson noah treuhaft david culler joseph hellerstein david patterson kathy yelick cluster river making fast case common proceedings sixth workshop input output parallel distributed systems iopads pages atlanta georgia luis-felipe cabrera darrell long swift distributed diskstriping provide high data rates computer systems garth gibson david nagle khalil amiri butler fay chang howard gobio charles hardin erikriedel david rochberg jim zelenka cost-e ective high-bandwidth storage architecture proceedings architectural support programming languages operating systems pages san jose california october john howard michael kazar sherri menees david nichols mahadev satyanarayanan robert sidebotham michael west scale performance distributed system acm transactions computer systems february intermezzo http inter-mezzo barbara liskov sanjay ghemawat robert gruber paul johnson liuba shrira michael williams replication harp system symposium operating system principles pages paci grove october lustre http lustreorg david patterson garth gibson randy katz case redundant arrays inexpensive disks raid proceedings acm sigmod international conference management data pages chicago illinois september frankschmuckand roger haskin gpfs shared-disk system large computing clusters proceedings usenix conference file storage technologies pages monterey california january steven soltis thomas ruwart matthew keefe gobal file system proceedings nasa goddard space flight center conference mass storage systems technologies college park maryland september chandramohan thekkath timothy mann edward lee frangipani scalable distributed system proceedings acm symposium operating system principles pages saint-malo france october 
cmc pragmatic approach model checking real code madanlal musuvathi david parky andy chou dawson engler david dill fmadan parkit acc engler dillg stanford computer systems laboratory stanford stanford abstract system errors emerge intricate sequence events occurs practice means systems errors trigger days weeks execution model checking ective subtle errors takes simpli description code exhaustively tests inputs techniques explore vast state spaces ciently model checking systems code wonderful practice building models hard signi cantly time write model write code checking abstraction code code easy miss errors paper rst contribution model checker cmc checks implementations directly eliminating separate abstract description system behavior major advantages reduces ort model checking reduces missed errors time-wasting false error reports resulting inconsistencies abstract description actual implementation addition implementation checked immediately updating high-level description paper contribution demonstrating cmc works real code applying implementations ad-hoc on-demand distance vector aodv networking protocol found distinct errors roughly bug lines code including bug aodv speci cation experience building supported gsrc marco grant ysupported national science foundation graduate research fellowship systems appears approach work contexts networking protocols introduction complex systems complex errors real systems variety mishandled corner cases triggered intricate sequences events practice leaves residue errors system crashes days weeks continuous execution detected problems cult diagnose errors reproducible sequence events leading reconstructed formal veri cation methods diagnose deep errors option explicit model checking systematically enumerates states system basic model checker starts initial state recursively generates successive system states executing nondeterministic events system states stored hash table ensure state explored process continues state space explored model checker runs resources works style state graph exploration achieve ect impractically massive testing avoiding redundancy occur conventional testing conventional model checkers assume design high level abstracts details actual implementation verifying actual code tool requires reconstructing abstract description code process requires great deal manual ort hampering model checking actual system design human errors manual abstraction process result missing bugs false alarms veri cation increasing cost reducing usefulness model checking errors introduced constructing model result drift actual system evolves reasons notable curiosity software model checked everyday occurrence introduce cmc model checker address issues cmc works unmodi implementations explores large state spaces ciently storing states traditional model checkers cmc achieves equivalent executing astronomical numbers tests reasonable time cmc require writing separate high-level model code extracting model implementation importantly nds bugs implementation miss implementation bugs omitted model waste user time bugs model implementation idea model checking actual implementation code advocated small number tools verisoft instance systematically executes implementation code store states software model checking tools specialized work classes java programs cmc designed combine ective techniques research orts veri cation community apply software written predominant programming languages industry ultimate goal work check systems code general initial focus networking code code naturally eventdriven execution model makes good model checkers correctness networking protocol implementations important core services rst target external security attacks network protocols cult design implement test involve complex interactions multiple machines network deal network failures packet losses link failures cult control test environment model checkers excel checking interactions cmc works real code demonstrated results applying implementations aodv networking protocol rst implementation mad-hoc released years ago active development implementation kernel aodv derives mad-hoc implementation released year ago implementation aodv-uu released year ago aodv speci cation active development rst version subsequently undergone ten revisions cult measure quality absolutely measure formal group devoted testing aodv implementations testbed check mad-hoc aodv-uu implementations cmc found unique errors total date rate roughly bug lines code bugs non-trivial cult method ironic twist model checking implementation found bug speci cation aodv error con rmed authors aodv speci cation protocol implementations similar aodv cmc enhancements broaden applicability concurrent systems good reason cmc systems cult debug means model checking overview fundamentally explicit state model checking systematic search error states state graph represents behavior system generate graph search report errors state graph large search completely important state graphs systems errors larger correct systems search algorithms newly discovered states stored queue policy depthrst breadthrst bestrst states removed queue successors generated expanded enqueued multiple successors nondeterminism system states searched stored hash table successors expanded model checking prove system satis speci property practical bugnding method model checking applicable ective conventional testing discovering bugs thoroughness exploring state space system including corner cases overlooked model checking cient random testing searches state code model check model environment relevant aspects network operating system calls environment model avoid false error reports resulting illegal inputs state occur actual system execution parts environment model unit testing system checked put model checker apparent problems model checking small system description result huge state graph called state explosion problem addressed ways including methods suppressing details input description abstraction optimizations save time importantly space state explosion problem remains culty applications model checking note state pruning randomized testing typically fare signi cantly worse situations addressing issues paper presents approach pragmatically apply model checking actual implementation code bugs end implemented tool called cmc bugs network protocol implementations sections design cmc cmc model checker generates state space system directly executing implementation section describes design cmc beginning description tool infrastructure steps required set system checking illustrated actual model checking algorithm finally techniques cope state explosion problem discussed cmc infrastructure cmc models system collection interacting concurrent agents called processes process runs unmodi code implementation cmc model checker responsible scheduling executing processes system checked cmc processes system run single operating system process unlike operating system cmc search system states reached alternative scheduling decisions nondeterministic events search erent possibilities cmc save restore complete state modelled system process system executes heap stack instant state process consists copy global static variables heap stack context registers processes communicate shared memory accessible context processes state system ned union states processes contents shared memory scheduled process allowed execute deterministic non-blocking set instructions ned transition transition atomic step system determines degree interleaving processes protocols cmc applied follow eventdriven execution model set event handler routines process incoming events packet arrivals timeouts event-driven protocol event handler mapped transition cmc event handler preserves state stack registers global variables heap saved restored protocols written event-driven style event-driven model restrictive feasible save restore full states including 
stack modelled processes feature implemented evaluated results reported paper creating cmc model implementation figure shows skeleton event-driven implementation routing protocol similar aodv protocol checked section implementation running discussion main function implementation calls initialization function line enters event dispatch loop line depending input event calls event handlers ned lines handler processes event user request route destination request node response node previous requests timer event requiring protocol invalidate routes subsection describes steps user perform apply cmc protocol steps essentially provide unit test sca required test environments running implementation simulator step additional requirement cmc step correctness properties system tested user correctness properties properties domain independent program access illegal memory leak memory domain-speci properties speci assertions points implementation protocol return invalid route line cases careful implementer assertions code long cmc applied properties inherently global requirement loops routing table properties speci boolean functions written access datastructures process contexts step environment user build test environment adequately represents behavior actual environment protocol executed networking protocols environment model fakes operating system protocol function decision part system checked environment decided user environment model collection substitute api functions data structures emulate state environment modelled detail super uous states generated model environmental behavior irrelevant checking protocol functions replaced simple stubs gettimeofday return constant counter model requires network exchanging routing packets simple network modelled unordered queue bounded length model include versions interface functions broadcast request function sends packets network environment processes exevent handlers user request dest route table route dest return route dest broadcast request dest recv request dest route table route dest send response route dest broadcast request dest recv response route install route route table route forwarded send response route timeout route route table route remove route route table init route table null insert route main init event dispatch loop depending event call user request recv request recv response timeout figure simple routing protocol implementation void malloc size cmcchoose return nondeterministic failure alloc bytes heap return figure implementation malloc cmc malloc nondeterministically fails allocate memory ample process nondeterministically removes packet network model lossy network represent nondeterminism environment cmc cmcchoose function similar toss verisoft cmcchoose takes integer argument returns integer range cmcchoose arbitrarily selects possibilities environment shown figure malloc implementation allocate requested memory cmc heap fail returning null cmcchoose make choices cmc attempt return values call cmcchoose calls cmcchoose environment code implementations standard system functions malloc select generally modify actual implementation providing environment model time consuming important reduce modelling ort required apply cmc previously unchecked protocol rst obvious step engineer models re-usable reducing incremental ort checking protocol bene cial related protocols checked large part reason paper checks erent implementations protocol finding ways reduce cost environmental modelling interesting area future work step identifying initialization functions event handlers event driven system user provide initialization functions event handlers process system user provide guard function event handler boolean function determines event handler enabled state instance guard function recv request handler returns true request pending process network cmc model checking algorithm model system built cmc explores state space system executing traces interleaving transitions pseudocode algorithm shown figure algorithm maintains data structures hash table states search queue states successors generated hash table guarantees algorithm explores subgraph rooted state generating initial state cmc computes initial state starting copy global variables initialized linker cmc calls initialization function process initial state consists states processes immediately initialization functions called values initialized shared memory generating successor states generate state graph on-they cmc compute set successors state state state space system successors nondeterminism arises sources choice process execute choice enabled transition process execute nondeterministic values returned calls cmcchoose state cmc chooses process enabled event handlers schedule cmc restores context process copying contents heap global variables process state event handler called function eventually returns guaranteed atomic point context process state saved yielding system state cmc generates successors state repeating process nondeterministic choices checking correctness properties model checking cmc checks range correctness properties simple pointer access void modelcheck systemstate sharedmem network procstate processes initial current successor queue stateq hash visitedstates build initial state forall processes pid call pid initfn initial procstate pid savectxt call sharedmem initfn initial sharedmem getsharedmem stateq insert initial current stateq pop visitedstates add current repeat forall nondeterministic choices forall processes pid forall event handlers pid forall return values cmcchoose calls set proc context sharedmem restorectxt current procstate pid setsharedmem current sharedmem enabled continue call event handler construct state successor sharedmem getsharedmem successor procstate pid savectxt forall processes pid pid state change successor procstate pid current procstate pid successor visitedstates continue successor fails assertions generate error stateq insert successor figure pseudocode cmc model checking algorithm violation errors complex protocol bugs execution event handler cmc runs implementation code directly automatically catching errors pointer access violations program assertion failures present code addition cmc detects use-after-free bugs overwriting freed memory random state generated cmc checks violations user-provided system invariants absence global routing loops cmc detects memory leaks generated state achieved standard mark-and-sweep algorithm reachable memory algorithm implemented cmc case study cmc detects memory leaks starting copy current state cmc calls cleanup functions present implementation heap memory left allocated reported leaked approach requiring additional manual ort potentially bugs cleanup code future cmc approach easily coupled dynamic debugging tools purify stackguard tools catch run-time errors uninitialized memory stack ows tools ective cmc ordinary testing cmc achieve greater ective test coverage level user ort conventional software testing methods handling state space explosion problems model checking practice so-called state explosion problem state space system large nite outset impossible explore entire state space limited resources time memory cmc techniques search state space ciently running resources unable formally prove correctness implementation cmc catch wide range errors including errors involving intricate interactions multiple processes model checkers memory critical resource time model checking memory consumed hash table states visited queue states successors generated cmc hash compaction reduce memory requirements hash table orders magnitude state cmc computes small signature bytes storing entire state order kilobytes signature stored hash table compacting states lead con icts hash table erent states compute signature state spaces order hundred million states practical hash table sizes hundred megabytes probability missing 
single state due signature con ict reduced lower states queue compacted information needed compute successor states queue good locality swapped disk model checking successive states queue lot commonality compressed instance transition cmc process state cient store erence generating successor state standardizing data structures cmc default interprets states streams bits equivalent data structures memory erent representations states order objects allocated heap considered ectively cmc automatically transform states deterministically traversing pointer data structures arranging objects heap order visited signature transformed state saved state table process performed simultaneously mark-and-sweep algorithm detect memory leaks automatic tool traversal development framework case study discussed section traversal code written manually additional equivalences states depend data structures program implementation linked list store unordered collection objects behavior implementation independent order objects list case user provide function sort list automatic standardization transformations applied finally ective reductions state space achieved methods risk missing errors bene catching remaining ciently down-scaling obvious approach reduce scale system ure instance model restrict number routing nodes network hard-tond bugs involve complex interactions small number processes preserved downscaling miss bugs occur larger instances system abstraction states addition standardizing distinct equivalent states eliminate information user judges unimportant properties checked abstraction process ignoring memory locations computing hash signature state abstracting states miss errors abstraction hash computation actual concrete state produce false positives heuristics exhaustive checking entire state space infeasible fails cmc act automated testing framework large number scenarios checked intelligently mere fact cmc cache states prevents redundant simulations goal exercise interesting scenarios memory exhausted end preliminary work heuristics prioritize state space search rst class heuristics involves dropping states altogether deemed uninteresting class heuristics involves exploring interesting states rst bestrst search cmc module monitor state variables history state bits changed checking basic idea number bit positions changed initial state suddenly increases variables frequented values state considered interesting explored earlier heuristic bias search cases outliers occur states diverge norm idea adapted diduce tool ags divergent cases reports user program testing preliminary results errors discovered heuristics discovered simple depthrst search heuristics accelerated discovery errors produced shorter examples executions leading error experimentation heuristics needed wider range protocols arrive reliable conclusions sections describe application results cmc check aodv protocol implementations description aodv protocol aodv ad-hoc on-demand distance vector loop-free routing protocol ad-hoc networks designed self-starting environment mobile nodes withstanding variety network behaviors node mobility link failures packet losses section describes aodv protocol reader referred complete details protocol node aodv maintains routing table routing table entry destination essential elds hop node sequence number hop count packets destined destination hop node sequence number acts form time-stamping measure freshness route hop count represents current distance destination node suppose nodes hop destination suppose sequence number hop count routes seqa hcnta seqb hcntb aodv protocol maintains property times seqa seqb seqa seqb hcnta hcntb words newer route shorter route equally recent partial order constraint protocol guaranteed free routing loops aodv nodes discover routes request-response cycles node requests route destination broadcasting rreq message neighbors node receives rreq message route requested destination turn broadcasts rreq message remembers reverse-route requesting node forward subsequent responses rreq process repeats rreq reaches node valid route destination node destination responds rrep message rrep unicast reverse-routes intermediate nodes reaches original requesting node end request-response cycle bidirectional route established requesting node destination node loses connectivity hop node invalidates route sending rerr nodes potentially received rrep receipt aodv messages rreq rrep rerr nodes update hop sequence number hop counts routes satisfy partial order constraint mentioned aodv model section describes aodv model implementations aodv protocol mad-hoc version kernel aodv version aodv-uu version mad-hoc implementation runs user space daemon approximately lines code kernel aodv implementation built nist based mad-hoc implementation lines code runs loadable kernel module linux arm based pdas aodvuu implementation runs user space daemon linux ported nssimula- tor roughly lines code aodv model reused minor modi cations implementations model built correctness properties table lists correctness properties checked aodv model generic assertions checked cmc model global invariant checks routing loops model performs sanity checks routing table entries network messages range violations elds environment environment model consists network modelled bounded-length unordered message queue model simulates message loss nondeterministically dequeuing message message queue shared nodes models completely connected topology implementations wrapper function send network packets model alternate nition wrapper function copy packets network model additionally kernel aodv implementation model implementations twenty-two kernel functions types checks examples generic assertions segmentation violations memory leaks dangling pointers routing loop invariant routing tables nodes form routing loop assertions routing table entries routing table entry destination route aodv-uu implementation hop count route present hop count nity number nodes network assertions message fields reserved elds set hop count packet nity table properties checked aodv enabling condition event invalid route destination initiation route request pending message network receipt aodv message pending message network message loss valid route routing table timeout route enabled detection link failure enabled node reboot table set event handlers aodv model checking kmalloc printk user space version socket library initialization functions event handlers implementations event dispatch loop calls event handlers initialization functions model obtained executing code event dispatch loop model maps event handler called dispatch loop transition model simulates node reboot calling initialization function implicitly resets contents routing table list transitions respective enabling conditions shown table table shows lines code implementations executed framework lines code model correctness speci cations shared implementations aodv-uu erent representation routing table required additional correctness speci cations network model environment shared implementations dealing state space explosion state space aodv protocol essentially nite protocol arbitrary number nodes network node types unbounded counters sequence number measure freshness route broadcast incremented node broadcast ective search nite state space bound search experiments downscaled aodv model run processes model discarded state sequence numbers broadcast ids exceeded prede ned limit size message queue network bounded sizes processes cmc miss errors applying bounds remaining state space contained interesting behavior uncover numerous bugs section time values stored state source state space explosion instance route response rrep lifetime eld determines freshness route receipt packet node adds lifetime current clock determine time route stale absolute stored routing table increase state space size 
aodv model problem modelling route timeouts nondeterministic events setting time variables prede ned constants environment model nition gettimeofday function returns constant handling time model miss timing related errors protocol checked correctness environment state code speci cation network stubs skbu canonicalization mad-hoc kernel aodv aodv-uu table lines implementation code cmc modelling code potentially lead false positives error reported caused sequence timeouts impossible real protocol aodv model hand-written code traverse routing table implemented linked list mad-hoc kernel aodv implementations hash table aodv-uu implementation traversal code created canonicalized representation routing table global variables formed state aodv node model amount lines required traversal code shown column table results table summarizes set bugs found cmc aodv implementations bugs range simple memory errors protocol invariant violations found total bugs unique kernel aodv implementation bugs shown parenthesis table instances bug madhoc aodv speci cation bug routing loop implementations cmc stops nding rst bug model prints failed assertion trace events starting initial state error state bug xed cmc run bugs iteratively bugs found minutes model checking time longest roughly minutes describe bugs high level give feel breadth coverage focus interesting bugs give feel depth memory errors rst error classes illustrate mishandling dynamically allocated memory checking allocation failure errors freeing allocated memory errors memory freeing errors implementations checked pointer reaodv deamon aodv recv message rerri rerri rerrhdr msg dst cnt rerri malloc sizeof break skip packet tpnext rerrhdr msg unr dst rerrhdr msg unr dst bug assumes rerrhdr msg dst cnt buffers allocated rec rerr info msg rerrhdr msg free list structs rec rerr rerri rerri rerrhdr msg dst cnt rerri bug null malloc failed rerrhdr msg unr dst rerrhdr msg unr dst rerrhdr msg unr dstnext free figure mishandled malloc failure malloc fails loop exit allocating rerrhdr msg dst cnt ers errors code assumes rerrhdr msg dst cnt ers allocated lead segmentation faults turned malloc null functions call malloc indirectly return null pointers allocations fail code erratically checked cases cmc directly executes implementation errors manifested segmentation faults memory-related bugs straightforward interesting errors code correctly check allocation failure recovery code broken figure representative error code attempts allocate rerrhdr msg dst cnt temporary message ers correctly checks malloc failure breaks loop code loop assumes rerrhdr msg dst cnt list entries allocated assumption leads bugs rst intraprocedural error mad-hoc kernel aodv aodv-uu mishandling malloc failures memory leaks free invalid routing table entry unexpected message generating invalid packets program assertion failures routing loops total table number bugs type implementations aodv gures parenthesis show number bugs instances bug mad-hoc implementation attempts dequeue rerrhdr msg dst cnt ers rerrhdr msg unr dst list order free list fewer entries expected code attempt null pointer segmentation fault interprocedural error rec rerr similarly walk rerrhdr msg dst cnt list entries seg faults list short memory leaks similarly caused mishandled allocation failures commonly code attempt memory allocations rst allocation succeeded failed return error leaking rst pointer unexpected messages cmc detected places unexpected messages mad-hoc crash segmentation violation figure shows errors error aodv encodes state messages error current node receives route request rreq message node req requesting route node dst node inserts reverse route req routing table route dst routing table route re-broadcasts rreq message rreq message address destination node dst requesting node req response request route response rrep message includes route dst address req node inserts route dst routing table attempts relay route req route req normal case lookup return reverse route inserted step step error code assumes normal case result routing table lookup req checking null lookup fail reasons machine rebooted implementation start empty routing table rrep message arrives reboot lookup req return null pointer attacker send bogus rrep node address exist crashing router invalid messages cases invalid packets created cases uninitialized variables detected gcc -wall cases invalid routes send routing updates violating aodv speci cation figure representative cmc detected instances integer resulted program assertion failures implementations bit integer store hop counts represent hopcount ity error cases nite hopcount erroneously incremented routing loops cmc found routing loops bugs caused implementation errors routing loop due error aodv protocol speci cation rst routing loop caused implementation fails increment sequence number processing speci rerr messages loop caused implementation performs sequence number comparison subsequent increment aodv speci cation requires comparison increment madhoc rrep rec rrep destination rrep forward rrepsrc infoip pkt entry source src getentry rrepsrc bug src exist add precursor src rtnxt hop send gratuitous rrep destination bug src invalid srchop cnt check getentry rrep hop cnt srchop cnt send datagram info rrep sizeof rrep figure bugs unexpected message invalid route response unexpected routeresponse rrep message getentry return null crashing machine route returned getentry invalidated hopcount code check sends message speci cation bug bug involved handling rerr route error messages node receives rerr hop sets sequence number route sequence number rerr message normal conditions thing underlying link layer reorder messages rerr message outdated sequence number resulting node setting sequence number older version ultimately result routing loop bug mentioned authors protocol suggested bug accepted protocol authors figure error speci cation bug found running aodv nodes depthrst search state space cmc error trace length bestrst search traces short performing breadthrst search state space give shortest trace breadthrst search aodv ran resources nding bug carefully hand-crafted simulation bug required transitions complex error cult catch conventional means testing madhoc rerr rec rerr pointer route table destination tmp rtentry getentry tmp unr dstunr dst tmp rtentry null bug sequence number incoming message tmp unr dst validation check tmp rtentrydst seq tmp unr dstunr dst seq return tmp rtentrydst seq tmp unr dstunr dst seq figure speci cation bug sequence number incoming message validation causing time backwards messages reordered fortunately error obvious surviving rounds speci cation revisions trivial related work paper proposes initial approach systematically ciently verify large class software create abstract models erent language compares work cmc orts traditional model checking software model checking static analysis traditional model checking basic idea state graph search verify network communication protocols dating back recent decades model checking made signi progress tackling veri cation complex concurrent systems tools smv 
spin murphi verify hardware software protocols exhaustively searching state space caching states employing sound state reduction techniques tools detect non-trivial bugs drawback traditional model checkers system veri modeled description language requiring signi amount manual ort easily error prone cmc speci cally designed goal reducing amount work required software development systematic veri cation software model checking recent formal veri cation tools idea executing checking systems implementation level verisoft instance systematically executes veri actual code successfully check communication protocols written verisoft store states potentially explore state problem alleviated degree partial order reduction sound state space reduction technique implemented verisoft eliminates exploration redundant interleavings transitions created commutative operations technique requires hints provided user static analysis code determine dependencies transitions set transitions system high degree interdependence case handlers protocol code veri partial order methods ective finally interesting systems state spaces cycles cases verisoft limited checking xed depth java pathfinder model checking verify concurrent java programs deadlock assertion failures relies specialized virtual machine tailored automatically extract current state java program cmc java pathfinder compresses stores states table prevent redundant searches relies abstraction techniques curb state space explosion problem infrastructure jpf relies applied software written predominant languages system software development slam tool converts code abstracted skeletons boolean types slam model checks abstracted program error state reachable culty tool slam giving speci cation correct behavior system slam static tool writing speci cation routing loops cult depends interleaved event behavior multiple nodes slam deal concurrent environments multiple processes queues static analysis static analysis gained ground recent years detecting bugs software tools esc lclint esp checker check source code errors statically detected minimal manual ort static techniques good nding speci set errors cmc approach deep conceptual errors code emergent routing loops cult statically addition cmc false positives scenario checked valid execution path conclusion future work paper cmc model checker targetting subtle bugs systems code experimental results cmc check implementations aodv routing protocol key features cmc checks implementation code directly stores states avoid redundant state explorations initial experiences cmc encouraging cmc powerful discover non-trivial bugs implementation speci cation protocols cmc verify larger complex protocols wider essential automate process converting implementation system cmc model results reported require considerable manual ort future improvements cmc signi cantly reduce exploring heuristics ciently search state space initial ndings suggest simple heuristics provide huge improvements state space search instance implemented monitor detects counters rogue variables uninitialized variables statistics variables monitor abstracts variables system state automatically pruning nite state space interesting avenue research simple facts discovered static analysis code direct search interesting parts state space acknowledgments satyaki das thoughtful discussions paper miguel castro anonymous reviewers providing valuable comments suggesting improvements previous versions paper thomas ball rupak majumdar todd millstein sriram rajamani automatic predicate abstraction programs proceedings sigplan conference programming language design implementation bhargavan obradovic gunter formal veri cation standards distance vector routing protocols brat havelund park visser model checking programs ieee international conference automated software engineering ase clarke grumberg peled model checking mit press corbett dwyer hatcli laubach pasareanu robby zheng bandera extracting nite-state models java source code icse crispan cowan calton dave maier jonathan walpole peat bakke steve beattie aaron grier perry wagle qian zhang heather hinton stackguard automatic adaptive detection prevention erover attacks proc usenix security conference pages san antonio texas jan perkins royer das hoc demand distance vector aodv routing ietf draft http ietf internet-drafts draftietf-manet-aodv- txt january manuvir das sorin lerner mark seigle esp pathsensitive program veri cation polynomial time conference programming language design implementation david detlefs rustan leino greg nelson james saxe extended static checking david dill andreas drexler alan han yang protocol veri cation hardware design aid ieee international conference computer design vlsi computers processors pages engler chelf chou hallem checking system rules system-speci programmerwritten compiler extensions proceedings fourth symposium operating systems design implementation october erik nordstrom hoc protocol evaluation testbed http apetestbed sourceforge net erik nordstrom aodv-uu implementation http user henrikl aodv david evans john guttag james horning yang meng tan lclint tool speci cations check code proceedings acm sigsoft symposium foundations software engineering pages godefroid model checking programming languages verisoft proceedings acm symposium principles programming languages hajek automatically veri data transfer protocols proceedings iccc pages sudheendra hangal monica lam tracking software bugs automatic anomaly detection proceedings international conference software engineering gerard holzmann model checker spin software engineering mcmillan symbolic model checking kluwer academic publishers luke klein-berndt kernel aodv implementation http antd nist gov wctg aodv kernel lilieblad mad-hoc aodv implementation http mad-hoc yinglinux net mccanne floyd ucb lbnl vint network simulator version april http isi nsnam mcmillan schwalbe formal veri cation gigamax cache consistency protocol proceedings international symposium shared memory multiprocessing pages tokyo japan inf process soc nelson techniques program veri cation xerox parc research report csl- june stanford park stern skakkebaek dill java model checking ieee international conference automated software engineering ase charles perkins elizabeth royer samir das private communication rational software purify advanced runtime error checking developers http rational products purify unix stern dill scheme memory-e cient probabilistic veri cation ifip joint international conference formal description techniques distributed systems communication protocols protocol speci cation testing veri cation stern dill automatic veri cation sci cache coherence protocol correct hardware design veri cation methods ifip advanced research working conference proceedings west general technique communications protocol validation ibm journal research development 
overview singularity project galen hunt james larus mart abadi mark aiken paul barham manuel hndrich chris hawblitzel orion hodson steven levi nick murphy bjarne steensgaard david tarditi ted wobber brian zill microsoft research microsoft redmond http research microsoft singularity microsoft research technical report msr-tr- abstract singularity research project microsoft research started question software platform designed scratch primary goal dependability singularity working answer question building advances programming languages tools develop system architecture operating system named singularity aim producing robust dependable software platform singularity demonstrates practicality technologies architectural decisions lead construction robust dependable systems report snapshot project motion performance measurements preliminary subject improvement contact galen hunt galenh microsoft jim larus larus microsoft check http research microsoft singularity latest results citation information mart abadi affiliated computer science department california santa cruz paul barham microsoft research cambridge mart abadi nick murphy ted wobber microsoft research silicon valley fortunate great group interns made contributions project past couple years michael carbin fernando castor adam chlipala jeremy condit daniel frampton chip killian prince mahajan bill mccloskey martin murray martin pohlack tom roeder avi shinnar mike spear yaron weinsberg aydan yumerefendi introduction software runs platform evolved past years increasingly showing age platform vast collection code operating systems programming languages compilers libraries run-time systems middleware hardware enables program execute hand platform enormous success financial practical terms platform forms foundation billion dollar packaged software industry enabled revolutionary innovations internet hand platform software running robust reliable secure users developers part problem current platform evolved computer architectures operating systems programming languages computing environment period today milieu computers extremely limited speed memory capacity small group technically literate non-malicious users rarely networked connected physical devices characteristics remains true modern computer architectures operating systems programming languages evolved accommodate fundamental shift computers singularity research project microsoft research started question software platform designed scratch primary goal dependability common goal performance singularity working answer question building advances programming languages programming tools develop build system architecture operating system named singularity aim producing robust dependable software platform dependability difficult measure research prototype singularity shows practicality technologies architectural decisions lead robust dependable systems future exponential rate progress hardware evolution commonly appears drive fundamental systems applications software glacial progress rarely creates opportunities fundamental improvements software evolve change makes rethink assumptions practices advances programming languages run-time systems program analysis tools provide building blocks construct architectures systems dependable robust existing systems expressive safe programming languages java type safety ensures object correctly interpreted manipulated memory safety ensures program memory bounds valid live objects optimizing compilers high performance run-time systems generate safe code runs speeds comparable unsafe code compilers unlike common just-in-time jit compilers perform global optimizations mitigate safetyrelated overhead garbage collectors systems reclaim memory overhead comparable explicit deallocation term dependability reliability ifip dependable computing fault tolerance defines terms notion dependability defined trustworthiness computing system reliance justifiably service delivers enables concerns subsumed single conceptual framework dependability includes special cases attributes reliability availability safety security validation techniques ensure end-to-end type safety compiler compiled code run-time system typed intermediate assembly language validate proper operations system components ensure language safety guarantees underlie system correctness sound specification-driven defect detection tools ensure correctness aspects system sound tool finds occurrences error false positives reliably defect eliminated specification-driven tools hardwired collection defects extensible adapted check program library-specific abstractions correctly languages tools based advances detecting preventing programming errors explored mechanisms enable deep system architecture turn advance ultimate goal preventing mitigating software defects rest paper describes singularity system detail section overview system aspects section describes singularity system architecture focusing kernel processes language run-time system section describes programming language support system section describes security system section performance benchmarks section surveys related work appendix list kernel abi calls singularity singularity operating system developed basis dependable system application software singularity exploits advances programming languages tools create environment software built correctly program behavior easier verify run-time failures contained key aspect singularity extension model based software-isolated processes sips encapsulate pieces application system provide information hiding failure isolation strong interfaces sips operating system application software building system abstraction lead dependable software sips processes singularity code kernel executes sip sips differ conventional operating system processes number ways sips closed object spaces address spaces singularity processes simultaneously access object communications processes transfers exclusive ownership data sips closed code spaces process dynamically load generate code sips rely memory management hardware isolation multiple sips reside physical virtual address space communications sips bidirectional strongly typed higher-order channels channel specifies communications protocol values transferred aspects verified sips inexpensive create communication sips incurs low overhead low cost makes practical sips fine-grain isolation extension mechanism sips created terminated operating system termination sip resources efficiently reclaimed sips executed independently extent data layouts run-time systems garbage collectors sips encapsulate application extensions singularity single mechanism protection extensibility conventional dual mechanisms processes dynamic code loading consequence singularity error recovery model communication mechanism security policy programming model layers partially redundant mechanisms policies current systems key experiment singularity construct entire operating system sips demonstrate resulting system dependable conventional system singularity kernel consists safe code rest system executes sips consists verifiably safe code including device drivers system processes applications untrusted code verifiably safe parts singularity kernel run-time system called trusted base verifiably safe language safety protects trusted base untrusted code integrity sips depends language safety system-wide invariant process hold process object space ensuring code safety essential short term singularity relies compiler verification source intermediate code future typed assembly language tal singularity verify safety compiled code tal requires program executable supply proof type safety produced automatically compiler safe language verifying proof correct applicable instructions executable straightforward task simple verifier thousand lines code end-to-end verification strategy eliminates compiler large complex program singularity trusted base verifier carefully designed implemented checked tasks feasible size simplicity memory independence invariant prohibits cross-object space pointers serves purposes enhances data abstraction failure isolation process hiding implementation details preventing dangling pointers terminated processes relaxes implementation constraints allowing processes run-time systems garbage collectors run coordination clarifies resource accounting reclamation making unambiguous process ownership piece memory finally simplifies kernel interface eliminating manipulate multiple types pointers address spaces major objection architecture difficulty communicating message passing compared flexibility directly 
sharing data singularity addressing problem efficient messaging system programming language extensions concisely communication channels verification tools extensibility software creators rarely anticipate full functionality demanded users system application satisfy monolithic system non-trivial software mechanisms load additional code microsoft windows supports party device drivers enable control hardware device similarly countless browser add-ons extensions augment browser interface components web pages open source projects theoretically modifiable provide plug-in mechanisms extensions easier develop distribute combine versions software extension consists code dynamically loaded parent address space direct access parent internal interfaces data structures extensions provide rich functionality flexibility high cost extensions major software reliability security backward compatibility problems extension code untrusted unverified faulty malicious loaded directly program address space hard interface boundary distinction host extension outcome unpleasant swift reports faulty device drivers diagnosed windows system crashes extension lacks hard interface unexposed aspects parent implementation constrain evolution program require extensive testing avoid incompatibilities dynamic code loading imposes obvious tax performance correctness software load code open environment impossible make sound assumptions system states invariants valid transitions java virtual machine jvm interrupt exception thread switch invoke code loads file overwrites class method bodies modifies global state general feasible analyze program running conditions start unsound assumption environment change arbitrarily operations alternative prohibit code loading isolate dynamically created code environment previous attempts lines widely popular isolation mechanisms performance programmability problems made appealing risks running isolation common mechanism traditional process high costs limit usability memory management hardware hard boundaries protects processor state makes inter-process control data transfers expensive processor switching processes cost hundreds thousands cycles including tlb cache refill misses recent systems java virtual machine microsoft common language runtime clr designed extensibility language safety hardware mechanism isolate computations running address space safe languages guarantee isolation shared data provide navigable path computations object spaces point reflection mechanisms subvert data abstraction information hiding consequence systems incorporate complex security mechanisms policies java fine grain access control clr code access security limit access system mechanisms interfaces mechanisms difficult properly impose considerable overhead equally important computations share run-time system execute process isolated failure computation running jvm fails entire jvm process typically restarted difficult isolate discard corrupted data find clean point restart failed computation singularity sips encapsulate device driver system process application extension runs sip communicates channels provide limited functionality code sip fails terminates system reclaim resources notify communication partners partners share state extension error recovery local facilitated explicit protocols channels run-time source code dynamic code generation commonly encapsulated reflection interface feature running program examine existing code data produce install methods reflection commonly produce marshalling code objects parsers xml schemas singularity closed sips run-time code generation singularity compile-time reflection ctr similar functionality executes file compiled normal reflection access runtime values general ctr cases class marshaled schemas parsed ahead execution cases ctr produces code compilation cases singularity support mechanism generating code running separate sip application abstraction operating systems treat programs applications first-class abstraction modern application collection files code data metadata untrusted agent installs copying pieces file system registering namespaces system largely unaware relationships pieces control installation process well-known consequence adding removing application break unrelated software singularity application consists manifest collection resources manifest describes application terms resources dependencies existing setup descriptions combine declarative imperative aspects singularity manifests declarative statements describe desired state application installation update process realizing state singularity responsibility manifest provide information singularity installer deduce installation steps detect conflicts existing applications decide installation succeeded singularity prevent installations impair system aspects singularity utilize information manifest singularity security model introduces applications security principal enables application entered file access control lists acl treating application principal requires knowledge application constituent pieces dependencies strong identity manifest discussion key contributions singularity construction system application model called software-isolated processes verified safe code implement strong boundary processes hardware mechanisms sips cost create schedule system applications support finer isolation boundaries stronger isolation model consistent extension model system applications simplifies security model improves dependability failure recovery increases code optimization makes programming testing tools effective fast verifiable communication mechanism processes system preserves process independence isolation enables process communicate correctly low cost language compiler support build entire system safe code verify interprocess communications explicit resource management elimination distinction operating system safe language run-time system java jvm microsoft clr pervasive specifications system describe configure verify components rest paper organized section describes implementation singularity system detail section discusses programming language compiler support singularity section describes singularity system services section performance measurements section discusses related work section concludes singularity architecture figure depicts architecture singularity built key abstractions kernel software-isolated processes channels kernel core functionality system including memory management process creation termination channel operations scheduling microkernels system functionality extensibility exist processes kernel trusted base code singularity verified trusted verified code type memory safety checked compiler unverifiable code trusted system limited hardware abstraction layer hal kernel parts run-time system kernel verifiably safe portions written assembler unsafe code written safe language translated safe microsoft intermediate language msil compiled bartok compiler trust bartok correctly verifies generates safe code unsatisfactory long run plan typed assembly language verify output compiler reduce part trusted computing base small verifier msil cpu-independent instruction set accepted microsoft clr singularity standard msil format features specific singularity expressed metadata extensions msil microkernel runtime kernel class library page mgr mgr scheduler channel mgr figure singularity architecture application runtime clr class library file sys runtime file sys library disk driver runtime driver class library extension runtime clr class librar dividing line types code blurred run-time system trusted unverifiable code effectively isolated computation verified safety prevents interacting run-time system data structures safe interfaces singularity compiler in-line routines safely moving operations traditionally run kernel user process kernel singularity kernel privileged system component controls access hardware resources allocates reclaims memory creates schedules threads intraprocess thread synchronization manages written mixture safe unsafe code runs garbage collected object space addition usual mechanism message-passing channels processes communicate kernel strongly versioned application binary interface abi invokes static methods kernel code interface design rest system isolates kernel process object spaces parameters abi values pointers kernel process garbage collectors coordinate exception location abi methods garbage collectors relocate code maintain invariant methods remain addresses abi maintains system-wide state 
isolation invariant process alter state process abi exceptions abi call affects state calling process exceptions alter state child process executes execution call create child process specifies code loaded child begins execution call stop child process reclaims resources threads cease execution state isolation ensures singularity process sole control state handle table kernel exports synchronization constructs mutexes auto manual reset events coordinate threads process thread manipulates constructs strongly typed opaque handle points kernel handle table strong typing prevents process changing forging handles addition slots handle table reclaimed process terminates prevent process freeing mutex retaining handle manipulate process object singularity reuse table entries process case retaining handle benign painful error process exchange heap figure exchange heap abi versioning kernel abi strongly versioned explicitly identifying abi version information program singularity clear path system evolution backward compatibility code process compiled compiled abi interface assembly namespace explicitly specifies version microsoft singularity threads namespace thread-related functionality version abi process source code names specific namespace desired version abi process binary code explicit metadata specific version abi install time program installed version abi supported target machine abi interface assembly replaced implementation assembly process-side implementation version abi system version kernel simplest implementation turns run-time calls kernel abi direct invocations static kernel method newer singularity system populate namespace earlier version library compatibility functions alternatively compatibility code run kernel kernel easily support multiple abi implementations distinct namespaces version kernel abi entry points appendix lists methods abi scheduler singularity supports compile-time replaceable scheduler implemented scheduler rialto scheduler multi-resource laxity-based scheduler round-robin scheduler implemented degenerate case rialto scheduler minimum latency round-robin scheduler minimum latency round-robin scheduler optimized large number threads communicate frequently scheduler maintains lists runable threads called unblocked list threads recently runable called preempted list runable threads pre-empted choosing thread run scheduler removes threads unblocked list fifo order unblocked list empty scheduler removes thread preempted list scheduling timer interrupt occurs threads unblocked list moved end preempted list thread running timer fired thread unblocked list scheduled scheduling timer reset net effect list scheduling policy favor threads awoken message small amount work send messages processes block waiting message common behavior threads running message handling loops avoid costly reset scheduling timer threads unblocked list inherit scheduling quantum thread unblocked combined two-list policy quantum inheritance effective singularity switch thread cycles processes singularity system lives single virtual address space virtual memory hardware protect pages mapping address space trap null pointer singularity system address space logically partitioned kernel object space object space process exchange heap channel data pervasive design decision memory independence invariant cross-object space pointers point exchange heap kernel pointers process object space process pointer process objects invariant ensures process garbage collected terminated cooperation processes kernel creates process allocating memory sufficient load executable image file stored microsoft portable executable format singularity performs relocations fixups including linking kernel abi functions kernel starts process creating thread running image entry point trusted thread startup code calls stack page manager initialize process process obtains additional address space calling kernel page manager returns unshared pages pages adjacent process existing address space garbage collectors require address space contiguous contiguous regions large objects arrays addition memory holds process code heap data process stack thread access exchange heap stack management singularity linked stacks reduce memory overhead thread stacks grow demand adding non-contiguous segments singularity compiler performs static interprocedural analysis optimize placement overflow tests compiler-inserted checks trusted code accesses system data structures residing process object space determine amount space remaining current stack segment running thread pushes stack frame potentially overflow current stack segment trusted code calls kernel method disable interrupts invokes page manager allocate stack segment code initializes stack frame segment running procedure callee call segment unlink routine deallocate segment stack popped processes run ring current stack segment leave room processor save interrupt exception frame handler switches dedicated interrupt stack exchange heap exchange heap underlies efficient communication singularity holds data passed processes figure exchange heap garbage collected counts track usage blocks memory called regions process accesses region structure called allocation allocations reside exchange heap enables passed processes owned accessible single process time allocation share read-only access underlying region allocations base bounds provide distinct views underlying data protocol processing code network stack strip encapsulated protocol headers packet copying region tracks number allocations point deallocated count falls singularity compiler hides extra level indirection allocation record strongly typing region automatically generating code dereference record threads process create additional threads untrusted verified code running process creates thread object initializes supplied function stores object unused slot run-time system thread table code invokes threadhandle create passing thread table index kernel method creates thread context hold registers allocates initial stack frame updates data structures returns process runtime calls threadhandle start schedule thread thread starts executing kernel running code calls process entry point passing thread index run-time thread table process startup code invokes function thread object starts thread execution process thread creation kernel aware address process thread startup code process entry point kernel abi methods relocated garbage collection garbage collection essential component safe languages prevents memory deallocation errors subvert safety guarantees singularity kernel processes object spaces garbage collected large number garbage collection algorithms experience strongly suggest garbage collector system application code singularity architecture decouples algorithm data structures execution process garbage collector selected accommodate behavior code process run global coordination aspects singularity make process closed environment run-time support pointers cross process kernel boundaries collectors cross-space pointers messages channels objects agreement memory layout messages data exchange heap kernel controls memory page allocation nexus coordinating resource allocation singularity run-time systems support types collectors generational semi-space generational sliding compacting adaptive combination previous collectors mark-sweep concurrent mark-sweep system code short pause times collection collector thread segregated free list eliminates thread synchronization normal case garbage collection triggered allocation threshold executes independent collection thread marks reachable objects collection collector stops thread scan stack introduces pause time microseconds typical stacks overhead collector higher non-concurrent collectors simpler non-concurrent marksweep collector applications sip collector solely responsible collection objects object space garbage collector perspective thread control enters leaves application kernel treated similarly call call-back native code conventional garbage collected environments garbage collection object spaces scheduled run 
completely independently application employs stop-theworld collector thread considered stopped respect application object space run kernel object space due kernel call thread stopped return application process space duration collection stack management garbage collected environment thread stack object potential roots collector calls kernel executed user thread stack store kernel pointers stack sight appears violate memory independence invariant creating cross-process pointers entangles user kernel garbage collections avoid problems singularity delimits boundary space stack frames garbage collector space cross-domain process kernel kernel process call singularity saves callee-saved registers special structure stack demarks crossdomain call structures mark boundary stack regions belong object space calls kernel abi pass object pointers garbage collector skip frames space delimiters facilitate terminating processes cleanly process killed threads stopped kernel throws exception skips deallocates process stack frames channels singularity processes communicate exclusively sending messages channels bidirectional behaviorally typed connection processes messages tagged collections values message blocks exchange heap transferred sending receiving process channel typed contract specifies format messages valid messages sequences channel section process creates channel invoking contract static newchannel method returns channel endpoints asymmetrically typed exporter importer output parameters exp importch imp exportch newchannel importch exportch process pass endpoints processes existing channels process receiving endpoint channel process holding endpoint application process communicate system service application creates endpoints sends request endpoint system server forwards endpoint service establishing channel process service send channel asynchronous receive synchronously blocks specific message arrives language features thread wait set messages channel wait specific sets messages channels data channel ownership passes sending process retain message receiving process ownership invariant enforced language run-time systems serves purposes prevent sharing processes facilitate static program analysis eliminating pointer aliasing messages permit implementation flexibility providing message-passing semantics implemented copying pointer passing channel implementation channel endpoints values transferred channels reside exchange heap endpoints reside process object space passed channels similarly data passed channel reside object space violate memory independence invariant message sender passes ownership storing pointer message receiver endpoint location determined current state message exchange protocol approach naturally copy implementations stack disk buffers network packets transferred multiple channels protocol stack application process copying customized run-time systems singularity architecture permits sips host completely run-time systems runtime customized process sips running sequential code support thread synchronization required sips multiple threads sips objects requiring finalization finalizers access data shared threads separate finalizer observe required language semantics finalizers sips allocation strategies pre-allocate stack-allocate memory objects obviating garbage collector sips runtime discussion safe programming languages offer advantages building reliable analyzable software immune low-level security exploits plague code practical benefits safe languages increasing popularity conventional operating systems offer special support safe programs benefit properties singularity contrast starts premise language safety builds system architecture supports enhances language guarantees singularity integrates language run-time system operating system processes safe system processes support distinct virtual machine jvm clr redundant simple approach providing homogeneous run-time system clr processes imposes unnecessary penalties services programs behavior match runtime system properties language runtimes provide services notably garbage collection interact poorly programs generational garbage collector introduce seconds-long pauses program execution disrupt media player operating system hand real-time collector suitable media player penalize computational task homogeneous environments evolve large complex expensive systems support union requirements application depends singularity supports heterogeneous execution environments process runtime system memory layout garbage collection algorithm libraries memory independence runtime tailored meet computation process garbage collector selected algorithm data structure layout awareness coordination counterparts processes heterogeneous environments provide mechanism enforce policy contents environment process circumscribe behavior device drivers run sparse environment primarily driver-specific abstractions ioports tailored class program abstractions unnecessary inappropriate drivers environment policy untrusted applications run environment security automata validate control program behavior singularity built offers model safely extending system application functionality model extensions access parent code data structures self-contained programs run independently approach increases complexity writing extension parent program developer define proper interface rely shared data structures extension developer program interface possibly re-implement functionality parent widespread problems inherent dynamic code loading argue alternatives increase isolation extension parent singularity mechanism works applications system code depend semantics api unlike domainspecific approaches nooks simple semantic guarantees understood programmers tools principal arguments singularity extension model center difficulty writing message-passing code hope programming models languages make programs type easier write verify modify advances area generally beneficial message-passing communication fundamental unavoidable distributed computing web services message passing increasingly familiar techniques improve objections programming system common finally singularity memory management hardware processors protection suggests possibility reevaluating hardware general programs functionality memory management hardware embedded systems adequately provisioned workstations servers rarely page memory inexpensive abundant large -bit address spaces reduce multiple address spaces -bit limitations singularity shows safe languages conservative sharing policies supplant process boundaries protection rings lower cost current hardware fully utilized replaced simpler mechanisms fewer performance bottlenecks tlbs singularity benefit memory protection trusted unverified base dma inherently unsafe interfaces device encapsulated virtualized system memory protection dma transfers protect system misdirected dma hardware support segmented stacks reduce compiler complexity run-time overhead mechanism programming language support singularity written sing extension spec language developed microsoft research spec extension microsoft language constructs preand post-conditions object invariants program behavior specifications statically verified boogie verifier checked compiler-inserted run-time tests sing extends language support channels low-level constructs system code developed implemented programming language extensions reasons languages support message-passing communication cases message passing relegated libraries syntactically semantically awkward grafting asynchronous operations synchronous language sing first-class support message-passing communications makes style communication sip abstractions efficient implement palatable programmers integrating feature language aspects program verified singularity constructs communication statically verified channel contracts channel contracts central singularity isolation architecture directly supported sing contract describing simple interaction channel contract message request int requires message reply int message error state start request reply error start contract declares messages request reply error message specifies types arguments contained message request reply single integer error carry values additionally message spec requires clauses restricting arguments messages tagged direction contract written exporter point view request message importer exporter reply error exporter importer qualifier messages travel directions message declarations contract specifies allowable message interactions state machine driven send receive actions state declared considered initial state interaction contract declares single state called start state action request start state export 
side channel receive request message construct reply error specifies exporter sends reply error message part start specifies interaction continues start state looping adinfinitum slightly involved portion contract network stack public contract tcpconnectioncontract message connect uint dstip ushort dstport message ready initial state state start ready readystate state readystate connect connectresult bindlocalendpoint bindresult close closed state bindresult bound invalidendpoint readystate message listen state bound listen listenresult connect connectresult close closed protocol specification contract serves purposes detect programming errors run time static analysis tool run-time monitoring drives contract state machine response messages exchanged channel watches erroneous transitions technique simple implement detects errors program execution detect liveness errors deadlock static program analysis provide stronger guarantee processes correct stuck-free program executions singularity combination run-time monitoring static verification messages channel checked channel contract detects correctness liveness problems static checker verifies safety properties statically ensure deadlock freedom plan verify contracts general static analysis based conformance checking addition singularity compiler contract determine maximum number messages outstanding channel enables compiler statically allocate buffers channel endpoints statically allocated buffers improve communication performance endpoints channels singularity manifest pair endpoints representing importing exporting sides channel endpoint type specifies contract channel adheres endpoint types implicitly declared contract contract represented class endpoint types nested types class imp type import endpoints channels contract exp type export endpoints channels contract send receive methods contract class methods sending receiving messages declared contract methods imp void sendrequest int void recvreply int void recverror exp void recvrequest int void sendreply int void senderror semantics send methods send message asynchronously receive methods block message arrives message arrives error occurs errors occur program passes contract verification check receiver message requires methods sing switch receive statement switch-receive statement code waits reply error message imported endpoint type imp void imp switch receive case reply console writeline reply break case error console writeline error break switch receive statement operates steps block set messages arrive set endpoints receive set messages bind arguments local variables switch receive patterns receive reply endpoint error endpoint case integer argument reply message automatically bound local variable switch receive construct general patterns involve multiple endpoints endpoints receive reply error messages void imp imp switch receive case reply reply console writeline replies break case error console writeline error reply break case error console writeline error reply break case channelclosed console writeline channel closed break illustrates shows wait combinations messages switch receive statement branch reply message received endpoints final case pattern channelclosed special pattern fires channel closed party messages remain received ownership order guarantee memory isolation endpoints data transferred channels blocks exchange heap resources tracked compile time static checks enforce access resources occur program points resource owned methods leak ownership resources tracked resources strict ownership model resource owned thread data structure thread point time endpoint message thread thread ownership endpoint message message receipt simplify static tracking resources pointers resources held directly local variables messages data structures tracked restrictions onerous sing means overcome storing tracked resources indirectly data structures abstraction called tref trefs tref storage cell type tref holding tracked data structure type tref signature class tref itracked public tref claims obj public acquire public void release claims newobj creating tref constructor requires object type argument caller ownership object construction site construction ownership passed newly allocated tref acquire method obtain contents tref tref full returns contents transfers ownership caller acquire tref empty release transfers ownership object caller tref tref full trefs thread-safe acquire operations block tref full trefs represent trade-off static dynamic checking tref incorrect multiple acquires turned deadlocks finalization mechanism garbage collector responsible reclaiming resource exchange heap ownership blocks memory transferred thread process message exchanges singularity allocate track blocks exchanged fashion channel system requires message arguments scalars blocks exchange heap kinds blocks exchange heap individual blocks vectors types written microsoft singularity channels exheap exheap type pointer specifies points struct exchange heap exheap type defined run-time system allocation deallocation support heap type vector exchange heap invariant exchange heap pointers process heap type exchangeable type primitive type int char enum rep struct rep structs simply structs fields exchangeable types endpoints represented individual blocks exchange heap verification verifying code executed singularity type safe satisfies memory independence invariants three-stage process sing compiler checks type safety ownership rules protocol conformance compilation singularity verifier checks properties generated msil code finally back-end compiler produce form typed assembly language enables properties checked operating system argue final stage strictly safety literally true practice programmers benefit finding mistakes early errors explained completely high level redundant verification guards errors verification compile-time reflection closed world sip incompatible reflection facilities integral part java clr environments generate invoke code run time consequence singularity support run-time reflection services compile-time reflection ctr partial substitute clr full reflection capability ctr similar techniques macros binary code rewriting aspects meta-programming multi-stage languages basic idea programs place-holder elements classes methods fields subsequently expanded generator ability produce boiler plate repetitious code template driven inspection existing program structures powerful feature singularity applications device drivers declaratively describe resource requirements ranges service channels startup code theses processes generated automatically descriptions generators written sing transforms transform pattern matching program structure code template build code elements combining enables transform analyzed checked independent code applied errors generating call non-existent method calling wrong type object detected transform respect ctr similar multi-stage languages note ctr transform part trusted computing base emit trusted code untrusted process transform drivertransform iorangetype iorange class drivercategory iorangeattribute iorangetype ioranges public readonly static drivercategory values generate static drivercategory values drivercategory implement private drivercategory ioconfig config ioconfig getconfig tracing log tracing debug config config toprint forall cindex ioranges cindex iorangetype config dynamicranges cindex transform named drivertransform generates startup code device driver declarative declaration driver resources declaration driver describes ioports requirements internal class resources ioportrange default length internal readonly ioportrange baseports ioportrange default length internal readonly ioportrange gameports internal readonly static resources values reflective private resources drivertransform matches class derives elements values field type placeholder private constructor keyword reflective denotes placeholder definition generated transform implement modifier placeholders forward enable code program refer code subsequently produced transform pattern variables transform start signs drivercategory bound resources variable matches element starts signs 
ioranges represents list fields type iorangetype derived iorange types fields order generate code element collections collection fields ioranges templates forall keyword replicates template binding collection resulting code produced transform equivalent class resources static resources values resources private resources ioconfig config ioconfig getconfig tracing log tracing debug config config toprint baseports ioportrange config dynamicranges gameports ioportrange config dynamicranges illustrates code generated transform type checked transform compiled deferring error checking transform applied case macros assignment values verifiably safe type constructed object drivercategory matches type values field singularity system built kernel sips channels language model singularity supports number conventional operating system services system singularity system consists layers hal manager drivers hal small trusted abstraction hardware ioports iodma ioirq iomemory abstractions access devices interfaces timer interrupt controller real-time clock debug console kernel debugging stub event logger interrupt exception vector bios resource discovery stack linking code written assembler assembler portions hal represent approximately trusted code system files singularity kernel manifest create bind device drivers startup kernel plug play configuration system kernel information acquired bios boot loader buses pci bus enumerate devices start device drivers pass drivers objects encapsulate access device hardware driver written safe code runs process drivers communicate parts system including network stack file system exclusively channels driver starts kernel types initialized objects enable driver communicate device objects provide safe interface checks directly accessing hardware memory mapped locations ioport interface device port registers verifies register bounds driver write read-only memory iodma access built-in dma controller legacy hardware ioirqs notify driver hardware interrupt arrives iomemory bounds checked access fixed region memory memory-mapped registers pinned dma unsafe aspect driver-device interface dma existing dma architectures provide memory protection misbehaving malicious driver program dmacapable device overwrite part memory diversity dma interfaces found good abstraction encapsulating anticipate future hardware provide memory protection dma transfers interrupt device serviced kernel masks interrupt signals driver ioirq driver process thread waiting irq event starts processing interrupt re-enables interrupt line kernel abi scheduler runs immediately interrupt handler signals events queue driver configuration singularity system makes extensive metadata describe pieces system explain fit behavior metadata singularity declaratively labels singularity component system application dependencies exports resources tools singularity metadata verify configure application system code system execution singularity system image compound artifact consists kernel device drivers applications sufficient metadata describe individual artifacts manifest declares policy system manifest points manifests describing individual component manifests software boot loader system verifier discover component singularity system singularity system image manifest sufficient enable off-line analysis system goal enable administrator description hardware devices system manifest answer questions system boot hardware drivers services initialize applications run singularity system image metadata describing device drivers metadata singularity maintains invariants singularity install device driver start successfully due resources conflicts driver portion system singularity start device driver run successfully due conflicting missing resource device driver access resources runtime declared metadata specification singularity custom attributes interleave metadata source code source document maintained custom attributes attached program entity class method field declaration compiler passes attributes resulting msil binary compilers linkers installation tools verification tools read metadata encoded attribute msil binary executing code file code shows attributes declare dependencies resource requirements trio video device driver drivercategory signature pci class trioconfig hardware resources pci config iomemoryrange default length iomemoryrange framebuffer fixed hardware resources iofixedmemoryrange base length iomemoryrange textbuffer iofixedmemoryrange base length iomemoryrange fontbuffer iofixedportrange base length ioportrange control iofixedportrange base length ioportrange advanced iofixedportrange base length ioportrange gpstat channels extensionendpoint typeof extensioncontract exp tref extensioncontract exp start iosys serviceendpoint typeof videodevicecontract exp tref serviceprovidercontract exp start video drivercategory signature attributes declare module device driver specific class pci video devices drivercategory denotes category applications implement device drivers specific hardware categories include servicecategory applications implementing software services webappcategory extensions singularity cassini web server iomemoryrange attribute declares framebuffer derived entry device pci configuration space entry discovered hardware configured hardware parameters size memory region compatibile values attribute iofixedmemoryrange iofixedportrange attributes driver fixed range address space memory mapped access fixed ranges ports access device registers extensionendpoint attribute specifies channel contract local endpoint communicate driver parent process case device drivers trio system parent process serviceendpoint attributes declares channel contract local endpoint accept incoming bind requests clients section describes system maps endpoint serviceprovidercontract system namespace compile time compile time compiler transfers custom attributes msil binary msil metadata access library singularity tools parse instruction metadata streams msil binaries link time mkmani tool reads custom attributes create application manifest application manifest xml file enumerating application components exports dependencies xml part manifest information trio device driver manifest application identity trio assemblies assembly filename trio exe assembly filename namespace contracts dll version assembly filename contracts dll version assembly filename corlib dll version assembly filename corlibsg dll version assembly filename system compiler runtime dll version assembly filename microsoft singsharp runtime dll version assembly filename ilhelpers dll version assembly filename singularity ill version assemblies drivercategory device signature pci iomemoryrange index baseaddress rangelength iomemoryrange baseaddress rangelength fixed true iomemoryrange baseaddress rangelength fixed true ioportrange baseaddress rangelength fixed true ioportrange baseaddress rangelength fixed true ioportrange baseaddress rangelength fixed true extension startstateid contractname microsoft singularityextending extensioncontract endpointend exp assembly namespace contracts serviceprovider startstateid contractname microsoftsingularity videodevicecontract endpointend exp assembly contracts drivercategory manifest installation time section application class abstraction singularity run piece code added system singularity installer installer starts metadata application manifest installer verifies application assemblies exists type memory safe verifies channel contracts implemented correctly assembly dependencies dependencies kernel abi resolved correctly theses internal properties resolved verified installer attempts resolve verify external dependencies install ensures hardware resources device driver conflict hardware resources required driver installer verifies existence type channel application application exports channel installer verifies exported channel conflict application conflicts arise policy system manifest resolves manifest declare device driver provide video console contract installation additional video drivers disallowed single video driver activated boot time section compile time reflection ctr generate trusted code initialize in-process objects referencing system resources ctr templates execute install time attributed program elements assemblies named application manifest installation process completed updating system manifest metadata incorporate application device driver current implementation entire installation process takes place offline installation visible system boot purely off-line installation trivially augmented on-line 
installation on-line installation required usage scenarios run time run time metadata drives initialization kernel device drivers services applications boot loader reads portion system manifest determine kernel device drivers services loaded order load start executing system infers dependencies application started kernel verifies resolves metadata dependencies builds process configuration record kernel trusted code emitted application ctr parses configuration record instantiates local objects accessing external resources puts local objects configuration object process object space returning trio device driver kernel records driver configuration record iomemoryrange objects framebuffer textbuffer fontbuffer kernel records ioportrange objects control advanced gpstat ports kernel creates channel connect device driver subsystem channel connect driver namespace channel endpoints added driver configuration record device driver starts executing trusted code runtime creates iomemoryrange ioportrange objects driver object space objects constructors accessible trusted runtime code device driver access resources declared metadata checked conflicts kernel subsystem declaring channel endpoints application metadata ensures important properties code singularity process statically verified ensure communicates fully declared channels strict conformance channel contracts applications global names trio video device driver unaware dev video system namespace driver local trio config video refer channel contract serviceprovidercontract entire layout namespace change affecting single line code video driver applications sandboxed conformance principle privilege remove source error security vulnerability current systems trio driver holds endpoint connected system space driver ability create names connect system process reflecting namespace facilitate access metadata reflected system namespace system creates namespace tree describing mapping device drivers current hardware hardware locations lists buses location bus location represented directory tree symbolic link device instance resides location similarly hardware registrations tree lists driver registered system tree symbolic link pointing driver registered hardware signature prefix hardware devices tree entry instance physical device system signature device determined device enumeration reflected directory structure tree instance device separate subtree symbolic links pointing entries locations drivers trees show device instance found activated hardware drivers tree lists registered driver subtree instantiation driver names based namespace driver class driver tree consists symbolic link pointing executable image driver subtree instance driver subtree holds links device instance contained space true bindings serviceprovidercontract endpoints created instance driver finally dev namespace public directory holding symbolic links serviceprovidercontract endpoints hardware drivers subtree manner application bound public knowing true driver server singularity single uniform space services system space encompasses transient system services device drivers network connections persistent store file system space implemented distinguished root server services server services register unregister hierarchical namespace discovered clients service responds requests implementing server contract extend space mount point space hierarchical client programs access service passing pathname fresh channel server sample pathnames include filesystems ntfs tcp conceptually space consists directories services directories collections directories services share common pathname prefix service active entity responds requests registered channel entire space exist single server service including identical root server register handle requests point hierarchy register deregister lookup messages subtree forwarded helper server functionality similar mount points unix file systems additional servers operate root tcp service export huge dynamic space addresses create connection demand helper server implement symbolic links slightly simplified form space contract client server contract includes registration public contract namespacecontract servicecontract message bind char path servicecontract exp start exp message ackbind message nakbind servicecontract exp start exp message notify char pathspec notifycontract imp start imp message acknotify message naknotify notifycontract imp start imp message find char pathspec message ackfind findresponse results message nakfind message success override state start success ready state ready bind ackbind nakbind ready find ackfind nakfind ready notify acknotify naknotify ready bind message path space channel passed service registered notify message passes channel receives notifications directory denoted path find message returns pathnames items namespace match path specification success message standard protocol initialize channel chronology illustrates server processes represent client service service nsc nss channels server held client service nss server registers fresh channel lookup nss register acknowledgement nsc bind fresh channel service lookup bind service lookup bind reply nsc bind reply communicate channel service file system singularity space convenient mechanism naming accessing services objects provide means persist data singularity file system service sub-tree space file system registers space service mount point services requests domain file system acts space server file system pathnames suffixes space paths foo bar singularity file system supports common abstractions operations consists directories files directories files directories support traditional operations enumeration files variable-length byte arrays clients read write arbitrary offsets files directories contract file contract permits read write operations directory contract file directory operations creation deletion attribute querying due file system role space provider operations directory enumeration lookup special messages file system contracts covered space ways integrate file system space contracts functionality significantly overlap implementation internally file system runs standard singularity process comprised types workers control worker space worker directory worker file worker control worker registers separately space fsctrl services file system creation initialization mount requests file system mounted space worker processes requests forwarded file system parent space provider importantly bind requests receives bind request space worker passes endpoint directory file worker depending type endpoint turn file directory workers endpoint service actual file system operations passed endpoints endpoints received part bind requests conceptually bound specific file system file directory requests endpoints path information file handle boxwood durably store retrieve data stable storage disks singularity modified version boxwood underlying storage system boxwood originally designed distributed storage system exported higher-level abstractions b-trees simple block interfaces demonstrate abstract interfaces storage applications built easily lower overhead building file system-like interface top boxwood difficult boxwood eliminates data manipulation concurrency consistency recovery code file system structures singularity identical boxfs files stored manipulated trees data file blocks keys block numbers directories stored manipulated b-trees data files directories keys string names metadata file system entity stored special key b-tree part boxwood significantly changed interface raw disks windows requests disk pass system calls singularity interactions disk pass channels lowest layer boxwood converted channels avoid excessive costly copying boxwood byte arrays replaced pointers data exchange heap consequence aligned file block boundaries entails copying file system disk security singularity strong isolation processes constructing security model top foundation seeks maintain system integrity control access resources application system policy installation-time mechanisms 
applications central singularity security model explained principals model made applications combinations explained section applications named hierarchical namespace trust application publishers reflected shape namespace system policy dictate applications signed microsoft publisher certificate inhabit portion namespace dedicated microsoft shape namespace differentiate groups programs based trusted system policy security enforcement singularity happen statically installation time access resources singularity occurs channels system installer control resources application access statically managing channels application requirements static manifest system installer resolves unbound channels application manifest providing application configuration instantiates channels run time static checking support least-privilege security environment application responsible local processing installer provide direct channel network dynamic access control singularity applications instantiated run time form processes process immutable identity due invocations resulted channel pairs created process initialized default identity channel endpoint passed process receiving process discover process identity peer endpoint process obtain principal messages received channel principal dynamic access control decisions resources shared principals subjects access control decisions compound entities formed identity requesting application applications invocation chain led requesting application execution cases server applications authority invocation chain suppressed trace invocation history process granularity identity results process invocation weaker stronger invoker contrast language-based security systems rely invocation stack history finer granularity stack frame typically results reduction authority environment users represented roles programs program authenticates user checking password certificate contributes resultant compound identity identity user logged remote protocol differs authenticated local smart-card handler singularity compound principals compound identities represented text strings sys login users fred apps word string represent system password login program running user fred invoking microsoft word lieu access control lists singularity access control expressions aces order define patterns principals matched expressions flexible word read files protected pattern fred running microsoft program access pattern language supports indirection common subexpressions naming hierarchy implementation facility equivalent group expansion conventional access control systems expect define policy rules aces derived hope replace large number disparate aces smaller set rules rules work structured environments file systems access resources file system subtrees adjudicated system installer make easier run-time mechanisms channel contracts subtyped messages holder endpoint send subtype tcpconnectioncontract section describe methods principal allowed listen connect subtype corresponds set permissions protocols channel establishment restricted access control check determines requestor granted permissions implied subtype channel subject run-time constraints endpoints passed freely processes passed authority send messages channel contract processes free act messages access checks access check performed case based identity holder channel holder masquerade originator default case process invocation identity process compound principal form invoker invokee process invocation additional scenarios process choose lend aspect identity process case process grant partner capability partner act joint identity respect capability case system policy service mediate access existing service adding functionality case mediator act behalf original client cases support identity inheritance specially blessing channel endpoint capability endpoint case channel bind mediator case blessed endpoint recipient inherit identity partner limited context processes holding multiple identities confused identities inappropriate fashion hope limit processes single identity cases make dealing multiple identities easy performance singularity goal dependable systems report include performance measurements answer simple numbers demonstrate architecture proposed incur performance penalty fast faster conventional architecture words practical basis build system hand paper validate goal increased dependence measuring aspect system significantly challenging performance results singularity section measurements comparing performance singularity systems systems ran amd athlon ghz nvidia nforce ultra chipset ram western digital rpm sata disk command queuing nforce ultra native gigabit nic hardware tcp offload acceleration freebsd red hat fedora core kernel version windows singularity ran concurrent mark-sweep collector kernel non-concurrent mark-sweep collector processes including drivers minimal round-robin scheduler cost cpu cycles singularity freebsd linux windows read cycle counter abi call thread yield thread wait-set ping pong message ping pong create start process table cost basic operations microbenchmarks table reports cost primitive operations singularity systems unix systems abi call clock getres windows setfilepointer singularity processservice getcyclespersecond calls operate readily data structure respective kernels unix thread tests ran user-space scheduled pthreads kernel scheduled threads performed significantly worse wait-set ping pong test measured cost switching threads process synchronization object message ping pong measured cost sending -byte message process back original process unix sockets windows named pipe singularity channel singularity system performance heavily tuned basic thread operations singularity yielding processor synchronizing threads comparable slightly faster systems singularity sip architecture cross-process operations run significantly faster mature systems calls process kernel times faster singularity call cross hardware protection boundary simple rpc-like interaction processes times faster creating process times faster systems advantages increase improve singularity thread implementation disk benchmarks quantify effect singularity architecture measured cost random sequential disk reads writes operating systems sequential tests read wrote data portion hard disk random read write tests performed operations sequences blocks disk tests single threaded performed synchronous raw test run times results averaged benchmarks ran hardware singularity benchmark communicated disk driver process channel freebsd linux system calls communicate drivers freebsd linux drivers support ataand theoretical maximum throughput singularity windows drivers support ataand theoretical maximum throughput figure shows throughput systems operations random read operations singularity performance unix variants marginally windows random write operations singularity highest performance majority block sizes interesting note systems higher throughput random write measurements random read disk drive buffers writes re-orders transferring sequential read operations windows performed significantly systems block sizes kilobytes difference systems pronounced constraint transitions number requests issued bandwidth disk systems perform performer freebsd block sizes attribute margin ata standards supported respective operating systems sequential write operations systems performer block sizes freebsd failed complete test block size bytes performance dropped operations test finish reasonable period time block sizes freebsd achieved highest performance margin worst performers block size random read performance blocksize bytes ond freebsd linux singularity windows random write performance blocksize bytes ons ond freebsd linux singularity windows sequential read performance blocks ize byt ons ond freebsd linux singularity windows sequential write performance blocks ize byt ons ond freebsd linux singularity windows figure raw disk benchmarks measurements figure show singularity competitive contemporary operating systems raw disk performance singularity disk driver highly optimized implement latest version ata specifications system performance comparable demonstrates running disk drivers sip communicating channels incur significant performance penalties singularity zero-copy channels previous disk benchmarks quantify 
cost channel operations modified disk driver execute sequential read operations driver directly channels table shows results operations block size bytes channels channels degradation table singularity performance channels block sizes sequential read performance limited fixed costs operation include issuing read request polling status bits device handling interrupt request channels impose additional overhead send receive operations read request comparison performance channels estimates cost channel operations micro benchmarks reported cost cycles send-receive operation discrepancy differences microbenchmark disk benchmark firstly disk driver elaborate select-receive pattern accommodate messages driver receive construct requires cycles finding matching pattern disk benchmark sendreceive operations transfer ownership shared heap buffer caller callee requires small amount bookkeeping work shared heap block sizes sequential read performance dominated dma transfer times measurements channel cost largely obscured experimental noise specweb benchmarks quantify overhead singularity extension mechanism realistic scenario measured performance specweb benchmark running cassini open source web server written test invoked fair amount software singularity cassini runs port microsoft net classes channels communicate singularity network stack file system cassini code largely unmodified channels communicate web extensions including benchmark code ran separate sips benchmark translated non-conformant aspects singularity tcp stack fully ipv compliant cassini fully standard compliant benchmark warm-up time reduced length execution reduced number iterations reduced server-side logging absent singularity fully implements dynamic operations benchmark workload mix standard singularity achieves ops weighted average throughput kbits contrast microsoft windows running iis web server identical hardware achieves ops weighted average throughput kbits system instability heavy load file system performance bottlenecks limit number connections singularity serve benchmark minimum acceptable rate reduced singularity score singularity average response time connections millisec comparable window time connections millisec suggests singularity benchmark score limited internal latency system sips singularity constrained file system limited throughput file system based boxwood abstractions performance problems limited singularity measured file system performance simple benchmark read randomly chosen files specweb benchmark windows throughput sec singularity throughput sec file system throughput sec system support specweb connections contrast singularity network stack bottleneck sustain transmission throughput mbits sec executable sizes memory overhead sip limits number granularity processes created system table reports size file minimal world program unix systems programs statically linked bring libraries singularity code linked full run-time system including measured bartok optimization remove unused code data amount code data singularity process roughly comparable unix program file size bytes world singularity free bsd linux windows sing table file sizes world program table reports amount virtual address space programs systems singularity process smaller systems processes exception order magnitude memory usage singularity freebsd linux windows static static sing table dynamic memory usage world program singularity hope share read-only pages run-time system similar processes reduce memory utilization accelerate process creation times approximately runtime originates executable heap allocated code read-only data read-write data vtables immutable strings system type objects moving locks immutable objects read-write items shared case address space sharable related work large amount related work divided major areas architecture system extensibility language safety defect detection architecture singularity microkernel operating system differs number respects previous microkernel systems mach spin vino exokernel microkernel operating systems partition monolithic kernel components run separate processes previous systems exception kernel extensions spin written unsafe programming language processor memory management hardware protection rings isolation mechanism singularity language safety message-passing communication isolate processes prevent access hardware resources hardware-enforced processes considerable overhead microkernel architectures evolved kernel extensions attempting protect system integrity spin implemented extensions safe language programming language features restrict access kernel interfaces vino sandboxing prevent unsafe extensions accessing kernel code data lightweight transactions control resource usage systems allowed extensions directly manipulate kernel data left open possibility corruption incorrect malicious operations inconsistent data extension failure singularity stronger extension model prevents data sharing parent extension singularity single general extension mechanism system device drivers applications specialized mechanism kernel engler exokernel defined kernel extensions packet filtering domain-specific language generated code kernel safe analyzable language approach attractive well-defined domains packet filtering difficult generalize previous operating systems written safe programming languages early examples open systems ran single address space supported threads confusingly called processes viewed single user systems paid attention security isolation fault tolerance smalltalkand lisp machine lisp dynamic typing run-time validation ensure language safety isolation depended programmer discipline subverted introspective system operations pilot cedar mesa single-user single-address space systems implemented mesa statically typed safe language inferno single address space operating system runs programs written safe programming language limbo unlike singularity supports single virtual machine image depends dynamic code loading memory failure isolation rmox operating system partially written occam architecture similar singularity system structured message-passing processes rmox kernel written oskit device drivers system process written safe language operating systems written java javaos port java virtual machine bare hardware replaces host operating system microkernel written unsafe language java code libraries unlike singularity supports single process object space system similar singularity respects microkernel system written safe language java processes share memory communicate synchronous rpc deep copying parameters processes run single hardware address space rely language safety isolation primary differences singularity communication extension mechanisms singularity asynchronous message passing strongly typed channels general rpc special case permits verification communication behavior system-wide liveness properties singularity -copy transfers channels preserving memory independence java extension model dynamic loading singularity sips closed failure isolation enables accurate program analysis facilitating code optimization defect detection device drivers common extensions operating systems largest source defects nooks protected environment run existing device drivers linux kernel memory management hardware isolate driver kernel data structures code calls protection boundary nooks runtime validates parameters tracks memory usage singularity pressure backward compatibility runs newly written drivers general sip complementary line research developed tools find defects drivers software analysis tools static driver verifier microsoft perform domain-specific analysis find errors drivers safe programming language make tools accurate enable make fewer unverified assumptions conformance language semantics application extensibility considerable interest developing mechanisms isolate extensions application software software fault isolation sfi isolates untrusted code domain inserting run-time tests validate memory indirect control transfers technique called sandboxing sandboxing high overhead offers memory safety type safety sandboxing provide mechanisms control data shared host extension finally sandboxing finds errors late execution compilation minor memory management hardware provide finer grain protection boundaries 
address space mondrian memory protection permits arbitrary access control word boundary reasonable overhead java goals strongly encouraged dynamic code loading applets required security model protect untrusted extensions jvm combines verified type-safe code fine-grain run-time access control provide environment system constrain execution general untrusted extensions singularity runs extensions separate processes provide stronger assurance isolation tractable security problem entail large number fine grain policy decisions projects implemented os-like functionality process scheduling mechanisms java runtime multiple applications run jvm process mechanisms control resource allocation facilitate cleanup failure j-kernel implemented protection domains jvm process provided revocable capabilities control object sharing developed clean semantics domain termination luna refined j-kernel runtime mechanisms extension java type system distinguishes shared data permits control sharing kaffeos process abstraction jvm mechanisms control resource utilization group processes java incorporated ideas feature called isolates similar existing concept appdomains microsoft clr singularity eliminates duplication resource management isolation mechanisms operating system language runtime providing consistent mechanism levels system singularity sips closed nonextensible greater degree isolation fault tolerance java clrbased approaches share common run-time system language safety safe programming languages recent phenomena pascal ada safe statically verifiable imperative languages moduladylan java safe object-oriented languages safe languages popular faster processors refined type systems improved run-time systems widely system implementation time space overhead higher low-level languages offer control data layout java overhead attributable language open execution environment reflection dynamic class loading constrain compiler ability globally analyze optimize code singularity eliminates features globally optimizing compiler produce object code competitive conventional unsafe languages line research led type safe dialects ccured compiler run-time system extensively analyzes code determine statically safe inserts run-time tests properties statically verified cyclone safe dialect aggressive inserting run-time tests ccured change layout structs incorporate type information cyclone reject programs inherently unsafe vault aggressive redesign introduces safe language constructs specification language explicit resource management low-level data representations retain binary compatibility rely garbage collection system depends language safety trust compiler verify safety code executes executables delivered typed intermediate languages java bytecodes microsoft msil verification straightforward process approach singularity ensure system application code type safe perform similar verification assembly language compiler augments type annotations low level unverified unsafe code potential weakness system problem systems rely memory protection singularity unsafe code lower levels language runtime operating system verifying safety code ensure system reliability area active research producing safe garbage collector defect detection tools singularity designed facilitate operation static defect detection tools analyzing systems written unsafe languages difficult languages weak guarantees provide clear semantics tool difficult analyze enforce tools languages heuristic make guarantees assumption programs violate language semantics loopholes casting pointers integers singularity compiled msil safe intermediate language clear albeit informal semantics firm basis program analysis difficulty facing defect detection tools openness environment code executes openness arises public interfaces invoked variety contexts dynamic code modification arising reflection code loading singularity annotates interfaces specifications describe functional behavior verified statically run time channels public interfaces process behavioral description protocol channel verified technique called conformance checking addition singularity processes closed compiler static analysis tool code rely remaining unchanged run time security abadi discussion related work security area conclusion singularity micro-kernel operating system advances programming languages compilers build lightweight software-isolated processes provide code protection failure isolation lower overhead conventional hardware supported processes singularity isolation boundary running verifiably safe programs preventing object pointers passing processes object spaces sips turn enable solution problem code extension systems applications singularity model extensions loaded parent process run process communicate strongly typed channels model fixes major problems extensions singularity directly access parents data interfaces fail easily terminated killing parents singularity laboratory exploring interactions system architecture programming languages compilers specification verification advances areas enable reinforce advances domains limits benefit impact studying area isolation singularity small structured make span arbitrary boundaries domains time large realistic demonstrate practical advantages techniques abadi birrell wobber access control world software diversity proceedings workshop hot topics operating systems hotos santa accetta baron bolosky golub rashid tevanian young kernel foundation unix development summer usenix conference atlanta association packaged software industry revenue growth software information industry association back hsieh lepreau processes kaffeos isolation resource management sharing java proceedings usenix symposium operating systems design implementation osdi san diego ball rajamani slam project debugging system software static analysis proceedings popl acm sigplan-sigact symposium principles programming languages portland barnes jacobsen vinter rmox raw-metal occam experiment communicating process architectures ios press enschede netherlands barnett leino schulte spec programming system overview proceedings construction analysis safe secure interoperable smart devices cassis springer verlag marseille france bershad chambers eggers maeda mcnamee pardyak savage sirer spin extensible microkernel application-specific operating system services proceedings acm sigops european workshop wadern germany bershad savage pardyak sirer fiuczynski becker eggers chambers extensibility safety performance spin operating system proceedings fifteenth acm symposium operating system principles copper mountain resort bush pincus sielaff static analyzer finding dynamic programming errors software-practice experience candea kawamoto fujiki friedman fox microreboot technique cheap recovery proceedings sixth symposium operating systems design implementation osdi san francisco chou yang chelf hallem engler empirical study operating systems errors proceedings acm symposium operating systems principles sosp alberta canada das lerner seigle esp path-sensitive program verification polynomial time proceedings acm sigplan conference programming language design implementation pldi berlin germany deline hndrich enforcing high-level protocols low-level software proceedings acm sigplan conference programming language design implementation pldi snowbird dorward pike presotto ritchie trickey winterbottom inferno operating system bell labs technical journal engler chelf chou hallem checking system rules system-specific programmer-written compiler extensions proceedings symposium operating systems design international osdi sand diego engler kaashoek toole exokernel operating system architecture application-level resource management proceedings fifteenth acm symposium operating system principles copper mountain resort evans guttag horning tan lclint tool specifications check code proceedings acm sigsoft symposium foundations software engineering orleans hndrich larus language support fast reliable message based communication singularity submitted eurosys fitzgerald knoblock ruf steensgaard tarditi marmot optimizing compiler java softwarepractice experience fitzgerald tarditi case profile-directed selection garbage collectors proceedings international symposium memory management ismm minneapolis ganger engler kaashoek brice hunt pinckney fast flexible application-level networking exokernel systems acm transactions computer systems goldberg robson smalltalkthe language implementation addison-wesley golm felser wawersich kleinoeder operating system proceedings usenix annual conference 
monterey rtig hohmuth liedtke sch nberg performance m-kernel-based systems proceedings sixteenth acm symposium operating systems principles sosp saint malo france hawblitzel chang czajkowski eicken implementing multiple protection domains java proceedings usenix annual technical conference orleans hawblitzel eicken luna flexible java protection system proceedings acm symposium operating system design implementation osdi boston hunt larus tarditi wobber broad research challenges opportunities proceedings workshop hot topics operating systems hotos santa ifip ifip dependable computing fault tolerance jim morrisett grossman hicks cheney wang cyclone safe dialect proceedings usenix annual conference monterey johnson lint program checker computer science technical report bell laboratories jones leach draves iii modular real-time resource management rialto operating system proceedings workshop hot topics operating systems hotos-v orcas island lampson sproull open operating system single-user machine proceedings seventh acm symposium operating systems principles sosp pacific grove maccormick murphy najork thekkath zhou boxwood abstractions foundation storage infrastructure proceedings sixth symposium operating systems design implementation osdi san francisco mcgraw felten java security hostile applets holes antidote john wiley sons york morrisett walker crary glew system typed assembly language acm transactions programming languages systems murphy levidow windows dependability proceedings ieee international conference dependable systems networks york necula proof-carrying code proceedings acm symposium principles programming languages paris france necula mcpeak weimer ccured type-safe retrofitting legacy code proceedings popl acm sigplan-sigact symposium principles programming languages portland paul evans net security lessons learned missed java annual computer security applications conference acsac tucson process application isolation api specification java specification request jsrrajamani rehof conformance checking models asynchronous message passing software proceedings international conference computer aided verification cav springer copenhagen denmark redell dalal horsley lauer lynch mcjones murray purcell pilot operating system personal computer communications acm saulpaugh mirho inside javaos operating system addison-wesley schneider enforceable security policies acm transactions information system security tissec seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings usenix symposium operating systems design implementation osdi seattle sreedhar burke choi framework interprocedural optimization presence dynamic class loading proceedings acm sigplan conference programming language design implementation pldi vancouver swift annamalai bershad levy recovering device drivers proceedings sixth symposium operating systems design implementation osdi san francisco swift bershad levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing swinehart zellweger beach hagmann structural view cedar programming environment acm transactions programming languages systems von behren condit zhou necula brewer capriccio scalable threads internet services proceedings nineteenth acm symposium operating systems principles sosp bolton landing wahbe lucco anderson graham efficient software-based fault isolation proceedings fourteenth acm symposium operating system principles asheville wang appel type-preserving garbage collectors proceedings acm sigplan conference programming language design implementation pldi berlin germany weinreb moon lisp machine manuel symbolics cambridge witchel cates asanovic mondrian memory protection proceedings international conference architectural support programming languages operating systems san jose appendix singularity kernel abi exposes methods general usage abi typically class library libc wrap syscalls unix systems msil verification ensures methods marked unsafe attribute accessed trusted code run-time system namespace microsoft singularity processes struct processhandle static unsafe bool create char args int arglengths int argcount processhandle handle static unsafe bool create char args int arglengths int argcount extensioncontract exp opt exheap exp processhandle handle static unsafe bool create char args int arglengths int argcount char role int rolelength extensioncontract exp opt exheap exp processhandle handle static void dispose processhandle handle static bool start processhandle handle static void join processhandle handle bool started static bool join processhandle handle timespan timeout bool started static bool join processhandle handle datetime stop bool started static bool suspend processhandle handle bool recursive static bool resume processhandle handle bool recursive static void stop processhandle handle int exitcode static void suspendbarrier static int getprocessid processhandle handle static int getexitcode processhandle handle namespace microsoft singularity services struct debugservice static unsafe void printbegin char buffer int length static unsafe void printcomplete char buffer int static unsafe void print char buffer static unsafe void print char buffer int length static void break static bool isdebuggerpresent struct deviceservice static unsafe uint getpnpsignature char output uint maxout static bool getpciconfig ushort pciaddressport ushort pcidataport ushort identifier static int getirqcount byte line static uint getdynamiciorangecount static bool getdynamicioportrange uint range ushort port ushort size bool readable bool writable static unsafe bool getdynamiciomemoryrange uint range byte data uint size bool readable bool writable static bool getdynamicioirqrange uint range byte line byte size static bool getdynamiciodmarange uint range byte channel byte size static uint getfixediorangecount static bool getfixedioportrange uint range ushort port ushort size bool readable bool writable static unsafe bool getfixediomemoryrange uint range byte data uint size bool readable bool writable static bool getfixedioirqrange uint range byte line byte size static bool getfixediodmarange uint range byte channel byte size struct endpointcore static endpointcore opt exheap allocate uint size systemtype static void free endpointcore opt exheap endpoint static void connect endpointcore opt exheap imp endpointcore opt exheap exp static void transferblockownership allocation ptr ref endpointcore target static void transfercontentownership ref endpointcore transferee ref endpointcore target static uint getprincipal endpointcore opt exheap endpoint char outprincipal uint maxout struct exchangeheapservice static unsafe uintptr getdata allocation allocation static unsafe uintptr getsize allocation allocation static unsafe uintptr gettype allocation allocation static unsafe allocation allocate uintptr size systemtype type uint alignment static unsafe void free allocation allocation static unsafe allocation share allocation allocation uintptr startoffset uintptr endoffset static unsafe allocation split allocation allocation uintptr offset struct pagetableservice static unsafe uint getpagetable static uintptr getpagecount static uint getprocesstag static uintptr allocate uintptr bytes uintptr reserve uintptr alignment static uintptr allocatebelow uintptr limit uintptr bytes uintptr alignment static uintptr allocateextend uintptr addr uintptr bytes static void free uintptr addr uintptr bytes static bool query uintptr queryaddr uintptr regionaddr uintptr regionsize struct processservice static void stop int exitcode static datetime getuptime static datetime getutctime static long getcyclecount static long getcyclespersecond static ushort getcurrentprocessid static int getstartupendpointcount static unsafe extensioncontract exp opt exheap getstartupendpoint int arg static int getstartupargcount static unsafe int getstartuparg int arg char output int maxout static unsafe void gettracingheaders logentry logbegin logentry loglimit logentry loghead byte txtbegin byte txtlimit byte txthead struct stackservice 
static void getunlinkstackrange ulong unlinkbegin ulong unlinklimit static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack static void linkstack namespace microsoft singularity threads struct autoreseteventhandle synchandle static bool create bool initialstate autoreseteventhandle handle static void dispose autoreseteventhandle handle static bool reset autoreseteventhandle handle static bool set autoreseteventhandle handle static bool setnogc autoreseteventhandle handle struct interrupthandle synchandle static bool create byte irq interrupthandle handle static bool dispose interrupthandle handle static bool ack interrupthandle handle struct manualreseteventhandle synchandle static bool create bool initialstate manualreseteventhandle handle static void dispose manualreseteventhandle handle static bool reset manualreseteventhandle handle static bool set manualreseteventhandle handle struct mutexhandle synchandle static bool create bool initiallyowned mutexhandle handle static void dispose mutexhandle handle static void release mutexhandle handle struct synchandle static bool waitone synchandle handle static bool waitone synchandle handle timespan timeout static bool waitone synchandle handle datetime stop static bool waitonenogc synchandle handle static int waitany synchandle handles int handlecount static int waitany synchandle handles int handlecount timespan timeout static int waitany synchandle handles int handlecount datetime stop struct threadhandle static bool create int threadindex threadhandle thread static void dispose threadhandle thread static void start threadhandle thread static threadstate getthreadstate threadhandle thread static timespan getexecutiontime threadhandle thread static bool join threadhandle thread static bool join threadhandle thread timespan timeout static bool join threadhandle thread datetime stop static threadhandle currentthread static uintptr getthreadlocalvalue static void setthreadlocalvalue uintptr static void sleep timespan timeout static void sleep datetime stop static void yield static void spinwait int iterations namespace microsoft singularity types struct systemtype static systemtype rootsystemtype static systemtype register long lowerhash long upperhash systemtype parent static bool issubtype systemtype child systemtype parent static unsafe bool issubtype allocation childdata systemtype parent static bool isnull systemtype 
iron file systems vijayan prabhakaran lakshmi bairavasundaram nitin agrawal haryadi gunawi andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison vijayan laksh nitina haryadi dusseau remzi wisc abstract commodity file systems trust disks work fail completely modern disks exhibit complex failure modes suggest fail-partial failure model disks incorporates realistic localized faults latent sector errors block corruption develop apply failure-policy fingerprinting framework investigate commodity file systems react range realistic disk failures classify failure policies taxonomy measures internal robustness iron includes failure detection recovery techniques show commodity file system failure policies inconsistent buggy generally inadequate ability recover partial disk failures finally design implement evaluate prototype iron file system linux ixt showing techniques in-disk checksumming replication parity greatly enhance file system robustness incurring minimal time space overheads categories subject descriptors operating systems file systems management operating systems reliability general terms design experimentation reliability keywords iron file systems disks storage latent sector errors block corruption fail-partial failure model fault tolerance reliability internal redundancy introduction disks fail commodity file systems expect years file system storage system designers assumed disks operate fail stop manner classic model disks working perfectly fail absolutely easily detectable manner fault model presented modern disk drives complex modern drives exhibit latent sector faults block set blocks inaccessible worse blocks silently corrupted finally disks exhibit transient performance problems permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee sosp october brighton united kingdom copyright acm reasons complex failures disks buggy disk controller issue misdirected write placing correct data disk wrong location interestingly failures exist today simply waiting disk technology improve remove errors errors worsen time due increasing drive complexity immense cost pressures storage industry escalated reliable ata disks desktop pcs large-scale clusters storage systems developers high-end systems realized nature disk faults built mechanisms systems handle redundant storage systems incorporate background disk scrubbing process proactively detect subsequently correct latent sector errors creating copy inaccessible blocks recent storage arrays incorporate extra levels redundancy lessen potential damage undiscovered latent errors similarly highly-reliable systems tandem nonstop utilize end-to-end checksums detect block corruption occurs technology filtered realm commodity file systems including linux file systems ext reiserfs ibm jfs windows file systems ntfs file systems pervasive home environment storing valuable non-archived user data photos home movies tax returns internet services google paper question pose modern commodity file systems react failures common modern disks answer query aggregate knowledge research literature industry field experience form model disk failure label model fail-partial failure model emphasize portions disk fail block errors data corruption model place develop apply automated failure-policy fingerprinting framework inject realistic disk faults beneath file system goal fingerprinting unearth failure policy system detects recovers disk failures approach leverages gray-box knowledge file system data structures meticulously exercise file system access paths disk characterize failure policy develop internal robustness iron taxonomy catalogs broad range detection recovery techniques output fingerprinting tool broad categorization iron techniques file system constituent data structures study focuses important substantially open-source file systems ext reiserfs ibm jfs closed-source file system windows ntfs platforms find great deal illogical inconsistency failure policy due diffusion failure handling code kernel inconsistency leads substantially detection recovery strategies similar fault scenarios resulting unpredictable undesirable fault-handling strategies discover systems implement portions failure policy incorrectly presence bugs implementations demonstrates difficulty complexity correctly handling classes disk failure observe tolerance transient failures file systems assume single temporarilyinaccessible block fatal whole-disk failure finally show file systems recover partial disk failures due lack in-disk redundancy behavior realistic disk failures leads question change file systems handle modern disk failures advocate single guiding principle design file systems don trust disk file system view disk utterly reliable component blocks corrupt file system apply measures detect recover corruption running single disk approach instance end-to-end argument top storage stack file system fundamentally responsible reliable management data metadata initial efforts develop family prototype iron file systems robust variants linux ext file system iron ext ixt investigate costs checksums detect data corruption replication provide redundancy metadata structures parity protection user data show techniques incur modest space time overheads greatly increasing robustness file system latent sector errors data corruption implementing detection recovery techniques iron taxonomy system implement well-defined failure policy subsequently provide vigorous protection broader range disk failures contributions paper define realistic failure model modern disks fail-partial model formalize techniques detect recover disk errors iron taxonomy develop fingerprinting framework determine failure policy file system analyze popular commodity file systems discover handle disk errors build prototype version iron file system ixt analyze robustness disk failure performance characteristics bring paper close discuss related work finally conclude disk failure reasons file system errors storage system section discuss common disk failure present realistic failpartial model disks discuss aspects model storage subsystem figure presents typical layered storage subsystem file system error occur layers propagate file system generic block device driver device controller firmware media transport host disk generic file system specific file system storage subsystem electrical mechanical cache figure storage stack present schematic entire storage stack top file system beneath layers storage subsystem gray shading implies software firmware white unshaded hardware bottom storage stack disk magnetic storage media mechanical motor arm assembly electrical components busses important component firmware code embedded drive control higher-level functions including caching disk scheduling error handling firmware code substantial complex modern seagate drive roughly lines code connecting drive host transport low-end systems transport medium bus scsi networks common higher-end systems fibrechannel top stack host hardware controller communicates device software device driver controls hardware block-level software forms layer providing generic device interface implementing optimizations request reordering software file system layer split pieces high-level component common file systems specific component maps generic operations data structures file system standard interface vnode vfs positioned disks fail motivate failure model describe errors layers storage stack failures media primary errors occur magnetic media classic problem bit rot occurs magnetism single bit bits flipped type problem detected corrected low-level ecc embedded drive physical damage occur media quintessential head crash culprit drive head contacts 
surface momentarily media scratch occur particle trapped drive head media dangers well-known drive manufacturers modern disks park drive head drive reduce number head crashes scsi disks include filters remove particles media errors lead permanent failure corruption individual disk blocks mechanical wear tear eventually leads failure moving parts drive motor spin irregularly fail completely erratic arm movements head crashes media flaws inaccurate arm movement misposition drive head writes leaving blocks inaccessible corrupted subsequent reads electrical power spike surge damage in-drive circuits lead drive failure electrical problems lead entire disk failure drive firmware interesting errors arise drive controller consists thousands lines real-time concurrent firmware disks return correct data circularly shifted byte memory leaks lead intermittent failures firmware problems lead poor drive performance firmware bugs well-enough field specific names misdirected writes writes place correct data disk wrong location phantom writes writes drive reports completed reach media phantom writes caused buggy misconfigured cache write-back caching enabled summary drive firmware errors lead sticky transient block corruption lead performance problems transport transport connecting drive host problematic study large disk farm reveals systems tested interconnect problems bus timeouts parity errors occurred frequency causing requests succeed slowly fail altogether transport transient errors entire drive bus controller main bus controller problematic eide controller series motherboards incorrectly completion disk request data reached main memory host leading data corruption similar problem controllers return status bits data floppy drive time hard drive observed ide protocol version problems yield corrupt data summary controller problems lead transient block failure data corruption low-level drivers recent research shown device driver code bugs rest operating system bugs crash operating system issue disk requests bad parameters data resulting data corruption fail-partial failure model discussion root failure ready put realistic model disk failure model failures manifest ways entire disk failure entire disk longer accessible permanent classic fail-stop failure block failure blocks accessible referred latent sector errors block corruption data individual blocks altered corruption insidious silent storage subsystem simply returns bad data read term model fail-partial failure model emphasize pieces storage subsystem fail discuss key elements fail-partial model including transience locality frequency failures discuss technology market trends impact disk failures time transience failures model failures sticky permanent transient temporary behavior manifests depends root problem low-level media problem portends failure subsequent requests contrast transport higher-level software issue block failure corruption operation succeed retried locality failures multiple blocks disk fail block failures dependent root block failure suggest forms block failure exhibit spatial locality scratched surface render number contiguous blocks inaccessible failures exhibit locality corruption due misdirected write impact single block frequency failures block failures corruptions occur commercial storage system developer succinctly stated disks break lot guarantees fiction frequently errors occur modeling reliability deciding failures important handle talagala patterson point disk drive manufacturers loathe provide information disk failures people industry refer implicit industry-wide agreement publicize details surprisingly actual frequency drive errors errors disk fail well-known literature previous work latent sector errors errors occur commonly absolute disk failure recent research estimates errors occur times absolute disk failures terms relative frequency block failures occur reads writes due internal error handling common disk drives failed writes sector remapped distant sector allowing drive transparently handle problems remapping imply writes fail failure component media stuttering transport lead unsuccessful write attempt move network-attached storage serves increase frequency class failures remapping succeed free blocks large scratch render blocks unwritable quickly reserved space reads problematic media unreadable drive choice return error trends areas processor performance technology market trends combine improve aspects computer systems contrast technology trends market forces combine make storage system failures occur frequently time reasons reliability greater challenge drives made increasingly dense bits packed smaller spaces drive logic complexity increases low-end drive market cost-per-byte dominates corners cut save pennies ide ata drives low-cost class drives tend tested internal machinery prevent failures occurring result field ata drives observably reliable cost pressures serve increase usage server environments finally amount software increasing storage systems noted software root errors storage system hundreds thousands lines software present lower-level drivers firmware low-level code generally type code difficult write debug source increased errors storage stack iron taxonomy section outline strategies developing iron file system file system detects recovers range modern disk failures main focus develop strategies disks common storage arrays single disk internal robustness iron needed protection file system cope failures modern disks iron file system includes machinery detect level partial faults recover level tables present iron detection recovery taxonomies note taxonomy means complete techniques exist raid variations proposed years detection recovery mechanisms employed file system define failure policy difficult discuss failure policy system iron taxonomy describe failure policy file system describe cache replacement file-layout policy levels detection level techniques file system detect problem occurred block accessed corrupted simplest detection strategy file system assumes disk works check return codes approach surprisingly common applied unintentionally errorcode pragmatic detection strategy file system implement check return codes provided lower levels storage system sanity sanity checks file system verifies data structures consistent check performed single block blocks checking single block file system verify individual fields pointers valid ranges verify type block file system superblocks include magic number older file systems pilot include header data block checking block correct type information file system guard forms block corruption checking blocks involve verifying blocks bitmap corresponds allocated blocks involve periodically scanning structures determine intact consistent similar fsck journaling file systems benefit periodic full-scan integrity checks buggy journaling file system unknowingly corrupt on-disk structures running fsck background detect recover problems redundancy final level detection taxonomy redundancy forms redundancy detect block corruption checksumming reliable systems years detect corruption recently applied improve security checksums number reasons assist detecting classic bit rot bits media flipped in-media ecc catches corrects errors checksums well-suited detecting corruption higher levels storage system stack buggy controller misdirects disk updates wrong location write block disk checksums carefully implemented detect problems specifically checksum level technique comment dzero detection assumes disk works derrorcode check return codes assumes lower level lower levels detect errors dsanity check data structures require extra consistency space block dredundancy redundancy detect corruption blocks end-to-end table levels iron detection taxonomy level technique 
comment rzero recovery assumes disk works rpropagate propagate error informs user rstop stop activity limit amount crash prevent writes damage rguess return guess wrong block contents failure hidden rretry retry read write handles failures transient rrepair repair data structs lose data rremap remaps block file assumes disk informs locale failures rredundancy block replication enables recovery forms loss corruption table levels iron recovery taxonomy stored data checksums detect misdirected phantom writes higher levels redundancy block mirroring parity error-correction codes detect corruption file system copies block reading comparing determine corrupted techniques designed correction discussed assume presence lower-overhead detection mechanism detection frequency detection techniques discussed applied lazily block access eagerly scanning disk idle time iron file systems form lazy detection additionally eager methods disk scrubbing classic eager technique raid systems scan disk discover latent sector errors disk scrubbing valuable means recovery replica exists repair nowunavailable block detect error occurred scrubbing typically leverages return codes explicitly provided disk discovers block failure corruption combined detection techniques checksums scrubbing discover block corruption levels recovery level iron taxonomy facilitates recovery block failure single disk drive techniques handle latent sector errors block corruptions simplest approach implement recovery strategy notifying clients failure occurred propagate straightforward recovery strategy propagate errors file system file system informs application error occurred assumes client program user respond appropriately problem stop recover disk failure stop current file system activity action levels granularity coarsest level crash entire machine positive feature recovery mechanism turns detected disk failures fail-stop failures preserves file system integrity crashing assumes problem transient faulty block repeatedly-accessed data script run initialization system repeatedly reboot attempt access unavailable data crash intermediate level kill process triggered disk fault subsequently mount file system read-only mode approach advantageous entire system processes continue finest level journaling file system abort current transaction approach lead system complex implement guess recently suggested rinard reaction failed block read manufacture response allowing system running spite failure negative artificial response desirable failing retry simple response failure retry failed operation retry appropriately handle transient errors wastes time retrying failure permanent repair iron file system detect inconsistency internal data structures repair fsck block pointed marked allocated bitmap freed discussed techniques context journaling file systems bugs lead corruption file system integrity remap iron file systems perform block remapping technique fix errors occur writing block recover failed reads specifically write block fails file system choose simply write block location sophisticated strategies remap entire semantic unit time user file preserving logical contiguity redundancy finally redundancy forms recover block loss simplest form replication block copies locations disk redundancy approach employs parity facilitate error correction similar raid adding parity block block group file system tolerate unavailability corruption block group complex encodings tornado codes subject worthy future exploration redundancy disk negative consequences replicas account spatial locality failure surface scratch corrupts sequence neighboring blocks copies allocated remote parts disk lower performance in-disk redundancy techniques incur high space cost desktop settings drives sufficient free space iron file system natural question file system implement detection recovery disk modern disks internal mechanisms detecting recovering errors sufficient view primary reason detection recovery file system found end-to-end argument lower-levels system implement forms fault tolerance file system implement guard forms failure file system place detect corruption data higher levels storage stack device driver drive controller reason implementing detection recovery file system file system exact knowledge blocks file system apply detection recovery intelligently block types file system provide higher level replication metadata leaving failure detection correction user data applications specific solution explore similarly file system provide machinery enable application-controlled replication important data enabling explicit performance reliability trade-off reason performance file systems storage systems unwritten contract file system lay blocks achieve high bandwidth unwritten contract stipulates adjacent blocks logical disk address space physically proximate disk-level recovery mechanisms remapping break unwritten contract performance problems file system assumes responsibility remap logically-related blocks file avoid problems complexities placing iron functionality file system techniques require persistent data structures track redundant copies parity blocks located mechanisms require control underlying drive mechanisms recover on-disk data modern drives attempt positioning reading strategies interface exists control low-level strategies current systems doesn raid make storage reliable question answered simply raid techniques provide reliable robust storage raid improve storage reliability complete solution reasons systems incorporate disk sine qua redundant storage systems desktop pcs ship single disk included cost driving force marketplace adding disk solely sake redundancy palatable solution raid protect failures higher storage stack shown figure layers exist storage subsystem file system errors occur layers file system ultimately responsible detecting recovering errors ironically complex raid controller consist millions lines code source faults depending raid system employed types disk faults handled lower-end raid controller cards checksums detect data corruption recently companies included machinery cope latent sector errors iron techniques file system single-disk systems multiple drives raid-like manner focus single-disk systems paper rich space left exploration iron file systems redundant storage arrays failure policy fingerprinting describe methodology uncover failure policy file systems main objective failure-policy fingerprinting determine detection recovery techniques file system assumptions makes underlying storage system fail comparing failure policies file systems learn file systems robust disk failures suggest improvements analysis helpful inferring iron techniques implemented effectively approach inject faults beneath file system observe file system reacts fault policy consistent file system simply run workload fail blocks accessed conclude reaction block failure fully demonstrates failure policy system file systems practice complex employ techniques depending operation performed type faulty block extract failure policy system trigger interesting cases challenge coerce file system code paths observe path handles failure requires run workloads exercising relevant code paths combination induced faults file system data structures describe create workloads inject faults deduce failure policy applied workload goal applying workloads exercise file system claim stress code path leaving avenue future work strive execute interesting internal cases workload suite sets programs run unix-based file systems fingerprinting ntfs requires set similar programs set programs called singlets focus single call file system api mkdir set generics stresses functionality common api path traversal table summarizes test suite file system test introduces special cases stressed ext inode imbalanced tree indirect doubly-indirect triply-indirect pointers support large 
files workloads ensure sufficiently large files created access structures file systems similar peculiarities make exercise -tree balancing code reiserfs type-aware fault injection step inject faults emulate disk adhering fail-partial failure model standard fault injectors fail disk blocks type oblivious manner block failed file system repeatedly injecting faults random blocks waiting uncover aspects failure policy laborious time-consuming process yielding insight key idea test file system efficient manner type-aware fault injection builds previous work semantically-smart disk systems type-aware fault injection failing blocks obliviously fail blocks specific type inode type information crucial reverse-engineering failure policy allowing discern strategies file system applies data structures disadvantage type-aware approach fault injector tailored file system tested requires solid understanding workload purpose singlets access chdir chroot stat statfs lstat open utimes read readlink exercise getdirentries creat posix api link mkdir rename chown symlink write truncate rmdir unlink mount chmod fsync sync umount generics path traversal traverse hierarchy recovery invoke recovery log writes update journal table workloads table presents workloads applied file systems test set workloads stresses single system call group invokes general operations span calls path traversal on-disk structures benefits typeawareness outweigh complexities block types file systems test listed table mechanism injecting faults software layer directly beneath file system pseudo-device driver layer injects block failures reads writes block corruption reads emulate block failure simply return error code issue operation underlying disk emulate corruption change bits block returning data cases inject random noise cases block similar expected corrupted fields software layer models transient sticky faults injecting failures file system emulate faults caused layers storage subsystem unlike approaches emulate faulty disks additional hardware imitate faults introduced buggy device drivers controllers drawback approach discern lower layers handle disk faults scsi drivers retry commands failure characterizing file systems react faults correct layer fault injection failure policy inference running workload injecting fault final step determine file system behaved determine fault affected file system compare results running fault perform comparison observable outputs system errors codes data returned file system api contents system log low-level traces recorded fault-injection layer human-intensive part process requires manual inspection visible outputs summary developed three-step fingerprinting methodology determine file system failure policy approach strikes good balance straightforward run exercises file system test workload suite roughly programs file system order block types block failed read write data corrupted file system amounts roughly relevant tests ext structures purpose inode info files directories directory list files directory data bitmap tracks data blocks group inode bitmap tracks inodes group indirect large files exist data holds user data super info file system group descriptor holds info block group journal super describes journal journal revoke tracks blocks replayed journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled reiserfs structures purpose leaf node items kinds stat item info files directories directory item list files directory direct item holds small files tail file indirect item large files exist data bitmap tracks data blocks data holds user data super info tree file system journal header describes journal journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled root internal node tree traversal jfs structures purpose inode info files directories directory list files directory block alloc map tracks data blocks group inode alloc map tracks inodes group internal large files exist data holds user data super info file system journal super describes journal journal data records transactions aggregate inode info disk partition bmap descriptor describes block allocation map imap control summary info imaps ntfs structures purpose mft record info files directories directory list files directory volume bitmap tracks free logical clusters mft bitmap tracks unused mft records logfile transaction log file data holds user data boot file info ntfs volume table file system data structures table presents data structures interest file systems test ext reiserfs jfs ntfs table list names major structures purpose note knowledge ntfs data structures incomplete closed-source system failure policy results present results failure policy analysis commodity file systems ext reiserfs version ibm jfs linux ntfs windows file system present basic background information discuss general failure policy uncovered bugs illogical inconsistencies source code explain problems discover due sheer volume experimental data difficult present results reader inspection file system studied depth present graphical depiction results showing workload blocktype pair detection recovery technique figure presents complex graphical depiction results caption interpretation details provide qualitative summary results presented figure linux ext linux ext similar classic unix file systems berkeley fast file system ext divides disk set block groups statically-reserved spaces bitmaps inodes data blocks major addition ext ext journaling ext includes set ondisk structures manage write-ahead log detection detect read failures ext primarily error codes derrorcode write fails ext record error code dzero write errors potentially leading file system problems checkpointing transaction final location ext performs fair amount sanity checking dsanity ext explicitly performs type checks blocks superblock journal blocks type checking important blocks directories bitmap blocks indirect blocks ext performs numerous sanity checks file-size field inode overly-large open detects reports error recovery detected errors ext propagates error user rpropagate read failures ext aborts journal rstop aborting journal leads readonly remount file system preventing future updates explicit administrator interaction ext retry rretry sparingly prefetch read fails ext retries originally requested block bugs inconsistencies found number bugs inconsistencies ext failure policy errors propagated user truncate rmdir fail silently important cases ext immediately abort journal failure implement rstop journal write fails ext writes rest transaction including commit block journal journal recovery file system easily corrupted ext perform sanity checking unlinkdoes check thelinkscount field modifying corrupted lead system crash finally ext redundant copies superblock rredundancy copies updated file system creation reiserfs reiserfs comprised vastly data structures ext virtually metadata data balanced tree similar database index key advantage tree structuring scalability allowing files coexist directory read failure write failure corruption ext detection j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ext recovery j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode reiserfs detection internalroot j-dataj-commit j-descj-header superdata indirectbitmap dir itemstat item reiserfs recovery internalroot j-dataj-commit j-descj-header superdata indirectbitmap dir itemstat item jfs detection imap-cntlbmap-desc aggr-inodej-data j-supersuper datainternal imapbmap 
dirinode jfs recovery imap-cntlbmap-desc aggr-inodej-data j-supersuper datainternal imapbmap dirinode figure file system failure policies tables detection recovery policies ext reiserfs jfs read write corruption faults injected block type range workloads workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery log write operations gray box workload applicable block type multiple mechanisms observed symbols superimposed key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate rredundancy rstop detection analysis reveals reiserfs pays close attention error codes reads writes derrorcode reiserfs performs great deal internal sanity checking dsanity internal leaf nodes balanced tree block header information level block tree number items free space super block journal metadata blocks magic numbers identify valid journal descriptor commit blocks additional information finally inodes directory blocks formats reiserfs checks blocks expected values fields blocks checked carefully bitmaps data blocks type information type-checked recovery prominent aspect recovery policy reiserfs tendency panic system detection virtually write failure rstop reiserfs calls panic file system crashes leading reboot recovery sequence reiserfs attempts ensure ondisk structures corrupted reiserfs recovers read write failures differently read failures reiserfs propagates error user rpropagate performs single retry rretry data block read fails indirect block read fails unlink truncate write operations reiserfs retries write failure bugs inconsistencies reiserfs exhibits inconsistencies bugs ordered data block write fails reiserfs journals commits transaction handling error rzero expected rstop lead corrupted data blocks metadata blocks point invalid data contents dealing indirect blocks reiserfs detects ignores read failure atruncate unlink updates bitmaps super block incorrectly leaking space reiserfs calls panic failing sanity check simply returning error code finally sanity type checking detect corrupt journal data replaying corrupted journal block make file system unusable block written super block ibm jfs jfs modern techniques manage data block allocation journaling scalable tree structures manage files directories block allocation unlike ext reiserfs jfs record-level journaling reduce journal traffic detection error codes derrorcode detect read failures ext write errors dzero exception journal superblock writes jfs employs minimal type checking superblock journal superblock magic version numbers checked sanity checks dsanity block types internal tree blocks directory blocks inode blocks number entries pointers block jfs checks make number maximum block type equality check field performed block allocation maps verify block corrupted recovery recovery strategies jfs vary dramatically depending block type error occurs journal superblock write jfs crashes system rstop write errors rzero block read failure primary superblock jfs accesses alternate copy rredundancy complete mount operation corrupt primary results mount failure rstop explicit crashes rstop block allocation map inode allocation map read fails error codes metadata reads handled generic file system code called jfs generic code attempts recover read errors retrying read single time rretry finally reaction failed sanity check propagate error rpropagate remount file system read-only rstop journal replay sanity-check failure replay abort rstop bugs inconsistencies found problems jfs failure policy jfs built-in redundancy expect jfs secondary copies aggregate inode tables special inodes describe file system error code returned aggregate inode read blank page returned user rguess design bug occurs read internal tree block pass sanity check bugs limit utility jfs recovery generic code detects read errors retries bug jfs implementation leads ignoring error corrupting file system windows ntfs ntfs non-unix file system study analysis requires detailed knowledge on-disk structures complete analysis figure find ntfs error codes derrorcode detect block read write failures similar ext jfs data write fails ntfs records error code dzero corrupt file system ntfs performs strong sanity checking dsanity metadata blocks file system unmountable metadata blocks journal corrupted ntfs surprisingly perform sanity checking corrupted block pointer point important system structures corrupt block pointed updated cases ntfs propagates errors rpropagate ntfs aggressively retry rretry operations fail times read failures writes number retries varies times data blocks times mft blocks file system summary present qualitative summary file systems tested table presents summary techniques file system employs excluding ntfs ext simplicity ext implements simple reliable failure policy matching design philosophy found ext family file systems checks error codes modest level sanity checking recovers propagating errors aborting operations main problem ext failure handling write errors problems including file system corruption reiserfs harm reiserfs concerned disk failure concern evident write failures induce panic reiserfs takes action ensure file system corrupted reiserfs great deal sanity type checking behaviors combine form hippocratic failure policy harm jfs kitchen sink jfs consistent diverse failure detection recovery techniques detection jfs sanity checks error codes recovery jfs redundancy crashes system retries operations depending block type fails error detection api called level ext reiser jfs dzero derrorcode dsanity dredundancy rzero rpropagate rstop rguess rretry rrepair rremap rredundancy table iron techniques summary table depicts summary iron techniques file systems test check marks higher relative frequency usage technique ntfs persistence virtue compared linux file systems ntfs persistent retrying failed requests times giving propagate errors user reliably testing ntfs needed order broaden conclusions part ongoing work technique summary finally present broad analysis techniques applied file systems detect recover disk failures concentrate techniques underused overused inappropriate manner detection recovery illogical inconsistency common found high degree illogical inconsistency failure policy file systems observable patterns figure reiserfs performs great deal sanity checking important case journal replay result single corrupted block journal corrupt entire file system jfs illogically inconsistent employing techniques scenarios similar note inconsistency problematic logically inconsistent good idea file system provide higher level redundancy data structures deems important root directory criticizing inconsistencies undesirable unintentional jfs attempt read alternate superblock read failure occurs reading primary superblock attempt read alternate deems primary corrupted estimation root illogical inconsistency failure policy diffusion code implements failure policy spread kernel diffusion encouraged architectural features modern file systems split generic specific file systems observed cases developers implement portions code implement failure policies cases reiserfs panic write failure arises due inconsistency indicative lack attention paid failure policy detection recovery bugs common found numerous bugs file systems tested found sophisticated techniques generally indicative difficulty implementing correct failure policy hints effort put testing debugging code suggestion literature helpful periodically inject faults 
normal operation part fire drill method reveals testing broad cover code paths testing indirect-block handling reiserfs observe classes fault mishandling detection error codes amazingly error codes file system common jfs found occasionally file systems testing framework part file system developer toolkit tools class error easily discovered detection sanity checking limited utility file systems sanity checking ensure metadata meets expectations code modern disk failure modes misdirected phantom writes lead cases file system receive properly formatted incorrect block bad block passes sanity checks corrupt file system file systems tested exhibit behavior stronger tests checksums recovery stop correctly file systems employed form rstop order limit damage file system types errors arose reiserfs calls panic virtually write error prevent corruption structures careful techniques write failure ext abort transaction correctly squelch writes file system leading corruption fine-grained rebooting difficult apply practice recovery stop overused downside halting file system activity reaction failure inconvenience recovery takes time requires administrative involvement fix file systems form rstop innocuous read failure occurred simply returning error requesting process entire system stops draconian reactions possibly temporary failures avoided recovery retry underutilized file systems assume failures transient lower layers system handle failures retry requests time systems employ retry generally assume read retry write retry transient faults due device drivers transport issues equally occur reads writes retry applied uniformly ntfs lone file system embraces retry issue higher number requests block failure observed recovery automatic repair rare automatic repair rarely file systems rstop technique file systems require manual intervention attempt fix observed problem running fsck detection recovery redundancy finally importantly virtually file systems include machinery detect disk failures apply redundancy enable recovery failures lone exception minimal amount superblock redundancy found jfs redundancy inconsistently jfs places copies close proximity making vulnerable spatiallylocal errors explored potentially handling failures common drives today investigate inclusion forms redundancy failure policy file system read failure write failure corruption ixt detection j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ixt recovery j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure ixt failure policy tables plot detection recovery policies ixt read write corruption faults injected block type range workloads workloads varied columns figure block types ixt file system varied rows workloads grouped manner figure key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate dredundancy rredundancy rstop iron file system describe implementation evaluation iron ext ixt ixt implement family recovery techniques commodity file systems provide increase robustness ixt applies checksums metadata data blocks pure replication metadata employs parity-based redundancy protect user data section describe implementation demonstrate robust broad class partial disk failures investigate time space costs ixt showing time costs small modest space costs reasonable performance measurements activate deactivate iron features independently understand cost approach implementation briefly describe ixt implementation explain add checksumming metadata replication user parity performance enhancement transactional checksums existing ext file system framework checksumming implement checksumming ixt borrow techniques recent research checksumming file systems specifically place checksums journal checkpoint checksums final location distant blocks checksum checksums small cached read verification current implementation shato compute checksums incorporating checksumming existing transactional machinery ixt cleanly integrates ext framework metadata replication apply similar approach adding metadata replication ixt metadata blocks written separate replica log checkpointed fixed location block group distant original metadata transactions ensure copies reach disk consistently parity implement simple parity-based redundancy scheme data blocks parity block allocated file simple design enables recover data-block failure file modify inode structure ext associate file parity block data blocks parity blocks allocated files created file modified parity block read updated respect contents improve performance file creates preallocate parity blocks assign files created transactional checksums explore idea leveraging checksums journaling file system specifically checksums relax ordering constraints improve performance updating journal standard ext ensures previous journal data reaches disk commit block enforce ordering standard ext induces extra wait writing commit block incurs extra rotational delay avoid wait ixt implements call transactional checksum checksum contents transaction placing checksum journal commit block ixt safely issue blocks transaction concurrently crash occurs commit recovery procedure reliably detect crash replay transaction checksum journal data match checksum commit block note transactional checksum crash semantics original ext iron extensions cleaning overheads note cleaning overhead large problem pure log-structured file systems major performance issue journaling file systems ixt -style checksumming replication journaling file systems incorporate cleaning on-line maintenance costs ext writes metadata journal cleans journal checkpointing data final fixed location additional cleaning performed ixt increases total traffic small amount evaluation evaluate prototype implementation ixt focus major axes assessment robustness ixt modern disk failures time space overhead additional redundancy mechanisms employed ixt robustness test robustness ixt harness fault injection framework running partial-failure experiments ixt results shown figure ixt detects read failures ext error codes lower level derrorcode metadata block read fails ixt reads replica copy rredundancy replica read fails behaves ext propagating error rpropagate stopping file system activity rstop data block read fails parity block data blocks file read compute failed data block contents rredundancy ixt detects write failures error codes derrorcode aborts journal mounts file system read-only stop writes disk rstop data metadata block read checksum contents computed compared checksum block dredundancy checksums match read error generated rpropagate read errors contents failed block read replica computed parity block rredundancy process building ixt fixed numerous bugs ext avoided cases ext commit failed transactions disk potentially corrupt file system employing checksumming detect corruption replication parity recover lost blocks ixt robust file service spite partial disk failures quantitatively ixt detects recovers partial-error scenarios induced result logical well-defined failure policy time overhead assess performance overhead ixt isolate overhead iron mechanism enabling checksumming metadata data metadata replication parity user data transactional checksumming separately combinations standard file system benchmarks ssh-build unpacks compiles ssh source distribution web server benchmark responds set static http requests postmark emulates file system traffic server tpc-b runs series debit-credit transactions simple database run experiment times present average results benchmarks exhibit broad set behaviors specifically ssh-build good albeit simple model typical action developer administrator web server read intensive concurrency postmark metadata intensive file creations deletions tpc-b induces great deal synchronous update traffic file system table reports relative performance variants ixt 
workloads compared stock linux ext numbers draw main conclusions ssh-build web server workload time overhead iron techniques enabled ssh-build indicative typical activity checksumming replication parity incurs cost similarly web server benchmark conclude read-intensive workloads suffer addition iron techniques ssh web post tpcb baseline ext table overheads ixt file system variants results running variants ixt ssh-build ssh web server web postmark post tpc-b tpcb benchmarks presented ssh-build time measures time unpack configure build ssh source tree tar source size web server benchmark transfers data http requests postmark run transactions file sizes ranging subdirectories files tpc-b run randomly generated debitcredit transactions rows vary redundancy technique implemented combinations implies metadata checksumming enabled data checksumming enabled replication metadata turned parity data blocks enabled transactional checksums results normalized performance standard linux ext interested reader running times standard ext ssh-build web postmark tpc-b seconds slowdowns greater marked bold speedups relative base ext marked brackets testing linux kernel ghz intel memory western digital wdc bbdaa disk metadata intensive workloads postmark tpc-b overhead noticeable postmark tpc-b row workloads metadata intensive results represent worst-case performance expect observe implementation metadata replication row incurs substantial cost data checksumming row user parity metadata checksums contrast incur cost rows untuned implementation ixt results demonstrate worst case costs robustness prohibitive finally performance synchronous tpc-b workload demonstrates benefits transactional checksum base case technique improves standard ext performance row combination parity checksumming replication parity reduces overhead roughly row row additional robustness checksums applied improve performance journaling file systems space overhead evaluate space overhead measured number local file systems computed increase space required metadata replicated room checksums included extra block parity allocated found space overhead checksumming metadata replication small range found parityblock overhead user files bit substantial range depending volume analyzed summary investigated family redundancy techniques found ixt greatly increases robustness file system partial failures incurring modest time space overheads work left designs implementation techniques explored understand benefits costs iron approach related work effort builds related work bodies literature file system analysis related efforts inject faults test robustness systems failure prototype iron file system draws recent efforts building software robust hardware failure discuss turn fault injection robustness testing fault-tolerance community worked years techniques injecting faults system determine robustness fiat simulates occurrence hardware errors altering contents memory registers similarly fine inject software faults operating system major difference previous work approach focuses file systems handle broad class modern disk failure modes previous work approach assumes implicit knowledge file-system block types ensure test paths file system code previous work inserts faults blind fashion uncover problems found work similar brown patterson work raid failure analysis authors suggest hidden policies raid systems worth understanding demonstrate fault injection software raid systems qualitatively failure-handling recovery policies discover failure policy target file system raid requiring complex type-aware approach recent work yang model-checking find host file system bugs techniques well-suited finding classes bugs approach aimed discovery file system failure policy interestingly approach uncovers file system bugs yang reason testing scale model-checking limited small file systems reduce run-time approach applied large file systems work builds earlier work failure injection underneath file systems work developed approach test file systems handle write failures journal updates current work extends data types read write corruption failures iron file systems work iron file systems partially inspired work google acharya suggests cheap hardware paranoid assume fail unpredictable ways google good reason treats application-level problem builds checksumming top file system disk-level redundancy drives machines drive extend approach incorporating techniques file system applications benefit note techniques complimentary application-level approaches file system metadata block inaccessible user-level checksums replicas enable recovery now-corrupted volume related approach driver hardening effort linux stated hardened driver extends realm well-written include professional paranoia features detect hardware software problems page drivers generally improve system reliability faults handled file system end-to-end argument fail-partial failure model disks understood high-end storage high-availability systems communities network appliance introduced row-diagonal parity tolerate disk faults continue operate order ensure recovery presence latent sector errors virtually network appliance products checksumming detect block corruption similarly systems tandem nonstop kernel include end-to-end checksums handle problems misdirected writes interestingly redundancy single disk instances ffs internal replication limited fashion specifically making copies superblock platters drive noted earlier commodity file systems similar provisions suggest making replicas disk raid array reduce rotational latency primary intention copies recovery storage array difficult apply techniques selective manner metadata work replication improving performance fault-tolerance future investigation iron strategies checksumming commonplace improve system security patil stein suggest implement evaluate methods incorporating checksums file systems systems aim make corruption file system data attacker difficult finally dynamic file system sun good file system iron techniques dfs checksums detect block corruption employs redundancy multiple drives ensure recoverability contrast emphasize utility replication drive suggest evaluate techniques implementing redundancy show embellish existing commodity file system dfs written scratch limiting impact conclusions commodity operating systems grown assume presence reliable hardware result case file systems commodity file systems include requisite machinery handle types partial faults expect modern disk drives time reexamine file systems handle failure excellent model operating system kernel networking subsystem network hardware long considered unreliable hardware medium software stacks designed well-defined policies cope common failure modes disks viewed fully reliable mistrust woven storage system framework challenges remain failures disks expose layers file system software architecture redesigned enable consistent well-defined failure policy kind controls exposed applications users low-overhead detection recovery techniques iron file systems employ answers questions lead understanding effectively implement generation robust reliable iron file systems acknowledgments extend steve kleiman network appliance dave anderson jim dykes seagate insights disks work fail liuba shrira shepherd dave dewitt mark hill jiri schindler mike swift anonymous reviewers members adsl excellent suggestions comments himani apte meenali rungta invaluable work implementing parity ext finally computer systems lab csl providing terrific computing environment systems research work sponsored nsf ccrccr- ngsitr- ibm network appliance emc acharya reliability cheap learned stop worrying love cheap pcs easy workshop october altaparmakov linux-ntfs project http linuxntfs sourceforge net ntfs august alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca pages denver colorado anderson drive manufacturers typically don 
talk disk failures personal communication dave anderson seagate anderson dykes riedel interface scsi ata proceedings usenix symposium file storage technologies fast san francisco california april arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance eighth workshop hot topics operating systems hotos viii pages schloss elmau germany bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca pages munich germany june bartlett spainhower commercial fault tolerance tale systems ieee transactions dependable secure computing january barton czeck segall siewiorek fault injection experiments fiat ieee transactions computers april jfs overview ibm developerworks library jfs html bitton gray disk shadowing proceedings international conference large data bases vldb pages los angeles california august brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june candea kawamoto fujiki friedman fox microreboot technique cheap recovery proceedings symposium operating systems design implementation osdi pages san francisco california december chou yang chelf hallem engler empirical study operating system errors proceedings acm symposium operating systems principles sosp pages banff canada october corbett english goel grcanac kleiman leong sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april devale koopman performance evaluation exception handling libraries proceedings international conference dependable systems networks dsngoteborg sweden june douceur bolosky large-scale study file-system contents proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages atlanta georgia dykes modern disk roughly lines code personal communication james dykes seagate august emc emc centera content addressed storage system http emc emerson essays english traits self-reliance harvard classics edited charles eliot york collier son volume foolish consistency hobgoblin minds adored statesmen philosophers divines engler chen hallem chou chelf bugs deviant behavior general approach inferring errors systems code proceedings acm symposium operating systems principles sosp pages banff canada october ghemawat gobioff leung google file system proceedings acm symposium operating systems principles sosp pages bolton landing lake george york october gibson rochberg zelenka nagle amiri chang feinberg gobioff lee ozceri riedel file server scaling network-attached secure disks proceedings joint international conference measurement modeling computer systems sigmetrics performance pages seattle washington june gray census tandem system availability technical report tandem computers green eide controller flaws version http mindprod eideflaw html february kalbarczyk iyer yang characterization linux kernel behavior error proceedings international conference dependable systems networks dsnpages san francisco california june gunawi agrawal arpaci-dusseau arpacidusseau schindler deconstructing commodity storage clusters proceedings annual international symposium computer architecture isca pages madison wisconsin june henson history unix file systems http infohost nmt val slides pdf hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference usenix winter san francisco california january hughes murray reliability security raid storage systems archives sata disk drives acm transactions storage february intel corp ibm corp device driver hardening http hardeneddrivers sourceforge net kari latent sector faults reliability disk arrays phd thesis helsinki technology september kari saikkonen lombardi detection defective media disks ieee international workshop defect fault tolerance vlsi systems pages venice italy october katcher postmark file system benchmark technical report trnetwork appliance october kleiman vnodes architecture multiple file system types sun unix proceedings usenix summer technical conference usenix summer pages atlanta georgia june lewis smart filers dumb disks nsic osd working group meeting april luby mitzenmacher shokrollahi spielman stemann practical loss-resilient codes proceedings twenty-ninth annual acm symposium theory computing stoc pages paso texas lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick joy leffler fabry fast file system unix acm transactions computer systems august mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- department computer science princeton november patil kashyap sivathanu zadok in-kernel integrity checker intrusion detection file system proceedings annual large installation system administration conference lisa atlanta georgia november patterson brown broadwell candea chen cutler enriquez fox kiciman merzbacher oppenheimer sastry tetzlaff traupman treuhaft recovery oriented computing roc motivation definition techniques case studies technical report csd- berkeley march patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june postel rfc transmission control protocol september ftp ftp rfc-editor in-notes rfc txt august prabhakaran arpaci-dusseau arpaci-dusseau model-based failure analysis journaling file systems proceedings international conference dependable systems networks dsnyokohama japan june redell dalal horsley lauer lynch mcjones murray purcell pilot operating system personal computer communications acm february reiser reiserfs namesys ridge field book scsi starch june rinard cadar dumitran roy leu william beebe enhancing server availability security failure-oblivious computing proceedings symposium operating systems design implementation osdi san francisco california december rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saltzer reed clark end-to-end arguments system design acm transactions computer systems november schindler experienced severe performance degradation identified problem disk firmware disk drives reprogrammed fix problem personal communication schindler emc july schlosser ganger mems-based storage devices standard disk interfaces square peg round hole proceedings usenix symposium file storage technologies fast pages san francisco california april schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december schwarz xin miller long hospodor disk scrubbing large archival storage systems proceedings annual meeting ieee international symposium modeling analysis simulation computer telecommunication systems mascots volendam netherlands october seltzer bostic mckusick staelin implementation log-structured file system unix proceedings usenix winter technical conference usenix winter pages san diego california january siewiorek hudak suh segal development benchmark measure system robustness proceedings international symposium fault-tolerant computing ftcstoulouse france june sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid proceedings usenix symposium file storage technologies fast pages san francisco california april sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california 
april solomon inside windows microsoft programming series microsoft press edition stein howard seltzer unifying file system protection proceedings usenix annual technical conference usenix boston massachusetts june sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january swift bershad levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing lake george york october talagala patterson analysis error behaviour large storage system ieee workshop fault tolerance parallel distributed systems san juan puerto rico april data clinic hard disk failure http dataclinic harddisk-failures htm transaction processing council tpc benchmark standard specification revision technical report tsai iyer measuring fault tolerance ftape fault injection tool international conference modeling techniques tools computer performance evaluation pages september tweedie journaling linux ext file system fourth annual linux expo durham north carolina wehman den haan enhanced ide fast-ata faq http thef-nym sci kun cgi-pieterh atazip atafq html weinberg solaris dynamic file system http members visi net thedave sun dynfs pdf wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february yang twohey engler musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october 

abstract paper describes network-attached secure disk nasd storage architecture prototype implementations nasd drives array management architecture filesystems built prototype nasd scalable storage bandwidth cost servers primarily transferring data peripheral networks scsi client networks ethernet increasing dataset sizes attachment technologies convergence peripheral interprocessor switched networks increased availability on-drive transistors motivate enable architecture nasd based main principles direct transfer clients secure interfaces cryptographic support asynchronous non-critical-path oversight variably-sized data objects measurements prototype system show services costeffectively integrated generation disk drive asic end-to-end measurements prototype drive filesystems suggest nasd support conventional distributed filesystems performance degradation importantly show scalable bandwidth nasd-specialized filesystems parallel data mining application nasd drives deliver linear scaling clientdrive pair tested pairs lab keywords file systems management distributed systems input output data communications introduction demands storage bandwidth continue grow due rapidly increasing client performance richer data types video data-intensive applications data mining storage subsystems deliver scalable bandwidth linearly increasing application bandwidth increasing numbers storage devices client processors data striped disks network links patterson technology office engineering data processing shops sufficient numbers disks scalable switched networking access storage storage controller distributed fileserver bottlenecks bottlenecks arise single server computer receives data storage peripheral network forwards client local area network adding functions concurrency control metadata consistency variety research projects explored techniques scaling number machines enforce semantics controllers fileservers cabrera hartman cao drapeau anderson lee thekkath section shows scaling number machines devoted store-and-forward copying data storage client networks expensive paper makes case scalable bandwidth storage architecture network-attached secure disks nasd separates management filesystem semantics store-and-forward copying evolving interface commodity storage devices scsiperhaps eliminate server resources required solely data movement earlier generations scsi nasd interface simple efficient flexible support wide range filesystem semantics multiple generations technology demonstrate nasd architecture deliver scalable bandwidth describe prototype implementation nasd storage manager nasd arrays simple parallel filesystem delivers scalable bandwidth parallel data-mining application figure illustrates components nasd system sections describing continue section discussion storage system architectures related research section presents enabling technologies nasd section presents overview nasd interface implementation performance section discusses ports nfs afs filesystems nasd-based system implementation nasd array management system simple parallel filesystem o-intensive data mining application exploits bandwidth prototype section reports scalability prototype compared performance fast single nfs server section discusses active disks logical extension nasd execute application code section concludes discussion ongoing research cost-effective high-bandwidth storage architecture garth gibson david nagle khalil amiri jeff butler fay chang howard gobioff charles hardin erik riedel david rochberg jim zelenka school computer science department electrical computer engineering carnegie mellon pittsburgh garth asplos cmu proceedings conference architectural support programming languages operating systems copyright association computing machinery permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copyrights components work owned acm honored abstracting credit permitted copy republish post servers redistribute lists requires prior specific permission fee request permissions dept acm permissions acm background related work figure illustrates principal alternative storage architectures local filesystem distributed filesystem dfs built directly disks distributed filesystem built storage subsystem network-dma distributed filesystem distributed filesystem smart object-based disks nasd distributed filesystem level objects storage management simplest organization aggregates application file management naming directories access control concurrency control low-level storage management disk data makes trip simple peripheral area network scsi fibrechannel disks offer fixed-size block abstraction stand-alone computer systems widely understood organization share data effectively computers intermediate server machine introduced server offers simple file access interface clients organization distributed filesystem server processes data behalf clients organization distributed database organization data makes network trip client server machine bottleneck serves large numbers disks amortize cost limitations single central fileserver widely recognized companies auspex network appliance attempted improve file server performance specifically number clients supported special purpose server hardware highly optimized software hitz hitz topic paper nasd architecture improve client-load-bearing capability traditional filesystems off-loading simple data-intensive processing nasd drives gibson transparently improve storage bandwidth reliability systems interpose computer raid controller patterson organization adds peripheral network transfer store-and-forward stage data traverse provided distributed filesystem reorganized logically dma data copy server fourth organization reduces number network transits data organization examined extensively drapeau long hpss implementation mass storage model watson miller organization applies systems clients trusted maintain filesystem metadata integrity implement disk striping redundancy hartman anderson case client caching metadata reduce number network transfers control messages data disks attached client machines presumed independently paid generally idle eliminates additional store-and-forward cost clients idle eliminating copy section nasd architecture embeds disk management functions device offers variable-length object storage interface organization file managers enable repeated client accesses specific storage objects granting cachable capability figure overview scalable bandwidth nasd system major components annotated layering logical components innermost box shows basic nasd drive section larger box essentials nasd-based filesystem adds file manager client detailed section finally outer box adds storage manager coordinate drives parallel filesystem built discussed section net protocol controller net hardware hda nasd object system filesystem access control namespace consistency net protocol net hardware file manager switch client nasd storage manager striping concurrency control mapping redundancy net protocol net hardware management read write access control section section section security layout nasdnasd nasd nasd nasd driver net protocol net hardware application filesystem data control travels network expensive store-and-forward computer idea simple disk-like network-attached storage server building block high-level distributed filesystems long time cambridge universal file server ufs abstraction similar nasd directory-like index structure birrell ufs reclaim space reachable root index successor project cambridge cfs performed automatic reclamation added undoable period time initiation transactions filesystem interface mitchell minimize coupling file manager device implementations nasd offers powerful semantics automatic reclamation transaction rollback object interface storage fixedblock interface moves data layout management disk addition nasd partitions variable-sized groupings objects physical regions disk media enabling total partition space managed easily manner similar virtual volumes virtual disks ieee lee specific implementations exploit nasd uninterpreted filesystem-specific attribute fields respond higher-level capacity planning reservation systems attribute-managed storage golding object-based storage pursued quality-of-service device transparent performance optimizations drive supported data sharing anderson isi netstation project vanmeter proposes form object-based storage called derived virtual devices dvd state open network connection augmented 
access control policies object metadata provided file manager kerberos neuman underlying security guarantees similar nasd mechanism nasd access control policies embedded unforgeable capabilities separate communication state interpretation persists objects connection terminated netstation dvd physical partition server visa vanmeter similar nasd singleobject server parallel distributed filesystem contrast isi approach nasd security based capabilities well-established concept regulating access resources dennis past systems capabilities rely hardware support trusted operating system kernels protect system integrity wulf wilkes karger nasd make assumptions integrity client properly maintain capabilities utilize cryptographic techniques similar iscap gong amoeba tanenbaum systems entity issuing capability entity validating capability share large amount private information issued capabilities systems generally implemented single entities issuing validating capabilities nasd functions distinct machines per-capability state exchanged issuer validator offer disk striping redundancy nasd layer nasd interface organization storage manfigure evolution storage architectures untrusted networks clients boxes computers horizontal lines communication paths vertical lines internal external interfaces lan local area network ethernet fddi peripheral area network scsi fibrechannel ibm escon san emerging system area network servernet myrinet fibrechannel ethernet common clients servers devices disk capable functions seek read write readahead simple caching object store binds blocks variable-length objects manages layout objects storage space offered device file manager naming directory hierarchies consistency access control concurrency control nasd storage management recursion object interface san local filesystem distributed distributed raid controller dma-based dfs nasd based dfs file manager disk lan lan pan lan pan pan pan pan bulk data transfer san san read write computers data control path application nasd-cheops-based dfs read write object store san san lan ager replaces file manager capability set capabilities objects make highlevel striped object costs additional control message equipped capabilities clients access storage objects directly redundancy striping objects accessible client set capabilities physical disk addresses storage management system cheops differs storage subsystems scalable processing power swift tickertaip petal long cao lee cheops client processing power scaling computational power storage subsystem cheops similar zebra xfs filesystems client trust required client manipulates objects access hartman anderson enabling technology storage architecture ready change result synergy overriding factors bound applications drive attachment technologies excess ondrive transistors convergence peripheral interprocessor switched networks cost storage systems o-bound applications traditional distributed filesystem workloads dominated small random accesses small files sizes growing time dramatically baker tpc contrast workloads o-bound including data types video audio applications data mining retail transactions medical records telecommunication call records drive attachment technology technology improvements increasing disk density year driving disk bandwidth year grochowski high transfer rates increased pressure physical electrical design drive busses dramatically reducing maximum bus length time people building systems clustered computers shared storage reasons storage industry moving encapsulating drive communication fibrechannel benner serial switched packet-based peripheral network supports long cable lengths ports bandwidth impact nasd evolve scsi command set encapsulated fibrechannel full advantage promises switched-network technology higher bandwidth increased flexibility excess on-drive transistors increasing transistor density inexpensive asic technology allowed disk drive designers lower cost increase performance integrating sophisticated special-purpose functional units small number chips figure shows block diagram asic heart quantum trident drive drive asic technology advances micron cmos micron cmos insert mhz strongarm microcontroller leaving gate-equivalent space functions onchip dram cryptographic support major jump siemen tricore integrated microcontroller asic architecture promises deliver mhz -way issue next-generation asic micron technology figure quantum trident disk drive features asic left integrated chip independent clock domains function units total logic gates sram disk formatter scsi controller ecc detection ecc correction spindle motor control servo signal processor sram servo data formatter spoke dram controller microprocessor port connected motorola class processor advancing higher asic density die area accommodate mhz strongarm microcontroller space left dram additional functional units cryptographic network accelerators current trident asic micron micron frees insert micron strongarm risc fits cache mhz dhrystone mips frees kgates cryptography network support -bit datapath onchip dram customer defined logic tricore convergence peripheral interprocessor networks scalable computing increasingly based clusters workstations contrast special-purpose highly reliable low-latency interconnects massively parallel processors paragon cosmic cube clusters typically internet protocols commodity lan routers switches make clusters effective lowlatency network protocols user-level access network adapters proposed adapter card interface virtual interface architecture standardized maeda wilkes boden horst voneicken intel developments continue narrow gap channel properties peripheral interconnects network properties client interconnects sachs make fibrechannel gigabit ethernet alike cost-ineffective storage servers high performance distributed filesystems high cost overhead server machine manages filesystem semantics bridges traffic storage network client network anderson figure illustrates problem bandwidth-intensive applications terms maximum storage bandwidth based cost peak performance estimates compare expected overhead cost storage server fraction raw storage cost servers built high-end components overhead starts server-attached disk assuming dual -bit pci busses deliver byte memory high-end server saturates disks network interfaces disk interfaces overhead cost low cost server cost effective disk suffers cost overhead bit pci bus limit disk system suffers cost overhead accurately anticipate marginal increase cost nasd current disks estimate disk industry happy charge bound reduction server overhead costs factor total storage system cost neglecting network infrastructure network-attached secure disks network-attached secure disks nasd enable cost-effective bandwidth scaling nasd eliminates server bandwidth bottleneck modifying storage devices transfer data directly clients repartitions traditional file server database functionality drive client server shown figure nasd presents flat space variable-length objects simple implemented efficiently flexible wide variety applications highest levels distributed filesystem functionality global naming access control concurrency control cache coherency vary significantly advocate storage devices subsume file server residual filesystem file manager define manage high-level policies nasd devices implement simple storage primitives efficiently operate independently file manager broadly define nasd storage exhibits properties direct transfer data transferred drive client indirection store-and-forward file server machine asynchronous oversight define asynchronous oversight ability client perform operations synchronous appeal file manager frequently consulted infrequently changed policy decisions authorization decisions encoded capabilities file manager subsequently enforced drives cryptographic integrity attaching storage network open drives direct attack adversaries apply cryptographic techniques defend potential attacks drives ensure commands data tampered generating verifying cryptographic keyed digests essentially requirement security proposed ipv deering disk interface network system memory processor unit interface figure cost model traditional server architecture simple model machine serves set 
disks clients set disk wide ultra ultra scsi network fast gigabit ethernet interfaces peak bandwidths neglecting host cpu memory bottlenecks estimate server cost overhead maximum bandwidth sum machine cost costs sufficient numbers interfaces transfer disks aggregate bandwidth divided total cost disks prices date basic problem high server overhead remain report pairs costs bandwidth estimates left show values low cost system built high-volume components show values high-performance reliable system built components recommended mid-range enterprise servers pricewatch ethernet seagate ultra scsi medallist cheetah object-based interface drives direct knowledge relationships disk blocks minimize security overhead drives export variable length objects fixed-size blocks improves opportunities storage self-management extending disk understanding relationships blocks disk anderson nasd interface experiments presented paper constructed simple capability-based object-store interface documented separately gibson interface requests including read write object data read write object attributes create remove object create resize remove partition construct copy-on-write object version set security key figure diagrams components nasd request illustrates layering networking security based loosely inode interface unix filesystem mckusick interface soft partitions control objects per-object attributes preallocation clustering capability revocation resizeable partitions capacity quotas managed drive administrator objects well-known names structures configuration bootstrap drives partitions enable filesystems find fixed starting point object hierarchy complete list allocated object names object attributes provide timestamps size capacity reserved objects linked clustering dejonge logical version number object changed filesystem immediately revoke capability temporarily permanently finally uninterpreted block attribute space file manager record long-term per-object state filesystem access control lists mode bits nasd security based cryptographic capabilities documented earlier gobioff capabilities protected small number keys organized four-level hierarchy primary keys manage key hierarchy construct capabilities clients clients obtain capabilities file manager secure private protocol external nasd capability public private portion public portion description rights granted object private portion cryptographic key generated keyed message digest bellare public portion drive secret keys drive verifies client perform operation confirming client holds private portion capability client sends public portion request generates digest request parameters keyed private field drive keys receives public fields capability request current version number object compute client private field client compute file manager drive secret keys field changed including object version number access fails client back file manager mechanisms ensure integrity requests presence attacks simple accidents protecting integrity privacy data involves cryptographic operations data potentially expensive software implementations operating disk rates computational resources expect disk schemes based multiple des function blocks hardware implemented tens thousands gates operate faster disk data rates verbauwhede knudsen measurements reported paper disabled security protocols prototype support hardware prototype implementation implemented working prototype nasd drive software running kernel module digital unix nasd prototype drive runs dec alpha mhz digital unix seagate medallist disks attached scsi busses bulky drive performance year machine similar predict drive controllers physical drives managed software striping driver approximate rates expect modern drives prototype code intended operate directly drive nasd object system implements internal object access cache disk space management modules total lines code interacts minimally digital unix communications prototype dce rpc udp implementation networking services heavyweight protocol suite implementation issue active research anderson anderson vanmeter figure shows disks baseline sequential access bandwidth function request size labeled raw read write test measures latency request drives write-behind caching enabled figure packet diagram major components nasd request current prototype details nasd objects requests security documented separate papers gibson gobioff request digest nonce request args capability security header rpc header network header key security options handling request includes approved logical version number access rights expiration time accessible object region protects replayed delayed requests digest data write actual completion time measured accurately resulting write throughput appears exceed read throughput evaluate object access performance modified prototype serve nasd requests user-level process machine rpc compared performance local filesystem variant berkeley ffs mckusick figure shows apparent throughput function request size nasd ffs roughly comparable ffs strange write-behind implementation principle differences nasd tuned disk access versus reads miss cache ffs tuned cache accesses fewer copies give versus reads hit memory cache scalability figure demonstrates bandwidth scalability nasd prototype satisfying requests cache experiment nasd drives linked ocatm client machines dec alphastation mhz digital unix client issues series sequential read requests striped nasds figure nasd deliver cache rpc protocol stack dce rpc push atm link receiving client saturates commodity nasd drives costly rpc mechanism test show simple access pattern nasd array deliver scalable aggregate bandwidth computational requirements prototype drive software baseline estimate computational power needed drive microcontroller support basic nasd functions atom code annotation tool srivastava alpha on-chip counters measure code paths read write operations measurements reported total instructions columns table byte requests measurements dcpi anderson show prototype consumes cycles instruction cpi reasons numbers predict drive performance approximate prototype alpha processor cpi properties embedded processor estimates neglect poorer cpi copying hardware assist real drive communications implementation expensive drive protocol stack numbers broadly addressing question implementing nasd drive asic table shows mhz version prototype figure nasd prototype bandwidth comparing nasd local filesystem ffs raw device sequential reads writes raw device stripes data units disks separate scsi bus response timing user-level process issuing single request amount data raw disk readahead effective requests smaller miss cases metadata cached cached accesses ffs benefits data copy nasd code exhibit degradation processor cache overflows nasd extra copy makes severe strange write performance ffs occurs acknowledges immediately writes writebehind waits disk media updated test nasd write-behind fully enabled disks request size request size ppa nasd read miss ffs read miss ffs read raw read nasd read nasd write miss ffs write miss ffs write raw write nasd write figure prototype nasd cache read bandwidth read bandwidth obtained clients accessing single large cached file striped nasd drives stripe unit shown client idle values limiting factor cpu power clients range ndw average client idle ent clients msecs small request spent communications codepath requests estimate msec work communications comparison examined seagate barracuda drive executing sequential reads important operation current drives large fraction operation directly handled hardware single sector reads barracuda takes msecs reads takes msecs conclude nasd control necessarily expensive workstation-class implementations communications vanmeter filesystems nasd shown previous section nasd drives attached clients high-bandwidth 
switched network capable scalable aggregate bandwidth client applications access data directly access data higher-level distributed filesystems demonstrate feasibility nasd ported popular distributed filesystems nfs afs sandberg howard nasd environment implemented minimal distributed filesystem structure passes scalable bandwidth network-attached storage applications scalability file managers traditionally meant increasing number clients supported total amount storage increased topic scope paper addressed howard anderson gibson thekkath scale achievable bandwidth increasing storage capacity requires simply attaching storage network application issues sufficiently large requests nfs afs break requests small transfer units limit number requests concurrently issued storage pragmatically class disk devices requires high sales volumes cost effectiveness filesystems effective performance fail strategy evolving existing systems architecture nasd drives traditional distributed filesystems penalty customers moved nasd-optimized filesystems bandwidth primary concern nfs afs nasd environment nasd-adapted filesystem files directories stored nasd objects mapping files directories objects depends filesystem nfs afs ports simple approach file directory occupies nasd object offsets files offsets objects common file attributes file length modify time correspond directly nasd-maintained object attributes remainder file attributes owner mode bits stored object uninterpreted attributes filesystem makes policy decisions based file attributes client directly modify object metadata commands impact policy decisions quota access rights file manager combination stateless server weak cache consistency filesystem management mechanisms make porting nfs nasd environment straightforward datamoving operations read write attribute reads getattr directed nasd drive requests handled file manager capabilities piggybacked file manager response lookup operations file attributes computed nasd object attributes modify times object size stored uninterpreted filesystem-specific attribute table measured cost estimated performance read write requests instruction counts distribution obtained instrumenting prototype atom alpha on-chip counters values shown total number instructions required service request size include communications dce rpc udp nasd code including kernel work behalf measured number cycles instruction cpi -byte requests larger requests processor copying implementation suffers significantly real disks substantial hardware assistance shown figure set columns instruction counts estimate duration operation mhz processor assuming cpi instructions including copying communications instructions comparison purposes experimented seagate barracuda drive read sequential sector cache msec read random single sector media msec requests reads cache msec media random location msec write warm cache means needed metadata cache operation starts operation total instructions communications operation time msec mhz cpi request size read cold cache read warm cache write cold cache write warm cache mode uid gid afs complex distributed filesystem personality readily mapped nasd environment nfs data-moving requests fetchdata storedata attribute reads fetchstatus bulkstatus directed nasd drives requests file manager afs clients perform lookup operations parsing directory files locally obvious operation piggyback issuing capabilities afs rpcs added obtain relinquish capabilities explicitly afs sequential conistency provided breaking callbacks notifying holders potentially stale copies write capability issued nasd file manager longer write operation arrived drive inform clients write occur issuing callbacks file outstanding write capability blocked expiration times set file manager capability ability directly invalidate capabilities file managers bound waiting time callback afs requires enforcement per-volume quota allocated disk space difficult nasd quotas logically managed file manager write file manager accessed write nasd byte range restriction capabilities file manager create write capability escrows space file grow selecting byte range larger current object capability relinquished file manager expired file manager examine object determine size update quota data structures appropriately nfs afs ports straightforward specifically transfers remain small directory parsing nfs server afs server concurrency limitations caused coroutine-based userlevel threads package primary goal demonstrate simple modifications existing filesystems nasd devices performance loss andrew benchmark howard basis comparison found nasd-nfs nfs benchmark times configurations drive client drives clients gibson report afs numbers afs server severe concurrency limitations make comparison unfair parallel filesystem nasd clusters fully exploit potential bandwidth nasd system higher-level filesystems make large parallel requests files striped multiple nasd drives illustrated figure layered approach filesystem manage logical object store provided storage management system called cheops cheops exports object interface underlying nasd devices maintains mapping higher-level objects objects individual devices prototype system implements cheops client library translates application requests manages levels capabilities multiple nasd drives separate cheops storage manager possibly co-located file manager manages mappings striped objects supports concurrency control multi-disk accesses cheops client manager lines code provide support parallel applications implemented simple parallel filesystem nasd pfs offers sio low-level parallel filesystem interface corbett employs cheops storage management layer mpich communications parallel applications mpi cheops dce rpc mechanism required nasd prototype evaluate performance cheops parallel data mining system discovers association rules sales figure nasd-optimized parallel filesystem nasd pfs conjunction mpi parallel applications cluster workstations filesystem manages objects directly backed data backed storage manager cheops redirects clients underlying component nasd objects parallel filesystem extends simple unix filesystem interface sio low-level interface corbett inherits service directory hierarchy access controls filesystem parallel access control namespace consistency net protocol net hardware parallel cheops net protocol application file manager client storage manager cheops manager concurrency control mapping redundancy net protocol net hardware management read write access control nasd nasdnasdnasd net hardware mpi switch object object object object object object transactions agrawal application goal discover rules form customer purchases milk eggs purchase bread store layout inventory decisions full scans data determining items occur transactions -itemsets information generate pairs items occur -itemsets larger groupings k-itemsets subsequent passes parallel implementation avoids splitting records boundaries simple round-robin scheme assign chunks clients client implemented producer threads single consumer producer threads read data requests stripe unit cheops objects configuration consumer thread performs frequent sets computation maintaining set itemset counts combined single master client threading maximizes overlapping storage utilization figure shows bandwidth scalability bound phases generation -itemsets processing sales transaction file single nasd drive array scales linearly nasd drives comparison show bandwidth achieved nasd pfs fetches single higher-performance traditional nfs file cheops nasd object nfs file server alphastation mhz digital unix ocatm links half clients communicate link seagate cheetah disks attached wide ultrascsi busses optimal code machine internally read disks raw disk interface show application throughput lines server line marked nfs-parallel shows performance client reading individual file independent disk achieves performance results show nfs server network bandwidth disk bandwidth perfect sequential access pattern disk loses potential performance cpu interface limits comparison nasd 
achieve raw underlying dual medallists finally nfs line comparable nasd line shows bandwidth clients read single nfs file striped disks configuration slower nfs-parallel prefetching heuristics fail presence multiple request streams single file summary nasd pfs cheops delivers bandwidth nasd drives application powerful nfs server fails deliver half performance underlying cheetah drives active disks recent work group focused logical extension exploiting growing amount on-drive computation providing full application-level programmability drives call active disks riedel acharaya generation storage devices execution environment directly individual drives code execute data interconnect network capability customize functionality specific data-intensive applications extending object notion basic nasd interface include code specialized methods accessing operating data type natural tie computation data scale capacity added system nasd enables type extension functionality time object-based interface sufficient knowledge data individual devices resort external metadata explored data mining multimedia applications active disks applications examined frequent sets computation discussed active disk experiments distribute sales transaction data set drives reading data network set clients itemset counting core frequent sets counting code executed directly inside individual drives advantage excess computational power drives completely eliminates client nodes application prototype drives discussed approximate active disks functionality achieve low-bandfigure scaling parallel data mining application aggregate bandwidth computing frequent sets sales transactions shown nasd line shows bandwidth clients reading single nasd pfs file striped drives scales linearly nfs configurations show maximum achievable bandwidth number disks fast nasd clients spread ocatm links comparable nfs line shows performance clients reading single file striped disks server bottlenecks configuration poor read-ahead performance inside nfs server add nfs-parallel line client reads replica file independent disk server configuration performs single file case raises maximum bandwidth nfs number disks nasd nfs nfs-parallel width ethernet networking hardware nasd pfs tests figure exploration active disks begun potential applications dramatic conclusions scalable storage bandwidth clusters achieved striping data storage devices network links provided switched network sufficient bisection bandwidth exists cost workstation server network adapters peripheral adapters generally exceeds cost storage devices increasing total cost cost simply buying storage presented promising direction evolution storage transfers data directly client network dramatically reduces cost overhead scalable network-attached storage defined properties support direct device-to-client transfers provide secure interfaces cryptography support asynchronous oversight file managers provide clients capabilities issue authorized commands directly devices fourth devices serve variable-length objects separate attributes fixed-length blocks enable self-management avoid trust client operating systems demonstrate concepts design implementation nasd prototype manages disks efficiently unix filesystem measurements prototype show microprocessor cores embedded asic modern disk drive provide adequate on-drive support nasd provided cryptographic hardware support security functions simple parallel distributed filesystem designed nasd show nasd architecture provide scalable bandwidth report experiments data mining application achieve client-drive pair system drives providing addition describe conventional distributed filesystems nfs afs ported nasd performance comparable current server-based systems acknowledgements mike leis quantum trident chip diagram figure paul mazaitis heroic efforts prototype environment configured keeping running dan stodolsky bill courtright joan digney greg ganger tara madhyastha todd mowry john wilkes ted wong anonymous reviewers taking valuable time provide comments improved paper members nsic working group network-attached storage dave anderson mike leis john wilkes conversations site visits finally members parallel data lab make research enjoyable research sponsored darpa ito darpa order issued indian head division nswc contract project team indebted generous contributions member companies parallel data consortium including hewlettpackard laboratories intel quantum seagate technology storage technology wind river systems corporation compaq data general clariion symbios logic acharya acharaya active disks acm asplos oct agrawal agrawal srikant fast algorithms mining association rules vldb sept anderson anderson serverless network file systems acm tocs feb anderson anderson continuous profiling cycles acm sosp oct anderson anderson network attached storage research nsic nasd meetings html march anderson anderson network attached storage research nsic nasd meetings html june anderson anderson cheating bottleneck network storage trapeze myrinet usenix june baker baker measurements distributed file system acm sosp oct bellare bellare canetti krawczyk keying hash functions message authentication crypto benner benner fibre channel gigabit communications computer networks mcgraw hill birrell birell needham universal file server ieee tse sept boden boden myrinet gigabit-per-second local area network ieee micro feb cabrera cabrera long swift distributed disk striping provide high data rates computing systems fall cao cao tickertaip parallel raid architecture acm isca corbett corbett proposal common parallel file system programming language scalable initiative caltech cacr nov deering deering hinden internet protocol version specification rfc dec dejonge dejonge kaashoek hsieh logical disk approach improving file systems acm sosp dec dennis dennis van horn programming semantics multiprogrammed computations cacm drapeau drapeau raid-ii high-bandwidth network file server acm isca gibson gibson file server scaling network-attached secure disks acm sigmetrics june gibson gibson filesystems networkattached secure disks cmu-cs- july gobioff gobioff gibson tygar security network attached storage devices cmu-cs- oct golding golding shriver sullivan wilkes attribute-managed storage workshop modeling specification san antonio oct gong gong secure identity-based capability system ieee symp security privacy grochowski grochowski hoyt future trends hard disk drives ieee trans magnetics hartman hartman ousterhout zebra striped network file system acm sosp dec hitz hitz unix component lightweight distributed kernel multiprocessor file servers winter usenix hitz hitz lau malcolm file systems design nfs file server appliance winter usenix january horst horst tnet reliable system area network ieee micro feb howard howard scale performance distributed file system acm tocs february ieee ieee model open storage systems interconnection-mass storage system model version sept intel intel corporation virtual interface architecture viarch dec karger karger improving security performance capability systems cambridge computer laboratory technical report oct knudsen knudsen preneel hash functions based block ciphers quaternary codes advances cryptology asiacrypt nov lee lee thekkath petal distributed virtual disks acm asplos oct long long swift raid distributed raid system computing systems summer maeda maeda bershad protocol service decomposition high-performance networking acm sosp dec mckusick mckusick fast file system unix acm tocs august miller miller model mass storage systems advances computers mitchell mitchell dion comparison network-based file servers acm sosp dec mpi mpi forum message-passing interface standard mcs anl gov mpi standard html neuman neuman kerberos authentication service computer networks ieee communications sept patterson patterson case redundant 
arrays inexpensive disks acm sigmod june pricewatch pricewatch july riedel riedel active storage large-scale data mining multimedia vldb aug sachs sachs lan convergence survey issues ieee computer dec sandberg sandberg design implementation sun network filesystem summer usenix june srivastava srivastava eustace atom system building customized program analysis tools wrl technical report tntanenbaum tanenbaum mullender van renesse sparse capabilities distributed system sixth conference distributed computing thekkath thekkath frangipani scalable distributed file system acm sosp oct tpc transaction performance council tpc-c executive summaries url tpc mar tricore tricore news release siemens -bit embedded chip architecture enables level performance real-time electronics design tri-core sept vanmeter van meter hotz finn derived virtual devices secure distributed file system mechanism nasa goddard conference mass storage systems technologies sep vanmeter van meter visa netstation virtual internet scsi adapter acm asplos oct verbauwhede verbauwhede security considerations design implementation des chip eurocrypt voneicken von eicken basu buch vogels u-net user-level network interface parallel distributed computing acm sosp dec watson watson coyne parallel architecture high-performance storage system hpss ieee symposium mass storage systems september wilkes wilkes needham cambridge cap computer operating system wilkes wilkes hamlyn interface senderbased communications hewlett-packard laboratories technical report hpl-osr- nov wulf wulf hydra kernel multiprocessor operating system cacm june 
automatically generating malicious disks symbolic execution junfeng yang sar paul twohey cristian cadar dawson engler stanford computer systems laboratory junfeng csar twohey cristic engler stanford abstract current systems data produced potentially malicious sources mounted file system file system code check data dangerous values invariant violations file system code typically runs inside operating system kernel single unchecked crash machine lead exploit validating file system images complex form dags complex dependency relationships massive amounts data bound intricate undocumented assumptions paper shows automatically find bugs code symbolic execution running code manually-constructed concrete input run symbolic input initially allowed code runs observes tests input constrains values generate test cases solving constraints concrete values approach works practice checked disk mounting code widely-used linux file systems ext ext jfs found bugs malicious data kernel panic form basis buffer overflow attack introduction current systems data produced potentially malicious sources mounted file system network code sanitize packet data system calls check parameters file system code vet data mounts ensure explicit implicit invariants obeyed candidate disk inode indexes block numbers prescribed bounds counts employed division operation file systems typically run privileged code inside kernel single unchecked crash machine worst lead exploit bitter experience shown difficulty validating network data validating allegedly safe file system image complex network packets simple linear structure file system data structures bind massive amounts data complex dags full intricate undocumented assumptions culturally file system designers lack level paranoia network implementors factors make disk-focused attacks easy discuss attacks general attack bad person generate malevolent disk image give good trusting person usb memory stick cd-rom mounts suffers ability virus infects user account buggy mail client web browser writes choice malicious blocks user removable media crashing machine user inserts media system unprivileged users mount devices common case bad person physical access machine crash intermediary mounting malevolent media physical access equated root access disk mounting attacks lower barrier entry public machines thinks lab user inserting unscrewing computer removing hard disk suspicious finally systems increasingly unprivileged users mount arbitrary data file system ability enables worst attack file system crash machine escalate privileges buffer overflow physical access machine mac lets unprivileged users mount normal files file systems part preferred softwaredistribution mechanism linux similar functionality loop back mounts likelinspire enablefor regular users file system validation bugs consequences difficult eliminate structure typical file system makes manual inspection erratic usual deeply nested conditionals function call chains sheer mass code prolific casting pointer operations difficult follow dynamic dispatch calls function pointers vfs interface addition checks tricky presence arithmetic overflow programmers reason poorly random testing faces difficulties bugs arithmetic overflow occur narrow input range making finding random test cases file system code resides thicket deeply nested conditionals vet initial disk reaching code means random testing correctly guess values conditionals depend linux ext read super block routine forty if-statements checking data super block randomly generated super block satisfy tests reach level vetting triggering execution real code performs actual file system operations worse conditionals equalities -bit values hitting exact satisfy conditional require billions attempts completely hopeless paper shows automatically find bugs file system code symbolic execution system exe execution generated executions developed prior work central insight exe code automatically generate potentially highly complex test cases high level mark disk symbolic input kernel run produce constraints test cases running code manually generated test cases exe instruments program runs symbolic input initially free code executes data interrogated results conditional expressions operations incrementally inform exe constraints place values input order execution proceed path time code performs conditional check involving symbolic exe forks execution adding true path constraint branch condition held false path constraint exe generates test cases program constraint solver find concrete values satisfy constraints approach nice features unlike checking approaches constructive finds error actual concrete input run code trigger error point view exe viewed automatic generate disk images enable exploits constructiveness means exe false positives input claims error automatically fed back uninstrumented version checked code verify error occur result users trust exe worked correctly verify input code crash inspecting test case saved regressions exponentially amplifies effect running single code path simultaneously reasons values path run satisfy current path constraints single set concrete values individual test case fact exe solvable accurate complete set path constraints constraints missed symbolic values uninstrumented code premature concretization occurred exe reasons values path execute illustrate dynamic memory checker purify catch out-of-bounds array access index base pointer bad time memory access specific set input values code run contrast exe identify bug input path out-of-bounds index array modulo caveats addition arithmetic expression symbolic data exe solve constraints values overflow division system check values single path forcibly construct input values ideally paths coverage practical reach random manual testing amplification effect potential finding security vulnerabilities security exploits difficult find standard testing techniques arise uncommon interactions corner cases test generation hard symbolic execution specializes generating inputs drive system states effort needed create testing framework leveraged create environment searches security problems finally approachworks checked disk mounting code widely-used linux file systems ext ext jfs found bugs paper organized section overview approach section details symbolic execution including limitations section details changesweneeded order make exe work linux section describes bugs found section discusses related work section concludes system overview section overview approach graphically sketched figure high level system consists pieces trivial test driver issues mount system call kernel mount symbolic disk file system modified version user mode linux kernel file systems testing modifications prior work consist simplifications removing threading changing kernel memory allocators call exe runtime section details virtual disk driver manages symbolic disk exe system consisting exe compiler exe-cc instruments code symbolic execution constraint management runtime interfaces stp constraint written david dill vijay ganesh stp departs decades standard approach nelson solver pieces compiled exe-cc resultant executable run exe runtime causing happen test driver triesto mount file system causing file system request disk blocks virtual disk driver block request virtual driver checks file system requested block creates unconstrained symbolic block contents initially allocating block memory large disk read calling exe runtime system mark memory symbolic returning pointer memory back file system returns pointer previously read 
copy implicitly preserving existing constraints block file system observes symbolic blocks constraints generated tracked exe runtime exe detects error mount system call finishes returns test driver exe generates concrete disk image constraint solver solution current set constraints literally actual values bits satisfy disk constraints current path disk image generated exe response error mounted uninstrumented versionof kernel verifythat error errors found true errors caused real input depend exe note disk images obvious bugs test cases provide run paths code aiding general correctness testing exe dynamic literally runs checked program exe access information dynamic analysis static analysis typically non-symbolic operations happen uninstrumented code produce values values constraints correct approximations symbolic expressions accurate exe models oppen cooperating decision procedure framework solve constraints preprocesses bitblasts constraints sat solves minisat stp approach simpler preliminary results suggest significantly faster traditional method figure symbolic execution overview instrumented file system code shaded left symbolic blocks symbolic block device code branches result symbolic operation symbolic execution runtime run code true false branches generate concrete test cases generated tests checked running unmodified kernel thatched language works presence pointers unions bit-fields casts integers pointers aggressive bit-operations shifting masking byte swapping checksumming exe loses single bit precision lets bit path exe missing constraints checked system called uninstrumented code assembly exe bug section discusses lost constraints detail context bit-level precision means exe full set constraints path constraint solver produce concrete solution code deterministic rerunning checked system concrete values file system code follow exact path error termination generated image handling exponential branching exe aims path coverage general number paths grows roughly exponential total lines code domain manageable branches symbolic expressions forked executions code check branches involve non-symbolic conditions means execute concretely uninstrumented code linear cost caveat loops simple loop compares counter symbolic bound run counter reaches potentially enormous maximum symbolic possibly expensive complex symbolic loop conditions run longer exe handles loops search heuristics exe forks execution chose branch follow resume child unexplored prior branch default exe depth-first search dfs number processes small linear depth process chain addition forking loop conditions default explore false branch means loops run times paths loop run exits combinations run simple dfs works poorly cases backtrack checked code consecutive loops dfs stuck unable backtrack executed times counter exe set heuristics guide search current favorite mixture best-first dfs search picks process execute line code run fewest number times runs process children dfs manner picks best-first candidate iterates challenges experiments needed hour generate tests trigger errors discuss mount system call implementations checked paper complex control flow symbolic looping sense happy system run weeks long tests exe generates explore paths difficult reach randomly real alternative manual test generation generally speaking impressive results finally tests generated run uninstrumented checked program full speed saved regression runs describe exe detail section including limitations require bit technical background discuss issues applying exe linux symbolic execution exe main goal point program path accurate complete set constraints symbolic input path exe solve constraints path things drive execution paths path constraints check input exists error division invalid dereference point path entire motivation working exe hope achieving path coverage checking large amounts code section high-level overviewof key features exe needed check file systems code paper mechanics supporting symbolic execution accurately tracking path constraints executing paths universal checks models memory expression constraints reflect memory locations expression refer close discussing exe limitations including cases track constraints operational detail interested reader refer introduced exe instrumentation step exe compile code check exe-cc cil front-end instrument checked program symbolic execution inserted instrumentation primary tasks supporting mixed concrete symbolic execution exploring program choices forking program execution symbolic actions discuss exe supports mixed concrete symbolic execution inserting dynamic checks expression assignments dereferences conditionals expression operands concrete expression performed concretely original uninstrumented program operands symbolic expression performed exe runtime system adds constraint expression exe checks concrete lets expression execute records holds concrete symbolic exe adds constraint records corresponds symbolic exe designed explore interesting happen input explores branches symbolic conditional literally fork system call clone execution adding true path symbolic constraint branch condition true false path if-statement concrete execution normal expression evaluated true true branch false branch symbolic exe forks execution true path asserts true false path note non-symbolic variable involved expression concretely evaluated encoded constant constraint figure rewrite transformation conditional statements exe forking situations drive execution often-buggy corner cases arithmetic overflow casting surprises exe attempts force overflow symbolic arithmetic operation builds symbolic expressions encodes operation precision program tested encodes operation essentially infinite precision exe queries constraint solver constraints current path expressions differ if-transformation expr stmt stmt is-symbolic false fork child add-symbolic-constraint true add-symbolic-constraint false figure rewrite transformation conditional expressions arithmetic overflow exe generates concrete test case triggers overflow narrowing casts lose information add bugs exe checks truncation cast n-bit symbolic expression m-bit symbolic expression lose bits manner similar overflow checking builds symbolic expression extracts bits signor zero-extends back bits queries constraint solver constructed expression differ forks execution execution paths adds constraints path loses information casts signed unsigned common source security holes signed variable negative unsigned representation large cast symbolic exe queries constraint solver check high bit symbolic exe forks execution create paths sign bit constrained constrained generate large unsigned symbolic checks power key advantage symbolic execution concrete concrete execution operates single set concrete values symbolic execution operates values current path constraints modulo power constraint solver exe ability provide universal checks execution reaches program operation error values division null out-of-bounds pointersfor dereferences exe checksif anypossible input exists satisfies current path constraints operation blow checking dramatic amplification concretely checking single exe universal checks integer divisor modulus dereferenced symbolic pointer null dereferenced pointer lies valid object checks follow general pattern front-end inserts check relevant point checked program calls constraint solver determine condition occur exe forks execution branch asserts condition occur emits test case terminates false path asserts condition occur index bounds divisor non-zero continues execution hunt bugs complex check determining symbolic pointer dereference bounds exe tracks size memory block block symbolic pointer point techniques similar cred purify information dereference exe asks constraint solver base object exe produces concrete assignment initial symbolic inputs 
makes concrete execution program perform out-of-bounds memory access adds constraint pointer object continues execution checks found complex buffer overflow error ext file system replicated ext discuss section finally note exe goal path coverage implicitly turns programmer asserts symbolic expression universal checks asserted condition exe hits assert systematically search set constraints reach false path assert check conditional assert passes exe find input violate thereis someinput lieswithin exe sconstraint solver ability solve find exponentially amplifies domain assertion code checking single concrete note generally correctness check programmer puts code receive amplification exe drive execution paths checking code including paths catch errors modeling memory memory stores coexist run exe program concrete store memory underlying machine flat byte array addressed -bit pointers symbolic store resides inside constraint solver concrete store concrete operations act includes heap stack data segments symbolic store includes set symbolic variables current set constraints addition constants set constraints solving symbolic store constraints concrete store symbolic store describes concrete stores constraints solution solutions symbolic store accurate complete set constraints solution guaranteed valid concrete store concrete bytes holding concrete values storagein symbolic store values start concrete user marks set bytes symbolic exe creates identically-sized range bytes symbolic store records correspondence hash table maps byte addresses symbolic bytes program executes table grows bytes symbolic assigning symbolic expression concrete variable parameter passing viewed form assignment indexing data block symbolic index accurately tracking constraints involve strongly-typed scalar variables simple variables uniquely original code names consistently constraints problems handle real code treats memory untyped bytes pointers give detail untyped memory simple natural build symbolic store map symbolic object textual program variable structure array object variable structure array symbolic store essentially associating single type memory location systems code observes single memory location multiple ways casting signed variables unsigned code checked treating array bytes inode superblock structure treats memory untyped bytes exe stp primitives bitvectors arrays encode memory symbolic object untyped stp array -bit bitvector elements bitvectors treat memory untyped arrays handle pointers discussed read memory generates constraints based static type read int unsigned types persist single constraint generated symbolic expression read observing bits unsigned access impede subsequent signed access accesses performed expected respective constraints conjoined symbolic pointers unlike scalars symbolic pointers refer symbolic variables array size in-bounds symbolic index simple boolean expression essentially big disjunction similarly simple array assignment update encoding array expressions raw sat-solver interface tricky stp worry encoding complexity main challenge taking pointer dereference symbolic mapping correct stp array index array mapping proceeds map pointer stp array steps exe machinery checking out-of-bounds memory address concrete memory location holds pointer return starting address memory block point base lookup base address auxiliary hash table stp array bsym symbolic counterpart allocate initial values set current concrete memory location symbolic array bsym build symbolic expression include assert int main unsigned char make symbolic macros make make symbolic make symbolic symbolic force bounds exit assert figure simple pointers possibly symbolic offset base concrete memory block points add original possibly symbolic offset final symbolicexpression bsym constraints accurately refer set symbolic locations original expression point end result gyrationsis exe handle reads writes pointer expressions pointer offset expression symbolic feel ability means code figure compiled exe-cc program execute correctly produce assertion violations code presents main challenges assignment symbolic index refer values creates symbolic store generate concrete memory stores depending element overwritten read symbolic indexes refer exe hits assert statement true branch statement line number concrete stores reduced condition true present array element overwritten program checks fact assert exe proves true limitations exe limitations ephemeral fundamental ephemeral category exe researchquality handling linux smooth ride fact concrete test case produced helps lot eliminating false positives check exe optionally tracks basic blocks visited generating give case verify path executed concrete rerun checked code check found bugs inside exe places exe replaces symbolic concrete constant concretization places additional constraints order make progress discarding execution paths values stp handle division modulo symbolic exe encounters operation constrains operand power replaces division modulo operation shift bitwise mask double-dereferences symbolic pointer symbolic exe concretize dereference fixing possibly storage locations refer result symbolic expression concretization artifact arraysin stp working removing places exe miss constraints stp solve constraints path exe terminates path termination happened checking mount code general constraint solving np-hard problem uninstrumented code inline assembly functions files correctly compiled exe-cc attempts symbolic values occurs code check exponential branching system missing features mattered paper stp correctly handle operations -bit primitives generate proper constraints code makes extensive long long values fortunately disk mounting code values simple ways stp handle stp support floating-point operations exe handle call symbolic function pointers added concretization correctly track symbolics passedto applying exe linux ideally file systems provide unit testing framework automatically extract host operating system check user level exe practice file systems tightly entwined host cutting file system code rest kernel manner hopeless interacts virtually part kernel timers virtual memory layer buffers approximating small portion kernel order check device drivers labor intensive prone inaccuracies requiring careful adjustments modeled functions run linux kernel exe-cc pushing entire linux kernel symbolic execution system check unanticipated interactions file system kernel code attempted model linux vfs implementation checking jfs missed jfs bug found involves complicated interactions linux inode manipulation routines run linux bare hardware cmc framework adaptation user mode linux uml run kernel unprivileged user-level process reasons current exe requires ability clone wait processes operationsthat wecannot linuxkernelrunning hardware checking code running bare hardwaremakes things unnecessarily difficult debuggers run poorly pointer errors reset machine causing catchable segmentation fault virtual disk driver expect main trick mentioned lazily make blocks symbolic read opposed simply making entire disk symbolic upfront laziness make big difference speed minimum size jfs disk making entire disk symbolic generate prohibitively expensive number constraints beginning lazily making individual disk blocks symbolic time inspected drastically reduces number constraints turn check jfs linux large piece software exciting things result adapt exe number ways work linux linux work exe made modifications exe response checking linux assignment symbolic expression concrete variable exe initially make symbolic constrained single checking special case assigning constrained hold dramatically reduced number symbolics freeing heap object 
reusing problems current implementation exe determine base object pointer involved double-dereference concretize workaround problem freeing heap objects symbolic modified linux ways exe support threads fortunately cmc control threading easily disable purposes running mount system call exe instruments code kernel fair amount hand-optimized assembly common memory manipulation routines memcpy strlen improve runtime performance real kernel exe lose symbolic constraints input routines symbolic values replaced optimized kernel routines slower instrumented versions ensure track constraints kernel calls uml kernel maps fixed virtual address range simplify porting real linux kernel large number temporary variables introduced exe-cc resulted code data segments large caused mapping fail elided problem linking kernel modules uml kernel reworking exe-cc transforms reduce number generated temporaries fourth front-end transformations instrumentation alwayshandle gnu linux written failed compile files compile problem files gcc treat functions inside uninstrumented library code exe designed halt execution error symbolic data passed uninstrumented code flag losses precision fortunately mount code check symbolic data passed uninstrumented file kernel read write total system panic arbitrary memory ext jfs total figure summary unique vulnerabilities found routines finally debugging wemanually simplified hash function hashfn hash block device number buffer cache lookups extensive shifts generated lot constraints results applied technique linux file systems ext ext jfs rest section treat ext ext file system code bugs implementing mount operation ext identical ext found vulnerabilities ext replicated ext vulnerability jfs instrumented exe code typically finds bugs minutes case hour modern desktop technical reasons run file system code top older kernel linux vulnerabilities found fixed latest linux kernels previously unaware vulnerabilities found present latest linux kernel figure summarizes errorswe found errors kernel panic error makes kernel read write arbitrary memory discuss errors detail sections inlined comments code snippets section provided clarity ext ext errors found ext ext kernel read write arbitrary memory make kernel crash dangerous exploit found carefully crafted malicious disk bypass upperbound check offset arithmetic overflow unconstrained offset read write arbitrary regions memory step bug detail order give feel type bugs exe find source code leads exploit shown figure function ext free blocks frees count disk blocks belonging inode inode starting block number block parameter block read disk treated symbolic variable exe lines ext checks block valid range block large block count overflow pass check block count cpu ess blocks count note cpu macro expands identity function endian machines ext computes block group lines block calls load block bitmap line load block bitmap procedure block group array index lines attacker read arbitrary memory addresses condition lines true code assigns block group slot line returns back caller load block bitmap function makean assignment variable line bhdata write arbitrary locations memory line bug fixed versions kernel series bug immediately found exe hard find manual inspection involves combination events arithmetic overflow buffer overflow easily overlooked bug highlights benefit exe memory model exe bit-level precision support symbolic pointers casting extremely difficult detect exploits addition exe multiple symbolic checks made easy diagnose error exe flagged arithmetic overflow lines buffer overflows lines identify root bug finally exe produced concrete values trigger overflows easily verify simply mounting exe-generated disk image uninstrumented linux kernel generated kernel panic block group index past bounds array ext balloc void ext free blocks struct inode inode unsigned long block unsigned long count exe block symbolic block count overflow smaller blocks count block cpu data block block count cpu blocks count ext error goto error return exe block group symbolic block group block cpu data block ext blocks group exe call load block bitmap symbolic argument block group load block bitmap return block group bitmap symbolic bitmap load block bitmap block group bitmap goto error return error read bounds ext block bitmap size ext block bitmap bitmap count error data point read write arbitrary kernel memory ext clear bit bit data static inline int load block bitmap struct super block unsigned int block group error read bounds ext max group loaded ext groups count ext max group loaded ext block bitmap number block group block group ext block bitmap block group exe slot symbolic slot block group slot load block bitmap block group return slot exe potential symbolic return figure ext buffer overflow found bug ext malicious disk panic kernel mount time bug live version kernel fixed linux file systems ext respond ways faced error continue execute continue read-only panic kernel actual behavior global mount options file called fstab setting flags disk super block correct implementation give priority global mount options set system administrators ext implementation incorrectly on-disk flags override global mount options malicious disks exploit panic kernel setting panic flag super block exploits found ext caused division bug modulo bug taking advantage file system code values read disk divisors checking errors fixed linux version exists latest kernel version time figure shows unfixed bug data argument ext read super symbolic values read symbolic disk function locates symbolic super block data line copies fields in-memory super block sbu ext line checks fields valid field inodes group specifies number inodes contained block group ext developers carefully check upper bound line fail check subsequent denominator line modulo error division bug fixed linux similar modulo bug presented spans single function fact modulo bug spans files checks performed super modulo operation ext inode explains fixed jfs bug found jfs null pointer dereference exists latest series kernels slightly modified form latest ext super exe parameter data symbolic values read symbolic disk struct super block ext read super struct super block void data int silent exe cast data points symbolic super block struct ext super block char data offset exe copy values ext inodes group symbolic ext inodes group cpu inodes group exe check uppper bound ext inodes group blocksize printk ext -fs inodes group big ext inodes group goto failed mount exe read root inode iget call ext read inode root alloc root iget ext root ino include linux ext define ext inodes group ext inodes group ext inode void ext read inode struct inode inode error ext inodes group inode symbolic offset inode ino ext inodes group inode ext inode size inode figure 
ext modulo-by-zero bug kernels mounting code discovers error provided disk undo partially completed mount calling jfs umount code shown figure function frees fileset inode allocation map sbiipimap sets pointer null line line close aggregate inode allocation map calling difreespecial nested function calls jfs umount calls difree retrieves ipimap null superblock line dereference line causing null pointer exception bug exists kernel fixed offset subtracted null pointer dereferenced causing page fault invalid virtual address figure shows actual disk image jfs jfs umount int jfs umount struct super block struct jfs info sbi jfs sbi struct inode ipaimap sbi ipaimap sbi ipimap null ipaimap sbi ipaimap exe eventually call difree ipaimap difreespecial ipaimap jfs jfs imap int difree struct inode exe jfs umount sets ipimap null ipimap null struct inode ipimap jfs sbi ipimap error expands imap ipimap generic ipimap null null pointer dereference struct inomap imap jfs ipimap imap figure jfs null pointer dereference offset hex values figure hex dump disk jfs dereference null pointer linux kernel repeats previous row reproduce null pointer dereference linux simply create empty file set sector disk jfs dereference garbage pointer kernel generated exe error path constraints mounted null bogus pointer dereference related work file system testing frameworks application interfaces stress live file system adversarial environment good focus errors occur runtime operation file system focus corruption data integrity issues recently work characterizing file system responds disk errors work starts initial good disk problems aim crash kernel get-go remainder section compare exe symbolic execution work static input generation model checking test generation finally generic bug finding methods symbolic execution prior work developed egt system approach similar spirit exe practice limitations pointers arrays bit-fields casting overflow sign extension extended checks describe section simultaneously egt dart project developed similar approach generating test cases symbolic inputs dart handles constraints integers form anxn negationslash handle symbolic pointers restrictionpreventsdart reasoning bit masking pointer operations inherent file system code cute project splintered dart handles pointer constraints approximate pointer theory miss errors form buf buf buf error symbolic egt dart cute focused unit-testing small user-level programs large complex kernel code additionally looked types errors focused paper cbmc bounded model checker ansic programs designed cross-check ansi implementation circuit verilog counterpart unlike exe mixture concrete symbolic execution cbmc runs code symbolically takes requires entire strictly-conforming ansi program translates constraints passed sat solver cbmc full support arithmetic control operations reads writes symbolic memory limitations handling systems code strongly-typedview memory prevents checking code accesses memory pointers types experimenting cbmc limit means check program calls memcpy program casts pointers integers back cbmc translate entire program sat check stand-alone programs interact environment systems calls calling code source limits prevent cbmc check file system code found bugs larson austin present system dynamically tracks primitive constraints tainted data data untrusted sources network packets warns data potentially dangerous potentially dangerous inputs array calls string library check index bounds string violate library function contract exe system detect error occur program concrete execution system lacks symbolic power exe unlike exe generate inputs paths executed require user provide test cases check paths covered test cases dynamic input generationtechniques past automatic input generation techniques focus primarily generating input reach path typically motivated problem answering programmer queries control reach statement exe differs work focusing problem comprehensively generating tests paths controlled input makes effective exploring state space programs tested model checking model checkers previously find errors design implementation software systems approaches tend require significant manual effort build test harnesses degree approaches complementary tests approach generates drive model checked code similar approach embraced java pathfinder jpf project jpf combines model checking symbolic execution check applications manipulatecomplex data structureswrittenin java jpf differs exe application domain support untyped memory needed java strongly typed language symbolic pointers static input generation long stream research attempts static techniques solve constraints generate inputs execution reach specific program point path nice feature static techniques require running code theory practice weaker dynamic technique exe access information impossible running program static checking recent work focused static bug finding insides tools dramatically exe saturn tool exception expresses program properties boolean constraints models pointers heap data bit level roughly speaking dynamic checking runs code limited executed paths effectively check deeper properties examples include program executions loop bad inputs byzantine errors occur formatting command printf properly obeyed errors paper difficult discover statically view static analysis complementary exe testing lightweight reason apply exe conclusion easy attack file systems mounting data produced malicious source paper shown aggressive symbolic execution find security holes real systems code interesting errors exe symbolic execution system find errors linux file systems error jfs ext replicated cut-and-paste ext plan extend techniques automatically harden code generating filters reject bad data exe finds path leads error complete accurate set path constraints input data error occur easily translate constraints if-statements reject concrete input satisfy constraints mount code checks inserted disk read calls reject bad blocks mount error basic approach generate filters reject dangerous network packets conversely plan exe automatically generate attacks natural domain operating systems code run exe-instrumented system calls mark input user system call parameters data copied manually userspace unconstrained crash found solve concrete input values synthesize small program call kernel values exe generate filters attacks acknowledgments vijay ganesh david dill outstanding work constraint solvers cvcl stp derek chan pointing linspire lea kissner editing shepherd david wagner patience extensive comments research supported nsf career award cns- department homeland security dhs grant junglee corporation stanford graduate fellowship distributing software internet-enabled disk images http developer apple documentation developertools conceptual soft waredistribution concepts disk images html linspire world easiest desktop linux http linspire user-mode linux kernel home page http user-mode-linux sourceforge net ball theory predicate-complete test coverage generation fmco symp formal methods components objects springerpress ball majumdar millstein rajamani automatic predicate abstraction programs pldi proceedings acm sigplan conference programming language design implementation pages acm press ball rajamani automatically validating temporal safety properties interfaces spin workshop model checking software boyer elspas levitt select formal system testing debugging programs symbolic execution acm sigplan notices june bush pincus sielaff static analyzer finding dynamic programming errors software practice experience cadar 
engler execution generated test cases make systems code crash proceedings international spin workshop model checking software august longer version paper appeared technical report cstr- computer systems laboratory stanford cadar twohey ganesh engler exe system automatically generating inputs death symbolic execution technical report cstr stanford clarke kroening hardware verification ansi-c programs proceedings asp-dac pages ieee computer society press january corbett dwyer hatcliff laubach pasareanu robby zheng bandera extracting finite-state models java source code icse das lerner seigle path-sensitive program verification polynomial time proceedings acm sigplan conference programming language design implementation berlin germany june orensson extensible sat-solver giunchiglia tacchella editors sat volume lecture notes computer science pages springer ext ext file system http fsprogs net ferguson korel chaining approach software test data generation acm trans softw eng methodol foster terauchi aiken flow-sensitive type qualifiers proceedings acm sigplan conference programming language design implementation june godefroid model checking programming languages verisoft proceedings acm symposium principles programming languages godefroid klarlund sen dart directed automated random testing proceedings conference programming language design implementation pldi chicago usa june acm press gotlieb botella rueher automatic test data generation constraint solving techniques issta proceedings acm sigsoft international symposium software testing analysis pages acm press gupta mathur soffa automated test data generation iterative relaxation method sigsoft fseproceedings acm sigsoft international symposium foundations software engineering pages acm press hastings joyce purify fast detection memory leaks access errors proceedings winter usenix conference dec holzmann model checker spin software engineering ibm journaling file system linux http wwwibm jfs khurshid pasareanu visser generalized symbolic execution model checking testing proceedings ninth international conference tools algorithms construction analysis systems larson austin high coverage detection input-related security faults proceedings usenix security symposium security august linux test project http ltp net musuvathi engler model checking large network protocol implementations proceedings symposium networked systems design implementation necula mcpeak rahul weimer cil intermediate language tools analysis transformation programs proceedings conference compilier construction march nelson oppen simplification cooperating decision procedures acm transactions programming languages systems prabhakaran bairavasundaram agrawal gunawi arpaci-dusseau arpacidusseau iron file systems sosp proceedings twentieth acm symposium operating systems principles pages york usa acm press ruwase lam practical dynamic buffer overflow detector proceedings annual network distributed system security symposium pages sen marinov agha cute concolic unit testing engine joint meeting european software engineering conference acm sigsoft symposium foundations software engineering esec fse sept stress http weather apw projects stress wagner foster brewer aiken step automated detection buffer overrun vulnerabilities network distributed systems security conference san diego feb xie aiken scalable error detection boolean satisfiability popl proceedings acm sigplan-sigact symposium principles programming languages pages york usa acm press yang twohey engler musuvathi model checking find file system errors proceedings sixth symposium operating systems design implementation dec 
model checking find file system errors junfeng yang paul twohey dawson engler junfeng twohey engler stanford computer systems laboratory stanford stanford madanlal musuvathi madanm microsoft microsoft research microsoft redmond abstract paper shows model checking find errors file systems model checking formal verification technique tuned finding corner-case errors comprehensively exploring state spaces defined system file systems dynamics make attractive approach errors destroy persistent data lead unrecoverable corruption traditional testing impractical exponential number test cases check system recover crashes point execution model checking employs variety state-reducing techniques explore vast state spaces efficiently built system fisc model checking file systems applied widely-used heavily-tested file systems ext jfs reiserfs found bugs total led patches day diagnosis file system fisc found demonstrable events leading unrecoverable destruction metadata entire directories including file system root directory introduction file system errors destructive errors deployed file systems reside operating system kernel simple error crash entire system midst mutation stable state bugs file system code range mere reboots lead unrecoverable errors stable disk state cases mindlessly rebooting machine correct mask errors fact make situation worse research supported nsf grant ccrand darpa grant dawson engler partially supported coverity nsf career award errors file systems dangerous file system code simultaneously difficult reason difficult test file system correctly recover internally consistent state system crashes point data mutated flushed flushed disk invariants violated anticipating failures correctly recovering hard results contradict perception importance file system errors led development file system stress test frameworks good focus non-crash based errors checking file system operations create delete link objects correctly testing file system correctly recovers crash requires reconstruction comparing reconstructed state legal state cost single crash-reboot-reconstruct cycle typically minute makes impossible test tiny fraction exponential number crash possibilities implementors validation testing effective heavily-tested systems errors arise deployed making errors impossible eliminate replicate paper model checking systematically test find errors file systems model checking formal verification technique systematically enumerates states system exploring nondeterministic events system model checkers employ state reduction techniques efficiently explore resulting exponential state space instance generated states stored hash table avoid redundantly exploring state inspecting system state model checkers identify similar set states prioritize search previously unexplored behaviors system applicable systematic exploration achieve effect impractically massive testing avoiding redundancy occur conventional testing dominant cost traditional model checking effort needed write abstract specification system commonly referred model upfront cost traditionally made model checking completely impractical large systems sufficiently detailed model large checked system empirically implementors refuse write written errors drift implementation modified model recent work developed implementation-level model checkers check implementation code directly requiring abstract specification leverage approach create model checking infrastructure file system checker fisc lets implementors model-check real unmodified file systems model checking knowledge fisc built cmc explicit state space implementation model checker developed previous work lets run entire operating system inside model checker check file system situ attempting difficult task extracting operating system kernel applied fisc widely-used heavily-tested file systems jfs reiserfs ext found bugs total led patches day diagnosis file system fisc found demonstrable events leading unrecoverable destruction metadata entire directories including file system root directory rest paper give overview fisc check file system describe checks fisc performs optimizations checks file system recovery code discuss results experiences fisc including sources false positives false negatives conclude checking overview system comprised parts cmc explicit state model checker running linux kernel file system test driver permutation checker verifies file system recover matter order buffer cache contents written disk fsck recovery checker model checker starts initial pristine state empty formatted disk recursively generates checks successive states systematically executing state transitions transitions test driver operations fs-specific kernel threads flush blocks disk test driver conceptually similar program run testing creates removes renames files directories hard links figure state exploration checking overview fisc main loop picks state state queue iteratively generates successor states applying operation restored copy generated state sprime checked validity valid explored inserted state queue writes truncates files mounts unmounts file system figure shows process state generated intercept disk writes checked file system forward permutation checker checks disk state fsck repair produce valid file system subset disk writes avoids storing separate state permutation fisc choose permutations check checker explained section run fsck host system model checker small shared library capture disk accesses fsck makes repairing file system generated writing permutation feed fsck generated writes crash recovery checker checker fisc recursively check failures fsck covered section figure outlines operation permutation fsck recovery checkers checkers copy disk starting state transition write copy avoid perturbing system copied disk modified model checker traverses file system recording properties checks consistency model file system figure disk permutation andfsckrecovery checkers size link count file directory system contents directory note model file system data file system code code traverse create manipulate file system model mirrors system call interface reused check file systems check model matches valid file systems computed section error flagged case mismatch state generated system runs series invariant checkers file system errors error found fisc emits error message stores trace error diagnosis records nondeterministic choices made error discards state error fisc state puts state queue visited similar state discards state states checkpointed added state queue exploration checkpointing kernel state captures current execution environment put state queue restored model checker decides state queue explore operations state consists kernel heap data disk abstract model current file system additional data invariant checks discussed section fisc searches state space breadthor depth-first search simple heuristics checking environment similar unit testing model-checking file system requires selecting layers cut checked system layer defines external interface test driver runs case driver run atop system call interface layer fake environment checked system runs environment model checked file system run bare hardware fisc virtual block device models disk collection sectors written atomically block device driver layer natural place cut well-documented boundary in-core persistent data modern unix derivatives provide virtual file system vfs interface vfs good place cut varies significantly operating systems versions kernel functions subtle dependencies cutting system call layer avoid headache modeling sparsely documented interactions make system easier port check vfs implementation design decision validated bugs found vfs code checking file system section 
overview file system implementor check file system basic setup cmc encases linux file system runs linux conforms fisc assumptions file system behavior require modifications checked fisc minimum disk memory sizes file system requires ext smallest requirements disk pages memory reiserfs highest disk pages addition commands make recover file system mkfs fsck ideally developer fsck options default recovery slow full recovery fast recovery replays journal recovery modes checked addition providing facts implementor modify file system expose dirty blocks consistency checks require knowing buffers dirty file system reiserfs machinery tracking dirty buffers changed explicitly dirty buffers file system fits fisc model file system works ext jfs takes days start checking hand reiserfs weeks effort run fisc violated larger assumptions made stated earlier crash checking fisc mounts copy disk checked file system block device check original file system independently manage disks reentrant manner reiserfs single kernel thread perform journal writes mounted devices deadlock journal thread writes log fisc suspends creates copy disk remounts file system remounting replays journal requires writing journal deadlocks waiting suspended journal thread run fixed problem modifying reiserfs wake journal thread clean file system mounted read-only modeling file system file system operation fisc compares checked file system believes correct volatile file system volatilefs volatilefs reflects effects file system operations sequentially defined standards fs-specific fisc construct fisc performs operation mkdir link checked concrete system emulates operation effect fake abstract file system verifies checked abstract file systems equivalent lossy comparision discards details time disk write fisc compares checked file system model believes current stable file system stablefs stablefs reflects state file system recover crash point running file system fsck repair utility current disk produce file system equivalent stablefs unlike volatilefs stablefs fs-specific file systems make wildly guarantees recovered crash ext file system journaling file system typically intends recover completed log record commit point soft-updates file system recovers difficult-to-specify mix data determining stablefs evolves requires determining fs-specific facts legally change fisc requires implementor modify checked file system call model checker stablefs journaling file systems change typically occurs journal commit record written disk identify annotate commit records easily ext reiserfs jfs difficult end variety false starts gave determine journal write represented commit-point stablefs change journal write assume file system implementor job stablefs difficult essentially requires writing crash recovery specification file system assume file system implementor check systems build shortcut fsckto generate stablefs copy experimental disk run fsck reconstruct file system image committed operations traverse file system recording properties interest approach miss errors guarantee fsck produce correct state thatfsck fail repairing perfectly behaving disk fail subsequent crashed disks persistent file system model compared checking basic file system checked main strategies implementor follow check file system downscaling canonicalization exposing choice points talk downscale operationally means making small plausible caches entries large file systems nodes node file directory model checking works ferreting complex interactions small number nouns files directories blocks threads small number caching techniques give leverage main places downscaled making disk small megabytes gigabytes checking small file system topologies typically nodes finally reducing size virtual memory checked linux system small number pages canonicalization technique modifies states state hashing irrelevant differences practice common canonicalization set things constant values clearing inode generation numbers mount counts time fields zeroing freed memory unused disk blocks journal blocks canonicalizations require fs-specific knowledge implementor fisc generic canonicalization constrains search space writing values data blocks significantly reducing number states providing resolution catch data errors hashing model file system fisc transforms file system remove superficial differences renaming files directories sequential numbering file system objects file system directory files model file system directory files files length content canonicalization lets move search space searching rare filename-specific bugs common bugs arise creating file system topologies expose choice points making sources nondeterminism choice points visible fisc lets search set file system behaviors low-level adding code fail fs-specific allocators generally file system makes decision based arbitrary time constraint environmental feature change call fisc fisc choose explore decision state reaches point mechanically exposing choice point reduces modifying file system code call choose number decision alternatives choose return callsite times return values caller return pick actions perform reiserfs ext flush in-memory journals disk amount time lapsed replaced time check call choose modified caller choose returns code flushes commit record returns file systems check buffer cache issuing disk reads care means cache miss path rarely checked check tiny file system topologies solve problem choose success path buffer cache read routine ensure fisc explores miss path addition fisc generically fails memory allocation routines permission checks inserting choice points implementor exploit well-defined internal interfaces increase set explored actions interface specifications typically range actions implementation pick subset routines invocation return memory error actual implementation allocate memory paths allocations mistake fail specific allocation calls implementation performs means callers system configurations failures simple fix insert choice point routine action allowing model checker test failure handled call easy expose choice points require restructuring parts system remove artificial constraints invasive modifications buffer cache made permutation checker buffer write orderings checkers section describes checks fisc performs generic checks fisc inspects actual state system catch errors difficult impossible diagnose static analysis capable set general checks apply code run kernel deadlock instrument lock acquisition release routines check circular waits null fisc reports error kernel dereferences null pointer paired functions kernel functions iget iput inode allocation dget dput directory cache entries called pairs instrument functions kernel check called pairs running model checker memory leak instrument memory allocation deallocation functions fisc track memory altered system-wide freelist prevent memory consumers allocating objects model checker knowledge state transition stop system perform conservative traversal stack heap allocated memory silent failures kernel request resource specific planned bug system call returns success calls resource allocation routine fails exception pattern code loops acquires resource case generate false positive function fails iteration loop succeeds suppress false positives manually marking functions resource acquisition loops consistency checks fisc checks consistency properties system calls map actions mutation file system success system call return produce user-visible change indication failure produce change model volatilefs ensure operation produces user-visible change correct change recoverable disk write ordering write 
arbitrary combinations dirty buffer cache entries disk checking system recovers valid state file system recovery code typically requires disk writes happen stylized orders illegal orders interfere normal system operation lead unrecoverable data loss crash occurs inopportune moment comprehensively checking errors requires largest legal set dirty buffers memory flush combinations blocks disk legal opportunity file systems check thwart desires background thread periodically write dirty blocks disk cleaned blocks subsequent reorder checking falsely constraining schedules generate vagaries thread scheduling hide vulnerabilities thread run system vulnerable state dangerous disk writes happen modified thread model checker track blocks legally written block added set write permutations set verify running fsck produces valid file system image set blocks written dirty buffers buffer cache dirty buffers written order requests disk queue disks routinely reorder disk queue set initially empty blocks added buffer cache entry marked dirty blocks removed set ways deleted buffer cache marked clean file system explicitly waits block written file system forces synchronous write specific buffer entire disk request queue changed buffers marked dirty file system block buffer cache mark dirty operating system eventually write block back disk blocks marked dirty flushed cache time initially thought generic dirty bit buffer track dirtiness buffer file system slightly concept means buffer dirty ext considers buffer dirty conditions true generic dirty bit set buffer journaled journal dirty bit set buffer journaled revoked revocation valid discovering dirty buffer invariants requires intimate knowledge file system design run checker ext buffer consistency journaling file system associates state buffer buffer cache rules state change buffer managed ext marked dirty journal dirty written journal journal dirty written location disk dirty double fsck default fsck journaled file system simply replays journal compare file system resulting recovering manner generated running fsck comprehensive mode scans entire disk checking consistency differ wrong scaling system brought system online ran number performance memory bottlenecks section describes important optimizations state hashing search exploring exponential state space game ignore irrelevant details quest explore states differ non-superficial ways fisc plays game places state hashing selectively discards details make bit-level states equivalent searching picks state explore describe initially hashed things checked file system state heap data segment raw disk practice meant hard comprehensively explore interesting states model checker spent time re-exploring states iterative experimentation settled hashing volatilefs stablefs list runnable threads ignore heap thread stacks data segment users optionally hash actual disk image abstract stablefs check higher-level detail discarding detail rarely explore states size checkpoint roughly state queue holding to-beexplored states consumes memory long fisc exhaust search space stave exhaustion randomly discarding states state queue size exceeds user-selected threshold provide heuristic search strategies alternatives vanilla dfs bfs heuristic attempts stress file system recovery code preferentially running states disks work repair crash crudely tracking sectors written state parent disk recovered sorts states approach found data loss error jfs trigger strategy heuristic quantify state previously explored states utility score state utility score based times states features explored features include number dirty blocks state abstract file system topology ext reiserfs jfs states total expanded states state transitions time memoization memoization table number states transitions cost checking file system point fisc runs memory times seconds reiserfs large virtual memory requirements limited fisc checks roughly order magnitude fewer states systems fsck memoization speeds checking ext factor reiserfs factor parent executed file system statements state score exponentially-weighted weighted sum number times feature systematically failing functions transition mkdir creat executed perform calls functions fail memory allocation permission checks blindly causing combinations functions fail risks fisc explore exponential number uninteresting redundant transitions state additionally cases fs-implementors uninterested failures triggered memory allocation failures disk read error occurs iterative approach fisc run transition failures run failing single callsite callsites failed similarly fail callsites users maximum number failures fisc explore default failure approach find smallest number failures needed trigger error efficiently modeling large disks figure shows naively modeling reasonable-sized disks contiguous memory allocation severely limits number states model checker explore quickly exhaust memory changing file system code works smaller disk non-trivial error prone code mutuallydependent macros structures functions rely offsets intricate ways efficiently model large disks hash compaction database disk chunks collections disk sectors hashes disk array memory number states memory usage compaction ext compaction compaction chunk database figure memory usage model checking ext disk disk compaction compaction model checker quickly exhausts physical memory dies reaches states chunk database consumes total memory maximum note spike state mark fisc starts randomly discarding states state queue hashed chunks write alters chunk hash inserting database chunk hash fsck memoization repairing file system expensive takes times long runfsck restore state generate state executing operation careful time run fsck dominates checking fortunately practical purposes recovery code deterministic input disk image produce output image determinism memoize result recovering specific disk running fsck check current disk hash table return computed result run fsck add entry table space optimization track sectors read written fsck memoization trivial huge performance win table fsck recovery checker run fsck times crash cluster-based model checking model checking run makes set configuration choices number files directories operations fail crashes happen recovery exploring values expensive miss bugs fortunately exploration easily parallelizable wrote script set configuration settings remote machines generates configurations remotely executes summary table shows fisc check states transitions ext seconds expanded states transitions explored data section computed pentium ghz machine memory crashes recovery classic recovery mistake incorrectly handle crash recovery number potential failure scenarios caused failure unwieldy number scenarios caused failure combinatorially exciting failures correlated crashes uncommon power outage run fsck power runs similarly bad memory board system crash promptly recovery section describes check file system recovery logic handle single crash recovery check fsck crashes recovery attempt final file system stablefs obtained running fsck time disk possibly modified previous run attempt succeeded case fsck crashes repeatedly recovery repeated failure intellectually interesting difficulty reasoning errors caused single crash implementors shown marked disinterest elaborate combinations conceptually basic algorithm simple disk image crash run fsck completion record ordered write-list sectors values written fsck recovery tuple sector written data written sector formal terms model fsck function denoted fsck maps input disk output 
disk dprime differences dprime values write-set purposes writes effects running fsck denote partial evaluation fsck performing writes fsck definition fsck fsck disk image obtained applying writes disk image disk image returned fsck rerun fsck verify produces file system running fsck fsck computing fsck fsck fsck simulates effect crash recovery fsck performed writes restarted illustrate invoking fsck writes sectors values algorithm apply write obtain crash check apply write obtain crash check approach requires refinements reasonable speed catch errors reason describe speed determinism naive algorithm checks cases dramatically reduce number exploiting facts practical purposes regardfsckas deterministic procedure determinism implies property invocations deterministic function read input values compute result write fsck change previously read crash rerun back current state fsckrarely writes data reads result writes require crash recover intersect set sectors fsck reads determinism influence disk produce state independence precisely rsi denote unordered set sectors read fsck denote disk produced applying writes order initial disk claim sector written read set rsi running fsck completion disk produces result running rsi implies fsck fsck recall rsi rsi tautologically deterministic function change computes values reads rsi implies fsck fsck read identical values steps forcing compute results step perform write making disks identical special cases checker exploits skip running fsck suppose write sector rsi writes disk fsck fsck surprisingly recovery programs empirically redundant writes sector written dominated earlier write read precedes affect overwritten fsck restarted checking write orderings algorithm miss errors sector writes complete order explicitly synchronously direct option unix blocked sync barrier sync system call unix believed return dirty blocks written disk generating disk images crashingfsckat point requires partitioning writes sync groups checking thatfsckwould work correctly rerun performing subset writes sync group power set writes contained sync group write sectors call sync write sector sync groups generates write schedules write schedule sectors written fsck crashed note schedule equivalent rerun fsck writes complete sync group checker things size equal user-defined threshold checker exhaustively verify interleavings size larger checker series trials picks random subset random size trials deterministically replayed seed pseudo-random number generator hash sectors involved typically set number random trials reordering found bugs fsck recovery found checked file systems finding perspective recovery errors important reasoning extremely difficult recovery errors information model checker form wrote block block disk image files figuring semantically error occurred blocks writing values caused problem easily entire day process replicated implementor fix find simplest error case checker modes roughly ordered increasing degrees freedom difficulty diagnosing errors limiting degrees freedom means ordered increasing cost blush modes excessive reality desperate attempt find perspective makes reasoning error tractable additional views add synchronous atomic logical writes simplest view group sector writes logical writes synchronously order occur program execution logical writes means group blocks written system call invocation group calls write system call writing sectors writing sectors logical operations apply writes crash check apply writes crash check strongest logical view disk operations complete order issued writes single logical operation occur atomically easy reason errors means fsck reentrant synchronous non-atomic left-to-right logical writes treat logical writes synchronous write contained sectors non-atomically left-toright write sector logical group crash check write errors easy reason tend localized single invocation system call data structure assumed internally consistent straddled sector boundary reordered atomic logical writes mode reorders logical writes sync group checking permutation errors fixed inserting single sync call synchronous non-atomic logical writes mode writes sectors logical operation order crashing schedule errors modular interesting effects reordered sector writes view hardest reason sternest test file system recovery reorder sectors sync group arbitrarily errors hit make attempt find prior modes results table summarizes errors found broken file systems categories errors reported respective developers found bugs total fixed remaining confirmed complex patch submitted errors supposedly stable committed data metadata typically entire directories lost jfs higher error counts part due responses jfs developers enabled patch errors continue checking jfs checking xfs file system preliminary results promising discuss bug categories detail highlighting interesting errors found errors reported paper found page http keeda stanford junfeng osdi-fisc-bugs html titled osdi fisc bugs error type vfs ext ext jfs reiserfs total lost stable data false clean security holes minor kernel crashes total table found errors permanent data loss intended errors programmers decided sacrifice consistency availability shown table unrecoverable data loss errors found caused irrevocable loss committed stable data errors execution sequence lead complete loss metadata data committed on-disk journal cases large parts long-lived directories including root directory obliterated data loss main invalid write ordering journal data recovery buggy implementations transaction abort fsck invalid recovery write ordering bugs type normal operation journaling file system journal flushed disk data describes file systems check inverse ordering constraint wrong recovery journal replayed data modified roll forward flushed disk journal persistently cleared crash occurs file system corrupt missing data journal empty unable repair file system figure representative error ext fsckprogram chain mishaps recover ext journal rolls journal forward calling journal recover journal recover replays journal writing file system cached writes calls fsync super flush modified data back disk macro defined due error made moving recovery code kernel separate fsck process control returns recover ext journal calls fsck journal release writes cleared journal disk lack sync barriers write reach disk modified data result crash occurs point obliterate parts file system journal empty causing data loss bug reported developers immedie fsprogse fsck jfs user error empty macro sync data define fsync super dev fsprogse fsck journal static errcode recover ext journal fsck ctx journal journal int retval journal init revoke caches retval fsck journal ctx journal retval journal recover journal flushes empty journal fsck journal release ctx journal return retval fsprogse fsck recovery int journal recover journal journal process journal records cached writes err pass journal info pass scan err err pass journal info pass revoke err writes persistent data recorded journal cached write calls err pass journal info pass replay write modified data back clearing journal fsync super journal dev return err figure journal write ordering bug ext fsck ately released patch reiserfs jfs similar bugs fixed systems code lacked attempt order journal clear writes journal data buggy transaction 
abort fsck bugs type jfs threefold jfs immediately applies journaled operations in-memory metadata pages makes hard roll back aborted transactions modifications interleaved writes ongoing committed transactions result jfs aborts transaction relies custom code carefully extricate side-effects aborted transactions non-aborted writer code forgets reverse modification flushed disk interlacing directories invalid entries aborted transactions jfs fsckmakes attempts recover valid entries directories recovery policy directory single invalid entry remove entries directory attempt reconnect subdirectories files lost found opens huge vulnerability file system mistake results persistently writing invalid entry disk fsck deliberately destroy violated directory jfs fsck incorrect optimization loss committed subdirectories files jfs dynamically allocates places inodes performance tracking location inode map speed incremental modifications map written on-disk journal flushing map disk inode allocation deletion reconstruction fsck code loss inodes correctly applies incremental modifications copy inode map deliberately overwrite out-of-date on-disk inode map correct reconstructed copy figure shows bug jfs code initial version jfs fsck years ago implementors incorrectly believed file system marked dirty flushing inode map unnecessary rebuilt fix trivial flushing map bug hard find model checker jfs developers chasing manifestations submitted bug report file system events operations sector writes choices made model checker jfs developer create patch couple days good fact model checking improves testing systematic repeatable controlled data loss bugs jfs journal spans sectors vulnerability jfs stores sequence number sector journal middle sectors crash jfs fsck checks sequence numbers match replays journal additional checking inopportune sector reorderings lead corrupt journal badly mutilate file system replayed jfs ext bug crashed file jfsutilslibfs logredo original incorrect comment don update maps aggregate dirty fsck rebuild maps vopen fsdirty check dirtiness update on-disk map updatemaps fsck send msg lrdo errorcantupdmaps goto error figure incorrect jfs fsckoptimization unrecoverable loss inodes data system superblock falsely marked clean fsck program repair system potentially leading data loss system crash data loss bug happened jfs incorrectly stored negative error code inode number directory entry invalid entry fsck invocation remove directory security holes target security fisc found security holes readily exploitable easiest exploit found storage leak jfs routine jfs link create hard links calls routine ucsname allocates bytes memory jfs link free storage returning leak occurs time jfs link called allowing user trivially denial service attack repeatedly creating hard links ignoring malice leaking storage hard link creation generally bad seemingly exploitable errors occurred ext caused lookup routines distinguish lookups failed entry existed memory allocation failed bug attacker create files directories preexisting file directory hijacking reads writes intended original file user delete nonempty directories write access case creating directory entry ext call routine ext find entry entry exists ext find entry returns null directory entry created returns error code low memory conditions ext find entry return null directory entry exists shown figure routine iterates pages directory page allocation fails ext page returns null skip directory worth entries low memory ext page fail entry checked ext find entry return null user write aclinux- ext dir struct ext dir entry ext find entry struct inode dir struct dentry dentry struct page res page unsigned long start unsigned long npages dir pages dir struct page page null iterate pages directory page ext page dir err page code check entry existence return entry found bug error return ext page return null figure ext security hole ext find entry cess directory effectively create files subdirectories existing file hijacking reads writes intended original file error similar ext rmdir calls routine ext empty dir ensure target directory empty return ext empty diris directory entries memory allocation fails allowing attacker delete non-empty directories permission remaining errors occurred ext identical ext bugs caused disk read errors low-memory conditions bugs kernel crashes bugs caused kernel crash null pointer dereference errors due improperly handled allocation failures error vfs layer error reiserfs jfs interesting error jfs fsck failed correctly repair file system marked clean subsequent traversal file system panic kernel incorrect code cases code wrong thing sys create creates file disk returns error subsequent allocation fails application file created error interesting heavily tested code vfs layer shared file systems leaks addition leak mentioned jfs routine jfs unmount leaks memory unmount file system experience section describes experiences fisc development sources false positives false negatives design lessons learned fisc-assisted development checked preexisting file systems comprehensively study model checking helps development process responsiveness jfs developers allowed micro-case study fisc-assisted software development evolution series mistaken fixes found reported kernel panics jfs transaction abort function txabortcommit called transaction commit function txcommit memory allocation failed days jfs developers patch removed txabortcommit made txcommit call txabort applied patch replayed original model checking sequence verified fixed panics ran full model checking seconds segmentation faults vfs code examination revealed newly created inode inserted vfs directory entry cache transaction committed failed commit freed inode left dangling pointer vfs directory entry cache report back jfs developers days replied patch applied fixed specific error occurred ran fisc patched code found error fsck complain parent directory contained invalid entry remove parent directory bit worse original error bug outstanding caveats mind model checking nice properties makes trivial verify original error fixed comprehensive testing patches appears commercial software houses finds corner-case implications seemingly local seconds demonstrates violate important consistency invariants false positives false positives found fell groups bugs model checking harness understanding underlying file system checked code minor problem file system implementors system replaced problems arising imperfect understanding underlying model checker iteratively correct series slight misunderstandings internals file systems checked group false positives stemmed implementors intentionally ignoring violating properties check reiserfs kernel panic disk read fails circumstances fortunately false positives easily handled disabling check false negatives absence proving total correctness check things verification briefly describe largest sources missed errors exploring thresholds poor job triggering system behavior occurs crossing threshold glaring test small number files directories miss bugs happen directories undergo reorganization change representations sufficient number entries real examples include re-balancing directory tree structures jfs hashed directory structure ext fisc check mixture large small files inode representations file names directories span sector boundaries crash recovery multi-threading support model checker single-threaded system call interface single user process file system operations 
state transition runs atomically completion means interfering state modifications occur checked system terms high-level errors file system operations interleave partially completed transactions memory disk expect fruitful source bugs white-box model checking fisc flag errors sees instrument code miss low-level errors memory corruption freed memory race condition crash invariant violation fortunately model-check implementation code simultaneously run dynamic tools unchecked guarantees file systems provide guarantees handled current framework include versioning undelete operations disk quotas access control list support journaling data fact reasonable guarantees data block contents crashes fix lack agreedupon guarantees non-sync data crashes check metadata consistency crashes data blocks precede sync point corrupted lost complaint file systems directed acyclic graphs trees events file system operations failures bad blocks topological independence events subgraph affect disjoint subgraph events temporal independence creating files directories harm files directories broaden invariants check infer fs-specific knowledge techniques missed states state hashing potentially discard detail discard details possibly missing real errors fs-specific knowledge opens host additional state optimizations profitable knew interleavings buffer cache blocks fsck written blocks independent files dramatically reduce number permutations needed checking effects crash aggressively verified statement coverage file systems unexercised statements design lessons hard lesson learned sort heisenberg principle checking make inspection checking code perturb state checked system violating principle leads mysterious bugs history code traversing mounted file system building model drives point home initially extracted volatilefs single block device test driver mutated traversed create model volatile file system mutation design deadlocked file system operation multi-sector write traversal code read file system sectors written file system code responsible write holds lock file written lock traversal code acquire removed specific deadlock copying disk test driver operation traversing copy essentially creating file systems hack worked started exploring larger file system topologies point deadlock creation file system copy kernel memory preventing traversal thread successfully complete final hack solve problem create reserve memory pool traversal thread retrospect solution run kernels side side dedicated mutating disk inspecting mutated disk isolation straightforwardly remove perturbations checked system similar lesson system checked instrumented modified absolutely code hidden assumptions easily violated changing code kernel kernel memory allocators re-implemented previous work part leak checking replacement worked fine original context checking tcp caused checked file systems crash turned deliberately mangling address returned memory ways intimately depended original allocator page alloc worked promptly restored original kernel allocators related work section compare approach file system testing techniques software model checking efforts generic bug finding approaches file system testing tools file system testing frameworks application interfaces stress live file system adversarial environment frameworks comprehensive model checking require work required jam entire model checker view testing complementary model checking reason test file system apply model checking vice versa case effective tools find errors irrespective theoretical strengths weaknesses software model checking model checkers previously find errors design implementation software systems compare work model checkers similar approach execute system implementation directly resorting intermediate description verisoft software model checker systematically explores interleavings concurrent program unlike cmc model checker verisoft store states checkpoints potentially explore state verisoft relies heavily partial order reduction techniques identify control data independent transitions reduce interleavings explored determining independent transitions extremely difficult systems tightly coupled threads sharing large amount global data result verisoft perform systems including linux file systems checked paper java pathfinder similar cmc systematically checks concurrent java programs checkpointing states relies specialized virtual machine tailored automatically extract current state java program techniques paper applicable java pathfinder generic bug finding recent work bug finding including type systems static analysis tools roughly speaking static analysis examine paths compile code order check finding errors surface properties visible source lock paired unlock contrast model checking requires running code makes strenuous apply days weeks hours lets check executed paths executes code effectively check properties implied code log valid records thatfsck delete directories based experiences static analysis errors paper difficult approach testing view static analysis complementary model checking lightweight reason apply model checking conclusion paper shown model checking find interesting errors real file systems found errors resulted loss crucial metadata including file system root directory majority bugs resulted patches heavily-tested file systems modelchecked severity errors found appears model checking works context file systems relief applied full system model-checking contexts successfully underlying reason effectiveness context file systems complex things single worst source complexity recoverable state face crashes power loss single program point hope model checking show similar effectiveness domains reason vast array failure cases database recovery protocols optimized consensus algorithms acknowledgments dave kleikamp answering jfs related questions diagnosing bugs submitting patches andreas dilger theodore viro christopher andrew morton stephen tweedie ext ext oleg drokin vitaly fertman reiserfs ted kremenek last-minute comments edits grateful andrew myers shepherd ken ashcraft brian gaeke lea kissner ilya shpitser xiaowei yang monica lam anonymous reviewers careful reading valuable feedback ball rajamani automatically validating temporal safety properties interfaces spin workshop model checking software boehm simple garbage-collector-safety proceedings sigplan conference programming language design implementation brat havelund park visser model checking programs ieee international conference automated software engineering ase bush pincus sielaff static analyzer finding dynamic programming errors software practice experience clarke grumberg peled model checking mit press corbett dwyer hatcliff laubach pasareanu robby zheng bandera extracting finitestate models java source code icse swat coverity software analysis toolset http coverity das lerner seigle path-sensitive program verification polynomial time proceedings acm sigplan conference programming language design implementation berlin germany june deline ahndrich enforcing high-level protocols low-level software proceedings acm sigplan conference programming language design implementation june dill drexler yang protocol verification hardware design aid proceedings ieee international conference computer design vlsi computer processors pages ieee computer society engler chelf chou hallem checking system rules system-specific programmer-written compiler extensions proceedings operating systems design implementation osdi sept engler musuvathi static analysis versus software model checking bug finding invited paper international conference verification model checking abstract interpretation vmcai pages jan ext ext file system http fsprogs net flanagan freund type-based race detection java sigplan conference programming language design implementation pages flanagan leino lillibridge nelson saxe stata extended static checking java proceedings acm sigplan conference programming language design implementation pages acm press foster terauchi aiken flow-sensitive type 
qualifiers proceedings acm sigplan conference programming language design implementation june ganger patt soft updates solution metadata update problem file systems technical report michigan godefroid model checking programming languages verisoft proceedings acm symposium principles programming languages holzmann model checker spin software engineering holzmann code models proc int conf applications concurrency system design pages newcastle tyne ibm journaling file system linux http wwwibm jfs symbolic model checking kluwer academic publishers kleikamp private communication linux test project http ltp net musuvathi engler model checking large network protocol implementations proceedings symposium networked systems design implementation musuvathi park chou engler dill cmc pragmatic approach model checking real code proceedings symposium operating systems design implementation reiserfs http namesys sandberg goldberg kleiman walsh lyon design implementation sun network file system sivathanu prabhakaran popovici denehy arpaci-dusseau arpacidusseau semantically-smart disk systems usenix conference file storage technologies stress http weather apw projects stress waldspurger memory resource management vmware esx server proceedings symposium operating systems design implementation 
exploiting gray-box knowledge buffer-cache management nathan burnett john bent andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison ncb johnbent dusseau remzi wisc abstract buffer-cache replacement policy significant impact performance intensive applications paper introduce simple fingerprinting tool dust uncovers replacement policy specifically identify initial access order recency access frequency access long-term history determine blocks replaced buffer cache show fingerprinting tool identify popular replacement policies literature fifo lru lfu clock random segmented fifo lru-k found current systems netbsd linux solaris demonstrate usefulness fingerprinting cache replacement policy modifying web server knowledge specifically web server infers contents file cache modeling replacement policy set page requests show servicing web pages believed resident buffer cache improve average response time throughput introduction specific algorithms manage buffer cache significantly impact performance o-intensive applications knowledge hidden user processes determine behavior buffer cache implementors forced rely documentation access source code general knowledge buffer caches behave relying hoc methods propose fingerprinting automatically uncover characteristics buffer cache paper describe dust simple fingerprinting tool identify buffer-cache replacement policy specifically identify initial access order recency access frequency access historical information fingerprinting microbenchmarking techniques identify algorithms policies system test idea fingerprinting insert probes underlying system observe resulting behavior visible outputs carefully controlling probes matching resulting output fingerprints algorithms identify algorithm system test key challenge inject probes create distinctive fingerprints algorithmic characteristics isolated significant advantages fingerprints automatically identifying internal algorithms fingerprinting eliminates developer obtain documentation source code understand underlying system fingerprinting enables programmers sophisticated experience algorithmic knowledge improve performance fingerprinting uncover bugs hidden complexities systems development deployed finally fingerprinting run-time allowing adaptive application modify behavior based characteristics underlying system paper investigate algorithmic knowledge exposing current contents buffer cache recent work shown o-intensive applications improve performance information contents file cache specifically applications handle data disk flexible order access blocks buffer cache disk current approaches suffer limitations require underlying export information accurately identify presence small files buffer cache observe application model simulate state buffer cache replacement policy file accesses dedicated web server greatly benefit knowing contents buffer cache servicing requests hit buffer cache implemented cache-aware web server based nest storage appliance show web server improves average response time throughput paper make contributions introduce dust fingerprinting tool automatically identifies cache replacement policies based prioritize initial access order recency access frequency access historical information demonstrate simulations dust distinguish variety replacement policies found literature fifo lru lfu random clock segmented fifo lru-k fingerprinting software identify replacement policies operating systems netbsd linux solaris show knowing replacement policy cache-aware web server service requests satisfied buffer cache obtain substantial performance improvements rest paper organized begin section describing fingerprinting approach section show simulation identify range popular replacement policies section identify replacement policies current operating systems section show web server exploit knowledge buffer-cache replacement policy improved performance briefly discuss related work section conclude section fingerprinting methodology describe dust software identifying page replacement policy employed operating system manipulating blocks accessed forcing evictions observing blocks replaced dust identify parameters page replacement policy algorithm dust relies probes infer current state buffer cache measuring time read byte file block determine block previously buffer cache intuitively probe slow infers block previously disk probe fast infers block cache dust correctly distinguish replacement polices identify file block attributes existing policies select victim block replacement search database research literature documentation existing operating systems identified attributes replacement order initial access block fifo recency accesses lru frequency accesses lfu historical accesses blocks correctly identify combinations attributes replacement policy note operating systems replacement policies attributes dust considers replacement policies pages dirty size file page replacement cost replacement pages performed global process basis finally real systems file pages cached file meta-data systems prefer evict pages files meta-data longer cached future replacement policies utilize attributes fingerprint dust identify parameters basic framework dust extended goal identifying replacement policies primary components dust size buffer cache measured simple microbenchmark input remaining steps short-term replacement algorithm fingerprinted based initial access recency access frequency access dust determines long-term history replacement algorithm microbenchmarking buffer cache size manipulate state buffer cache interpret contents dust size buffer cache information readily common interface systems dust simple microbenchmark dust accesses progressively larger amounts file data notices blocks longer fit cache increase tested size steps step dust touches file blocks newly increased size fetch buffer cache step dust probes block measuring time probe verify block cache technique similar technique determine memory now-sort important features approach probing file block step algorithm independent replacement policy manage buffer cache algorithm works buffer cache integrated virtual memory system assuming dust memory buffer cache grow maximum size show fingerprinting algorithm robust slight inaccuracies estimation buffer cache size initial accces order higher recent file position block initial access order lru queue position higher newer file position block access recency access count file position block access frequency figure short-term attributes blocks graphs show priority block test region metrics order initial access recency access frequency access x-axis block number file forming test region y-axes initial accesses order left recency access center frequency access fingerprinting replacement attributes buffer cache size dust determines attributes file blocks shortterm replacement policy fingerprinting stage involves simple steps dust reads file blocks buffer cache simultaneously controlling replacement attributes block accessing blocks initial access recency frequency orders dust forces blocks evicted buffer cache accessing additional file data finally contents buffer cache inferred probing random sets blocks cache state file blocks plotted illustrate replacement policy describe steps detail configuring attributes step moves buffer cache well-controlled state data blocks resident initial access recency frequency attributes resident block control imposed performing pattern reads blocks single file refer blocks test region ensure data resident size test region set slightly smaller estimate buffer cache size precisely estimated cache size adjust size ten stripes discussed page aligned controlling initial access parameter block dust identify replacement policies based initial access order blocks fifo exert control access pattern begins sequential scan test region resulting initial access queue ordering shown graph figure specifically blocks end file priority remain buffer cache fifo-based policy dust identify replacement policies based temporal locality lru controlling recently block accessed ensuring ordering match initial access ordering ensure criteria pattern 
reads ten stripes file performed specifically indices file maintained left pointer starts beginning file pointer starts center test region workload alternates reading stripe left pointer stripe pointer pattern continues left pointer reaches center test region pointer reaches end controlled pattern access induces recency queue order shown middle graph figure specifically blocks end left regions priority lru-based policy finally identify policies frequency based component dust ensures stripes test region distinctive frequency counts reading stripes recency ordering dust touches stripe multiple times frequency ordering pattern stripes center test region read beginning end test region read number reads area test region shown right-most graph figure blocks middle priority lfu-based policy impose frequencies parts file part motivation dividing test region fixed number stripes instance block test region frequency count runtime dust exponential size file simulation experiments determined ten good number stripes precise fingerprint greater variety frequency recency regimes greater number stripes makes stripe smaller making data susceptible noise forcing evictions state buffer cache configured dust performs eviction scan file data read portion test region evicted cache goal evicting pages give information ability differentiate replacement policies dust evict approximately half cached data note eviction scan read page multiple times frequency counts pages higher pages test region dust identify frequencybased replacement policies eviction region replace pages illustrates limitations approach differentiate lifo mru mfu replacement policies replace eviction region feel limitation acceptable policies streaming large files tend behave similarly conditions probing file-buffer contents determine state buffer cache eviction scan perform probes measuring time read byte selected pages read call returns quickly assume block file resident cache read returns slowly assume disk access required noted perform probe block determine state state buffer cache specifically dust probes block disk block replace block previously buffer cache changing state perform probes selectively obtain number samples probe stripe times total twenty probes probes spaced evenly test region location chosen randomly half stripe keeping probes ensure interfere probe due prefetching choosing random offset probes run benchmark multiple times generate picture cache state running dust multiple times platform accurately determine cache replacement policy chooses victim pages based initial access recency access frequency access precisely size eviction scan set equal difference size cache size test region cache size half size cache hot cold evict evict figure access pattern fingerprint history distinct regions file blocks hot cold evict evict accessed set attributes evictions order toidentify history replacement algorithm arrow region accessed reads time move page width arrow number shows number times block read set frequency attributes fingerprinting history fingerprinting tool identify replacement policies single queue ranking blocks based attributes previous step controls short-term attributes blocks identify algorithms track blocks longer memory track recency block lru-k determine long-term tracking performed dust observes preference pages referenced evicted describe long-term history identified shown figure regions file blocks accessed test region divided separate regions half total cache size hot cold portion algorithm begins touching hot pages evicting touching evict region evict region sufficient blocks fill buffer cache hot pages longer cache historical information tracked dust touches hot cold regions times touches cold times point evict evicted cold preferred initial access recency frequency attributes replacement policy cold touched cold region preferred traditional lru lfu hot retouched additional hot region preference policies history step prior eviction rereference hot cold regions sequentially notice point hot region touched number times cold region touched migrated long-term queue lrucache cold region short-term fingerprint phase dust probe test region determine blocks read time microseconds file position simulated first-in first-out fingerprint read time microseconds file position simulated recently fingerprint read time microseconds file position simulated frequently fingerprint figure fingerprints basic replacement policies fifo lru lfu graphs show time required probe blocks test region file depending buffer cache replacement policy x-axis shows offset probed block y-axis shows time required probe low times block cache high times block cache left graphs simulate fifo lru lfu file cache hot region remains cache infer history cold region remains cache infer history identification history attributes specific replacement algorithm focus simple historical fingerprint simulation fingerprints illustrate ability dust accurately fingerprint variety cache replacement policies implemented simple buffer cache simulator section describe simulation framework present number results simulation results verify distinctive short-term replacement fingerprints produced pure replacement policies fifo lru lfu simple replacement policies random segmented fifo explore impact internal state replacement policy investigate clock two-handed clock demonstrate ability identify historical information replacement policy focusing lru-k conclude section showing dust robust inaccuracy estimate buffer-cache size simulation methodology simulator meant illustrate ability dust identify buffer cache replacement policies rest system simple specifically assume process running fingerprinting software ignore irregularities due scheduling interference model buffer cache fixed size contention virtual memory system simulations model buffer cache approximately pages finally assume reads hit file cache require constant time reads disk require basic replacement policies begin showing simulation results strict fifo lru lfu replacement policies precisely matches derive ordering graphs shown figure fingerprints simulations shown figure show dust identify random replacement segmented fifo fingerprints shown figure graphs observe levels probe times blocks cache verify approximately half test data remains cache examine basic policies turn fifo fingerprint shows half test region remains cache matches initial access ordering shown figure blocks end file priority lru fingerprint shows roughly quarter fourth quarter test region remains buffer cache expected behavior blocks accessed recently finally lfu fingerprint shows middle half file remains resident expected blocks highest frequency counts lfu fingerprint small discontinuous regions remain cache left main in-cache area behavior due fact stripe blocks frequency count in-cache regions part stripe beginning evicted fingerprinting random replacement policy stresses importance running dust multiple times single fingerprint run twenty probes exists probability random replacement behaves identically fifo lru lfu fingerprinting system times definitively random pages selected replacement illusread time microseconds file position simulated random fingerprint read time microseconds file position simulated segmented fifo fingerprint read time microseconds file position simulated segmented fifo fingerprint figure fingerprints random segmented fifo left-most graph shows random page replacement policy distinctive fingerprint run fingerprint pages evicted buffer cache middle graph shows segmented fifo buffer cache devoted secondary queue resulting fingerprint cyclic shift 
fifo fingerprint right-most graph shows segmented fifo buffer cache devoted secondary queue queue managed lru fingerprint identical lru trated graph figure horizontal lines indicating fast slow access times original vms system implemented segmented fifo sfifo page replacement policy sfifo divides buffer cache queues primary queue managed fifo non-resident pages faulted primary queue page evicted primary queue moved secondary queue page accessed secondary queue moves back primary queue key parameter sfifo fraction buffer cache devoted secondary queue denoted fraction devoted primary queue traditional choice fingerprinted middle graph figure resulting sfifo fingerprint cyclic shift pure fifo fingerprint reason pattern initial read test area sets contents primary secondary queues pages accessed left portion test area shifted secondary queue tail primary queue portion head primary queue pages touched set recency frequency attributes left portion test area moved back head primary queue portion shifted secondary queue end primary queue blocks evicted portion evicted blocks left portion queue sizes sfifo produces distinctive fingerprint uniquely identify policy asa increases sfifo behaves lru fingerprint identical lru shown figure secondary queue large time page touched time progressed secondary queue fingerprint reveals lru behavior policy matches lru fingerprint feel segmented fifo approximate lru high acceptable fingerprint distinguished lru replacement policies initial state clock replacement algorithm popular approach managing unified file virtual memory caches modern operating systems ability approximate lru replacement simpler implementation clock algorithm interesting policy fingerprint pieces internal initial state initial position clock hand bit set ensure clock identified fingerprint initial state describe small modifications methodology guarantee behavior basic implementation clock buffer cache viewed circular buffer starting current position clock hand single bit page frame page accessed bit set replacement needed clock hand cycles page frames frame cleared bit clearing bits inspects frame clock approximates lru replacing pages bit set accessed time clock treats buffer cache circular initial position clock hand affect current fingerprint initial position clock hand simply determines block test region subsequent actions relative initial position positionis transparent dust modify fingerprinting methodology account hand position state bits impact fingerprint depending fraction set bits clock fingerprint fifo lru specifically extremes read time microseconds file position simulated clock fingerprint bits set read time microseconds file position simulated clock fingerprint bits set figure fingerprints clock replacement policy identify clock basic fingerprinting algorithm run time run bits set case clock behaves identically fifo shown graph left time run half bits set case clock fingerprint lru shown graph fingerprint fifo fingerprint lru describe intuition behavior simplest case frame starting clock hand allocated sequential pages test region result clock hand wraps back beginning buffer cache allocation dust touches page set attributes bit page set eviction pages test region replaced matching behavior fingerprint fifo policy note results identical behavior clock hand sweep frames clearing bits allocates test region sequentially left portions test region data randomly interleaved memory interleaving occurs pages allocated passes pass frames cleared bits allocated left-hand portion test region bits frames set bits remaining frames cleared pass remaining frames allocated righthand portion test region accesses set locality frequency attributes pages bits frames set eviction phase begins half pages left portions test region replaced frames set bits uniformly distributed coincidentally matches evictions lru policy distribution bits uniform fingerprint show blocks frames bits initially clear replaced case uniformly distributed consistent recognizable fingerprint identify clock dust brings initial state bits configurations observes resulting fingerprints steps configure bits dust sets bits allocating warmup region pages fills entire buffer cache touching pages intervening allocations bits set setting half bits slightly complex step set bits previous scenario step dust allocates pages warmup region bits set point clock hand pass entire buffer cache clearing bits find page evict final step randomly touch half pages setting bits dust configure state bits summary modify dust slightly account internal state running fingerprint dust allocates warmup region effect setting bits replacement policy implements resulting fingerprint fifo dust runs half bits set fingerprint fifo conclude bits underlying policy fifo fingerprint lru conclude clock underlying policy result running steps clock replacement policy shown figure replacement policies history show dust distinguish replacement policies long-term history begin briefly showing policies examined fifo lru lfu random segmented fifo clock history discuss detail behavior policies lru-k history figure shows long-term fingerprints representative policies history graph left lru fifo lfu segmented fifo identical shown graph shows results probing hot cold regions test data expected hot data evicted shown high probe times initial portion cold data evicted due size eviction region cold data preferred policies middle graph shows read time microseconds file position simulated lru history fingerprint read time microseconds file position simulated random history fingerprint read time microseconds file position simulated clock history fingerprint random initial bits figure history fingerprint short-term policies probes performed pages hot blocks left cold blocks test regions graph left shows fingerprint fifo lru lfu segmented fifo cold test region remains buffer cache policies prefer pages history graph middle shows random preference pages history history finally graph shows historical fingerprint clock ambiguous bits set bits properly set fingerprint identical leftmost graph read time microseconds file position simulated lrufingerprint correlated count read time microseconds file position simulated lrufingerprint correlated count read time microseconds file position simulated lruhistory fingerprint figure fingerprints lruthe graph shows short-term fingerprint lruwhen correlated count set case lrudisplaces pages frequency count second-to-last oldest graph shows short-term fingerprint lruwhen correlated count increased pages eviction frequency count higher evicted finally graph shows history fingerprint lruverifying prefers hot pages random preference hot cold data finally graph shows historical behavior clock difficult determine bits explicitly controlled graph bits set result hot cold regions interleaved file buffer region replaced sequentially illustrate clock history dust ensure bits cleared set initialization step history fingerprint clock identical graph figure fifo lru lfu segmented fifo random clock history making replacements lru-k replacement policy introduced database community address problem lru discriminate frequently infrequently accessed pages idea lru-k tracks -th page past replaces page oldest -th page -th traditional lru equivalent lrugiven exhibits benefits general case commonly lrufurther lruis sensitive parameter correlated period intuition accesses page period counted distinct setting correctly non-trivial task default lruis 
complex note implementation derived version provided original authors begin briefly exploring sensitivity lruto correlated period short-term fingerprints lruare shown graphs figure default resulting fingerprint variation pure lru shown left-most graph specifically stripe test region evicted lrusince stripe accessed second-to-last page initially referenced correlated period increased thata fingerprint similar lfu shown middle graph setting pages eviction region classified correlated replace pages frequency count greater read time microseconds file position simulated two-queue fingerprint read time microseconds file position simulated two-queue history fingerprint read time microseconds file position simulated secondary queue fingerprint figure fingerprints fingerprint shows short-term replacement policy fifo fingerprint shows history preferring pages accessed evicted fingerprint shows replacement policy pages main queue lru memory finally large accesses treated correlated pages second-to-last case behavior degenerates pure lru shown summary lruproduces distinctive fingerprint uniquely identifies approximate setting correlated period verify lruuses history graph figure shows historical fingerprint lruas desired hot region preference data cold region occurs second-to-last pages hot region recent second-to-last cold region replacement made hot region oldest secondto-last chosen algorithm proposed simplification lruwith run-time overhead similar performance basic intuition removing cold pages main buffer admits hot pages main buffer buffer cache divided buffers temporary queue shortterm accesses managed fifo main buffer managed lru pages initially admitted thea queue evicted reaccessed admitted structure remember pages accessed longer buffer cache experiments set buffer cache remember number past equal number pages cache show fingerprints figure graph shows short-term fingerprint identical fifo queue managed fifo short-term fingerprint access pages evicted expected result easily distinguished pure fifo observing history fingerprint shown graph historical fingerprint hot region remains buffer cache accesses moved buffer finally identify replacement policy employed long-term buffer setting initial access recency frequency attributes hot region forcing evictions methodology specific replacement policy describe detail fingerprint shown graph figure correctly identifies lru policy buffer note lruor policies history similar technique determine replacement strategy long-term queue explicitly setting state long-term queue requires knowledge policy short-term queue policy moving block queue fingerprintingtechnique long-term queue nature specific policy short-term queue sensitivity buffer size estimate set experiments verify robustness dust inaccuracies estimate size buffer cache estimate buffer cache size significantly actual resulting fingerprints identifiable estimate cache small dust touch pages force evictions occur estimate large dust evicts entire region short-term fingerprint sensitive estimate historical fingerprint short-term fingerprint observe presence absence stripes buffer cache historical fingerprint observe hot cold region half buffer cache figure shows short-term fingerprint lru distinguishable estimates real sizes replacement policies exception clock robust similar read time microseconds file position simulated lru fingerprint overestimate read time microseconds file position simulated lru fingerprint perfect estimate read time microseconds file position simulated lru fingerprint underestimate figure sensitivity lru fingerprint cache size estimate graphs show short-term fingerprints lru estimate size buffer cache varied graph estimate high graph estimate perfect graph estimate low fingerprints uniquely identify lru read time microseconds file position simulated clock fingerprint underestimate read time microseconds file position simulated clock fingerprint perfect estimate read time microseconds file position simulated clock fingerprint overestimate figure sensitivity clock fingerprint cache size estimate graphs show short-term fingerprints clock half bits set estimate size buffer cache varied clock expected lru graph estimate high graph estimate perfect graph estimate low clock fingerprint robust inaccuracies estimate algorithms degree clock replacement algorithm sensitive estimate due configure state bits specifically size warm-up region dust fill buffer cache accurate figure shows dust tolerant errors cache-size estimate identifying clock robust identifying algorithms platform fingerprints buffer caching modern operating systems complex simple replacement policies operating systems textbooks part complexity due fact filesystem buffer cache integrated virtual memory system current systems amount memory dedicated buffer cache change dynamically based current workload control effect dust minimizes amount virtual memory maximize amount memory devoted file buffer cache run dust idle system minimize disturbances competing processes section describe experience fingerprinting unix-based operating systems netbsd linux solaris fingerprints real systems variation simulations addition fingerprinting replacement policy buffer cache dust reveals cost hit versus miss buffer cache size buffer cache buffer cache integrated virtual memory system dust takes considerable amount time run real system generating sufficient number data points requires running iterations test scan eviction scan probes experiments allowed iterations found iteration seconds minutes depending system test note systems smaller buffer caches tested shorter period time test region smaller feel long running time acceptable system configuration dust run results stored made applications programmers experiments section run systems dual pentium iii-xeon processors physical ram scsi storage subsystem ultra rpm disks read time microseconds file position netbsd dust fingerprint read time microseconds file position netbsd history fingerprint figure fingerprints netbsd graph shows short-term fingerprint netbsd indicating lru replacement policy graph shows longterm fingerprint indicating history netbsd netbsd straightforward replacement policy systems examined begin fingerprint shown figure simulations examine shortterm long-term fingerprints graph figure shows expected pattern pure lru replacement dust produces fingerprint attempts manipulate bits infer netbsd implements strict lru clock conclusion verified graph figure showing netbsd history documentation inspection source code confirm finding fingerprints infer parameters specifically time reading byte page buffer cache order time disk varies machine physical memory netbsd devotes buffer cache easily shown fact history fingerprint devotes memory hot cold regions infer file buffer cache segregated system read time microseconds file position linux dust fingerprint bits cleared read time microseconds file position linux dust fingerprint random bits figure fingerprints linux graph shows short-term fingerprint linux bits set graph shows fingerprint bits untouched linux linux popular version linux kernel production environments section run nest web server top important understand fingerprint short-term fingerprint linux shown figure graph left shows results dust attempts set bits graph fifo investigate determine clock graph shows fingerprint bits left random state fingerprint noisy priority pages recently referenced pages fourth quarters filtering data verify pages quarters cache cache fingerprint similar lru fingerprint expected clockbased replacement algorithm examination source code documentation confirms replacement policy clock based finally buffer cache size close amount physical ram system conclude buffer cache integrated linux memory management system linux underwent large revision version fingerprint linux complex replacement scheme linux netbsd short-term fingerprint shown graph figure suggests linux recency frequency 
component clock graph dust shows linux history decision examination linux source code existing documentation confirms results linux maintains separate queues active inactive list memory scarce linux shrinks size buffer cache pages recently referenced bit moved active list inactive list inactive list scanned replacement victims form page aging age counter frame indicating desirable page memory scanning page evict page age decreased considered eviction page age reaches page candidate eviction age incremented page referenced solaris solaris presented greatest challenge platforms studied subsystem solaris studied believed two-handed global clock algorithm researchers noted non-intuitive behavior twohanded clock hand clears bits hand fixed distance selecting page replacement bit clear hands advanced unison bit page cleared opportunity re-referenced candidate eviction implemented simulator fingerprint two-handed clock identical fifo shown short-term fingerprint solaris shown graph figure out-of-cache areas left fingerprint strongly suggests solaris frequency aging component eviction decision addition clock graph figure shows historical fingerprint solaris data noisy shows clear preference hot region suggesting history page aging solaris fingerprint shows time service buffer cache hit significantly higher solaris linux fingerprint shows hit time read time microseconds file position linux dust fingerprint read time microseconds file position linux history fingerprint figure fingerprints linux graph shows short-term fingerprint linux indicating combination lru lfu graph shows long-term fingerprint indicating history hit time linux platform cache-aware web server section describe knowledge buffer cache replacement algorithm exploited improve performance real application modifying web server re-order accesses serve requests hit file system cache serve miss idea handling requests non-fifo service order similar introduced connection scheduling web servers work scheduled requests based size request schedule based predicted cache content re-ordering based cache content lowers average response time emulating shortest-job scheduling discipline improves throughput reducing total disk traffic approach key challenge implementing cache-aware server gray-box knowledge file caching algorithm determine files cache keeping track file access stream presented kernel web server simulate read time microseconds file position solaris dust fingerprint read time microseconds file position solaris history fingerprint figure fingerprint solaris graph shows short-term fingerprint solaris graph shows history fingerprint operating system buffer cache predict time data cache term algorithmic mirroring general powerful manner exploit gray-box knowledge important assumption algorithmic mirroring application induces traffic file system mirror cache accurately represent state real cache assumption hold general case multi-application environment feasible single application dominates filesystem activity server applications web server database management system perfect match mirroring methods nest storage appliance supports http access protocols nest configurable number requests serviced simultaneously requests received number queued pending requests completes default nest services queued requests fifo order term default behavior cache-oblivious nest modified nest request scheduler model current state buffer cache model updated time request scheduled nest bases model underlying file cache algorithm exposed dust nest model reorder requests requests files believed cache serviced note nest perform caching files relies strictly buffer cache cache mirror accurately reflect internal state nest reasonable estimate cache size current approach nest static estimate produced dust disadvantage approach estimate produced contention virtual memory system larger amount web server running increase robustness estimate plan modify nest dynamically estimate size buffer cache measuring time file access time low file cache high file disk comparing timings prediction provided mirror cache nest adjust size mirror cache performance evaluate performance benefits cache-aware scheduling compare performance cacheaware nest cache-oblivious nest workloads tests web server run dual pentium iii-xeon machine main memory ultra disks clients machines identical server main memory running client threads clients connected server gigabit ethernet server clients running linux shown section clock replacement algorithm cache-aware nest configured model clock algorithm configuration server approximately memory dedicated buffer cache experiments explore performance cache-aware nest vary estimate size buffer cache experiment workload client thread repeatedly requests random file set files figures show average response time throughput web servers apache web server cache-oblivious nest cache-aware nest function estimate cache-size begin comparing response time throughput nest apache figures nest incurs overhead flexible structure nest handle multiple transfer protocols ftp nfs achieves respectable performance web server reasonable platform studying cache-aware scheduling importantly adding cache-aware scheduling signifaverage response time cache size estimate cache-aware scheduling working set cache-oblivious apache cache-aware figure response time function cache size estimate response time cache-aware nest lowest estimate cache size closest true size cache icantly improves response time throughput nest servicing requests hit cache cache-aware scheduling improves average response time servicing short requests dramatically cache-aware scheduling improves throughput reducing number disk reads verified proc interface in-cache requests handled data evicted cache finally performance cache-aware nest improves estimate cache size closer real robust large range cache size estimates experiment workload created surge http workload generator surge workload approximately distinct files sizes zipf distribution approximately surge representative web workload presented surge workload measure qualitatively similar results main differences performance cache-oblivious nest relative apache degrades slightly average response time cache-oblivious nest seconds apache seconds result expected nest designed staging data grid optimized large files small files typical web workloads performance cache-aware nest sensitive estimate cache size performance improves approximately cache size estimate improved apache achieves future plan experiment web servers workloads related work idea algorithmic knowledge underlying operating system improve performance throughput cache size estimate cache-aware scheduling working set cache-aware apache cache-oblivious figure sensitivity cache estimate accuracy performance cache aware nest improves estimate cache size approaches true size buffer cache buffer cache approximately cache-oblivious nest apache shown comparison recently explored context gray-box systems work showed os-like service implemented information control layer icl algorithmic knowledge probes statistical analysis concrete solutions proposed developers icls obtain algorithmic knowledge paper show fingerprinting obtain graybox knowledge simple automatic manner fingerprinting system components determine behavior successfully contexts notably networking storage specifically fingerprinting uncover key parameters tcp protocol identify remote host primary difference fingerprinting tcp context identify policies arbitrary behavior implementations expected adhere specifications techniques similar dust determine characteristics disks size prefetch window prefetching algorithm caching policy fingerprinting shares common microbenchmarking specifically perform requests underlying system order characterize behavior simple probes microbenchmarks determine parameters memory hierarchy processor cycle time characteristics disk geometry view key difference fingerprinting microbenchmarking fingerprint discover policy algorithm employed underlying layer microbenchmark typically uncover specific system parameters idea discovering 
characteristics lower layers system knowledge higher layers improve performance traxtents file system layer operating system modified avoid crossing disk track boundaries minimize cost incurred due head switching exploit zero-latency access developed method predicting position disk head hardware support information determine rotational replicas service request giving software expanded knowledge hardware state approach involves informing application buffer cache replacement policy operating system sleds dynamic sets seek increase knowledge application operating system approach embellishing interface application explicit exchange types information case dynamic sets application ability provide knowledge future access patterns reorder fetching data improve cache performance sleds export performance data application enabling application modify workload based performance characteristics underlying system idea servicing requests web server order explored connection-scheduling web servers main thesis research performance obtained controlling scheduling requests web server approach static file size schedule requests cache-aware nest dynamic estimate contents buffer cache future work hope investigate interactions scheduling requests based file size cache content cache-aware web server similarities locality-aware request distribution lard clusterbased web servers lard front-end node directs page requests specific back-end node based back-end recently served page modulo load-balancing constraints front-end simple model cache contents backend improve cache hit rates approaches complementary lard partitions requests nodes cache content service requests order single node conclusions future work shown buffer cache replacement algorithms uniquely identified simple fingerprint fingerprinting tool dust classifies algorithms based initial access locality frequency history choosing block replace simple simulator shown fifo lru lfu clock random segmented fifo lru-k produce distinctive fingerprints allowing uniquely identified begun address challenging problem fingerprinting real systems running dust netbsd linux solaris shown determine attributes considered page replacement algorithm finally shown algorithmic knowledge revealed dust predicting contents file cache specifically implemented cache-aware web server services requests predicted hit file cache improving response time bandwidth future extend range policies dust recognize specifically adaptive policies eelru lrfu identified policies attributes size page cost replacing page current system visually interpret fingerprint graphs produced dust automate process well-known replacement policies long-term plan continue exploring fingerprinting subsystems cpu scheduler determine algorithmic knowledge user processes main challenge performing model simulation access inputs required accuracy finally investigating algorithmic knowledge infer contents file cache change contents acknowledgments brian forney tim denehy muthian sivathanu florentina popovici helpful discussion comments paper shepherd greg ganger anonymous reviewers helpful comments work sponsored nsf ccrngs- ccrccr- itrand wisconsin alumni research foundation apache foundation apache web server http apache arpaci culler krishnamurthy steinberg yelick empirical evaluation cray-t compiler perspective annual international symposium computer architecture iscapages santa margherita ligure italy june arpaci-dusseau arpaci-dusseau information control gray-box systems symposium operating systems principles sosp october arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations sigmod tucson barford crovella generating representative web workloads network server performance evaluation proceedings sigmetrics conference june bent venkataramani leroy roy stanley arpaci-dusseau arpaci-dusseau livny flexibility manageability performance grid storage appliance hpdcj bertoni understanding solaris filesystems paging technical report tr- sun microsystems cao felten implementation performance application-controlled file caching proceedings symposium operating systems design implementation pages crovella frangioso harchol-balter connection scheduling web servers usenix symposium internet technologies systems forney arpaci-dusseau arpaci-dusseau storage-aware caching revisiting caching heterogeneous storage systems usenix symposium file storage technologies fast monterey january glaser tcp stack fingerprinting principles http sans newlook resources idfaq tcp fingerprinting htm october johnson shasha low overhead high performance buffer management replacement algorithm proceedings international conference large databases pages september lee choi kim noh min cho kim existence spectrum policies subsumes recently lru frequently lfu policies sigmetrics atlanta georgia levy lipman virtual memory management vax vms operating system ieee computer march linux kernel archives linux source code http kernel mckusick bostic karels quarterman design implementation bsd operating system addison wesley netbsd kernel archives netbsd source code http netbsd nicola dan dias analysis generalized clock buffer replacement scheme database transaction processing sigmetrics performance neil neil weikum lru-k page replacement algorithm database disk buffering proceedings acm sigmod conference pages neil lrusource code ftp ftp umb pub lru-k lruk tar padhye floyd identifying tcp behavior web servers sigcomm june pai aron banga svendsen druschel zwaenepoel nahum locality-aware request distribution cluster-based network servers eighth international conference architectural support programming languages operating systems san jose california robinson devarakonda data cache management frequency-based replacement proceedings acm sigmetrics conference measurement modeling computer systems pages saavedra smith measuring cache tlb performance effect benchmark runtimes ieee transactions computers schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler griffin lumb ganger trackaligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey smaragdakis kaplan wilson eelru simple effective adaptive page replacement sigmetrics conference measurement modeling computer systems atlanta staelin mcvoy mhz anatomy micro-benchmark proceedings usenix annual technical conference pages berkeley june steere exploiting non-determinism asynchronyof set iterators reduce aggregate file latency proceedings acm symposium operating systems principles sosp pages saint-malo france october talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley turner levy segmented fifo page replacement acm sigmetrics international conference measurement modeling computer systems vahalia unix internals frontiers prentice hall van meter gao latency management storage systems proceedings fourth symposium operating systems design implementation osdi october van riel page replacement linux memory management http surriel lectures linux -vm html june worthington ganger patt wilkes online extraction scsi disk drive parameters proceedings acm sigmetrics performance conference measurementand modeling computer systems pages gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings fourth symposium operating systems design implementation osdi san diego 
