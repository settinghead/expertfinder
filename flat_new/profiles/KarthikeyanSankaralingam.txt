appears proceedings annual international symposium microarchitecture design space evaluation grid processor architectures ramadass nagarajan karthikeyan sankaralingam doug burger stephen keckler computer architecture technology laboratory department computer sciences texas austin cart utexas utexas users cart abstract paper survey design space class architectures called grid processor architectures gpas architectures designed scale technology allowing faster clock rates conventional architectures providing superior instruction-level parallelism traditional workloads high performance range application classes gpa consists array alus limited control connected thin operand network programs executed mapping blocks statically scheduled instructions alu array executing dynamically dataflow order organization enables critical paths instruction blocks executed chains alus transmitting temporary values back register file avoiding large unscalable structures limit scalability conventional architectures finally present simulation results preliminary design gpawith half-cycle routing delay obtain performance roughly equal ideal -way -entry window superscalar core inter-alu delay perfect memory perfect branch prediction ipc gpais ideal superscalar core achieving average ipc spec cpu mediabench benchmarks introduction microprocessor performance improved rate year past decades wider datapaths hardware support memory management contributed performance improvement microprocessors benefited levels integration allowed mainframe techniques fit single chip memory hierarchies speculation superscalar execution bulk performance growth faster clock rates copious research efforts instruction-level parallelism improved clock actual products current high-end superscalar processors typically sustain instruction cycle comparatively four-fifths performance growth solely faster clock rates annual increase mhz ghz clock rate improvements technology scaling deeper pipelines pipeline depths increasing factor decade growth end deeper pipelines reach limits number gates pipeline stage limit reached clock rates increase gate speeds estimated improve rate year performance improvements higher levels instructionand thread-level parallelism increasing wire resistance make achieving higher ilp conventional architectures difficult today agarwal estimate latency transmit signal dimension chip approximately clock cycles optimal repeater placement addition limiting number devices conventional core wire delays make memory-oriented microarchitectural structures slower making difficult sustain current levels ilp slow instruction issue windows rename tables branch predictors bypass networks register files caches reduce ipc clock feature sizes nanometers issues first-order design constraints alpha clustered functional units partitioned register file overcome wire delays intel pentium devotes pipeline stages solely routing information instruction distribution delivery values register file future microprocessors achieve ilp considerably higher today designs partitioned high clock rate future processors exploit increased device counts meet goals increased communication delays partitioning requirements paper introduce class architectures intended address problems faced future systems grid processor architectures gpas short designed enable faster clock rates higher ilp conventional architectures devices shrink wire delays increase computation core gpa consists two-dimensional array nodes small instruction buffer execution unit fine-grained computation nodes connected dedicated communication network passing operands data controlled single thread control maps large blocks instructions nodes masse organization eliminates centralized instruction issue window converts conventional broadcast bypass network routed point-to-point network similar vliw architectures compiler detect parallelism statically schedule instructions computation substrate topography dataflow graph matches mapping instructions issued dynamically execution order determined availability input operands gpa large structures reside critical execution path enhancing scalability wire resistance increases out-of-order execution achieved greatly reduced register file bandwidth associative issue window register rename table compiler-controlled physical layout ensures critical path scheduled shortest physical path banked instruction caches reside units issue instructions finally large instruction blocks mapped nodes single units computation amortizing scheduling decode overhead large number instructions gpa register file bandwidth reduced experiments show register file writes reduced direct communication producing consuming instructions set conventional uniprocessor spec cpu mediabench benchmarks simulation results show ipcs running substrate clocked faster conventional designs scale technology assuming small routing delays perfect memory perfect branch prediction gpa averages eleven instructions cycle benchmarks remainder paper organized section describes block-atomic execution model programs gpas programs mapped section describes gpaone implementation grid processor architecture section presents experimental results characterize pertinent aspects program behavior show potential actual performance gains section discusses design tradeoffs extensions gpa class machines section describes related work pertaining wide-issue dataflow-oriented machines finally section concludes discussion strengths weaknesses gpas plans future work block-atomic execution model execution model implemented grid processor architectures treats groups instructions atomic unit fetching mapping execution resources committing execution substrate collection alus architecturally visible named simplicity paper assume alus homogeneous execute instruction instruction groups block-atomic execution model instructions groups compiler group internal transfers control branches instruction group transfer control succeeding group group basic block predicated hyperblock run-time trace data consumed group types group outputs values created group subsequent groups group temporaries values produced consumed group group inputs values produced preceding groups read execution group begins block-atomic execution group temporaries forwarded directly producers consumers written back central storage group outputs written central storage register file group commits output control transfer instructions address succeeding group treated group outputs modifications memory maintained temporary storage group committed group execution compiler statically assigns instruction group named alus alu assigned instruction special move instructions read group inputs assigned register file execution instruction group proceeds group fetched mapped alus execution substrate instruction group stored instruction buffer alu similar reservation station statically assigned move instructions issued register file read group inputs forward values alus instruction operands arrived alu instruction executed data-driven execution model similar traditional dataflow machine instruction completes result forwarded alus holding consuming instructions register file result group output physical destinations operation result encoded explicitly instruction destination referred alu result directly target instruction operands delivered directly producers consumers point-to-point grid network broadcast alus operands forwarded location instructions buffered instruction encode source locations register names inputs outputs instructions group completed group committed group outputs written back register file updates memory carried subsequently group removed alus group mapped execution substrate event exception raised instruction group entire group re-executed exception serviced implementations execution model overlap fetch add add add beqz xdeac code schedule block-atomic codeb dataflow graph mover alualu- mover alui aluadd alui aluld alui aluadd alui aluadd alur alubeqz xdeac physical mapping register file branch register figure simple block-atomic mapping mapping execution subsequent group execution current group figure shows mapping execution group gpa part depicts original code fragment basic block consisting instructions part shows dataflow graph dfg group inputs group output block scheduled alus named aluthrough aluas shown part part shows physical mapping execution substrate consisting 
array alus moveinstructions shown part mapped register file read group inputs forward consumers aluand alutemporaries produced instructions alualu- aluand aluand written register file output group output written register file branch instruction implicitly writes result special branch register transfer control hyperblock key advantages block-atomic model effective number instructions group large yield long dependence chains benefit alu chaining grid experimental results section show compiler-generated group sizes significant predication eliminate control flow hazards execution model addresses challenges microprocessor performance scaling section fewer large structures involved execution centralized associative issue window register renaming table fewer register file reads writes lack structures instructions execute dynamic order expensive hazard checking broadcasting bypassing forwarding network scales poorly increasing execution width physical instruction layout corresponds dataflow graph communication producers consumers place short point-to-point wires instructions critical path afford longer communication latencies distant alus physical layout alus exposed instruction scheduler wire communication delays scheduler minimize critical path section describe implementation gpa realizes block-atomic data-driven execution model gpa implementation figure show high-level diagram gpaour grid processor architecture design alus arranged array shown -bygrid implementation instructions delivered instruction cache banks left side array block sequencer block termination control determines instruction groups map grid group completed committed instruction group inputs fetched register file banks injected top grid operands passed producer consumer instructions lightweight network shown mesh augmented diagonal channels memory accesses routed primary cache banks located side grid separate network architecture grid node shown figure note terminology node refers functional unit logic shown figure full-fledged processor program counter node input ports arriving operands instruction operand buffers router delivers values output ports grid network buffers hold instructions input operands operands arrived instruction execute router deliver values produced node alu routed node destination grid instruction operand buffers multiple entries enabling multiple instructions mapped single physical node frame consists single instruction slot grid nodes pictured single virtual grid additional slot frame virtual grid nodes scheduling dynamic data forwarding purposes coordinate grid node array slot label destination frame consists instructions instruction slot grid nodes grid frames capable mapping total instructions time groups larger frame allowed span multiple frames free frames block termination control i-cache bank i-cache bank i-cache bank i-cache bank i-cache bank d-cache bank d-cache bank d-cache bank d-cache bank register file banks queues oad tore con tenc router alu inst control input ports output ports rom ction encer figure high-level grid processor organization map speculatively fetched groups describe major features design instruction fetch map group instructions mapped grid consists predicated hyperblock hyperblocks single point entry multiple exits internal transfers control primary instruction cache consists multiple banks bank row hyperblock mapped grid bank reads row worth instructions delivers instructions horizontally grid instruction distribution wires taking cycles hyperblock mapped branch target predictors block sequencer predict succeeding hyperblock begin fetching mapping grid prior completion previous hyperblock instruction execution top grid resides register file three-ported bank column hyperblock mapped grid move instructions fetched delivered queues register file banks bank issue move instructions cycle injecting operands grid move instructions register number read location target alus grid operand arrives node control logic attempts wakeup select issue instruction frame identifier arriving operand operands present instruction issued alu completion result output router frame identifier address target alus operand arrives node cycle instruction operand arrived wait operands ready instruction selected issued operand routing physical locations consumers explicitly encoded producer instructions trade-off instruction fanout instructions encode large number target consumers instruction overly large targets results extra instruction overhead replicate values grid support consumers instruction results show producer instructions fewer consumers instruction consuming instructions data movement instruction called split instruction inserted schedule forward results multiple consumers inter-node network kinds delays inhibit back-toback execution instructions consecutive cycles routing delays transmission wire delays instruction wakeup delay delays induced contention wires ports nodes instructions critical path delays minimized sensitivity analysis features simulation results discovered amount contention grid substantial ports node sufficient router wire delays single important factor performance gpahyperblock control increase instruction group size reduce number branches program gpauses hyperblocks include predication multiple exit points hardware support provided execute hyperblocks predicated instructions early block exits predication strategies handling predication block-atomic execution model gpauses execute-all approach predicate paths execute path delivers result common instructions predicate approach implemented predicating leaf instructions dfg predicated region strategy reduces number instructions predicate delivered permits execution leaf instructions dfg predicate calculated reducing critical path special class instructions called cmove conditional move instructions implement predication add cmp add add beqz xdeac add cmp add cmove add beqz xdeac code schedule rescheduled code dataflow graph figure code grid processor accept input operand boolean condition create output cmove instructions types cmove cmove cmove instruction forwards input operand output condition true cmove forwards input condition false boolean condition met output produced figure shows predicated sequence code instructions form predicated region register defined conditionally redefined byi depending predicate figure shows rescheduled code cmove instruction inserted added ensure produced reaches dataflow graph rescheduled code shown figure data dependences shown solid lines predicate values passed dotted lines note predicate original region produces temporary created destroyed predicated region leaf node predicated region discuss alternatives support predication section approach chosen high performance instructions wait predicate calculated approach results efficient power approaches preferable power-constrained implementation early exits branch middle hyperblock called early exit hyperblock early exit branch gpamust ensure correct values ultimately written back register file memory maintain program correctness branch instructions executed serial order gpauses predication enforce sequentiality natural data dependences branch instruction predicated complement condition immediately preceding branch hyperblock branch executed previous branch predication early exits introduce potential output register produced points hyperblock gpashould guarantee reaches block termination control output instruction predicated condition immediately branch branch output written extra predication register produced multiple instructions block output instruction mapped blocks execute dataflow order instructions generating output values execute prior branch results filtered modify register file memory index number assigned instruction position static program order filter values block commit logic branch executes sends target global control outputs generated instructions branch discarded block commit logic block commit gpas benefit distributing execution state distribution makes decisions global control complicated hyperblock committed stores output register values produced 
presence early exits predication detecting values produced requires additional logic block termination control gpaemploys count output values statically hyperblock store register output fires send signal commit logic sums signals detect block completion output produced instruction predicate false signal commit logic policy means block committed instructions fired false predicate paths branches investigating specialized networks in-grid combining trees ease restriction block stitching gpa design fetch map execute serialized hyperblocks fetching mapping execution overlapped utilize frames functional units grid effectively hyperblock executing block speculatively fetched mapped address returned block level predictor concurrent execution multiple hyperblocks feasible similar previously proposed speculative threads register values passed concurrently executing blocks communicated in-grid network bypassing register file register inputs speculatively mapped blocks produced previously mapped block delivered consumers executing move instructions call mechanism block stitching block size register usage static dynamic inputs temp reads temp writes outputs branch exits memory conflicts adpcm dct mpeg gzip mcf parser ammp art equake table program characteristics memory access gpathe primary data cache resides right-hand side execution array load executes node forwards effective address targets node data cache performs memory access sends result load directly targets maintaining correct ordering loads stores explicit visible data dependence wellknown problem traditional load-store queues maintain sequential memory semantics loads stores arrive order queues critical path memory slow accesses wiredominated design ordering violations detected evaluation section presents study pertinent program characteristics evaluation gpaperformance set applications chose spec cpu floating-point benchmarks equake ammp art spec cpu integer benchmarks parser gzip mcf mediabench benchmarks adpcm dct mpeg enc analysis benchmarks compiled trimaran tool set generate hyperblocks spec benchmarks compiled train input set run ref input set mediabench applications compiled run input set compilations performed full optimizations characterize applications trimaran simulator performs functional execution trimarangenerated code modified track block size profiles register usage collected dynamic statistics billion instructions executed benchmark custom instruction scheduler accepts trimaran-generated hyperblocks inputs inserts overhead instructions schedules blocks gpathe scheduler assigns instructions nodes grid greedy critical path scheduling strategy strategy schedules instruction node longest path dfg shortest physical path grid proximity caches load instructions grid finally assume full floating-point integer units node node gpacan execute instruction estimated performance custom event-driven timing simulator modified functional front-end trimaran simulator generate execution trace hyperblock timing back-end simulate execution gpathe simulator accounts dynamic behavior including routing latencies contention wires grid input output ports node memory hierarchy levels data caches main memory next-hyperblock prediction fast-forwarded million instructions application simulate million instructions obtain timing results application characteristics table display characteristics benchmarks compiled trimaran compiler average number instructions hyperblock statically produced compiler shown column average number instructions dynamically executed hyperblock shown column sizes correspond instructions hyperblock overhead instructions move split cmove included static sizes instructions receive false predicates early exit included dynamic sizes unsurprisingly integer spec cpu benchmarks show smallest dynamic hyperblock sizes ranging instructions average columns show number register inputs hyperblock number temporary reads writes output values temporary reads writes values produced forwarded consumed grid require register file accesses significant reduction register file bandwidth achieved reduction lost large hyperblocks split fit gpa finite size branch exits column shows average number branches hyperblock larger numbers potential early exits require complicated control pass correct register values register file subsequent blocks group stitching supported finally rightmost column shows fraction hyperblocks store load address number significant greater benchadpcm ammp art dct equake gzip mcf mpeg parser fraction move split cmove figure overhead block-atomic execution marks indicating greatly reduced register spills due large number compiler-visible intra-hyperblock temporaries mechanism reduce penalty frequency load-store conflicts dependence speculation needed figure shows fraction overhead instructions move split cmove required block-atomic execution model splitinstructions added instruction requires targets average benchmarks approximately instructions split cmove instructions add execution overhead consuming instruction slots grid move instructions separate instruction cache bank consume execution resources reside dependences critical path discounting move instructions affect program binary size lowerlevel cache miss rates remaining overhead instructions consume instructions scheduled grid performance evaluation baseline gpaconfiguration grid frames node connected neighbors row express channels higher-level metal channels route operands bottom top grid double velocity short node-to-node wires simulated implementation integer add logical instructions require single cycle execute instruction latencies configured similar alpha assume long latency operations floating-point adds multiplies fully pipelined evaluate performance gpaconfigurations perfect realistic assumptions memory hyperblock-prediction modeling realistic memory system simulate memory hierarchy -way data caches -cycle access -way cycle cache -cycle physical memory latency modeling realistic branch prediction simulate -level global branch predictor -bit history entry pattern history table -entry -way branch target buffer assumed perfect instruction cache experiments gpa simulator model wrong-path execution misprediction occurs account delay adpcm dct mpeg gzip mcf parser ammp art equake ipc gpa alu-alu -way -ruu -way vliw figure performance comparison gpa superscalar stalling mapping subsequent hyperblock target permitting correct control flow resume assume ideal behavior store-load pairs address loads stalled store completes independent loads allowed issue ready assumption optimistic conventional system discussed section major features influence performance gpaare organization alus number alus layout interconnect network latency number ports node default gpaconfigura- tion assumes grid alus ports node interconnect network configured nodes directly node reached single hop components constitute routing delays passing operand producer consumer wire delay depends physical distance nodes producer consumer router delay node path default gpaparameters assume wire router consume quarter cycle totaling half cycle hop routing delay comparison alternate architectures figure shows direct performance comparison gpato idealized -way issue superscalar processor left bar benchmark cluster shows performance gpawhile bar shows superscalar processor white portions bar represent ipc assuming perfect memory perfect branch next-hyperblock prediction colored portions show ipc assuming realistic memory realistic branch prediction superscalar processor simulated simplescalar tools assumed -entry ruu instruction window reorder buffer assumed clock rates machines difficulty building -wide large window superscalar core full bypassing clocked frequency distributed-window gpawe simulated benchmark trimaran vliw processor simulator assuming -way issue perfect memory static branch prediction figure bar cluster shows performance vliw machine assumption perfect memory simulated vliw processor performs worse average superscalar procesperfect mem realistic mem ipc stitching ipc stitching stitch speedup stitch speedup adpcm dct mpeg gzip mcf parser ammp art equake table 
speedup achieved stitching sor realistic memory results show promise gpain benchmarks perfect memory branch prediction benchmarks realistic models gpademonstrates superior performance idealized large-window superscalar engine turn showed higher performance case vliw core disparity higher perfect memory prediction indicating gpahas higher performance potential improvements memory system branch predictor important gpas conventional superscalar cores code gpaperforms dct showing ipc perfect memory prediction realistic assumptions architecture harvest substantial ilp exists benchmarks gpaperforms substantially worse superscalar processor adpcm mcf parser due disparities compilers compaq compiler full optimization versus lower-performing trimaran compiler individual analysis benchmarks showed superscalar core perfect memory predictors achieves higher ipc ilp trimaran-generated code assuming ideal machine infinite resources extensions compiler infrastructure handle small loops loop-carried dependences improve performance benchmarks gpablock stitching concurrent block execution gpaefficiently utilizes frames functional units benefits block stitching table shows speedup obtained due stitching columns correspond baseline gpa configuration perfect memory prediction assumptions columns correspond gpa configuration realistic assumptions fourth columns show ipc achieved gpa stitching columns show speedups stitching block stitching roughly factor speedup perfect realistic assumptions results ability map multiple blocks speculatively gpa critical competitive performance sequential threads average hops data input temporary memory adpcm dct mpeg gzip mcf parser ammp art equake table average hops types data operands wire delay cycles hop ipc zero-router gpa -router gpa -router gpa -way -ruu -way vliw figure sensitivity wire delays routing delay results show routing delay operand producer consumer single largest determinant aggregate gpaperformance portions operand routing delay measure vary section significant components total delay number hops traversed inter-node wire delay router delay hop table shows average number hops needed route data grid gpabecause number input register values non-negligible input operands typically routed hops network scheduler effective reducing number hops needed temporaries require roughly hops average number hops memory varies routing operands represents performance bottlenecks sophisticated schedulers avoid inter-node wire router delay critical performance simulate gpaconfigurations router delays cycles wire delays varying cycles figure shows effects ipc benchmarks inter-node wire delays varied assuming perfect memory prediction bottom curve corresponds fixed router delay cycles gpaconfiguration wire router delays models back-to-back execution consecutive cycles performance limited availability functional resources instruction buffers horizontal lines show ipc vliw superscalar cores perfect memory predictor assumptions circled dot gpacontention connectivity ipc adpcm dct mpeg gzip mcf parser ammp art equake table sensitivity analysis features shows gpaconfiguration performance results shown figure wire delay gpashrinks cycles close factor improvement ipc achieved router delay wire delays set ipc router operation hop assumed parallel instruction execution shown table average number hops communication corresponds router wires traversed resulting effective communication delay baseline gpa gpa configuration wire delay affects performance router delay wire router delays analogous operand bypass delays conventional superscalar microarchitecture inter-cluster delays partitioned superscalar vliw processor baseline parameters assume quarter cycle delay wire router conversely assume superscalar processor dependent operations issue back-to-back cycles point-to-point communication gpa faster operand bypassing future superscalar processors analysis scope paper connectivity table presents effect connectivity contention ports ipc results presented table assume grid perfect memory perfect prediction column shows ipc baseline gpaconfiguration corresponds gpawith input output ports node fourth column corresponds gpawith contention ports wires grid infinite inter-node communication paths improves performance minimally indicating contention plays minor role examine effect richness interconnect grid varied number nodes node connected results shown columns correspond node connected neighbors row rows corresponds node connected neighbors rows average performance speedup simulating point-to-point paths node higher connectivity increase delays network due router complexity number wires height grid rows ipc gpa -way -ruu -way vliw figure sensitivity grid height adpcm dct mpeg gzip mcf parser ammp art equake relative ipc ipc limit gpa pmp gpa figure gpa effectiveness grid dimension figure shows sensitivity performance grid height performance increases linearly rows performance improvement tapers benchmarks perform rows loss performance rows added due longer routing delays reach bottom grid reach termination control pass express channel programs large ilp large block sizes benefit increase number rows benchmarks studied dct mpeg ammp art fall category gpa effectiveness figure shows fraction achievable ilp gpaexploits benchmark perfect memory prediction realistic assumptions bar normalized dividing gpaperformance ipc observed ideal machine ideal machine modeled simply traversing program dataflow graph dividing number instructions critical path length ideal ipc benchmark resides atop bar middle bar corresponds gpaconfiguration assuming perfect memory prediction pmp bottom bar corresponds configuration realistic assumptions memory prediction gpaexploits ilp benchmark results promising represent unoptimized first-cut gpa design performance benefits diminish add detail simulator equally improve enhance scheduler tune performance architecture section describe extensions alternatives gpathat improve performance design alternatives design space gpas large unknowns performance complexity trade-offs aspects system section describe unexplored opportunities performance tuning grid network block control logic memory system describe radical extensions block-atomic execution model eventually provide higher performance flexibility architecture performance tuning grid network design key determinants gpa performance logic wire delay producers consumers critical path clock rates close speed grid alus latency routers wires network short latency communicate alu results producer consumer depend heavily distance number hops grid network larger degree routers reduce number hops increase delay hop achieving balance communication producers consumers critical high performance trade-off similar found multiprocessor interconnection networks fine granularity operand network magnifies effect routers necessarily dedicated channels ports efficient flow control operand packets critical obtaining high performance reduce handshaking overhead required flow control protocols examining techniques pre-reserving network channels consumer node producer alu executing instruction techniques similar flit-reservation flow control proposed coarser-grained on-chip networks examining express channels reduce communication latency trimming number hops distant producer consumer alus circuit techniques minimize delays routers predication strategies gpas run datadriven execution model predication difficult implement problems predication communication predicate bits instructions predicated regions added complexity block termination control handle instructions receive false predicates simplest strategies send predicate bits instructions predicated region strategy avoids superfluous execution requires high bandwidth predicates alternately predicates root instructions data dependence subgraph controlled predicate reducing predicate fan-out approaches limit performance instructions predicated region wait predicate bit received current solution compiler predicates instructions update stable storage stores register file writes strategy critical path reduction predicate computation needed lower fanout predicate values needed expense efficient power memory 
system efficient delivery instructions alu grid relies placing instructions instruction cache bank avoid routing delays schedule holes due unused alu slots uncompressed version large discussed anticipate maintaining program code compressed format memory hierarchy instruction cache conserve capacity bandwidth data memory first-order challenge maintain proper ordering load store instructions memory locations existing design requires structure similar load store queue store-load forwarding exploring speculative conservative strategies detect ordering violations enforce ordering subsets load store instructions exploring scheme previously communicating store-load pairs speculatively communicate point-to-point messages bypassing memory system execution model extensions grid speculation addition speculative block mapping execution section gpa potentially support speculation inexpensive recovery single hyperblock dependence load store unknown gpa issue load speculatively fetch data memory pass instructions consume load determined needed result prior store entire hyperblock nullified grid employ selective re-execution injecting load result grid routed instructions mapped alus values trigger instructions dependence path load end block re-execution capability greatly reduce overheads rollback data misprediction frame management multiple frames essentially provide multiple logical processors grid substrate managing frames key execution model implemented gpa frames speculative mapping execution hyperblocks sequential program applications consist independent threads control frames support multithreaded execution model subset frames allocated thread manner chosen thread sequential speculative mapping data parallel mapping reuse threads share frame thread physically subset alus complexity level sharing appears prohibitively complex alu control section describes alu single instruction frame data triggered control examining extensions small amount additional local control alu site extensions include increasing amount local instruction data storage treating alus simple microcontrollers mapping reuse grid delivering kernels alu self-sequence instruction blocks limited intervention global grid controller potential exists mapping repeated hyperblock grid dynamically reinstantiating block locally refetching multiple iterations iterations executed frames related work goals high clock rate high ipc unique gpas prior approaches attempted static dynamic techniques discover execute critical path program numerous discuss section describe relevant related work dennis misunas proposed static dataflow architecture arvind proposed tagged-token dataflow architecture purely data-driven instruction scheduling programs expressed dataflow language culler proposed hybrid dataflow execution model programs partitioned code blocks made instruction sequences called threads dataflow execution threads approach differs conventional programming interface dataflow execution limited window instructions rely compiler instruction mapping reduce complexity token matching sense gpas hybrid approach vliw conventional superscalar architectures gpa statically schedules instructions compiler dynamically issues based data dependences efforts attempted enhance vliw architectures dynamic execution rau proposed split-issue mechanism separate register read execute writeback delay buffer support dynamic scheduling vliw processors grid processors share characteristics transport triggered architectures proposed corporaal mulder including data driven execution reducing register file traffic non-broadcasting bypass execution unit results looked naming mechanisms values reduce register pressure register file size smelyanskiy proposed register queues allocating live values software pipelined loops llosa proposed register sacks low bandwidth port-limited register files allocating live values pipelined loops patt proposed block-structured instruction set architecture increasing fetch rate wide issue machines atomic unit execution block instruction researchers exploring distributed partitioned uniprocessor designs waingold proposed distributed execution model extensive compiler support raw architecture raw architecture assumes coarsergrain execution grid processor exploiting parallelism multiple compiler-generated instruction streams ranganathan franklin empirical study decentralized ilp execution models sohi proposed multiscalar processors single program broken collections speculative tasks approach creating distributed window dynamic traces execution partitions work vajapeyam mitra proposed renaming temporary registers trace reduce needed global register file rename bandwidth gpas similar approach renaming performed statically unlike design gpaexecutes hyperblocks fine-grain dataflow fashion overlaps speculative tasks hyperblocks computation substrate finally uht investigating architecture intended exploit high ilp alus single execution core communication mechanisms grid processor architectures conclusion paper introduced grid processor architectures class microarchitectures intended enable continued scaling clock rate instruction throughput mapping dependence chains array alus conventional large structures register files instruction windows distributed alu array permitting scalability processing core delivering alu results point-to-point broadcasting gpas mitigate growing global wire delay overheads conventional bypass architectures initial studies sequential applications promising grid processor achieving ipcs ranging competitive idealized superscalar microarchitectures exceeding vliw microarchitectures clear gpas superior conventional alternatives find incremental equally good solutions wire delay clock scaling problems gpas disadvantages force data caches alus incur delays dependent operations due network router wires significant complexity frame management block stitching allowing successor hyperblocks execute speculatively significant interfere goal fast clock rates future architectures partitioned partitioning flow operations exposed compiler preserving dynamic execution techniques discussed future designs actively working refine microarchitecture gpaand hyperblock scheduler anticipation hardware complexity reduced undue burden software future work include exploration grid execution models mapping streaming media scientific vector multithreaded codes acknowledgments anonymous referees valuable suggestions earlier version paper trimaran support team rodric rabbah answering numerous questions trimaran project supported defense advanced research projects agency contract -cnsf career grants ccrand ccrtwo ibm partnership awards grant intel research council agarwal hrishikesh keckler burger clock rate versus ipc end road conventional microarchitectures proceedings annual international symposium computer architecture pages june arvind nikhil executing program mit tagged-token dataflow architecture ieee transactions computers august connors mahlke sias crozier cheng eaton olaniran hwu integrated predicated speculative execution impact epic architecture proceedings international symposium computer architecture pages july burger austin simplescalar tool set version technical report computer sciences department wisconsin june corporaal transport triggered architectures phd thesis delft technology september corporaal mulder move framework highperformance processor design supercomputingpages november culler sah schauser von eicken wawrzynek fine-grain parallelism minimal hardware support compiler-controlled threaded abstract machine proceedings international conference architectural support programming languages operating systems pages april dally express cubes improving performance k-ary n-cube interconnection networks ieee transactions computers september dennis misunas preliminary architecture basic data-flow processor proceedings annual symposium computer architecture pages january fisher long instruction word architectures eliin proceedings tenth annual international symposium computer architecture pages june hao chang evers patt increasing instruction fetch rate block-structured instruction set architectures proceedings international symposium microarchitecture pages december hinton sager upton boggs carmean kyker roussel microarchitecture pentium processor intel technology journal kessler alpha microprocessor ieee micro march lee potkonjak mangione-smith mediabench tool evaluating synthesizing multimedia communications systems international symposium microarchitecture pages llosa valero fortes ayguade sacks organize register files vliw machines conpar vapp pages september mahlke lin chen hank bringmann 
effective compiler support predicated execution hyperblock proceedings annual international symposium microarchitecture pages june palacharla jouppi smith complexityeffective superscalar processors proceedings annual international symposium computer architecture pages june peh dally flit-reservation flow control proceedings international symposium high-performance computer architecture pages january ranganathan franklin empirical study decentralized ilp execution models international conference architectural support programming languages operating systems pages october rau dynamically scheduled vliw processors proceedings annual international symposium microarchitecture pages december rotenberg jacobson sazeides smith trace processors proceedings annual international symposium microarchitecture pages december national technology roadmap semiconductors semiconductor industry association smelyanskiy tyson davidson register queues hardware software approach efficient software pipelining international conference parallel architectures compilation techniques pact pages october sohi breach vijaykumar multiscalar processors proceedings international symposium computer architecture pages june taylor kim miller ghodrat greenwald johnson lee shnidman strumpen wentzlaff frank amarasinghe agarwal raw processor scalable -bit fabric embedded general purpose computing proceedings hot chips xiii august trimaran infrastructure research instruction-level parallelism http trimaran uht morano khalafi alba wenisch ashouei kaeli ipc resource flow computing levo technical report department electrical computer engineering rhode island kingston september vajapeyam mitra improving superscalar instruction dispatch issue exploiting dynamic code sequences proceedings annual international symposium computer architecture pages june waingold taylor srikrishna sarkar lee lee kim frank finch barua babb amarsinghe agarwal baring software raw machines ieee computer september 
appears annual international symposium computer architecture exploiting ilp tlp dlp polymorphous trips architecture karthikeyan sankaralingam ramadass nagarajan haiming liu changkyu kim jaehyuk huh doug burger stephen keckler charles moore computer architecture technology laboratory department computer sciences texas austin cart utexas utexas users cart abstract paper describes polymorphous trips architecture configured granularities types parallelism trips mechanisms enable processing cores on-chip memory system configured combined modes instruction data thread-level parallelism adapt small large-grain concurrency trips architecture out-of-order -wide-issue grid processor cores partitioned easily extractable fine-grained parallelism exists approach polymorphism performance wide range application types approach small processors aggregated run workloads irregular parallelism results show high performance obtained modes ilp tlp dlp demonstrating viability polymorphous coarse-grained approach future microprocessors introduction general-purpose microprocessors owe success ability run diverse workloads today application-specific processors desktop network server scientific graphics digital signal processors constructed match parallelism characteristics application domains building processors general purpose singlethreaded programs types concurrency provide substantive benefits terms system flexibility reduced design mask costs design trends applying pressure opposite direction designs specialized performance fragility applications incur large swings performance based map design result combination trends diversification workloads media streaming network desktop emergence chip multiprocessors cmps number granularity processors fixed processor design time strategy combating processor fragility build heterogeneous chip multiple processing cores designed run distinct class workloads effectively proposed tarantula processor integrated heterogeneity major downsides approach increased hardware complexity design reuse types processors poor resource utilization application mix balance ideally suited underlying heterogeneous hardware alternative approach designing integrated solution multiple heterogeneous processors build homogeneous processors die mitigates aforementioned complexity problem application maps homogeneous substrate utilization problem solved application limited heterogeneous processors solve fragility problem homogeneous hardware run wide range application classes effectively define architectural polymorphism capability configure hardware efficient execution broad classes applications key question granularity processors memories cmp polymorphous capabilities future billion-transistor chips thousands fine-grain processing elements pes fewer extremely coarse-grain processors success failure polymorphous capabilities strong effect answer questions figure shows range points spectrum granularities chip technology topologies exist shown diagram represent good cross-section space runs applications effectively exploits fine-grain parallelism effectively fpga millions gates pim proc elements fine-grain cmp in-order cores coarse-grain cmp out-of-order cores trips ultra-large cores figure granularity parallel processing elements chip ultra-fine-grained fpgas hundreds primitive processors connected memory banks processor-in-memory pim architecture reconfigurable alu arrays rapid piperench pact tens simple in-order processors raw piranha architectures coarse grained architectures consisting -issue cores power cyclops multiscalar processors proposed speculatively-threaded cmps polymorphous smart memories architecture wide-issue processors alus grid processors finer-grained architectures left spectrum offer high performance applications finegrained data parallelism difficulty achieving good performance general-purpose serial applications pim topology high peak performance performance control-bound codes irregular memory accesses compression compilation dismal extreme coarser-grained architectures traditionally capability internal hardware show high performance fine-grained highly parallel applications polymorphism bridge dichotomy competing approaches synthesis approach fine-grained cmp exploit applications finegrained regular parallelism tackles irregular coarsergrain parallelism synthesizing multiple processing elements larger logical processors approach builds hardware left spectrum figure emulates hardware farther partitioning approach implements coarse-grained cmp hardware logically partitions large processors exploit finer-grain parallelism exists approach polymorphous architecture outperform custom hardware meant application graphics processing successful polymorphous system run application classes ideally running small performance degradations compared performance customized solutions application paper proposes describes polymorphous trips architecture partitioning approach combining coarse-grained polymorphous grid processor cores adaptive polymorphous on-chip memory system goal design cores large providing maximal singlethread performance remaining partitionable exploit fine-grained parallelism results demonstrate partitioning approach solves fragility problem polymorphous mechanisms yield high performance coarse fine-grained concurrent applications successful competing approach synthesizing coarser-grain processors fine-grained components overcome challenges distributed control long interaction latencies synchronization overheads rest paper describes polymorphous hardware configurations exploit types parallelism broad spectrum application types section describes planned trips silicon prototype polymorphous hardware resources permit flexible execution highly variable application domains resources support modes execution call major morphs suited type parallelism instruction-level parallelism desktop d-morph section threadlevel parallelism threaded t-morph section data-level parallelism streaming s-morph section section shows performance increases morphs trips core scaled wide coarser-grain -wide issue processor conclude section building large partitiondram interface dram interface dram interface dram interface dram interface cache trips core execution node trips chip icache icache icache icache predictor block block control dcache dcache lsq lsq lsq dcache dcache lsq frame frame router control inst operands icache stitch table register file frame figure trips architecture overview polymorphous cores single homogeneous design exploit classes concurrency making approach promising solving emerging challenge processor fragility trips architecture trips architecture large coarse-grained processing cores achieve high performance singlethreaded applications high ilp augments polymorphous features enable core subdivided explicitly concurrent applications granularities contrary conventional large-core designs centralized components difficult scale trips architecture heavily partitioned avoid large centralized structures long wire runs partitioned computation memory elements connected point-to-point communication channels exposed software schedulers optimization key challenge defining polymorphous features balancing granularity workloads involving levels ilp tlp dlp maximize resources time avoid escalating complexity nonscalable structures trips system employs coarsegrained polymorphous features level memory banks instruction storage minimize software hardware complexity configuration overheads remainder section describes high level architecture trips system highlights polymorphous resources construct morphs sections core execution model trips architecture fundamentally block oriented modes operation programs compiled trips partitioned large blocks instructions single entry point internal loops possibly multiple exit points found hyperblocks instruction thread level parallel programs blocks commit atomically interrupts block precise meaning handled block boundaries modes execution compiler responsible statically scheduling block instructions computational engine inter-instruction dependences explicit block static set state inputs potentially variable set state outputs depends exit point block runtime basic operational flow processor includes fetching block memory loading computational engine executing completion committing results persistent architectural state proceeding block architectural overview figure shows diagram trips architecture implemented prototype chip architecture scalable larger dimensions high clock rates due partitioned structures short point-to-point wiring connections trips prototype chip consist polymorphous -wide cores array memory tiles connected routed network set distributed memory controllers channels external memory prototype chip built process targeted completion figure shows expanded view trips core primary memory system trips core grid processor family designs typically composed array homogeneous execution nodes 
integer alu floating point unit set reservation stations router connections input output reservation station storage instruction source operands reservation station valid instruction pair valid operands node select instruction execution execution node forward result operand slots local remote reservation stations alu array nodes directly connected nearest neighbors routing network deliver results node array banked instruction cache left couples bank row additional instruction cache bank issue fetches values registers injection alu array banked register file alu array holds portion architectural state execution nodes set banked leveldata caches accessed alu local grid routing network alu array block control logic responsible sequencing block execution selecting block backside caches connected secondary memory tiles chip-wide two-dimensional interconnection network switched network robust scalable connection large number tiles wiring conventional dedicated channels components trips architecture main types resources hardcoded non-polymorphous resources operate manner present view internal state modes operation examples include execution units nodes interconnect fabric nodes instruction cache banks type polymorphous resources modes operation configured operate differently depending mode type resources required modes disabled mode polymorphous resources frame space shown figure execution node set reservation stations reservation stations index nodes combine form physical frame combining slot nodes grid forms frame frame space collection frames polymorphous resource trips managed differently modes support efficient execution alternate forms parallelism register file banks programming model execution mode sees essentially number architecturally visible registers hardware substrate extra copies ways speculation multithreading depending mode operation block sequencing controls block sequencing controls determine block completed execution block deallocated frame space block loaded free frame space implement modes operation range policies govern actions deallocation logic configured block execute streaming applications loop applied multiple data elements block selector configured limit speculation prioritize multiple concurrently executing threads multithreaded parallel programs memory tiles trips memory tiles configured behave nuca style cache banks scratchpad memory synchronization buffers producer consumer communication addition memory tiles closest processor present special high bandwidth interface optimizes stream register files d-morph instruction-level parallelism desktop morph d-morph trips processor polymorphous capabilities processor run single-threaded codes efficiently exploiting instruction-level parallelism trips processor core instantiation grid processor family architectures similarities previous work important differences section achieve high ilp d-morph configuration treats instruction buffers processor core large distributed instruction issue window trips isa enable out-of-order execution avoiding associative issue window lookups conventional machines instruction buffers effectively large window d-morph provide high-bandwidth instruction fetching aggressive control data speculation high-bandwidth low-latency memory system preserves sequential memory semantics window thousands instructions frame space management treating instruction buffers alu distributed issue window orders-of-magnitude increases window sizes window fundamentally three-dimensional scheduling region dimensions correspond physical dimensions alu array z-dimension corresponds multiple instruction slots alu node shown figure three-dimensional region viewed series frames shown figure frame conn dataflow graph frame frame frame frame frame frame frame reg file figure d-morph frame management sists instruction buffer entry alu node resulting slice scheduling region fill scheduling regions compiler schedules hyperblocks region assigning instruction node space hyperblocks predicated single entry multiple exit regions formed compiler region array set frames hyperblock mapped called architectural frame a-frame figure shows four-instruction hyperblock mapped a-frame shown figure mapped buffer slots frames physical alu node communication block determined compiler schedules operand routing directly alu alu consumers encoded producer instructions relative offsets prior work instructions direct produced element a-frame lightweight routed network alu array maximum number frames occupied program block maximum a-frame size architecturally limited number instruction bits destinations physically limited total number frames implementation current trips isa limits number instructions hyperblock current implementation limits maximum number frames a-frame maximum number a-frames frames total multiblock speculation trips instruction window size larger average hyperblock size constructed hardware fills empty a-frames speculatively mapped hyperblocks predicting hyperblock executed mapping empty a-frame a-frames treated circular buffer oldest a-frame non-speculative a-frames speculative analogous tasks multiscalar processor a-frame holding oldest hyperblock completes block committed removed oldest hyperblock non-speculative released frames filled speculative hyperblock misprediction blocks past offending prediction squashed restarted a-frame ids assigned dynamically intra-hyperblock communication occurs single frame producer instruction prepends a-frame z-coordinate consumer form correct instruction buffer address consumer values passed hyperblocks transmitted register file shown communication figure values aggressively forwarded produced register stitch table dynamically matches register outputs earlier hyperblocks register inputs hyperblocks high-bandwidth instruction fetching fill large distributed window d-morph requires high-bandwidth instruction fetch control model program counter points hyperblock headers sufficient frame space map hyperblock control logic accesses partitioned instruction cache broadcasting index hyperblock banks bank fetches row worth instructions single access streams bank respective row hyperblocks encoded vliw-like blocks prepended header number frames consumed block next-hyperblock prediction made highly tuned tournament exit predictor predicts binary branch predicted exit hyperblock per-block accuracy exit predictor shown row table predictor detail generated exit predictor index btb obtain predicted hyperblock address avoid forwarding register outputs produced past predicted branch subsequent blocks memory interface support high ilp d-morph memory system provide high-bandwidth low-latency data cache maintain sequential memory semantics shown figure side trips core distributed primary memory system banks tightly coupled processing logic low latency banks interleaved low-order bits cache index process multiple non-conflicting accesses simultabenchmark adpcm ammp art bzip compress dct equake gzip hydro good insts block exit target pred acc avg frames window benchmark mcf mgrid mpeg parser swim tomcatv turb twolf vortex good insts block exit target pred acc avg frames window table execution characteristics d-morph codes adpcmbzip compr gzip ksimmcf parsertwolf vortex ipc perf mem perf mem ammpart dct equakehydro dmgrid mpeg swim tomcatvturb dmean ipc figure d-morph performance function a-frame count neously bank coupled mshrs cache bank partition address-interleaved load store queues enforce ordering loads stores mshrs load store queues cache banks interleaving scheme stores written back cache lsqs block commit secondary memory system d-morph configures networked banks non-uniform cache access nuca array elements set spread multiple secondary banks capable migrating data two-dimensional switched network connects secondary banks network high-bandwidth link bank parallel miss processing fills summarize accurate exit prediction high-bandwidth i-fetching partitioned data caches concurrent execution hyperblocks inter-block forwarding d-morph instruction buffers polymorphous out-of-order issue window effectively shown subsection d-morph results subsection measure ilp achieved mechanisms results shown section assume -wide issue core 
physical frames data cache requires cycles access instruction cache partitioned banks cycles hop alu array -cycle branch misprediction penalty exit predictor -cycle access penalty cache -cycle main memory access penalty optimistic assumptions simulator include modeling tlbs page faults oracular load store ordering simulation centralized register file issue wrongpath instructions memory system binaries compiled trimaran tool set based illinois impact compiler scheduled trips processor custom scheduler rewriter row table shows average number dynamically executed instructions block discounting overhead instructions instructions false predicates instructions past block exit row shows average dynamic number frames allocated block scheduler grid steady-state block exit prediction accuracies shown row benchmarks holds instructions distributed window average shown row table figure shows ipc scales number frames increased permitting deeper speculative execution integer benchmarks shown left floating point mediabench benchmarks shown a-frame bar additional ipc values showing performance perfect memory hashed fraction bar adding perfect branch prediction shown white increasing number a-frames consistent performance boost benchmarks permits greater exploitation ilp providing larger window instructions benchmarks show performance improvements a-frames bzip ksim tomcatv reach peak frames adpcm gzip twolf hydro cases large frame space underutilized running single thread due low hyperblock predictability cases lack program ilp graphs demonstrate control mispredictions large performance losses integer codes close average large window tolerate memory latencies extremely resulting negligible slowdowns due imperfect memory system benchmarks mgrid t-morph thread-level parallelism t-morph intended provide higher processor utilization mapping multiple threads control single trips core similar simultaneous multithreading execution resources alus memory banks shared t-morph statically partitions reservation station issue window eliminates replicated smt structures reorder buffer t-morph implementation multiple strategies partitioning trips core support multiple threads row processors frame processors row processors space-share alu array allocating rows thread advantage approach thread cache d-cache bandwidth capacity proportional number rows assigned disadvantage distance register file non-uniform penalizing threads mapped bottom rows frame processors evaluated section time-share processor allocating threads unique sets physical frames describe polymorphous capabilities required classes mechanisms frame space management holding nonspeculative speculative hyperblocks single thread d-morph physical frames partitioned priori assigned threads trips core dedicate frames single thread morph frames threads t-morph uneven frame sharing thread frames divided number a-frames speculative execution allowed thread additional register file space required storage hold state speculative blocks store state multiple non-speculative speculative blocks additional frame support needed thread-id bits register stitching logic augmentations a-frame allocation logic instruction control t-morph maintains program counters number concurrent threads allowed global history shift registers exit predictor reduce thread-induced mispredictions t-morph fetches block thread prediction made shared exit predictor maps array addition extra prediction registers copies commit buffers block control state provided hardware threads memory memory system operates d-morph per-thread ids cache tags lsq cams prevent illegal crossthread interference provided shared address spaces implemented t-morph results evaluate performance multi-programmed workloads running t-morph classified applications high memory intensive low memory intensive based cache miss rates picked benchmarks ran combinations benchmarks executing concurrently high memory intensive benchmarks low memory intensive benchmarks examine performance obtained executing multiple threads concurrently quantify sources performance degradation compared single thread executing d-morph running threads concurrently introduces sources performance loss inter-thread contention alus routers grid cache pollution pollution interaction branch predictor tables reduced speculation depth thread number frames thread reduced table shows t-morph performance trips core parameters similar baseline morph column lists combined instruction throughput running threads column shows sum ipcs benchmarks run separate core number frames thread t-morph comparing throughput column throughput column performance drop due inter-thread interaction t-morph column shows cumulative ipcs threads run trips core frames comparison column column performance drop incurred inter-thread interaction reduced speculation benchmarks throughput aggregate ipc thread speedup t-morph constant a-frames scaled a-frames efficiency efficiency threads threadsa threadsa table t-morph thread efficiency throughput t-morph experiments showed t-morph performance largely insensitive cache branch predictor pollution highly sensitive instruction fetch bandwidth stalls column shows t-morph efficiency defined ratio multithreading performance throughput threads running independent cores column column column breaks showing fraction peak d-morph performance achieved thread sharing trips core threads column shows estimate speedup provided t-morph versus running applications time single trips core assumption application approximately running time efficiency varies threads threads low memory benchmarks resident simultaneously provided highest efficiency mixes high memory benchmarks provided lowest efficiency due increased t-morph cache contention effect pronounced -thread configurations pairing high memory benchmarks equally efficient speedup provided multithreading ranges factor depending number threads summary benchmarks completely exploit deep speculation provided a-frames d-morph due branch mispredictions t-morph converts a-frames non-speculative computations multiple threads jobs future work evaluate t-morph multithreaded parallel programs s-morph data-level parallelism s-morph configuration trips processor leverages technology scalable array alus fast inter-alu communication network streaming media scientific applications applications typically characterized data-level parallelism dlp including predictable loop-based control flow large iteration counts large data sets regular access patterns poor locality tolerance memory latency high computation intensity tens hundreds arithmetic operations performed element loaded memory s-morph heavily influenced imagine architecture imagine execution model set stream kernels sequenced control thread figure highlights features s-morph s-morph mechanisms frame space management control flow programs highly predictable s-morph fuses multiple a-frames make super a-frame separate a-frames speculation multithreading loops streaming application unrolled fill reservation stations super a-frames code required set execution loops connect multiple loops run ways embedded program frames morph execution executed core trips chip similar function imagine host processor run set frames core running dlp kernels mode subset frames dedicated data parallel thread subset dedicated sequential control thread instruction fetch reduce power instruction fetch bandwidth overhead repeated fetching code block inner-loop iterations s-morph employs mapping reuse block reservation stations multiple times s-morph imbenchmark original iteration fused iterations kernel size inputs unrolling compute insts block total revitalizations insts outputs constants factor block size constants convert dct fir fft idea transform table characteristics s-morph codes morph memory system supera frame nuca cache srf chip memory tiles processor frame frame frame figure polymorphism s-morph plements mapping reuse repeat instruction similar rptb tms block instructions constitute loop execute finite number times determined runtime set iteration counter instructions iteration complete hardware decrements iteration counter triggers revitalization signal resets reservation stations maintaining constant values residing reservation station 
fire operands arrive iteration iteration counter reaches super a-frame cleared hardware maps block alus execution memory system similar smart memories trips s-morph implements imagine stream register file srf subset on-chip memory tiles morph memory tile configuration includes turning tag checks direct data array access augmenting cache line replacement state machine include dmalike capabilities enhanced transfer mechanisms include block transfer tile remote storage main memory tiles strided access remote storage gather scatter indirect gather scatter remote addresses access contained subset tile storage imagine programming model expect transfers tile remote memory orchestrated separate thread shown figure memory tiles adjacent processor core srf augmented dedicated wide channels bits row assuming -bit channels array alu array increased srf bandwidth s-morph dlp loops execute srf read acts load multiple word instruction transferring entire srf line grid spreading alus fixed pattern row grid data easily moved alu high-bandwidth in-grid routing network requiring data switch srf banks alu array streams striped multiple banks srf stores srf aggregated store buffer transmitted srf bank narrow channels memory tile memory tiles adjacent processing core configured conventional levelcache accessible unchanged levelcache hierarchy conventional cache hierarchy store irregularly accessed data structures texture maps results evaluate performance trips s-morph set streaming kernels shown table extracted mediabench benchmark suite kernels selected represent computation-tomemory ratios varying kernels hand-coded trips meta-assembly language mapped alu array custom scheduler akin d-morph scheduler simulated event-driven simulator models trips morph program characteristics columns table show intrinsic characteristics iteration kernel code including number arithmetic operations number bytes read written memory number unique run time constants required unrolling factor loop determined size kernel capacity super a-frame grid frames instructions instructions block includes computation instructions block size numbers include overhead instructions memory access data movement grid total constant count number reservation stations filled constant values register file iteration unrolled loop convert dct fft fir idea transform compute inst cycle d-morph s-morph s-morph ideal norevitalize figure s-morph performance register moves eliminated allowing constants remain reservation stations revitalizations number revitalizations corresponds number iterations unrolled loop unrolling kernels based -kbyte input output streams striped stored srf performance analysis figure compares performance d-morph s-morph trips core frames entry store buffer cycle revitalization delay pipelined -cycle srf access delay d-morph configuration experiment assumes perfect caches -cycle hit latencies figure shows s-morph sustains average computation instructions cycle counting overhead instructions address compute instructions factor higher d-morph idealized s-morph configuration employs frames revitalization latency improves performance compute ops cycle higher realistic s-morph alternative approach s-morph polymorphism tarantula architecture exploits data-level parallelism augmenting processor core alpha dedicated vector data path alus approach sustains flops cycle results trips s-morph provide competitive performance data-parallel workloads grid consisting alus sustains average compute ops cycle polymorphous approach superior area efficiency compared tarantula large heterogeneous cores srf bandwidth investigate sensitivity morph srf bandwidth investigated alternative design points load bandwidth decreased bits row store bandwidth increased bits row decreasing load bandwidth drops performance percentage drop augmenting store bandwidth increases average ipc performance improvement average trips core experiments show increased store bandwidth improve performance expected compute intensive kernels fir idea show sensitivity srf bandwidth revitalization shown norevitalize bar figure eliminating revitalization s-morph performance drop factor average effect due additional latency mapping instructions grid redistributing constants register file unrolled iteration unrolled loop dct kernel requires cycles fetch instructions assuming instructions fetched cycle cycles fetch constants banked register file overhead exposed unlike d-morph speculative instruction fetch s-morph hard synchronization boundaries iterations solution examining reduce impact instruction fetch overlap revitalization execution extensions configuration individual alus node act separate mimd processors technique benefit applications frequent data-dependent control flow real-time graphics network processing workloads scalability larger cores experiments sections reflect performance achievable application classes alus question granularity remains fixed single-chip silicon budget processors chip powerful processor address question examined performance application class function architecture granularity varying issue width trips core information determine sweet spot application class describe sweet spot achieved application class configurability trips system figures show aggregate performance ilp dlp workloads trips cores dimensions including selected benchmarks represent general behavior benchmark suite unsurprisingly benchmarks low instruction-level concurrency benefit trips cores larger class represented adpcm sees benefit benchmarks higher concurrency swim idea diminishing returns mgrid fft continue benefit increasing ilp single thread dlp ilp multiple threads adpcm vortex mean-dlow mgrid swim mean-dhigh ipc fft idea mean-s compute inst cycle -way -way -way -way ipc figure trips single-core scalability cmp throughput alu density table shows suited configurations applications column variations applications application domains demand large coarse-grain processors small fine-grain processors nonetheless single-threaded ilp dlp applications larger processors provide aggregate performance expense low utilization applications multithreaded multiprogrammed workloads decision complex table shows alternative trips chip designs ranging trips cores cores assuming die technology equivalent real estate construct alpha processors on-chip cache figure shows instruction throughput aggregate ipc bar representing core dimensions cluster bars showing number threads core number atop bar showing total number threads cores times threads core array worst performing large number threads configurations number cores due changing on-chip cache capacity total number alus instruction buffers full chip ample threads threads core design point topology matter total threads bars labeled threads configuration highest-performing results validate large-core approach core higher performance single-threaded ilp dlp workloads smaller core shows higher throughput smaller cores area threads exploring evaluating space-based subdivision tlp dlp applications time-based multithreading approach paper conclusions future directions polymorphous trips system enables single set processing storage elements configured grid preferred trips total dimensions applications cores adpcm vortex swim idea mgrid fft table trips cmp designs multiple application domains unlike prior configurable systems aggregate small primitive components larger processors trips starts large technologyscalable core logically subdivided support ilp tlp dlp goal system achieve performance efficiency approaching specialpurpose systems paper proposed small set mechanisms managing reservation stations memory tiles large-core processor enables adaptation modes diverse application domains shown modes achieve goal high performance respective application domain d-morph sustains ipc average serial codes t-morph achieves average thread efficiencies threads s-morph executes arithmetic instructions clock -alu core average core trips system distinct personalities s-morphs reality configurations composed basic mechanisms mixed matched execution models addition minor reconfigurations adjusting levelcache capacity 
require change programming model major challenge polymorphous systems designing interfaces software configurable hardware determining initiate reconfiguration extreme application programmers compiler writers fixed number static morphs programs written compiled static machine models extreme polymorphous system expose configurable mechanisms application layers enabling select configurations time reconfiguration exploring hardware software design issues development trips prototype system acknowledgments anonymous reviewers suggestions helped improve quality paper research supported defense advanced research projects agency contract -cnsf instrumentation grant eiansf career grants ccrand ccrtwo ibm partnership awards grants alfred sloan foundation peter donnell foundation intel research council tms dsp set volume mnemonic instruction set literature number spru march barroso gharachorloo mcnamara nowatzyk qadeer sano smith stets verghese piranha scalable architecture based single-chip multiprocessing proceedings annual international symposium computer architecture pages june baumgarte uckel vorbach weinhardt pact xpp self-reconfigurable data processing architecture international conference engineering reconfigurable systems algorithms june casc aval castanos ceze denneau gupta lieber moreira strauss evaluation multithreaded architecture cellular computing proceedings international symposium high performance computer architecture pages january chang mahlke chen warter mei hwu impact architectural framework multipleinstruction-issue processors proceedings annual international symposium computer architecture pages cintra mart nez torrellas architectural support scalable speculative parallelization shared-memory multiprocessors proceedings annual international symposium computer architecture pages june ebeling cronquist franklin configurable computing catalyst high-performance architectures international conference application-specific systems architectures processors pages espasa ardanaz emer felix gago gramunt hernandez juan lowney mattina seznec tarantula vector extension alpha architecture proceedings international symposium computer architecture pages goldstein schmit budiu cadambi moe taylor piperench reconfigurable architecture compiler ieee computer april jacobson bennett sharma smith control flow speculation multiscalar processors proceedings international symposium high performance computer architecture feb khailany dally rixner kapasi mattson namkoong owens towles chang imagine media processing streams ieee micro march april kim burger keckler adaptive non-uniform cache structure wire-delay dominated on-chip caches international conference architectural support programming languages operating systems asplos pages october lee potkonjak mangione-smith mediabench tool evaluating synthesizing multimedia communications systems international symposium microarchitecture pages mahlke lin chen hank bringmann effective compiler support predicated execution hyperblock proceedings international symposium microarchitecture pages mai paaske jayasena dally horowitz smart memories modular reconfigurable architecture proceedings annual international symposium computer architecture pages june nagarajan sankaralingam burger keckler design space evaluation grid processor architectures proceedings annual international symposium microarchitecture pages december ranganathan nagarajan burger keckler combining hyperblocks exit prediction increase front-end bandwidth performance technical report tr- department computer sciences texas austin september rixner dally kapasi khailany lopezlagunas mattson owens bandwidth-efficient architecture media processing proceedings international symposium microarchitecture pages december sohi breach vijaykumar multiscalar processors proceedings international symposium computer architecture pages june steffan colohan zhai mowry scalable approach thread-level speculation proceedings annual international symposium computer architecture pages june talla john burger bottlenecks multimedia processing simd style extensions architectural enhancements ieee transactions computers pages tendler dodson fields sinharoy power system microarchitecture ibm journal research development january tullsen eggers levy simultaneous multithreading maximizing on-chip parallelism proceedings international symposium computer architecture pages june kathail schlansker rau hpl-pd architecture specification version technical report hpl- hewlettpackard laboratories february waingold taylor srikrishna sarkar lee lee kim frank finch barua babb amarsinghe agarwal baring software raw machines ieee computer september 
appears annual international symposium microarchitecture universal mechanisms data-parallel architectures karthikeyan sankaralingam stephen keckler william mark doug burger computer architecture technology laboratory department computer sciences texas austin cart utexas abstract data-parallel programs growing importance increasing diversity resulting specialized processors targeted specific classes programs paper presents classification scheme data-parallel program attributes proposes micro-architectural mechanisms support applications diverse behavior single reconfigurable architecture focus broad kinds data-parallel programs dsp multimedia scientific networking real-time graphics workloads programs exhibit high computational intensity coarse-grain regular control behavior regular memory access behavior show wide variance computation requirements fine grain control behavior frequency types memory accesses based study application attributes paper proposes set general micro-architectural mechanisms enable baseline architecture dynamically tailored demands application mechanisms provide efficient execution spectrum data-parallel applications applied diverse architectures ranging vector cores conventional superscalar cores results baseline trips processor show configurability architecture application demands harmonic performance improvement scalable flexible architectures performs competitively specialized architectures introduction data-parallel programs growing importance increasing diversity demanding increased performance hardware specialized hardware commonplace real-time graphics signal processing network processing high-performance scientific computing domains modern graphics processors sustain high gflops mhz programmable hardware suggests forty -bit floating point units software radios wireless baseband receivers developed digital signal processors require gops deliver adequate performance arithmetic processor earth simulator forty vector pipelines delivers peak performance gflops domains data-parallel applications common characteristics typically show differences types memory accesses computation requirements control behavior data-parallel architectures target subset dataparallel programs poor support applications subset vector architectures provide efficient execution programs regular memory accesses simple control behavior vector model effective programs require computation multiple vector elements access memory unstructured irregular fashion simd architectures provide support communication execution units enabling computation multiple data elements globally synchronized provide poor support applications conditional execution data dependent branches mimd architectures typically constructed coarse-grain processors operate larger chunks data single-program multiple data spmd execution model poor support fine-grain synchronization emerging applications real-time graphics exhibit control behavior requires fine grain mimd execution fine-grain communication execution units data-parallel applications consist components exhibit characteristics implemented specialized hardware units real-time graphics processing hardware specialized hardware coupled programmable components mpeg decoding tms dsp chip integrates specialized units targeted convolution encoding forward error correction processing specialized accelerators dedicated single narrow function architectures emerging consist multiple programmable data-parallel processors specialized ways sony emotion engine includes specialized vector units tuned geometry processing graphics rendering specialized behavioral physical simulation recently announced sony handheld engine integrates dsp core graphics core arm risc core single chip targeted distinct type data-parallel computation integrating specialized cores leads increased design cost area types processors designed integrated goal determine underlying set mechanisms combined ways tailor data parallel architecture based application demands paper identify characterize application demands data parallel program classes classes common attributes high computational intensity high memory bandwidth show important differences memory access behavior instruction control behavior instruction storage requirements result applications demand hardware capabilities varying simple enhancements efficient lookup tables execution models simd mimd based program attributes identified propose set general microarchitectural mechanisms augmenting memory system instruction control execution core build flexible data-parallel architecture show mechanisms combined ways dynamically adapt architecture providing support broad spectrum data-parallel applications mechanisms universal support type dlp behavior determined characterization application space applied diverse architectures ranging vector processors superscalar processors paper trips architecture baseline performance evaluation show rough comparison performance mechanisms current best-of-breed specialized processors application domain space trips processor suited data-parallel execution high functional unit density efficient alualu communication high memory bandwidth technology scalability dataflow style isa design relevant capabilities including ability map communication patterns statically dynamically issued execution enable straight-forward implementation mechanisms major isa programming model required execution core local operand storage alus distributed control major modifications required integrate mechanisms addition extra hardware registers status bits maintain local state storage tables previous work proposed evaluated configuration named s-morph trips processor targeted data level parallelism dlp remainder paper organized section discusses behavior attributes classes data-parallel applications section reviews classic data-parallel architectures mapping applications architectures section describes microarchitectural mechanisms supporting data-parallel execution section evaluates performance improvements provided mechanisms compares performance specialized architectures section discusses related work section concludes application behavior data-parallel workloads classified domains based type data processed nature computation varies domain domains applications vary simple computations image data converting color space comprising instructions complex encryption routines network packets comprising instructions broad categories cover significant part spectrum digital signal processing scientific network security real-time graphics section describe behavior applications categorized parts architecture affect memory instruction control execution core describe suite data-parallel programs present attributes program attributes abstract programming level data-parallel programs consist loop body executing parts input data data parallel architecture loop body typically executed execution units operating parts memory parallel refer loop body kernel typically iterations loop independent execute concurrently kernels exhibit types memory accesses control behavior varying computation data-parallel execution computation discrete cosine transform dct blocks image case parallelism exploited processing blocks image computation nodes concurrently processing instance kernel identical performed globally synchronous manner differread record write record instructions write read read record write record instructions write read write read read record write record instructions sequential data dependent branchingb static loop bounds figure kernel control behavior ent computation nodes complex data-parallel computation technique called skinning animation graphics processing dynamically varying number matrix-vector multiplies performed polygon vertex model vertices model operated parallel completely independent amount computation varies vertex vertex memory behavior memory behavior dataparallel applications classified types regular memory accesses irregular memory accesses named constant scalar operands indexed constant operands characterizing dlp programs interested frequency occurrence types accesses kernel types accesses exclusive kernel make accesses categories regular memory data-parallel kernels typically read memory structured manner strided accesses term record refer group elements single iteration kernel operates image processing record consist elements primary color components regularity accesses microarchitectures pipeline accesses amortize address calculation overheads accessing memory issuing instruction fetch full records irregular memory data-parallel kernels access parts memory random access fashion similar conventional sequential programs behavior texture accesses graphics programs unlike regular memory accesses overheads accesses amortized aggregating typical texture data structures graphics scenes require megabytes storage scalar constants operations data parallel kernels runtime constants unmodified full execution kernel constants 
convolution filters applied image number coefficients small stored machine registers memory indexed constants dlp applications require small lookup tables index determined runtime encryption kernels lookup tables -bit entries substitute byte byte computation accesses frequent kernels reducing performance long access latencies storing tables leveldata caches consumes storage space tremendous cache bandwidth control behavior complexity control structure kernel determines type synchronization instruction sequencing required figure shows types control behavior sequential instructions simplest kernels sequence instructions internal control flow degenerate case single vector operation dct transformed model unrolling internal computations kernel iteration kernels executes exact fashion kernels well-suited vector simd control figure shows type control behavior rgb yiq color conversion kernel pseudo-code simple static loops slightly complex type control behavior occurs kernel loops static loop bounds figure shows type control behavior encryption kernel pseudo-code simple instruction sequences iteration kernel executed vector simd style kernels unrolled compile time increasing code size kernel kernels transformation results prohibitively large instruction storage requirements architectures lack branching support graphics fragment processors rely complete unrolling execute loops runtime loop bounds figure shows generic control behavior data dependent branching kernels require masking instructions execute vector simd machines ideally suited fine-grain mimd machines processing element independently controlled local branching behavior benchmark description benchmark description multimedia processing network processing security byte packets convert rgb yiq conversion checksum dct dct image block rijndael rijndael aes packet encryption highpassfilter high pass filter blowfish blowfish packet encryption scientific codes fft -point complex fft decomposition decomposition dense matrix real-time graphics processing vertex-simple basic vertex lighting ambient diffuse specular emissive lighting fragment-simple basic fragment lighting ambient diffuse specular emissive lighting vertex-reflection vertex shader reflective surface fragment-reflection fragment shader rendering reflective surface cube maps vertex-skinning vertex shader animation multiple transformation matrices anisotropic-filtering fragment shader implementing anisotropic texture filtering table benchmark description runtime conditionals simple nested if-then-else statements make loop control templates complex data-parallel architectures traditionally implemented conditionals predication conditional streams vector masks finer partitioning control provided fine-grain mimd architecture reduce eliminate overheads conditionals highly synchronized architectures benchmark attributes table describes suite dlp kernels selected major application domains table characterizes kernels computation memory control criteria presented previously computation columns list number instructions inherent ilp kernel ilp number instructions iteration kernel divided dataflow graph height loop bound variable kernel completely unrolled memory column lists size record -bit words kernel reads writes column number irregular memory accesses fourth memory columns describe static coefficients kernel size lookup table indexed constants needed control column number loop iterations kernel loop bounds variable kernel instances case kernels exhibit data dependent control prefer fine grain mimd execution model anisotropic-filter kernel number instructions executed varies instance vector simd architectures lack support fine grain branching instance execute instructions predication techniques nullifying unwanted instructions collectively benchmarks exhibit wide variation attributes demonstrating diversity fundamental behavior dlp applications application study drive identification attributes complementary microarchitectural mechanisms ctrlinst ctrlinst ctrlinst memory memory inst ctrl memory mimd ctrl inst register filevector vector simd figure vector simd mimd architectures classic data-parallel architectures traditionally dlp architectures constructed applications exhibited characteristics section overview basic dlp architecture models highlights differences shown figure architectures differ principally implementations instruction control communication memory alus running dct illustrate differences architectures vector vector architectures examples include crayvectoriram tarantula global control single instruction fetch decode unit vector register file vrf serves staging area values memory alus back centralized control unit sequences vector elements vrf alus vector architectures provide efficient regular memory access vrf chaining hardware communication vector iterations place vrf global synchronization alus precludes data dependent branch control flow alu level vector machine dct decomposed dct columns transposition vrf dct rows simd simd architectures examples include cmand maspar global control single instruction fetch decode unlike vector architectures private memories present node values broadcast regular manner centralized memory simd machines provide mechanisms point-to-point communication neighboring alus lack vector register files efficient transcomputation memory control benchmark inst ilp record irregular constants indexed loop size words memory scalar bounds read write accesses constants convert dct highpassfilter fft blowfish rijndael vertex-simple fragment-simple vertex-reflection fragment-reflection vertex-skinning variable anisotropic-filter variable table benchmark attributes position support memory system severe limitation early simd machines lack efficient support irregular indexed memory accesses execute dct block image decomposed evenly processing elements alu performs part dct piece owns exchanges values neighbors complete full dct simd execution appeared conventional high performance microprocessors form sub-word parallelism multimedia extensions mmx sse altivec vis newer dsp-oriented processors imagine intrinsity fragment processing modern real-time graphics hardware mimd mimd machines memory organizations similar simd machines processing elements independently controlled local instruction control private instruction memories processing element mimd processors appeared variety granularities ranging iwarp coarse grained multiprocessors cmor modern smps ibm regatta system communication synchronization typically coarse grained message passing shared memory dedicated synchronization networks modern real-time graphics hardware moved providing fine grain mimd execution model vertex processing individual alus locally controlled operate parallel vertices dlp applications dct exploit mimd execution sufficiently fine grained architecture dct mimd computation similar simd execution instructions processing elements synchronized instruction level explicit synchronization instructions exchanging values applications architectures architecture suited application varies applications regular memory accesses static loops bounds control flow prefer vector simd architecture presence irregular memory accesses accesses indexed scalar constants significantly reduces performance conventional simd vector machines application exhibits data dependent branching vertex-skinning anisotropic-filtering fine-grain mimd architecture choice universal data-parallel architecture supports execution applications data parallel domain supports type behavior section data-parallel microarchitectural mechanisms figure shows block diagram abstract microarchitecture data-parallel architectures basic requirements execution substrate large number functional units efficient communication functional units shuffle data technology scalability previous section identified attributes data-parallel programs affecting main microarchitecture components column table summarizes attributes column lists proposed mechanisms targeted microarchitecture components shown column column lists benchmarks benefit mechanism mechanisms implemented memory system software managed streamed memory subsystem support high bandwidth regular memory accesses hardware managed cached memory subsystem support efficient irregular memory accesses execution core enhanced additional local operand storage efficiently support named scalar operand accesses additional software managed local data storage accessing indexed named conattributes mechanisms implemented benchmarks benefit regular memory access software managed streamed memory memory irregular memory access cached memory 
subsystem memory fragment-simple fragment-reflection scalar named constants local operand storage operand revitalization execution core register file convert dct highpassfilter rijndael graphics programs indexed named constants software managed data store alus execution core blowfish rijndael vertex-skinning tight loops local instruction storage instruction revitalization execution core instruction fetch data dependent branching local program counter control instruction fetch execution core vertex-skinning anisotropic-filtering table data-parallel program attributes set universal microarchitectural mechanisms mechanisms parenthesis trips specific implementations execution core bypass network functional units reservation stations reg file fetch memory main memory figure microarchitecture block diagram stants finally examining control behavior instruction storage alu execution core added supporting short simple loops local program counter alu added provide data dependent branching behavior sub-sections describe microarchitectural mechanisms detail implementation baseline data-parallel architecture architectures proposed large number functional units targeted subset dataparallel applications exploit instruction-level parallelism including imagine architecture raw architecture tarantula trips processor grid processor architecture family high-performance technology-scalable execution substrate paper processor baseline architecture describe evaluate set microarchitectural mechanisms conclude section discussion applying mechanisms architectures trips processor consists array alus connected lightweight routed network alu array local instruction storage data storage buffers banked instruction data storage caches array alus backed partitioned secondary level cache banks processor blockatomic model execution entire block instructions fetched mapped execution array dataflow style isa encodes instructions placement consumers statically dynamically issued spdi execution model dataflow style isa distributed control local storage inherently provided architecture makes implementation mechanisms straight-forward memory system mechanisms software managed cache figure shows configuration memory system high-bandwidth memory system regular accesses portions secondary-level cache banks reconfigured fully software managed cache smc configuration hardware replacement scheme tag checks cache banks disabled smc banks dma engine explicitly programmed software banks exposed fully managed programmer compiler regular memory accesses statically identifiable compiler smc bypass -cache temporal locality poor programming abstraction interface imagine stream register file srf manage smc wide loads overhead latency access smc reduced lmw load multiple word instruction reads lmw instruction issued alu fetches multiple contiguous values sends alus single row inside array reduce write port pressure store buffer coalesces stores nodes writing back smc high-bandwidth streaming channels deliver operands fast rate execution core dedicated channels provided smc banks row alus array based design natural partitioning cache banks rows alus cached -memory irregular memory accesses efficiently handled levelcache banks levelnot configured smc banks applications graphics rendering caching mechanism irregular texture lookups provide low latency access instruction fetch control mechanisms branching behavior data-parallel kernels dictate instruction fetch control requirements repeated fetching mapping kernel instructions reservation stations resulting instruction cache pressure dynamic cache access power mimd processing support kernels exhibit fine grain data dependent dcache dcache dcache dcache cache register file block control store buffers router control data local revitalize inst operands inst fast channels smc smc router control data cache software managed cache figure microarchitectural mechanisms memory system instruction operand revitalization -data storage local -instruction store provide mimd execution branching avoid repeatedly fetching instructions loop alus enhanced reuse instructions successive iterations reading local storage efficiently support data dependent branching alu augmented local program counter instruction revitalization trips processor alus local instruction storage efficiently support execution loops augment alus support re-using instruction mappings successive iterations loop mechanism call instruction revitalization works start kernel setup block executes repeat instruction run-time loop bounds kernel saved special hardware count registerctr instructions kernel mapped execution core execute iteration iteration completes determined block control logic ctr register decremented counter reached block control logic broadcasts global revitalize signal nodes execution array resets status bits instructions reservation stations priming executing iteration thectrregister reaches kernel execution commences amortize cost global revitalize broadcast delay blocks unrolled determined number reservations stations reduce number revitalizations figure shows datapath control path modifications added mechanism shaded regions reservation stations status bits required revitalization trips processor instruction revitalization vector simd-like architecture model local program counters support fine grain data dependent branching execution core configured mimd processing array adding local pcs alus simplify datapath add separate instruction storage instructions fetched executed sequentially slightly complex area efficient implementation re-use local instruction storage present alus read storage prior executing kernels mimd mode instructions loaded store executing setup block copies instructions memory storage resets local alu setup block terminates array alus begin executing mimd fashion node independently sequences fetching local instruction store operand storage buffers read write registers providing simple in-order fetch register-read execute pipeline figure shows schematic modified alu datapath support mimd model mimd model time startup delay instruction revitalization incurs revitalization delay iteration multiple nodes aggregated execute iteration kernel mimd model providing logical wide-issue machine iteration kernel inter-alu network fine-grain alu-alu synchronization configuration alu array partitioned multiple dynamically issued cores mode operation execute kernels alus passing values inter-alu network real-time graphics processing rendering pipeline implemented partitioning alus vertex processing rasterization fragment processing kernels alus homogeneous fully programmable partitioning alus dynamically determined based scene attributes strategy overcomes limitations current graphics pipelines vertex rasterization fragment engines specialized distinct units execution core mechanisms efficient scalar operand indexed scalar operand access supported data-parallel execution large statically unrolled loops reading values registers iteration loop expensive terms power register file bandwidth overheads register file access memory system indexed scalar operands incurs cache access overheads consumes cache bandwidth mechanisms implemented execution core support types accesses efficiently operand revitalization mechanism reuses register values received alu providing persistent register-file storage reservation station successive iterations loop reuse values reservation stations accessing global register file implement operand revitalization add status bits reservations stations shown figure data storage software managed data storage alu support indexed scalar constants lookup tables encryption kernels figures show data store accessed index computed instruction result written reservation stations index read data store provided alus results written back local registers shown applications examined sufficient store constants summary mechanisms trips processor baseline universal applicable architectures smc store buffer lmw instructions added straightforward manner conventional wide-issue centralized clustered superscalar architectures adding direct channels caches functional units augmenting pipeline wakeup instructions dependent loads operands arrive smc tarantula architecture similar support transfers memory vector register file hardware techniques generate conflict free addresses banks memory contrast approach packing regular accesses single 
bank support indexed scalar access irregular memory accesses architecture -cache memory addressable special scatter gather instructions conventional superscalar processors provide good support -cache memories reservation stations trips one-to-one correspondence reservation stations superscalar architectures instruction operand revitalization mechanisms applied provide instruction operand re-use dsp processors implemented zero-overhead branches ways support tight loops provide mimd support local pcs added local alu control logic modified fetch local instruction store buffer conventional simd vector cores conversely local storage augmented local storage buffers provide mimd model execution results section presents compilation strategy simulation methodology performance evaluation mechanisms results focus evaluating measuring performance improvement provided mechanism benefit mechanisms application performance flexible architecture constructed combination mechanisms flexible architecture performance relative specialized architectures simulation methodology baseline trips processor executes hyperblocks constructed impact compiler scheduled software schedulers event-driven timing simulator model microarchitecture mechanisms integrated simulator performance experiments programs hand-coded trips instruction set exploit data-parallel mechanisms simulated statically unrolled kernels fill instruction storage alus results show relative speedup measured terms execution cycles baseline machine configurations simulations assumed data resident software managed cache smc storage applications datasets applications fit smc baseline trips performance baseline configuration mesh interconnect array smc banks row processor total cache partitioned data instruction cache functional unit cache access latencies configured match alpha node processor consists integer alu integer multiplier fpu add multiply divide capability assumed technology clock rate making hop delay adjacent alus half cycle table shows performance baseline measured terms number computation operations sustained cycle including overhead instructions address compute load store instructions dsp programs sustain high computation throughput averaging ops cycle applications sustain low throughputs averaging ops cycle baseline trips processor optimized ilp converting data level parallelism applications sufficient infrastructure datasets realistic simulation anisotropic-filtering exclude performance tables figures benchmark ops cycle benchmark ops cycle convert fragment-reflection dct fragment-simple highpassfilter vertex-reflection fft vertex-simple vertex-skinning blowfish rijndael table performance baseline trips config store revitalization architecture model inst data inst ops simd s-o simd scalar constant access s-o-d simd scalar constant access lookup table mimd m-d mimd lookup table table machine configurations ilp results inefficiencies dlp programs include loops sufficiently unrolled provide large blocks efficiently utilize array alus scalar operand memory proceed shared structures cache common register file dlp programs large demands resources limited bandwidth prevents architecture achieving potential performance mechanism evaluation mechanisms section combined ways application requirements produce run-time machine configurations single flexible architecture frequency type memory access control behavior kernels instruction size kernels measured table determines ideal combination mechanisms trips processor paper focus machine configurations shown table cover application set examined configurations memory bank row configured software managed cache smc banks store buffers high speed channels communicate execution core combining memory system instruction revitalization mechanism creates baseline model similar simd vector machine baseline machine augmented operand revitalization create s-o machine s-o-d machine adds local data storage alu s-o machine combining memory system local pcs creates baseline mimd machine local data storage addition creates m-d machine figure shows application speedups obtained machine configurations relative baseline paragraphs classify applications preferred configurations benchmarks preferred preferred s-o preferred m-d configuration simd execution fft vector-oriented benchmarks require high memory bandwidth high instruction fetch rate compared baseline four-fold speedup achieved higher alu utilization higher memory bandwidth configuration adding mechanisms improve performance routing overhead mimd execution degrades performance slightly simd scalar operand access s-o performance applications dictated frequency scalar operand access constants vertex-reflection perform s-o machine configuration shown set programs figure simd scalar operand lookup table access s-od blowfish rijndael large lookup tables show speedups s-o configuration perform poorer m-d machine mimd baseline mimd configuration degrades performance relative s-o-d applications vertex-skinning degradation arises mimd model load instructions alu routed network reach memory interface previous simd configurations synchronized block boundaries multi-word load instruction memory interface behave vector fetch unit node operates independently mimd model schedule mimd lookup table access m-d mimd machine lookup table support performs blowfish rijndael vertex-skinning local looping control programs require instruction storage unrolled aggressively providing parallelism vertex skinning data dependent branching overheads predicated execution conditional vectors removed flexibility single bar labeled flexible figure shows harmonic speedups achieved flexible architecture subset mechanisms combined application running fft convert vertex simple light s-o rest m-d averaged applications flexible dynamic tuning properformance benchmark trips clock normalized specialized hardware hardware units convert mpc ghz iterations sec highpassfilter dsp processor iterations sec dct imagine ops cycle multimedia processor fft tarantula ops cycle vector core ops cycle cryptomaniac cycles block blowfish cycles block rijndael cycles block fragment-reflection nvidia quadrofx mhz million fragments sec fragment-simple graphics processor million fragments sec vertex-reflection benchmarked million triangles sec vertex-simple ghz pentium million triangles sec vertex-skinning million triangles sec table performance comparison trips dlp mechanisms specialized hardware fft convertdct speedup s-o s-o-d m-d s-o m-d flexible figure speedup mechanisms relative baseline architecture programs grouped machine configuration vides performance fixed configuration fixed s-o fixed m-d machine comparison specialized architectures table shows results rough comparison performance configurable trips architecture published performance results specialized hardware columns show performance column describes specialized hardware column shows performance metrics vary applications picked combination mechanisms trips baseline normalized clock rate trips specialized hardware scaling clock violate microarchitecture assumptions trips processor designed potentially faster clock rates conventional designs signal processing codes trips core s-o configuration times faster mpc improvement coming higher issuewidth trips core roughly times number functional units imagine architecture performs roughly times dct scientific codes trips configuration store bandwidth limited factor worse tarantula architecture network processing programs exploiting extensive data level parallelism network flows trips s-o s-o-d configurations perform order magnitude specialized hardware packets processed serially smaller numbers table programs performance vertex-simple graphics application trips outperforms dedicated hardware primarily higher issue width functional unit count fragment-simple hand specialized hardware outperforms trips roughly exact details number functional units fixed point floating point units quadrofx publicly disclosed part high performance attributed larger number functional units graphics processing kernels complex instructions constants data dependent branching case benchmarked perform kernels poorer related work classic vector processors built expensive srams high-speed memory large vector register files machines designed programs regular control data behavior tolerate degree irregular structured memory accesses scatter gather operations programs 
frequent irregular memory accesses lookup tables performed poorly number architectures proposed built overcome limitations rigid vector execution model dynamic instruction scheduling removing limitation make architectures widely applicable provided support subset data parallel programs short vector processing found commercial microprocessors form instruction extensions mmx sse altivec vis architectures similar requirements regular control data access restrictions data alignment isa extensions mmx sse poor support scalar-vector operations operating sub-word mmx sse register scalar register operand early fine-grain simd machines cmand maspar mpprovided high alu density lacked support fine-grain control latency tolerance irregular memory accesses addressing problems recently announced intrinsity processor includes simd array traditional cache memory system architectures provide support provided mechanisms propose complete data-parallel architectures imagine dubbed stream processor simd vector hybrid simd control unit coupled memory system resembling vector machine forms stream processing similar mimd execution streams data pipelined multiple processors highly structured fashion examples include graphics pipelines video processing mark describe motivation application requirements mimd processing real-time graphics finegrained on-chip mimd architectures smart memories raw emerging targeting style stream processing conclusions paper presents comprehensive treatment programs covering large spectrum dlp application space including signal processing scientific network security real-time graphics applications dlp applications domains studied paper provide comprehensive coverage application space identified key memory control computation demands dlp applications characterized behavior dlp application suite proposed set complementary universal microarchitectural mechanisms targeted memory system instruction control execution core support type dlp behavior memory system proposed streamed software managed cache memory hardware managed levelcache execution core instruction control proposed local operand storage local instruction storage software managed local storage local program counters alu site mechanisms combined ways based application demand powerful provide simd mimd execution model substrate found approach customizing architecture resulted performance fixed scalable architecture approach paper customizing architecture application similarities philosophy custom-fit processors customization propose enables execution models substrate performed fabrication compared application-specific processors domains architecture built mechanisms paper achieves performance similar range normalizing clock rate alu count application specific processor performs domain significant flexibility perform dlp applications domain mechanisms propose strictly limited trips processor paper hybrid simd fine-grain mimd execution models reasonable goal dlp architectures future systems execute multiple classes dlp applications benefit implementing mechanisms dynamically configuring architecture based application subset dlp behavior supported flexibility sacrificed simplicity implementing subset mechanisms fixed architecture matching mechanisms application attributes finally approaches foresee appearance mechanisms general purpose processors targeted identifying accelerating applications dlp space exhibit dlp behavior paper focused design performance evaluation dlp mechanisms directions future work mechanisms evaluated detailed metrics including cycle time power area heterogeneous architectures integrate multiple specialized data-parallel processors targeting distinct type workload present competing design philosophy comparison evaluation dlp mechanisms paper heterogeneous architectures yield interesting results acknowledgments anonymous reviewers suggestions helped improve quality paper research supported defense advanced research projects agency contract -cnsf instrumentation grant eiansf career grants ccrand ccrtwo ibm partnership awards grants alfred sloan foundation intel research council equipment donation nvidia akeley reality engine graphics proc ann conf computer graphics pages asanovic beck irissou kingsbury morgan wawrzynek vector microprocessor proc hot chips vii august blank maspar mparchitecture proc ieee compcon spring pages borkar cohn cox gleason gross kung lam moore peterson pieper rankin tseng sutton urbanski webb iwarp integrated solution high-speed parallel computing proc supercomputing pages november bove watlington cheops reconfigurable dataflow system video processing ieee transactions circuits systems video technology buck data parallel computation graphics hardware graphics hardware panel presentation connection machine cmtechnical summary april connection machine cmtechnical summary october espasa ardanaz emer felix gago gramunt hernandez juan lowney mattina seznec tarantula vector extension alpha architecture proc int symp computer architecture pages june espasa valero smith order vector architectures proc ann int symp microarchitecture pages december fernando kilgard tutorial addisonwesley publishing company fisher faraboschi desoli custom-fit processors letting applications define architectures proc ann int symp microarchitecture pages december hakura gupta design analysis cache architecture texture mapping proc int symp computer architecture pages june hennesy patterson computer architecture quantitative approach morgan kaufmann publishers kapasi dally rixner mattson owens khailany efficient conditional operations data-parallel architectures proc ann int symp microarchitecture pages kapasi mattson dally owens towles stream scheduling proc workshop media streaming processors pages december kozyrakis gebis martin williams mavroidis pope jones patterson yelick vector iram media-oriented vector processor embedded dram proc hot chips xii august kozyrakis patterson overcoming limitations conventional vector processors proc int symp computer architecture pages june kunimatsu vector unit architecture emotion synthesis ieee micro march mai paaske jayasena dally horowitz smart memories modular reconfigurable architecture proc int symp computer architecture pages june mark glanville akeley kilgard system programming graphics hardware language proc ann conf computer graphics mitchell radeon shading ati technologies white paper july moore padegs smith bucholz concepts system architecture proc int symp computer architecture pages june nagarajan sankaralingam burger keckler design space exploration grid processor architectures proc ann int symp microarchitecture pages december nvidia corp fragment program nvidia opengl extension specifications jan nvidia corp vertex program nvidia opengl extension specifications jan olson advanced processing techniques intrinsity fastmath processor embedded processor forum pharr humphreys design implementation physically-based rendering system draft edition august rajagopal rixner cavallaro programmable baseband processor design software defined radios proc ieee int midwest symp circuits systems rixner dally kapasi khailany lopezlagunas mattson owens bandwidthefficient architecture media processing proc ann int symp microarchitecture pages december russell craycomputer system communications acm january sankaralingam nagarajan liu kim huh burger keckler moore exploiting ilp tlp dlp polymorphous trips architecture proc int symp computer architecture pages june smith faanes sugumar vector instruction set support conditional operations proc int symp computer architecture pages june taylor kim miller wentzlaff ghodrat greenwald hoffman johnson jae-wook lee saraf seneski shnidman strumpen frank amarasinghe agarwal raw microprocessor computational fabric software circuits general-purpose programs ieee micro march tendler dodson fields sinharoy power system microarchitecture ibm journal research development january weaver austin cryptomaniac fast flexible architecture secure communication proc int symp computer architecture pages june 
appears international symposium high performance distributed computing distributed pagerank systems karthikeyan sankaralingam simha sethumadhavan james browne department computer sciences texas austin karu simha browne utexas abstract paper defines describes fully distributed implementation google highly effective pagerank algorithm peer peer systems implementation based chaotic asynchronous iterative solution linear systems implementation enables incremental computation pageranks documents entered deleted network incremental update enables continuously accurate pageranks centralized web crawl computation internet documents requires days suggests applicability distributed algorithm pagerank computations replacement centralized web crawler based implementation internet documents complete solution distributed pagerank computation inplace network converges rapidly accuracy iterations large systems time iteration long incremental computation resulting addition single document converges extremely rapidly typically requiring update path lengths nodes large networks accurate solutions implementation pagerank uniform ranking scheme documents systems integration keyword search solution network traffic problems engendered return document hits basic keyword search document hits returned querying node causing large network traffic incremental keyword search algorithm keyword search document hits sorted pagerank incrementally returned querying node proposed evaluated integration algorithm keyword search produce dramatic benefit terms effectiveness users decrease network traffic incremental search algorithm provided approximately ten-fold reduction network traffic two-word three-word queries introduction peer peer networks emerging potentially important structures information content management distribution effective keyword search crucial method information content management recent papers implementations keyword search documents called attention document ranking system documents stored network systems absence ranking system keyword search return qualified documents node initiated query amount traffic generated typical search flood network probable keyword search practical absence effective implementable document ranking algorithm crawler based centralized server approach computation ranks incompatible implementation keyword search google pagerank computation relevance rankings web pages made internet keyword searches orders magnitude effective google pagerank algorithm reduces computation eigenvectors large matrix represents document linkages web pages google web crawler traverses internet tabulating links documents returning link structure central server generates graph link structure ranks documents obtained eigenvalues sparse matrix representing graph iterative numerical method computational server process explicitly generating solving large order billion matrix takes days google search return pages assessed relevant search ranking results based computed pagerank fetching additional pages incrementally required paper reports method generating pageranks matrix representing relationships documents explicitly generated solution matrix distributed nodes hold documents method developed context keyword search networks implemented internet service formal structure network paper defines describes fully distributed peer peer equivalent google highly effective pagerank algorithm promising results performance distributed implementation solution phase pagerank computation based chaotic asynchronous iterative solution linear systems complete initial computation set pageranks converges rapidly accuracy iterations large systems algorithm enables incremental computation pageranks documents entered network incremental updates pagerank computations continuously updated compared weekly monthly updates characteristic centralized pagerank computations distributed computation system large computational server complexes eliminated keyword searches networks define approach based incremental return documents highest rank evaluate effectiveness integration distributed pagerank algorithm implementations keyword search incremental return documents produces dramatic benefit terms effectiveness users decrease network traffic engendered document transfer scheme potential make keyword search systems efficient internet keyword search rest paper organized section defines describes static version algorithm section describe caching document locations protocols joining leaving peers documents section detailed evaluation proposed distributed pagerank algorithm section briefly discusses centralized alternatives distributed algorithm section describes future directions section discusses related work section concludes static algorithm design section describe document link structure formulation queries static version distributed formulation pagerank algorithm pageranks computed in-place network documents links hyperlinks fundamental structuring elements internet documents web pages core data google computation pageranks files documents stored peer peer file systems equivalent link structure documents make documents file system systems bounded search pastry chord guid global unique identifier implements pointer document systems distributed hash table dht based systems systems freenet providing bounded search guarantees maintain unique identifier ssk subspace key paper focus dht based systems mechanisms proposed extended freenet systems document link structure html-like syntax assumed underlying assumption document originally html document documents freenet syntax freenet browsing web browser localhost fproxy google formulation pageranks google pagerank algorithm assigns numeric rank document system denotes importance intuition pagerank algorithm based random surfer model user visiting page click links equal probability random point decides move page formally pagerank defined inlinks pagerank document number outbound links page damping factor due lack space restrict treatment pagerank algorithm discussion details found matrix form equation rewritten matrix link matrix column vector length representing ranks pages constant column vector shown solution corresponds eigenvectors formulation shown iterative solution google pagerank system implemented internet web pages crawler crawls web pages internet writes central database database examined extract link structure matrix obtained ranks calculated central server dominant eigenvector page showed formally algorithm converges distributed pagerank formulation system documents reside peers links document point documents peers distributed algorithm computing pagerank documents systems peer initializes documents initial pagerank peer sends pagerank update message documents documents stores linked out-links pageranks nodes present peer updated network update messages receiving update message document receiving peer updates document pagerank update pagerank document system results generation pagerank update messages out-links difference successive pageranks document measure convergence absolute difference falls error threshold update messages document documents attain final pagerank times algorithm pseudo-code figure algorithm essentially distributed peer peer implementation chaotic asynchronous iterations linear equation solver peers acting simple state machines exchanging messages asynchronous iteration natural algorithm solving linear systems distributed networks central control global synchronization required peer executes process substantial literature mathematical properties chaotic interactions shown chazan miranker asynchronous chaotic iterations paper systems proven converge albeit typically slowly systems maintain guids full documents documents fetched peers outlinks extracted contents stored document documents saved issue replication document caching systems reduce retrieval time systems distributed pagerank computation work accurately pointers maintained document sources point cached copies document term peer peer-node refer peer computer system document node refer documents system initialize set pageranks nodes initial time concurrently peers documents peer compute newrank based inlinks relerr abs oldrank newrank newrank relerr epsilon send pagerank update message out-links peers weights out-links peer updated pass peer listens pagerank update messages pagerank update message received recompute newrank based message relerr abs oldrank newrank newrank relerror epsilon send pagerank update 
message out-links peers weights out-links peer updated figure distributed pagerank epsilon user defined error threshold testing convergence copies document correct computed pagerank integration peer peer search algorithms exact method pagerank search algorithms varies based underlying layer section sketches briefly pagerank algorithm utilized keyword search system fasd freenetlike systems dht based systems pastry chord propose incremental search algorithm efficient search pagerank dht based systems fasd freenet fasd metadata key representing document vector document metadata keys stored distributed manner search queries represented vectors documents match query close search vector make modification original fasd algorithm incorporate pagerank search scheme results forwarded based linear combination document closeness pagerank details found dht systems paper focus computing pageranks dht based systems coupling keyword search algorithms keyword search dht based systems typically implemented distributed index index entry keyword pointing documents keyword propose adding extra entry index store pageranks documents pagerank computed node index update message pagerank noted index search performed pagerank relevance sorting boolean multi-word queries inefficient dht based systems documents ids hits keywords passed peer peer word query avoid excessive traffic bloom filter based solutions proposed incremental searching propose incremental searching addresses issue network traffic caused multi-word queries incrementally returning documents sorted pagerank search scheme works boolean multiword query received term query examined routed peer owns part index term index accessed list documents index entry points examined sorted pagerank forwarding documents peer responsible term top hits forwarded peer receives small fraction albeit encompassing important documents peer procedure finds documents match term performs boolean operation sets documents resulting set sorted pagerank top hits forwarded peer procedure repeated term query practice found approach provided order magnitude reduction traffic approach coupled bloom filter based method provide reduction traffic extension dynamic systems section extend distributed pagerank computation handle dynamic behavior basic dynamic behaviors handled documents peers entering leaving system caching shown initial convergence pageranks incrementally updated peers documents added deleted entering leaving protocols system assuming peers alive times system rarely case documents regularly added deleted logically peers joining leaving documents added deleted identical fact peers join leave large set documents disappear difference document deleted forever peer leaves rejoin network time peer leaves joins peers leave system reappear documents transient behavior possibly result pagerank updates documents unavailable peers lost forever avoid peer detected unavailable update messages stored sender periodically resent delivered successfully worst case amount state saved scales linearly sum outlinks documents peer document inserts deletes document inserted network pagerank initialized fixed constant update messages outlinks immediately integrated distributed pagerank computation scheme similarly document removed pagerank update message pagerank negated pagerank update message received outlinked documents update pageranks system eventually reconverges caching dht based systems specific anonymity guarantees network traffic generated pagerank update messages reduced caching addresses peers pagerank update message document layer routing mechanism find location document location found address cached source node subsequent update messages exchanged directly source destination storage requirement scheme scales linearly sum outlinks documents peer graph size passes peers present table convergence rate distributed pagerank algorithm peers error threshold freenet order honor anonymity guarantees addresses cached manner update message individually routed intermediate nodes evaluation section evaluate distributed pagerank algorithm proposed key components modeled graph structure document links underlying system search queries subsequent sections describe modeling document link structure details simulation infrastructure discuss results categories convergence rate quality pagerank amount pagerank message traffic generated execution time quantitative effects document insertion deletion traffic reduction obtained pagerank search queries graph structure broder studied graph structure internet documents performing crawl approximately million nodes concluded link structure similar small world networks number nodes degree proportional numerically estimated in-degree out-degree determined hypothesize files storage systems show similar link structure synthesized graphs based model million nodes experiments node representing document link structure documents distributed computation simulation methodology explained graph representing documents constructed previous subsection document graph randomly assigned peer model logic distributed computation pagerank assuming underlying dht based system computation pagerank concurrently peers compute pagerank pagerank inlinks previous iteration pageranks computed assume pagerank messages received instantaneously peers start iteration concurrently passes sets peers randomly leave join network multiple iterations performed computation converges computation converges error absolute difference successive values pagerank document documents error threshold defined section strong convergence criterion network latency effects message routing system overheads modeled simulation experiments subsections simulate peers convergence table shows number passes required convergence graph sizes examined error threshold checking convergence nodes distributed peers system peers present column number passes convergence order convergence rate grows slowly problem size factor increase graph size increases number passes test quality pagerank computed pageranks conventional synchronous iterative solver compared error pagerank distributed asynchronous scheme pagerank conventional approach practice pagerank converges passes observed graphs nodes converged passes detailed analysis pagerank quality section dynamic effects evaluate effect peer joins leaves simulate fixed fraction randomly selected nodes leaving joining network end iteration table columns show results quarters peers half peers time algorithm converges presence transient effects albeit slower rate half peers present time factor slowdown convergence rate graph sizes threshold scale pages relative error nodes max avg relative error nodes max avg relative error nodes max avg relative error nodes max avg threshold graphs nodes error max graph nodes error max graph nodes error max graph nodes error nodes error max threshold graphs nodes error max graph nodes error max graph nodes error max graph nodes error max table relative error distribution error thresholds relative error relative error table expressed percentage scale shown row threshold number messages exec time hrs total avg total avg total avg total avg sec sec nodes nodes nodes nodes table variation message traffic error threshold total number messages shown millions execution time convergence shown hours kbytes sec kbytes sec networks quality pagerank examine quality pageranks generated distributed pagerank computation quality pageranks characterized relative error pagerank higher quality pageranks lower relative error produced lower error threshold convergence primary disadvantage low threshold increase number pagerank update messages execution time reduction network traffic quality pagerank opposing goals accurately characterize relationship error threshold pagerank quality simulated distributed pagerank scheme graph sizes error thresholds report statistical indicators maximum error pagerank thresholds average error documents distribution 
relative error document set table distribution error documents shown thresholds examined threshold values graphs table show collective data pages lines maximum relative error average relative error column lists percentages columns maximum error percentage pages note columns error reported scale fourth row table headings table threshold pages graph relative error pages relative error indicating pages relative error error distribution table threshold high performs extremely producing extremely good quality pageranks pages examining values column pageranks documents extremely close moderately high thresholds examining values graph sizes trends hold independent graph size threshold produces extremely good results graph sizes summary error results table shown left table quality pageranks achievable huge set documents high error thresholds remarkable message traffic examine number pagerank update messages generated pagerank computation lower error thresholds convergence produces higher message traffic table number pagerank update messages generated error thresholds shown graphs columns show total number pagerank update messages millions generated convergence columns show average number pagerank messages node obtained dividing columns graph size average number messages node graph size independent metric measuring message traffic table increase message traffic threshold approximately logarithmic threshold decreases factor message traffic increases factor message traffic node largely independent graph size suggests scalability algorithm large problem sizes execution time total execution time convergence distributed pagerank scheme strongly dependent network characteristics latency bandwidth congestion dependent characteristics processors implementing computations execution time estimated execution environments modeling typical peer peer network modeling case algorithm implemented web servers backbone service internet briefly sketched section peer peer implementation simplify analysis assumptions made homogeneous network peers present network transfer model peers collect pagerank messages generated pass single message transmit message network call peer peer serializes sending messages peers sending concurrently peers address caching scheme proposed section pagerank update messages directly exchanged peers routed time computational work constant passes execution times estimated model conservative assumptions serialized network transfer peers constant computational work pass computation time pass peer number document links peer peer size pagerank message average transfer rate network execution time pass peer estimated execution time pass peers based simulations pentium iii pentium class machines estimate computation required pass node graph order minute transfer rate peers peer peer networks vary considerable range conservative transfer rate kbytes sec estimate upper bound execution time aggressive transfer rate kbytes sec message size bytes message bits guid bits pagerank execution time convergence node graph shown columns table cases execution times measured hours totally dominated communication time decrease rapidly faster transfer rates web server implementation internet scale assume distributed algorithm implemented function web server average transfer rates networks connecting web servers expected rate line megabytes estimate execution time network billion documents days convergence threshold days convergence threshold execution times order magnitude current web crawler based centralized method distributed pagerank scheme incrementally incorporates document insertions deletions pageranks require frequent recomputations quantitative results document insertions discussed section graph converges passes correspond approximately days document insertions deletions evaluate effect document insertions deletions measure total number network messages generated document inserted document inserted outlinks figure propagation pagerank increments document inserts document inlinks pointing adding node equivalent adding extra column row matrix extra entry matrix equation row added matrix zeroes node inlinks coming column added values link present node node number outlinks node measure network traffic terms number pagerank update messages performing experiment graph sizes pick random node set pagerank initial pagerank case propagate pagerank outlinks outlink contribution messages reach outlinks turn send messages outlinks shown figure sends update message turn send update message incrementing pagerank amount increment increment turn point increment smaller error threshold point pagerank update messages generated measure path length total number nodes update message called node coverage node coverage upper bound number messages document insert generate adding multiple documents simultaneously generate fewer messages separately adding inserted documents links documents effect pronounced graph sizes grow table path length node coverage shown graph sizes error thresholds numbers obtained averaging results randomly picked nodes graphs path length node coverage largely independent grow extremely slowly graph size indicating scalability algorithm large anomaly node coverage threshold low threshold entire graph reachable graph node coverage limited threshold path length node coverage table path lengths node coverage graph size examining values column node coverage grows rapidly threshold linearly dependent expected document deletions document deletions similar document inserts pagerank message negative increment mathematically removing document equivalent deleting row column matrix pagerank summary previous subsections evaluated proposed distributed pagerank scheme wide range graph sizes convergence error thresholds results convergence quality pageranks message traffic execution time demonstrate scalability performance dynamic effects affected convergence rate small extent based results conclude error threshold ideal pageranks maximum error low message traffic generated graphs sizes table summarizes conclusions incremental search section presents preliminary results measuring effectiveness incremental search mechanism reducing network traffic executing keyword search queries key factors contribute reducing traffic firstly presence pagerank greatly reduces traffic multi-word queries presence ranking scheme ensures user sees convergence fast convergence high tolerance adaptability peer leaves joins good scalability graph size pagerank quality high typically error good scalability graph size message traffic low message traffic node constant logarithmic growth accuracy execution time low dominated network transfer time document insertion deletion handled naturally global recomputes required pageranks continuously updated table distributed pagerank computation summary important documents documents fetched incrementally requested built document corpus performing crawl set news web pages computed pagerank pages distributed pagerank scheme automatically synthesized search queries simulated measure reduction traffic documents search queries built document corpus consisting documents amounting storage removing common stopwords thresholding based frequently appearing terms document corpus reduced dimensional data two-word three-word search queries generated randomly combining top frequent terms randomly distributed documents peers computed pagerank distributed scheme search queries simulated node peer peer system search results simulated automatically synthesized search queries measure performance incremental search assumed search term query present peer baseline case word query set document ids transferred peers owning term term finally document ids transferred user twenty word queries experiments simulated instances incremental search algorithm top based pagerank hits transferred peer top hits transferred peer results shown table reduction traffic measured terms number document ids term queries term queries 
average traffic reduction top forwarded top forwarded average hits returned top forwarded top forwarded baseline table network traffic reduction incremental search pagerank transferred peers finally back user baseline compare system pageranks document ids transferred peers top hits forwarded factor reduction traffic obtained twoand three-word queries top hits forwarded factor reduction obtained cases number results returned manageable amount unlike baseline case reason fewer term hits top forwarding top simulation artifact top documents falls threshold results forwarded cases top amounts number top cases entire set hits greater forwarded centralized crawler implementation briefly address issue centralized crawler dht based storage systems firstly centralized crawler philosophy systems rudimentary centralized crawler generate extremely large amount traffic basically amounts fetching files system central storage server scheme undesirable efficient crawler based system transmit link structure central server compute pageranks redistribute ranks peers owning documents document inserts deletes handled peers directly communicate pagerank server avoids recrawl internet centralized crawler unworkable freenet based system anonymity guarantees future work current work focussed design computation scalability feasibility aspects pagerank computation systems simulated simple system model perform experiments future work implement distributed computation pagerank system interesting extensions optimizations address link structure documents mapping documents peers alleviate network overheads computation pagerank arasu noted utilizing structure web materially speedup convergence iteration approach closely related fast multipole method solving linear systems propose investigate adaptation fast multipole solver pagerank computations thirdly investigate effectiveness distributed asynchronous linear solutions executing systems problem domains generation elements matrices distributed network finally internet wide scalability distributed pagerank web servers computing pageranks service explored related work aware distributed implementations computation pagerank document relevance metrics great deal research speeding centralized computation eigenvectors markov matrices underlie pagerank computation appears basis limited results asynchronous iteration converge rapidly acceleration methods studied verbeke proposed framework peerto-peer computation coarse-grain parallelization jxta protocols closely related work conventional parallelization pagerank computation multiprocessors results found chen zhang authors compared synchronous asynchronous iteration processors found asynchronous iteration efficient conclusions question paper proposes evaluates distributed algorithm enables computations pageranks peer peer networks applies problem multiple word keyword searches peer peer networks distributed algorithm converges rapidly produces high quality pageranks enables incremental continuous computation pageranks documents added deleted formulation coupled standard information retrieval methods enable effective keyword search systems incremental search mechanism ten-fold network traffic reduction multi-keyword search queries networks defined evaluated primary target distributed pagerank computation peer peer networks algorithm extended straightforward manner world wide web vast corpus web pages augmenting web servers http protocol exchange messages web servers collectively responsible computing pageranks documents host web servers play role peers peer peer network exchange pagerank update messages application algorithm level practical removes central server computing pageranks periodic recrawls reflect assuming distributed algorithm applied scale glance plausible interesting possibility arises entire internet keyword index computed stored distributed manner efficient keyword search requires components pagerank keyword index coupling distributed keyword index web server computed distributed pagerank enable fully distributed pagerank based keyword search internet acknowledgments anonymous reviewers suggestions helped improve quality paper mike dahlin insightful comments suggestions research supported part national science foundation grant performance-driven adaptive software design control pagerank explained http webrankinfo english pagerank arasu novak tomkins tomlin pagerank computation structure web experiments algorithms nov bloom space time trade-offs hash coding allowable errors communications acm july broder kumar maghoul raghavan rajagopalan stata tomkins wiener graph structure web experiments models proceedings world wide web conference chazan miranker chaotic relaxation linear algebra applications cheng zhang parallelization page ranking google search engine http manip crhc uiuc chen pagerank clarke hong miller sandberg wiley protecting free expression online freenet ieee internet computing gnawali keyword-set search system peer-topeer networks thesis department electrical engineering computer science mit greengard rokhlin fast algorithm particle simulations greengard rokhlin version fast multipole laplace equation dimensions haveliwala efficient computation pagerank technical report stanford digital library technologies project haveliwala topic senstive pagerank proceedings eleventh international world wide web conference jeh widom scaling personalized web search proceedings twelfth international world wide web conferemce kamvar haveliwala manning golub extrapolation methods accelerating pagerank computations proceedings twelfth international world wide web conferemce kronfol fasd fault-tolerant adaptive scalable distributed search engine technical report princeton moler world largest matrix computation http mathworks company newsletter clevescorner oct cleve shtml page brin motwani winograd pagerank citation ranking bringing order web technical report stanford digital library technologies project ratnasamy francis handley karp shenker scalable content addressable network technical report tr- berkeley reynolds vahdat efficient peer-to-peer keyword searching technical report rice rowstron druschel pastry scalable distributed object location routing large-scale peer-to-peer systems proceedings ifip acm international conference distributed systems platforms middleware pages november sankaralingam sethumadhavan browne initial specification distributed pagerank scheme systems http utexas karu docs projectreport pdf stoica morris karger kaashoek balakrishnan chord scalable peer-to-peer lookup service internet applications proceedings acm sigcomm conference pages verbeke nadgir ruetsch sharapov framework peer-to-peer distributed computing heterogeneous decentralized environment proceedings international workshop grid computing november 
page curriculum vitae karthikeyan sankaralingam karu utexas http utexas karu work address department computer sciences texas austin station austin home address willow creek austin research interests high performance computer architecture microarchitecture vlsi compilers education bachelor technology indian institute technology madras computer science august texas austin computer science december texas austin work experience aug present research assistant texas austin stephen keckler douglas burger cart lab played leadership role trips processor microarchitecture design isa design design verilog implementation components trips processor led processor verification chip physical design efforts fall spring implemented powerpc port simplescalar simulator jan teaching assistant dept computer sciences texas austin computer organization programming spring instructor stephen keckler duties included discussion sections hours week holding office hours grading homework exams preparing solution sets addition head charge electronic grading programming assignments coordinating discussion sections tas jun aug internship ibm austin research lab added event-driven micro-architecture model powerpc port simos full system simulator addition worked caching techniques web servers resulted ibm technical disclosure aus patent patent awards honors james browne fellowship department computer sciences texas austin fifteen papers selected ieee micro top picks computer architecture outstanding research assistant department computer sciences annual competition outstanding graduate student employee texas austin page robert hamilton research paper award design space evaluation grid processor architectures cooperative society texas austin student presentation award international symposium microarchitecture micro december design space evaluation grid processor architectures patents method system enhanced cache efficiency utilizing selective replacement exemption keller patent assigned high-performance technology-scalable processor architecture nagarajan burger keckler patent application filed refereed conference papers sankaralingam nagarajan mcdonald desikan drolia govindan gratz gulati hanson kim liu ranganathan sethumadhavan sharif shivakumar keckler burger distributed microarchitectural protocols trips prototype processor international symposium microarchitecture micro december smith nagarajan sankaralingam mcdonald burger keckler mckinley dataflow predication international symposium microarchitecture micro december sankaralingam keckler mark burger universal mechanisms data parallel architectures international symposium microarchitecture micro december sankaralingam singh keckler burger routed inter-alu networks ilp scalability performance international conference computer design iccd october sankaralingam nagarajan liu kim huh burger keckler moore exploiting ilp tlp dlp polymorphous trips architecture annual international symposium computer architecture isca june sankaralingam sethumadhavan browne distributed pagerank systems international symposium high performance distributed computing hpdc june keckler burger moore nagarajan sankaralingam agarwal hrishikesh ranganathan shivakumar wire-delay scalable microprocessor architecture high performance systems international solid-state circuits conference isscc february nagarajan sankaralingam burger keckler design space evaluation grid processor architectures international symposium microarchitecture micro december student presentation award sankaralingam chakravarthy computer model flamelet distribution burner surface composite solid propellants aerospace sciences meeting conference exhibit reno nevada sankaralingam sankaralingam arbitrary boundary packed arithmetic international conference high performance computing hipc december page refereed journal articles sankaralingam nagarajan liu kim huh ranganathan burger keckler mcdonald moore trips polymorphous architecture exploiting ilp tlp dlp acm transactions architecture code optimization taco vol issue invited paper sankaralingam nagarajan liu kim huh burger keckler moore exploiting ilp tlp dlp polymorphous trips architecture ieee micro vol issue award top picks sankaralingam yalamanchi sethumadhavan browne pagerank computation keyword search distributed systems networks journal grid computing vol issue invited paper sankaralingam chakravarthy computer model flamelet distribution burning surface composite solid propellant combustion science technology vol refereed workshop papers sankaralingam nagarajan burger keckler technology scalable architecture fast clocks high ilp annual workshop interaction compilers computer architectures interactjanuary keller sankaralingam hofstee optimal file allocation strategy specweb workshop workload characterization september technical reports singh sankaralingam keckler burger design analysis routed inter-alu networks ilp scalability performance texas austin department computer sciences technical report tr- spring sankaralingam nagarajan keckler burger simplescalar simulation powerpc instruction set architecture texas austin department computer sciences technical report tr- february sankaralingam keller improved cache replacement scheme web servers ibm technical disclosure aus august professional activities reviewer micro asplos hpca ics isca isca ics isca micro student member acm sigarch ieee computer society personal information date birth february citizenship india visa status 
appears bfbl annual international symposium microarchitecture distributed microarchitectural protocols trips prototype processor karthikeyan sankaralingam ramadass nagarajan robert mcdonald rajagopalan desikandd saurabh drolia govindan paul gratzdd divya gulati heather hansondd changkyu kim haiming liu nitya ranganathan simha sethumadhavan sadia sharifdd premkishore shivakumar stephen keckler doug burger department computer sciences dddepartment electrical computer engineering texas austin cart utexas utexas users cart abstract growing on-chip wire delays future microarchitectures distributed hardware resources single processor nodes switched micronetworks large processor cores require multiple clock cycles traverse control distributed centralized paper describes control protocols trips processor distributed tiled microarchitecture supports dynamic execution details types reused tiles compose processor control data networks connect distributed microarchitectural protocols implement instruction fetch execution flush commit describe physical design issues arose implementing microarchitecture transistor asic prototype chip composed -wide issue distributed processor cores distributed nonuniform nuca on-chip memory system introduction growing on-chip wire delays coupled complexity power limitations severe constraints issue-width scaling centralized superscalar architectures future wide-issue processors tiled meaning composed multiple replicated communicating design blocks multi-cycle communication delays large processors control distributed tiles large processors routing control data tiles implemented microarchitectural networks micronets micronets provide high-bandwidth flow-controlled transport control data wiredominated processor connecting multiple tiles clients micronets higher-level microarchitectural protocols direct global control micronets tiles manner invisible software paper describe tile partitioning micronet connectivity distributed protocols provide global services trips processor including distributed fetch execution flush commit prior papers approach exploiting parallelism high-level performance results intertile connectivity protocols tiled architectures raw static orchestration manage global operations dynamically scheduled distributed architecture trips hardware protocols required provide functionality processor understand design complexity timing area performance issues dynamic tiled approach implemented trips design transistor asic chip prototype chip processor cores implements edge instruction set architecture -way multithreaded execute peak instructions cycle processor core types tiles communicating micronets data instructions control orchestrate distributed execution trips prototype tiles range size principal processor elements instruction data caches register files execution units subdivided replicated copies respective tile type instruction cache composed instruction cache tiles computation core composed execution tiles tiles sized small wire delay tile cycle largely global perspective tile interacts neighbors micronets roles transmitting operands instructions distributing instructions instruction cache tiles execution tiles communicating control messages program sequencer avoiding global wires broadcast busses clock reset tree interrupt signals design inherently scalable smaller processes vulnerable wire delays conventional designs preliminary performance results prototype architecture cycle-accurate simulator show compiled code outperforms alpha half benchmarks expect results improve trips compiler optimizations tuned hand optimization benchmarks produces ipcs ranging performance relative alpha isa support distributed execution explicit data graph execution edge architectures conceived goal high-performance singlethreaded concurrent distributed execution allowing compiler-generated dataflow graphs mapped execution substrate microarchitecture defining features edge isa block-atomic execution direct communication instructions block enable efficient dataflow-like execution trips isa edge architecture aggregates instructions single block obeys block-atomic execution model block logically fetched executed committed single entity model amortizes per-instruction bookkeeping large number instructions reduces number branch predictions register file accesses model reduces frequency control decisions execute made fetch commit providing additional latency tolerance make distributed execution practical bebabd cpd cxd cxd cxd cccac bud crczd compiler constructs trips blocks assigns instruction location block block divided -byte chunks microarchitecture block includes header chunk encodes read write instructions access architectural registers read instructions pull values registers send compute instructions block write instructions return outputs block architectural registers trips microarchitecture read write instructions distributed register banks section header chunk holds types control state block -bit store mask opcode xop opcode imm xop general instruction formats opcode imm lsid opcode imm lsid load store instruction formats opcode offsetexit branch instruction format opcode const constant instruction format read instruction format write instruction format instruction fields opcode primary opcode xop extended opcode predicate field imm signed target specifier target specifier lsid load store exit exit number offset branch offset const -bit constant valid bit general register index read target specifier read target specifier figure trips instruction formats memory instructions stores block execution flags execution mode block number instruction body chunks block store mask section enable distributed detection block completion block body chunks consisting instructions maximum instructions loads stores executions block emit number outputs stores register writes branch predicated path block constraint detect block completion distributed substrate compiler generates blocks conform constraints bebabe bwcxd cxcqd ctcs crd cxd cpcrctd ctd direct instruction communication instructions block send results directly intra-block dependent consumers dataflow fashion model supports distributed execution eliminating intervening shared centralized structures issue window register file intra-block producers consumers figure shows trips isa supports direct instruction communication encoding consumers instruction result targets producing instruction microarchitecture determine precisely consumer resides forward producer result directly target instruction nine-bit target fields target instruction bits operand type left predicate remaining microarchitecture supporting isa maps block instructions coordinates determining distributed flow operands block dataflow graph instruction coordinates implicitly determined position chunk non-traditional elements isa include field specifies instruction predicate lsid field specifies relative load store ordering sdc dma ebcsdcdma nnn processor processor conda memo stem irqsdram sdram ebi figure trips prototype block diagram distributed microarchitecture goal trips microarchitecture processor scalable distributed meaning global wires built small set reused components routed networks extended wider-issue implementation recompiling source code changing isa figure shows tile-level block diagram trips prototype major components chip processors secondary memory system connected internally micronetworks processor cores implemented unique tiles global control tile execution tiles register tiles data tiles instruction tiles major processor core micronetwork operand network opn shown figure connects tiles twodimensional wormhole-routed mesh topology opn separate control data channels deliver -bit data operand link cycle control header packet launched cycle advance data payload packet accelerate wakeup select bypassed operands traverse network processor core micronetworks instruction dispatch global dispatch network gdn control global control network gcn committing flushing blocks global status network gsn transmitting information block completion global refill network grn i-cache miss refills data status network dsn communicating store completion information external store network esn determining store completion cache memory links networks connect nearest neighbor tiles messages traverse tile cycle figure shows links networks type tiled microarchitecture composable dei global dispatch network gdn global 
status network gsn operand network opn global control network gcn issues block fetch command dispatches instructions handles transport data operands issues block commit block flush commands signals completion block execution i-cache miss refill block commit completion figure trips micronetworks sign time permitting numbers topologies tiles implementations moderate tile logic software model arrangement tiles prototype produces core -wide out-of-order issue instruction cache data cache smt threads microarchitecture supports trips blocks flight simultaneously speculative single thread running blocks thread threads running -instruction blocks provide in-flight window instructions processors communicate secondary memory system on-chip network ocn embedded ocn wormhole-routed mesh network -byte data links virtual channels network optimized cache-line sized transfers request sizes supported operations loads stores uncacheable pages ocn acts transport fabric inter-processor cache dram dma traffic bfbabd bzd cqcpd bvd cccxd bzccb figure shows contents include blocks pcs instruction cache tag arrays i-tlb next-block predictor handles trips block management including prediction fetch dispatch completion detection flush mispredictions interrupts commit holds control registers configure processor speculation execution threading modes interacts control networks opn provide access block pcs maintains state in-flight blocks block slots free accesses block predictor takes cycles emits global control tile instruction tile ocn gdn grn gsn gdn grn gsn gdn control logic refill buffer -bit read port write port i-cache array -bit sram port ocn control register tile gsn gcn gdn gcn gdn opn commit decod write queue architecture register file read queue execution tile dispatch decode status bits select logic gdn gcn opn opn alu instruction buffer operand buffer data tile dsn gcn ocn esn gsn gdn opn dsn cache bank load store queue dependence predictor data tlb miss handler main control control registersopn ocn opn gcn gsn esn gsngdn grn refill uniti-cache mshrs fetch unit itlb i-cache dir block predictor retire unit commit flush control figure trips tile-level diagrams predicted address target block block emit exit branch predicated branches block predictor branch instruction three-bit exit field construct exit histories not-taken bits predictor major parts exit predictor target predictor predictor exit histories predict block exits employing tournament local gshare predictor similar alpha bits local global tournament exit predictors predicted exit number combined current block address access target predictor next-block address target predictor major structures branch target buffer bits call target buffer bits return address stack bits branch type predictor bits btb predicts targets branches ctb calls ras returns branch type predictor selects target predictions call return branch sequential branch distributed fetch protocol necessitates type predictor predictor sees actual branch instructions directly ets bfbabe crd cxd cccxd ccb figure shows -way bank total i-cache acts slave holds single tag array banks hold -byte chunk total bytes maximum-sized block distinct blocks bfbabf cactcvcxd ctd cccxd caccb reduce power consumption delay trips microarchitecture partitions registers banks bank register tiles nodes opn allowing compiler place critical instructions read write bank close bank def-use pairs instructions converted intra-block temporaries compiler access register file reducing total register bandwidth requirements approximately average compared risc cisc processor distributed banks provide sufficient register bandwidth small number ports trips prototype bank read ports write port rts -register bank smt threads core supports total registers registers thread rts addition per-thread architecture register file banks read queue write queue shown figure queues hold read write instructions block header blocks flight forward register writes dynamically subsequent blocks reading registers read write queues perform function equivalent register renaming superscalar physical register file complex implement due read write instructions trips isa bfbabg bxdcctcrd cxd cccxd bxccb shown figure ets consists fairly standard single-issue pipeline bank reservation stations integer unit floating-point unit units fully pipelined integer divide unit takes cycles reservation stations hold instructions in-flight trips blocks reservation station fields -bit data operands one-bit predicate bfbabh bwcpd cccxd bwccb figure shows block diagram single client opn holds -way data cache bank total dts virtual addresses interleaved dts granularity -byte cache-line addition cache bank copy load store queue lsq dependence predictor one-entry back-side coalescing write buffer data tlb mshr supports requests outstanding cache lines dts distributed network implemented memory-side dependence predictor closely coupled data cache bank loads issue ets dependence prediction occurs parallel cache access load arrives dependence predictor entry bit vector aggressively issued load dependence misprediction subsequent pipeline flush dependence predictor sets bit load address hashes load predictor entry set bit stalled prior stores completed clear individual bit vector entries scheme hardware clears dependence predictor blocks execution hardest challenge designing distributed data cache memory disambiguation hardware trips isa restricts block maximum issued loads stores blocks flight memory operations flight mapping memory operations dts unknown effective addresses computed resultant problems determining distribute lsq dts determining earlier stores completed dts held-back load issue centralizing lsq distributing lsq capacity dts feasible options time solved lsq distribution problem largely brute force replicated copies -entry lsq solution wasteful scalable maximum occupancy lsqs complex alternative prototype lsq accept load store cycle forwarding data earlier stores additional details design found bfbabi cbctcrd cscpd ctd cbddd ctd trips prototype supports static nuca array organized memory tiles mts holds -way bank includes on-chip network ocn router single-entry mshr bank configured cache bank scratch-pad memory sending configuration command ocn aligning ocn dts pair private port secondary memory system supporting high bandwidth cores streaming applications network tiles nts surrounding memory system act translation agents determining route memory system requests programmable routing table determines destination memory system request adjusting mapping functions tlbs network interface tiles nts programmer configure memory system variety ways including single shared levelcache independent levelcaches processor on-chip physical memory levelcache combinations tiles chip ocn clients section distributed microarchitectural protocols enable concurrent out-of-order execution distributed substrate implemented traditionally centralized microarchitectural functions including fetch execution flush commit distributed protocols running control data micronets bgbabd bud crcz byctd crcw crd fetch protocol retrieves block trips instructions distributes array ets rts block fetch pipeline takes total cycles including cycles prediction cycle tlb instruction cache tag access cycle hit miss detection cache hit sends pipelined indices global dispatch network gdn prediction instruction cache tag lookup block overlapped fetch commands current block running peak machine issue fetch commands cycle 
bubbles beginning block fetch cycles receives block dispatch command accesses i-cache bank based index gdn message cycles sends instructions outgoing gdn paths row ets rts instructions written read write queues rts reservation stations ets arrive respective tiles execute arrive fetch commands fetched instructions delivered pipelined fashion ets rts furthest receives instruction packet ten cycles packet cycles issues fetch command latency appears high pipelining block completion commit fetch finish-s finish-r commit commit-r commit-s fetch finish-r finish-s commit commit-s commit-r fetchblock block block finish-r register completes finish-s store completes commit-r register commits commit-s store commits ack-r register commit acked ack-s store commit acked time ack-s ack-r ack-r ack-s execution read movi teq muli lsid mov lsid callo func null rtrt figure trips operational protocols enables high-fetch bandwidth instructions cycle steady state instruction cycle i-cache miss instigates distributed cache refill global refill network grn transmit refill block physical address processes misses chunk independently simultaneously support outstanding miss executing thread -byte cache lines -byte block chunk return south neighbor finished fill signals refill completion northward gsn receives refill completion signal top issue dispatch block bgbabe bwcxd cxcqd ctcs bxdcctcrd cxd begin process arriving read instruction entire block fetched searches write queues older in-flight blocks matching in-flight write register found simply reads register architectural register file forwards consumers block opn matching write found takes actions write instruction received forwards read instruction consumers write instruction awaiting buffers read instruction woken tag broadcast pertinent write arrives arriving opn operands wake instructions selects executes enabled instructions target fields selected instruction determine send resulting operand arithmetic operands traverse opn ets load store instructions addresses data opn dts branch instructions deliver block addresses opn issuing instruction target remote targets local dependent instruction woken executed cycle local bypass path permit back-to-back issue dependent instructions target remote control packet formed cycle operation complete execution wake dependent instruction early opn tightly integrated wakeup select logic control packet arrives opn targeted instruction accessed speculatively woken instruction begin execution cycle opn router injects arriving operand directly alu opn hop dependent instructions extra cycle consuming instruction executes figure shows code sequence executed rts ets dts block execution begins read instruction issued triggering delivery opn left operand instructions teq muli test instruction receives register movi instruction fires produces predicate routed predicate field predicated false routed operand muli fire multiply arriving left operand send result address field load word load fires sends request pertinent responds routing loaded data load store ids load store ensure execute proper program order share address result load fanned mov instruction address data fields store predicate inject result opn suppressing execution dependent load null instruction fires targeting address data fields store word note instructions targeting operand store fire due predicate store pertinent block-ending call instruction routed block produced outputs ready commit note store nullified affect memory simply signals store issued nullified register writes stores ensure block produces number outputs completion detection bgbabf bud crczbbc cxd ctd cxd byd crd trips executes blocks speculatively pipeline flushed periodically distributed protocol due branch misprediction load store ordering violation notified misspeculation occurs detecting branch misprediction gsn message indicating memory-ordering violation initiates flush wave gcn propagates ets dts rts gcn includes block identifier mask indicating block blocks flushed processor support multi-block flushing speculative blocks caused mis-speculation flushed wave propagates hop cycle array issues flush command gcn issue dispatch command start block gcn gdn predictable latencies instruction fetch dispatch command catch pass flush command bgbabg bud crcz bvd cxd crd block commit complex microarchitectural protocols trips involves phases illustrated figure block completion block commit commit acknowledgment phase block complete produced outputs number determined compile-time consists register writes stores branch rts dts receive register writes stores block inform global status network gsn detects block writes arrived informs east neighbor completion message daisy-chained eastward rts reaches indicating register writes block received detecting store completion difficult priori stores enable dts detect store completion implemented dt-specific network called data status network dsn block header -bit store mask memory operations encoded lsid bit mask block stores store mask dts block dispatch executed store arrives -bit lsid block dts dsn marks store received store address data load learns previous stores received dts nearest notifies expected stores block arrived receives gsn signal closest received branch block opn block complete speculative execution occurring block paths eventually nullified predicates execution affect block outputs phase block commit broadcasts commit command global control network updates block predictor commit command informs rts dts commit register writes stores architectural state prevent distributed commit bottleneck designed logic support pipelined commit commands legally send commit command gcn block commit command older in-flight blocks commit commands older blocks flight pipelined commits safe tile guaranteed receive process order commit command gcn flushes speculative in-flight state ets dts block phase acknowledges completion commit finished committing architectural state block received commit completion signal neighbor gsn similar block completion detection signals commit completion gsn received commit completion signals rts dts block safe deallocate block outputs written architectural state oldest block acknowledged commit initiates block fetch dispatch sequence block slot physical design performance overheads physical design implementation trips chip driven principles partitioning replication chip floorplan directly corresponds logical hierarchy trips tiles connected point-topoint nearest-neighbor networks exceptions nearest-neighbor communication global reset interrupt signals latency tolerant pipelined multiple stages chip bhbabd bvcwcxd cbd ctcrcxaccrcpd cxd trips chip implemented ibm cuasic process drawn feature size layers metal chip includes million transistors chip area square ball-grid array package figure shows annotated floorplan diagram trips chip directly design database coarse area breakdown function diagram shows boundaries trips tiles placement register sram arrays tile label network tiles nts surround ocn small ease viewing omitted individual logic cells plot proc ocn proc rtrtrt etetet etetet etetet etetet dma dma ebc sdc sdc figure trips physical floorplan addition core tiles trips includes controllers attached rest system on-chip network ocn mhz ddr sdram controllers sdc connect individual sdram dimm chip-to-chip controller extends on-chip network four-port mesh router gluelessly connects trips chips links nominally run one-half core processor clock mhz direct memory access dma controllers programmed transfer data regions physical address 
space including addresses mapped trips processors finally external bus controller ebc interface board-level powerpc control processor reduce design complexity chose off-load operating system runtime control powerpc processor trips relies trends hierarchical design styles replicated components differs socs cmps individual tiles designed diverse functions cooperate implement powerful scalable uniprocessor entire trips design composed types tiles greatly simplifying design verification table shows additional details design trips tile cell count column shows number placeable instances tile relative estimate complexity tile array bits total number bits found dense register sram arrays per-tile basis size shows area type tile tile count shows total number tile copies entire chip chip area fraction total chip area occupied type tile table trips tile specifications cell array size tile chip tile count bits count area sdc dma ebc chip total bhbabe btd ctcp dactd cwctcpcsd bwcxd cxcqd ctcs bwctd cxcvd principal area overheads distributed design stem wires logic needed implement tile-interconnection control data networks shown table expensive terms area data networks operand network opn on-chip network ocn addition physical wires link opn includes routers buffering processor tiles -port routers links tile consume significant chip area account approximately total processor area strictly speaking area overhead takes place bypass network expensive routed opn -issue conventional processor ocn carries larger area burden buffering virtual channels -ported routers consumes total total chip area larger bus architecture smaller scale memory system trips nuca cache general processor control networks large area impact cost wires interconnecting tiles found full-chip routing easily accomplished large number wires large source area overhead due partitioning oversized load store queues accounting processor core area lsq cell count area skewed lsq cam arrays implemented discrete latches suitable dense array structure asic design library entire chip area overhead distributed design stem largely on-chip data networks control protocol overheads insignificant exception load store queue table trips control data networks network bits global dispatch gdn i-fetch global status gsn block status global control gcn commit flush global refill grn i-cache refill data status dsn store completion external store esn misses operand network opn operand routing on-chip network ocn memory traffic bhbabf cccxd cxd dactd cwctcpcsd difficult timing paths found logiclevel timing optimization local bypass paths multi-cycle floating point instructions control paths cache access state machine remote bypass paths operand network processor core operand network paths problematic increasing latency cycles significant effect instruction throughput retrospect underestimated latency required multiple levels muxing required implement operand router customized design reduce routing latency results research ultra-low-latency micronetwork routers bhbabg ctd cud cpd crct dactd cwctcpcsd examine performance overheads distributed protocols simulation-based study cycle-level simulator called tsim-proc models hardware detailed level higher-level simulators simplescalar performance validation effort showed performance results tsim-proc average obtained rtl-level simulator test suite randomly generated test programs methodology fields attribute percentages critical path program microarchitectural activities partitioning overheads benchmark suite study includes set microbenchmarks dct sha matrix vadd set kernels signal processing library cfar conv genalg svd subset eembc suite time bezier basefp rspeed tblook handful spec benchmarks mcf parser bzip twolf mgrid general small programs program fragments tens millions instructions limited speed tsim-proc spec benchmarks input set employ subsets program recommended benchmarks reflect run simulation environment benchmarks selected leave unrealistically rosy impression performance trips compiler toolchain takes fortran code produces complete trips binaries run hardware trips compiler compile major benchmark suites correctly eembc spec trips-specific optimizations pending completion performance compiled code lacking trips blocks small report results compiled code employed hand optimization microbenchmarks kernels eembc programs optimized compiler-generated trips high-level assembly code hand feeding result back compiler assign instructions alus produce optimized binary report results trips compiler hand-optimized code optimized spec programs hand working improve compiler code quality approach hand-optimized distributed protocol overheads measure contributions microarchitectural protocols computed critical path program attributed cycle number categories categories include instruction distribution delays operand network latency including hops contention execution overhead instructions fan operands multiple target instructions alu contention time spent waiting global control tile notified block outputs branches registers stores produced latency block commit protocol complete table shows overheads percentage critical path program column labeled includes components critical path found conventional monolithic cores including alu execution time instruction data cache misses largest overhead contributor critical path operand routing hop latencies accounting contention accounting overheads evil architectures distributed execution units mitigated scheduling minimize distance producers consumers critical path increasing bandwidth operand network benchmarks overheads replicating fanning operand values rest distributed protocol overheads small typically summing critical path results suggest overheads control networks largely overlapped instruction execution data networks benefit optimization total performance understand impact distributed protocols performance compared execution time tsim-proc conventional table network overheads preliminary performance prototype distributed network overheads percentage program critical path preliminary performance opn opn fanout block block speedup ipc ipc ipc benchmark ifetch hops cont ops complete commit tcc hand alpha tcc hand dct matrix sha vadd cfar conv genalg svd time bezier basefp rspeed tblook mcf parser bzip twolf mgrid albeit clustered uniprocessor baseline comparison point mhz alpha processor programs compiled native gem compiler -arch flags set chose alpha aggressive ilp core supports low clock periods isa lends efficient execution amazing compiler generates extraordinarily high-quality code sim-alpha simulator validated alpha hardware baseline measurements normalize levelcache memory system comparison processor primary caches trips alpha table shows performance trips processor compared alpha focus disparity processor cores simulated perfect levelcache processors eliminate differences performance due secondary memory system column shows speedup trips compiled code tcc alpha computed speedup comparing number cycles needed run program column shows speedup hand-generated trips code alpha columns show instruction throughput instructions clock ipc configurations ratio ipcs correlate directly performance instruction sets differ give approximate depiction concurrency machine exploiting results show hand optimized codes trips executes times instructions alpha largely due fanout instructions single-to-double conversions required trips codes -bit floats current code bloat larger compiled code times instructions worst case results expect obtain provide insight capabilities trips results show hand optimized programs trips distributed microarchitecture sustain reasonable instruction-level concurrency ranging speedups alpha core range sha sees slowdown trips serial benchmark concurrency mined alpha core trips processor sees slight degradation block overheads 
inter-block register forwarding convolution conv vadd speedups close trips core double memory bandwidth alpha ports opposed resulting upper-bound speedup compiled trips code fare exceed performance alpha half benchmarks maturation time compiler processor short anticipate significant improvements hyperblock generation optimization algorithms line conclude analysis trips microarchitecture sustain good instruction-level concurrency distributed overheads kernels sufficient concurrency aggressive handcoding core exploit ilp full benchmarks compiler generate sufficiently optimized code remain open questions subjects current work related work trips architecture inspired important prior work computer architecture domains including tiled architectures dataflow architectures superscalar processors vliw architectures tiled architectures transistor counts approaching billion tiled architectures emerging approach manage design complexity raw architecture pioneered research issues facing tiled architectures including scalar operand networks subset class micronetworks designed operand transport recent tiled architecture raw homogeneous tiles smart memories emerging fine-grained cmp architectures sun niagara ibm cell viewed tiled architectures architectures implement complete processors tile general tiled architectures interconnected memory interfaces raw register-based inter-processor communication trips differs ways tiles heterogeneous types tiles composed create uniprocessor trips distributed control network protocols implement functions centralized conventional architecture dataflow architectures work similar trips recent dataflow-like architectures support imperative programming languages architectures developed concurrently trips wavescalar ash wavescalar breaks programs blocks waves similar trips differs execution model control paths mapped executed speculated control path trips major differences include dynamic static placement instructions load speculation hierarchy networks wavescalar execution units trips ash similar predication model dataflow concepts targets application-specific hardware small programs opposed compiling large programs sequence configurations programmable substrate trips behavior inside single trips block builds rich history dataflow architectures including work dennis arvind hybrid dataflow architectures work culler iannucci superscalar architectures trips microarchitecture incorporates high-ilp techniques developed aggressive superscalar architectures twolevel branch prediction dependence prediction trips block atomic execution model descended block-structured isa proposed patt increase fetch rate wide issue machines current research efforts aim exploit large-window parallelism means checkpointing speculation vliw architectures trips shares similarities vliw architectures trips compiler decides instructions execute trips compiler decide instruction timing unlike vliw architectures vliw compilation algorithms forming large scheduling regions predicated hyperblocks effective techniques creating large trips blocks conclusions trips paper appeared high-level results promising unclear technology implementable practice deliver performance high-level study microarchitecture paper existence proof design challenges unanswered solvable distributed protocols designed implement basic microarchitecture functions instruction fetch operand delivery commit feasible incur prohibitive overheads distributed control overheads largely overlapped instruction execution logic required implement protocols significant pipelined protocols critical timing paths data networks carry larger area performance burden critical paths data dependent instructions prototype working reduce overheads scheduling reduce hop-counts architectural extensions trips include operand network bandwidth original work assumed ideal centralized load store queue assuming partitioned final design partitioning turned unworkable elected put multiple full-sized copies combined area-hungry standard-cell cam implementation caused lsqs occupy dts solving problem area-efficiently partitioning lsqs focus research past year distributed protocols enabled construct -wide -instruction window out-of-order processor works small set regular handoptimized kernels demonstrated code compiled efficiently architecture processor competitive high-quality code real applications completed prototype work remains areas performance tuning compilation understand microarchitectural isa compiler bottlenecks design systems running fall commence detailed evaluation capabilities trips design understand strengths weaknesses system technology forward partitioned processors composed interconnected tiles provide opportunity dynamically adjust granularity subdivide tiles processor create multiple smaller processors balance instruction-level threadlevel parallelism change expect substrates heterogeneous homogeneous tiles provide flexible computing platforms tailored runtime match concurrency applications acknowledgments design partners ibm microelectronics synopsys generous program research supported financially defense advanced research projects agency contracts -cand nbch nsf instrumentation grant eiansf career grants ccrand ccribm partnership awards grants alfred sloan foundation intel research council arvind nikhil executing program mit taggedtoken dataflow architecture ieee transactions computers budiu venkataramani chelcea goldstein spatial computation international conference architectural support programming languages operating systems pages october burger keckler mckinley dahlin john lin moore burrill mcdonald yoder scaling end silicon edge architectures ieee computer july cristal santana valero martinez kiloinstruction processors acm transactions architecture code optimization december culler sah schauser von eicken wawrzynek fine-grain parallelism minimal hardware support compiler-controlled threaded abstract machine international conference architectural support programming languages operating systems pages april dennis misunas preliminary architecture basic data-flow processor international symposium computer architecture pages january fields rubin bodik focusing processor policies critical-path prediction proceedings annual international symposium computer architecture pages july hao chang evers patt increasing instruction fetch rate block-structured instruction set architectures international symposium microarchitecture pages december iannucci dataflow von neumann hybrid architecture international symposium computer architecture pages kessler alpha microprocessor ieee micro march april kim burger keckler adaptive non-uniform cache structure wire-delay dominated on-chip caches international conference architectural support programming languages operating systems pages october kongetira aingaran olukotun niagara multithreaded sparc processor ieee micro march april mahlke lin chen hank bringmann effective compiler support predicated execution hyperblock international symposium microarchitecture pages june mai paaske jayasena dally horowitz smart memories modular reconfigurable architecture international symposium computer architecture pages june nagarajan sankaralingam burger keckler design space evaluation grid processor architectures international symposium microarchitecture pages december pham asano bolliger day hofstee johns kahle kameyama keaty masubuchi riley shippy stasiak suzuoki wang warnock weitzel wendel yamazaki yazawa design implementation first-generation cell processor international solid-state circuits conference pages february sethumadhavan mcdonald desikan burger keckler design implementation trips primary memory system international conference computer design october sherwood perelman calder basic block distribution analysis find periodic behavior simulation points applications international conference parallel architectures compilation technique pages september smith burrill gibson maher nethercote yoder burger mckinley compiling edge architectures international symposium code generation optimization pages march srinivasan rajwar akkary ghandi upson continual flow pipelines international conference architectural support programming languages operating systems pages october swanson michaelson schwerin oskin wavescalar international symposium microarchitecture pages december taylor lee amarasinghe agarwal scalar operand networks on-chip interconnect ilp partitioned architectures international symposium high performance computer architecture pages february waingold taylor srikrishna sarkar lee lee kim frank finch barua babb amarasinghe agarwal baring software raw machines ieee computer september 
list karthikeyan sankaralingam november karthikeyan sankaralingam polymorphous architectures unified approach extracting concurrency granularities phd thesis texas austin department computer sciences aaron smith ramadass nagarajan karthikeyan sankaralingam robert mcdonald doug burger stephen keckler kathryn mckinley dataflow predication proceedings annual international symposium microarchitecture december karthikeyan sankaralingam ramadass nagarajan robert mcdonald rajagopalan desikan saurabh drolia govindan paul gratz divya gulati heather hanson changkyu kim haiming liu nitya ranganathan simha sethumadhavan sadia sharif premkishore shivakumar stephen keckler doug burger distributed microarchitectural protocols trips prototype processor proceedings annual international symposium microarchitecture december karthikeyan sankaralingam madhulika yalamanchi simha sethumadhavan james browne pagerank computation keyword search distributed systems networks journal grid computing karthikeyan sankaralingam ramadass nagarajan haiming liu changkyu kim jaehyuk huh nitya ranganathan doug burger stephen keckler robert mcdonald charles moore trips polymorphous architecture exploiting ilp tlp dlp acm transactions architecture code optimization taco march karthikeyan sankaralingam ramadass nagarajan haiming liu changkyu kim jaehyuk huh stephen keckler doug burger charles moore exploiting ilp tlp dlp polymorphous trips architecture ieee micro november karthikeyan sankaralingam stephen keckler william mark doug burger universal mechanisms data-parallel architectures proceedings annual international symposium microarchitecture pages december karthikeyan sankaralingam vincent ajay singh stephen keckler doug burger routed inter-alu networks ilp scalability performance proceedings international conference computer design pages october vincent ajay singh karthikeyan sankaralingam stephen keckler doug burger design analysis routed inter-alu networks ilp scalability performance technical report department computer sciences texas austin austin july karthikeyan sankaralingam ramadass nagarajan haiming liu changkyu kim jaehyuk huh stephen keckler doug burger charles moore exploiting ilp tlp dlp polymorphous trips architecture proceedings annual international symposium computer architecture pages june karthikeyan sankaralingam simha sethumadhavan james browne distributed pagerank systems proceedings international symposium high performance distributed computing pages june stephen keckler doug burger charles moore ramadass nagarajan karthikeyan sankaralingam vikas agarwal hrishikesh nitya ranganathan premkishore shivakumar wire-delay scalable microprocessor architecture high performance systems proceedings international solid-state circuits conference february ramadass nagarajan karthikeyan sankaralingam stephen keckler doug burger design space evaluation grid processor architectures proceedings annual international symposium microarchitecture pages december karthikeyan sankaralingam ramadass nagarajan stephen keckler doug burger simplescalar simulation powerpc instruction set architecture technical report department computer sciences texas austin austin february karthikeyan sankaralingam ramadass nagarajan doug burger stepehen keckler technology scalable architecture fast clocks high ilp proceedings workshop interaction compilers computer architecture january tom keller karthikeyan sankaralingam peter hofstee optimal file allocation strategy specweb proceedings workshop workload characterization september karthikeyan sankaralingam satyanarayana chakravarthy computer model flamelet distribution burning surface composite solid propellant combustion science technology june karthikeyan sankaralingam satyanarayana chakravarthy computer model flamelet distribution burning surface composite solid propellant proceedings aerospace sciences meeting conference exhibit karthikeyan sankaralingam ranganathan sankaralingam arbitrary boundary packed arithmetic proceedings international conference high performance computing hipc pages december 
distributed pagerank systems karthikeyan sankaralingam simha sethumadhavan james browne texas austin department computer sciences contributions distributed computation pageranks based asynchronous iteration application systems application internet scale practical keyword search systems large scale asynchronous iteration computation overview motivation keyword search systems system overview state art keyword search approach solution pagerank computation distributed computation pageranks systems incremental retrieval documents keyword search performance results distributed computation pageranks internet peer peer systems systems effective distributed storage systems efficient retrieval efficient search retrieval distributed hash tables dht chord pastry freenet unstructured systems gnutella morpheus kazaa characteristics distributed storage centralized server peer-to-peer communication dynamic effects peers enter leave frequently systems distributed hash tables routing peers hash fff fff fff fff fff xbfff xdfff key space bit keys xffff systems retrieval fetch peers fff fff fff fff fff xbfff xdfff key space xffff systems search state art index based keyword search reynolds vahdat gnawali document vectors kronofol combinations based problem retrieval responses easy estimate relevance index based keyword search keyword list doc ids keys tree oak spider linux centralized index index based keyword search hashed keyword list doc ids keys tree oak spider linux hash distribute embed index system systems search state art index based keyword search reynolds vahdat gnawali document vectors kronofol combinations based problem retrieval responses easy estimate relevance large network traffic fetch documents solution google pagerank apply pagerank environment give document system rank link structure incremental retrieval based pageranks google computation pagerank centralized solution crawler updated weeks computation farm solving billion order matrix problem computation time days acceleration methods proposed kamvar challenge implementation files distributed crawler systems centralized computation peers entering leaving pagerank assign numeric rank page document link structure key inlink outlink respect pagerank contd page contributes equally outlinks pagerank page sum inlinks web graph backedges pagerank computed iteratively mathematical formulation arr distributed pagerank compute pagerank locally peer node send pagerank updates linked documents peers stop local pagerank converges peers documents nodes process work asynchronous iterations pagerank eigenvalue computation problem page haveliwala link matrix sparse diagonally dominant asynchronous iterations chazan miranker peers act simple state machines exchanging messages integration systems crawling centralized computation storage store rank document computation execute distributed pagerank computation algorithm peer communication pagerank update messages routed based linked document key caching optimization save routed traffic route message layer cache address key sender deliver subsequent messages point point dynamic systems peer joins leaves transport layer detect peer unavailable buffer update messages peer unavailable periodically retry peer back document insertion deletion documents initialized pagerank deleted documents send pagerank update messages negative pagerank incremental continuously updated pageranks integration search dht systems augment index pagerank field return results sorted pagerank nodes update index pagerank converge hashed keyword list doc ids keys tree oak spider linux rxx pageranks multi-word search tree oak white keys tree keys tree oak keys tree oak white user query tree oak white keys transferred leading high network traffic ranking scheme incremental search query tree oak white traffic reduction incremental forwarding quality hits relevance sorting tree oak white keys tree keys tree oak keys tree oak white user results modeling document sets peer network simple network transfer model power law distribution link structure nodes degree broder evaluation parameters convergence passes quality pagerank error relative centralized scheme message traffic number pagerank update messages execution time scalability results msgs doc error threshold msgs doc error threshold msgs doc independent docs traffic grows logarithmically error threshold message traffic high small errors max error typically quality pagerank fast convergence iterations documents converge iterations convergence results hrs hrs hrs slow sec hrs hrs hrs fast sec error threshold dominated network speedexecution time convergence quality messages doc independent docs execution times grows logarithmically docs scalability results incremental search built document set -word -word queries synthesized frequent terms reduction network traffic -word queries reduction network traffic -word queries conclusions distributed computation google pagerank document ranking scheme systems significant benefits keyword search performance scalability demonstrated systems internet search engine computation pagerank internet documents web servers acts peers exchange messages compute pagerank pagerank free public commodity work link web space providers billion node graph computed days re-crawls required document inserts deletes automatically handled build distributed internet scale keyword index web server implementation future work implement pagerank system link structure map documents peer-to-peer chaotic iterations solutions work domains explore internet scale application questions 
cart ut-cs design space exploration grid processor architectures karu sankaralingam ramadass nagarajan doug burger stephen keckler computer architecture technology laboratory department computer sciences texas austin cart ut-cs technology architecture trends good news lots transistors faster transistors bad news pipeline depth optimal pipelining limits slow clock rate improvements half performance ilp ipc doubled decade considerable effort global wire delays growing die -cycle-reachable goals future architectures scalability process technology improvements fast clock high ilp cart ut-cs approach alu chaining execution model eliminates majority register reads associative issue windows rename tables global bypass partitions i-cache register file banks alus statically map dynamically issue cart ut-cs outline grid processor architecture gpa block compilation program execution evaluation conclusions future work cart ut-cs grid processor router data caches bank moves bank bank bank bank load store queues bank bank bank bank instruction caches banked register file block termination logic inst alu cart ut-cs block compilation add add beqz xdeac move move intermediate code data flow graph inputs temporaries outputs cart ut-cs block compilation scheduler move move mapping move move data flow graph cart ut-cs block compilation add gpa code code generation instruction location targets opcode move move mapping cart ut-cs block atomic execution model block instructions atomic unit fetch schedule execute commit blocks expose critical path operand chains hidden large structures instructions consumers explicit targets blocks simple internal control flow single point entry if-conversion predication predicated hyperblocks cart ut-cs block execution block termination logic icache bank icache moves icache bank icache bank icache bank load store queues dcache bank dcache bank dcache bank dcache bank bank bank bank bank load store queues bank bank bank bank load beqz add add cart ut-cs block execution block termination logic icache bank icache moves icache bank icache bank icache bank load store queues dcache bank dcache bank dcache bank dcache bank bank bank bank bank load store queues bank bank bank bank load beqz add add cart ut-cs instruction buffers frames blocks exceed grid size overlap fetch map frames virtualize grid slots frames logical partitioning node storage space local ooo issue control router inst alu inst cart ut-cs execution opportunities serialized block fetch map execute overlapped instruction distribution execution overlapped fetch map next-block predictor block level squash mis-prediction overlapped execution blocks next-block predictor block level squash mis-prediction block stitching input output register masks cart ut-cs evaluation specint specfp mediabench benchmarks adpcm dct mpeg encode gcc mcf parser ammp art equake compiled trimaran toolset hyperblocks parsed scheduled custom tools event driven configurable timing simulator performance estimates cart ut-cs gpa evaluation parameters grid cycle router cycle wire delay slots node stage pipeline -wide cycle router wire delay entry instruction window gpa superscalar alpha functional unit latencies cycles cycles main memory cycles cart ut-cs gpa performance comparison adpc peg par mmp equak ean ipc gpa superscalar mediabench specint specfp cart ut-cs sensitivity communication delay wire delay router delay router delay router delay superscalar cart ut-cs conclusions technology trends enforce partitioning wire delays order constraint gpa distributed execution engine central structures technology scalable fast clock rate high ilp challenges block control mechanisms distributed memory interface design optimizing predication mechanisms cart ut-cs future work alternate execution models smt support frames run threads stream based execution loop re-use data partitioning caches scientific vector-based execution rows vector execution units vector loads read caches hardware prototype cart ut-cs related work dataflow static dataflow architecture dennis misunas tagged-token dataflow arvind hybrid dataflow execution culler raw architecture waingold multiscalar processors sohi trace processors vajapeyam clustered speculative multithreaded processors marcuello gonz lez levo uht cart ut-cs questions 
cart ut-cs routed inter-alu networks ilp scalability performance karthikeyan sankaralingam vincent ajay singh stephen keckler doug burger computer architecture technology laboratory department computer sciences texas austin cart ut-cs bypass paths memory single alu local bypass fan-out fan-in cart ut-cs fan-out bypass paths fan-out fan-in wires longer distances memory alu alu cart ut-cs bypass paths bypassing complexity scales pipeline width depth fan-out fan-in increases wiring complexity increases bypassing complexity area increases due wiring alu alu alu cart ut-cs bypass path delays bypass delays vary distance issue width modern processors bypass delay fixed fraction clock cycle longest path circuit simulation clock cycle technology delays shown cycles -issue shortest path -issue -issue cart ut-cs solution routed inter-alu networks current bypass paths implemented all-to-all broadcast routed inter-alu networks rians point-to-point links neighboring nodes multiple hops communicate distant nodes fewer wires lower wiring area overhead low fan-in fan-out delay router delay hop destinations circuit complexities cart ut-cs outline taxonomy inter-alu networks circuit analysis rians architectures dynamically scheduled architectures statically scheduled vliw architectures statically scheduled dynamically issued grid processor architectures conclusion cart ut-cs taxonomy inter-alu networks all-to-all broadcast bypass networks part broad class inter-alu networks taxonomy types networks classified axes dynamicstaticrouter control operand pass multiple-hops operand directly producer consumer single-hop network architecture targets explicitly operands point-to-point outputs broadcast alus execution model cart ut-cs taxonomy inter-alu networks m-machine multicluster degenerate alpha grid processorpmd rian examplesacronym router control network architecture execution model cart ut-cs circuit analysis network architecture analyze single-hop multi-hop networks examine circuit delay components wire delay fan-out delay fan-in delay wiring overhead compare networks modeling spice simulation technology libraries alus arranged rectangular grid manhattan routing alu side distances measured segments -segment distance adjacent alus cart ut-cs single-hop networks delay fan-out fan-in wire fan-out fan-in -wide network fan-out fan-in dominates short distances wire delay dominates long distances cart ut-cs single-hop networks nodes reducing fan-out fan-in helps short-distance communication distance delay fan-out fan-in delay fan-out fan-in cart ut-cs multi-hop networks delay fan-out fan-in router hops wire fan-out fan-in mesh network fewer wires wiring overhead router delays accumulate long distances effectively hidden lookahead arbitration scheme routed inter-alu network rian fan-out fan-in mesh cart ut-cs wiring area overhead area wiring tracks area alus wiring overhead area wiring tracks area alus increase wiring length conservative area models show networks nodes single-hop -wide networks cart ut-cs singlevs multi-hop networks nodes distance single-hop alpha single-hop alpha multi-hop crossover dist conservative wiring overhead model single-hop networks outperform multi-hop networks detailed modeling required accurately determine cart ut-cs evaluation architectures compare single-hop bsd bss multi-hop pmd networks architectures dynamically scheduled superscalar processors statically scheduled vliw processors statically scheduled dynamically issued grid processors network configurations benchmarks gzip mcf parser ammp art equake dct adpcm mpeg encode radar machine configuration alpha functional unit latencies -cycle -kb -cache -cycle -cache -level branch predictor clock cycle technology wiring overhead single-hop networks performance metrics routing latency contention hops ipc cart ut-cs vliw architectures -wide machines simulated wire fan-out fan-in delays circuit analysis single-hop networks worst real ideal multi-hop networks cart ut-cs vliw architectures results routing latency cycles multi-hop networks multi-hop networks real worst contention latency due contention networks number hops scheduler effective placing consumers close producers cart ut-cs machine width sideal sreal alpha sworst alpha vliw architectures results ideal real worst real worst ideal cart ut-cs grid processor architectures mesh startriangle statically scheduled dynamically issued array alus rian fast clock rate high ilp array delays circuit analysis compare ideal real worst cart ut-cs grid processor architectures results real worst triangle mesh star ideal ipc hops contention latency cycles network cart ut-cs conclusions traditional operand transmission networks large wiring overhead delay increases architectures technology scales rians outperform traditional broadcast scale alus low bandwidth interconnect faster broadcast network circuits performance ipc routing latency projections future rians dynamically scheduled architectures require rescheduling techniques statically scheduled architectures require destinations encoded isa cart ut-cs questions 
isscc session digital architecture systems paper wire-delay scalable microprocessor architecture high performance systems stephen keckler doug burger charles moore ramadass nagarajan karthikeyan sankaralingam vikas agarwal hrishikesh nitya ranganathan premkishore shivakumar department computer sciences texas austin department electrical computer engineering texas austin microprocessor pipeline depths increased dramatically decade fast approaching optimal depth shown fig number logic levels modern processors nearing fanout-offo inverter delays substantial reductions undesirable due pipeline overheads power consumption technology trends show global on-chip wire delays growing significantly increasing cross-chip communication latencies tens cycles rendering expected chip area reachable single cycle technology shown fig challenge architects design architectures achieve fast clock rate low high concurrency slow global wires existing superscalar microarchitectures rely global communication poorly matched technology challenges coming decade grid processor architecture gpa designed address challenges shown fig gpa implementation consists array scalable larger dimensions alus connected routed operand network i-cache d-cache register file banks periphery alu array alu includes integer unit floating point unit instruction buffers operand buffers operand router proposes alu chaining similar gpa clustered vliw architectures similar partitioning strategies gpas permit out-of-order execution fast clock rates achieving performance higher conventional architectures gpa program spatial instruction placement static execution order dynamic compiler forms large singleentry multiple exit regions hyperblocks schedules alu array current hyperblock generation techniques yield instruction blocks consisting average instructions typically fewer input output registers benchmarks shown fig specint specfp instructions critical path block mapped grid minimize communication latency short physical paths adjacent alus fig bypass path alu runtime instructions block fetched masse multi-ported instruction cache distributed horizontally grid instructions execute dataflow order dictated arrival time operands alu intermediate values routed directly producing alu consuming alu written back register file block outputs written register file block completion dynamically bypassed directly instructions block next-block predictor speculatively selects subsequent blocks mapped executed current block executed misspeculations exceptions rollback committed block boundary figure shows instructions clock ipc achieved gpa comparing alpha gpa offers specific technology scaling advantages conventional architectures facilitates partitioning alu array instruction caches register files providing faster access higher bandwidth memory structures communication delays alu array exposed compiler optimization reducing broadcast communication networks core gpas enable block-atomic state tracking orchestration opposed conventional instruction-oriented approaches block-atomic model serves eliminate per-instruction overheads centralized structures instruction fetch rename register read commit exception handling finally instruction buffers alu serve set distributed reservation stations enabling effective dynamic scheduling window hundreds thousands instructions gpa-based systems provide unique opportunities power efficiency elimination structures dedicated instructionlevel register renaming associative operand comparisons state tracking reduce overhead circuitry power peralu basis alu chaining dramatically reduces number global register file accesses exchange short point-to-point connections dynamic power alu array banked memory structures actively managed reduce consumption periods light utilization dataflow execution model gpa amenable power-efficient asynchronous design techniques addition high ilp secondary design goal gpas polymorphism ability adapt hardware execution characteristics application grid processors easily subdivided sub-processors allowing discrete threads assigned sub-processors high thread-level parallelism tlp grid processors configured target data-level parallelism dlp exhibited media streaming scientific codes dlp applications gpa hardware employs execution model instructions kernels loops mapped alus stay resident multiple iterations addition access data cache bank multiple values distributed alus row initial results set signal processing kernels show gpa average compute instructions cycle assuming -ghz clock cmos configuration achieve performance level gflops shown fig prototype chip consist cores shared cache structure built array memory banks connected routed network set distributed memory controllers channels external memory planned prototype built process targeted completion future technology generations enable similar chips powerful cores acknowledgments research supported darpa contract -cand grants nsf sloan foundation donnell foundation ibm intel hrishikesh jouppi farkas burger keckler shivakumar optimal logic depth pipeline stage inverter delays iscapp nagarajan sankaralingam burger keckler design space evaluation grid processor architectures micropp december ozawa imai ueno nakamura nanya performance evaluation cascade alu architecture asynchronous super-scalar processors async march ieee international solid-state circuits conference ieee isscc february salon figure historical reduction cycle time driven pipelining figure grid processor block diagram figure gpa achieves greater instructions clock ipc conventional out-of-order core figure projected fraction chip reachable cycle clock period figure mapping dataflow critical path physical alus figure diagram proposed chip-multiprocessor cmp prototype gpa cores ieee international solid-state circuits conference ieee ieee international solid-state circuits conference ieee figure historical reduction cycle time driven pipelining ieee international solid-state circuits conference ieee figure projected fraction chip reachable cycle clock period ieee international solid-state circuits conference ieee figure grid processor block diagram ieee international solid-state circuits conference ieee figure gpa achieves greater instructions clock ipc conventional out-of-order core ieee international solid-state circuits conference ieee figure mapping dataflow critical path physical alus ieee international solid-state circuits conference ieee figure diagram proposed chip-multiprocessor cmp prototype gpa cores 
appears annual workshop interaction compilers computer architectures interacta technology-scalable architecture fast clocks high ilp karthikeyan sankaralingam ramadass nagarajan doug burger stephen keckler computer architecture technology laboratory department computer sciences texas austin cart utexas utexas users cart abstract cmos technology scaling poses challenges designing dynamically scheduled cores sustain high instruction-level parallelism aggressive clock frequencies paper present architecture maps compiler-scheduled blocks two-dimensional grid alus mapped window execution instructions execute dataflow-like manner alu forwarding result short wires consumers result describe studies program behavior preliminary evaluation show architecture potential high clock speeds high ilp offer vliw dynamic superscalar architectures introduction conventional microarchitectures improving performance approximately year improving instructions cycle ipc transistors chip increasing clock speed strategies fail future technologies clock speed growth slowing fundamental pipelining limits wire delays making architectures communication bound today architectures scale showing diminishing returns ipc increasing chip transistor budgets designs address issues efficiently utilizing increasing transistor budget overcoming communication bottlenecks approach extracting ilp conventional superscalar cores detect parallelism run-time amount ilp detected limited issue window logic complexity grows square number entries conventional architectures rely frequently accessed global structures register files re-order buffers issue windows bottlenecks limiting clock speed pipeline depths approach extracting parallelism vliw machines ilp analysis performed compile time instruction scheduling performed compiler orchestrating flow execution statically approach performs regular workloads suffers drawback dynamic events handled stall functional unit forces entire machine stall functional units synchronized paper describe architecture called grid processor takes consideration technology constraints wire delays pipelining limits compiler detect parallelism statically schedule instructions computation substrate instructions issued dynamically propose execution substrate consists set named distributed computing elements compiler statically assigns individual instructions architecture suffer vliw issue restrictions instructions issued dynamically executed dataflow fashion instructions compiler-generated basic block hyperblock mapped statically nodes computation array node assigned instructions nodes issue instructions dynamically input operands temporary values produced consumed inside block visible architectural state forwarded directly producers consumers propose fine-grained partitioning issue window functional units computation array includes grid issue window-fu pairs nodes dedicated communication network passing data data produced node routed dynamically intermediate nodes eventual destinations architecture hybrid conventional superscalar conventional vliw architectures issuing instructions dynamically static scheduling hardware substrate designed extract high ilp run aggressive clock rates grid processor transistor budget build array computation elements aimed overcoming challenges communication overhead future systems forwarding values directly producers consumers reliance centralized structures reduced compiler controlled physical layout ensures critical path scheduled shortest physical path finally instruction blocks mapped grid single units computation amortizing scheduling decode overhead large number instructions reduced reliance centralized structures computation substrate clocked high speeds remainder paper organized secmemory interface instruction sequencer block termination control nearest neighbor interconnectexpress channel memory register file figure grid block diagram express channels connect row row tion describes key features grid processor demonstrates programs mapped section characterizes aspects program behavior indicating existing applications amenable execution grid processor section describes related work pertaining wide-issue dataflow oriented machines section concludes discussion secondary advantages including power reduction speculation control remaining issues solved architecture grid processor consists computation substrate configured two-dimensional grid fine-grained computation nodes connected interconnection network compiler partitions program sequence blocks basic blocks hyperblocks performs renaming temporaries schedules instructions block nodes grid instruction traces generated run-time blocks generated compiler blocks fetched time instructions mapped computation nodes masse assigned compiler execution proceeds dataflow fashion instruction sending results directly instructions set interfaces computation substrate access external data computation nodes figure shows high level overview grid interfaces nearby neighbors grid connected short wires small communicaop alufrom instruction sequencer storageinst input ports router output ports figure organization computation node tion delays fast express channels connect nodes physically grid instruction sequencer fetches blocks instructions instruction memory places instructions nodes scheduled compiler block termination control interfaces register file memory interface detects block completes execution commits architecturally visible data register file memory memory interface communicate load store queue caches main memory computation nodes lightweight units perform function execution temporary storage data forwarding computation node consists set functional units storage structures instruction wakeup unit router read write ports communication figure shows layout computation node functional units consist integer unit optionally floating point unit perform actual execution storage structures include set queues buffers storing instructions input operands data tokens forwarded nodes grid instruction wakeup unit matches instructions operands arrive issues functional units execution router examines tokens storage structures forwards paths node eventual targets data tokens meant nodes bypass alu directly forwarded router destinations execution model compiler partitions program sequence blocks blocks constructed internal control flow control transfers block initiate instructions blocks blocks basic blocks hyperblocks program traces generated run-time figure shows stream instructions partitioned compiler blocks figure shows grid interconnect topology add add add beqz xdeac end block add add mul bne xbee end block xor sll add add end block add jmp figure sample instruction stream basic blocks case explicit move instructions separate computation instructions generated registers read block move instructions fetch block inputs register file pass internal temporary values block figure shows data flow graph dfg blocks figure move instructions shown figure instructions renamed temporary registers operands move instructions generated input register block move instructions move move generated compiler input registers inside block values referenced temporary names move instructions associates register inputs block temporaries data values passed blocks written register file run-time instruction sequencer fetches block instruction memory maps grid masse serialization fetch decode rename instructions block individual instructions written storage structures nodes assigned compile time block execution initiated move instructions read register data send consumers instruction wakeup unit matches incoming data instruction issues ready instructions functional unit execution results computation tagged forwarded router interconnect eventual destinations instruction mapping compiler generates mapping physically laying data flow graph block grid computation instruction block assigned node grid critical path scheduled shortest physical path output operands renamed positions consumer nodes move instructions serve purpose associating register data positions consumer nodes figure illustrates layout grid instructions mapped computation elements instructions block figure mapped grid positions correspondingly move instructionmove encoded destination fields register input field instruction wakeup execution section multiple instructions mapped single node data written operand buffer arrive arrival data token instruction operand buffers 
examined wake-up issue ready instructions wakeup delays considerably smaller conventional cores smaller issue windows computation node serially performs operations operand arrives wakeup execute serializing wakeup execute increase cycle time execute-execute path dependent instructions wakeup-execute pipelined stages instruction wakeup slow clock execute phase producer overlapped wakeup phase consumer conventional superscalar cores dedicated bypass paths forward data guarantee execution instruction dependents data cycle grid processor data routed dynamically dedicated path guaranteed free instruction completes execution forward data consumer mechanisms alleviate problem special wakeup tokens generated issue stage producer instruction reach consumer nodes end stage reserving channel data follow cycle alternately speculative instruction issue hide select latency local rollback mechanisms event incorrect issue block mapping instruction operand storage structures node buffer multiple instructions data tags reasons multiple instructions mapped node graphs larger physical grid folded mapped add add add beqz xdeac move move add add mul bnez xbee move move move move move xor sll add add figure basic blocks shown dataflow graphs registers marked temporaries move move add add figure basic blocks mapped grid dimension nearest neighbors reachable directly instruction destinations ordered pairs identify consumer relative position nodes nodes producer grid instruction node instructions blocks fetched speculatively control speculation speculative threads mapped node finally blocks threads mapped support multithreading instruction encoding grid processor isa divided data movement instructions computation instructions data movement instructions include move split repeat instructions move instructions fetch block inputs register file pass temporary values block encoding space limitations restrict number targets instruction split instructions replicate data reach additional targets range target distance producer finite repeat instructions forward data targets range trade-off instruction size number specifiable targets range target instruction encoded opcode field destination field case move instructions input field destination field consists multiple targets target encoded position consumer expressed offset move instructions encoded input register destinations figure shows sample encoding move instruction computation instruction move instructionmove encoded move input register consumer instructions mapped instruction encoded destinations consumers temporary mapped node directly mapped node extra bit shown figure order input operands role compiler compiler plays important role grid processor detecting ilp compiler constructs blocks scheduled grid defines mechanisms intra inter-block communication compiler generate data movement instructions overcome encoding space limitations grid processor blocks fetched single unit mapped grid executed dataflow fashion instructions fetched block granularity desirable large blocks good block utilization block utilization defined ratio dynamically executed instructions static size block method building large blocks build hyperblocks based profiling information register file communication data passed successively executed blocks bypassed grid interconnect stitching blocks single dataflow graph compiler define interfaces mechanisms stitch multiple blocks data movement instructions add overhead scheduling graph compiler minimize critical path attempt minimize number instructions preliminary analysis section investigate amenability existing applications grid processor examine aspects program behavior affect performance large blocks significant number block temporaries input output registers desirable low register file bandwidth desirable large blocks high utilization amortize cost block fetch map encoding space needed representing temporaries determined number targets instruction fewer average targets produced permits compact encoding examine characteristics existing applications determine map grid processor experimental analysis spec cpu benchmarks compiled trimaran tool set floating point equake ammp art integer parser gzip mcf benchmarks selected analysis hyperblocks generated benchmarks trimaran impact compiler train input set profiling benchmarks simulated trimaran simulator million instructions ref input set collected dynamic statistics modifications made simulator track block size profiles register usage benchmark average instructions block executed dynamically due early static size executed branches nops gzip mcf parser art equake ammp table block utilization instruction behavior section examine aspects program behavior performance affected block sizes programs grid configurations profile dynamic block size obtained benchmarks analyze trade-off block size respect block utilization wide grids performance cost increased area fewer nodes mapped instructions analyze trade-off grid widths block size analysis spec cpu benchmarks observed large hyperblocks built figure shows dynamic block size profiles benchmarks benchmarks figure plots percentage execution time spent dynamic block size cumulative distribution function dynamic block size number instructions block executed excluding predicated instructions converted nops execution time spent blocks size greater integer benchmarks blocks size greater floating point benchmarks benchmarks average number dynamic instructions block ranges high utilization percentages desirable blocks fetched mapped single unit blocks poor utilization large number instructions fetched mapped executed hyperblock instructions executed early exits block predicated instructions converted nops table shows average number instructions block belong categories benchmarks average utilization average static block sizes ranging benchmark gzip shows worse utilization benchmarks instructions fetched executed preliminary results show potential building large benchmarks integer benchmarks percentage execution time block size equake percentage execution time block size gzip percentage execution time block size art percentage execution time block size mcf percentage execution time block size ammp percentage execution time block size parser figure cdf block size profiles x-axis represents number dynamically executed instructions block y-axis represents percentage execution time spent blocks sizes expressed cumulative distribution function block approximate execution time dynamic instruction count width min height nodes utilization ammp insts parser insts table grid utilization grid widths frequently executed blocks ammp parser blocks good utilization grid utilization grid dimensions connectivity determine performance utilization define grid utilization fraction nodes grid instructions scheduled affected richness interconnect fewer connections result sparser schedule turn resulting lower utilization conservatively chose connectivity restrictive guaranteeing technological scalability future studies explore connectivity design space preliminary analysis simulator schedules dfg block grid finite width infinite depth greedy critical path scheduling strategy strategy schedules instruction node longest path dfg shortest physical path grid sufficient encoding space assumed targets instruction grid position selected frequently executed blocks benchmarks examined grid utilization grid configurations blocks ammp parser account dynamic instructions executed static sizes instructions blocks scheduled grids connectivity widths increasing width grid increases number independent instructions scheduled single row wider row reduces height schedule wastes nodes schedule computed grid utilization minimum grid height required table shows grid utilization cases table shows width blocks exhibit high utilization requiring grid height increase grid width decreases height ammp parser improving performance poorer node utilization width increased drop height utilization drops dramatically balance grid height node utilization note input registers number 
blocks regs ammp equake art gzip parser mcf temporary registers output registers table input output temporary registers blocks mapping multiple blocks threads increase utilization minimal cost register behavior block inputs block outputs data read written register file number inputs outputs determine bandwidth number ports required register file block temporaries data created block large number temporaries significant amount communication register file eliminated report results simulations estimate number input registers output registers block temporaries program execution input output temporary data well-behaved block number register inputs outputs small number temporaries large table shows number input output temporaries selected benchmarks percentage executed blocks versus number registers types shown benchmark ammp fewer registers written blocks executed additional blocks output registers integer benchmarks show slightly behavior blocks integer benchpercentage registers fanout gzip parser mcf percentage registers fanout ammp art equake figure register fanout number targets produced marks read write fewer registers benchmarks blocks read write registers larger number due larger block sizes benchmarks average block size integer benchmarks parser benchmarks temporaries blocks showing significant reduction register bandwidth achieved internal renaming provided architecture register fanout limited instruction encoding space constrains number consumers fanout refers number targets instruction produce data largefanout instructions require extra split instructions reach consumers figure shows average fanout produced benchmarks benchmarks fanout instructions equal instructions shows support targets onefifth producers require split instructions results consistent obtained franklin sohi work studied liveness registers terms number instructions fanout registers analyze register behavior block level technology evaluation evaluate grid processor technology scaling technology-independent area delay estimates gupta agarwal minimum features include -bit integer alu multiplier -entry instruction operand buffers -bit alu buffers router node assumed chip area reserved grid interconnect computed area occupied node grid height technology clock speed grid dimensions row delay delay ghz cycles cycles table area delay estimates technologies technologies number nodes accommodated chip node dimensions derived area estimates computed wire delay nodes adjacent rows delay traverse entire height grid sia projected clock rates results summarized table technologies wire delay adjacent rows close single cycle result shows grid fast local interconnect built technologies feature size shrinks node density delay height grid increases super-linearly technologies grid sizes larger required map typical block express channels prohibitively long delays cases computation substrate partitioned multiple grids related work number related approaches preceding grid processor dennis misunas proposed static dataflow architecture programs expressed fortran-like dataflow language arvind proposed tagged-token dataflow architecture purely datadriven instruction scheduling programs expressed dataflow language culler proposed hybrid dataflow execution model programs partitioned code blocks made instruction sequences called threads dataflow execution threads approach conventional programming interface dataflow execution limited window instructions hybrid approach vliw conventional superscalar architectures statically scheduling instructions compiler dynamically issuing efforts enhance dynamic execution vliw machines rau proposed split-issue mechanism separate register read execute writeback delay buffer support dynamic scheduling vliw processors looked naming mechanisms values reduce register pressure register file size smelyanskiy proposed register queues allocating live values software pipelined loops llosa proposed register sacks low bandwidth port-limited register files allocating live values pipelined loops corporaal proposed explicitly communication mechanism transport triggered architectures general purpose computing researchers exploring distributed partitioned uniprocessor designs waingold proposed distributed execution model extensive compiler support raw architecture raw architecture assumes coarsergrain execution grid processor exploiting parallelism multiple compiler-generated instruction streams ranganathan franklin empirical study decentralized ilp execution models sohi proposed multiscalar processors single program broken collections tasks tasks distributed number parallel processing units reside processor complex units fetches executes instructions belonging assigned task rotenberg proposed trace processors processing elements work traces program passing data values common register bus unlike trace processor grid processor executes trace fine-grain dataflow fashion overlaps multiple traces computation substrate finally patt proposed block-structured instruction set architecture increasing fetch rate wide issue machines atomic unit execution block instruction conclusion paper proposed technology-driven architecture combines advantages compiler-scheduled instruction level parallelism data-driven execution fast clocked grid execution units instruction blocks mapped grid single units computation amortizing fetch decode large number instructions access global storage elements register files reduced maintaining temporaries transient values grid overheads reduced overlapping execution instruction block fetch mapping initial evaluation existing programs ripe mapped substrate typical block sizes range dynamically executed instructions anticipate sufficiently large amortize scheduling overheads number input output values required large fraction blocks benchmarks indicating amount register file communication blocks small average number temporary registers block larger ranging depending benchmark range substantial amount communication centralized register file eliminated producer consumer communication grid finally average number consumers produced shows network grid require large bandwidth intra-block communication addition direct performance advantages proposed grid processor benefits offers substantial power savings scheduling performed compile time execution unit idle operands arrive blocks program loops mapped reused times time power required block mapping reduced substantially conventional architectures instructions fetched repeatedly iteration loop mapping reuse permit grid processor act high performance substrate dsp codes finally data driven computation model grid processor amenable polypath execution selective re-execution misspeculation conditionally executed instructions block started speculatively speculation incorrect block re-executed loading correct values graph nodes letting values propagate data dependent downstream instructions instructions refetched independent instructions re-executed mis-speculation rollback paper outlines basic grid processor architecture potential performance improvement substantial challenges lie ahead complete design dataflow-style execution amenable computation memory parallelism dynamic dependences loads stores detected ensure proper ordering memory system dynamic dataflow execution compiler-controlled static scheduling removes synchronization execution substrate conventional architectures enforce challenge lies detecting instructions block terminated architecturally visible storage committed finally traditional precise exception model exception occur point instruction stream challenging grid processor changing granularity rollback instruction block level enable efficient exception implementation acknowledgements anonymous reviewers cart group members feedback early versions paper work supported national science foundation career awards ccrand ccrcise research instrumentation grant eiauniversity partnership awards ibm grant intel research council agarwal hrishikesh keckler burger clock rate versus ipc end road conventional microarchitectures proceedings annual international symposium computer architecture pages june agarwal keckler burger scaling microarchitectural structures future process technologies technical report department computer sciences texas austin austin february arvind nikhil executing program mit tagged-token dataflow architecture ieee transactions computers corporaal transport triggered architectures phd thesis delft technology september culler sah schauser von 
eicken wawrzynek fine-grain parallelism minimal hardware support compiler-controlled threaded abstract machine proceedings international conference architectural support programming languages operating systems pages april dennis misunas preliminary architecture basic data-flow processor proceedings annual symposium computer architecture pages january fisher long instruction word architectures eliin proceedings tenth annual international symposium computer architecture pages june franklin sohi register traffic analysis streamlining inter-operation communication fine grain parallel processors proceedings annual international symposium microarchitecture pages gupta keckler burger technology independent area delay estimates microprocessors building blocks technical report department computer sciences texas austin austin february hao chang evers patt increasing instruction fetch rate block-structured instruction set architectures proceedings international symposium microarchitecture pages december llosa valero fortes ayguade sacks organize register files vliw machines conpar vapp september palacharla jouppi smith complexityeffective superscalar processors proceedings annual international symposium computer architecture pages june ranganathan franklin empirical study decentralized ilp execution models international conference architectural support programming languages operating systems pages october rau dynamically scheduled vliw processors proceedings annual international symposium microarchitecture pages december rotenberg jacobson sazeides smith trace processors proceedings annual international symposium microarchitecture pages december mahlke lin chen hank bringmann effective compiler support predicated execution hyperblock proceedings annual international symposium microarchitecture pages june national technology roadmap semiconductors semiconductor industry association smelyanskiy tyson davidson register queues hardware software approach efficient software pipelining international conference parallel architectures compilation techniques pact october sohi breach vijaykumar multiscalar processors proceedings international symposium computer architecture pages june trimaran infrastructure research instruction-level parallelism http trimaran waingold taylor srikrishna sarkar lee lee kim frank finch barua babb amarsinghe agarwal baring software raw machines ieee computer pages september 
gpr gpr gpr fpscr xer ctr gpr fpr fpr fpr fpr arbitrary boundary packed arithmetic karthikeyan indian institute technology madras department aerospace engineering iit madras india aero iitm ernet ranganathan sri venkateswara college engineering department computer science engineering svce pennalur india psranga mail utexas abstract recent microprocessors enhanced media instruction sets accelerating media algorithms exploit fact media algorithms small data types widths processor current media instruction sets support -bit subdatatypes scheme ineffecient applications bit lengths user programmable sub-datatype bit lengths discusses arbitrary boundary packed addition media algorithms based multiplyaccumulate algorithms full acceleration arbitrary boundary packed multiplication present scheme based wallace tree multiplication expand provide detailed treatment intermediate carries sub-datatypes lost previous work carries saturation arithmetic flow control introduction find general purpose processors incorporating special instructions manipulate multimedia data mmx vis sparc maxof pa-risc examples trend motivated fact small increases hardware results appreciable increase speed operations media algorithms factors small data types operations repeated times high data parallelism instruction sets special integer instructions operate packed units integers -bit integer interpreted packed -bit integers adding -bit numbers minor algorithm result adding -bit integers similar instructions provided multiply-accumulate operations interesting observation made balakrishnan nandy size sub-datatypes allowed chosen convenience implementation architectures -bit data subdivided -bit parts instances data sizes required summarized mpeg -bit fixed point data modem -bit data graphics bits audio -bit data applications medical imaging -bit data idct mpeg -bits cases mismatch actual sub-datatype sizes speedup achievable existing architectures optimum processing -bit values requires -bit subtypes leads slower processing efficieny improved allowing sub-datatype widths previous work scheme adding arbitrarily packed subdatatypes based carrylookahead algorithm provide synopsis carry-lookahead scheme carries generated parallel terms called carry propagate carry generate defined binary numbers nbnzr nbnzr nbnzr nbnzr defined carry propagate carry generate adding -bit numbers packed bit sub-datatypes carry generated bit assuming lsb bit packed words separate numbers carry lower nibble propagated higher nibble achieved logically ing bit position generally term carry propagate term sum exclusive-or mask register width original datatype bits boundary sub-datatype bit belongs leads modified expressions carry propagate sum ibnzr bnzr summary mask register mark boundaries sub-datatypes sub-datatypes width modified expressions carry propagate sum motivation idea arbitrary boundary packed arithmetic operations form till addition dealt media algorithms based multiply-add operation full acceleration arbitrary boundary packed multiply-accumulate implemented present method arbitrary boundary packed multiplication based wallace tree algorithm previous work block carries generated subdatatype record carries generated addition sub-datatypes importance saturation arithmetic handing numbers representing sound samples color intensitites dealt generation usage subdatatype carries detail arbitrary boundary packed addition describe carry lookahead addition scheme describe extended arbitrary boundary packed addition carry lookahead addition binary numbers nbnzr nbnzr sum bits nbnzr carry bits bnzr sum bits cla calculated scheme propagate generate carry ibnzr bnzr sum ibnzr ideally carry lookahead hardware constructed bit length digits impractical limitations fan-in fan-out irregularity structure length wires required overcome problem group carry adders essentially small carry lookahead adders stringed group smaller cla generate group generate group propagate similar propagate generate normal cla final group carry generated group group carry group arbitrary packed cla section dealing arbitrarily boundary packed addition n-bit numbers cla scheme section extended group carry addition sake simplicity brevity discuss implementation cla define boundary packing define mask register loaded time packing changed mask register n-bit register n-bit machine -bit machine bit packing mask -bit machine bit packing mask lsb sub-datatype rest examples mask extra n-bit carry register carries generated previous addition bubbled back bnzr addition equations cla change propagate generate carry ibnzr bnzr sum ibnzr carry previous add carries sub-datatype carry handling defined takes care carry generated previous addition suppose addition datatype generated extra carry bit carry status mask carry xxxx xxxx addition carryin sub-datatype bubble carry lsb sub-datatype case zeroth location bubbled carries saved updated addition addition extra hardware implement saturation arithmetic controlling program flow means test-andjump instructions defined initially set values set left definition implemented recursive definition introduce clock delay calculating product terms n-bit adder unacceptable unroll recursion redefine simplified speeded evaluation introduce propagation delay gates maxmimum products summed introduce delay addition term term reduce considerably precompute product terms save mask register set product purely function independent total numer product terms nbnzr saved n-bit pseudo-registers call pseudo-registers registers requiring complex wiring memory locations wired single place circuit require bnzr registers require pseudo-registers definition equation terms implies ibnzr term carry excess baggage terms ibnzr ibnzr ibnzr rewrite ibnzr ibnzr ibnzr equation number terms remain constantly investigated reduce complexity increase speed arbitrary boundary packed multiplication booth multiplication speedup sequential multiplication wallace tree algorithm popular methods implementing multiplication hardware describe aribitrary boundary packed algorithm high speed multiplication based wallace trees pages wallace tree multiplication essential idea wallace tree multipliers split multiplication smaller operands multiply operands seperately parallel add partial products smaller operands typically -bit operands multiplication purely combinatorial logic -bit computer bit operands rewritten higher lower order operands partial products added correct bit weights arbitrary boundary packed multiplication deal discussed multiplier problem hypothetical -bit machine machine explain operation modified wallace tree multiplier -bit combinatorial multiply operands mask figures pertain computer multiply require partial products rest killed shown figure dotted lines represent products thick lines represent products retained array mask bits call mask array refer figure shows bits boxed thick lines implemented making combinatorial multiplier circuit original multiplier circuit found page modified circuit shown figure kill bit products required replace input gates input gates input gate appears international conference computer design routed inter-alu networks ilp scalability performance karthikeyan sankaralingam vincent ajay singha stephen keckler doug burger computer architecture technology laboratory department computer sciences department electrical computer engineering texas austin cart utexas utexas users cart abstract modern processors rely heavily broadcast networks bypass instruction results dependent instructions pipeline clock rates increase architectures wider pipelines deeper broadcasting complex slower difficult implement complexity compounded shrinking feature size communication speed decreases relative transistor switching speeds paper examines fundamental bypassing networks proposes method classifying inter-alu networks based operands routed producers consumers propose evaluate circuit architectural level fine grain point-to-point routed inter-alu network rian delivers higher instruction throughput full bypass network higher speeds fewer wires introduction critical loop pipelined processors enables data dependent instructions execute consecutive cycles shown prior research increasing path single cycle dramatically reduces instruction throughput rates modern processors including dynamically scheduled superscalar vliw architectures form broadcast deliver instruction results places consumer instruction reside alu execution delay bypass latency deliver alu output back input sets cycle time machine complexity delay bypass paths increasing modern processors technologies wire delay path source destination fan-out delay source fan-in delay destination increasing increasing issue width functional unit count wire complexity growth proportional square number alus shown ahuja fan-out source alu fan-in destinations increases roughly product pipeline depth width alu result routed places larger fan-out fan-in increases bypass delay capacitive load network multiplexor complexity sink rises based optimistic wiring overhead models estimate shortest longest bypass path delays future ultra-wide -issue processor clock cycle cycles contrast conventional processor designs worst case bypass delay small incorporated critical path fraction clock cycle paper analyze fundamental requirements interalu networks ians microprocessor bypass networks propose evaluate emerging class bypass networks called routed inter-alu networks rians scale technology functional unit count viable alternative broadcast bypass networks future designs networks neighboring alus connected direct links lightweight routers communication alus making multiple hops network broadcast operands routed source destination based destination identifier encoded instruction rians reduce fan-in fan-out alu potentially crippling wiring area overhead network lower bisection bandwidth broadcast network network significantly reduces bypass latency nearby alus increase latency distant alus traverse hops rian present full taxonomy ians section classifying based number target alus result delivered number alus alu directly connected routing decision made rians propose classified point-to-point multi-hop dynamically routed networks represented acronym pmd discuss applicability networks dynamically scheduled monolithic clustered processors destination operand evaluate network statically scheduled architectures source destination operand determined compile time explore utility wide clustered nonclustered vliw architectures finally examine network strategy emerging architecture supports static placement dynamic issue tolerate run-time determined instruction latencies key applicability point-to-point routing architectures results inherently producer consumer broadcast alus remainder paper organized section describes design space inter-alu networks discusses relate prior bypass network architectures section examines circuit implementations high bandwidth single-hop networks conventional bypass networks thin multihop networks rian describe mechanisms execution model network architecture router control acronym examples point-to-point multi-hop dynamic pmd parcerisa grid processor point-to-point multi-hop static pms raw point-to-point single-hop dynamic psd m-machine multicluster point-to-point single-hop static pss degenerate case pms broadcast multi-hop dynamic bmd alpha broadcast multi-hop static bms broadcast single-hop dynamic bsd superscalar broadcast single-hop static bss vliw table taxonomy bypass networks reducing router overhead thin network sections explore thin networks dynamically scheduled superscalar architectures vliw architectures grid processor architectures finally section summary concluding remarks taxonomy inter-alu networks bypass networks intended provide fast paths outputs alus inputs prior stages pipeline downstream register file primary effect performance reduce eliminate read-after-write hazards pipeline stalls result issuing back-to-back producer consumer instructions conventional processors bypass paths typically implemented broadcast networks output alu routed input alu broadcast bypass networks part broader class inter-alu networks ians classified axes execution model network architecture router control execution model output alu broadcast default alus target alus explicitly operands point-to-point network architecture operand directly output alu input single-hop pass intermediate routers multi-hop router control routing decisions made prior execution alu instruction producing data static routing decisions place runtime dynamic networks differ dramatically multiprocessor networks payload scalar multi-word message cache line table lists bypass network configurations architectures -letter acronym network criterion pipelined superscalar architectures classified bsd networks operands broadcast target alus intermediate routers arbitration dynamically clustering alpha operands broadcast local remote cluster classified bmd operands hop reach remote cluster arbitration dynamically traditional vliw processors shared register file namespace broadcast data alus statically scheduled busses bss transistors faster wires slower broadcast networks attractive due increasing wiring overhead large connectivity networks architectures proposed implemented family point-to-point ians broadcast networks m-machine psd network destinations statically encoded instruction delivery occurs dynamically source cluster destination cluster multicluster architecture psd dynamically routes operands demand clusters partitioned register file superscalar architecture mit raw processor includes bypass routing network integrated pipeline routing overhead mitigated statically scheduled router eliminates dynamic arbitration shared router wire resources making pms network architecture achieved per-hop latencies single cycle experience showed latencies high achieve sufficient ilp raw requires full cycle hop components communicate complete processors small alus found conventional processor core finally budding category ians pmd point-point multi-hop dynamically routed operand networks parcerisa proposed multi-hop routing network clustered superscalar architectures partitioned register files similar principle multicluster microarchitecture track location producers consumers dynamically inserts instructions transmit operands source destination cluster paper focus flavor pmd networks instruction dependencies expressed explicitly instruction encoding physical locations producer consumer instructions prior execution knowledge bookkeeping hardware dynamically track instruction dependencies required instruction results broadcast alu providing support fast alu chaining technology scalability examine large networks alus explore range topologies connectivities key feature routed point-to-point networks operand bypass identification producer-consumer pairs prior execution producer scheduling pairs nearby alus focus static compile-time schedulers place instructions minimize communication distance runtime scheduling retire-time cluster assignment scheme proposed bhargava john taxonomy scalar operand networks consists -tuple cost model closely based network transport models proposed taylor -tuple consists send occupancy cycles alu spends transmitting send latency cycles sender consuming alu cycles network hop latency cycles spent network receive latency receive occupancy class networks table characterized -tuple axes classification focus architecture circuit characteristics operand network specific delay models -tuple model expressive terms delays insufficiently expressive 
characterize diversity operand interconnect networks completely types networks tuple psd bmd networks map bsd bss networks implemented conventional superscalar vliw processors map circuit modeling inter-alu networks section describes circuit modeling inter-alu networks explains technology models circuit estimation tools delay components circuit proposed taxonomy ians key circuit feature network architecture single-hop multi-hop network circuits broadcast point-to-point execution models router control static dynamic circuit design ian largely independent execution model router control section analyze single-hop multi-hop network circuits detail compare delays show multi-hop networks outperform single-hop networks large networks technology modeling estimated circuit latencies spice models derived international technology roadmap semiconductors estimated wire delays assuming optimal buffer placement capacitance numbers obtained space dimensional field solver technology parameters wire delay tool based international technology roadmap semiconductors technology point scaled match spice libraries analysis assume functional units producing consuming values laid -dimensional rectangular array manhattan routing scheme refer functional units nodes distances measured segments segment distance adjacent nodes network size total number nodes network figure shows single-hop multi-hop inter-alu network size experiments node consists alu integer multiplier fpu -entry register file functional units bits wide area dimensions nodes estimated empirical area model node square side occupying area half channel length minimum sized transistor processing core alpha comparison occupies area approximately single-hop inter-alu networks main components contribute communication delay single-hop networks fan-out gate delay segment node single-hop network multi-hop network figure single-hop multi-hop networks size wires top left node shown wire delay fan-in gate delay total delay segment path equation term equation denotes wire delay product number segments traversed wire delay unit length length segment inputs mask array element mask array initialisation mask register loaded bit product valid pseudo-code explains algorithm setting values implemented sequential circuit time delay clocks n-bit computer fillarray initialise mask array subdatalimit reached mask bit subdatatype subdatalimit break subdatalimit subdatalimit subdatalimit subdatalimit subdatalimit array initialized sequential circuit mask array require bits pseudo registers level complexity addition applications dft dct convolution dsp operations essentially multiply-add motivation instruction mmx instruction set architecture general purpose graphics multimedia techniques alpha blending sound mixing yuv yuv common data formats frame grabbers data arbitrary boundary packed multiplication-add efficient conclusion motivated comprehensive arbitrary boundary packed schemes support native data types future architectures presented scheme multiplication addition small overheads compared conventional adders multipliers balakrishnan nandy arbitrary precision arithmetic simd style proceedings eleventh 
international conference vlsi design wallace suggestion fast multiplier ieee transactions electron comp vol ecpp feb booth signed binary multiplication technique quart mech appl math vol fadavi-ardekani booth encoded multiplier generator optimized wallace trees ieee trans vlsi vol jun pihl aan multiplier squarer generator high performance dsp applications tech rep norwegian science technology cavanagh digital computer arithmetic design implementation mcgraw-hill book company figure bitwise multiplication blocks figure bitwise multiplication digits fafa fafa fafa fafa mmm figure combinatorial multiplier modified arbitrary bit packing 
wiring distance overhead wiring distance overhead factor incorporate physical vlsi design constraints wire routing number tracks required route wires fits area occupied alus indicating wiring overhead wires require extra area routing ratio length wires increase excess area routed wiring overhead strongly dependent technology alu dimensions data-path width routing strategies repeater placement area -bit data-path assuming wire pitch simple wiring area models account repeater area overhead show single-hop networks size greater incur wiring overhead network size multi-hop network configurations examined low fan-outs incur wiring overhead delay analysis spice simulations total fan-out fan-in gate delays -wide -wide networks measured figure shows percentage component contributes total communication latency distances -wide configurations communication latency evenly shared wire delay fan-out fan-in delay short distances delay due fan-in fan-out communication neighbors reducing fan-out fan-in delay low fan-out network produce significant reduction short distance communication latency total wire delay nanoseconds ideal -wide single-hop network assuming wiring overhead shown dashed line graph figure dotted line shows realistic delay incorporating wiring overhead estimation multi-hop network addition lower fan-in fan-out comparison aggressive clock period technology distance segments fan-out fan-in wide wide distance segments delay nanoseconds single-hop overhead single-hop multi-hop crossover point segments distances figure percentage wire delay contribution total communication delay table shows crossover point single-hop multi-hop networks crossover point maximum distance communication multi-hop networks delays reduce wire delay wiring overhead making effective communication distances shorter multi-hop inter-alu networks multi-hop networks defined networks require routing decisions made source destination nodes parts delay source router delay fan-out gate delay wire delay fan-in gate delay intermediate router delay compared single-hop networks wire delay segment reduced typically wiring area overhead fan-in fan-out delays reduced low-degree networks typically fan-in fan-out multi-hop network routes data multiple nodes incurring router delay node data pass destination node denote number hops figure shows topology multi-hop network routers node total delay -segment path network equation solid line figure shows variation communication delay distance multi-hop networks short distance communication multi-hop networks idealized single-hop network -wide network delay shown dashed line wiring overhead single-hop network included shown dotted line graph multi-hop networks distances router design source router intermediate router delays incurred arbitration performed hop communication path avoid resource hazards peh dally describe latency-hiding approach inter-chip networks lookahead flit reservation mechanism based work evaluated lookahead scheme routers rians hide arbitration delay networks implemented control payload data operand control packet arrives advance payload reserves path payload taking routing decision making logic critical data transmission path details design scope paper explained lookahead reservation scheme multi-hop delay effectively degenerates components shown equation based spice simulations determined router packet processing delay approximately gate delays technology fan-out fan-in gate delay fan-out network gate delays effectively lookahead reservation scheme hides packet processing delay data transmission critical path making per-hop forwarding latency fan-out fan-in gate delay approximately cycle clock period delay analysis implications short distances multihop networks faster lower fan-out fan-in delay wiring overhead multi-hop network forwarding delay incurred hop long distance multi-hop communication accumulated forwarding delay overcome fan-out fan-in wire delay savings eventually making multi-hop network delay larger single-hop network delay distance equating determine distance measured number segments single-hop network delay equals multi-hop network delay communication delay lower multi-hop network single-hop network distances segments cross-over point varies based network size fan-in fan-out wiring overhead single-hop networks dependent size table figure shows crossover point networks size values idealized single-hop network assuming wiring overhead short distances segments size networks faster multi-hop networks crossover point increases super-linearly multihop networks faster communication nodes network distance crossover points specific machine configurations technology wiring overhead architects parameters choosing operand network design based circuit analysis delays sections discuss design performance single-hop multihop networks processor architectures compare rians point-to-point multi-hop routed networks pmd conventional broadcast single-hop networks bss bsd examine dynamically scheduled superscalar architectures vliw architectures grid processor architectures dynamically scheduled architectures bypass delays critical conventional dynamically scheduled superscalar architectures current designs broadcast singlehop network alus execution model architectures provide information instruction stream determine consumers operands instruction future superscalar processors large instruction windows large numbers functional units extract parallelism bypass delay instruction wakeupselect delay designs unmanageably large partitioning instruction window clusters tightly coupled functional units mechanism keeping delays small increasing machine issue-width clustered architectures typically broadcast single-hop network functional units cluster number alus single cluster small bypass delay small compared cycle time clusters separate network variable inter-cluster communication delay absence mechanisms determine alu cluster consumers operands produced instructions assigned alu broadcast results alu processor clustering helps scaling instruction window size solve broadcast problem avoid broadcast operands 
parcerisa proposed clustered architecture instruction steering stage processor pipeline assigns instructions clusters inserts copy instructions orchestrate point-to-point inter-cluster communication cluster operands broadcast full bypass network bsd studied network interconnects cluster configurations concluded low bandwidth torus-like routed network pmd performed idealized full broadcast high bandwidth inter-cluster network based bypass network circuit characteristics fan-in fan-out wire delay wiring overhead circuit models suggest inter-cluster operand networks large clusters lower delays built single-hop network multi-hop network port limitations storage structures remote register files clusters impose limitations fan-in allowed cluster work parcerisa point constraints show rians preferable inter-cluster networks vliw architectures vliw architectures naturally amenable point-to-point routing operands multi-hop networks instructions statically scheduled producer-consumer pairs determined compile time encoded instructions conventional vliw processors single-hop high bandwidth broadcast networks bmd bss implement operand bypassing section compare networks pmd networks evaluate effectiveness software schedulers placing source consumer nodes close key feature multi-hop networks xxx xxx single hop networks sworst real ideal multi hop networks figure vliw interconnect networks scaling factors arcs refer relative delays wires based distance traveled machine model model conventional vliw machine compiler statically assigns instructions named functional units nodes decides execution order instructions greedy bottom algorithm generating schedules examine family single-hop full bypass networks ideal network set wire delays shortest delay path realistic case single-hop network scale wire delays nodes linearly distance worst case single-hop network set wire delays longest delay path conventional bypass networks resemble delay paths alus set worst case delay compare single-hop networks multi-hop networks short paths network wires adjacent nodes medium distance paths network wires nearest neighbors diagrams connectivity shown figure simulated multi-hop networks infinite bandwidth infinite wires ports connected nodes study effect contention bound sensitivity wiring overhead simulated single-hop networks examined -wide -wide -wide machine configurations multi-hop networks examined low fan-outs expect wiring area overhead examine configurations networks equations obtain delays simulations simulations assume processor executing clock cycle technology technology router forwarding delay cycles wire delay traverse segment cycles custom event-driven simulator model micro-architecture parameters functional unit latencies similar alpha -cycle -cache cycle -cache -level history based branch predictor simulator models aggressive lookahead resource reservation scheme implemented router assume data packet catches control packet fan-out fan-in forwarding delay incurred hop multi-hop networks full router processing delay hidden benchmarks evaluate performance networks realistic workloads selected set benchmarks spec cint spec cfp mediabench benchmarks gzip mcf parser ammp art equake dct adpcm mpeg encode examined in-house benchmark radar performs latency cycles contention config hops -wide vliw -wide vliw -wide vliw table interconnect network performance vliw architectures latencies shown processor cycles clock cycle single-hop networks contention pair alus connected dedicated wire number hops radar signal-processing computation dominated -point complex fir filter trimaran tool set targets hpl play-doh isa compile benchmarks fast forwarded benchmarks hundred million instructions simulate hundred million instructions results routing latency routing latency number cycles operand production receipt destination source destination nodes assume direct bypass execution cycle routing latency assumption makes average latency shorter fastest transmission path network routing latency machine configurations shown columns table width routed multi-hop network worse networks network size larger machine widths routed multi-hop network routing latencies versus versus network network incorporate wiring overhead single-hop networks networks good machine widths contention measure percentage contribution delay due contention measuring percentage difference routing latency real multi-hop network idealized multi-hop network infinite ports wires connected nodes idealized network delays incurred due resource hazards interconnect network percentage latency due contention shown column single-hop networks contention dedicated path pair alus average ipc sideal sreal sworst sreal alpha sworst alpha figure vliw ipc averaged high ipc benchmarks dct mpeg encode radar networks show roughly amount contention higher bandwidth network showing slightly contention contention accounts roughly latency -wide machine -wide machine suggesting higher bandwidth multi-hop networks improve performance number hops number hops route operands source destination effectiveness scheduler placing producer-consumer pairs close conventional single-hop networks number hops dedicated wire node node shown column table average number hops networks low compared machine width showing scheduler effective placing producer-consumer pairs close ipc figure shows ipcs averaged high performance benchmarks -wide -wide -wide configurations benchmarks suite exhibit improvement performance machine width increased benchmarks included figure shows ipcs averaged dct mpeg encode radar benchmarks performance idealized interconnect doubles machine width increased factor shown bar graph multi-hop networks effective extracting significant fraction idealized performance good single-hop networks wiring overhead incorporate wiring overhead single-hop networks multihop networks machine widths performance details individual benchmarks presented examined clustered vliw processors results showed inter-cluster routed multi-hop network connecting node full bypass intra-cluster network performed -wide processor grid processor architectures grid processor architectures gpas static placement dynamically issue instructions goals family architectures extract high ilp execute fast clock rate scale technology array alus short mesh startriangle figure multi-hop gpa interconnects wires center node representative array shown paths mapping compiler-generated hyperblocks array multi-hop networks ideally suited class architectures primary goal avoid global communication extract performance alu chaining mapping critical path shortest physical path previous work architecture demonstrated criticality interconnect latency section detailed analysis effects latency network configurations performance similar vliw machine model simulate perfect lookahead reservation scheme hiding router processing delay incurring fan-out fan-in forwarding delay cycles segment delay cycles simulate clock cycle examine parameters performance vliw experiments investigated connection topologies grid paper discusses representative multi-hop interconnect networks shown figure details networks investigated presented networks range low fan-out moderately high fan-out triangle network connects neighbors similar vliw interconnect moderately rich star network fan-out connects neighbors comparison looked ideal realistic worst case single-hop networks examined wiring overhead factors determine wiring area effect single-hop high-bandwidth networks routing latency column table shows average routing latency grid network averaged benchmarks realistic single-hop network -cycle routing latency achieves performance closest ideal network low fan-out mesh triangle multi-hop pointto-point networks show significantly higher routing latency star network fan-out high bandwidth all-to-all network account wiring overhead single-hop networks shown rows table perform worse star network contention contention network closely correlated fan-out topologies star network contention triangle mesh show higher contention trend expected interconnect richness increases latency due contention decreases making star network efficient number hops shown column table number hops star 
network triangle mesh networks requiring slightly networks number hops approximately node array indicating effective scheduling mesh network higher average number hops lower average latency simply network latency contention avgerage cycles hops ipc star mesh triangle table grid processor interconnect network performance instruction throughput higher contention triangle network ipc due dynamic issue capability grid processor achieves significantly higher ipc vliw machines previous section column table shows ipc averaged benchmarks interconnects fully connected single-hop network achieves ideal performance versus wiring overhead multihop networks provide roughly performance approach performance network wiring overhead wiring area overhead imposed single-hop networks performance drops significantly average network making worse multi-hop networks results show rians effective perform single-hop high bandwidth interconnects wide-issue architectures conclusions dramatic increases on-chip real-estate driven architectures scale number execution units search higher performance traditional operand transmission networks rely broadcasting scale technology constraints faster transistors slower wires addition wiring overheads broadcast networks scale poorly paper provided taxonomy inter-alu networks ians includes traditional routing networks emerging classes point-to-point operand networks key components networks execution model broadcast point-to-point connectivity single-hop multi-hop routing decisions made dynamically statically proposed dynamically routed point-to-point multi-hop network classified pmd taxomony called routed inter-alu network rian communication architecture scalable tens alus circuit analysis showed multi-hop networks scale broadcast networks suffer primarily wire delays resulting significantly larger area required wiring designed measured features router tailored fine grain rian including simple topologies lookahead routing prior data arrival mechanisms measurements show limit per-hop latency cycle technology clock cycle applied routing techniques conventional vliw architecture grid processor architecture showing operand broadcast architectures existing scheduling algorithms effective placing producers consumers close network results show rians outperform single-hop broadcast networks architectures conservative wiring overheads incorporated modeling single-hop networks results similar dally conclusions design ary interconnection networks multicomputers low-degree k-ary n-cube networks express channels performed high degree networks difficult build physical wire limitations exact constraint faced ians microprocessors lessons design multicomputer networks applicable scalar on-chip ians important difference multicomputer networks node-delay forwarding delay order magnitude higher wire delay rians delays comparable future architectures address issue operand bypass large number alus rians propose scale tens alus designed work fast clock rates key feature processor architectures enabled point-topoint routing strategy knowledge producer consumer instruction locations statically scheduled architectures naturally provide support rians dynamically scheduled superscalar processors foresee architectures adopting rians dynamic rescheduling techniques clustering agarwal keckler burger scaling microarchitectural structures future process technologies technical report department computer sciences texas austin austin february ahuja clark rogers performance impact incomplete bypassing processor pipelines proceedings annual international symposium microarchitecture pages november bhargava john improving dynamic cluster assignment clustered trace cache processors proceedings international symposium computer archictecture pages june brown stark patt select-free instruction scheduling logic proceedings annual international symposium microarchitecture pages december colwell nix donnell papworth rodman vliw architecture trace scheduling compiler ieee transactions computers august dally express cubes improving performance k-ary cube interconnection networks september farkas chow jouppi vranesic multicluster architecture reducing cycle time partitioning proceedings international symposium microarchitecture pages december fillo keckler dally carter chang gurevich lee m-machine multicomputer proceedings international symposium microarchitecture pages december gupta keckler burger technology independent area delay estimations microprocessor building blocks technical report tr- department computer sciences texas austin austin february mai horowitz future wires proceedings ieee april kessler alpha microprocessor ieee micro march lee potkonjak mangione-smith mediabench tool evaluating synthesizing multimedia communications systems proceedings annual international symposium microarchitecture pages december nagarajan sankaralingam burger keckler design space evaluation grid processor architectures proceedings annual international symposium microarchitecture pages december palacharla jouppi smith complexity effective superscalar processors proceedings annual international symposium computer architecture pages june parcerisa sahuquillo gonz alez duato efficient interconnects clustered microarchitectures proceedings international conference parallel architectures compilation techniques pages september peh dally flit reservation flow control proceedings international symposium high performance computer architecture pages january national technology roadmap semiconductors semiconductor industry association international technology roadmap semiconductors semiconductor industry association singh keckler burger routing network grid processor architecture technical report tr- texas austin april singh sankaralingam keckler burger design analysis routed inter-alu networks ilp scalability performance technical report tr- texas austin july sprangle carmean increasing processor performance implementing deeper pipelines proceedings international symposium computer archictecture pages taylor kim miller wentzlaff ghodrat greenwald hoffman johnson jae-wook lee saraf seneski shnidman strumpen frank amarasinghe agarwal raw microprocessor computational fabric software circuits general-purpose programs ieee micro march taylor lee amarasinghe agarwal scalar operand networks on-chip interconnects ilp partitioned architectures proceedings international symposium high performance computer architecture pages february tomasulo efficient algorithm exploiting multiple arithmetic units ibm journal research development january kathail schlansker rau hpl-pd architecture specification version technical report hpl- hewlettpackard laboratories february waingold taylor srikrishna sarkar lee lee kim frank finch barua babb amarsinghe agarwal baring software raw machines ieee computer september 
bottom stack addresses addresses top stack string values pointers strings argv argv null envp null envp design analysis routed inter-alu networks ilp scalability performance vincent ajay singha karthikeyan sankaralingam stephen keckler doug burger computer architecture technology laboratory department computer sciences department electrical computer engineering texas austin cart utexas utexas users cart abstract modern processors rely heavily broadcast networks bypass instruction results dependent instructions pipeline architectures wider pipelines deeper broadcasting complex slower difficult implement complexity compounded shrinking feature size communication speed decreases relative transistor switching speeds paper examines fundamentals bypass networks proposes method classifying inter-alu networks based operands routed producers consumers propose evaluate circuit architectural level fine grain point-to-point routed inter-alu network rian delivers instruction throughput full bypass network higher speeds fewer wires introduction critical loop pipelined processors enables data dependent instructions execute consecutive cycles fact alu execution delay bypass latency deliver alu output back input sets cycle time machine shown prior research increasing path single cycle dramatically reduces instruction throughput rates modern processors including superscalar vliw architectures form broadcast deliver instruction results places consumer instruction reside complexity delay bypass paths increasing modern processors technologies wider-issue machines conventional broadcasting techniques incur wire complexity growth proportional square number alus contributing increased wiring area larger fan-in bypass targets fan-out source alu increases roughly product pipeline depth width alu result routed places larger fan-out fan-in increases bypass delay capacitive load network multiplexor complexity execution model network architecture router control acronym examples point-to-point multi-hop dynamic pmd parcerisa grid processor point-to-point multi-hop static pms raw point-to-point single-hop dynamic psd m-machine multicluster point-to-point single-hop static pss degenerate case pms broadcast multi-hop dynamic bmd alpha broadcast multi-hop static bms broadcast single-hop dynamic bsd superscalar broadcast single-hop static bss vliw table taxonomy routed bypass networks sink rises finally increases wiring resistance increase transmission latencies pipeline stages alus source alu based optimistic wiring overhead models estimate shortest longest bypass path delays future ultra-wide -issue processor clock cycle cycles contrast conventional processor designs worst case bypass delay small incorporated critical path fraction clock cycle reduce delays improve scalability broadcast bypass networks propose evaluate class routed inter-alu networks rians networks neighboring alus connected direct links lightweight routers communication distant alus make multiple hops network broadcast operands routed source destination based destination identifier encoded instruction rians reduce fan-in fan-out alu potentially crippling wiring area overhead network significantly improves bypass latency nearby alus increase latency distant alus traverse hops rian general bypass inter-alu networks ians classified number target alus result delivered number targets alu directly connected routing decision made present full taxonomy ians section rian networks propose classified point-to-point multi-hop dynamically routed networks represented acronym pmd evaluate network statically scheduled architectures source destination communication determined compile time explore utility wide clustered non-clustered vliw machines targets links identified compile time routing arbitration performed run time examine network strategy emerging architecture supports static scheduling dynamic execution tolerate run-time determined latencies key applicability point-to-point routing architectures results inherently producer consumer broadcast alus show scheduling algorithms effective placing producers consumers restricting communication distance hops common case multi-hop point-to-point networks efficient patterns communication section describes design space inter-alu networks discusses relate prior bypass network architectures section examines circuit implementations multi-hop switched networks describes mechanisms reducing router overhead thin network section explores thin networks vliw architectures section dynamically executed grid processor architecture finally section summary concluding remarks taxonomy inter-alu networks bypass networks intended provide fast paths outputs alus inputs prior stages pipeline downstream register file prime effect performance reduce eliminate readafter-write hazards pipeline stalls result issuing back-to-back producer consumer instructions conventional processors typically implemented broadcast networks essentially output alu routed input alu broadcast bypass networks part broader class inter-alu networks ians classified axes execution model network architecture router control execution model output alu broadcast default alus target alus explicitly prior execution instruction routed point-to-point network architecture operand routed directly output alu input single-hop pass intermediate routers multi-hop router control routing decisions made prior execution alu instruction producing data static routing decisions place runtime dynamic note networks differ dramatically multiprocessor networks payload scalar multi-word message cache line table lists bypass network configurations architectures letter acronym network criterion pipelined superscalar architectures classified bsd networks operands broadcast target alus intermediate routers routing commence alu operation complete clustering alpha operands broadcast local remote cluster classified bmd traditional vliw processors shared register file namespace broadcast data alus statically scheduled busses bss transistors faster wires slower broadcast networks attractive due long wire lengths increasing wiring overhead large connectivity networks major challenge networks reduce latency communication level equaling approaching conventional bypass networks components required achieve network interface integrated pipeline operands delivered directly consuming alus results injected network directly alu outputs latency route network minimized architectures proposed implemented family point-to-point ians meet goals m-machine psd network destinations statically encoded instruction delivery occurs dynamically source cluster destination cluster multicluster architecture psd dynamically routes operands demand clusters partitioned register file superscalar architecture mit raw processor includes bypass routing network integrated pipeline routing overhead mitigated statically scheduled router eliminates dynamic arbitration shared router wire resources rendering pms network architecture achieved per-hop latencies single cycle experience showed latencies high achieve sufficient ilp part components communicate complete processors small alus found conventional processor core finally budding category ians pmd point-point multi-hop dynamically routed networks parcerisa proposed multi-hop routing network clustered superscalar architectures partitioned register files similar principle multicluster microarchitecture track location producsegment node single-hop network multi-hop network figure single-hop multi-hop networks size wires top left node shown ers consumers dynamically inserts instructions transmit operands source destination cluster evaluate small scale networks clusters ring mesh torus topologies paper focus flavor pmd networks instruction dependencies expressed explicitly instruction encoding physical locations producing consuming instructions prior execution knowledge bookkeeping hardware dynamically track instruction dependencies required instruction results broadcast alu restrict statically scheduled architectures dynamic behavior variable load store latencies tolerated runtime examine large networks alus explore range topologies connectivities circuit modeling inter-alu networks section describe modeling inter-alu networks explaining aspects technology models circuit estimation tools conventional bypass networks delay components scalability networks propose point-to-point networks alternative large numbers communicating nodes required communication adjacent nodes router design crucial network throughput point-to-point networks discuss routing protocol router design hides latency network finally bytes bytes system call code kread kwrite open close sbrk kioctl address toc memory kread kwrite open close sbrk kioctl memory unique number compare performance 
multi-hop networks single-hop networks experiments section examine generic class switched multi-hop networks implemented pmd pms bmd single hop networks implemented psd pss bsd bss programming execution models determine communication point-to-point broadcast router control static dynamic address tradeoffs types networks technology modeling estimated circuit latencies spice models derived international technology roadmap semiconductors estimated wire delays assuming optimal buffer placement capacitance numbers obtained space dimensional field solver technology parameters wire delay tool based international technology roadmap semiconductors technology point scaled refer wire delay obtained represented picoseconds analysis assume functional units producing consuming values laid wire delay node area square microns network size nodes fanin fanout delay table network delay components fanin treebuffered wires destination fanout buffers source node node node figure circuit bypass path dimensional rectangular array manhattan routing scheme refer functional units nodes distances measured segments segment distance adjacent nodes network size total number nodes network figure shows single-hop multi-hop inter-alu network size experiments node consists alu integer multiplier fpu -entry register file functional units bits wide area dimensions nodes estimated empirical area model node square side occupying area half channel length minimum sized transistor processing core alpha comparison occupies area approximately table shows wire delay node area obtained circuit tools technology modeling conventional bypass networks conventional broadcast networks fall bsd class networks general communication path bypass schemes shown figure network topology decisions routing protocol decisions reflected abstract model shown figure main components contribute bypass delay fan-out delay wire delay fan-in delay total delay equation term equation denotes wire delay product number segments traversed wire delay unit length length segment wiring distance overhead wiring distance overhead factor incorporate physical vlsi design constraints wire routing number tracks required route wires fits area occupied alus indicating wiring overhead wires require extra area routing ratio length wires increase excess area routed wiring overhead strongly delay variation delay variation distance network size distance segments fan-out fan-in wide wide network size wire delay segments segments segments segments segments figure percentage wire delay contribution total communication delay dependent technology alu dimensions data-path width routing strategies repeater placement area -bit data-path assuming wire pitch simple wiring area models account repeater area overhead show single-hop networks size greater incur wiring overhead network size multi-hop network configurations examined low fanouts incur wiring overhead layout circuit single-hop network shown figure outbound wires originating top left node shown fanin fanout delays correspond delays destination multiplexors fanout buffers source delays obtained spice simulations network sizes values obtained shown table delay analysis single-hop network large fan-out fan-in delays incurred communication figure shows percentage contribution components total communication latency networks varying network sizes distances traversed -wide configurations communication latency evenly shared wire delay fan-out fan-in delay communication short distances delay due fanin fanout communication adjacent nodes segment reducing fanin fanout contribution significant benefits short distance communications hand wire delay dominates long distance communications figure shows percentage contribution wire delay communicating distances networks size note increase network size total number nodes network communication distance fanin fanout delay increases logarithmically wire delay remains constant percentage contribution wiredelay drops graph -segment path wire delay contribution drops network size extreme -segment path longest path network wire delay contribution drops marginally architectures incur frequent long-distance communications nodes stick conventional single-hop networks ease design performance recall figure plotted percentage contribution fanin fanout communication disdestination routersource node destination node routerintermediate hopsfanin treerouter buffered wiresfanout buffers router source node router intermediate figure circuit multi-hop network tance varied showed results network sizes serves motivation mutli-hop networks suited architectures exhibit frequent short distance communication multi-hop inter-alu network multi-hop networks defined networks require routing decisions made source destination nodes parts delay multi-hop configuration shown figure outgoing router delay fan-out delay wire delay fan-in delay intermediate router delay wire delay incurred identical wire delay single-hop networks fan-in fanout delays dependent richness interconnect multi-hop network routes data multiple nodes causing router delay node data pass destination node number hops figure shows topology multi-hop network total delay equation multi-hop networks typically small number connections neighbors relying multi-hop routing non-local connections suited architectures communication predominantly nearby alus crucial optimize routers communication hop requires routing operation router design overcome challenges posed technology scaling large single hop networks multi-hop networks attractive alternative fine grain control low wiring overhead fan-out wire delays fan-in delays inherently serial removed critical path router delay end communication path incurred arbitration performed avoid resource hazards propose lookahead scheme hide arbitration delay order networks implemented control payload actual data operand control arrives advance payload reserves path payload taking routing decision making logic critical path path due contention node buffer slot reserved incoming operand information cycle advance payload destination encoded instruction processed time produce operand advance knowledge circuit techniques domino logic increase speed operand transmission peh describe similar latency-hiding approach inter-chip networks control data packets flit stalled fwd flit control flit pre select operand packets data stalled fwd data alu incoming control data flit control flit computation node router logic data switch packet processing forwarding delay decoder encoder operands control switch incoming data figure router schematic advance knowledge incoming operands efficient flow control mechanism knowledge open buffer slots communication latency producer nodes simple throttling mechanism implemented assuming cycle required send operand node throttling signal asserted control flit arrives buffer slots free data arrival router requires slot operand arriving cycle payload control signal operand throttle signal traveling back producing node producing node receives throttle signal cease transmit data stall signal deasserted common case producing node send operand receive acknowledgment guaranteed storage space consumer node contention routing path uncommon case backward information required figure shows schematic router key component router decoder encoder control packets steers control switch forwarding neighboring nodes sets alu datapath ready receive meant node router cycle stalled control data packets written separate buffers cycles full nodes upstream throttled packets forwarded small forwarding delay incurred larger packet processing delay paid packets created alu router circuit modeled circuit tools determine delays packet processing delay forwarding delay circuits technology router multi-hop delay equation transformed delay analysis implications delays types networks represented delay equations built core circuit components equating delays define cross-over point 
number hops multi-hop network outperforms single-hop network shown equation assume router delay wide network wide network distance segments delay nanoseconds single-hop overhead single-hop multi-hop number hops delay nanoseconds single-hop multi-hop figure wire delays nanoseconds full bypass point point networks network size alu nodes network size alu nodes overheads estimated wiring overhead model wiring overhead crossover point figure crossover point fully hidden figure equation crossover occurs fan-in fan-out overhead accumulated hops multi-hop network exceeds single fan-in fan-out delay full broadcast single-hop network subscripts denote single-hop multi-hop networks figure shows variation wire delays number hops multi-hop single-hop networks node array assuming extra wiring overhead multi-hop network latency communicating segment distances node network shifts circuit models calculated delays segments -segment trip multi-hop network slower full-bypass network faster -segment trip faster -segment trip crucial common short paths fast general latencies common paths important multi-hop network favored communication orchestrated neighboring nodes overhead account -wide single-hop network outperforms multi-hop network dotted line higher slope larger intercept solid line figure plots equation single-hop network sizes fan-out fan-in wire delays obtained circuit models sensitivity wiring overhead significant asymptotic nature curve graph important wiring area estimates conservative account repeater placement area graph wiring overhead broadcast networks resulted crossover point reasonable large insertion repeaters results wiring overhead exceeding multi-hop networks outperform singlehop broadcast networks interconnect physical design issues crucial significant impact interconnect architecture vliw architectures vliw architectures routing arbitration run time delay alu bypass network critical loop illustrated previous section choice made network architecture depending number hops traveled common case examine design space bypass networks context unclustered vliw architectures vliw machines location producer consumer instructions compile time multi-hop network route packets source nodes destination nodes provided close machine multi-hop network sustain higher instruction throughput single-hop broadcast network execution data dependent operations consecutive cycles critical section describe machine model configurations studied describe benchmark suite compilation tools examine impact future designs examined wide issue machines width examined machine widths determine applicability multi-hop networks current designs machine model model vliw machine instructions statically assigned named functional units nodes compiler generates schedule execution order long instruction words individual instructions instruction word allowed execute order independent examine xxx xxx single hop networks sworst real ideal multi hop networks figure vliw interconnects multi-hop networks single networks bypassing values single-hop network bypass values directly source node destination node dedicated paths multi-hop network values dynamically routed network source destination examine family single-hop full bypass networks ideal network set wire delays shortest delay path realistic case single-hop network scale wire delays nodes linearly distance worst case single-hop network set wire delays longest delay path conventional bypass networks resemble compare single-hop networks multi-hop networks short paths network wires adjacent nodes medium distance paths network wires nearest neighbors diagrams connectivity shown figure simulated multi-hop networks infinite bandwidth infinite wires ports connected nodes study impact contention bound sensitivity wiring overhead simulated single-hop networks simulated -wide -wide -wide machine configurations fanin fanout contribution accounted networks determining total delays delays simulations derived circuit models equations nodes model closer nodes distance model incur total delay fan-in traverse node farther nodes cart ut-cs universal mechanisms data-parallel architectures karu sankaralingam stephen keckler william mark doug burger computer architecture technology laboratory department computer sciences texas austin cart ut-cs data-parallel systems cart ut-cs programmable architecture cart ut-cs conventional data-parallel architectures vector simd mimd architectures specialized narrowly focused hardware mpeg decoding specialized error correction code units convolution encoding dsps cart ut-cs architecture trends programmability programmable dlp hardware model emerging incur sony delay emotionengine wire distance conventional specialized vector units real-time graphics processors specialized pixel processor vertex processor sony handheld engine dsp core graphics core arm core specialized types processors designed application mix match composition integration costs increase level programmability encompass diversity cart ut-cs unified systematic approach dlp basic characteristics demands dlp applications instruction fetch control memory computation design set complementary universal mechanisms dynamically adapt single architecture based application demands applied diverse architectures cart ut-cs outline application study application behavior benchmark suite benchmark attributes microarchitecture mechanisms instruction fetch control memory system execution core results conclusions cart ut-cs application behavior dlp program model loops executing parts memory parallel dct image identical computation globally synchronous computation block dct cart ut-cs application behavior dlp program model loops executing parts memory parallel vertex skinning data dependent branching vertex ntrans product xyz characterize applications parts architecture affect cart ut-cs program attributes control read record write record instructions sequential read record write record instructions static loop bounds vector simd control single vadd vector simd control branching required dct read record write record instructions data dependent branching mimd control masking required simd architectures skinning cart ut-cs program attributes memory regular memory memory accessed structured regular fashion reading image pixels dct compression irregular memory accesses memory accessed random access fashion texture accesses graphics processing scalar constants run time constants typically saved registers convolution filter constants dsp kernels indexed constants small lookup tables bit swizzling encryption cart ut-cs program attributes computation instruction level parallelism kernel alus iteration kernel low ilp high ilp cart ut-cs benchmark suite vertex-simple vertex-reflection vertex-skinning fragment-simple fragment-reflection anisotropic-filtering real-time graphics processing rijndael blowfishnetwork processing security fft luscientific computing convert dct high pass filtermultimedia processing benchmarksdomain cart ut-cs benchmark attributes lar irr ula ala sta ind tan ati ria ble ilp memory control computation cart ut-cs baseline trips processor moves bank i-cache i-cache i-cache i-cache gcu cache banks spdi static placement dynamic issue alu chaining short wires wire-delay constraints exposed architectural level block atomic execution d-cache d-cache d-cache d-cache cart ut-cs high level architecture register file cart ut-cs register file dlp attributes mechanisms regular memory accesses software managed cache cart ut-cs register file dlp attributes mechanisms regular memory accesses scalar named constants cart ut-cs register file dlp attributes mechanisms indexed constants software managed data store alus tight loops instruction revitalization instruction revitalization data dependent branching local program counter control alu regular memory accesses scalar named constants cart ut-cs i-fetch control mechanisms moves bank i-cache i-cache i-cache i-cache gcu d-cache d-cache d-cache d-cache instruction revitalization support tight loops dynamically create loop engine power savings i-caches accessed map execute revitalize cart ut-cs i-fetch control mechanisms moves bank i-cache i-cache i-cache i-cache gcu d-cache d-cache d-cache d-cache local pcs data dependent branching reservation stations i-caches cart ut-cs results baseline machine trips processor mesh interconnect technology clock rate kernels hand-coded custom schedulers dlp mechanisms combined produce configurations software managed cache instruction revitalization software managed cache instruction revitalization operand reuse s-o software managed cache local pcs lookup table support m-d performance comparison specialized hardware cart ut-cs evaluation mechanisms fft ert dct hig sfi lte fra t-r efl tio fra t-s ple lig rte x-r efl tio rte x-s ple lig blo ish rijn rte x-s kin nin s-o m-d s-o m-d instruction revitalization instruction revitalization operand reuse local pcs lookup tables baseline trips processor cart ut-cs comparison specialized hardware mpc imagine tarantula cryptomaniac quadrofx quadrofx specialized trips speedup scaled speedup specialized graphics encryption scientific multimedia cart ut-cs conclusions key dlp program attributes identified memory control computation complementary universal mechanisms competitive performance compared specialized processors mechanisms enable merging multiple markets single design easier customization single market cart ut-cs questions cart ut-cs future directions future directions universal design complexity generality tradeoff applications dlp space loop intensive phases regular array accesses cart ut-cs heterogeneous architecture dsp encryption vector graphics configurable chip cart ut-cs heterogeneous architecture dsp encryption vector graphics configurable core configurable core configurable core configurable core cart ut-cs trips baseline performance benchmark ipc benchmark ipc convert fragment-reflection dct fragment-simple highpassfilter vertex-reflection fft vertex-simple vertex-skinning blowfish rijndael cart ut-cs benchmark attributes computation static instructions loop body ilp memory diverse mix types accesses regular accesses irregular accesses constants indexed constants control sequential benchmarks static loop bounds benchmarks data dependent branching benchmarks cart ut-cs memory system mechanisms moves bank cache banks i-cache i-cache i-cache bypass networks simulated equal order understand effect wiring overhead additionally simulations run infinite bandwidth contention links order percentage latency due contention simulations assumed processor executing clock making router forwarding delay cycles time simply traverse node length wire cycles noted real machine support arbitrary delays circuits synchronized alu clock assumed routers clocked alu clock quad-pumped rounded delays nearest quarter cycle benchmarks evaluate performance networks realistic workloads selected set benchmarks spec cint spec cfp mediabench benchmarks gzip mcf parser ammp art equake dct adpcm mpeg encode examined in-house benchmark performs radar signalprocessing computation predominantly -point complex fir filter trimaran tool set targets hpl play-doh isa compile benchmarks custom built scheduler latency cycles contention config hops -wide vliw -wide vliw -wide vliw table interconnect network performance vliw architectures latencies shown processor cycles clock cycle single-hop networks contention pair alus connected dedicated wire hops aware delay paths architecture optimizes local critical path custom eventdriven simulator model micro-architecture performance simulator models aggressive lookahead resource reservation scheme implemented router assume data packet catches control packet contention transmitting control packets pay constant router forwarding delay hop multi-hop network incur full packet processing delay benchmarks forwarded hundred million instructions simulated hundred million instructions results routing latency routing latency number cycles operand production receipt destination source destination nodes assume direct bypass execution cycle routing latency assumption makes average latency shorter fastest transmission path network routing latency machine configurations shown columns table width routed multi-hop network worse networks network size larger machine widths routed multi-hop network routing latencies versus versus network network incorporate wiring overhead single-hop networks networks good machine widths contention measure percentage contribution delay due contention measuring percentage difference routing latency real multi-hop network idealized multi-hop network infinite ports wires connected nodes idealized network delays incurred due average ipc sideal sreal sworst sreal alpha sworst alpha figure ipc averaged high ipc benchmarks dct mpeg encode radar vliw machines resource hazards interconnect network percentage latency due contention shown column single-hop networks contention dedicated path pair alus networks show roughly amount contention higher bandwidth network showing slightly contention expected contention accounts roughly latency -wide machine -wide machine suggesting higher bandwidth multi-hop networks improve performance number hops number hops route operands source destination effectiveness scheduler placing producer-consumer pairs close conventional singlehop networks number hops dedicated wire node node shown column table average number hops networks low compared machine width showing scheduler effective placing producer-consumer pairs close ipc figure shows ipcs averaged high performance benchmarks -wide -wide -wide configurations benchmarks suite exhibit improvement performance machine width increased benchmarks included figure shows ipcs averaged dct mpeg encode radar benchmarks performance idealized interconnect doubles machine width increased factor shown bar graph multi-hop networks effective extracting significant fraction idealized performance good single-hop networks wiring overhead incorporate wiring overhead single-hop networks multi-hop networks machine widths examined clustered vliw processors results showed inter-cluster routed multi-hop network connecting node full bypass intra-cluster network performed -wide processor figure shows performance individual benchmarks -wide machines low ipc benchmarks show 
variation performance machine width interconnect network varied figure shows performance benchmarks wiring overhead accounted assuming equal configurations multi-hop networks outperform single-hop configurations grid processor architectures grid processor architectures gpas static placement dynamically issue instructions goal architecture extract high ilp execute fast clock rate scale technology array width adpcm ammp art dct equakegzip mcf mpeg encodeparser radar average ipc sideal sreal sworst width adpcm ammp art dct equakegzip mcf mpeg encodeparser radar average ipc sideal sreal sworst width adpcm ammp art dct equakegzip mcf mpeg encodeparser radar average ipc sideal sreal sworst figure ipc vliw machines wiring overhead single-hop multi-hop networks width adpcm ammp art dct equakegzip mcf mpeg encodeparser radar average ipc sideal sreal sworst width adpcm ammp art dct equakegzip mcf mpeg encodeparser radar average ipc sideal sreal sworst width adpcm ammp art dct equakegzip mcf mpeg encodeparser radar average ipc sideal sreal sworst figure ipc vliw machine wiring overhead assumed single-hop multi-hop networks mesh startriangle figure grid interconnects alus short paths mapping compiler-generated hyperblocks array multi-hop networks ideally suited class architectures primary goal avoid global communication extract performance alu chaining mapping critical path shortest physical path previous work demonstrated criticality interconnect latency gpas section detailed analysis effect latency network configurations performance similar vliw machine model simulate perfect lookahead reservation scheme hiding router processing delay incurring fanout fanin forwarding delay cycles segment delay cycles simulate clock cycle results discussion examine parameters performance vliw experiments examined multitude connection topologies grid shown figure networks range low fanout moderately high fanout triangle network connects neighbors similar vliw interconnect moderately rich star network fanout connects neighbors comparison looked ideal realistic worst case single-hop networks similar vliw machines scaled alu array examined wiring overhead factors determine wiring area effect single-hop high-bandwidth networks assumed configurations wires connecting bottom grid top express channel configurations denoted suffix tables graphs wire laid higher level metal times faster grid express channels total delay cycle routing latency examining table average routing latency grid network ideal case lowest connectivities latency highest triangle configurations pretty close worst case realistic single-hop network wiring overhead closest ideal star network mesh network triangle network order noted express channels make difference average latency numbers multi-hop networks account wiring overhead single-hop networks average latencies cases cycles star network performs wiring overhead incorporated contention amount contention closely related fan-out topologies star network contention exhibiting contention topologies express channels mesh network contention express channels triangle network contention express channels interconnect latency cycles cont delay hops mesh meshe star stare triangle trianglee table communication latencies number hops contention percentages interconnects grid processor area overhead factor equal expected interconnect richness increases latency due contention decreases making star network efficient number hops table number hops single-hop networks dedicated path node node number hops multi-hop topologies varies topology lowest star network hops highest mesh network hops triangle network averaged hops multi-hop networks exhibited close average number hops express channel cases fact mesh network higher average number hops lower average latency simply due higher contention triangle network ipc topologies averaged wiring overhead mesh star triangle topologies averaged networks express channels networks express channels ipcs benchmarks presented figures split low high ipc benchmarks equal figures present simulations results wiring overhead incorporated multi-hop networks single-hop networks expected topology performs topology performs worst wiring overhead topology star network wiring overhead included star networks outperform network mesh i-cache d-cache d-cache d-cache d-cache software managed cache software managed cache programmable dma engines bank exposed compiler programmer high bandwidth streaming channels lmw load muti-word store buffers cached -memory cart ut-cs i-fetch control mechanisms moves bank i-cache i-cache i-cache i-cache d-cache d-cache d-cache d-cache local pcs data dependent branching cart ut-cs execution core mechanisms moves bank i-cache i-cache i-cache i-cache d-cache d-cache d-cache d-cache data storage provide lookup tables operand revitalization scalar constants cart ut-cs data parallel architecture execution substrate functional units efficient inter-alu communication shuffle data technology scalability register file cart ut-cs comparison specialized hardware mpc imagine tarantula cryptomaniac quadrofx fragment quadrofx vertex relative performance relative issue-width graphics encryption scientific multimedia cart ut-cs future directions application mechanisms dynamic tuning selection mechanisms multiple classes applications supported flexibility simplicity trade-off applications supported dlp behavior exploited traditional dlp domains evaluation mechanisms cycle time power area metrics comparison dlp mechanisms heterogeneous architecture cart ut-cs register file dlp attributes mechanisms regular memory accesses software managed cache scalar named constants operand reuse operand reuse indexed constants software managed data store alus tight loops instruction revitalization instruction revitalization data dependent branching local program counter control alu cart ut-cs comparison specialized hardware mpc imagine tarantula cryptomaniac quadrofx fragment quadrofx vertex graphics encryption scientific multimedia clock adjusted relative performance cart ut-cs comparison specialized hardware mpc imagine tarantula cryptomaniac quadrofx fragment quadrofx vertex 
graphics encryption scientific multimedia clock adjusted relative performance relative issue width cart ut-cs data-parallel applications architectures performance gflops arithmetic processor earth simulator gops required software radios gflops gpu programmable units single chip spread diverse domains high performance computing hpc digital signal processing dsp real-time graphics encryption conventional architecture models vector simd mimd specialized narrowly focused hardware mpeg decoding dsps specialized error correction code units convolution encoding cart ut-cs conclusion future triangle close summary table shows normalized average ipc achieved interconnects ipcs normalized respect network full broadcast single-hop network achieves ideal performance network directly corresponds broadcast network dynamically scheduled superscalar processors star express channels multi-hop routed network order magnitude bandwidth broadcast network achieves ideal performance wiring overhead account network achieves ideal network achieving wiring overhead radar art dct equake mpeg encode ipc sideal sreal sworst mesh meshe star stare triangle trianglee wiring overhead assumed single-hop multi-hop networks radar art dct equake mpeg encode ipc sideal sreal sworst mesh meshe star stare triangle trianglee figure ipc grid processors grid high ipc benchmarks wiring overhead adpcm ammp gzip mcf parser average ipc sideal sreal sworst mesh meshe star stare triangle trianglee wiring overhead assumed single-hop multi-hop networks adpcm ammp gzip mcf parser average ipc sideal sreal sworst mesh meshe star stare triangle trianglee figure ipc grid processors grid low ipc benchmarks configuration efficiency mesh meshe star stare triangle trianglee table normalized average ipcs interconnects ipcs normalized ideal network star network worst performing multi-hop network triangle express channels interconnect performed moderately low ipc benchmarks parallelism high triangle interconnect bottleneck multi-hop network constrained low bandwidth deliver operands produced time performance trends multi-hop inter-alu networks optimized router design extremely effective performing full broadcast networks bandwidth richness interconnect reduced low bandwidth bottleneck programs lots parallelism efficiently executed conclusion dramatic increases on-chip real-estate driven architectures scale number execution units search higher performance traditional operand transmission networks rely broadcasting scale technology constraints faster transistors slower wires addition wiring overheads broadcast networks scale poorly paper provided taxonomy inter-alu networks ians includes traditional routing networks emerging classes point-to-point operand networks key components networks execution model broadcast directions point-to-point key dlp connectivity program single-hop attributes multi-hop identified memory routing instruction decisions control computation made core dynamically proposed statically complementary universal proposed mechanisms dynamically single routed architecture point-to-point adapt multi-hop network application called resembling routed vector inter-alu simd network mimd rian machine competitive communication performance architecture compared scalable specialized processors alus future circuit directions analysis universal showed design complexity multi-hop generality networks scale tradeoff applications dlp broadcast space networks loop suffer intensive primarily phases regular wire array delays resulting accesses significantly cart larger area ut-cs required data-parallel implement applications broadcast networks architectures similarities high designed measured features router tailored fine grain rian including simple topologies lookahead routing prior data arrival mechanisms measurements show limit per-hop latency technology applied routing techniques conventional vliw architecture dynamic grid architecture showed operand broadcast existing scheduling algorithms effective placing producers consumers close network result results show equivalent performance richer expensive broadcast network impose area communication delay penalty wires required implement broadcast network rian significantly outperform broadcast network key feature processor architectures enabled routing strategy knowledge source destination instruction locations optimization prior instruction execution static compile-time scheduler place instructions minimizing communication distance similar analysis performed performance runtime arithmetic trace processor generation gflops dynamic software compilation radios techniques gops feasibility gpu gflops future work programmable units demonstrate chip lots time concurrency required differences generate diverse good domains schedule types concurrency bottleneck architecture models vector simd mimd vikas specialized agarwal narrowly stephen focused hardware keckler mpeg doug decoding burger specialized scaling error correction microarchitectural code units structures convolution future encoding process dsps technologies 
technical report department computer sciences texas austin austin february 
ahuja clark rogers performance impact incomplete bypassing processor pipelines proceedings annual international symposium microarchitecture pages november mary brown jared stark yale patt select-free instruction scheduling logic proceedings annual international symposium microarchitecture pages december robert colwell robert nix john donnell david papworth paul rodman vliw architecture trace scheduling compiler ieee transactions computers august keith farkas paul chow norman jouppi zvonko vranesic multicluster architecture reducing cycle time partitioning proceedings international symposium microarchitecture pages december marco fillo stephen keckler william dally nicholas carter andrew chang yevgeny gurevich whay lee m-machine multicomputer proceedings international symposium microarchitecture pages december gupta stephen keckler doug burger technology independent area delay estimations microprocessor building blocks technical report tr- department computer sciences texas austin austin february richard kessler alpha microprocessor ieee micro march chunho lee miodrag potkonjak william mangione-smith mediabench tool evaluating synthesizing multimedia communications systems proceedings annual international symposium microarchitecture pages december ramadass nagarajan karthikeyan sankaralingam doug burger stephen keckler design space evaluation grid processor architectures proceedings annual international symposium microarchitecture pages december palacharla jouppi smith complexity effective superscalar processors proceedings annual international symposium computer architecture pages june joan manuel parcerisa julio sahuquillo antono gonz alez jos duato efficient interconnects clustered microarchitectures proceedings international conference parallel architectures compilation techniques pages september shiuan peh william dally flit reservation flow control proceedings international symposium high performance computer architecture pages january national technology roadmap semiconductors semiconductor industry association international technology roadmap semiconductors semiconductor industry association eric sprangle doug carmean increasing processor performance implementing deeper pipelines proceedings international symposium computer archictecture pages michael bedford taylor jason kim jason miller david wentzlaff fae ghodrat ben greenwald henry hoffman paul johnson walter lee jae-wook lee albert arvind saraf mark seneski nathan shnidman volker strumpen matt frank saman amarasinghe anant agarwal raw microprocessor computational fabric software circuits general-purpose programs ieee micro march tomasulo efficient algorithm exploiting multiple arithmetic units ibm journal research development january kathail schlansker rau hpl-pd architecture specification version technical report hpl- hewlett-packard laboratories february elliot waingold michael taylor devabhaktuni srikrishna vivek sarkar walter lee victor lee jang kim matthew frank peter finch rajeev barua jonathan babb saman amarsinghe anant agarwal baring software raw machines ieee computer september 

appears proceedings annual ieee acm international symposium microarchitecture dataflow predication aaron smith ramadass nagarajan karthikeyan sankaralingam robert mcdonald doug burger stephen keckler kathryn mckinley department computer sciences texas austin asmith ramdas karu robertmc dburger skeckler mckinley utexas abstract predication facilitates high-bandwidth fetch large static scheduling regions typically complex out-of-ordermicroarchitectures paper describes dataflow predication per-instruction predication dataflow isa low predication computation overheads similar vliw isas low complexity out-of-order issue twobit field instruction specifies instruction predicated case arrivingpredicate tokendetermines instruction execute dataflow features thatreducepredication overheads dataflow predicate computation permits computation compound predicates virtually overhead instructions early mispredication termination squashes in-flight instructions false predicates time eliminating overhead falsely predicated paths finally implicit predication mitigates fanout overhead dataflow predicates reducing number explicitly predicated instructions predicating heads dependence chains dataflow predication exposes compiler optimizations disjoint instruction merging path-sensitive predicate removal increased performance predicated code out-of-order design introduction predication linearizes instruction flows converting control dependences data dependences improving control flow predictability instruction fetch bandwidth size instruction scheduling window compiler vliw vector machines successfully applied predication obtain improvements advantages predicated execution achieved widespread out-of-order architectures complexitiesof mergingpredicationwith dynamicscheduling register renaming outweighed perceived benefits isas supporting dynamically scheduled implementations alpha sparc provide limited support predication conditional moves stores due scaling limits conventional superscalar designs researchers recently begun investigating architectures combine dataflow-like behavior conventional programming models historical dataflow machines typically implemented limited partial predication gate switch operators subsequent advances predication primarily vliw architectures present opportunity reduce predication overheads dataflow-like machines paper describes dataflow predication lightweight isa support predicating instruction dataflow-like architecture dataflow predication instruction producing produce predicate two-bit field instruction specifies instruction receive matching predicate tokentoissue isa support dataflow-like isas exploit benefits afforded predication dynamic out-of-order issue lower complexity superscalar architectures dataflow predication incorporates features enable low predication overhead sinceanyinstruction receive predicate compiler gate data operandsfor instructionexplicitly unlikeprior dataflow architectures simply predicate consumer eliminating gate switch operators per-instructiondataflow predicationreduces dependencepath height permits low-overhead predicate inversions conjunctions disjunctions bipolarized predicates predicated test instructions routing multiple predicates single instruction implicit predication isa supports predication instruction compiler predicate instructions predicated basic block explicitly compiler predicate head thusimplicitlypredicating successors early mispredication termination dataflow predication supportssquashingof instructionson false predicate path time capability prevents dependence height falsely predicated paths reducing performance support implicit predication features dataflow predication lend compiler optimizations reduce predicate overheads improve performance predicate fanout removal major source overhead dataflow predication fanning predicates potentially consumers dataflow-like architectures communication requires building software fanout tree distribute predicate consumers reduce overhead compiler apply speculative hoisting implicit predication predicating tails heads dependence chains techniques reduce number instructions predicates communicated path-sensitive predicate removal inter-block liveness analysis compiler remove predicate instruction result unused path complementaryto predicate disjoint instruction merging compiler merge identical instructions disjoint control flow paths eliminating redundant instructions exposing opportunities optimization trips architecture implements dataflow predication instantiation edge explicit data graph execution architecture edge architecture itsupportsblock-atomic execution statically defined blocks instructions commit atomically block instructions commit block edge isa implements dataflow communication instructions dependences explicitly encoded icates dataflow arcs rely centralized predicate register file addition isa microarchitectural support implementation dataflow predication trips prototype paper describes compiler algorithms optimizations support efficient predication insertingpredicates merging multiple basic blocks hyperblocks itappliesscalar predicate optimizations produce compact efficient hyperblocks simulation results show dataflow predicate optimizations improve performance aggressively predicated baseline dataflow predicationmakespossible clean synergybetween predication out-of-order execution lower previous dataflow architectures lower hardware complexity proposed predicated out-of-order superscalar designs prior predicate optimizations crayimplemented predication form vector masks guard individual vector operations predicated execution prevalent vliw machines inthe sand ported partial predication select instruction cydra iaintel itanium processors isas include predicate operand instruction risc architectures support predicated execution in-order arm processor predicates instructions architectures limit predication conditional move instructions predication research generally fallen categories isa microarchitecturesupport efficient execution compiler algorithms optimizations exploit predication allen ifconversion convert control dependences data dependences mahlke proposed hyperblocks effective compiler structure performing predication exposing scheduling regions compiler enabling software pipelining loops control structures solutions proposed alleviate overheads predication vliw architectures limited extent dynamic superscalar architectures fetch execution overhead previous processors issue predicated instruction execution resultinginwastedfetch canotherwisebeutilized instructions addition instruction predicateis computed invliwmachines instructionsthat execute consume waste issue slots potentially elongatingthe schedule augustet proposea framework predication alleviate problems out-oforder processors researchers proposed predicate prediction predicts resolution predicate dispatch logic branches enable hardware dynamically selectively employ predicated execution predicate slip delays guard predicate commit register renaming predication complicates task dynamicregister renamingin out-of-orderprocessordue multiple potential definitions if-converted control paths definition lack thereof detected guarding predicate resolved researchers proposed solutions problem including predicate prediction operators conditional select -op selectop solutions renaming remains prohibitively complex predicate registers conventional architectures typically save results predicate-defining instructions general purpose register space private predicate namespace addition data source operands predicated instruction predicate operand iathe predicate operand consumes bits instruction due encoding pressure architectures predication small number instructions arm notable exception alpha sparc architectures offer single conditional move operation simple control constructs extend predication instructions pnevmatikatos propose guard instruction instruction conjunctionwith predicateregisterfile set successor instructions predicate computation generate predicates instructions inside complex control structures compiler invert merge predicates generated ifconverted branch long predicate computation chain addition increasing instruction overhead end program critical path researchers addressed problem ways generating complementary predicates wired operators program decision logic minimization conventional architectures vliw architectures benefit low-overhead predication lose performance falsely predicated instructions lengthen critical path execution superscalar processors benefited predication due complexity implementation out-of-order microarchitecture conventional architectures historical dataflow machines combined partial predication dyif add t-gate f-gate add switch figure conversion con structusing gateandf gate aswitch instruction namic scheduling dataflow predication conversion control dependences data dependences essential dataflow execution static dataflow thet-gate f-gate operator copies input output control input operand carries true false produce output mit-tagged token dataflow machine combined functions operatorsusingthe switch operator whichconditionallysteers input operand destinations based control input destination instruction receive input execute figure illustrates howthecompilerconvertsan if-thenelse construct dataflow gate switch operators ifiis equaltoj target add instruction shown figure conversely equal f-gate absorbs produces output instruction execute figure shows compiler transforms code switch instruction equals switch instruction steers data input add instruction instruction add instruction executes join operator depicted 
figure figure logical placeholder dataflowgraph indicatinga single producerforthe variable represent instruction inhibiting delivery input operands compiler guards execution instructions if-then-else statements whenappliednaively limit parallelism dataflow machines gating input conditional statement results significant overhead addition gate operators serialize execution succeeding instructions figure add instructions execute preceding gate switch operators limiting parallelism prior research dataflow eliminated overheads beck propose techniques eliminate unnecessary switch instructions researchers applied full range optimizations developed vliw dataflow architectures dataflow predication instantiated trips architecture differs previous partially predicated dataflow architectures major ways predicates directly guard individual instructions avoiding gate switch instructions instruction generate predicate targeting predicate operand instruction instructions receivemultiple predicate operandsbeforefiring features enable dense encoding -bit instruction requires bits predicated enable efficient compound predicate computation dataflow predication supports disjunction arbitrary number predicates predicateproducing instructions predicated finally supportimplicit predication input instructionsto dataflowgraphneedto bepredicatedto implicitly predicate entire graph trips isa support predication edge architecture distinguishing features whichstatically defined blocks instructions commit atomically instructions block complete commit block supports dataflow execution direct-instruction communication isa explicitly encodes dependences trips edge prototype block completes producesa consistent set outputs execution block write registers execute number stores generate branch target trips hardware counts outputs signals block instructionscommunicate blocks registers memory block instructions direct instruction communication includes predicates instruction identifiers instructions depend result operands instruction dataflow execution model dictates instruction executes operands parent instructions instruction completes execution sends result directly instruction targets block figure shows simple codefragmentof anif-then-elseanda diagramrepresenting if-converted dataflow graph right-most field instruction target identifier specifies consumer instruction result trips isa instructions block instruction produces arithmetic operations comparisons target eachofwhichuses -bittargetencoding sevenbits ofthe targetidentifyone instructiontargetswithin block remaining bits result left predicate operandof target instruction predication rules trips isa follow number rules produce well-formed predicated blocks instruction specific data movement constant generation instructions predicated two-bit predicatefield instructionis predicatedand polarity arriving predicate instruction executed predicatedinstruction fire execute receive data operands matching predicateoperand polarityofthe waiting instruction forexample instructionwaitingfora false predicatewill onlyfire false predicate arrives multiple instructions target predicate operand instruction deliver matching predicate capability permits aggressive instruction merging deliver predicate multiple predicated instructions tree series multi-targetmove instructions predicated dataflow graphs preserve exceptionbehaviorofanunpredicatedprogram meaning block boundaries predicate encoding dataflow predication ability compute exploitingearly mispredication detection implicit predication instantiatedintrips require two-bit field instruction fanout instructions routing predicates predicate consumers previous architectures significant overheads predication partially predicated dataflow isas added extra split merge instructions vliw architectures required larger per-instruction fields bits predicate registers figure illustrates predicate generation test instruction teq receivesiandj computesa predicate teq x-op predicate predicate addi x-op left addi x-op left slli x-op target teq addi slli addi figure predication trips isa sends addi instructions note addi instructions predicated opposite polarities black circle predication true white circle predication false addi instructions receive predicate instruction matching predicate fires delivers result subsequent shift slli instruction addi instruction fires shift receive token representing updated similar dataflow graph figure reduces predicate overheadcomparedto partialpredicationby eliminatingthe gate switch instructions figure shows encodings instructions instruction fields include opcode bits predicate bits extended opcode bits target bits target bits predicate field specifies whethertheinstructionis predicatedona truepredicate predicated false predicate unpredicated unpredicated teq instruction field produces true predicate low-order bit routed consumers equal teq targets correspond predicate operands addi instructions addi instruction target target field needed encode dataflow predicate computation partially predicated dataflow machines incurred overheads computation compound predicates recent vliw architectures provided special operations wired-and wired-or instructions permitted restricted efficient compound predicate computation dataflow predicationcombines per-instructionpredication bipolar predicates implicit predication implement efficient combiningof predicates implicit inversion operators combination solution compound predicate computation equally efficient general vliw solutions dataflow context predicate ands figure shows code simple loop dataflow graph statically unrolled times single trips block iteration consists load add test instruction unrolled iteration executes previous unrolled conditions evaluate true compiler avoids generatingcompound predicates predicating test iteration forexample thetestfor true predicate implicitly iteration producing true predicate true predicate generated previous iteration trips compilerimplements implicit predicateand chain implementation efficient explicit compoundpredicate operators eliminates predicate-and instruction reducing code size critical path length predicated architectures playdoh iaprovide special instruction modes predicate-defining instructions reduce height predicate computation tree wired-and wired-or modespermit conjunctionor disjunction limited set predicates additional combining instructions dataflow predication generalizes wired-and wired-or operations enabling arbitrary number predicates block reducing compute tree height compoundpredicates predicate ors trips dataflow execution model enables optimizations eliminate instructions common multiple predicated paths figure number loop iterations multiple block terminate executing fraction predicates dataflow graph bro instruction executes loopterminates otherwisethebro tperformsanotheriteration loop prior predicationmodels needed explicit predicate-or instructions implement tests trips predication isa implicitly computes tgti instructions produce matchingpredicate forexample ifall ofthetgtiinstructions produce true predicates bro instruction issue loop body executes unrolled iteration evaluates false bro instruction receives non-matching true predicates iterations matching false predicate iteration instruction issues routing multiple predicates single instruction compiler avoid generating explicit instructions compute compound predicate predicate-or feature ptr ptr ptr tgti addi ptr tgti tgtiaddi addi bro loop ptr ptr bro exit ptr ptr muli addi muli addi ldtgt write read write read figure predicate computation fanout reduction handling long dependences opportunities instruction merging shown section implicit predication edge architectures employ direct producer consumer bypassing automatically broadcasting instruction results common register file delivering single predicate predicated instructionsmayincursignificantoverhead forexample ifabasic block predicatedon predicatep naive implementation predicates instruction basic block due instruction size limitations instructions generate targets naive compiler build software fanout tree distributes predicate instructions increasing dependence height adding overheadto block dataflow predicating compiler eliminate thesepredicatesusingtwo techniques hoistingandimplicit predication figure dependence chains predicated opposite values predicate right-hand chain compiler performs predication false bottom effectively hosting chain instructions execute speculatively parallel computation left-hand chain compiler inserts predication true top routing predicate instruction top dependence chain implicitly predicating instructions predicate non-matching false predicated instruction fire implicitly predicated instructions left-hand chain fire implicit predication correct early mispredication termination implicitly predicated instructions fire ancestors receives non-matching predicate block fire microarchitectural support areas microarchitectural support needed correct implementation dataflow predication execution logic handle dataflow predicates correctly predicates part inter-block architectural state execution cores issue support register files caches improve performance handling conditional register writes stores nullification flushing mispredicated state block completes removesmispredicated dependence height performance consideration 
makes implicit predication fourth microarchitecture raise exceptions occur non-speculative paths masking discarding mispredicated exceptions predicated issue windows trips microarchitectureemploys reservation station eachof alus hold instructionsthat arewaiting execute reservation station operandrequires valid bit indicating operand arrived field store dataflow executioncore apredicateoperandis similar toanormaloperand difference status bit tracks operand arrived matching predicate arrived targeting predicate operand arrives atareservationstation tion predicate bits checks matching predicate match updates instruction status arrival non-matching predicates predication complicates bypassing logic route operand additional field operand arrives remote local execution unit block output nullification complication dataflow predication arises block output register write store trips requires block produce outputsregardlessofthepathtakenthroughtheblock thecompiler generate outputs alternate paths option read architectural register file memory write back predicated paths produce modified trips isa microarchitecture provide alternative knownasanulltoken register file memory system block generated outputbut architecturalstate shouldbe modified compiler inserts null instructions predicatedpaths purpose microarchitectureincludes additional tag bit operands indicating operand null token alu control logic propagates null values instructions reach block output additional hardware eliminates superfluous copies block inputs block outputs cost hardware complexity null tokens arriving register writes block re-issue read register obtaining earlier write nullifying block architectural register file early mispredication termination overheadcommon predicated architectures expense fetching executing mispredicated instructions fetching reduce effectiveinstruction fetch bandwidth effect mitigated predication eliminate expensive branch mispredictions mispredicated instructions fetched pipeline convenient selectivelyflush aresult programwitha instructions form long sequential dependence chain waste large number execution slots trips incurs overhead fetching mispredicated instructions microarchitecture eliminates mispredicated instruction execution overheads compiler predicates tops dependence chains implicitly predicated instructions non-matchingpath triggered execution operands arrive consume instructionexecutionslotsoroperandnetworkbandwidth microarchitecture wait mispredicated instructions completed terminating block microarchitecture detects block completed execution produces outputs register writes stores branch squash long mispredicated chains instructions executing processor core quickly reclaim reservation stations block forward progress affected waiting orphaned instructions trickle pipeline predicated exceptions predication opportunity compiler speculative instruction execution dataflow predication preserve exception semantics original program conventionalpredicated processors suchas implementa formofpoisonbit architecture sets exceptional conditions poison bit triggers exception speculatively computed non-speculative trips implementation employs similar solution trips services exceptions block boundary predication architecture form poison bit called exception bit instruction raises exception microarchitecture tags produced operand exception bit whichit iftheblock producesanyoutputwith exceptionbit set exception raised handled system rect predication instruction raises exception reaches instruction receive matching predicate architecture filters exception semantics precisely required behavior morecomplexproblem arises predicate carries exceptionbit isa specifies arriving predicate exception flag set interpreted false predicate instruction fires produces exception-tagged output well-formed trips blocks single dataflow path output thisschemeensuresthatthe exception propagated predicated path execution hyperblock compiler safely predicate bottom priorto completionof theblock predicatestransmitted hyperblock writes reads common architectural register file require special care preservethe correctexceptionbehavior figread read tgti slli addi mov mov movi teqi movi mov write write true false true false figure trips block predicate flow graph ure shows predicate produced hyperblock hyperblock speculative chain instructions spans hyperblocks bottomofthe chainin exception-taggedoperands produced speculativelyexecutedinstructionsandtransmittedfromh illegally trigger exceptions block boundaries ensure correct execution basic block split trips blocks output hyperblock guarded predicate opposed guarding outputs hyperblock compiler predicate optimizations compiler supports dataflow predication apply optimizations mitigate predication overheads predicate fanout reduction removes predicates based intra-block dependence chains path-sensitive define inter-block values finally generalformof instruction merging combines duplicate instructions reduce size block trips block region control flow graph executes atomically adheres architecturallyspecified block constraints absence predication trips block simply basic block maximize performance scale compiler forms hyperblocks combining regions control flow graph ifconversion performs traditional loop scalar optimizations forms hyperblocks hyperblock formation performs predication optimizations global common sub-expression elimination peephole optimization scale represents hyperblocks internally predicate flow graph pfg shown figure pfg directed represents predicate block basic block instructions predicated incoming edges previous work presence predication phase compilerrepresentsinstructionsinthree-addressform whereall intra-block communicationis expressed temporary register names read write instructions access registerfile whichis transfervaluesbetweeninstructions blocks optimizations section operate predicate flow graph static single assignment form final compilerphase schedules translates target form predicate fanout reduction dataflow predicating compiler apply implicit predication speculative hoisting reduce predicate fanout avoiding insertion move instructions required forward predicates consumers figure test instruction tgti defines predicate predicates instructions trips isa instructions target single instruction worst case compiler icate consumers scale static single assignment form free apply implicit predication hoisting remove predicate instruction raise exceptions subject restrictions discussed section strategy yield fanout reduction dataflow graph roots leaves predicating roots offers fanout reductionat expenseof losing performanceif predicate computation critical path compiler removes predicate conditions met instruction branch store instruction define predicate instruction define register live-out trips block instruction define register ssa -instruction figure shows figure compiler performs predicate fanout reduction removes predicate slli instruction enables compiler promotethe instruction dominating predicate block runtime execute instruction intra-block control flow path sensitive predicate removal trips isa requires paths block produce set register writes instruction defines register live path compiler insert additional instructions produce definition register paths compiler read register live copy mov movi teqi movi mov write write read read tgti slli slli addi mov mov movi teqi movi mov write write read read tgti slli addi slli addi mov movi mov write write read read tgti slli addi mov movi slli addi mov mov movi teqi true false falset true true false true false true true false false true false removed added figure predicate fanout reduction path sensitive predicate removal instruction merging paths definition insert null instructions nullify write paths figure live tions write original values back paths definitions move instructions -true -true predicate blocks set temporary register move instruction -false predicate block sets temporary register compiler preserve registers paths live due multipledefinitions liveregisters execute unconditionally optimization reduces amount predicate fanout increases speculation early resolution inter-block dependences instruction candidate optimization instruction defines live register live path instruction dominates exits live instruction raise exception candidate instruction found promoted execute unconditionally implies instructions define candidate operands excluding instructions define predicates promoted recursive promotion legal provided exceptions raised additional instructions upward dependencechain speculative figure shows path-sensitive predicate removal assuming live -true path addi instruction promoted dominating predicate block causing executed unconditionally enables compiler remove mov instruction -false block disjoint instruction merging instruction merging combines lexically equivalent instructions single instruction prior 
approaches require instructions predicated complementary predicates dataflow predication permits merging lexically equivalent instructions providing additional optimization opportunities compiler categories instruction merging instructions predicates opposite conditions true false instructions predicates conditions true true opposite conditions true false mahlke describe straightforward method category merging compiler identifies lexically equivalent instructions promotes instruction removingthe thecompilerpromotesthe instructionby moving block dominates predicate blocks original instructions branchesinthet -trueandt -falsepredicateblocks shown figure lexically equivalent receive predicatet arepredicatedon oppositeconditions compiler merges branches single branch instruction promotes dominating -false predicate block figure merge instructions belong category compiler exploits predicate-or capabilities section compiler identifies instructions merge category creates instructions predicated multiple predicates removes original instructions mov instructions -true -true predicate blocks shown figure candidates category merging compiler eliminates instructions introduces mov labeled figure note instruction predicated exploiting predicate-or capability isa runtime predicates true obeying predication rules similarly compiler merges mov instructions -false -false predicate blocks single instruction loop extracted genalg popx fitness fitness instruction sequence loop body fstod fsub fitness addi addi fitness fgt tlt bro genalg loop back bro genalg loop exit bro genalg loop exit predicate guards live-outs mov mov mov mov mov mov predicate combining movi tmp mov tmp mov tmp mov tmp figure instruction merging genalg compiler merges category instructions flipping condition test instruction generates predicate applying category transformation principal benefit instruction merging hyperblock eliminates instructions creating space inclusion instructions hyperblock inclusion exposes opportunities additional compiler optimization depicted figure reduces size trips block instructions instructions eliminated added figure shows instruction merging kernel snippet extracted genalg genetic algorithm application developed mit lincoln laboratories top left portion figure depicts source code bottomleft portionshows loop body including register read write instructions fitness live past loop test instructions fgt tlt represent predicate-and chain required implementing short-circuiting loop condition checks section and-chain ensures correct exception semantics loop executes iterations statically unrolling fill -instruction block iterations maximizes parallelism found instructionwindow exposes opportunities instruction merging merges exiting branch instructionsof iteration compiler applies category merging form single branch instruction likewise move instructions generating live registers candidates merging instruction mergingalso enables predicate fanout reduction shown figure loop exit predicates iteration control live register values instruction encode targets consumers predicate includingthebranch soa required predicates compiler merges predicates shown figure sends resulting predicate register producing instructions eliminates fanout instructions nofanout instructions required fanout instruction required tmp performing optimizations hand unrolled iterations genalg loop maximally fill block comparedtothebest performingcompiler optimizations improvedthe performance times performance analysis evaluate predicate fanout reduction pathsensitive predicate removal eembc benchmarks scale compiler trips back end cycle-accurate simulator called tsim-proc simulator closely models trips prototype microarchitecture recent performance evaluation estimated tsim-proc rtl-level simulator differ average set microbenchmarks tsim-proc faithfully models delays prototype implementation including one-cycle hop tile adjacent tile -way set-associative distributed data cache -cycle latency -way set-associative distributed instruction cache -cycle latency -cycle block fetch latency -cycle branch prediction latency figure shows results compiler optimizations allofwhich hyperblocks intra predicate fanout reduction inter path-sensitive predicate removal predicate fanout reduction path-sensitive predicate removal hyperblocks predicate optimizations baseline show results basic blocks predication process adding optimization scale compiler hyperblocks observe average reduction dynamic move instructions reduction total dynamic instructions reduction numberofdynamicblocks average speedup benchmarks stand benefitingfrom conven iirflt speedups benchmarks instruction promotion indirectly benefits branch predictor load-store dependence predictor enabling early resolution branches stores resulting higher prediction accuracy improved performance combining optimizations produces average speedup benchmark rotate shows aifftr aifirf aiifft autc basef bezi bitmnp cac heb rdr con ven dither fbital fft idctrn iirflt mat rix ospf pktfl pntr puwmod rotate rou telookup rspeed tblook text ttspr viterb average spee dup cyc les intra inter figure speedup eembc embedded benchmark suite marked improvement combined speedup basic blocks average slower hyperblocks slower hyperblocks predicate optimizations benefits optimized predicationaremultifold reductioninthe numberofblocks static executed reduced branch mispredictions improved i-cache performance conclusions dataflow predication exploits isa features microarchitectural mechanisms compiler algorithms reduce predication overheads edge isa maintaining low-complexity out-of-order issue vliw architectures execution overhead falsely predicated instructions limits compiler ability perform aggressive predication superscalar architectures hardware complexity isa encoding difficulties inhibit incorporation full predication dataflow predication avoids limitations reducing predicate encoding space consumed bits instruction dataflow predication incurs costs fanning predicates consumers components dataflow predication reduce overhead provide opportunities improved performance computation compound dataflow predicates supporting dual-polarity predicates predicated test instructions receipt multiple non-matching predicates instruction predicate-oring isa reduces overhead computing compound predicates implicit predication early mispredication termination microarchitecture supports removal blocks falsely predicated instructions executing speculatively capability support implicit predication speculative hoisting significantly reduce number consumers predicates software fanout trees disjoint instruction merging isa supports multiple sources instruction predicate compiler merge instructions distinct predicates eliminating redundant instructions presented preliminary evaluation dataflow predication optimizations trips architecture predicated hyperblocks improve performance average basic blocks fanout minimizations multiple hyperblocks improve performance additional instruction merging scalecompilerisstill immature wedemonstratedthat hand merging instructions significant speedups achievable benchmarks term additional optimizations reduce overheads dataflow predication short-circuiting instruction semantics permit tree-based computation predicate chains reduce predicate dependence height predicatemulticast operationsthat tradeinstructionplacementflexibilityfortheabilitytorouteapred- icate consumers shorter wider fanout trees reduce dependence heights instruction overheads improving performance longer term biggest remaining overheads predicationmay eventually fraction mispredicated instructions window reduce effective window size moment program execution classes instructions window instructions correctly predicated useless instructions falsely predicated instructions past branch misprediction useless window size sweet spot predication pure superscalar predication pure dataflow maximum parallelism instruction window sizes continue increase relative costs increased predication continue decline pushing ideal balance aggressive predication longterm solution branch mispredictions accurate predictors conversion unpredictable branches predicates extremely large instruction windows solution viable formof predicate predication reduce increasesin dependenceheights causedbypredication true paths execution acknowledgments research supported defense advanced research projects agency contracts -cand nbch nsf instrumentation grant eiansf career grants ccrand ccribm partnership awards grants alfred sloan foundation intel research council allen kennedy porterfield warren conversion ofcontrol dependence todata dependence proceedings ofthe annual symposium principles programming languages pages january arvind nikhil executing program mit taggedtoken dataflow architecture ieee transactions computers march august systematic compilation predicated execution phd thesis illinois urbana-champaign august hwu puiatti mahlke connors crozier hwu program decision logic approach predicated execution proceedings annual international symposium computer architecture pages 
august hwu ands mahlke aframework forbalancing control flow predication proceedings international symposium microarchitecture pages dec beck johnson pingali control flow data flow journal parallel distributed computing budiu venkataramani chelcea goldstein spatial computation proceedings international conference architectural support programming languages operating systems pages october burger keckler mckinley dahlin john lin moore burrill mcdonald yoder tripsteam scaling tothe endofsilicon edgearchitectures ieee computer july chuang calder predicate prediction efficient out-oforder execution proceedings annual international conference supercomputing pages june coons chen kushwaha burger mckinley spatial path scheduling algorithm edge architectures proceedings international conference architectural support programming languages operating systems san jose cytron ferrante rosen wegman zadeck efficiently computing static single assignment form control dependence graph acm transactions programming languages systems october dehnert hsu bratt overlapped loop support cydra proceedings international conference architectural support programming languages operating systems pages april dennis misunas preliminary architecture basic data-flow processor proceedings annual symposium computer architecture pages january kathail schlansker rau hpl-pd architecture specification version technical report hpl- hewlettpackard laboratories february kessler alpha microprocessor ieee micro march april kim mutlu stark patt branches combining conditional branching predication adaptive predicated execution proceedings annual ieee acm international symposium microarchitecture pages lowney freudenberger karzes lichtenstein nix donnell ruttenberg multiflow trace scheduling compiler journal supercomputing maher smith burger mckinley merging head tail duplication convergent hyperblock formation proceedings annual ieee acm international symposium microarchitecture december mahlke chen hwu rau schlansker sentinel scheduling vliw superscalar processors proceedings international conference architectural support programming languages operating system asplos pages october mahlke lin chen hank bringmann effective compiler support predicated execution hyperblock proceedings international symposium microarchitecture pages dec pnevmatikatos sohi guarded execution branch prediction dynamic ilp processors proceedings annual international symposium computer architecture pages apr rau yen yen towie cydra departmental supercomputer design philosophies decisions trade-offs computer january russell craycomputer system communications acm jan smith burrill gibson maher nethercote yoder burger mckinley compiling edge architectures fourth international ieee acm symposium code generation optimization cgo pages march swanson michelson schwerin oskin wavescalar proceedings annual ieee acm international symposium microarchitecture pages december traub compiler mit tagged-token dataflow architecture technical report trlcs mit cambridge august wang wang kling ramakrishnan shen register renaming scheduling dynamic execution predicated code proceedings international symposium high-performance computer architecture pages january warter lavery andw hwu thebenefit ofpredicated execution software pipelining proceedings annual hawaii international conference system sciences pages january 
digital immortality gordon bell jim gray october technical report msr-tr- microsoft research microsoft corporation howard street san francisco work submitted communications acm copyright transferred notice publisher post accepted version version article appears http research microsoft pubs digital immortality gordon bell jim gray microsoft research gbell gray microsoft digital immortality ordinary immortality continuum enduring fame end endless experience learning stopping short endless life preserving transmitting ideas one-way immortality allowing communication future endless experience leaning two-way immortality allowing part communicate future sense artifact continues learn evolve current technology extend corporal life decades one-way two-way immortality require part person converted information cyberized stored durable media two-way immortality experiences digitally preserved life century exploring points one-way two-way spectrum cyber project hamarabi aristotle shakespeare mozart rembrandt euler immortal ideas recorded ideas enduring form passed future great ideas images music writing architecture algorithms survive long people people dead ideas effectively immortal paper printing press made easier expensive record preserve disseminate ideas voice recorders cameras camcorders make easy record events experiences moore law bringing recording costs point record hear digital technologies offer kinds information convey future create immortality sized community family future generations intellectual community web sites net legacy forevernetwork memorymountain offer fee store letters essays photos videos stories forever order pass future generations digital equivalents tombs crypts libraries future technologies surely enhance ability convey ideas experiences creating one-way relationship future generations care listen today reasonable record read hear retaining conversation person heard requires terabyte adequate quality work submitted copyright transferred notice publisher post accepted version gordon bell engaged building personal archive lines envisioned bush gates memory aid research tool cyber project store documents photos music audio video recordings bell archive gigabytes including store books encoded video lectures music cds thousand documents archive messages accumulation rate gigabytes year rate increase speech video added fairly modest expense real cost archive data capture data organization data presentation research efforts directed table requirements storing media lifetime modest fidelity years personal stores terabyte cost hundred dollars person immortal terms media encountered famous people access entire life table estimated lifetime storage requirements data-types rate bytes hour day year lifetime amount read text pictures papers written text photos voice photos ten images day spoken text wpm spoken text kbps music high quality sound video-lite pots video vhs-lite dvd video unresolved technical social issues cyber project information preserved media platforms programs organized presented lifetime lifetime legal ethical rights responsibilities information involves people exploring issues focusing basic tasks acquisition preservation recall one-way immortality begin hints aspects person expressed program interacts future generations interesting archive person spoken output make compelling avatar person avatar live forever virtual world respond queries person past life great people albert einstein posthumous web sites addition computer science researchers cmu authored avatar einstein responds questions viewers fact avatar actor hired read quotes einstein writings demonstration understand future easier easier author avatars needing actors real question program learn stay current speculation clear challenge immortal interactive program begins bit two-way immortality live communicate forever ray kurzweil hans morovec faithful avatars century morovec predicts robots smart humans successive generations question-answering avatars gradually indistinguishable actual persons love enabling person live forever bell cyber project personal store microsoft research technical report msr- july bell http research microsoft gbell co-authored books papers talks videotaped lectures trip albums cybermuseum items bell dear appy committed signed lost forgotten data acm ubiquity february issue http acm ubiquity views bell html bush atlantic monthly july http isg sfu duchier misc vbush vbush shtml gates road ahead penguin books kurzweil ray age spiritual machines computers exceed human intelligence viking morovec hans robot mere machine transcendent mind oxford press stevens scott cmu project creating virtual einstein http cmu afs cmu project web papers bibliography html 
dozen information-technology research goals jim gray june technical report ms-tr- microsoft research advanced technology division microsoft corporation microsoft redmond dozen information-technology research goals jim gray microsoft research howard usa abstract charles babbage vision computing largely realized verge realizing vannevar bush memex distance passing turing test visions problems provided long-range research goals scalability problem motivated decades talk defines set fundamental research problems broaden babbage bush turing visions extend babbage computational goal include highly-secure highlyavailable self-programming self-managing self-replicating systems extend bush memex vision include system automatically organizes indexes digests evaluates summarizes information human group problems extends turing vision intelligent machines include prosthetic vision speech hearing senses problem simply stated orthogonal share common core technologies introduction talk argues long-range research societal benefits creating ideas training people make ideas turn ideas products education component research setting argues government support long-term research part talk outlines sample long-term information systems research goals begin thanking acm awards committee selecting acm turing award winner lucent technologies generous prize mentors colleagues years learned brilliant people time team effort project mike jim don jim franco jim irv jim andrea jim andreas jim dina jim tom jim robert jim present day case hard point personally collaborative effort joy work people closest friends broadly large community working problems making automatic reliable data stores transaction processing systems proud part effort proud chosen represent entire community association computing machinery selected turing award recipient approximately text talk gave receipt award slides talk http research microsoft gray talks turing ppt exponential growth means constant radical change exponential growth driving information industry years moore law predicts doubling months means months storage storage built processing processors built area curve months equals area curve human history george glider predicted deployed bandwidth triple year meaning doubles months prediction pessimistic deployed bandwidth growing faster doubling true underlying technology scientific output field doubling slowly literature grows year doubling years exponential growth forever coli bacteria stomach double minutes eventually limit growth years information industry managed sustain doubling inventing successive barrier progress accelerating figure argue acceleration continue argue stop stop innovating stop tomorrow rapid technology doublings information technology constantly redefine things impossibly hard ten years ago easy tradeoffs ten years cyberspace world information technology revolution cyberspace continent equivalent discovery americas years ago cyberspace transforming world goods services changing learn work play trillion dollar year industry created trillion dollars wealth economists united states economic growth industry high-paying high-export industries credited long boom economy skipped recessions boom started doubles years doubles years doubles yearsops figure graph plots performance price versus time performance operations-per-second bits-per-op price system price years performance price improvements accelerating growth curves transistors performance price doubling years discrete electronics performance price doubling years performance price doubled year vlsi sources hans moravec larry roberts gordon bell money sloshing gold rush mentality stake territory startups staking claims great optimism good thing world explorers pioneers settlers lost sight fact cyberspace territory exploiting explored pioneers decades ago prototypes transforming products gold rush mentality casing research scientists work near-term projects make rich taking longer term view generation prototypes explorers startups generation students faculty leave universities industry time start lewis clark style expeditions cyberspace major research efforts explore far-out ideas train generation research scientists recall tomas jefferson bought louisiana territories france ridiculed folly time jefferson predicted territories settled year accelerate lewis clark expedition explore territories expedition back maps sociological studies corps explorers led migratory wave west mississippi similar opportunity today invest expeditions create intellectual human seed corn industry responsibility government industry private philanthropy make investment children parents made investment pioneering research pays long-term recommend read nrc brooks southerland report evolving high-performance computing communications initiative support nations information infrastructure recently funding revolution figure based figure appears reports shows government-sponsored industrysponsored research time sharing turned billion dollar industry decade similar things happened research graphics networking user interfaces fields incidentally research work fits pasteur quadrant workstations lisp machine stanford xerox alto apollo sun networking arpanet internet ethernet pup datakit decnet lans tcp graphics sketchpad utah ibm lucasfilm sgi pixar windows englebart rochester alto smalltalk star mac microsoft time-sharing ctss multics ssd unix sds vms government funded industrial billion dollar year industry figure government-sponsored research ten years product ten years billion-dollar industry based internet world-wide web recent examples huge industries report written research generally focuses fundamental issues results enormous impact benefit society closer discipline decade research relational databases products products needed research deliver usability reliability performance promises active research problems continues day research ideas constantly feeding products time researchers explore parallel distributed database systems search huge databases explore data mining techniques quickly summarize data find interesting patterns trends anomalies data research ideas creating billion-dollar-per-year industry research ideas typically ten year gestation period products time lag shortening gold rush research ideas time mature develop products long-term research public good billions made government subsidize research trilliondollar year industry companies rich growing fast don research answer leading companies ibm intel lucent hewlett packard microsoft sun cisco aol amazon spend revenues research development product development guess total pure long-term research connected term product advanced development improve existing products guess industry spends million dollars long-range research funds researchers conservative estimate estimate number times large conservative measure scale long-term industrial research comparable number tenure-track faculty american computer science departments industry fund long-range research competitive companies mci-worldcom line item annual report consulting company eds dell computer small budget general service companies systems integrators small budgets reason long-term research social good necessarily benefit company invented transistor unix languages xerox invented ethernet bitmap printing iconic interfaces wysiwyg editing companies intel sun apple microsoft main commercial benefits research society products services research public good long-term research public good funded making boats rise funding part society industry paying tax long-term research benefits great society add fund research funding research added benefit training generations researchers workers advocate government funding industrial research labs government labs strong teaching component argue government funding long-term research benefits world fund long-term research social good world research europeans asians africans fund long-term research argument altruistic jingoistic altruistic argument long-term research investment future generations world-wide jingoistic argument leads industry industry extremely good transforming 
research ideas products nation maintain leadership people students universities ideas commercialize clear highly competitive business cyberspace global workers international united states complacent leadership move nations pitac report recommendations views topic grow year study presidential advisory committee pitac http ccic gov report report recommends government sponsor lewis clark style expeditions century recommends government double research funding funding agencies shift focus long-term research making larger longer-term grants hope researchers attack larger ambitious problems recommends fix near-term staff problem facilitating immigration technical experts congress acted recommendation year adding extra visas technical experts entire quota exhausted months visas granted early june long range systems research goals made plea funding long-term research talking examples long-term research goals mind present dozen examples long-term systems research projects turing lectures presented research agendas theoretical computer science list complements makes good long range research goal presenting list important describe attributes good goal good longrange goal key properties understandable goal simple state sentence paragraph suffice explain goal intelligent people clear statement helps recruit colleagues support great friends family challenging obvious achieve goal goal long time goals describe explicit implicit goals years camp goal impossible goal achieved resulting system people computer scientists people large testable solutions goal simple test measure progress goal achieved incremental desirable goal intermediate milestones progress measured small steps researchers scalability sample goal give specific work motivated scalability goal john cocke goal devise software hardware architecture scales limits kind limit billions dollars giga-watts space realistic goal scale node million nodes working problem scalability devise software hardware architecture scales factor application storage processing capacity automatically grow factor million jobs faster speedup larger jobs time scaleup adding resources attacking scalability problem leads work aspects large computer systems system grows adding modules module performing small part task system grows data computation migrate modules module fails modules mask failure continue offering services automatic management fault-tolerance load-distribution challenging problems benefit vision suggests problems plan attack start working automatic parallelism load balancing work fault tolerance automatic management start working scaleup problem eye larger problems research focused building highly-parallel database systems service thousands transactions developed simple model describes transactions run parallel showed automatically provide parallelism work led studies computers fail improve computer availability exploring large database applications http terraserver microsoft http sdss returning scaleabilty goal work scalability succeeded years progress astonishing reasons lot unexpected direction internet internet world-scale computer system surprised computer system million nodes doubling size year grow larger pitac worries scale network servers share apprehension research needed protocols network engineering hand build huge servers companies demonstrated single systems process billion transactions day comparable cash transactions day comparable aol interactions day lot addition systems process transaction micro-dollar cheap cheap transactions free access internet data servers essence accesses paid advertising combination hardware improvement year software improvement year performance price performance doubled year years progress sight figures dirty laundry computer scientists make parallel programming easy scaleable systems databases file servers online transaction processing embarrassingly parallel parallelism application learned preserve records sorted doubles year sorted dollar doubles year figure top picture node computer lanl bottom scaleabilty computer systems simple task sorting data sort speed sorting priceperformance doubled year years progress partly due hardware partly due software creating automatically running big monolithic job highly parallel computer modest progress parallel database systems automatically provide parallelism give answers quickly successful parallel programming systems programmer explicitly write parallel programs embraced resort examples beowulf clusters scientists inexpensive supercomputers http beowulf huge asci machines consisting thousands processors top figure groups report spectacular performance report considerable pain managing huge clusters problem automation postulated scaleable systems achieved virtually large clusters custom-built management system return issue scalability problem urgent decade appears computer architectures multiple execution streams single chip processor chip smp symmetric multi-processor pursuing processors imbedded memories disks network interface cards http iram berkeley istore trend movement processors micro-electro-mechanical systems mems mems sensors effectors onboard processing programming collection million mems systems challenge scaleabilty problem interesting long-term goal lecture describe broad spectrum systems-research goals long-term systems research goals remaining eleven long-term research problems read previous turing lectures consulted people ultimately settled organizing problems context seminal visionaries field charles babbage vision programmable computers store information compute faster people vannevar bush articulated vision machine stored human knowledge alan turing argued machines eventually intelligent problems selected systems problems previous turing talks excellent job articulating theory research agenda problems necessarily prove clause problems pose challenging theoretical issues picking problems avoid specific applications focus core issues information technology generic applications area topic ubiquitous computing alan newell articulated vision intelligent universe part environment intelligent networked research problems mentioned bear ubiquitous computing vision unable crisply state specific long-term research goal unique turing vision machine intelligence begin recall alan turing famous computing machinery intelligence paper published turing argued years computers intelligent radical idea time debate raged largely echoed today computers tools conscious entities identity volition free turing pragmatist intelligence define evaluate free proposed test called turing test intelligence litmus test turing test turing test based imitation game played people imitation game man woman room judge communicate judge questions minutes discover man woman easy man lies pretends woman woman judge find truth man good impersonator fool judge time practice judge time turning test replaces man computer pretending woman computer fool judge time passes turing test turing test build computer system wins imitation game time turing actual text matter worth re-reading fifty years time programme computers storage capacity make play imitation game average interrogator cent chance making identification minutes questioning original question machines meaningless deserve discussion end century words general educated opinion altered speak machines thinking expecting contradicted benefit hindsight turing predictions read technology forecast astonishingly accurate pessimistic typical computer requisite capacity comparably powerful turing estimated human memory bytes high end estimate stands today hand forecast machine intelligence optimistic people characterize computers intelligent interview chatterbots internet http loebner net prizef loebner-prize html judge long passing turing test enormous progress years expect eventually machine pass turing test specific happen years persuaded argument nearing parity storage computational power simple brains date machine-intelligence partnership scientists symbiotic relationship give stunning examples progress machine intelligence computers helped proofs theorems four-color problem famous 
solved open problems mathematics front page news ibm deep blue beat world chess champion computers design conceptualization simulation manufacturing testing evaluation roles computers acting tools collaborators intelligent machines vernor vinge calls intelligence amplification opposed computers forming concepts typically executing static programs adaptation learning cases pre-established structure parameters automatically converge optimal settings environment adaptation learning things child spider progress general pessimism machine intelligence artificial intelligence winter community promised breakthroughs deliver people trouble turing test rise expression turing tar pit easy complete short harder complete part pun turing famous contribution proof simple computers compute computable paradoxically today easier research machine intelligence machines faster expensive counting argument turing desktop machines intelligent spider frog supercomputers nearing human intelligence argument experiments measures human brain stores bytes terabytes neurons synaptic fabric execute tera-operations thirty times powerful biggest computers today start intelligence supercomputers day kidding personal computers million times slower times smaller similar argument human genome billion base pairs junk residue common chimpanzees residue common people individual million unique base pairs fit floppy disk arguments true missing fundamental megabyte difference babies software databases super-computers track pass turing test decade needed out-of-the-box radical thinking needed handed puzzle genomes brains work clueless solution understanding answer wonderful long-term research goal turing tests prosthetic hearing speech vision implicit turing test sub-challenges daunting read understand human write human difficult turing test interestingly problems easier difficult great progress computers hearing identifying natural language music sounds speech-to-text systems useable benefited faster cheaper computers algorithms benefited deeper language understanding dictionaries good natural language parsers semantic nets progress area steady error rate dropping year unlimitedvocabulary continuous speech trained speaker good microphone recognizes words joke computers understand english people note people understand english joking blind hearing impaired disabled people speech-to-text text-to-speech systems reading listening typing speaking person prepared text received attention speech recognition problem important machines communicate people major thrust language translation topic fallen favor simple language translation systems exist today system passes turing test english rich internal representation teaches system language mandarin computer similar internal representation information language opens possibility faithful translation languages direct path good language translation obvious bablefish http babelfish altavista fair current state art translates context-free sentences english french german italian portuguese spanish translates sentence pass turing test veuillez passer essai turing translates back pass test turing area visual recognition build system identify objects recognize dynamic object behaviors scene horse-running man-smiling body gestures visual rendering area computers outshine man-machine symbiosis special effects characters lucasfilm pixar stunning challenge remains make easy kids adults create illusions real time fun communicate ideas turing test suggests prosthetic memory reserve bush section additional turing tests speech text hear native speaker text speech speak native speaker person recognize objects behavior limited current progress areas boon handicapped industrial settings optical character recognition scan text speech synthesizers read text aloud speech recognition systems deaf people listen telephone calls people carpal tunnel syndrome disabilities enter text commands programmers voice input programming majority deaf devices couple directly auditory nerve convert sounds nerve impulses replacing eardrum cochlea understands coding body problem solved someday longer term prosthetics wider audience revolutionize interface computers people computers hear easier intrusive communicate hear remember hope agree tests meet criterion set good goal understandable challenging testable incremental steps bush memex vannevar bush early information technologist built analog computers mit world war ran office scientific research development war ended wrote wonderful piece government called endless frontier defined america science policy fifty years bush published visionary piece atlantic monthly http theatlantic unbound flashbks computer bushf htm article memex desk stored billion books newspapers pamphlets journals literature hyper-linked addition bush proposed set glasses integral camera photograph things demand dictaphone record information fed memex memex searched documents document addition annotate document links annotated documents shared users bush realized finding information memex challenge postulated association search finding documents matched similarity criteria bush proposed machine recognize spoken commands type spoken casually mentioned direct electrical path human nervous system efficient effective questions answers years memex scientific literature online scientific literature doubles years years online turing work bush articles online literature online protected copyright visible web library congress online web visitors day regular visitors tiny part library online similarly acm conference recorded converted web site month times people visited web site original conference months people spent total hours watching presentations web site substantially people time attendees actual event site averages visitors hours week wonderful web aware limitations hard find things web things web web impressive close bush vision place information information increasingly migrating online cyberspace information created online today times expensive store letters magnetic disk store file cabinet ten cents versus dollars similarly storing photo online times expensive printing storing photo shoebox year cost cyberspace drops cost real space rises reason migrating information cyberspace searched robots programs scan large document collections find match predicate faster cheaper easier reliable people searching documents searches document england easily accessed australia isn cyberspace simple answer information valuable property cyberspace respect property rights cyberspace culture information freely anytime information cluttered advertising free consequence information web advertising form substantial technical issues protecting intellectual property thorny issues revolve law protection party law cyberspace trans-national business issues economic implications change issues retarding move high content internet preventing libraries offering internet access collections customers physical library browse electronic assets technical solutions copy-protect intellectual property table property owner paid property view subscription time basis viewers listeners property easily anonymously legal business issues resolved technical solutions schemes devised protect intellectual property time scientists work scientific literature online freely paid taxpayers corporations locked publisher copyrights credit technical society acm progressive view web publishing acm technical articles posted web site department web site computer science online research repository corr hope societies follow acm lead personal memex returning research challenges sixth problem build personal memex box records hear read safeguards information command find relevant event display key thing memex data analysis summarization returns sees hears personal memex record person sees hears quickly retrieve item request records hear personal memex violate copyright issues raises difficult ethical issues private conversation memex disclose conversation sell conversation permission takes conservative approach record permission make private memex 
legal bounds designers vigilant privacy issues memex feasible today video personal record read recording hear terabytes personal memex grow megabytes year hold things read gigabytes year hold things hear capacity modern magnetic tape modern disks years disk tape year start recording stick tapes rest life video memex technology today decades economic high visual quality hundreds times terabytes year lot storage petabytes lifetime continue individuals afford people high definition stereo images petabyte easily rise ten times hand techniques recognize objects give huge image compression rate terabyte year offer current compression technology ten tvquality frames decade quality capturing storing organizing presenting information fascinating long-term research goal world memex bush vision putting professionally produced information memex interestingly book megabyte text books printed literature petabyte unicode movies short record dvd quality petabyte scanned books literature library congress images petabytes million sound recordings short add petabytes consumerquality digitized contents library congress total petabytes librarians preserve images sound fidelity recording scanning images exabyte recording radio broadcasts add year michael lesk nice analysis question information concludes exabytes recorded information excluding personal surveillance videotapes interesting fact storage industry shipped exabyte disk storage exabytes tape storage near-line tape on-line disk storage cost terabyte prices falling faster moore law storage hundred times cheaper ten years close time record exists inexpensively lifetime cyberspace cemetery plot recent research report photo family cost cents cents year cents year cents successive years cents insurance lead cyberspace find web search engines joy frustration wonderful find summarization giving title sentences real analysis summarization challenge personal memex returns undigested memex analyzes large corpus material presents convenient raj reddy system read textbook answer questions end text good college student demanding task corpus text internet computer science journals encyclopedia britannica answer summarization questions human expert field master text obvious step build similar system digest library sounds speeches conversations music challenge system absorb summarize collection pictures movies imagery library congress million text graphic items smithsonian million items wright brothers airplane moving items cyberspace interesting challenge visible humans http nlm nih gov research visible visible human html millimeter slices versions cadavers give sense exciting project copying leonardo devinci work cyberspace world memex build system text corpus answer questions text summarize text precisely quickly human expert field music images art cinema challenge case automatically parse organize information question question posed natural interface combines language gesture graphics forms interface system respond answers level user demanding task complete excellent goal simpler computer plays imitation game human telepresence interesting aspect record people observe event immediately retrospectively routinely listen lectures recorded research institution live on-demand extraordinarily convenient find time-shifting valuable space-shifting fundamentally television-on-demand audio radio-on-demand turning internet world expensive vcr higher-quality experience computers virtual reality recording event high-fidelity angles computers reconstruct scene high-fidelity perspective viewer sit space wander space sporting event spectator field watching action close business meeting participant sit meeting read facial gestures body language meeting progresses challenge record events create virtual environment demand observer experience event called teleobserver geared passive observer event past event observers passive watching interacting television radio give low-quality version today completely passive challenge participant interact members event tele-present tele-presence exists form telephones teleconferences chat rooms experience lower quality present people travel long distances improved experience operational test telepresence group students taking telepresent class score students physically present classroom instructor instructor rapport telepresent students physically present telepresence simulate place retrospectively observer teleoberserver hear participant simulate place participant telepresent interacting environment great interest allowing telepresent person physically interact world robot robot electrical mechanical engineering rest system information technology left robot dave huffman computer science patent byte algorithm electron physics energy matter charles babbage computers turing bush visions heady stuff machine intelligence recording telepresence time long-term research issues traditional computers charles babbage computer designs difference engine numeric computations fully programmable analytical engine punched card programs -address instruction set memory hold variables babbage loved compute things trends patterns wanted machines computations babbage vision computer realized computers power envisioned generating tables numbers bookkeeping generally computers babbage vision computational algorithms faster machines focus aspect babbage computers computers free infinitely fast infinite storage infinite bandwidth happen anytime computation million times cheaper trillion times cheaper figure decade thousand times cheaper perspective computers today free infinite speed storage bandwidth figure charts things changed babbage time measures priceperformance systems larry roberts proposed performance price metric icesystem measures bits-processed dollar roberts observed metric doubling months contemporaneous gordon moore observation gates-per silicon chip doubling measured system level data hans moravac web site http frc cmu hpm book corrections gordon bell plotted data herman hollerith forward systems mechanical electro-mechanical performance doubled years computing shifted tubes transistors doubling time dropped years microprocessors vlsi scene combination lower systems prices faster machines doubling time dropped year acceleration performance price astonishing rules similar graphs apply cost storage bandwidth real deflation processing storage transmission cost micro-dollars real data organization computer scientists dirty laundry programs typically bug thousand lines code free computers cost thousand dollars year care-and-feeding system administration computer owners pay comparatively today hundred dollars palmtop desktop computer thousand dollars workstation tens thousands server folks pay large operations staff manage systems self-organizing system manages simple systems handheld computers customer system work store data lose data system repairing call home schedule fix replacement system arrives mail replacement module arrives mail information lost software data problem software data refreshed server sky buy appliance plug refreshes server sky appliance failed vision server companies working building information appliances prototypes webtvs web browser trouble-free systems manages server-in-the sky server systems complex semi-custom applications heavier load provide services hand-held appliances desktops depend extent complexity disappeared moved people servers mind managing server content business systems management experts server systems managing human systems manager set goals polices budget system rest distribute work servers modules arrive add cluster plugged server fails storage replicated storage computation move locations hardware breaks system diagnose order replacement modules arrive express mail hardware software upgrades automatic suggests babbage goals trouble-free systems trouble-free systems build system millions people day administered managed single part-time person operational test serves millions people day managed fraction person administrative tasks system -hour day coverage substantial staff special expertise 
required upgrades maintenance system growth database administration backups network management dependable systems issues hiding previous requirements deserve special attention rash security problems recently melissa chernobyl mathematical attack rsa makes -bit keys dangerously small trust assets cyberspace trend continues major challenge systems designers develop system services authorized service denied attackers destroy data force system deny service authorized users data authorized added hook systems penetrated stealing passwords entering authorized user authentication based passwords tokens insecure physio-metric means retinal scans unforgeable authenticator software signed unforgeable operational test research goal tiger team penetrate system test prove security instances security system rest proof secure threats guarded attribute system availability availability today managed systems web experience availability due fragile nature web protocols current emphasis time-to-market nonetheless added years years order-of-magnitude improvement availability aim expectation outage century extreme goal achievable hardware cheap bandwidth high replicate services places transactions manage data consistency design diversity avoid common mode failures quickly repair nodes fail test achieving goal require careful analysis proof secure system assure system problem services authorized users service denied unauthorized users information stolen prove alwaysup assure system unavailable hundred years availability prove automatic programming brings final problem software expensive write thing cyberspace expensive reliable individual pieces software reliable typical program bug thousand lines tested retested typical software product grows fast adds bugs grows programs expensive simple designing creating documenting program costs line costs test code code shipped costs support maintain code lifetime grim news computers cheaper programs burden worse worse solution write fewer lines code moving high-level non-procedural languages big successes code reuse sap peoplesoft enormous savings large companies building semi-custom applications companies write lot code small fraction written user-written code database applications web applications tiny tools areas impressive based scripting language javascript set pre-built objects software reuse end users create impressive websites applications tools problem fits pre-built paradigms luck back programming java producing lines code day cost line code solution past logjam automatic programming holy grail programming languages systems years sad report progress factor factor improvement productivity problem fits application-generator paradigms mentioned earlier methodical software-engineering approaches finally yield fruit pessimistic approach needed turing tar pit high level specification language thousand times easier powerful current languages computers compile language language powerful applications systems today things essence imitation gave programming staff customer programming staff describes application staff returns proposed design discussion prototype built discussion eventually desired application built automatic programmer devise specification language user interface makes easy people express designs easier computers compile describe applications complete system reason application questions exception cases incomplete specification onerous operational test replace programming staff computer produce result requires time dealing typical human staff system alan turing machine intelligence matter time summary dozen interesting research problems long-term research problem government invest long-term research suspect years future generations computer scientists made substantial progress problems paradoxically dozen problems require machine intelligence envisioned alan turing problems fall thee broad categories turing intelligent machines improving human-computer interface bush memex recording analyzing summarizing babbage computers finally civilized program fail safe matter turns exciting beginning progress appears accelerating base-technology progress months equal previous progress moore law holds lots doublings dozen long-term systems research problems scalability devise software hardware architecture scales factor application storage processing capacity automatically grow factor million jobs faster speedup larger jobs time scaleup adding resources turing test build computer system wins imitation game time speech text hear native speaker text speech speak native speaker person recognize objects motion personal memex record person sees hears quickly retrieve item request world memex build system text corpus answer questions text summarize text precisely quickly human expert field music images art cinema telepresence simulate place retrospectively observer teleoberserver hear participant simulate place participant telepresent interacting environment trouble-free systems build system millions people day administered managed single part-time person secure system assure system problem services authorized users service denied unauthorized users information stolen prove alwaysup assure system unavailable hundred years availability prove automatic programmer devise specification language user interface makes easy people express designs easier computers compile describe applications complete system reason application questions exception cases incomplete specification onerous graph based data hans moravec robot mere machines transcendent mind oxford isbn http frc cmu hpm book personal communication larry roberts developed metric personal communication gordon bell helped analyze data corrected errors cstb nrc evolving high-performance computing communications initiative support nation information infrastructure national academy press washington cstb nrc funding revolution government support computing research national academy press washington isbn information technology research investing future president information technology advisory committee report president feb national coordination office computing information communications arlington donald stokes pasteur quadrant basic science technological innovation brookings isbn stephen ambrose undaunted courage meriwether lewis thomas jefferson opening american west simon schuster isbn micro-device smart dust science news vol alan newell fairy tales appears kruzweil age intelligent machines mit press isbn alan turing computing machinery intelligence mind vol lix web sites appel haken solution four-color-map problem scientific american oct http math gatech thomas fourcolor html manual proof vernor vinge technological singularity visionsymposium sponsored nasa lewis research center ohio aerospace institute march http frc cmu hpm book vinge singularity html endless frontier vannevar bush engineer american century pascal zachary free press isbn vannevar bush science-the endless frontier appendix report committee science public welfare washington government printing office reprinted national science foundation report online http rits stanford siliconhistory bush bush text html vannevar bush atlantic monthly july http theatlantic unbound flashbks computer bushf htm anne wells-branscomb owns information privacy public access basic books isbn micheal lesk information world http lesk mlesk ksg ksg html raj reddy dream dream cacm vol 
device web services cardlet webservice webservice webservice storage space session information cardlet data interpretter comm module translation layer 
jini friends work turn gsm sim web server eurescom project internal result roger kehr telekom joachim posegga telekom february overview mobile networks gsm umts provide security infrastructure builds subscriber-individual secret keys key stored smart card so-called sim lives mobile sims fact small complex computers operating system file system built-in java platform websim-approach turns sim mobile web server transparently accessed internet processes http request works personal web server person applications technology authentication internet online payments mobile keyboard display secure channel user orders product web site site simply submit http request prompt user confirmation order mobile technical description websim gsm sim built-in web-server integrates sims internet transparent access sim http cgi scripts internet hosts result gsm operators market gsm security infrastructure internet provide sim-services internet websim speaks lingua franca internet http trivial exercise internet developers include websim-based services applications services websim accessible internet hosts classical security services authentication encryption gsm commands provide simple interface user mobile http requests web application services websim establishing secure channel internet user websim noted websim application horizontal technology layer applications built contribution jini friends work technology layer designed convenient internet developers offers radically simple interface gsm sim services usage assume internet customer ordered item online web shop web shop mobile number customer simply integrate http-request web application confirm transaction gsm network note http-encoding space character stands sim toolkit select item gsm http-request websim prompts user menu mobile shown figure vendor trust transaction enhanced security gsm system security today superior internet offer trusted channel customers vendors missing internet implementation prototype websim implemented java sim schlumberger simera web server sim implemented toolkit-applet commands accessible cgi-scripts approach works gsm phase mobile connection internet websim handled proxy host tunnels http-requests sms web server applet sim figure shows architecture http-request arriving internet websim determined number packed sms sim request unpacked processed response back sms proxy normal http-response send back originator internet figure screenshot internet customer gsm compliant mobile sending sms direct access smsc websim proxy sim figure websim networking 
