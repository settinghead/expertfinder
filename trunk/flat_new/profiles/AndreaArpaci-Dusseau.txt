dependability analysis virtual memory systems lakshmi bairavasundaram andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison west dayton madison wilaksh dusseau remzi wisc abstract recent research shown modern hard disks complex failure modes conform failstop operation disks exhibit partial failures block access errors block corruption commodity operating systems required deal failures commodity hard disks failure-prone important operating system component exposed disk failures virtual memory system paper examine failure handling policies virtual memory systems classes partial disk errors type context aware fault injection explore internal code paths experiments find failure handling policies current virtual memory systems simplistic inconsistent absent fault injection technique identifies bugs failure handling code systems study identifies reasons poor failure handling design failure-aware virtual memory system introduction modern commodity operating systems assume disk drives work perfectly fail perfectly modern hard disks perfect operate fail-stop fashion exhibit complex partial failures set blocks inaccessible data stored blocks silently corrupted complex errors study worse errors expected diminish disk technology improves increase disk drive complexity increased low-cost unreliable ata disks incidence errors increase commodity operating systems equipped deal partial disk failures highend systems typically employed mechanisms deal disk faults techniques checksumming disk scrubbing commodity operating systems explicitly failure handling mechanisms policies recent work explored failure handling policies commodity file systems paper explore failure handling capabilities virtual memory systems integral part modern operating system file systems significant user disk storage type context aware fault injection techniques elicit failure handling policies virtual memory systems operating systems linux freebsd perform preliminary study windows virtual memory system characterize policies systems based internal robustness iron taxonomy proposed earlier work find virtual memory systems wellequipped deal partial failures file systems studied earlier virtual memory systems policies illogically inconsistent failure handling routines bugs cases failure handling policy simplistic cases absent disregard partial disk failures leads problems ranging loss physical memory abstraction data corruption system security violations paper organized section background partial disk failures virtual memory systems iron taxonomy section describes fault injection analysis methodology section presents experimental results analyzes failure handling approaches systems section discusses related work section concludes background section discusses partial failures commodity hard disks background virtual memory systems finally presents taxonomy failure handling policies paper characterize failure handling virtual memory systems partial disk failures section presents partial failures disk subsystem discusses suitable failure model disks layers storage stack contribute partial failures exhibited disk subsystem range classic problems media errors due bit rot head crashes errors bus controllers recent problems arising buggy firmware code additionally shown device drivers bugs rest operating system entire range sources disk failures documented recent paper proposes failure model disks called fail-partial failure model adopt model injecting disk errors study study makes aspects fail-partial failure model types errors partial disk failures errors blocks read written error code returned disk block corruption contents disk block read operating system altered error code returned disk transience errors disk failures permanent sticky temporary transient case transient failure errors operation performed failure model incorporate specific frequencies error types data frequency partial disk failures scarce drive manufacturers loathe provide information schwarz estimate partial disk errors occur times absolute disk failures recent experiments gray ingen sata disk drives uncovered uncorrectable read errors point view operating system -month experiment period partial disk failures occur dealt question arises component deal failures instance argue disk mirroring deal errors belief operating system components rely mirrored disks provide dependable computing environment component-specific policies optimizations employed simple mirroring operating system component disks include failure handling policy virtual memory systems virtual memory system disk storage provide applications address space larger physical memory helps system execute multiple processes large address spaces simultaneously disk area virtual memory system called swap space virtual memory system swap space store memory pages expected typically systems tend remove pages accessed recently accessed frequently memory store disk called page-out page stored disk accessed brought back physical memory called page-in page-out pagein process transparent applications performance effects virtual memory system responsible handling disk errors maintaining illusion page physical memory virtual memory systems make file systems situations directly on-disk space swap space maintained file file system virtual memory systems applications memory-map file data mmap system call file portion file memory mapped applications operate file data memory locations user code pages memory-mapped executable file program executed situations involving file system virtual memory system depends file system recover propagate disk errors subsections outline features virtual memory systems linux freebsd failure handling policies studied paper features windows virtual memory system discussed evaluation section linux linux virtual memory system largely derived previous linux versions performs swapping user-mode pages user-mode pages data stack code pages form user process order virtual memory system simple pages belong kernel paged simplification highly restrictive kernel pages occupy small portion main memory page replacement algorithm similar algorithm paged-out pages accessed space created pages read disk system issues reads advance read-ahead based application accesses improve performance swap area separate disk partition file file system swap header information swap area number blocks list faulty blocks level technique comment detection assumes disk works errorcode check return codes lower levels assumes lower level detect errors sanity check data structures consistency require extra space block redundancy redundancy blocks detect corruption end-to-end table levels iron detection taxonomy table describes levels detection iron taxonomy levels developed earlier paper level technique comment recovery assumes disk works propagate propagate error informs user record record operation succeed prevents dependent actions proceeding stop stop activity crash prevent writes limit amount damage guess return guess block contents wrong failure hidden retry retry read write handles failures transient repair repair data structs lose data remap remaps block file locale assumes disk informs system failures redundancy block replication forms enables recovery loss corruption table levels iron recovery taxonomy table describes levels recovery iron taxonomy levels developed earlier level record added paper freebsd freebsd open source operating system derived bsd unix design virtual memory system freebsd originally based mach virtual memory system considerable updates years freebsd virtual memory system allocates pages requested free list pages maintains sufficient free pages paging frequently inactive pages freebsd virtual memory system paging entire processes implies 
addition user-mode pages kernel thread stacks processes paged page tables freed system extreme memory pressure unlike linux freebsd virtual memory system perform extra read-ahead issue separate block read commands read blocks part read command block needed linux freebsd swap area disk partition file freebsd swap area data structures linux swap header failure handling policy taxonomy paper extend iron taxonomy proposed earlier paper taxonomy originally designed describe failure handling policies file systems find applicable virtual memory systems iron taxonomy consists axis detection recovery table table describe levels detection recovery addition levels proposed previously include recovery level record level system records operation succeed level recovery prevents system performing action assumes successful completion operation write error detected system recovers record free dirty memory page assuming successfully written disk avoiding data loss extend iron taxonomy adding axis prevention prevention axis encompasses techniques avoid loss due partial disk failures table describes levels prevention system special prevention techniques case system assumes disk works errors dealt occur remember basic prevention strategy remember specific block bad system bad experience block strategy prevent future data loss reboot phenomenon observed long time systems fail faults cured systems rebooted reinitialized systems rid effects transient bugs accumulated time fact failure prevention strategy periodically rebooting subsystems rebooting strategy virtual memory systems range disabling enabling swap area periodically re-initializing drivers disk controllers loadbalance prevention technique attempts reduce wear data blocks balancing load level technique comment prevention assumes disk works remember remembers disk errors prevents usage blocks errors reboot periodically re-initializes system avoid bugs due excess state loadbalance balances read write load disk blocks attempts reduce effects wear blocks scan performs read write checks bogus data detects possibly sticky block errors table levels iron prevention taxonomy table describes levels prevention technique wearleveling file systems flash drives jffs scan final prevention technique scanning disk bad blocks performing accesses bogus data technique raid systems weed potential bad blocks process called disk scrubbing technique classified eager detection technique earlier feel employed prevention technique virtual memory systems scan swap area periodically disk idle time freeblock scheduling avoid disk blocks found bad scan methodology section describe fault injection analysis methodology fault injection framework consists components benchmark injector benchmark layer sets system exposure disk faults layer consists types user processes coordinator managing benchmarking fault injection victims allocate large memory region sleep read memory region aggressors allocate large memory regions force victims pages swap area file system disk errors injected victims pages paged disk read back victims error injection performed injector layer interposes virtual memory system hard disk specifically injector built pseudo-device driver linux geom layer freebsd upper filter driver windows injector located device drivers drivers significant source errors virtual memory system equipped handle errors policies virtual memory system observed isolation method types errors injected injector read errors write errors corruption errors case read write errors error code returned virtual memory system ensure valid data memory read failed error code technique needed virtual memory system ignore error code returned case valid data respective memory page system work fine corruption errors block contents altered block experiments case corruption detected perform detailed analysis corrupting field data structure field-specific values separate experiments perform type context aware fault injection injecting disk errors specific disk blocks specific times data type user-level private data segment user data error injected disk block holds private user data page type-aware context basic function performed virtual memory system interface offered virtual memory system applications context swapoff system call error injected disk block swapoff progress context-aware table presents data types errors injected virtual memory systems table presents contexts error injection performed types contexts explored dependent system study order perform type-aware error injection injector detect type blocks read written detection accomplished variety ways benchmark layer communicates type information data pages injector benchmark allocates data pages initializes pages specific values conveys values injector cases injector block contents determine block type method employed determine type location block linux swap header located block failure handling policy system identified manual observation results error injection specifically sources information injector injector logs operations detail enabling determine failure handling policies instance virtual memory system performing retries read write repeated disk block number remapping disk write repeated disk block memory page benchmark benchmark records return values signals received helps determining error propagated benchmark checks reports validity data read back helps checking block type description detection virtual memory system swap header describes swap space location linux user data page private user data segment content linux freebsd user stack page user stack segment content linux freebsd shared shared memory page processes content linux freebsd mmapped memory-mapped file data content linux freebsd user code page user code segment location linux freebsd kernel stack page kernel thread stack user process kernel information freebsd table data types table describes types blocks failed detection method applicable virtual memory system type order detect kernel thread stack pages made simple modification freebsd kernel obtain memory addresses pages context workload virtual memory system actions swapon makes swap space swapping read swap header initialize in-core structures swapoff removes swap space page-in valid blocks free swap space pagetouch page accessed victim read page disk readahead workload induces readahead reading nearby pages perform read-ahead reading blocks disk madvise victim issues madvise madv willneed page-in blocks hint future reads specifiedinhint pageout aggressors create memory pressure causing page-out write inactive memory pages disk umount file system unmounted write dirty mmaped file data complete process scheduled complete page-out page-in essential data structures process table contexts table shows workload contexts experiments actions performed virtual memory system context data corruption system messages operating system emit error messages system message log techniques determine failure handling policies adopted virtual memory systems combinations data type context error type techniques primarily determine detection recovery policies discuss experiments determine prevention policies section analysis section present results experiments virtual memory systems analyze failure handling approach systems finally discuss experience fault injection techniques experimental results performed detailed analysis linux freebsd virtual memory systems preliminary analysis windows virtual memory system focus detection recovery techniques linux freebsd discuss prevention techniques systems finally evaluate windows present scenarios combinations data 
type context error type linux freebsd experiments involving swap space performed separate disk partition swap space windows experiments involving memory-mapped files user code pages ext file system linux unix file system ufs freebsd observed failure handling policy experiments involving file system combination policies virtual memory system file system linux tables present results fault injection linux virtual memory system detection find read errors detected errorcode checking return codes exceptions occur swapoff virtual memory system pages valid blocks memory error detected application data belongs junk data future memory access lead application crashes data corruption write errors detected read page write error virtual memory system page-in disk block previous contents missing errors lead application crashes application data corruption bad data system security problems application possibly read data belongs process swapon swapoff pagetouch readahead madvise pageout umount user data user stack shared mmapped user code rea err swap header user data user stack shared mmapped user code ite swap header user data user stack shared mmapped user code uptio swap header symbols errorcode sanity experiment applicable comments sanity checks swap space signature version semantically-smart disk systems past present future andrea arpaci-dusseau remzi arpaci-dusseau lakshmi bairavasundaram timothy denehy florentina popovici vijayan prabhakaran muthian sivathanu computer sciences department google wisconsin madison mountain view california abstract paper describe research on-going group past years semantically-smart disk systems semantically-smart system typical blockbased storage systems extracting higher-level information stream traffic disk enables interesting pieces functionality implemented lowlevel storage systems describe development efforts past years highlighting key technologies needed build semantically-smart systems main weaknesses approach discuss future directions design implementation smarter storage systems introduction past years group working ways increase functionality performance reliability security storage systems approach consistent build storage systems tomorrow living constraints real world notion design constraint major research thrusts group real world deal things world storage main constraints encounters presented interface storage typically disk raid presents linear array blocks clients block read written scsi good interface advantages primarily simple portable file systems direct clients storage access disk drives virtually complexity head positioning error handling details drive access hidden client high-level abstraction downsides lampson famously don hide power array-based interface storage preventing large number interesting pieces functionality implemented research suggests rotationally-aware disk schedulers greatly improve performance low-level information required perform scheduling hidden disk interface examples exist flavor desired functionality requires information higher-level system file system lowerlevel system disk natural solution problem simply change interface storage change fraught peril requiring broad industry consensus massive upheaval existing infrastructure embarked alternate required build storage systems tomorrow limitations today interfaces researchers including high road end building file systems awareness disk system underneath chose low road enhancing low-level storage systems knowledge file system motivating reason choosing storage system target innovation practical multi-billion dollar storage industry largely builds ships block-level storage systems paper describe work semantically-smart disk systems compared typical dumb storage device semantically-smart system knowledge file system data structures operations knowledge build interesting storage systems work focused improving performance reliability security storage systems applying techniques typical devices present development work past years discussing key pieces technology reflecting step led paths discuss technology filter industrial world utility semantically-smart techniques time surprising aspect work sound theoretical underpinnings theory practice required build correctly functioning semantically-smart disks rest paper organized section provide background describing work gray-box techniques present generation semantically-smart disk prototypes sections discuss importance theoretical framework section present future directions section conclude section background work semantically-smart disk systems finds roots earlier research gray-box techniques basic idea gray-box approach simple building component system great deal knowledge components system designed implemented taking advantage knowledge component building exploiting fact components black boxes inner-workings exploited knowledge components perfect white boxes dealing imperfect knowledge critical challenge leveraging gray-box techniques application scan set files application underlying operating system files cache application flexibility choose order access access cached files improves latency files cache accessed quickly bandwidth files accessed displace files cache operating system fetch disk problem arises operating system reveal information challenge advantage gray-box knowledge operating systems caches cache replacement policies well-known determine contents cache approaches earlier work demonstrated utility probing cache accessing blocks file timing long takes access application determine file present cache high probability subsequent work route learning behavior cache-replacement algorithm recencyor frequency-based history make replacement decisions application simulate replacement algorithm build accurate model contents cache initial line work targeting applicationos boundary began interface file systems block-level storage systems beneath clear simple interface storage benefits limited interesting optimizations enhancements functionality require information file system storage system information difficult layer storage stack information layer solution information gap change interface storage people advocating change years change problematic reasons broad industry consensus required instantiate change enormous disks raids add capabilities clients systems migrate benefits obvious chicken-and-egg problem changing successful interface requires anticipate usage scenarios good design team relevant situations account finally change expensive huge investments required enable make pervasive problem presented limited interface file systems storage explicit interface change unattractive variety reasons found wondering didn change interface built smarter storage systems learned inferred information file systems essence obtain benefits interface requiring change storage interface first-generation systems mindset began work effort semantically-smart storage systems published fast work major thrusts tool eof automatically infer data structures client file system set run-time techniques disk requires determine relevant pieces file system information collection case studies demonstrate utility semantic-awareness storage additional case study published isca discuss pieces turn describing challenges learned offline techniques extracting static information eof challenge simple disk raid system gain knowledge file system data structures approach simply embed static file system information disk raid system built-in knowledge file system data structures locations disk approach work initially felt approach limiting wondered automate process eof extraction file systems tool basic operation simple userlevel process host issues series disk requests disk in-disk agent monitors resultant traffic carefully controlling exact file operations issued file system eof infer great deal knowledge on-disk structures technique eof isolation combined patterns blocks written disk test data block inode identify fill data block pattern monitoring contents written blocks storage system detect data blocks disk workload inode data block written disk successfully isolate inode block block filled pattern manner eof acquire remarkable amount detailed information on-disk structures file system on-line techniques classification association operation inferencing eof realized important component semantically-smart disk system on-line inference specifically static knowledge disk system monitor current traffic make inferences state file system disk system block live dead make inference simply knowing static location bitmaps examine contents bitmap complicated section developed set basic on-line techniques semantically-smart disk systems garner type knowledge basic called direct classification technique disk system examines block address read write request determine type block read write directed inode region disk simply checking address sufficient determine block inodes slightly sophisticated form classification indirect classification technique examines contents blocks determine type block determine block holds directory contents typical unix file system examine inode points block call process monitoring inode contents inode snooping indirect blocks similarly identified technique call block association technique connect related blocks simple efficient manner data block read written inode block belongs table maps associations delivers information final technique term operation inferencing method semantically-smart disk infer higher-level operations invoked file system infer file creations deletions unix file system operations 
detected monitoring file system state observing change inode bitmap infer creation deletion file bit set creation bit deletion case studies basic infrastructure techniques place constructed set prototype semantically-smart disks demonstrate utility prototypes built software pseudo-device drivers mounted beneath real file systems cases mentioned utilized simulation explore idea case study discuss in-disk implementation track-aligned extents basic idea allocate files fit track avoiding costly track-switches file access performance improved disk-level implementation semantic knowledge file system structures influence file system placement track-aligned specifically marking blocks track boundaries allocated disk coerce file system allocating files proper manner resulting number bad block count table linux detection techniques table presents linux detection techniques read write corruption errors combinations data type rows context columns comments provided tables corruption errors detected corrupted data returned application exception sanity swap header swapon sanity checks correct swap space signature correct version number number bad blocks maximum allowable recovery cases disk error detected linux basic recovery mechanisms read error application-accessed page sigbus signal inform application error propagate case shared memory page processes touch page read error occurs receive sigbus signal words virtual memory system retry read process accesses page propagate swap header corrupted case error returned swapon call experiments memory-mapped file data user code retry observed retry specific disk block system original operation involved disk blocks retry performed block retry initiated file system virtual memory system read swap header fails swapon aretryisperformed retry due implementation bugs results retry swapon performance returns success improvement read errors noticeable call case fails study swapon focuses swapoff pagetouch caching readahead madvise system pageout umount user x-ray data infer contents user stack cache monitoring shared stream traffic disk mmapped generates key insight user x-ray code time file rea read err inode swap updated header access time user eventually data flushed user disk watching stack inode accesstime shared updates x-ray mmapped build user coarse model code ite cache x-ray swap header cache user data user job stack managing cache shared aiming exclusivity mmapped simulations show user performance code dramatically improved uptio swap header smarter second-level caching symbols strategy x-ray employs propagate final retry case study record focus experiment applicable comments implementation sigbus journaling signal beneath separate non-journaling retry file block system needed turned original request retry difficult case study operation implement fails block success level returned error semantically-smart disk infer file system transaction taking place group related updates occurring makes challenging file system behavior file systems fundamentally delay reorder filter operations disk disk difficult time decoding happened solution problem point simple mount file system synchronously guaranteeing updates reflected disk complete timely manner result disk implemented journaling benefits non-journaling file system case linux ext lessons learned year working project yielded interesting results infer on-disk structures automatically techniques developed eof developed numerous online techniques determine true state file system infer operations invoking finally case studies observed great potential semantically-smart disk systems enabling interesting storage functionality change file system initial work demonstrated numerous difficulties approach originally thought on-line techniques challenging develop understood asynchronous nature modern file systems greatly complicate on-line inference wished perform understood embedding static information data structures disk reasonable on-disk data structures tend evolve slowly file systems world work improve eof automatic data structure inference tools assuming semantically-smart ship built-in knowledge important file system structures surprised learn difficulties working underneath linux ext file system chose ext thought simplest operate underneath proved challenging primary reason hardship laissez faire manner ext writes blocks disk unlike unix-based file systems ext imposes ordering kind disk writes making semantic inference challenging discuss broadest conclusion work experience case studies clear case study learned lot technology needed build semantically-smart systems develop technology find interesting pieces storage functionality develop semantically-smart found ruminating possibilities bit functionality lucky found second-generation systems generation semantically-smart prototypes semantically-smart technology heights greatly increasing understanding systems work began limitations approach extremes pushed technology primary contribution work understanding operate file systems asynchronous operations correctness required generation semantically-smart systems comprised in-depth case studies d-graid raid array degrades gracefully faded secure-deleting disk operates asynchronous file systems removing major limitation earlier attempt secure delete discuss turn present lessons learned works d-graid degrading gracefully d-graid exploits semantic intelligence disk array place file system structures disks fault-contained manner unexpected failure disk occurs d-graid continues operate serving files accessed key techniques d-graid provide higher level availability technique replicate naming metadata structures file system high degree standard redundancy techniques data small amount overhead excess disk failures render entire array unavailable entire directory hierarchy traversed fraction files missing proportional number missing disks technique fault-isolated data placement ensure meaningful units data failure d-graid places semantically-related blocks blocks file storage array unit fault-containment disk observing natural failure boundaries found array failures make semantically-related groups blocks unavailable leaving rest file system intact fault-isolated data placement improves availability cost related blocks longer striped drives reducing parallelism found raid techniques remedy d-graid implements access-driven diffusion improve throughput frequently-accessed files copying blocks hot files drives system underneath linux ext determining blocks semantically-related challenging blocks dynamically typed block user-data block indirect-pointer block directory-data block order writes file system disk arbitrary result storage system accurately classify type block block propagated operation remembered page touched error propagates processes touch page read error occurs table linux recovery techniques table presents linux recovery techniques read write corruption errors combinations data type rows context columns bug implementation comments provided tables internally propagate error record handle read errors readahead madvise byusingr record system records failure read future readaheadand madvise data required immediately read-ahead optimization virtual memory system madvise hint block accessed readahead case error propagated page touched madvise retry performed page touched actions fact read unsuccessful freebsd tables present results fault injection freebsd virtual memory system detection errorcode single case detecting read write errors freebsd virtual memory system checks error code returned freebsd detect block corruption leads application crash data corruption cases leads kernel crash corruption kernel swapon swapoff pagetouch madvise complete pageout umount user data user stack shared mmapped user code rea err kernel stack user data user stack shared mmapped user code ite kernel stack user data user stack shared mmapped user code uptio kernel stack symbols errorcode experiment applicable table freebsd detection techniques table presents freebsd detection techniques read write corruption errors combinations data type rows context columns freebsd read block swapon read pages madvise table thread stack blocks detected case errors system unbootable recovery recovery mechanisms freebsd deal detected errors retry memory-mapped data written file system unmount fact system retries times umount call retries performed file system virtual memory system document behavior behavior observed application memory-mapped file data feature supported virtual memory system read errors page accesses virtual memory system deliver sigsegv segmentation fault application instance propagate experiments showed case shared memory unlike linux processes sharing memory region operate independently error propagated processes accessed page disk access retried process accesses page propagate write retries failed umount error returned application stop read errors swapoff read errors page-in kernel thread stack cases result kernel panic conservative action pageout virtual memory system attempts free memory pages writing swap swapon swapoff pagetouch madvise complete pageout umount user data user stack shared mmapped user code rea err kernel stack user data user stack shared mmapped user code ite kernel stack user data user stack shared mmapped user code uptio kernel stack symbols propagate retry record stop experiment applicable comments kernel crashes stack filled indirect pointers identified observing inode due reordering behavior file system time disk writes inode indirect block block freed original inode reallocated file normal data block disk operations place memory reflected disk inference made semantic disk wrong due inherent staleness information d-graid deals uncertainty allowing fault-isolated placement file compromised limited amount time time bounded inode file written d-graid detect correct classification move block d-graid optimizations reduce number misclassifications checking contents indirect blocks valid number valid unique pointers null pointers slots non-null implemented d-graid ext vfat d-graid behaves desired analysis shows d-graid users access files additional disk failures occur raid naming meta-data replication percentage accessible files matches percentage working disks utilize process availability figure merit number processes run unaffected disk failure d-graid degrades expected linear drop-off processes access user files run successfully storage unavailable faded forgotten smarter storage systems understand blocks live dead investigated block liveness inferred semanticallysmart storage specifically explored difficult case infer generational liveness block belongs live file context implemented faded file-aware dataerasing disk implements secure delete ensuring deleted data recovered disk secure delete functionality pushes disk ability perform correct inferences false positive detecting delete leads irrevocable deletion valid data false negative results deleted data recoverable faded detects file deleted faded shreds blocks belonging file overwriting block multiple times specific patterns fact block shredded detected ways faded bit bitmap cleared indicating block freed generation count inode incremented indicating inode freed reallocated block pointed inode indicating block freed reallocated file challenge address reordering reuse file system block pointed inode faded definitively current contents block file faded deal uncertainty conservative converting apparent correctness problem performance problem faded perform shredding operations required mechanism introduce conservative overwrite erases past layers data block leaves current contents block intact conservative overwrites means valid data inadvertently shredded overhead suspicious blocks tracked shredded multiple times prototype implementation found minor needed ext operate correctly top faded ensures file truncates treated deletes ensures inability definitively classify indirect blocks lead missed deletes faded typical unix workload find implicit inferences conservative overwrites impose approximately overhead compared disk perfect information lessons learned implementing challenging case studies learned great sigsegv signal kernel panic memory page freed deal semantically-smart disk systems fundamental challenges pose system designers important lesson living uncertainty core building systems due asynchronous nature file systems worst case disk system receives incomplete 
information state file system time learned imprecision interesting prototypes constructed careful design d-graid faded worked lack complete information achieved goals cases subtle reasoning required order build robust working prototypes handled corner cases times deep implementation realized problem approach requiring back drawing board rethink realized needed careful needed theory file systems disks interacted systems theory began effort build formal logic file system disk interactions logic began means reasoning semantically-smart disks realized possibilities broader logic file system developers understand complex interactions file systems disks logic begins set basic entities containers pointers generations file system simply collection containers linked pointers container reused freed represents generation logic formulated beliefs actions belief model state file system on-disk in-memory action state file system beliefs true time fundamental understanding impact actions beliefs ordering actions special care constructing temporal relationship actions proofs finally constructed starting basic axioms applying series event sequence substitutions implies observe simply replace subsequence initial results prove correctness existing file system consistencymaintenance techniques soft updates show linux ext file system needlessly conservative performs transaction commit demonstrating logic enable aggressive performance optimizations show logic aid development functionality building analyzing correctness consistent undelete functionality linux found simple logical framework critical development semantic technology reasoning disk interaction required formal approach required build robust correct systems future directions semantic disks project learned great deal file systems disk systems interactions harness experience forward ruminate future semantic disk technology block-level storage primary question semantic techniques applicability real world case studies complex industry fundamentally conservative adopt approach successful industry adoption aimed radical case studies imagine disk array performed smarter prefetching paying attention file boundaries requires semantic knowledge require wrong performance suffer question semantic inference applied clients disk systems database management systems performed initial work lines met mixed success techniques translate readily complex specific data structures typical dbms complicate matters occasionally dbms structures ripe kind reverse engineering advocate transaction log replete information dbms candidate future semantic technology lines noticed sea change modern file systems journaling make semantic inference easier difficult dbms file system journal takes chaotic update sequence simple file system ext turns orderly understandable affair linux ext perfect file system study semantically-smart disks underneath pushed deal extreme asynchrony arbitrary ordering writes future systems interpret log contents simpler easily verified correct major change storage interface object-based disks horizon change semantic inference obviated drives generally information clients typical block-based disks straight oneto-one file-to-object mapping drive easily determine blocks free evolved interface room inference semantic technology directory structure part interface journaling file systems databases place logs disk structures require semantic inference valuable sources information storage systems finally broader place semantic inference technology simply building storage systems current work explores low-level tracing fault injection understand file system performance failure characteristics systems grow increasingly complex tools deconstruct behavior integral part design implementation maintenance systems conclusions presented retrospective work semantically-smart disk systems work began simple question smart make block-level disks changing disk interface evolved development series increasingly challenging case studies beginnings formal theory understanding file system disk interactions modern world avoiding constraints layering system structuring artifacts impossible semantic inference provided means reclaim lost nature designs anderson osd drives snia events past developer dba snia osd pdf arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca pages munich germany june burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey california june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june denehy arpaci-dusseau arpaci-dusseau journalguided resynchronization software raid proceedings usenix symposium file storage technologies fast pages san francisco california december denehy bent popovici arpaci-dusseau arpaci-dusseau deconstructing storage arrays proceedings international conference architectural support programming languages operating systems asplos pages boston massachusetts october ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december ganger patt metadata update performance file systems proceedings symposium operating systems design implementation osdi pages monterey california november ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement proceedings twenty-sixth annual hawaii international conference system sciences volume pages gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon gray computers stop international conference reliability distributed databases june gunawi agrawal arpaci-dusseau arpaci-dusseau schindler deconstructing commodity storage clusters proceedings annual international symposium computer architecture isca pages madison wisconsin june jacobson wilkes disk scheduling algorithms based rotational position technical report hpl-csp- hewlett packard laboratories lampson hints computer system design proceedings acm symposium operating system principles sosp pages bretton woods hampshire october lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october nugent arpaci-dusseau arpaci-dusseau controlling place file system gray-box techniques proceedings usenix annual technical conference usenix pages san antonio texas june prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems proceedings usenix annual technical conference usenix pages anaheim california april prabhakaran arpaci-dusseau arpaci-dusseau modelbased failure analysis journaling file systems proceedings international conference dependable systems networks dsnpages yokohama japan june prabhakaran bairavasundaram agrawal gunawi arpaci-dusseau arpaci-dusseau iron file systems proceedings acm symposium operating systems principles sosp pages brighton united kingdom october ridge field book scsi starch june schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november schindler griffin lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix symposium file storage technologies fast monterey california january schindler schlosser shao ailamaki ganger atropos disk array volume manager orchestrated disks proceedings usenix symposium file storage technologies fast san francisco california april seltzer chen ousterhout disk scheduling revisited proceedings usenix winter technical conference usenix winter pages washington january sivathanu arpaci-dusseau arpaci-dusseau jha logic file systems proceedings usenix symposium file storage technologies fast pages san francisco california december sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau database-aware semantically-smart storage proceedings usenix symposium file storage 
technologies fast pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpacidusseau improving storage system availability d-graid proceedings usenix symposium file storage technologies fast pages san francisco california april sivathanu prabhakaran popovici denehy arpacidusseau arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california april talagala arpaci-dusseau patterson microbenchmarkbased extraction local global disk characteristics technical report csd- california berkeley wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans louisiana february wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october 
alternate victim chosen page-out upto retries write operation blocks error returned table freebsd recovery techniques table presents freebsd recovery techniques read write corruption errors combinations data type rows context columns comments provided tables freebsd read block swapon read pages madvise table space write errors occur page-out process freebsd virtual memory system recovers record case virtual memory system remembers write operation performed successfully memory page freed virtual memory system successfully free memory page proceeds select alternate victim page-out prevention techniques determining prevention policies difficult determining detection recovery policies prevention policy triggered disk fault methodology uncovering prevention policy specific test prevention technique remember technique triggered faults test remember injecting sticky error repeatedly disk block checking virtual memory system stops disk block workload performs iterations page-out page-in victim pages linux freebsd find bad disk block repeatedly spite returning error time results obtained read write errors linux freebsd track bad blocks remember test loadbalance causing virtual memory system page-out pages numerous times checking blocks swap area fairly evenly workload performs iterations pageout page-in victim pages linux freebsd disk blocks reused repeatedly blocks swap area written systems perform wear-leveling loadbalance finally detect reboot scan simply observe activities occur interval virtual memory system observe instance ofp reboot orp scan experiments infer linux freebsd techniques summary experiments linux freebsd prevention techniques windows section outlines features windows virtual memory system discusses failure handling policies windows file ntfs partition store memory pages paged-out failure handling 
policy extract combination policies ntfs virtual memory system windows paging user kernel memory inject faults user data pages read corruption errors injected pagetouch write errors injected pageout error code status device data error read write errors detection windows error code returned disk detect read write errors errorcode corruption errors detected recovery recovery read errors terminating user application reporting error inpageerror propagate recovery write errors involved primarily record memory pages error occurs written selected paging disk block error read back read succeeds half-block write performed read fails half-block read performed irrespective success failure halfblock operations block future writes record deal errors writes identify purpose half-block operations transient write error disk blocks subsequently successfully written read back application accesses data leading application receiving junk data bug handling write errors investigation required ascertain behavior prevention error injection experiments demonstrated disk block re-used errors block remember block added bad cluster file disk re-formatted failure handling approaches section discuss approaches current virtual memory systems adopt handle disk failures contrasting techniques identifying deficiencies systems compare approach virtual memory systems file systems explored start summarizing approaches virtual memory systems linux linux fails detect disk errors error codes returned simple recovery schemes deal detected errors respect corruption swap header corruption detected freebsd freebsd correctly detects disk errors error codes ignores corruption errors simple recovery schemes deal errors conservative linux cases kernel calls panic stop entire system read fails swapoff read affects single application windows windows detects disk errors error codes ignores corruption errors simple recovery schemes system observed prevention technique remember general systems suffer deficiencies simple recovery techniques virtual memory systems studied simple recovery techniques deal disk errors attempt techniques redundancy completely recover disk errors ignoring data corruption data corruption experiments case linux swap header detected virtual memory systems assume disks store data reliably true commodity hardware under-developed mechanisms aprimeexampleofan under-developed mechanism remembering bad blocks linux swap header provision store list bad blocks list effectively prevent data loss remember list initialized mkswap updated afterward errors occur hand windows actively updates bad cluster file avoid error-prone blocks memory abstraction mismatch applications expect pages behave memory virtual memory system maintain memory abstraction disk errors important part maintaining abstraction error reporting error handled system propagated manner fits memory abstraction linux thesigbussignal propagate page read errors definition hardware failures sigbus generated freebsd sigsegv signal intended programming error propagate read errors retries instances retrying operation error occurs retrying solve problem case transient error systems benefit greatly employing retries illogical inconsistency error recovery techniques employed inconsistent cases freebsd read error user data page result propagate case pagetouch results kernel panic swapoff buggy implementation observed linux failure handling code buggy result retry making useless suspect failure handling code rarely tested bugs security issues system fairly secure normal operation insecure partial failure linux data read back failed write disk block previous contents returned application possibly delivering data application authorized read failures dealt increasing awareness exploiting transient hardware errors attack systems kernel exposure systems special care kernel-mode data stored disk freebsd corruption kernel thread stack detected result undesirable crashes severe data corruption policies virtual memory systems compared file systems observe kinds systems share problems illogical inconsistency implementation bugs failure handling code points general disregard partial disk errors exposing commodity computer systems data loss data corruption inexplicable crashes linux virtual memory system file systems misses large number write errors virtual memory systems file systems deal corruption errors elegant manner file systems perform sanity checking deal corruption file system data structures protection user data data handled virtual memory systems part freebsd windows leverage important difference file systems virtual memory systems writes required succeed file systems virtual memory systems alternatives choosing page victim writing disk experience experimenting multiple systems helps compare systems insight advantages limitations methodology experience techniques simple applied systems tool rewritten environment find task onerous observed limitation easy identify source disk accesses accesses attributed error recovery unrelated problem occurs read error injected user data page freebsd windows observe retry read retry succeeds application terminated indicating bug retry code closer examination revealed read performed create core dump recover error interesting explore techniques identify exact source disk accesses future work related work techniques developed years inject faults systems techniques fault injection studies explore operating system behavior errors studies explore partial disk failures detail bring techniques policies operating system deal failures study similar spirit brown patterson study failure policies software raid systems software raids type context agnostic behavior virtual memory system differs considerably data types contexts requires complex fault injection analysis study related earlier studies file systems handle partial disk failures file systems virtual memory system important operating system components disks significantly applications aware disk store files disks virtual memory system completely transparent applications requiring virtual memory system robust disk failures conclusions commodity hardware increasingly unreliable due escalating complexity cost pressures operating systems longer assume hardware components hard disks work fail virtual memory system important subsystem modern operating system virtual memory systems designed deal partial disk failures fault injection experiments find current virtual memory systems employ consistent failure policies provide complete recovery partial failures improving failure-awareness systems enable virtualize memory providing applications robust memory abstraction acknowledgments meenali rungta helping experimental setup windows nitin agrawal comments paper anonymous reviewers thoughtful suggestions finally computer systems lab csl providing great environment research work sponsored nsf ccritr- cnsnetwork appliance emc freebsd operating system http freebsd journalling flash file system version http sourceware jffs anderson drive manufacturers typically don talk disk failures personal communication dave anderson seagate anderson dykes riedel interface scsi ata proceedings usenix symposium file storage technologies fast san francisco california april bartlett spainhower commercial fault tolerance tale systems ieee transactions dependable secure computing january barton czeck segall siewiorek fault injection experiments fiat ieee transactions computers april bovetandm cesati understanding linux kernel edition reilly december brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june candea kawamoto fujiki friedman fox microreboot technique cheap recovery proceedings symposium operating systems design implementation osdi pages san francisco california december chou yang chelf hallem engler empirical 
study operating system errors proceedings acm symposium operating systems principles sosp pages banff canada october corbett english goel grcanac kleiman leong sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april engler chen hallem chou chelf bugs deviant behavior general approach inferring errors systems code proceedings acm symposium operating systems principles sosp pages banff canada october govindavajhala appel memory errors attack virtual machine proceedings ieee symposium security privacy page washington usa gray van ingen empirical measurements disk failure rates error rates microsoft research technical report msrtr- december green eide controller flaws version http mindprod eideflaw html february kalbarczyk iyer yang characterization linux kernel behavior error proceedings international conference dependable systems networks dsnpages san francisco california june ishikawa nakajima oikawa hirotsu proactive operating system recovery poster session acm symposium operating systems principles sosp brighton united kingdom october johnson shasha low-overhead high performance buffer management replacement algorithm proceedings international conference large databases vldb pages santiago chile september kanawati kanawati abraham ferrari flexible software-based fault error injection system ieee transactions computing kari saikkonen lombardi detection defective media disks ieee international workshop defect fault tolerance vlsi systems pages venice italy october lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick neville-neil design implementation freebsd operating system addison-wesley professional august musuvathi park chou engler dill cmc pragmatic approach model checking real code proceedings symposium operating systems design implementation osdi boston massachusetts december prabhakaran arpaci-dusseau arpaci-dusseau model-based failure analysis journaling file systems proceedings international conference dependable systems networks dsnpages yokohama japan june prabhakaran bairavasundaram agrawal gunawi arpaci-dusseau arpaci-dusseau iron file systems proceedings acm symposium operating systems principles sosp pages brighton united kingdom october schwarz xin miller long hospodor disk scrubbing large archival storage systems proceedings annual meeting ieee international symposium modeling analysis simulation computer telecommunication systems mascots volendam netherlands october shah elerath reliability analysis disk drive failure mechanisms proceedings annual reliability maintainability symposium pages alexandria january swift bershad levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing lake george york october talagala patterson analysis error behaviour large storage system ieee workshop fault tolerance parallel distributed systems san juan puerto rico april data clinic hard disk failure http dataclinic hard-disk-failures htm tsai iyer measuring fault tolerance ftape fault injection tool international conference modeling techniques tools computer performance evaluation pages september tweedie journaling linux ext file system fourth annual linux expo durham north carolina wehman den haan enhanced ide fast-ata faq http thef-nym sci kun cgi-pieterh atazip atafq html 
logic file systems muthian sivathanu andrea arpaci-dusseau remzi arpaci-dusseau somesh jha google computer sciences department wisconsin madison muthian google fdusseau remzi jhag wisc abstract years innovation systems highly successful improving performance functionality cost complicating interaction disk variety techniques exist ensure consistency integrity system data precise set correctness guarantees provided technique unclear making hard compare reason absence formal framework hampered detailed veri cation system correctness present logical framework modeling interaction system storage system show apply logic represent prove correctness properties demonstrate logic main bene enables reasoning existing system mechanisms allowing developers employ aggressive performance optimizations fear compromising correctness logic simpli introduction adoption system functionality facilitating rigorous proof correctness finally logic helps reason smart storage systems track semantic information system key aspect logic enables incremental modeling signi cantly reducing barrier entry terms actual system designers general framework transforms hitherto esoteric error-prone art system design readily understandable formally veri process introduction reliable data storage cornerstone modern computer systems file systems responsible managing persistent data essential ensure function correctly modern systems evolved extremely complex pieces software incorporating sophisticated performance optimizations features disk key bottleneck system performance optimizations aim minimizing disk access cost complicating interaction system storage system early systems adopted simple update policies easy reason modern systems signi cantly complex interaction disk stemming asynchrony updates metadata work wisconsin-madison reasoning interaction system disk paramount ensuring system corrupts loses data complex update policies precise set guarantees system obscured reasoning behavior translates manual intuitive exploration scenarios developers hoc exploration arduous possibly error-prone recent work found major correctness errors widely systems ext reiserfs jfs paper present formal logic modeling interaction system disk formal modeling show reasoning system correctness simple foolproof formal model illustrated existence similar frameworks areas correctness paramount existing models authentication protocols database reliability database recovery examples general theories modeling concurrent systems exist frameworks general model systems effectively domainspeci logic greatly simpli modeling logic systems serves important purposes enables prove properties existing system designs resulting understanding set guarantees enabling aggressive performance optimizations preserve guarantees signi cantly lowers barrier providing mechanisms functionality system enabling rigorous reasoning correctness absence framework designers tend stick time-tested alternatives finally logic helps design functionality class storage systems facilitating precise characterization proof properties key goal logic framework simplicity order general system designers barrier entry terms applying logic low logic achieves enabling incremental modeling complete model system starting logic simply model piece functionality mechanism isolation prove properties case studies demonstrate utility cacy logic reasoning system correctness properties represent prove soundness important guarantees provided existing techniques system consistency soft updates journaling logic prove linux ext system needlessly conservative transaction commits resulting sub-optimal performance case study demonstrates utility logic enabling aggressive performance optimizations illustrate utility logic developing system functionality propose system mechanism called generation pointers enable consistent undelete les prove correctness design incremental modeling mechanism logic demonstrating simplicity process implement mechanism linux ext system verify correctness logic empirically show inconsistency occur undeletes absence mechanism rest paper organized rst present extended motivation background systems present basic entities logic formalism represent common system properties logic logic prove consistency properties existing systems prove correctness unexploited performance optimization ext reason technique consistent undeletes apply logic semantic disks finally present related work conclude extended motivation systematic framework reasoning interaction system disk multifarious benets describe key applications framework reasoning existing systems important usage scenario logic model existing systems key bene modeling enables clear understanding precise guarantees mechanism assumptions guarantees hold understanding enables correct implementation functionality system layers disk system ensuring adversely interact system assumptions write-back caching disks results reordering writes media negate assumptions journaling based logic enables aggressive performance optimizations reasoning complex interactions hard system developers tend conservative perform unnecessarily waits logic helps remove barrier enabling developers aggressive performance optimizations con dent correctness section analyze real opportunity optimization linux ext system show logic framework prove correctness nal bene logic framework potential implementation-level model checkers clear model expected behavior validate existing system enable comprehensive cient model checking current technique relying fsck mechanism expensive cost fsck explored state limits scalability model checking building system functionality recovery consistency traditionally viewed tricky issues reason classic illustration view arises database recovery widely aries algorithm pointed correctness issues earlier proposals ironically success aries stalled innovation database recovery due dif culty proving correctness techniques innovation system deals interaction disk correctness implications inertia changing time-tested alternatives sti incorporation functionality systems systematic framework reason piece functionality greatly reduce barrier entry section propose system functionality logic prove correctness illustrate cacy logic reasoning functionality examine section common system feature journaling show starting simple logical model journaling systematically arrive corner cases handled involve complex interactions developers linux ext designing semantically-smart disks logic framework signi cantly simpli reasoning class storage systems called semantically-smart disk systems provide enhanced functionality inferring system operations inferring information accurately underneath modern systems complex dependent dynamic system properties section show logic simplify reasoning semantic disk turn enable aggressive functionality background system organizes disk blocks logical les directories order map blocks logical entities les system tracks forms metadata section rst describe forms metadata systems track discuss issue system consistency finally describe asynchrony systems major source complexity interaction disk file system metadata file system metadata classi types directories directories map logical perle metadata mapped directory directories enable hierarchy les user opens path system locates perle metadata reading directory path required file metadata file metadata information speci examples information set disk blocks comprise size systems fat metadata embedded directory entries systems metadata stored separately inodes pointed directory entries pointers metadata disk blocks indirected indirect pointer blocks case large les allocation structures file systems manage resources disk set free blocks allocated les track resources systems maintain structures bitmaps free lists point free resource instances addition systems track metadata super block focus types file system consistency proper operation internal metadata system data blocks consistent state metadata consistency state metadata structures obeys set invariants system relies directory entry point valid metadata structure directory points metadata uninitialized marked free system inconsistent systems provide metadata consistency crucial correct operation stronger form consistency data consistency system guarantees data block contents correspond metadata structures point discuss issue section modern systems linux ext 
reiserfs provide data consistency file system asynchrony important characteristic modern systems asynchrony exhibit updates data metadata updates simply buffered memory written disk delay interval reordering writes asynchrony crucial performance complicates consistency management due asynchrony system crash leads state arbitrary subset updates applied disk potentially leading inconsistent on-disk state asynchrony updates principal reason complexity interaction system disk raison etre logic basic entities notations section basic entities constitute system logic present notations section build entities present formalism operation system basic entities basic entities model containers pointers generations system simply collection containers containers linked pointers system differs exact types containers nes relationship container types abstraction based containers pointers general describe system containers system freed reused container considered free pointed container live instance container reuse free called generation generation speci incarnation container generations reused container reused previous generation container freed generation container life generation fully ned container logical generation number tracks times container reused note generation refer contents container abstraction current incarnation contents change affecting generation illustrate notion containers generations simple typical unix-based system system xed set designated inodes inode slot container point inode slot inode generation corresponds speci deleted inode generation deleted forever inode container simply marked free created reuse inode container logically inode generation note single container inode point multiple containers data blocks single container pointed multiple containers hard links unix systems notations notations depict basic entities relationships listed table note notations table ned section containers denoted upper case letters generations denoted lower case letters entity description represents container generation symbol description set entities point container set entities pointed container jaj container tracks container live set entities point generation set entities pointed generation denotes container pointer denotes entity points kth epoch container type kth epoch container generation kth epoch container container generation generation container table notations containers generations pointer denoted symbol container pointer container paper pointers containers live section relax assumption introduce notation pointers involving dead containers attributes containers make logic expressive modern systems extend vocabulary attributes container generation attributes container epoch epoch container ned time contents container change memory epoch incremented system sets elds inode step results epoch inode container system batch multiple contents due buffering set epochs visible disk subset total set epochs container denote epoch superscript notation denotes kth epoch note nition epoch expressivity logic imply system tracks epoch note distinction epoch generation generation change occurs reuse container epoch change contents container reused type containers type type container static change lifetime system dynamic container belong types points time ffsbased systems inode containers statically typed block containers change type data directory indirect pointers denote type container notation shared unshared container pointed container called shared container container pointer leading unshared default assume containers shared denote unshared containers operator unshared note unshared property container type system ensures container belonging type unshared pointer pointing systems designate data block containers unshared memory disk versions containers system manage structures domains volatile memory disk accessing contents container system read on-disk version container memory subsequently system makes modi cations inmemory copy container modi contents periodically written disk system writes modi container disk contents container memory disk formalism present formal model operation system rst formulate logic terms beliefs actions introduce operators logic proof system basic axioms logic beliefs state system modeled beliefs belief represents state memory disk statement enclosed represents belief beliefs memory beliefs disk beliefs denoted fgm fgd bgm belief system memory container points memory bgd means disk belief timing belief begins hold determined context formula logic describe subsection terms timing belief ned relative beliefs actions speci formula isolated belief temporal dimension memory beliefs represent state system tracks memory on-disk beliefs ned belief holds disk time crash system conclude belief purely based scan on-disk state time ondisk beliefs solely dependent on-disk data system manages free reuse containers beliefs terms generations fak bjgm valid note refers generation container on-disk beliefs deal containers generation information lost disk sections propose techniques expose generation information disk show database-aware semantically-smart enables storage improved muthian guarantees sivathanu actions lakshmi bairavasundaramy component andrea logic arpaci-dusseauy actions remzi result arpaci-dusseauy system google state ycomputer actions sciences alter department set beliefs wisconsin hold madison abstract recent time research demonstrated actions potential bene building storage arrays understand systems semantically-smart disk systems knowledge system structures operations improve performance availability security ways precluded traditional storage system architecture paper study applicability semantically smart disk technology underneath database management systems case studies analyze differences building database-aware storage semantically-smart disk systems successfully applied underneath database techniques log snooping explicit access statistics needed introduction processing power increasing modern storage systems symmetrix storage array highend raid emc processors memory ability leverage computational power traditional storage systems limited due narrow blockbased interface protocols scsi storage arrays receive simplest commands read write range blocks storage system knowledge blocks part block live dead bridge information gap recent research proposed idea semantically smart disk system learns embedded knowledge system semantic information storage system vendors build functional reliable higher-performing secure storage systems exploiting knowledge directory structures storage system deliver improved data availability failure previous research semantically smart disk systems assumed commodity work wisconsin-madison system linux ext linux ext netbsd ffs windows fat windows ntfs interacting disk paper explore techniques semanticallysmart disk systems operate beneath database management systems dbms database systems form signi important group clients storage systems bene semantically smart storage applied realm operating beneath system database semantically smart disk system performs similar operations tracking table block allocated dbms tracks information organizes data disk differently system systems record metadata statistics recent access modi time dbms specialized track general statistics system workloads directory structure reasonable approximation semantic groupings users place related les single directory dbms semantic grouping tables indexes dynamic depending query workload general nding differences fundamental require semantically smart storage build database-aware storage investigate techniques required systems explore log snooping storage system observes write-ahead log wal records written dbms monitoring log storage system observe operation performed dbms effect reaches disk explore bene dbms explicitly gather access statistics write statistics storage simple add statistics dbms investigate database-aware storage implement analyze case studies found work underneath systems study improve storage system availability d-graid raid system degrades gracefully failure implement dbms-specialized version faded storage system guarantees data unrecoverable user deleted finally explore improve second-level storage-array cache hit rates technique x-ray experience semantically-smart disks work underneath database systems cases database systems systems semantically-smart storage secure delete case presence transactional semantics dbms disk accurately track dynamic information result functionality requires absolutely correct inferences implemented changing dbms contrast functionality required system case studies d-graid x-ray dbms supply desired access information storage system result results obtained slightly modify dbms rest paper organized section review related work database-aware storage discuss advantages disadvantages semantically-smart disks section describe general techniques needed semantic disk extract information dbms sections present case studies finally discuss range techniques section conclude section background placing intelligence disk systems database systems favor years summary work area keeton dissertation page earliest examples idea logic track devices proposed disk computational ability head natural application lter data passes rest system idea database-speci machines refuted boral dewitt primary reason failure approaches required non-commodity components outperformed technology moved ahead worse database vendors rewrite substantial code base advantage speci features offered specialized architectures processing power faster cheaper idea active disks focus recent work includes acharya riedel efforts portions applications downloaded ned logic read operation system read contents on-disk container current generation memory system container memory modify read contents memory on-disk fagm fagd write operation results ushing current contents container disk operation contents memory on-disk fagd fagm ordering beliefs actions fundamental aspect interaction system disk ordering actions ordering actions determines order beliefs established order actions resulting beliefs operators means occurred time note ordering beliefs notation indicating event creation belief state existence belief belief agm represents event system assigns pointers special ordering operator called precedes belief left operator operator ned means belief occurs means belief holds occurs implies intermediate action event invalidates belief operator transitive imply belief hold necessarily note simply shortcut note implies beliefs grouped parentheses semantics precedes group beliefs precedes belief belief parentheses precedes belief proof system primitives sequencing beliefs actions rules formulas logic terms implication event sequence sequence traditional operators implication double implication logical combine sequences logical rule notation means time event action occurs event occurs point occurrence rule occurs absolute time occur order occurs rule valid occurred general left hand side rule involves complex expression disjunction components belief rhs holds point occurrence rst event makes lhs true occurrence makes sequence true rule rule denotes time occurs occurred note rule event occurs sides event constitutes temporal point referring time instant lhs rhs temporal interpretation identical events crucial rule serving intended implication rhs refer instant rules logical proofs event sequence substitution rule subsequence occurs sequence events logically implies event apply rule event sequence replacing subsequence matches left half rule half rule postulate proof system enables deriving invariants system building basic axioms basic axioms subsection present axioms govern transition beliefs memory disk container points memory current generation points memory fbx agm agm points memory write lead disk belief points agm write agd converse states disk belief implies belief rst occurred memory agd agm agd similarly points disk read result system inheriting belief agd read agm on-disk contents container pertain epoch generation pointed generation memory write converse holds faygd write faygd akgm write faygd note refers generation rule generation 
points akg ajg hold memory points time container freed instants akgm ajgm akgm ajgm note rule includes scenario intermediate generation occurs container pointed disk subsequently system removes pointer memory write lead disk belief point agd bgm write bgd unshared container write lead disk belief container points free agd write dynamically typed container type instants freed xgm ygm xgm ygm completeness notations notations discussed section cover wide range set behaviors model system means complete set notations model aspect system show section section speci system features require notations main contribution paper lies putting framework formally reason system correctness notations introduced speci system features framework apply modi cation connections temporal logic logic bears similarity linear temporal logic syntax linear temporal logic ltl dened formula ltl formula set atomic propositions ltl formulas ltl formulas nition time future release temporal operators formalism fragment ltl set atomic propositions consists memory disk beliefs actions temporal operators allowed formalism equivalent execution sequence states ltl formula denotes true execution system satis ltl formula executions satisfy precise semantics satisfaction relation meaning found chapter semantics formalism standard semantics ltl proof system set axioms section desired property data consistency property section prove axioms denoted system satis properties set satisfy property file system properties systems provide guarantees update behavior guarantee translates rules logical model system complement basic rules reasoning system section discuss properties container exclusivity system exhibits container exclusivity guarantees on-disk container dirty copy container contents system cache requires system ensure in-memory contents container change container written disk systems bsd ffs linux ext vfat exhibit container exclusivity journaling systems ext exhibit property equations refer containers memory refer latest epoch container memory case systems obey container exclusivity means time container latest epoch memory points similarly write means latest epoch time written referring speci version epoch notation container exclusivity holds epoch container exists memory container exclusivity stronger converse agd agm agd assume unshared stronger equation equation disk belief agd hold written system note containers typical systems data blocks unshared agd agm write agd reuse ordering system exhibits reuse ordering ensures reusing container commits freed state container disk pointed generation memory freed generation made point freed state container generation pointer removed written disk reuse occurs agm agm write agm reuse results commit freed state extend rule agm agm write agm ffs soft updates linux ext examples systems exhibit reuse ordering pointer ordering system exhibits pointer ordering ensures writing container disk system writes containers pointed agm write agm write write ffs soft updates system exhibits pointer ordering modeling existing systems ned basic formalism logic proceed logic model reason system behaviors section present proofs properties important system consistency discuss data consistency problem system model journaling system reason non-rollback property journaling system data consistency rst problem data consistency system crash data consistency contents data block containers consistent metadata data blocks words end data system recovers crash assume metadata container pointers data blocks respective data block container disk belief points holds on-disk contents written generation epoch pointed time past kth generation memory generation rule summarizes fbx agd faygd fbx akgm fbx agd prove system exhibits reuse ordering pointer ordering suffers data consistency violation show system obey ordering data consistency compromised crashes simplicity make assumption data containers system nonshared les share data block pointers assume system obeys container exclusivity property modern systems ext vfat properties block exclusivity fbx agd fbx agm fbx agd rewrite rule fbx akgm fbx agd faygd rule hold means represented generation points generation contents written generation case data corruption show rule hold assume negation prove reachable sequence valid system actions faygd disks write event sequences implied lhs fbx akgm fbx agd write order prove prove interleaving sequences clause invalid disprove prove interleavings valid fbx akgm fbx agd event occur events due container exclusivity unshared similarly fbx akgm occur write interleavings fbx akgm fbx agd write write fbx akgm fbx agd case applying akgm fbx agd write applying akgm fbx agd write step valid sequence system execution generation freed due delete represented generation subsequent generation block reallocated represented generation memory shown violation occur assume system obeys reuse ordering equation additional constraint equation imply akgm fbx agd write write akgm fbx agd write facgd contradiction initial assumption started bgd reuse ordering shown scenario arise case write fbx akgm fbx agd applying write akgm fbx agd eqn write akgm fbx agd valid system sequence generation pointed data block generation generation deleted generation container assigned generation consistency violation occur scenario interestingly apply write write akgm fbx agd apply case belief agd hold rule led belief immediately write belief overwritten fbx agd sequence invalidate sequence reuse ordering guarantee data consistency case make assumption system obeys pointer ordering assume unshared container exclusivity holds apply equation write akgm write fbx agd applying pointer ordering rule eqn write akgm write write fbx agd agm write faygd write fbx agd faygd fbx agd contradiction implies contents disk belong generation started assumption reuse ordering pointer ordering system suffers data consistency violation system obey ordering ext data consistency compromised crashes note inconsistency fundamental xed scan-based consistency tools fsck veri inconsistency occurs practice reproduce case experimentally ext system modeling system journaling extend logic rules behavior journaling system model reason key property journaling system journaling technique commonly systems ensure metadata consistency single system operation spans multiple metadata structures system groups transaction guarantees transaction commits atomically preserving consistency provide atomicity system rst writes writeahead log wal propagates actual on-disk location transaction committed log transaction committed logged special commit record written log indicating completion transaction system recovers crash checkpointing process replays belong committed transactions model journaling logical transaction object determines set log record containers belong transaction logically pointers log copies containers modi transaction denote log copy journaled container symbol top container container log journal system note assume physical logging block-level logging ext physical realization transaction object commit record logically points containers changed transaction wal property hold commit container written log copy modi containers transaction points written commit container wal property leads rules axgm write axgm write write axgm write axgm write write rst rule states transaction committed commit record written containers belonging transaction 
written disk rule states on-disk home copy container written transaction container modi committed disk note unlike normal pointers considered point containers generations pointers container rules point epochs epoch pointers commit record speci epoch snapshot container replay checkpointing process depicted rules axgd ftgd write faxgd axgd aygd write faygd rst rule container part transaction transaction committed disk on-disk copy container updated logged copy pertaining transaction rule container part multiple committed transactions on-disk copy container updated copy pertaining transactions belief transitions hold bxgm fbx agm write fbx agd axgm write faxgd rule states points belongs transaction commit leads disk belief fbx agd rule disk belief faxgd holds immediately commit transaction part creation belief require checkpoint write happen disk belief pertains belief system reach start current disk state journaling systems containers types journaled updates containers directly disk transaction machinery proofs cases complete journaling containers journaled selective journaling containers type selective case address possibility container changing type journaled type non-journaled type vice versa container belongs journaling type converse equation fbx agd bxgm fbx agm write fbx agd show complete journaling data inconsistency occurs omit due space constraints non-rollback property introduce property called non-rollback pertinent system consistency rst formally property reason conditions required hold journaling system non-rollback property states contents container disk overwritten older contents previous epoch property expressed faxgd faygd faxgm faygm rule states on-disk contents move epoch logically imply epoch occurred epoch memory non-rollback property crucial journaling systems absence property lead data corruption proof logically derive corner cases handled property hold show journal revoke records effectively ensure disk believes xth epoch possibilities type journaled type belonged transaction disk observed commit record transaction belief faxgd occurs immediately commit point actual contents written system part checkpoint propagation actual on-disk location re-establishing belief faxgd set journaled types faxgd jgm faxgm axgm write faxgd write faxgd possibility type journaled case disk learnt prior commit faxgd jgm faxgm write faxgd journaled rst assume belong journaled type prove non-rollback property lhs faxgd faygd journaled sequence events led beliefs faxgm axgm write faxgd write faxgd faygm aygm write faygd write faygd omitting write actions sequences simplicity sequences events faxgm faxgd faxgd faygm faygd faygd note sequence instances disk belief created rst instance created transaction committed instance checkpoint propagation time snapshot-based coarse-grained journaling systems ext transactions committed order epoch occurred committed rst instance faxgd occur rst instance faygd property true journaling checkpointing in-order committed transactions copies data version pertaining transaction propagated checkpoint sequences events lead interleavings depending epoch occurs epoch vice versa ordering epoch xed rest events constrained single sequence interleaving faxgm faygm faxgd faygd faygd faxgm faygm interleaving faygm faxgm faygd faxgd faxgd faygd faxgd interleaving results contradiction initial statement started faxgd faygd rst interleaving legal sequences events combined rst interleaving implies faxgm faygm proved epochs journaled non-rollback property holds journaled case type epochs belongs journaled type start statement faxgd faygd equations sequences events faygm aygm write faygd write faygd faxgm write faxgd omitting write actions sake readability sequences faygm faygd faygd faxgm faxgd prove non-rollback property show interleaving sequences faygm faxgm results contradiction co-exist faxgd faygd interleavings faygm faxgm faygm faxgm faxgd faygd faygd faygm faygd faxgm faxgd faygd faygm faygd faygd faxgm faxgd faygm faxgm faygd faxgd faygd faygm faxgm faygd faygd faxgd faygm faygd faxgm faygd faxgd scenarios imply faygd faxgd invalid interleavings scenarios valid interleavings contradict initial assumption disk beliefs time imply faygm faxgm scenarios violate non-rollback property dynamic typing journaling mechanism guarantee nonrollback due violation contents corrupted stale metadata generations scenario occur checkpoint propagation earlier epoch journaled occurs overwritten non-journaled epoch prevent impose checkpoint propagation container context transaction happen on-disk contents container updated commit journal revoke records ext precisely guarantee revoke record encountered log replay pre-scan log block propagated actual disk location scenario epoch committed disk transaction modi earlier epoch committed prevent form reuse ordering imposes container type reused memory transaction freed previous generation committed transactions commit order freeing transaction occur transaction guarantee jgm jgm faygm faxgm faygm write faxgm rule scenario handled revoke record solution properties non-rollback property holds redundant synchrony ext examine performance problem ext system transaction commit procedure arti cially limits parallelism due redundant synchrony disk writes ordered mode ext guarantees newly created point stale data blocks crash ext ensures guarantee ordering commit procedure transaction committed ext rst writes disk data blocks allocated transaction waits writes complete writes journal blocks disk waits complete writes commit block inode container data block container transaction commit container commit procedure ext expressed equation fix fkgm ixgm write fix fkgm ixgm write write write examine condition ensure no-stale-data guarantee rst formally depict guarantee ext ordered mode seeks provide equation fix fkgm fix fgd ffygd fix fgd equation states disk acquires belief fix contents data container disk pertain generation pointed memory note ext obeys reuse ordering ordered mode guarantee cater case free data block container allocated prove equation examining conditions hold equation true lhs equation fix fkgm fix fgd applying equation fix fkgm ixgm write fix fgd applying equation fix fkgm ixgm write write write fix fgd equation fix fkgm ixgm ygd write write fix fgd ygd fix fgd current ext commit procedure equation guarantees no-stale-data property waits procedure required reorder actions write write fix fkgm ixgm write write write fix fgd applying equation ygd fix fgd ordering actions write write inconsequential guarantee ext ordered mode attempts provide conclude wait ext employs write data blocks redundant unnecessarily limits parallelism data journal writes severe performance implications settings log stored separate disk illustrated previous work speci points general problem system design developers rigorous frameworks reason correctness tend conservative conservatism translates unexploited opportunities performance optimization systematic framework enables aggressive optimizations ensuring correctness support consistent undelete section demonstrate logic enables quickly formulate prove properties system features mechanisms explore functionality traditionally considered part core system design ability undelete deleted les consistency guarantees ability recover deleted les demonstrated large number tools purpose tools rebuild deleted les scavenging on-disk metadata extent systems freed metadata containers simply marked free unix system block pointers deleted inode blocks belong deleted existing tools undelete guarantee consistency assert recovered contents 
valid undelete fundamentally best-effort les recovered blocks subsequently reused user trustworthy recovered contents demonstrate logic existing systems consistent undelete impossible provide simple solution prove solution guarantees consistent undelete finally present implementation solution ext undelete existing systems model undelete logic express pointers containers holding dead generation introduce notation pointer call dead pointer operator container denotes set dead live entities pointing container undel undelete action container undelete process summarized equation undel fbx agd fbggd fbx agd fby agd words dead free container points disk container alive dead pointing undelete makes generation live makes point guarantee hold consistency dead pointer brought alive ondisk contents time pointer brought alive correspond generation epoch originally pointed memory similar data consistency formulation fbx akgm fbx agd fby agd fbx agd fazgd note clause required lhs cover case generation brought life true undelete show guarantee hold necessarily negation rhs fazgd show condition co-exist conditions required undelete equation words show undel fbx agd fbggd fazgd arise valid system execution utilize implications proof fbx agd fbx akgm write fazgd write interleaving event sequences write fbx akgm write valid system sequence represented generation points written disk block freed killing generation generation allocated generation deleted written disk disk beliefs fbx agd fazgd initial state disk sequence simultaneously lead disk belief fbggd shown conditions fbx agd fbggd fazgd hold simultaneously undelete point lead violation consistency guarantee associate stale generation undeleted shown reuse ordering pointer ordering guarantee consistency case undelete generation pointers propose notion generation pointers show pointers consistent undelete guaranteed assumed pointers disk point containers discussed section pointer pointed speci generation leads set system properties implement generation pointers on-disk container generation number incremented time container reused addition on-disk pointer embed generation number addition container generation pointers on-disk contents container implicitly generation fbkgd valid belief means disk contents belong generation generation pointers criterion undelete undel fbx akgd fakgd fbx akgd fby akgd introduce additional constraint fazgd left hand side equation previous subsection fbx akgd fakgd fazgd denote on-disk container holds generation number fahgd equation fbx akgd fakgd fahgd contradiction means ondisk container generations simultaneously undelete occur scenario alternatively agged inconsistent undeletes occurring generation pointers consistent implementation undelete ext proof consistent undelete implemented generation pointer mechanism linux ext block generation number incremented time block reused generation numbers maintained separate set blocks ensuring atomic commit generation number block data straightforward data journaling mode ext simply add generation update create transaction block pointers inode extended generation number block implemented tool undelete scans on-disk structures restoring les undeleted consistently speci cally restored generation information metadata block pointers match block generation data blocks ran simple microbenchmark creating deleting directories linux kernel source tree observed roughly deleted les les roughly detected inconsistent undeletable remaining les successfully undeleted illustrates scenario proved section occurs practice undelete tool generation information wrongly restore les corrupt misleading data application semantic disks interesting application logic framework systems enables reasoning recently proposed class storage systems called semanticallysmart disk systems sds sds exploits system information storage system provide functionality admitted authors reasoning correctness knowledge tracked semantic disk hard formalism memory disk beliefs sds model extra system state tracked sds essentially disk belief section rst logic explore feasibility tracking block type semantic disk show usage generation pointers system simpli information tracking sds block typing important piece information required semantic disk type disk container identifying type statically-typed containers straightforward dynamically typed containers hard deal type dynamically typed container determined contents parent container indirect pointer block identi observing parent inode block indirect pointer eld tracking dynamically typed containers requires correlating type information typedetermining parent information interpret contents dynamic container accurate type detection sds guarantee hold kgd kgm words disk interprets contents epoch belonging type contents belonged type memory guarantees disk wrongly interpret contents normal data block container indirect block container note equation impose guarantee disk identies type container states association type contents correct prove rst state algorithm disk arrives belief type sds snoops metadata traf type-determining containers inodes container written observes pointers container concludes type pointers assume pointer type points container disk examines container written time freed interprets current contents belonging type written time contents type equation kgd fby agd faxgd words interpret belonging type disk container points current on-disk epoch type function abstracts indication disk contents epoch order associate contents type explore logical events led components side equation applying fby agd fby agm fby agd fby agm kgm fby agd similarly component faxgd faxgd write faxgd verify guarantee equation assume hold observe leads valid scenario add clause jgm equation equation prove fby agd faxgd jgm event sequences fby agm kgm fby agd jgm write type epoch unique write container implies type jgm write jgm write sequences interleaved ways epoch occurs epoch kgm interleaving fby agm kgm fby agd jgm write fby agm kgm fby agd jgm write valid sequence container freed disk acquired belieffb agand version written actual type changed memory leading incorrect interpretation belonging type order prevent scenario simply reuse ordering rule rule sequence imply fby agm kgm fby agd write jgm write fby agm kgm fby agd jgm write written disk treating free wrongly associate type interleaving proceeding similarly interleaving epoch occurs assigned type arrive sequence jgm write fby agm kgm fby agd simply applying reuse ordering rule prevent sequence stronger form reuse ordering freed state includes containers pointed allocation structure jaj tracking liveness rule sequence jgm write write jaj fby agm kgm fby agd add behavior sds states sds observes allocation structure indicating free inherits belief free write jaj applying sds operation eqn jgm write fby agm kgm fby agd sequence sds observe write treated free associate type subsequently written shown sds accurately track dynamic type underneath system ordering guarantees shown system exhibits strong form reuse ordering dynamic type detection made reliable sds utility generation pointers subsection explore utility systemlevel generation pointers context sds illustrate utility show tracking dynamic type sds straightforward system tracks generation pointers generation pointers equation kgd fby aggd faggd causal event sequences explored previous subsection fby aggm kgm fby aggd jgm write sequences imply generation types violates rule straightaway arrive contradiction proves violation 
rule occur related work previous work recognized modeling complex systems formal frameworks order facilitate proving correctness properties logical framework reasoning authentication protocols proposed burrows related work spirit paper authors formulate domain-speci logic proof system authentication showing protocols veri simple logical derivations domain-speci formal models exist areas database recovery database reliability body related work involves generic frameworks modeling computer systems wellknown tla framework automaton framework frameworks general model complex systems generality curse modeling aspects system extent paper tedious generic framework tailoring framework domain-speci knowledge makes simpler reason properties framework signi cantly lowering barrier entry terms adopting framework speci cations proofs logic lines contrast thousands lines tla speci cations automated theorem-proving model checkers bene generic framework tla previous work explored veri cation correctness system implementations recent body work model checking verify implementations body work complementary logic framework logic framework build model invariants hold model implementation veri finally system properties listed section identi previous work soft updates recent work semantic disks conclusions dependability computer systems important essential systematic formal frameworks verify reason correctness systems critical component system dependability formal veri cation correctness largely making systems vulnerable hidden errors absence formal framework sti innovation skepticism correctness proposals proclivity stick time-tested alternatives paper step bridging gap system design showing logical framework substantially simplify systematize process verifying system correctness acknowledgements lakshmi bairavasundaram nathan burnett timothy denehy rajasekar krishnamurthy florentina popovici vijayan prabhakaran vinod yegneswaran comments earlier drafts paper anonymous reviewers excellent feedback comments greatly improved paper work sponsored nsf ccrccr- ccrngs- itribm network appliance emc attie lynch dynamic input output automata formal model dynamic systems acm podc jfs overview ibm developerworks library jfs html bjorner browne colon finkbeiner manna sipma uribe verifying temporal properties reactive systems step tutorial formal methods system design fmsd burrows abadi needham logic authentication acm sosp pages clarke grumberg peled model checking mit press ganger mckusick soules patt soft updates solution metadata update problem file systems acm tocs hadzilacos theory reliability database systems acm hagmann reimplementing cedar file system logging group commit sosp nov kuo model veri cation data manager based aries acm trans database systems lamport temporal logic actions acm trans program lang syst mckusick joy lef fabry fast file system unix acm transactions computer systems august mogul update policy usenix summer boston june mohan haderle lindsay pirahesh aries transaction recovery method supporting finegranularity locking artial rollbacks write-ahead logging acm tods march musuvathi park chou engler dill cmc pragmatic approach model checking real code osdi dec pnueli temporal semantics concurrent programs theoretical computer science tcs prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems usenix r-undelete r-undelete file recovery software http undelete reiser reiserfs namesys restorer restorer data recovery software http bitmart net sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level osdi pages san francisco december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid fast tweedie future directions ext filesystem freenix monterey june tweedie ext journaling file system http olstrans sourceforge net release ols -ext ols ext html july yang twohey engler musuvathi model checking find file system errors osdi dec manolios lamport model checking tla specications lecture notes computer science 
tailoring disk running program research focuses partition applications host disk cpus minimize data transferred contrast previous work semantically-smart approach require specialized hardware components sophisticated programming environments high-end storage arrays good match technology multiple processors vast quantities memory building semantic knowledge higher-level systems storage array bene drawbacks main bene semantic-disk approach increases functionality placing high-level semantic knowledge storage system enables systems require low level control storage array high level knowledge dbms systems precluded traditional storage architectures previous research shown semantic disks improve performance layout caching improve reliability provide additional security guarantees semantically-smart approach leads concerns concern processing required disk system researchers noted trend increasing intelligence disk systems modern storage arrays exhibit fruits moore law emc symmetrix storage server con gured processors ram resources idle nonetheless hint relative simplicity adding intelligence concern placing semantic knowledge disk system ties disk system intimately system dbms dbms on-disk structure storage system change systems ondisk formats rarely change format ext system signi cantly changed years existence current modi cations great pains preserve full backwards compatibility older versions system case dbms format concern gain insight storage vendor deliver rmware updates order pace dbms-level studied development postgres times revision history dump restore required migrate version found dump restore needed months average frequent expected commercial databases store terabytes data requiring dump restore migrate tolerable users recent versions oracle great lengths avoid on-disk format nal concern storage system semantic knowledge layer system dbms possibly run fortunately systems database systems supported cover large fraction market functionality semantic disk independent layer small portion code handle issues speci system dbms finally storage vendor reduce burden supporting database platforms target single important database oracle provide standard raid functionality systems interestingly highend raid systems perform bare minimum semantically-smart behavior storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically summary storage vendors commit resources support database technology database-aware techniques implement powerful functionality storage system leverage higher-level semantic information system dbms running top section describe types information semantic disk requires underneath dbms discuss information acquired database-speci semantic information broadly categorized types static dynamic experience primarily predator dbms built shore storage manager illustrate techniques speci examples predator techniques general database systems static information static information comprised facts database change database running storage system obtain static information knowledge embedded rmware explicitly communicated out-of-band channel system installation cases static information describes format on-disk structures knowing format database log record semantic disk observe update operation disk knowing structure b-tree pages disk determine internal pages versus leaf pages nally understanding format data pages semantic disk perform operations scanning page holes byte ranges deleted cases static information describes location on-disk structures predator knowing names ids system catalog tables rootindex sindxs table dynamic information dynamic information pertains information dbms continually operation examples dynamic information include set disk 
blocks allocated table disk block belongs table index unlike static information dynamic information continually tracked disk track dynamic information semantic disk utilizes static information data structure formats monitor key data structures correlated higher level operations systems databases buffer reorder writes performing accurate inference higher level operations complex solve problem technique log snooping storage system observes log records written dbms log snooping storage system leverages fact database write-ahead log wal track operation on-disk contents wal property log operation reaches disk effect operation strong ordering guarantee makes inferences underneath dbms accurate straightforward implementation log snooping assume log record log sequence number lsn lsn byte offset start record log volume lsn semantic disk accurately infer exact ordering events occurred database presence group commits log blocks arrive order order events disk maintains expected lsn pointer lsn log record expected disk semantic disk receives write request log block block log record semantic disk processes log record advances expected lsn pointer point record log blocks arrive order semantic disk utilizes lsn ordering process blocks order log blocks arriving order deferred expected lsn reaches block describe detail implementation database-aware storage log snooping infer important pieces dynamic information transaction status block ownership block type relationships blocks describe importance nal piece dynamic information access statistics transaction status basic piece dynamic information current state transaction written disk transaction pending committed pending transaction aborted performing work transaction semantic disk choose pessimistically recognize committed transactions optimistically begin work pending transactions trade-offs pessimistic optimistic approaches pessimistic approach semantic disk implements functionality requires correctness implementing secure delete section semantic disk shred data belonging pending transaction transaction abort dbms require data pessimistic approach worse performance optimistic approach pessimistic version delay work require signi amount buffering optimistic approach bene cial aborts rare dbms implements group commits delay committing individual transactions long period determining status transaction straightforward log snooping semantic disk observes log record written adds list pending transactions disk observes commit record log determines transactions committed moves committed list block ownership semantic disk understand logical grouping blocks tables indices involves associating block table index store logically owns block performing association semantic disk straight forward effect allocating block recoverable dbms rst logs operation performing allocation semantic disk observes traf disk block simple associate block owning table index show cases suf cient semantic disk map blocks store owning table cases semantic disk map store actual table index allocating block shore writes create extlog record block number owning store semantic disk observes log entry records block number store internal block store hash table map store actual table index disk static knowledge system catalog tables predator mapping maintained tree called rootindex logical store statically disk observes btree add records log rootindex semantic disk identify newly created mappings add store hash table block type piece information semantic disk type store block block data page index page track information semantic disk watches updates system catalog tables names part static information disk predator sindxs table indexes database tuple sindxs index table attribute index built semantic disk detects inserts table page insert records log semantic disk determine block part table index owning store information derived sindxs table block relationships type information consists relationships blocks relationships semantic disk table set indices built table stated predator association indices tables sindxs catalog table semantic disk consult information derived sindxs table associate table indices vice versa access patterns addition previous dynamic information semantic disk tables indexes accessed current workload transaction status block ownership block type block relationships inferred easily log snooping access patterns dif cult infer inferring access patterns found easy underneath general-purpose system fact set les lies directory implicitly conveys information storage system les accessed similarly systems track time accessed periodically write information disk modern database systems track access statistics performance diagnosis statistics gathered coarse granularity automatic workload repository oracle maintains access statistics experience revealed dbms track types statistics information optimize behavior dbms write statistics periodically disk additional catalog tables transactional avoid logging overhead basic statistic dbms communicate semantic disk access time block table statistic derive statistics statistic summarizes access correlation entities tables indexes dbms record query set tables indexes accessed correlation statistics capture semantic groupings tables collocating related tables finally statistic tracks access counts number queries accessed table duration piece information conveys importance tables indexes case studies actual static dynamic information required database-aware disk depends functionality disk implementing investigate number case studies previously implemented underneath systems investigate d-graid raid system degrades gracefully failure implement faded guarantees data unrecoverable user deletes finally explore x-ray implements second-level storage-array cache partial availability d-graid rst case study implement d-graid underneath dbms d-graid semantically-smart storage system lays blocks ensures graceful degradation availability unexpected multiple failures d-graid enables continued operation system complete unavailability multiple failures previous work shown approach signi cantly improves availability systems section begin reviewing motivation partial availability d-graid summarize past experience implementing d-graid underneath systems describe techniques implementing d-graid underneath dbms finally evaluate version d-graid discuss lessons motivation importance data availability emphasized settings downtime cost millions dollars hour cope failures systems database systems store data raid arrays employ redundancy automatically recover small number disk failures existing raid schemes effectively handle catastrophic failures number failures exceeds tolerance threshold array multiple failures occur due primary reasons faults correlated single controller fault component error render number disks unavailable system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure page extra failures existing raid schemes lead complete unavailability data contents array restored backup effect severe large arrays disks roughly raidarray fully operational disk system database completely unavailable availability cliff arises traditional storage systems employ simplistic layout techniques striping oblivious semantic importance blocks relationships blocks excess failures occur odds semantically-meaningful data table remaining low modern storage arrays export abstract logical volumes single disk system dbms control data placement ensure semantically-meaningful data remains single disk failure filesystem-aware d-graid basic goal d-graid make semantically meaningful fragments data failures workloads access fragments run completion oblivious data loss parts system working top redundancy technique raidd-graid graceful degradation number failures exceed tolerance threshold redundancy technique implemented 
d-graid system found layout techniques important fault-isolated data placement needed ensure semantic fragments remain entirety fault isolated placement entire semantic fragment collocated single disk found system workloads reasonable semantic fragment consists single entirety data blocks inode block potentially indirect blocks les single directory selective replication needed ensure essential meta-data data required system context essential meta-data found consist directories data inode blocks structures system superblock bitmap blocks essential data found system binaries directories usr bin bin lib access-driven diffusion popular data striped disks needed improve throughput large single disk found popular data dynamically identi tracking logical segments semantic knowledge access-driven diffusion implemented manner beneath system dbms database-aware d-graid describe techniques implementing graid underneath dbms explore techniques fault-isolated data placement target widely database usage patterns moderately-sized tables coarse-grained fragmentation large tables ne-grained fragmentation explore structures selectively replicated describe implementation accessdriven diffusion finally describe infallible writes technique required systems identifying semantic fragments fundamental differences dbms versus system dbms extremely large tables single disk describe techniques separately moderately-sized tables coarse-grained fragmentation entire table disk large tables ne-grained fragmentation stripe tables indexes multiple disks dbms queries performed directly impact tables indexes accessed describe semantic groupings affected popular types queries scans index lookups joins fault-isolated placement coarse-grained simplest case occurs database large number moderately-sized tables situation semantic fragment ned terms entire table present layout strategies improved availability query type scenario scans queries selection queries lter non-indexed attribute aggregate queries single table involve sequential scan entire table scan requires entire table order succeed simple choice semantic fragment set blocks belonging table entire table single disk failures occur subset tables entirety scans involving tables continue operate oblivious failure index lookups index lookups form common class queries selection condition applied based indexed attribute dbms index tuple record ids reads relevant data pages retrieve tuples traversing index requires access multiple pages index collocation index improves availability index table viewed independently placement index query fails index table unavailable decreasing availability strategy improve availability collocate table indexes call strategy dependent index placement joins queries involve joins multiple tables queries typically require joined tables order succeed improve availability join queries d-graid collocates tables joined single semantic fragment laid single disk identi cation join groups requires extra access statistics tracked dbms implementation modi predator dbms record set stores tables indexes accessed query construct matrix access correlation pair stores information written disk periodically seconds modi cations predator straight-forward involving lines code d-graid information collocate tables accessed fault-isolated placement fine-grained collocation entire tables indexes single disk enhanced availability single table index large single disk disk capacities roughly doubling year scenario require ne-grained approach semantic fragmentation approach graid stripes tables indexes multiple disks similar traditional raid array adopts techniques enable graceful degradation detailed scans scans fundamentally require entire table striping strategy impact availability scan queries availability hierarchical approach large table split minimal number disks hold disk group treated logical faultboundary d-graid applied logical fault-boundaries alternatively database supports approximate queries provide partial availability scan queries missing data index lookups large tables index-based queries common oltp workload tpc-c involves index lookups small number large tables queries require entire index table d-graid simple techniques improve availability queries internal pages b-tree index aggressively replicated failure instance root b-tree index page collocated data pages tuples pointed index page collocation d-graid probabilistic strategy leaf index page written d-graid examines set rids contained page rid determines disk tuple places index page disk greatest number matching tuples note assume table clustered index attribute page-level collocation effective case non-clustered indexes joins similar indexes page-level collocation applied tables join group collocation feasible tables join group clustered join attribute alternatively tables join group small replicated disks larger tables striped selective replication data structures dbms query system run system catalogs information table index frequently consulted structures unavailable partial failure fact data remains accessible practical d-graid aggressively replicates system catalogs extent map database tracks allocation blocks stores experiments employ -way replication important meta-data -way replication feasible readmostly nature meta-data minimal space overhead entails database log plays salient role recoverability database ability make partial availability important log multiple failures providing high availability log size active portion log determined length longest transaction factored concurrency workload portion log highly reasonable modern storage arrays large amounts persistent ram obvious locations place log high availability replicating multiple nvram stores addition normal on-disk storage log ensure log remains accessible face multiple disk failures access-driven diffusion stated coarse-grained fragmentation entire table single disk table large accessed frequently performance impact parallelism obtained disks wasted remedy d-graid monitors accesses logical address space tracks logical segments bene parallelism d-graid creates extra copy blocks spreads disks array normal raid blocks hot d-graid regains lost parallelism due collocated layout providing partial availability guarantees reads writes rst diffused copy background updates actual copy technique underneath dbms essentially identical underneath system infallible writes partial availability data introduces interesting problems transaction recovery mechanisms dbms transaction declared committed ected log partially system crash redo transaction fail pages affect durability semantics transactions problem considered solved aries context handling ine objects deferred restart ensure transaction durability d-graid implements infallible writes guarantees write succeeds block written destined dead disk d-graid remaps live disk writes assuming free space remaining live disk remapping prevents failure ushing committed transaction disk evaluation evaluate availability improvements performance d-graid prototype implementation d-graid prototype functions software raid driver linux kernel operates underneath predator shore dbms availability improvements evaluate availability improvements d-graid d-graid array disks study fraction queries database serves successfully increasing number disk failures layout techniques d-graid complementary existing raid schemes parity mirroring show d-graid level redundancy data measurements simplicity microbenchmarks analyze availability provided layout techniques d-graid coarse-grained fragmentation rst evaluate availability improvements due coarse-grained fragmentation techniques d-graid figure presents availability scan index lookup join queries synthetic workloads multiple disk failures percentage queries complete successfully reported leftmost graph figure shows availability scan queries database tables tuples workload query chooses table random computes average non-indexed attribute requiring scan entire table graph shows collocation tables enables database partially serving proportional fraction queries comparison queries succeed failed disks table scans availability queries succeed failed disks index lookup queries dependent 
index placementindependent index placement queries succeed failed disks join queries join collocationwithout collocation figure coarse-grained fragmentation graphs show availability degradation scans index lookups joins varying number disk failures -disk d-graid array steeper fall availability higher number failures due limited -way replication metadata straight diagonal line depicts ideal linear degradation queries succeed failed disks split tables indexes replication colocationwith replication plain striping queries succeed failed disks split tables indexes hot-cold tuples hot-cold -wayhot-cold -way hot-cold -way figure index lookups ne-grained fragmentation graphs show availability degradation index lookup queries left graph considers uniformly random workload graph considers workload small set tupes popular failure traditional raidsystem results complete unavailability note redundancy maintained parity mirroring d-graid traditional raid tolerate failure availability loss middle graph figure shows availability index lookup queries similar workload layouts layouts entire store index table collocated disk independent index placement d-graid treats index table independent stores possibly allocates disks dependent index placement d-graid carefully allocates index disk table dependent placement leads availability failure finally evaluate bene join-group collocation micro-benchmark database pairs tables joins involving tables pair join queries randomly select pair join tables rightmost graph figure shows collocating joined tables d-graid achieves higher availability fine-grained fragmentation evaluate effectiveness ne-grained fragmentation focus availability index lookup queries interesting category workload study consists index lookup queries randomly chosen values primary key attribute single large table plot fraction queries succeed varying number disk failures left graph figure shows results layouts examined graph lowermost line shows availability simple striping replication system catalogs availability falls drastically multiple failures due loss internal b-tree nodes middle line depicts case internal b-tree nodes replicated aggressively expected achieves availability finally line shows availability data index pages collocated addition internal b-tree replication techniques ensure linear degradation availability graph figure considers similar workload small subset tuples hotter compared speci cally tuples accessed queries workload simple replication collocation provide linear degradation availability hot pages spread uniformly disks hot-cold workload d-graid improve availability replicating data index pages d-graid raidslowdown table scan index lookup bulk load table insert table time overheads d-graid table compares performance d-graid ne-grained fragmentation default raidunder microbenchmarks array disks hot tuples lines depict availability hot pages replicated factors small fraction read data hot d-graid utilizes information enhance availability selective replication performance overheads evaluate performance implications faultisolated layout d-graid experiments section -disk d-graid array comprised ibm ultrastar lzx disks peak throughput database single table records sized bytes index primary key time space overheads rst explore time space overheads incurred d-graid prototype tracking information database laying blocks facilitate graceful degradation table compares performance graid ne-grained fragmentation linux software raid basic query workloads workloads examined scan entire table index lookup random key table bulk load entire indexed table inserts indexed table graid performs raidfor workloads scans poor performance scans due predator anomaly scan workload completely saturated cpu table disks extra cpu cycles required d-graid impacts scan performance interference prototype competes resources host hardware raid system interference exist overheads d-graid reasonable evaluated space overheads due aggressive metadata replication found minimal overhead scales number tables database tables overhead -way replication important data access-driven diffusion evaluate bene diffusing extra copy popular tables table shows time scan scan time raidd-graid d-graid diffusion table diffusing collocated tables table shows scan performance -disk array coarse-grained fragmentation table coarse-grained fragmentation d-graid simple collocation leads poor scan performance due lost parallelism extra diffusion aimed performance d-graid performs closer default raidcomparison implementation d-graid underneath dbms uncovered fundamental challenges present system notion semantically-related groups complex dbms inter-relationships exist tables indexes system case les directories reasonable approximations semantic groupings dbms goal graid enable serving higher level queries notion semantic grouping dynamic depends query workload identifying popular data aggressively replicated easier systems standard system binaries libraries obvious targets independent speci system running dbms set popular tables varies dbms dependent query workload effectively implementing d-graid underneath dbms requires slightly modifying dbms record additional information finally ensure transaction durability implemented infallible writes version dbms comparing d-graid performs beneath dbms versus system similarities versions d-graid successfully enable graceful degradation availability versions enable expected number processes queries complete successfully xed number disk failures fact versions enable expected number complete subset data popular similarly versions d-graid introduce time overhead interestingly slowdowns database version generally lower system version finally versions require access-driven diffusion obtain acceptable performance secure delete faded case study implement faded underneath dbms faded semantically smart disk detects deletes records tables dbms level securely overwrites shreds relevant data make irrecoverable extend previous work implemented functionality systems motivation deleting data recovery impossible important system security government regulations require guarantees sensitive data forgotten requirements important databases recent legislations data retention sarbanes-oxley act accentuated importance secure deletion secure deletion data magnetic disks involves overwriting disk blocks sequence writes speci patterns cancel remnant magnetic effects due past layers data block early work overwrites block required secure erase recent work shows overwrites suf modern disks system dbms ensure secure deletion functions top modern storage systems transparently perform optimizations storage system buffer writes nvram writing disk presence nvram buffering multiple overwrites system dbms collapsed single write physical disk making overwrites ineffective presence block migration storage system overwrites system dbms miss past copies secure deletion requires low level information control storage system time higher level semantic information system dbms detect logical deletes semanticallysmart disk system ideal locale implement secure deletion filesystem-aware faded running underneath system faded infers deleted tracking writes inodes indirect blocks bitmap blocks due asynchronous nature systems faded guarantee current contents block belong deleted newly allocated shredded ensure shred valid data faded conservative overwrites shreds version block restoring current contents block previous work implemented faded systems linux ext linux ext windows vfat faded work correctly system changed linux ext modi ensure data bitmap blocks ushed indirect block allocated freed windows vfat changed track generation number nally linux ext modi list modi data blocks included transaction database-aware faded implement faded beneath dbms semantic disk identify handle deletes entire tables individual records discuss cases turn simplest case faded table deleted drop table command issued faded shred blocks belonging table faded log snooping identify log records freeing extents stores 
shore free ext list log record written extent freed faded list freed blocks issue secure overwrites pages transaction aborts undoing deletes contents freed pages required faded pessimistically waits transaction committed performing overwrites handling record-level deletes faded challenging speci tuples deleted sql delete statement speci byte ranges pages tuples shredded delete dbms typically marks relevant page slot free increments free space count page freeing slots logged faded learn record deletes log snooping faded shred page records page valid read current page disk defer shredding faded receives write page ecting relevant delete receiving write faded shreds entire page disk writes data received complications basic technique rst complication identify correct version page deleted record assume faded observes record delete page waits subsequent write written faded detect version written reects version stale dbms wrote page delete block reordered disk scheduler arrives disk issue similar le-system version faded conservative overwrites database-aware version wal property dbms ensure correct operation speci cally database-aware faded pagelsn eld page identify ects delete pagelsn page tracks sequence number run time workload workload default faded faded faded table overheads secure deletion table shows performance faded overwrites workloads workload deletes contiguous records workload deletes records randomly table latest log record describing change page faded simply compare pagelsn lsn delete complication dbms bytes belonged deleted records result data remains page faded observes page write scans page free space explicitly zeroes deleted byte ranges page remain dbms cache subsequent writes page scanned zeroed appropriately evaluation brie evaluate cost secure deletion faded prototype implementation prototype implemented device driver linux kernel works underneath predator workloads operating table -byte records rst workload perform delete rows half table deleted deleted pages contiguous workload tuples deleted selected random table compares default case faded faded overwrite passes expected secure deletion performance cost due extra disk multiple passes overwrites modern disks effectively shred data overwrites focus faded case performance slower overhead incurred deletes sensitive data deleted manner costs reasonable situations additional security required comparison primary difference versions faded database-aware version leverage transactional properties dbms nitively track block shredded result system version faded required system exception data journaled ext implementation faded require dbms version require detailed information on-disk page layout dbms record-level granularity deletes dbms makes secure deletion complex system counterpart versions faded incur overhead depending workload number overwrites delete-intensive database workloads faded slower overwrites similarly system workloads faded slower overwrites table summary slowdown incurred faded depends workload number overwrites dbms system exclusive caching x-ray nal case study implement x-ray underneath dbms x-ray exclusive caching mechanism storage arrays attempts cache disk blocks present higher-level buffer cache providing illusion single large lru cache previous work demonstrated approach performs buffer cache maintained system motivation modern storage arrays possess large amounts ram caching disk blocks instance high-end emc storage array main memory caching typically cache second-level cache system database system maintains buffer cache host main memory current caching mechanisms storage arrays account block array cache read duplicating blocks cached cache space wasted due inclusion strategy contents buffer cache disk array cache exclusive wong proposed avoid cache inclusion modifying system disk interface support scsi demote command enables treating disk array cache victim cache database system approach require dbms inform disk evictions buffer pool requiring explicit change scsi storage interface makes scheme hard deploy industry consensus required adopting change filesystem-aware x-ray x-ray predicts contents system buffer pool chooses cache recent victims cache x-ray requires storage interface x-ray access time statistics block accessed perform predictions hit rate array cache size blocks hit rate x-ray multi-queue lru execution time seconds array cache size blocks execution time lru multi-queue x-ray hit rate write period write period variation period variation hit rate segment size blocks segment size variation segment size variation figure x-ray performance gure presents evaluation x-ray tpc-c benchmark dbms buffer cache set blocks studies hit rate x-ray compared caching mechanisms segment size blocks access information written execution times compared times based buffer cache hit time disk array cache hit time disk read time hit-rate x-ray measured segment sizes write period write period varied x-ray hit rate measured segment size blocks systems linux ext access statistics recorded granularity directly inodes x-ray access statistics maintain ordered list block numbers lru block mru block complicated fact access statistics tracked perle basis ordered list updated x-ray obtains information system reads disk making read block recently accessed system writes access time disk disk read arrives block x-ray infers evicted buffer cache time past infer block earlier access time evicted assuming lru policy access time block updated x-ray observe disk read x-ray infers block blocks access time present buffer cache higher-level cache policy lru usual case blocks close mru end list predicted system buffer cache blocks lru part list considered exclusive set x-ray caches recent blocks exclusive set extra internal array bandwidth idle time disk requests read blocks cache database-aware x-ray database-aware version x-ray similar system-aware version primary difference creating database-aware x-ray occurs dbms typically track access statistics database systems maintain access statistics administrative purposes awr oracle statistics coarse granularity written long intervals implement database-aware x-ray modify database buffer manager write access statistics periodically speci cally table index divided xed-sized segments buffer manager periodically writes disk access time segments accessed period time x-ray assumes blocks segment accessed sees access time statistic updated accuracy x-ray predict contents database cache sensitive size segment update interval advantage explicitly adding information tune implementation changing size segment update interval alternative adding access information modify dbms directly report evicted block cache demote adding access statistics approach statistics general semantic disks implementing functionality d-graid evaluation evaluate performance database-aware version x-ray simulation database buffer cache disk array cache evaluation lesystem-aware x-ray performed simulation database buffer cache maintained lru fashion dbms periodically writes access information granularity segment array cache managed x-ray assume x-ray suf cient internal bandwidth block reads instrumented buffer cache manager postgres dbms generate traces page requests buffer cache level postgres predator programming api linux required implement tpc-c approximate implementation tpc-c benchmark evaluation adheres tpc-c speci cation access pattern total transactions performed evaluate performance x-ray terms array cache hit rate execution time 
compare static dynamic catalog tables log record format b-tree page format data page format ransaction status block wnership block type block relationships access statistics d-graid basic ne-grained frags join-collocation faded basic record-level delete x-ray basic table dbms information required case studies table lists static information embedded semantic disk dynamic state automatically tracked disk ray plain lru multi-queue mechanism designed level caches explore sensitivity segment size access time update periodicity figure compares hit rate x-ray schemes figure compares execution times segment size set blocks access information written study x-ray hit rate lru multi-queue hit rate advantage extends execution time overhead writing access information x-ray performs lru multi-queue figure evaluates sensitivity x-ray cache hit rate segment size expected hit rate drops slightly increase segment size figure shows sensitivity access information update interval x-ray tolerate reasonable delay seconds obtaining access updates comparison system database versions x-ray similar implement x-ray semantic disk requires access statistics blocks accessed layer systems track periodically write statistics dbms x-ray dbms modi explicitly track access times segments table advantage explicitly adding information tune statistics appropriately size segment update interval running beneath system database ray found substantially improve array cache hit rate relative lru multi-queue information case studies section review static dynamic information required database-aware disk needed information depends functionality implemented exact information required variants case studies listed table biggest concern database vendors static information exported storage system understands format catalog table database vendor loathe change format amount static information varies bit case studies case studies format catalog tables log records d-graid support ne-grained fragmentation faded record-level deletes detailed knowledge b-tree page format data page format dynamic information varies case studies fundamental piece dynamic information block ownership shown fact required case study block type generally property needed d-graid faded pieces dynamic information widespread faded precisely transaction committed correct pessimistic determining overwrite data d-graid associate blocks table blocks index vice versa finally access correlation access count statistics needed d-graid variants collocate related tables aggressively replicate hot data simple access time statistic needed x-ray predict contents higher-level buffer cache conclusions today database community sort simple-minded model disk arm platter holds database fact holding database raid arrays storage area networks kinds architectures underneath hood masked logical volume manager written operating system people databases transparency good makes productive care details hand optimizing entire stack elds talk hand accept things -pat selinger semantic knowledge storage system enables powerful functionality constructed storage system improve performance caching improve reliability provide additional security guarantees paper shown semantic storage technology deployed beneath commodity systems beneath database management systems found techniques required handle database systems investigated impact transactional semantics dbms cases transactions simplify work semantic disk log snooping enables storage system observe operations performed dbms nitively infer dynamic information changing dbms storage system ensure interfere transactional semantics found infallible writes ensure transaction durability disks failed explored lack access statistics dbms complicates interactions semantic disk case found helpful slightly modify database system gather relay simple statistics acknowledgements david black encouraging extend semantic disks work databases david dewitt jeff naughton rajasekar krishnamurthy vijayan prabhakaran insightful comments earlier drafts paper jeniffer beckham pointing pat selinger quote finally anonymous reviewers thoughtful suggestions greatly improved paper work sponsored nsf ccrccr- ccrngs- itribm network appliance emc acharya uysal saltz active disks programming model algorithms evaluation proceedings international conference architectural support programming languages operating systems asplos viii san jose california october agrawal kiernan srikant hippocratic databases vldb bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids isca bauer priyantha secure data deletion linux file systems usenix security august boral dewitt database machines idea time passed international workshop database machines brown yamaguchi oracle hardware assisted resilient data oracle technical bulletin note chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks usenix pages emc corporation symmetrix enterprise information storage systems http emc ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december gray computers stop international conference reliability distributed databases june gribble robustness complex systems eighth workshop hot topics operating systems hotos viii schloss elmau germany grochowski emerging trends data storage magnetic hard disk drives datatech september gutmann secure deletion data magnetic solidstate memory usenix security july hellerstein haas wang online aggregation sigmod pages hughes coughlin secure erase disk drive data idema insight magazine keeton computer architecture support database applications phd thesis california berkeley keeton wilkes automating data dependability proceedings acm-sigops european workshop pages saint-emilion france september carey shoring persistent applications proceedings acm sigmod conference mohan haderle lindsay pirahesh schwarz aries transaction recovery method supporting finegranularity locking partial rollbacks write-ahead logging acm tods march oracle self-managing database automatic performance diagnosis https oracleworld published doc patterson gibson katz case redundant arrays inexpensive disks raid sigmod pages patterson availability maintainability performance focus century key note lecture fast postgres postgresql database http postgresql riedel gibson faloutsos active storage largescale data mining multimedia vldb selinger winslett pat selinger speaks sigmod record december seshadri paskin predator or-dbms enhanced data types sigmod sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid fast sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems usenix symposium file storage technologies fast pages slotnick logic track devices volume pages academic press tpc-c transaction processing performance council http tpc tpcc tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wong wilkes cache making storage exclusive usenix zhou philbin multi-queue replacement algorithm level buffer caches usenix pages 
model-based failure analysis journaling file systems vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau wisconsin madison computer sciences department west dayton street madison wisconsin vijayan dusseau remzi wisc abstract propose method measure dependability journaling file systems approach build models journaling file systems behave journaling modes models analyze file system behavior disk failures techniques measure robustness important linux journaling file systems ext reiserfs ibm jfs analysis identify design flaws correctness bugs present file systems file system errors ranging data corruption unmountable file systems introduction disks fail modern file systems storage systems include internal machinery cope failures ensure file system integrity reliability presence failures disks fail changing traditional systems assume disks fail-stop assumption disk working failure easily detectable disk complexity increases pressures time-to-market cost increase disk failure modes common specifically latent sector faults occur specific block faulty transient permanent manner disk viewing disk working longer paper investigate modern file systems cope class fault modern file systems journaling systems logging data separate journal writing fixed locations file systems maintain file system integrity presence crashes analyze file systems develop modelbased fault-injection technique specifically file system test develop abstract model update behavior orders writes disk maintain file system consistency model inject faults interesting points file system transaction monitor system reacts failures paper focus write failures file system writes change on-disk state potentially lead corruption properly handled fault-injection methodology test widely linux journaling file systems ext reiserfs ibm jfs analysis find design flaws file systems catastrophically affect on-disk data specifically find ext ibm jfs designed handle sector failures failures file systems shown commit failed transactions disk lead problems including unmountable file system contrast find reiserfs part paranoid write failures specifically reiserfs crashes system write journal fails crashing manner reiserfs ensures file system integrity maintained cost potentially expensive restart configurations reiserfs abide general policy coerced committing failed transactions result corrupted file system reiserfs assumes failures transient repeated failure block result repeated crashes restarts rest paper organized give introduction journaling file systems explain methodology analyzing journaling file systems discuss results analysis ext reiserfs jfs present related work finally conclude background file system update takes place set blocks written disk system crashes middle sequence writes file system left inconsistent state repair inconsistency earlier systems ffs ext scan entire file system perform integrity checks fsck mounting file system scan time-consuming process hours large file systems journaling file systems avoid expensive integrity check recording extra information disk form write-ahead log writes successfully committed log transfered final fixed locations disk process transferring writes log fixed location disk referred checkpointing crash occurs middle checkpointing file system recover data log write fixed locations modern file systems provide flavors journaling subtle differences update behavior disk discuss approaches data journaling ordered journaling writeback journaling journaling modes differ kind integrity provide type data write log order data written data journaling strongest data integrity block written disk irrespective data metadata block written log transaction committed journaled data written fixed file system locations writeback journaling logs file system metadata enforce ordering data writes journal writes ensuring metadata consistency writeback journaling guarantee data consistency specifically file metadata updated in-place data reaches disk file data contents data block ordered journaling adds data consistency writeback mode enforcing ordering constraint writes data blocks written fixed locations metadata blocks committed ordering constraint ensures file system metadata points corrupt data methodology section describe methodology testing reliability journaling file systems basic approach simple inject disk faults beneath file system key points operation observe resultant behavior testing framework shown figure consists main components device driver called fault-injection driver user-level process labeled coordinator driver positioned file system disk observe traffic file system inject faults points stream coordinator monitors controls entire process informing driver specific fault insert running workloads top file system observing resultant behavior flow diagram benchmarking process shown figure describe entire process detail fault-injection driver fault-injection driver driver pseudodevice driver appears typical block device file system internally simply interposes requests real underlying disk driver main roles system classify block written disk based type specific file-system data structure write represents developed techniques perform classification simply employ techniques driver model journaling file system specifically model represents correct sequence states transaction committing disk inserting failures specific points transaction sequence observe file system handles types faults judge correctly handles faults injected driver inject faults system faults occur state transitions based model file system stream coordinator coordinator monitors entire benchmarking process inserts fault-injection driver linux kernel coordinator constructs file system passes fault specification driver spawns child process run workload errors running tests coordinator process moves file system state mounting file system cleanly depending type block fail coordinator process passes fault specification driver spawns child process run workload top file system expected block written file system driver injects linux vfs layer jfsreiserfsext idescsi log system log workload fault injection driver ioctl coordinator block match fault pass request disk inject fault block match model pass error file system receive file system read write requests save fault specification model build journaling specification classify block types report error yesno yesno coordinator figure benchmarking framework algorithm flow figure shows benchmarking framework measure fault tolerance journaling file systems write failures main components figure user level process issues fault sba driver classifies blocks injects faults figure shows simplified flowchart benchmarking algorithm implemented sba driver fault failing block write errors manifest numerous locales log errors coordinator collate specifically child process receive errors file system driver observe errors sequence state transitions coordinator system logs errors reported file system reflected calling child process journaling models describe model journaling file systems explained section journaling modes journaling modes differs type data journals order writes blocks build model journaling modes based functionality models represent journaling modes type data accept order data written model ordered journaling mode specifies ordered data written metadata committed log build models construct regular expression journaling mode regular expressions represent journaling modes concisely easy construct understand build model based regular expression figure shows models journaling mode journaling models consist states states represent state on-disk file system ondisk file system moves state based type write receives file system track state change moving correspondingly model explain 
briefly regular expression journaling mode represent journal writes represent data writes represent journal commit writes represent journal super block writes represent checkpoint data writes represent write failures data journaling data journaling expressed regular expression data journaling mode file system writes journaled represented ordered unordered writes writing journal blocks commit block represented byc written file system mark end transaction file system write transactions log transactions committed file system write checkpoint blocks represented fixed locations journal super block represented mark head tail journal convert regular expression state diagram shown figure add failure state ordered journaling ordered journaling exs data journaling model ordered journaling model writeback journaling model figure journaling models figure shows models verifying journaling modes model built based regular expression state represents state reached write failure added models represents journal writes represents data writes represents journal commit writes represents checkpoint writes represents journal super block writes represents write failure pressed regular expression ordered mode ordered data writes written metadata blocks committed journal note data blocks parallel journal writes writes commit block written commit block written transaction transactions similar data journaling file system write checkpoint blocks journal super block transactions regular expression converted state diagram failure state added shown figure writeback journaling writeback journaling regular expression writeback journaling mode unordered data written time file system written journal writes journal writes commit block written transaction committed file system write journal super block checkpoint blocks unordered writes writeback journaling model figure obtained regular expression adding state error model error model assume latent errors originate storage subsystem errors accurately modeled software-based fault injection linux low-level errors reported file system uniform manner errors device-driver layer errors inject block write stream attributes similar classification faults injected linux kernel coordinator passes fault specification fault-injection driver attributes specifies file system test driver understands ext reiserfs ibm jfs file system semantics attribute specifies block type determines request traffic stream failed request types supported file systems attribute change file system request failed dynamically-typed journal commit block statically typed journal super block long determines fault injected transient error fails requests succeeds permanent fails subsequent requests failure classification classify ways file system fail due write failures type losses incur write failure loss file system handles write failure properly prevents data corrupted lost data corruption case write failures lead data corruption metadata corruption file system metadata structures remain consistent data files corrupted type failure occur data block pointers metadata blocks point invalid contents disk note type errors detected fsck data loss type failure file data lost due transient permanent write failures data loss occur data block pointers updated correctly files directories loss case file system metadata corrupted result lost files directories unmountable file system write failures happen file system corrupt important metadata blocks super block group descriptors result unmountable file system crash write failures lead file system reactions system-wide crash failure initiated explicit call panic due reasons dereferencing null pointer semantic fault injection question address fault injection technique file-system aware conduct similar analysis semantic knowledge device driver fail disk writes understands file system block types transaction boundaries high-level information driver type block receives determine failing journal block data block information important file systems behave differently block-write failures reiserfs crashes journal write failures crash data-block write failures depending type block failed file system errors vary data corruption unmountable file systems filesystem knowledge answer file system fails higher-level semantic knowledge enables identify design flaws identified fault injection performed semantic information analysis putting fault injection conclude methodology section fault injected journaling model figure shows sequence steps fault-injection driver track file system writes inject fault failing commit block write transaction ordered journaling mode step figure captures transition state initially transaction starts set ordered data writes figure data writes journal blocks logged figure commit block written data journal writes failed figure file system oblivious commit block failure continue checkpoint journaled blocks figure file system recognize failure steps figure fault injection figure shows sequence steps fault-injection driver track file system writes fail specific writes prevent file system corruption moving state figure state file system abort failed transaction bad block remapping remount read-only crash system sufficient block types inject fault file system requests model fault-injection driver reason requests write failure belong failed transaction transactions file system keeping track writes journaling model fault-injection driver explain block write failure leads file system errors fault injection experiments statistical carefully choose fault injection points inject faults main points ordered data writes journal writes commit writes checkpoint writes superblock writes journal writes perform fault injection journal metadata journal data blocks fault injection experiment proceeds file system tested freshly created files directories needed testing created fault specification attributes passed sba driver controlled workload creating file directory generate block write failed run child process driver injects fault reports file system writes violate journaling model fault injected coordinator collects error logs child process system log driver process automated error logs interpreted manually figure extent file system damaged extraneous writes analysis section explain failure analysis linux based journaling file systems ext reiserfs ibm jfs ext analysis ext journaling file system based ext file system ext logs file system writes journal block level types journal metadata blocks track transactions blocks logged journal descriptor blocks store fixed location block numbers journaled data journal revoke blocks prevent file system replaying data replayed recovery journal commit blocks mark end transactions journal super block stores information journal head tail transaction journal metadata blocks log stores journal data blocks journaled versions fixed location blocks ext designed journal metadata blocks journal super block descriptor block revoke block commit block magic number identifies journal metadata blocks journal metadata blocks sequence number denotes transaction number transaction occur recovery block read journal correct magic number treated journal data block magic number sequence number match transaction expected blocks skipped based ext analysis found design flaws handling write failures committing failed transactions write transaction fails ext continues write transaction log commits fixing failed write affect file system integrity ordered data write fails ordered journaling mode expect file system abort transaction commits transaction metadata blocks end pointing wrong data contents disk problem occurs 
ext failure ordered write data corruption checkpointing failed transactions write transaction fails file system checkpoint blocks journaled part transaction checkpointing crash occurs file system replay failed transaction properly recovery phase result corrupted file system ext commits transaction transaction write fails committing failed transaction ext checkpoints blocks journaled transaction depending journaling mode checkpointing partial complete partial checkpointing cases ext checkpoints blocks failed transaction data journaling mode journal descriptor block journal commit block write fails cases checkpointing file system metadata blocks transaction checkpointed data blocks checkpointed data journaling mode file created data blocks transaction descriptor block fails metadata blocks file inode data bitmap inode bitmap directory data directory inode blocks written fixed locations data blocks file journaled data journaling mode written data blocks written fixed locations metadata blocks file end pointing wrong contents disk complete checkpointing ordered writeback journaling mode file system metadata blocks journaled data blocks written log modes ext checkpoints journaled blocks failed transaction describe generic case file system corruption transactions committed block journaled blocks journaled assume transaction fails file system continues checkpoint blocks failed transaction crash occurs writing blocks fixed locations file system log recovery runs mount recovery transaction recovered failed transaction recovered contents block overwritten contents recovery file system inconsistent state block transaction block transaction problem occurs ext happen journal metadata block descriptor block revoke block commit block fails lead file system corruptions resulting loss files inaccessible directories replaying failed checkpoint writes checkpointing process writing journaled blocks log fixed locations checkpoint write fails file system attempt write mark journal checkpoint write happen log replay ext replay failed checkpoint writes data corruption data loss loss files directories replaying transactions journaling file systems maintain state variable mark log dirty clean file system mounted log dirty transactions log replayed fixed locations journaling file systems update state variable starting transaction checkpointing transaction write update state variable fails things possibly happen file system replay transaction replayed fail replay transaction recovery replaying transaction integrity problems possibility replaying journal contents lead corruption loss data files directories ext maintains journal state journal super block ext clears field writes journal super block clean journal mark journal dirty journal super block written non-zero field journal super block write fails ext attempt write save super block locations journal super block failure ext continues commit transactions log journal super block written mark journal dirty failed journal appears clean mount transaction needed replay due previous crash ext fails replay result lost files directories replaying failed transactions journal data block write fails transaction aborted replayed transaction replayed journal data blocks invalid contents read written fixed location handled properly lead file system errors earlier ext abort failed transactions continues commit log recovery write invalid contents file system fixed location blocks corrupt important file system metadata result unmountable file system show created transaction journaled group descriptor block file system failed journal write group descriptor block ext committed transaction failed mark invalid commit crashed file system forced ext recovery mount recovery ext read block journal supposed group descriptor block overwrote fixed location group descriptor block invalid contents journal corrupted group descriptor block resulted unmountable file system ext summary find ext designed system crash mind ext effectively handle single block write failures features ext designed ext crash entire system failed writes magic numbers transaction ids journal metadata blocks ext prevents replay invalid contents main weakness ext design abort failed transactions continues commit lead file system errors ranging data corruption unmountable file system found ext logs empty transactions transactions blocks commit block affect integrity result unnecessary disk traffic reiserfs analysis journaling reiserfs similar ext reiserfs circular log capture journal writes logs file system writes block level reiserfs supports journaling modes journal metadata blocks journal descriptor block journal commit block journal super block describe transactions fixed location blocks journal metadata blocks reiserfs magic number transaction number similar ext based analysis found design flaws reiserfs crashing file system write fails reiserfs time crashes file system making panic call necessitates entire system rebooted affect processes running reiserfs affects processes running system crashing entire file system single write error benefit journal write journal data journal metadata fails system crashes failed transaction committed disk system boots mounts file system reiserfs performs recovery recovery replays transactions successfully committed failed transaction failed transactions committed replayed file system remains consistent state recovery avoids problems ext checkpointing failed transactions replaying successful transactions replaying failed transactions words reiserfs converts problem fail-stutter fault tolerance fail-stop journal block write fails reiserfs repeatedly crash system reiserfs crashes system checkpoint write fails crash recovery takes place failed checkpoint write replayed properly note works fine transient write failures permanent write errors reiserfs requires fsck run handle replay failures crashing checkpoint write failures prevents problem replaying failed checkpoint writes ext committing failed transactions write failures reiserfs crash continues commit failed transaction ordered journaling mode ordered data block write fails reiserfs journals transaction commits handling write error result corrupted data blocks failed transactions metadata blocks file system end pointing invalid data contents reiserfs uniform failure handling policy crashes write failures file system corruption prevented reiserfs crashing system ordered write failures reiserfs summary find reiserfs avoids mistakes ext expensively cost crashing entire file system basically reiserfs converts fail-stutter system fail-stop handle write errors find committing failed transaction reiserfs desirable design decision solve problems ext block write errors permanent reiserfs make system unusable repeated crashing model find bug reiserfs linux data journaling mode version behaving ordered journaling mode journaling model find bugs semantics journaling violated jfs analysis ibm jfs works ordered journaling mode unlike ext reiserfs support data writeback journaling modes jfs differs ext reiserfs information written log ext reiserfs log blocks journal jfs writes records modified blocks log ordered data block writes written blocks similar file systems jfs record level journaling log blocks classified journal data blocks journal commit blocks single log write journal data records commit records hard separate commit record journal records transactions small fit single journal block modified ordered journaling model work jfs record level journaling performed failure analysis jfs found design mistakes crashing file system similar reiserfs jfs crashes file system 
write failures system crashes journal super block write fails mount operation earlier crashing system affects processes running system crashing system graceful provide fault tolerance write errors permanent replaying failed checkpoint writes checkpoint block write fails jfs attempt rewrite mark transaction replay jfs simply ignores error lead corrupted file system behavior similar ext file systems record failed checkpoint writes identifying transactions replayed committing failed transactions found journaling file systems commit failed transaction ordered block write failure jfs notify application ordered write failure commits transaction lead data corruption failing recover journal block write fails jfs abort failed transaction commits crash journal write failure logredo routine jfs fails unrecognized log record type lead unmountable file system jfs summary jfs design flaws ext reiserfs jfs commits failed transactions replay failed checkpoint writes crashes file system reiserfs journal super block write failures found bug jfs jfs flush blocks file sync call created sized file called fsync file descriptor fsync call returned flushing blocks ext reiserfs ibm jfs committing failed transactions checkpointing failed transactions replaying failed checkpoint writes replaying transactions replaying failed transactions crashing file system table design flaws table summary type design flaws identified ext reiserfs ibm jfs ext reiserfs ibm jfs block type journal descriptor block journal revoke block journal commit block journal super block journal data block checkpoint block data block table analysis summary table presents summary type failures occur ext reiserfs ibm jfs block writes fail data block represents ordered unordered writes ext reiserfs represents ordered writes jfs dcmeans data corruption means data loss fdl means files directory loss ufs means unmountable file system means crash means block type file system jfs separate commit revoke blocks records type journal expect file system write metadata blocks file disk analysis summary summary analysis presented table table table lists design flaws identified linux journaling file systems table types file system failures happen block writes fail find linux journaling file systems uniform failure handling policies handle fail-stutter systems related work section discuss related work talk related work fault injection general specific work file storage systems testing fault injection fault injection long time measure robustness systems koopman argues faults injected directly modules test give representative results dependability evaluation fault injected external environments module test fault activated inputs real execution similar approach inject faults external file system module activate running workloads top file system software simulate effects hardware faults inject faults dynamically determining block types file system ftape tool performs dynamic workload measurements inject faults automatically determining time location maximize fault propagation fiat early systems fault injection techniques simulate occurrences hardware errors changing contents memory registers fine tool developed kao inject hardware induced software faults unix kernel trace execution flow kernel recent work fault injection techniques test linux kernel behavior errors file storage system testing file system testing tools test file system api types invalid arguments siewiorek develop benchmark measure system robustness test dependability file system libraries similarly koopman ballista testing suite find robustness problems safe fast sfio library test file system robustness model checking techniques apply file system code recent work yang model checking comprehensively find bugs file systems ext reiserfs jfs formal verification techniques systematically enumerate set file system states verify valid file system states work identify problems deadlock null pointers work focuses file systems handle latent sector errors previous work studied reliability storage systems brown developed method measure system robustness applied measure availability software raid systems linux solaris windows emulate disk disk emulator inject faults test software raid systems work targets file systems file system knowledge carefully select fail specific block types don require semantic information fault injection studies evaluated raid storage systems reliability availability studies developed detailed simulation models raid storage arrays network clusters obtain dependability measures conclusion paper propose evaluate robustness journaling file systems disk write failures build semantic models journaling modes semantic block-level analysis technique inject faults file system disk requests evaluate widely linux journaling file systems analysis find ext ibm jfs violate journaling semantics block write failures result corrupt file systems contrast reiserfs maintains file system integrity crashing entire system write failures permanent write failures result repeated crashes restarts based analysis identify design flaws correctness bugs file systems catastrophically affect on-disk data find modern file systems uniform failure handling policy jfs overview ibm developerworks library jfs html brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june corbett english goel grcanac kleiman leong sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april devale koopman performance evaluation exception handling libraries dependable systems networks june gray reuter transaction processing concepts techniques morgan kaufmann kalbarczyk ravishankar yang characterization linux kernel behavior error dependable systems networks pages june huang kalbarczyk iyer dependability analysis cache-based raid system fast distributed simulation ieee symposium reliable distributed systems barton czeck fault injection experiments fiat ieee transactions computers volume pages april kaniche romano kalbarczyk iyer karcich hierarchical approach dependability analysis commercial cache-based raid storage architecture twenty-eighth annual international symposium fault-tolerant computing june koopman wrong fault injection dependability benchmark workshop dependability benchmarking conjunction dsn washington july lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems proceedings usenix annual technical conference usenix april reiser reiserfs namesys schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december siewiorek hudak suh segal development benchmark measure system robustness twenty-third international symposium fault-tolerant computing sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january tsai iyer measuring fault tolerance ftape fault injection tool intl conf modeling techniques tools conp perf evaluation pages sept tweedie journaling linux ext file system fourth annual linux expo durham north carolina yang twohey engler musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december 

journal-guided resynchronization software raid timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison abstract investigate problem slow scan-based software raid resynchronization restores consistency system crash augmenting raid layer quicken process leverage functionality present journaling file system analyze linux ext introduce mode operation declared mode guarantees provide record outstanding writes case crash utilize information augment software raid interface verify read request repairs redundant information block combination features provide fast journal-guided resynchronization evaluate effect journal-guided resynchronization find improved software raid reliability availability crash suffering performance loss normal operation introduction providing reliability storage level entails raid prevent data loss case disk failure high-end storage arrays specialized hardware provide utmost performance reliability solutions multimillion dollar price tags infeasible small medium businesses organizations cost-conscious users turn commodity systems collection disks house data popular low-cost solution reliability arena software raid range platforms including linux solaris freebsd windows-based systems software-based approach attractive specialized cluster-in-a-box systems instance emc centera storage system built cluster commodity machines linux software raid manage disks life storage arrays pay case software raid lack non-volatile memory introduces consistent update problem specifically write issued raid layer disks updated consistent manner possibility crashes makes challenge raidarray untimely crash occurs parity write completes data block written writes issued parallel completed stripe left inconsistent state inconsistency introduces window vulnerability data disk fails stripe made consistent data disk lost automatic reconstruction missing data block based inconsistent parity silently return bad data client hardware raid circumvents problem gracefully non-volatile memory buffering update nvram disks consistently updated hardware-based approach avoids window vulnerability outcome ideal performance reliability excellent current software-based raid approaches performance reliability trade-off made current software raid implementations choose performance reliability simply issue writes disks parallel hoping untimely crash occur crash occur systems employ expensive resynchronization process scanning entire volume discrepancies found repaired large volumes process hours days alternate software raid approach chooses reliability performance applying write-ahead logging array record location pending updates issued systems avoid time-consuming resynchronization recovery raid simply repairs locations recorded log removing window vulnerability high performance cost update raid preceded synchronous write log greatly increasing total load disks solve consistent update problem software raid develop solution high performance reliability global view storage stack leverage functionality layers system assist cases client software raid system modern journaling file system default linux file system ext reiserfs jfs windows ntfs standard journaling techniques maintain consistency file system data structures solve consistent update problem raid level find journaling readily augmented specifically introduce mode operation linux ext declared mode writing permanent locations declared mode records intentions file system journal functionality guarantees record outstanding writes event crash consulting activity record file system blocks midst updated dramatically reduce window vulnerability crash complete process file system communicate information vulnerabilities raid layer purpose add interface software raid layer verify read receiving verify read request raid layer reads requested block mirror parity group verifies redundant information irregularity found raid layer re-writes mirror parity produce consistent state combine features integrate journal-guided resynchronization file system recovery process record write activity vastly decreases time needed resynchronization cases period days mere seconds approach avoids performance reliability trade-off found software raid systems performance remains high window vulnerability greatly reduced general key solution cooperative nature removing strict isolation file system software raid layer subsystems work solve consistent update problem sacrificing performance reliability rest paper organized section illustrates software raid consistent update problem quantifies likelihood crash lead data vulnerability section introduction ext file system operation section analyze ext write activity introduce ext declared mode addition software raid interface merge raid resynchronization journal recovery process section evaluates performance declared mode effectiveness journal-guided resynchronization discuss related work section conclude section consistent update problem introduction task raid maintain invariant data redundant information stores invariants provide ability recover data case disk failure raidthis means mirrored block data parity schemes raidthis means parity block stripe stores exclusive-or data blocks blocks reside disk updates applied atomically maintaining invariants face failure challenging crash occurs write array blocks left inconsistent state mirror successfully written disk data block written parity update note consistent update problem solutions distinct traditional problem raid disk failures failure occurs redundant information array lost data vulnerable disk failure situation solved process reconstruction regenerates data located failed disk failure models illustrate consistent update problem shown figure diagram depicts state single stripe blocks disk raidar- ray time progresses left software raid layer residing machine servicing write data block update parity block machine issues data block write time written disk time machine notified completion time similarly parity block issued time written time notification arrives time data write block time stripe enters window vulnerability denoted shaded blocks time failure disks result data loss stripe data parity blocks exist inconsistent state data residing failed disk reconstructed inconsistency corrected time write failure models possibility independent failures host machine time cpczip zdisk array machine figure failure scenarios diagram illustrates sequence events data block write parity update disk raidarray time progresses left boxes labeled request issued labeled represent completions shaded blocks denote window vulnerability array disks discuss turn relate consequences figure machine failure model includes events operating system crashes machine power losses machine crashes times array remains active stripe left inconsistent state write completes time model disk failure model considers power losses disk array failure occurs time time stripe left vulnerable state note disk failure model encompasses non-independent failures simultaneous power loss machine disks measuring vulnerability determine crash failure leave array inconsistent state instrument linux software raidlayer scsi driver track statistics record amount time write issued stripe write issued stripe measures difference times figure corresponds directly period vulnerability machine failure model record amount time write completion stripe write completion stripe measures difference time time note vulnerability disk failure model occurs time time measurement approximation results slightly overestimate underestimate actual vulnerability depending vulnerable time number writers software raid vulnerability disk failure model write completion machine failure model write issue figure software raid vulnerability graph plots percent time duration experiment inconsistent disk state exists raidarray number writers increases x-axis vulnerabilities due disk failure machine failure plotted separately time takes completion processed host machine finally track number stripes 
vulnerable models calculate percent time stripe array vulnerable type failure test workload consists multiple threads performing synchronous random writes set files array experiments performed intel pentium xeon ghz processor ram running linux kernel machine ibm lzx disks configured software raidarray raid volume sufficiently large perform benchmarks small reduce execution time resynchronization experiments figure plots percent time duration experiment array stripe vulnerable number writers workload increased x-axis expected cumulative window vulnerability increases amount concurrency workload increased vulnerability disk failure model greater dependent response time write requests small number writers disk failure result inconsistent state higher concurrency array exists vulnerable state length experiment period vulnerability machine failure model lower depends processing time needed issue write requests experiment vulnerability reaches approximately higher concurrencies ability issue requests impeded full disk queues case machine vulnerability depend disk response time increase solutions solve problem high-end raid systems make non-volatile storage nvram write request received log request data written nvram updates propagated disks event crash log records data present nvram replay writes disk ensuring consistent state array functionality expense terms raw hardware cost developing testing complex system software raid hand frequently employed commodity systems lack non-volatile storage system reboots crash record write activity array indication raid inconsistencies exist linux software raid rectifies situation laboriously reading contents entire array checking redundant information correcting discrepancies raidthis means reading data mirrors comparing contents updating states differ raidscheme stripe data read parity calculated checked parity disk re-written incorrect approach fundamentally affects reliability availability time-consuming process scanning entire array lengthens window vulnerability inconsistent redundancy lead data loss disk failure additionally disk bandwidth devoted resynchronization deleterious effect foreground traffic serviced array exists fundamental tension demands reliability availability allocating bandwidth recover inconsistent disk state reduces availability foreground services giving preference foreground requests increases time resynchronize observed brown patterson default linux policy addresses trade-off favoring availability reliability limiting resynchronization bandwidth disk slow rate equate days repair time vulnerability moderately sized arrays hundreds gigabytes figure illustrates problem plotting analytical model resynchronization time disk array raw size array increases x-axis disks default linux policy minutes time scan repair gigabyte disk space equates half days terabyte capacity disregarding availability array modern interconnects approximately hour full bandwidth resynchronize terabyte array days days hours hour mins time raw array size software raid resynchronization time linux default disk gigabit ethernet sata fibre channel ultra scsi serial attached scsi figure software raid resynchronization time graph plots time resynchronize disk array raw capacity increases x-axis solution problem add logging software raid system manner similar discussed approach suffers drawbacks logging array disks decrease performance array interfering foreground requests high-end solution discussed previously benefits fast independent storage form nvram adding logging maintaining acceptable level performance add considerable complexity software instance linux software raid implementation buffering discarding stripes operations complete logging solution buffer requests significantly order batch updates log improve performance solution perform intent logging bitmap representing regions array mechanism solaris volume manager veritas volume manager provide optimized resynchronization implementation linux software raidis development merged main kernel logging array approach suffer poor performance instance linux implementation performs synchronous write bitmap updating data array ensure proper resynchronization performance improved increasing bitmap granularity cost performing scanbased resynchronization larger regions software raid layer storage hierarchy configuration modern journaling file system layer logging disk updates maintain consistency on-disk data structures sections examine journaling file system solve software raid resynchronization problem ext background section discuss linux ext file system operation data structures details analysis write activity description modifications support journal-guided resynchronization section focus ext techniques general apply journaling file systems reiserfs jfs linux ntfs windows linux ext modern journaling file system aims complex on-disk data structures consistent state file system updates written log called journal journal records stored safely disk updates applied home locations main portion file system updates propagated journal records erased space occupied re-used mechanism greatly improves efficiency crash recovery crash journal scanned outstanding updates replayed bring file system consistent state approach constitutes vast improvement previous process fsck relied full scan file system data structures ensure consistency natural make journaling mechanism improve process raid resynchronization crash modes ext file system offers modes operation data-journaling mode ordered mode writeback mode data-journaling mode data metadata written journal coordinating updates file system strong consistency semantics highest cost data written file system written journal home location ordered mode ext default writes file system metadata journal file data written directly home location addition mode guarantees strict ordering writes file data transaction written disk metadata written journal committed guarantees file metadata data block written mechanism strong consistency data-journaling mode expense multiple writes file data writeback mode file system metadata written journal ordered mode file data written directly home location unlike ordered mode writeback mode ordering guarantees metadata data offering weaker consistency instance metadata file creation committed journal file data written event crash journal recovery restore file metadata contents filled arbitrary data writeback mode purposes weaker consistency lack write ordering transaction details reduce overhead file system updates sets grouped compound transactions transactions exist phases lifetimes transactions start running state file system data metadata updates current running transaction buffers involved linked in-memory transaction data structure ordered mode data running transaction written time kernel pdflush daemon responsible cleaning dirty buffers periodically running transaction closed transaction started occur due timeout synchronization request transaction reached maximum size closed transaction enters commit phase buffers written disk home locations journal transaction records reside safely journal transaction moves checkpoint phase data metadata copied journal permanent home locations crash occurs checkpoint committed transaction checkpointed journal recovery phase mounting file system checkpoint phase completes transaction removed journal space reclaimed journal structure tracking contents journal requires file system structures journal superblock stores size journal file pointers head tail journal sequence number expected transaction journal transaction begins descriptor block lists permanent block addresses subsequent data metadata blocks descriptor block needed depending number blocks involved transaction finally commit block signifies end transaction descriptor blocks commit blocks begin magic header sequence number identify transaction design implementation goal resynchronization correct raid inconsistencies result system crash failure identify outstanding write requests time crash significantly narrow range blocks inspected result faster resynchronization 
improved reliability availability hope recover record outstanding writes file system journal end begin examining write activity generated phase ext transaction ext write analysis section examine ext transaction operations detail emphasize write requests generated phase characterize disk states resulting crash specifically classify write request targeting location unknown location bounded location based record activity journal goal restarting system failure recover record outstanding write requests time crash running ext ordered mode pdflush daemon write dirty pages disk transaction running state crash occurs state affected locations unknown record ongoing writes exist journal commit ext writes un-journaled dirty data blocks transaction home locations waits complete step applies ordered mode data datajournaling mode destined journal crash occurs phase locations outstanding writes unknown ext writes descriptors journaled data metadata blocks journal waits writes complete ordered mode metadata blocks written journal blocks written journal data-journaling mode system fails phase specific record ongoing writes exist writes bounded fixed location journal ext writes transaction commit block journal waits completion event crash outstanding write bounded journal block type data-journaling mode superblock fixed location journal bounded fixed location home metadata journal descriptors home data journal descriptors block type ordered mode superblock fixed location journal bounded fixed location home metadata journal descriptors home data unknown table journal write records table lists block types written transaction processing locations determined crash checkpoint ext writes journaled blocks home locations waits complete system crashes phase ongoing writes determined descriptor blocks journal affect locations ext updates journal tail pointer superblock signify completion checkpointed transaction crash operation involves outstanding write journal superblock resides fixed location recovery ext scans journal checking expected transaction sequence numbers based sequence journal superblock records committed transaction ext checkpoints committed transactions journal steps write activity occurs locations table summarizes ability locate ongoing writes crash data-journaling ordered modes ext case data-journaling mode locations outstanding writes determined bounded crash recovery journal descriptor blocks fixed location journal file superblock existing ext datajournaling mode amenable assisting problem raid resynchronization side data-journaling typically performance ext family ext ordered mode hand data writes permanent home locations recorded journal data structures located crash recovery address deficiency modified ext ordered mode declared mode ext declared mode previous section concluded crash occurs writing data directly permanent location ext ordered mode journal record outstanding writes locations raid level inconsistencies caused writes remain unknown restart overcome deficiency introduce variant ordered mode declared mode declared mode differs ordered mode key guarantees write record data block resides safely journal location modified effectively file system declare intent write permanent location issuing write track intentions introduce journal block declare block set declare blocks written journal beginning transaction commit phase collectively list permanent locations data blocks transaction written construction similar descriptor blocks purpose descriptor blocks list permanent locations blocks journal declare blocks list locations blocks journal descriptor commit blocks declare blocks begin magic header transaction sequence number declared mode adds single step beginning commit phase proceeds declared commit ext writes declare blocks journal listing permanent data locations written part transaction waits completion ext writes un-journaled data blocks transaction home locations waits complete ext writes descriptors metadata blocks journal waits writes complete ext writes transaction commit block journal waits completion declare blocks beginning transaction introduce additional space cost journal cost varies number data blocks transaction case declare block added data blocks space overhead worst case declare block needed transaction single data block investigate performance consequences overheads section implementing declared mode linux requires main guarantee data buffers written disk declared journal accomplish refrain setting dirty bit modified pages managed file system prevents pdflush daemon eagerly writing buffers disk running state mechanism metadata buffers data buffers data-journaling mode ensuring written written journal track data buffers require declarations write declare blocks beginning transaction start adding declare tree in-memory transaction structure ensure declared mode data buffers tree existing data list beginning commit phase construct set declare blocks buffers declare tree write journal writes complete simply move buffers declare tree existing transaction data list tree ensures writes occur efficient order sorted block address point commit phase continue modification implementation minimizes shared commit procedure ext modes simply bypass empty declare tree software raid interface initiating resynchronization file system level requires mechanism repair suspected inconsistencies crash viable option raidarrays file system read re-write blocks deemed vulnerable case inconsistent mirrors newly written data data restored block achieves results current raidresynchronization process raidlayer imposes ordering mirrored updates differentiate data data chooses block copy restore consistency read re-write strategy unsuitable raidhowever file system re-writes single block desired behavior raid layer calculate parity entire stripe data raid layer perform read-modify-write reading target block parity re-calculating parity writing blocks disk operation depends consistency data parity blocks reads disk consistent produce incorrect results simply prolonging discrepancy general interface required file system communicate inconsistencies software raid layer options interface requires file system read vulnerable block re-write explicit reconstruct write request option raid layer responsible reading remainder block parity group re-calculating parity writing block parity disk dissuaded option perform unnecessary writes consistent stripes vulnerabilities event crash opt add explicit verify read request software raid interface case raid layer reads requested block rest stripe checks make parity consistent newly calculated parity written disk correct problem linux implementation verify read request straight-forward file system wishes perform verify read request marks buffer head raid synchronize flag receiving request software raidlayer identifies flag enables existing synchronizing bit stripe bit perform existing resynchronization process presence read entire stripe parity check functionality required verify read request finally option added software raidlayer disable resynchronization crash significant modification strict layering storage stack raid module asked entrust functionality component good system apprehensive software raid implementation delay efforts hopes receiving verify read requests file system requests arrive start resynchronization ensure integrity data parity blocks recovery resynchronization ext data-journaling mode declared mode guarantees accurate view outstanding write requests time crash restart utilize information verify read interface perform fast file system guided resynchronization raid layer make file system journal ordering constraints operations combine process journal recovery dual process file system recovery raid resynchronization proceeds recovery resync ext performs verify reads superblock journal superblock ensuring consistency case written crash ext 
scans journal checking expected transaction sequence numbers based sequence journal superblock records committed transaction committed transaction journal ext performs verify reads home locations listed descriptor blocks ensures integrity blocks undergoing checkpoint writes time crash transaction examined checkpoints occur order checkpointed transaction removed journal processed note verify reads place writes replayed guarantee parity up-to-date adding explicit reconstruct write interface mentioned earlier negate step process ext issues verify reads committed transaction head journal length maximum transaction size corrects inconsistent blocks result writing transaction journal reading ahead journal ext identifies declare blocks descriptor blocks uncommitted transaction descriptor blocks found performs verify reads permanent addresses listed declare block correcting data writes outstanding time crash declare blocks transactions descriptors presence constitutes evidence completion data writes permanent locations ext checkpoints committed transactions journal section implementation re-uses existing framework journal recovery process issuing verify reads means simply adding raid synchronize flag buffers reading journal replaying blocks verify reads locations listed descriptor blocks handled replay writes processed journal verify reads declare block processing uncommitted transaction performed final pass journal recovery bandwidth random write performance ext ordered sorted ext declared ext ordered ext journaled slowdown amount written figure random write performance top graph plots random write performance amount data written increased x-axis data-journaling mode achieves writing data bottom graph shows relative performance declared mode compared ordered mode sorting evaluation section evaluate performance ext declared mode compare ordered mode datajournaling mode hope declared mode adds overhead writing extra declare blocks transaction performance evaluation examine effects journal-guided resynchronization expect greatly reduce resync time increase bandwidth foreground applications finally examine complexity implementation ext declared mode begin performance evaluation ext declared mode microbenchmarks random write sequential write test performance random writes existing file call fsync end experiment ensure data reaches disk figure plots bandwidth achieved ext mode amount written increased x-axis graphs plot experimental trials identify points interest graph data-journaling mode underperforms ordered mode amount written increases note data-journaling mode achieves writing data random write stream transformed large sequential write fits journal amount data written increases outgrows size journal performance datajournaling decreases block written journal home location ordered mode garners performance writing data directly permanent location bandwidth sequential write performance ext ordered ext declared ext journaled slowdown amount written figure sequential write performance top graph plots sequential write performance amount data written increased x-axis bottom graph shows relative performance declared mode compared ordered mode find declared mode greatly outperforms ordered mode amount written increases tracing disk activity ordered mode reveals part data issued disk sorted order based walking dirty page tree remainder issued unsorted commit phase attempts complete data writes transaction adding sorting commit phase ordered mode solves problem evidenced performance plotted graph rest performance evaluations based modified version ext ordered mode sorted writing commit finally bottom graph figure shows slowdown declared mode relative ordered mode sorting performance modes extremely close differing experiment tests sequential write performance existing file figure plots performance ext modes amount written increased x-axis fsync ensure data reaches disk ordered mode declared mode greatly outperform data-journaling mode achieving compared bottom graph figure shows slowdown ext declared mode compared ext ordered mode declared mode performs ordered mode data points disk traces reveal performance loss due fact declared mode waits fsync begin writing declare blocks data ordered mode begins writing data disk slightly earlier declared mode alleviate delay implement early declare mode begins writing declare blocks journal data blocks modified fill declare block operations sprite microbenchmark create phase ext ordered ext declared ext journaled slowdown number files figure sprite create performance top graph plots performance create phase sprite lfs microbenchmark number files increases x-axis bottom graph shows slowdown declared mode compared ordered mode modification result performance improvement early writing declare blocks data blocks offset seek activity journal home data locations shown examine performance sprite lfs microbenchmark creates reads unlinks number files figure plots number create operations completed number files increased x-axis bottom graph shows slowdown declared mode relative ordered mode declared mode performs ordered mode cases performance declared mode ordered mode identical phases benchmark ssh benchmark unpacks configures builds version ssh program tarred compressed distribution file figure plots performance mode stages benchmark execution time stage normalized ext ordered mode absolute times seconds listed bar data-journaling mode slighter faster ordered mode configure phase slower build slower unpack declared mode comparable ordered mode running faster unpack configure slower build phase examine ext performance modified version postmark benchmark creates files directories performs number transactions deletes files directories modification involves addition call sync phase benchmark ensure data written disk unmodified version exhibits unusually high variances modes operation execution time benchmark shown figbuildconfigureunpack normalized execution time ssh benchmark ext ordered ext journaled ext declared figure ssh benchmark performance graph plots normalized execution time unpack configure build phases ssh benchmark compared ext ordered mode absolute execution times seconds listed bar execution time postmark ext journaled ext ordered ext declared slowdown number transactions figure postmark performance top graph plots execution time postmark benchmark number transactions increases x-axis bottom graph shows slowdown declared mode compared ordered mode ure number transactions increases axis data-journaling mode extremely slow concentrate modes identify interesting points large numbers transactions declared mode compares favorably ordered mode differing approximately worst cases small number transactions declared mode outperforms ordered mode disk traces reveal reason ordered mode relies sorting provided per-file dirty page trees write requests scattered disk declared mode sort performed commit global view data written transaction sending write requests device layer efficient order finally examine performance tpc-blike workload performs financial transaction execution time tpc-b ext journaled ext declared ext ordered slowdown number transactions figure tpc-b performance top graph plots execution time tpc-b benchmark number transactions increases x-axis bottom graph shows slowdown declared mode compared ordered mode files adds history record fourth file commits disk calling sync execution time benchmark plotted figure number transactions increased x-axis case declared mode consistently underperforms ext ordered mode approximately data-journaling mode performs slightly worse highly synchronous nature benchmark presents worst case scenario declared mode tpc-b transaction results small ext transaction data blocks descriptor block journaled metadata block commit block declare block 
beginning transaction adds overhead number writes performed benchmark compound problem data writes serviced parallel array disks accentuating penalty declare blocks examine problem test modified version benchmark forces data disk frequently effect increasing size application level transaction alternatively simulating concurrent transactions independent data sets figure shows results running tpc-b benchmark transactions interval calls sync increases x-axis interval increases performance declared mode datajournaling mode quickly converge ordered mode declared mode performs ordered mode sync intervals transactions conclusion find declared mode routinely outperforms data-journaling mode performance close ordered mode random write sequential write file creation microbenchmarks performs ordered mode macrobenchmarks execution time tpc-b varied sync intervals ext journaled ext declared ext ordered slowdown tpc-b transactions sync figure tpc-b varied sync intervals top graph plots execution time tpc-b benchmark interval calls sync increases x-axis bottom graph shows slowdown declared mode compared ordered mode ssh postmark worst performance declared mode occurs tpc-b small application-level transactions improves greatly effective transaction size increases results declared mode attractive option enabling journal-guided resynchronization journal-guided resynchronization final set experiments examine effect journal-guided resynchronization expect significant reduction resync time shortening window vulnerability improving reliability addition faster resynchronization increase amount bandwidth foreground applications crash improving availability compare journal-guided resynchronization linux software raid resync default rate rates availability versus reliability spectrum experimental workload consists single foreground process performing sequential reads set large files amount read bandwidth achieves measured intervals approximately seconds experiment machine crashed rebooted machine restarts raid resynchronization process begins foreground process reactivates figure shows series experiments plotting foreground bandwidth y-axis time progresses x-axis note origin x-axis coincides beginning resynchronization duration process shaded grey top left graph figure shows results default linux resync limit disk prefers availability reliability process takes seconds bandwidth time software raid resync default disk bandwidth time software raid resync disk bandwidth time software raid resync disk bandwidth time software raid resync journal-guided resync resync foreground vulnerability vulnerability type rate limit bandwidth window default default disk medium disk high disk journal-guided figure software raid resynchronization graphs plot bandwidth achieved foreground process performing sequential scans files software raid array system crash ensuing array resynchronization recovery period highlighted grey duration listed graphs bandwidth allocated resynchronization varied default disk disk disk final graph depicts recovery journal guidance table lists availability foreground service vulnerability array compared default resynchronization period seconds restart scan raw disk space raidarray time period foreground process bandwidth drops unimpeded rate resynchronization completes foreground process receives full bandwidth array linux resynchronization rate adjusted sysctl variable top graph figure shows effect raising resync limit disk representing middle ground reliability availability case resync takes seconds bandwidth afforded foreground activity drops bottom left graph resync rate set disk favoring reliability availability effect reducing resync time seconds foreground bandwidth drops period bottom graph figure demonstrates journal-guided resynchronization knowledge write activity crash performs work correct array inconsistencies process finishes seconds greatly reducing window vulnerability present previous approach foreground service activates access full bandwidth array increasing availability results experiments summarized table figure metric calculated period restart machine order compare default linux resynchronization resync processes sacrifice availability foreground bandwidth variability improve reliability array reducing vulnerability windows default journal-guided resync process hand improves availability foreground process reliability array reducing vulnerability default case important note execution time scan-based approach scales linearly raw size array journal-guided resynchronization hand dependent size journal expect complete matter seconds large arrays complexity table lists lines code counted number semicolons braces modified added linux software raid ext file system journaling modules modifications needed add verify read interface software raid module core functionality existed needed activated requested stripe ext involved hiding dirty buffers declared mode orig mod percent module lines lines lines change software raid ext journaling total table complexity linux modifications table lists lines code counting semicolons braces original linux source number modified added software raid ext file system journaling modules verify reads recovery majority occurred journaling module writing declare blocks commit phase performing careful resynchronization recovery point comparison experimental version linux raidbitmap logging consists approximately lines code increase raidalone journaling module increasing size modifications consist lines code change modules observations support claim leveraging functionality cooperating layers reduce complexity software system related work brown patterson examine software raid systems work availability benchmarks find linux solaris windows implementations offer differing policies reconstruction process regenerating data parity disk failure solaris windows favor reliability linux policy favors availability unlike work authors focus improving reconstruction processes identifying characteristics general benchmarking framework stodolsky examine parity logging raid layer improve performance small writes writing parity blocks directly disk store log parity update images batched written disk large sequential access similar discussion nvram logging authors require fault tolerant buffer store parity update log reliability performance efforts avoid small random writes support argument maintaining performance raid level logging complex undertaking veritas volume manager facilities address faster resynchronization dirty region log speed raidresynchronization examining regions active crash log requires extra writes author warns coarse-grained regions needed maintain acceptable write performance volume manager supports raidlogging non-volatile memory solid state disk recommended support extra log writes contrast declared mode offers fine-grained journal-guided resynchronization performance degradation additional hardware schindler augment raid interface provide information individual disks atropos volume manager exposes disk boundary track information provide efficient semi-sequential access two-dimensional data structures database tables similarly raid disk boundary performance information augment functionality informed file system verify read interface complex providing file system access functionality exists software raid layer conclusions examined ability journaling file system provide support faster software raid resynchronization order obtain record outstanding writes time crash introduce ext declared mode mode guarantees declare intentions journal writing data disk extra write activity declared mode performs predecessor order communicate information software raid layer file system utilizes verify read request request instructs raid layer read block repair redundant information combining features implement fast journal-guided resynchronization process improves software raid reliability availability hastening recovery process crash general approach advocates system-level view developing storage stack file system journal improve raid system leverages existing functionality maintains performance avoids duplicating complexity multiple layers layers implement abstractions protocols mechanisms policies interactions define properties system acknowledgements john bent nathan burnett anonymous reviewers excellent feedback work sponsored 
nsf ccrccr- ngsitr- network appliance emc jfs overview ibm developerworks library jfs html brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june clements bottomley high availability data replication proceedings linux symposium ottawa canada june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june emc emc centera content addressed storage system http emc emc corporation symmetrix enterprise information storage systems http emc mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june reiser reiserfs namesys rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february schindler schlosser shao ailamaki ganger atropos disk array volume manager orchestrated disks proceedings usenix symposium file storage technologies fast san francisco california april solomon inside windows microsoft programming series microsoft press stodolsky gibson holland parity logging overcoming small write problem redundant disk arrays proceedings annual international symposium computer architecture isca pages san diego california sun solaris volume manager administration guide http docs sun app docs doc july teigland mauelshagen volume managers linux proceedings usenix annual technical conference freenix track boston massachusetts june tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june tweedie journaling linux ext file system fourth annual linux expo durham north carolina tweedie ext journaling file system olstrans sourceforge net release ols -ext ols ext html july veritas features veritas volume manager unix veritas file system http veritas products volumemanager whitepaperhtml july 
geiger monitoring buffer cache virtual machine environment stephen jones andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison stjones dusseau remzi wisc abstract virtualization increasingly address server management administration issues flexible resource allocation service isolation workload migration virtualized environment virtual machine monitor vmm primary resource manager attractive target implementing system features scheduling caching monitoring lack runtime information vmm guest operating systems called semantic gap significant obstacle efficiently implementing kinds services paper explore techniques vmm passively infer information guest operating system unified buffer cache virtual memory system created prototype implementation techniques inside xen vmm called geiger show accurately infer pages inserted evicted system buffer cache explore nuances involved passively implementing eviction detection previously addressed importance tracking disk block liveness effect file system journaling importance accounting unified caches found modern operating systems case studies show information provided geiger enables vmm implement vmm-level services implement working set size estimator vmm make informed memory allocation decisions show vmm drastically improve hit rate remote storage caches eviction-based cache placement modifying application operating system storage interface case studies hint future inference techniques enable broad class vmm-level functionality categories subject descriptors organization design general terms design measurement performance keywords virtual machine inference permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee asplos october san jose california usa copyright acm reprinted asplos proceedings international conference architectural support programming languages operating systems october san jose california usa introduction virtualizationtechnology increasingly common component serverand desktop pcsystems asboth software hardware support forlow-overhead virtualization develops virtualization included popular commercial systems expect virtualized computing environments ubiquitous virtualization prevalent virtual machine monitor vmm naturally supplants operating system primary resource manager machine vmm attractive target implementing traditionally considered operating system features flexible resource management service device driver isolation load balancing security monitoring fault tolerance transition functionality vmm hasmany potentialbenefits forexample aguestoperating system isolated vmm stable virtual hardware interface services implemented vmm portable guest operating systems vmm place innovative features inserted system guest operating system legacy closedsource finally virtualized environment vmm theonly component thathas totalcontrol oversystem resources make informed resource management decisions pushing functionality layer software stack vmm disadvantages significant problem lack higher-level knowledge vmm referred semantic gap previous work virtualized environments partially recognized dilemma researchers developed techniques infer information guests utilize virtual hardware resources techniques vmm manage resources system effectively reallocating idle page thatcould useit paper describes techniques vmm infer information performance-critical software component operating system buffer cache virtual memory system specifically show vmm carefully observe guest operating system interactions virtual hardware mmu storage devices detect pages inserted evicted operating system buffer cache geiger isa prototype implementation techniques xen virtual machine monitor paper discuss detailsofgeiger simplementationand geiger eviction detection techniques geiger inferencing techniques vmm similar chen withina pseudo-device driver evaluation focuses geiger techniques needed circumstances show unified buffer caches virtualmemory systems found modern operating systems require vmm track disk traffic memory allocations show vmm basic storage system behavior account accurately detect cache eviction vmm track data block live dead disk order avoid reporting spurious evictions show journaling file systems ext linux require vmm distinguish writes journal writes parts storage avoid aliasing problem leads false eviction reporting summary passively detecting cache eventswithinmodern operating systemsrequiresnew sophistication techniques passive inferencing result inaccurate information worse information case studies demonstrate inferred eviction information provided geiger enable services inside vmm case study implement vmmbased working set size estimator complements existing techniques allowing estimation case virtual machine thrashing study explores geiger-inferred evictions vmm enable remote storage caches implement eviction-based cache placement changing application operating system storage interface enhancing adoption feature rest paper organized begin presenting extended motivation eviction information address related work section section describe techniques geiger detail section discusses implementation geiger section evaluate accuracy overhead prototype geiger implementation sections discuss case studies finally section summarize conclude motivation related work vmm understand hosted virtual machines memory section describe contextswhereavmmcan exploitinformation aboutbuffercache promotion eviction events working set size virtualized environment knowing working set size virtual machine allocating amount memory hosted environments popular common vms running simultaneously physical host situation knowing theworking setsize ofeach vmallows thevmm toallocate dynamically re-allocate amount memory competing migrating vms grid computing environment working set size information enables job scheduler intelligently select host adequate amount memory techniques estimating working set size virtual machine explored waldspurger part vmware esx server product esx server technique determine working set size virtual machines full allocation guest begins thrash sampling technique esx server simply reports working set memory allocation fact larger determine true working set size esx technique trial error vmm repeatedly giveathrashing virtualmachine incrementallymorememory re-measure working set size drops system physical memory accommodate full working set trial error method fail contrast vmm detect eviction events virtual machine vmm directly model hit rate virtual machine function amount memory result vmm quickly efficiently determine physical memory give competing virtual machine migrate virtual machine secondary-level caching avirtualized environment knowing contents buffer cache implementing effective secondary-level cache multiple vms run machine thevmmcanmanage ashared secondary cache initsownmemory thevmssharepages additionally hosted legacy system address large amount memory secondary cache enable legacy exceed natural addressing limits finally vmm explicitly communicate remote storage server cache informing itofwhich pages cached withineach designing secondary cache management policy non-trivial secondary storage caches exhibit locality client caches stream filtered client cache fact secondary storage caches size client caches led innovations cache replacement policies cache placement policies promising placement policy called eviction-based placement inserts blocks secondary cache evicted client cache approach make caches overlap leads effective secondary cache utilization eviction-based placement similar micro-architectural victim caches processor cache hierarchy modify eviction-based cache placement straight-forward wong extend block-based storage interface demote operation explicitly notifies interested parties remote storage caches pages evicted client caches goal modify vmm passive techniques infer page evicted page cache vmm explicitly notify storage cache passive eviction detection explored extent exclusive caching storage systems x-ray file system semantic information storage blocks inodes snoop updates file accessed time field knowing files recently accessed x-ray build approximate model client cache x-ray limited 
inferences storage system access block stream exclusive caching work assumed access information chen perform inferencing pseudo-device driver access addresses memory pages read written infer eviction occurred memory page storing disk data reused disk data geiger approach similar chen additional information toa vmm improve itsability infer cache events geiger builds previous work important ways geiger handles guest operating systems implement unified buffer cache virtual memory system unified buffer cache events disk evict pages geiger techniques handle anonymous memory allocation cache evictions geiger recognizes blocks disk freed block free disk reuse memory buffer recently held contents imply eviction distinction important counterproductive cache blocks file system believes free geiger supports journaling file systems file system writes block distinct locations disk occurs journaling file system geiger avoids reporting false evictions additional techniques geiger handle range situations occur modern operating systems geiger techniques section discuss techniques geiger begin providing relevant background virtual machine monitors andcontinue describing thebasictechniques geigerusestoinfer page cache promotion eviction describe geiger performs complex inferences handles unified buffer caches virtual memory systems present inallmodernoperating systems due storage system interactions background virtualization studied technique recently revitalized ubiquitous environment virtualized environment thin layer software virtual machine monitor vmm virtualizes components host computer system allowing guest operating systems safely share resources transparently key feature virtualized environment guest operating systems execute unprivileged mode processor vmm runs full privilege guest accesses sensitive system components mmu peripherals exception control transferred vmm point vmm decide action emulate behavior privileged hardware operation software paper advantage vmm entry points observe interesting activity geiger observes architecturally visible events page faults page table updates disk reads disk writes infer occurrence buffer cache eviction promotion major benefit observing entry points performance adding small piece observation code points induces overhead basic techniques buffer cache promotion occurs disk page added cache buffer cache eviction occurs cache page freed operating system previous contents remain reloaded disk eviction occurs contents anonymous page written swap partition page freed similarly eviction occurs page read file system freed writing back disk data reloaded original location disk eviction occur frees page contents lost anonymous page process exits detect promotion eviction geiger performs tasks geiger tracks contents page diskand ifso wecalltheondisk location memory page page disk location adl geiger detect page freed describe steps turn disk locations geiger associates disk location physical memory page disk location adl issimply pair device block offset representing recent disk location vmm associate page vmm associates disk location memory page page involved disk read write operation page target read disk location page similarly page source write disk location page associations persist replaced association memory page freed relevant disk blocks freed vmm virtualizes disk disk reads writes initiatedby guest areexplicitly visibleto thevmm special action part vmm required establish adl page tocorrectlyinvalidate anadlwhen thedisk block refers longer requires detecting disk block freed discuss section detecting page reuse geiger mustalso determine memory page freed guest explicitly notify vmm frees page difference active free page entry private data structure free list bitmap assume vmm detailed os-specific information required locate interpret data structures detecting page freed geiger detects page reused reuse implies page freed proxy page free event geiger numerous heuristics detect page reused heuristic corresponds scenario guest allocates page memory geiger detects page allocation newly allocated page current adl geiger signals previous contents page defined adl evicted basic techniques geiger monitoring disk reads disk writes builds previous work chen monitors reads writes device driver disk read geiger disk reads infer page allocated page read disk page allocated buffer cache allocated page current adl refers disk location read geiger reports page previous contents evicted adl affected page updated point disk location consequence kind eviction disk write geiger disk writes infer page allocated full page data written disk page reside page cache allocate page buffer data asynchronously written disk geiger detects case observing disk writes signaling eviction write source page current adl target disk location write note thatifa previous read writecaused diskblock toalready exist cache geiger erroneously signal duplicate eviction page adl change read-eviction heuristic adl affected page updated refer target disk location techniques unified caches techniques previous research work old-style file system buffer caches distinct virtual memory system virtually modern operating systems including linux bsd solaris windows unified buffer cache virtual memory system unification complicates inferences geiger detect page reuse additional cases virtual memory system introduce detection techniques microbenchmark description read evict sequentially reads section file larger memory multiple times write evict sequentially writes file larger memory repeated multiple times cow evict allocates memory buffer approximately size physical memory writes virtual page ensure physical page allocated forks writes page child allocation evict allocates memory buffer exceeds size memory writes virtual page ensure page allocated figure microbenchmark workloads table describes microbenchmarks isolate specific type page eviction application description read write cow alloc dbench file system benchmark simulating load network file server mogrify scales converts large bitmap image osdl-dbt tpc-w-like web commerce benchmark simulating web purchase transactions online store spc web search storage performance council block device traces web search engine server traces replayed real file system figure application workloads table describes application workloads reports percentage total eviction events caused eviction type copy write copy-on-write cow technique widely operating systems implement efficient read sharing memory page shared cow marked read-only process virtual address space shares processes attempts write cow-shared page action page fault operating system transparently allocates private page copies data page page subsequently writable virtual memory mapping established refers page private copy requires allocation free page lead page reuse geiger detects page reuse occurs result cow observing page faults page table updates geiger detects page fault write read-only page saves affected virtual address page table entry small queue short time guest creates writable mapping virtual address physical page geiger infers physical page newly allocated newly allocated page active adl geiger signals eviction modified private copy existing page disk location allocation modern operating systems allocate memory lazily application requests memory brk anonymous mmap immediately allocate physical memory virtual address range reserved physical memory allocated on-demand page accessed property means physical memory allocation occurs context servicing page 
fault similar cow heuristic geiger observes page faults due guest accessing virtual page virtualto-physical mapping saves affected virtual address small queue short time guest creates writable mapping faulting virtual address geiger infers page allocation newly allocated physical page current adl geiger signals eviction techniques storage storage systems introduce nuances inferences made geiger file system features journaling lead aliasing problem fact disk blocks deleted leads problem liveness detection describe issues geiger handles turn journaling basic write heuristic signals eviction contents page adl written location disk match adl page adl written disk location eviction reported contents disk location basic write heuristic over-reports evictions cases data written buffer cache page multiple disk locations view aliasing problem page wrongly disk addresses journaling file systems linux ext reiserfs jfs andxfs diskfrom cache page journal location fixed disk location inthe worst-case journaling scenario dataand metadata written journal actual number write evictions reported common case metadata-only journaling smaller penalty incurred negative effect journaling virtual memory mitigated vmm identifies reads writes file system journal straightforward systems journal separate easily identifiable partition file file system partition made file system superblock avoid problem journal aliasing geiger monitors disk addresses write requests ignores writes directed journal block liveness geiger signals page evicted page current adl blocks adl refers deallocated disk time adl mapping established geiger detects page reused case geiger falsely report eviction adl exists data adl refers deallocated longer accessible problem block liveness lead large numbers false evictions workloads files regularly deleted truncated processes die significant parts virtual memory swapped disk eviction count time allocation eviction actual inferred eviction count time cow eviction actual inferred eviction count time read eviction actual inferred eviction count time write eviction actual inferred figure eviction inference counts figure compares inferred actual eviction counts time microbenchmarks isolate eviction type inferred geiger fraction evictions lag time allocation lag fraction evictions lag time cow lag fraction evictions lag time read lag fraction evictions lag time write lag figure eviction lag figure shows cumulative lag distribution microbenchmarks isolate eviction type file systems virtual machine monitor passively track file systemblock liveness inthesameway asmartdisksystem cantrack block liveness allocation state file system block typically noted on-disk structure bitmap file system superblock stored fixed location disk locate bitmap structures examining guest operating system writes on-disk areas vmm snoop file system determine disk blocks adl refers freed blocks adl refers deallocated adl invalidated future reuse affected page misinterpreted eviction implementing block liveness observing disk writes significant drawback substantial lag file system structure allocation bitmap updated memory written disk operating systems interval seconds geiger observe file system blocks page adl points deallocated page reused false eviction occur timeliness block deallocation notification important avmmcan deallocation notification tracking updates in-memory versions allocation bitmaps locations bitmaps vmm observe bitmaps loaded disk memory time vmm mark buffers read-only guest updates in-memory bitmap minor page fault occur vmm observe fault due attempted bitmap update respond invalidating affected adls geigerimplements thisstyleof in-memory block livenesstracking bitmap blocks identified reading parsing file system superblock forknown filesystem types pagesused tocache file system allocation bitmaps marked read-only memory geiger write page detected due page protection fault effect faulting instruction emulated guest memory register state faulting instruction skipped observed handled vmm overhead block liveness tracking low spite additional minor page faults due low frequency disk bitmap updates sivathanu embedding file system layout information format superblock vmm reasonable technique commonly file systems on-disk data structure formats file systems change slowly vmm provided layout information commonly file systems information expected remain valid long time on-disk format ext changed introduction longer interval typical system software upgrade cycle swapspace liveness tracking techniques geiger file system partitions apply disk space swap area rule swap space on-disk data structures track block allocation data swap expected persist system restarts swap allocation information managed exclusively volatile system memory swap liveness tracking techniques found effective workloads preventing false evictions due adls point deallocated swap space technique invalidates adl points set disk blocks overwritten disk blocks overwritten data adl refers destroyed adl invalidation isappropriate thistechnique isimplemented maintaining reverse mapping cached disk blocks adls technique makes implicitly obtained process lifetime information provided antfarm accurate information guest processes mapping memory pages owning process adls invalidated process exits specifically adl page belonging dead process points swap space disk block invalidated technique appears promising buthas fully implemented inthe current version ofgeiger implementation geiger implemented extension xen virtual machine monitor version xen open source virtual machine monitor intel architecture xen paravirtualized processor interface enables lower overhead virtualization expense porting system software explicitly make feature xen mechanisms describe equally applicable conventional virtual machine monitor vmware geiger consists set patches xen hypervisor xen block device backends concentrated handlers events page faults page table updates block device reads writes geiger patches consist approximately workload false neg false pos read evict write evict cow evict alloc evict figure microbenchmark heuristic accuracy table reports false positive false negative ratios complete set eviction heuristics microbenchmark workloads workload journal opt journal opt neg pos neg pos journal metadata data figure effect journaling table reports false positive false negative ratios write-eviction microbenchmark workload run journaling metadata journaling ordered mode data journaling linux ext file system table shows benefits turning geiger specialization detect writes journal lines code files files xen hypervisor linux kernel required small order implement instrumentation tracing allexperiments paper performed witha ghzpentiumivprocessor gbofsystem memory ata disk drives linux kernel version xen control domain linux kernel version unprivileged domains ext ext file system depending theexperiment thexencontrol domain configured memory noted unprivileged guest virtualmachine isassigned mbof memory evaluation section evaluate ability geiger accurately infer page cache evictions promotions occurring guest operating systems begin describing workloads metrics evaluate geiger set microbenchmarks application workloads conclude measuring overheads geiger imposes system workloads experimental evaluation geiger sets workloads workload set consists microbenchmarks microbenchmarks constructed generate specific type page cache eviction read write copy-on-write cow orallocate thesemicrobenchmarks isolate geiger ability track evictions due specific events microbenchmarks detail figure set workloads consists application benchmarks represent realistic workloads workload mix eviction types read write cow allocation figure lists application workloads breakdown eviction types generated application workloads 
stress geiger ability track evictions occur reasons metrics methodology evaluating accuracy geiger compare trace evictions signaled geiger trace evictions produced guest operating system modified linux kernel generate trace guest operating system complete information pages evicted comparison ideal eviction detector workload geiger opts false neg false pos dbench block liveness dbench block liveness mogrify block liveness mogrify block liveness tpc-w spc web figure application heuristic accuracy table reports false positive false negative ratios geiger application workloads dbench mogrify workloads evaluate geiger optimizations detect block live disk alloc-evict thrash dbench thrash alloc-evict thrash dbench thrash runtime geiger runtime overhead unmodified xen geiger figure geiger runtime overhead figure shows geiger imposes small runtime overheads workloads stress inference heuristics eviction records traces physical memory address disk address evicted data time stamp weconsider thefirstmetric simply eviction count reported geiger compared reported guest time metric detection lag time eviction takes place detected geiger finally metric detection accuracy tracks percentage records inferred actual traces match one-toone mapping report percentage false negatives actual evictions detected geiger false positives toos-reportedevictions microbenchmarks begin running workloads consisting microbenchmarks figure shows resulting eviction count time-lines microbenchmarks eviction counts inferred geiger closely match actual counts depending workload interesting differences occur cow workload guest reclaims pages groups leading slight stair-step eviction pattern geiger inferences lag slightly case write workload guest begins evicting pages early continues evict eagerly experiment pages reused time geiger inferences based page reuse eviction detected page reused inferred evictions lag noticeably actual evictions caused writes figure shows thecumulative microbenchmarks expected lag times read cow allocation eviction concentrated small values lag times write microbenchmark concentrated seconds due operating system eager reclamation behavior figure reports geiger detection accuracy false negatives false positives workloads false negatives uncommon worst fewer total number evictions missed geiger false positives common worst geiger over-reports inferred evictions final microbenchmark experiment explore geiger ability detect aliased writes file system journal write workload stress detection figure shows accuracy geiger specialization disregard write traffic file system journal specialization geiger performs satisfactorily journaling disabled metadata journaled linux ext ordered-mode metadata journaling blocks aliases data journaling blocks aliases result half evictions reported un-specialized geiger false positives contrast full version geiger accurately handles journaling modes linux ext data journaling geiger false positive percentage application benchmarks workloads realistic applications figure reports detection accuracy geiger application workloads workloads false negative ratios small worst case geiger misses evictions reported dbench mogrify workloads interesting behavior false positives block liveness dbench mogrify workloads illustrate benefit geiger attempt track liveness block disk dbench creates deletes files result pages memory reused files disk blocks mogrify large amounts swap allocated deallocated execution vmm change association memory page disk block infer eviction vmm concludes evictions occurred false positives live block detection geiger false positive rate dbench false positive rate mogrify geiger tracks disk block free detect page simply reused previous contents evicted result false positive rate improves dramatically dbench mogrify adequately handle deleteintensive truncate-intensive workloads geiger includes techniques track disk block liveness limitations mentioned previously expect current techniques tracking block liveness swap space adequate situations demonstrate remaining problem microbenchmark crafted results large numbers false positives efforts geiger track block liveness program forces large buffer allocated mmap swapped disk buffer released linux buffer released swap space deallocated geiger detect event additional memory allocated program pages reused adls point deallocated swap space resulting eviction false positive ratio overhead geiger observes events intrinsically visible vmm page faults page table updates disk case disk block liveness tracking additional memory protection figure memrxoperation figure shows schematic cache simulation implemented memrx page evicted guest event detected geiger entry added head series queues queue entries ripple tail queue head reload queue entry removed array entry queue incremented entry tracks sub-queue appears enable fast depth estimation traps requests caused geiger liveness tracking imposes additional minor page fault disk bitmap update occur rarely expect runtime overhead imposed geiger small validate expectation compare runtime workloads running unmodified version xen geiger interested performance regimes regime common case workload sufficient memory evictions occur regime occurs machine thrashing implies evictions taking place geiger inference mechanisms stressed evaluate cases carefully chosen workloads geiger interposes code paths handling page faults page table updates disk microbenchmark allocation-evict figure dbench figure allocation-evict page faults page-table updates stressing portion geiger inference machinery dbench large number file creations reads writes deletes exercise portions geiger heuristics figure shows results experiment shown average runs standard deviation shown error bars largest observed overhead occurs thrashing dbench cases results geiger unmodified xen comparable geiger requires extra space physical memory page track adls prototype amounts bytes memory page testsystem configured physical memory total additional memory allocated vmm leading space overhead approximately space overhead concern substantially reduced preallocated fixed size sparsely-populated data structures prototype hardware trends major microprocessor vendors intel amd ibm begun include optional hardware virtualization features server desktop products reduce overhead imposed virtualization features hide information vmm favor reducing guest-to-vmm transitions cases page faults guest page table updates vmm invoked geiger page fault page table update information detect cache eviction events absence impact geiger functionality future benchmark activity sequential sequentially scan section file system file times sequential sequentially scan section allocated virtual memory times random randomly read page-sized blocks file system file times random randomly touch virtual memory pages virtual memory allocation times figure calibrated microbenchmarks table describes microbenchmarks evaluate vmm-memrx techniques required provide geiger-like functionality latest vmm-aware architectures summary order handle modern operating systems unified system caches journaling file systems geiger number sophisticated inferencing techniques microbenchmarks geiger highly accurate measured eviction counts oneto-one eviction accuracy write-intensive workloads geiger experience significant lag detects eviction occurred application workloads geiger swap-intensive workloads additional techniques required avoid detection false evictions due difficulty tracking liveness swap shown geiger full range techniques needed circumstances cow allocation techniques needed handle mogrify tpc-w workloads unified buffer cache live block detection improves accuracy delete-intensive workloads finally writes journal isolated handle file system data journaling case study working set size estimation eviction detection techniques geiger implementing number pieces functionality case study show geiger implement memrx vmm tracks working sets guest vms begin describing implementation memrx present 
performance results memrx implementation previous research waldspurger esx server shown vmm determine system working set size memory footprint fits physical memory memrx complements esx server technique enabling vmm determine working set size thrashing virtual machine memrx accomplishes thisusing geiger toobserve evictions subsequent reloads guest operating system buffer cache memrx quantifies number memory accesses transformed misses hits memory sizes memrx simulates page cache behavior virtual machine memory increment method similar patterson ghost buffering figure shows schematic page cache simulation implemented memrx page evicted page location disk inserted head queue maintained lru order memrx subsequent evictions push previous deeper inthe queue previously evicted page read disk page removed queue distance head queue computed distance approximately equal number evictions place page eviction subsequent reload memrx compute amount memory required prevent original eviction taking place sizepage information compute miss-ratio curve working set size read miss-ratio curve locating curve primary knee evaluation evaluate accuracy memrx measure working set size microbenchmark workloads working set size approximately table lists microbenchmarks actions perform working set size approximately virtual machine configured memory compare working set size predicted memrx working set size determined trialand errorfor realisticapplication workloads mogrify dbench figure shows predicted actual miss ratio curves microbenchmark workloads miss ratio curve shows fraction capacity cache misses occurring smallest memory configuration remain misses larger memory configurations predicted curve calculated memrx measurements single run smallest memory configuration simulating page cache behavior guest operating system on-line larger memory configurations increments actual curve calculated running workload noted memory sizes counting actual capacity misses page cache calibrated tests show memrx locate working set size simple workloads accurately prediction made memrx identical found direct measurement trial error result surprising simple workloads geiger incurs eviction false positives figure shows results application workloads mogrify dbench leftmost graphs show predicted actual missratiocurves cases inferred working set size predicted memrx slightly larger actual working set size found trial error determine discrepancy due geiger false positive negative evictions lag memrx cache simulation error implemented memrx linux compared predicted actual miss ratio curves produced version operating system memrx access precise eviction promotion information eliminates geiger source error rightmost graphs figure show miss ratio curves obtained mogrify dbench workloads operating system implementation memrx dbench workload version memrx shows deviation produced memrx vmm leads conclude deviation memrx simulation error memrx models guest buffer cache strict lru policy match policy linux akin difference modeled policy true policy leads simulation errors shown case mogrify os-based miss ratio curve matches actual curve closely leading error observed vmmpredicted working set size due small inference errors imposed geiger granularity experiment summary information provided geiger enabling vmm estimate working set sizes thrashing vms predictions made memrx accurate highly allocating memory competing vms single machine selecting target host virtual machine migration miss ratio memory size sequential predicted actual miss ratio memory size sequential predicted actual miss ratio memory size random predicted actual miss ratio memory size random predicted actual figure vmm-memrxpredicted actual miss ratio figure shows miss ratio predicted vmm-memrx actual miss ratio measured varying memory sizes working set marked vertical dashed line miss ratio memory size dbench vmm predicted actual miss ratio memory size mogrify vmm predicted actual miss ratio memory size dbench predicted actual miss ratio memory size mogrify predicted actual figure application predicted actual miss ratio figure shows miss ratio curve predicted memrx actual miss ratio measured varying memory sizes application workloads results memrx implemented vmm left memrx implemented shown cache hit ratio cache size mogrify eviction-os eviction-geiger eviction-buffer demand cache hit ratio cache size dbench cache hit ratio cache size tpc-w cache hit ratio cache size spc web search figure secondary cache hit ratio figure compares cache hit ratio secondary storage cache workloads demand placement demand eviction placement based inferred evictions eviction-buffer eviction-geiger eviction placement based actual evictions eviction-os experiments performed cache sizes case study eviction-based cache placement case study show geiger convey eviction information secondary cache basic idea vmm geiger infer pages evicted buffer cache sends information demoteoperation tothe storage server ispotentially remote storage server explicitinformation perform eviction-based cache placement implementation implementation eviction-based secondary cache components vmm interposes virtual block device interface provided byxen tosee block request streamgenerated workload vmm geiger infer blocks evicted page cache events communicated remote storage server simulate behavior storage server actual trace gathered running geiger workload input refer approach eviction-geiger evaluate implementation compare alternatives approach eviction-os operating system modified report actual evictions represents ideal case approach eviction-buffer vmm performs eviction detections client buffer addresses chen read write evictions finally simulate storage cache information client evictions performs traditional demand-based placement cases lru-based replacement policy evaluation application workloads listed figure evaluate vmm-implementation eviction-based cache placement workload remote caches evaluate placement policies eviction-os evictiongeiger eviction-buffer demand metric cache hit ratio figure shows graphs cache hit ratio cache size workloads cache policies cases geiger eviction-based placement outperform demand-based placement significantly largest gains occur moderate cache sizes working set application fits client cache storage cache individually fit aggregate cache geiger evictionbased placement improve cache hit rate percentage points workloads mogrify workload secondary cache size cache hit ratio demand placement eviction-based placement secondary cache size large full system working set geiger eviction-based placement perform similarly demandbased placement case spc web search traces exhibit locality results included completeness workload dbench eviction-based placement support outperforms inferred evictions geiger secondary cache size observe difference hit rates percentage points performance difference due significant time lag actual inferred write-eviction events approximately seconds events experiment inferred evictions delayed secondary cache loses opportunity place block prior block referenced client cache miss occurs eviction-based approaches perform significantly demand-based placement buffer fact eviction-buffer performs significantly worse straight-forward demand-based placement problem occurs eviction-buffer detect fewer evictions occur large false negatives workloads mogrify tpc-w significant number non-i based evictions occur missing evictions lead poorer cache performance missing evictions problem large secondary caches blocks effectively adequate cache space case tpc-w missing eviction events change cache hit rate percentage points mogrify difference points summary geiger effectively notify secondary cache evictions performed clients confirmed studies secondary caches eviction-based placement perform demand-based placement results show eviction information provided geiger good provided directly 
modified exception occurs significant lag occurs time actual eviction inference case geiger enables hit rates simple demand-based placement eviction-based placement essential miss evictions clients eviction detection based reads writes miss important evictions leading hitrates thatare worse simple demand-based placement full set techniques geiger buffer cache inferences conclusion backend servers desktop pcs virtualization commonplace virtual machine monitor sole resource manager system pieces interesting functionality migrate operating system vmm key enabling vmm-level functionality information knowledge os-level constructs typically needed implement features paper explored techniques required make inferences pages added removed buffercache wehave found thatcertain key featuresof modern operating systems including unified buffer caches andvirtualmemorysystems journalingfilesystems diskblock liveness techniques efficient implement prototype case studies working set size estimator eviction-based cache placement second-level caches inferring information boundary vmm itsguestoperating systemsisapowerful technique thatenables systems innovation implemented portably vmm accurate timely general techniques made successfully applied commercial domain acknowledgments anonymous reviewers thoughtful comments suggestions research sponsored sandia national laboratories doctoral studies program nsf ccritr- cnsand generous donations network appliance emc arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca munich germany june ballmer keynote address microsoft management summit april jfs overview ibm developerworks library jfs html bressoud schneider hypervisor-based fault tolerance sosp proceedings fifteenth acm symposium operating systems principles pages acm press bugnion devine rosenblum disco running commodity operating systems scalable multiprocessors proceedings acm symposium operating systems principles sosp pages saint-malo france october chen noble virtual real hotos proceedings eighth workshop hot topics operating systems page ieee computer society chen ahang zhou scott schiefer empirical evaluation multi-level buffer cache collaboration storage systems proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics banff canada june chen zhou eviction-based placement storage caches proceedings usenix annual technical conference usenix pages san antonio texas june clark fraser hand hansen jul limpach pratt warfield live migration virtual machines proceedings symposium networked systems design implementation nsdi boston massachusetts denning working set model program behavior communications acm denning working sets past present ieee transactions software engineering sejanuary dragovic fraser hand harris pratt warfield barham neugebauer xen art virtualization proceedings acm symposium operating systems principles sosp bolton landing lake george york october figueriredo dinda fortes case grid computing virtual machines proceedings international conference distributed computing systems icdcs fraser hand neugebauer pratt warfield williamson safe hardware access xen virtual machine monitor oasis asplos workshop garfinkel pfaff chow rosenblum boneh terra virtual machine-based platform trusted computing proceedings acm symposium operating systems principles sosp bolton landing lake george york october goldberg survey virtual machine research ieee computer gum system extended architecture facilities virtual machines ibm journal research development november imagemagick studio llc imagemagick image processing software http imagemagick intel corporation intel virtualization technology specification ftp download intel technology computing vptech pdf johnson shasha low-overhead high performance buffer management replacement algorithm proceedings international conference large databases vldb pages santiago chile september jones arpaci-dusseau arpaci-dusseau antfarm tracking processes virtual machine environment proceedings usenix annual technical conference usenix boston massachusetts june jouppi improving direct-mapped cache performance addition small fully-associative cache prefetch buffers proceedings annual international symposium computer architecture isca pages seattle washington king chen backtracking intrusions proceedings acm symposium operating systems principles sosp banff canada october muntz honeyman multi-level caching distributed file systems cache ain nuthin trash proceedings usenix winter conference pages january open source development labs osdl database test suite http osdl lab activities kernel testing osdl database test suite patterson gibson ginting stodolsky zelenka informed prefetching caching proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december reiser reiserfs namesys sapuntzakis chandra pfaff chow lam rosenblum optimizing migration virtual computers proceedings symposium operating systems design implementation osdi pages boston massachusetts december sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau lifeordeath atblock level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid proceedings usenix symposium file storage technologies fast san francisco march storage performance council spc web search engine storage traces http traces umass storage sugerman venkitachalam lim virtualizing devices vmware workstation hosted virtual machine monitor proceedings usenix annual technical conference usenix boston massachusetts june sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january tridgell dbench filesystem benchmark http samba ftp tridge dbench tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june tweedie ext journaling filesystem olstrans sourceforge net release ols -ext ols -ext html july waldspurger memory resource management vmware esx server proceedings symposium operating systems design implementation osdi boston massachusetts december whitaker shaw gribble scale performance denali isolation kernel proceedings symposium operating systems design implementation osdi boston massachusetts december wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june zhao zhang figueriredo distributed file system support virtual machines grid computing proceedings high performance distributed computing hpdc july zhou philbin multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference usenix pages boston massachusetts june 
improving storage system availability d-graid muthian sivathanu vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau wisconsin madison present design implementation evaluation d-graid gracefully degrading quickly recovering raid storage array d-graid ensures files file system remain unexpectedly high number faults occur d-graid achieves high availability aggressive replication semantically critical data fault-isolated placement logically related data d-graid recovers failures quickly restoring live file system data hot spare graceful degradation live-block recovery implemented prototype scsi-based storage system underneath unmodified file systems demonstrating powerful file-system functionality implemented semantically smart disk system narrow block-based interface categories subject descriptors operating systems storage management secondary storage operating systems reliability fault tolerance general terms design algorithms reliability additional key words phrases disk array raid block-based storage fault isolation file systems smart disks introduction tree falls forest hears make sound george berkeley storage systems comprised multiple disks backbone modern computing centers storage system entire center grind halt downtime expensive on-line business world millions dollars hour lost systems keeton wilkes patterson work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation earlier version article appeared proceedings usenix symposium file storage technologies fast san francisco authors addresses computer sciences statistics wisconsin madison dayton street madison muthian vijayan dusseau remzi wisc permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit direct commercial advantage copies show notice page initial screen display full citation copyrights components work owned acm honored abstracting credit permitted copy republish post servers redistribute lists component work works requires prior specific permission fee permissions requested dept acm broadway york usa permissions acm acm acm transactions storage vol pages sivathanu storage system availability formally defined time failure mtbf divided sum mtbf time recovery mttr mtbf mtbf mttr gray order improve availability increase mtbf decrease mttr surprisingly researchers studied components storage availability increase time failures large storage array data redundancy techniques applied bitton gray burkhard menon chen gray hsiao dewitt orji solworth park balasubramanian patterson savage wilkes wilkes keeping multiple copies blocks sophisticated redundancy schemes parity-encoding storage systems tolerate small fixed number faults decrease time recovery hot spares employed holland menon mattson park balasubramanian reddy banerjee failure occurs spare disk activated filled reconstructed data returning system normal operating mode quickly problem reduced availability due semantic ignorance techniques proposed improve storage availability narrow interface file systems storage ganger curtailed opportunities improving mtbf mttr raid redundancy schemes typically export simple failure model fewer disks fail raid continues operate correctly disks fail raid unavailable problem corrected time-consuming restore tape raid schemes small disks working users observe failed disk system availability cliff result storage system laying blocks oblivious semantic importance relationship files corrupted inaccessible extra disk failure storage array information blocks live file system recovery process restore blocks disk unnecessary work slows recovery reduces availability ideal storage array fails gracefully disks system data unavailable ideal array recovers intelligently restoring live data effect important data disappear failure data restored earlier recovery strategy data availability stems berkeley observation falling trees file isn process access recovered failure solution d-graid explore concepts provide storage array graceful failure semantics present design implementation evaluation d-graid raid system degrades gracefully recovers quickly d-graid exploits semantic intelligence sivathanu disk array place file system structures disks fault-contained acm transactions storage vol improving storage system availability d-graid manner analogous fault containment techniques found hive operating system chapin distributed file systems saito unexpected double failure occurs gray d-graid continues operation serving files accessed d-graid utilizes semantic knowledge recovery specifically blocks file system considers live restored hot spare aspects d-graid combine improve effective availability storage array note d-graid techniques complementary existing redundancy schemes storage administrator configures d-graid array utilize raid level single disk fail data loss additional failures lead proportional fraction unavailable data article present prototype implementation d-graid refer alexander alexander semantically-smart disk system sivathanu built underneath narrow block-based scsi storage interface disk system understands on-disk file system data structures including superblock allocation bitmaps inodes directories important structures knowledge central implementing graceful degradation quick recovery intricate understanding file system structures operations semantically smart arrays tailored file systems alexander functions underneath unmodified linux ext vfat file systems make important contributions semantic disk technology deepen understanding build semantically smart disk systems operate correctly imperfect file system knowledge demonstrate technology applied underneath widely varying file systems demonstrate semantic knowledge raid system apply redundancy techniques based type data improving availability key techniques key aspects alexander implementation graceful degradation selective meta-data replication inwhich alexander replicates naming system meta-data structures file system high degree standard redundancy techniques data small amount overhead excess failures render entire array unavailable entire directory hierarchy traversed fraction files missing proportional number missing disks fault-isolated data placement strategy ensure semantically meaningful data units failure alexander places semantically related blocks blocks file storage array unit fault-containment disk observing natural failure boundaries found array failures make semantically related groups blocks unavailable leaving rest file system intact fault-isolated data placement improves availability cost related blocks longer striped drives reducing natural acm transactions storage vol sivathanu benefits parallelism found raid techniques ganger remedy alexander implements access-driven diffusion improve throughput frequently-accessed files spreading copy blocks hot files drives system alexander monitors access data determine files replicate fashion finds space replicas preconfigured performance reserve opportunistically unused portions storage system evaluate availability improvements d-graid trace analysis simulation find d-graid excellent job masking arbitrary number failures processes enabling continued access important data evaluate prototype alexander microbenchmarks trace-driven workloads find construction d-graid feasible imperfect semantic knowledge powerful functionality implemented block-based storage array find run-time overheads d-graid small storage-level cpu costs compared standard array high show access-driven diffusion crucial performance live-block recovery effective disks under-utilized combination replication data placement recovery techniques results storage system improves availability maintaining high level performance rest article structured section present extended motivation section discuss related work present design principles d-graid section section present trace analysis simulations discuss semantic knowledge section section present prototype implementation evaluate prototype section section present custom policies levels d-graid discuss resilience d-graid incorrect information section conclude section extended motivation section discuss graceful degradation multiple failures describe semantically smart disk system locale incorporate support graceful 
degradation case graceful degradation motivation graceful degradation arises fact users applications require entire contents volume present matters set files question arises realistic expect catastrophic failure scenario raid system raidsystem high mtbf reported disk manufacturers disk failure highly occur failed disk repaired multiple disk failures occur primary reasons correlated faults common systems expected gribble raid carefully designed orthogonal manner single controller fault component error render fair number acm transactions storage vol improving storage system availability d-graid disks unavailable chen redundant designs expensive found higher end storage arrays gray pointed system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure gray evidence suggests multiple failures occur ibm serveraid array controller product includes directions attempt data recovery multiple disk failures occur raidstorage array ibm organization data stored file servers raidin servers single disk failed indicator informed administrators problem problem discovered disk array failed full restore backup ran days scenario graceful degradation enabled access large fraction user data long restore approach dealing multiple failures employ higher level redundancy alvarez burkhard menon enabling storage array tolerate greater number failures loss data techniques expensive three-way data mirroring bandwidth-intensive write redundant store graceful degradation complementary techniques storage administrators choose level redundancy common case faults graceful degradation enacted worse expected fault occurs mitigating ill effect semantically smart storage basic design principles d-graid apply equally implementation alternatives tradeoffs subsection motivate decision implement d-graid semantically smart disk system discuss benefits approach addressing obvious concerns compare semantic disk approach alternatives implementing d-graid benefits semantic disk approach implementing functionality semantically smart disk system key benefit enabling wide-scale deployment underneath unmodified scsi interface modification working smoothly existing file systems software base desire evolve interface file systems storage gibson reality current interfaces survive longer anticipated bill joy systems protocols live forever similarly modern processors innovate beneath unchanged instruction sets semantic disk-level implementation nonintrusive existing infrastructure making technology d-graid adopted semantically smart storage systems require detailed knowledge file system concerns arise commercial feasibility systems main concerns acm transactions storage vol sivathanu concern arises placing semantic knowledge disk system ties disk system intimately file system on-disk structure file system storage system change issue problematic on-disk formats evolve slowly reasons backward compatibility basic structure ffs-based file systems changed introduction period years mckusick linux ext file system introduced roughly exact layout lifetime finally ext journaling file system tweedie backward compatible ext on-disk layout extensions freebsd file system dowse malone backward compatible evidence storage vendors maintain support software specific file system emc symmetrix storage system emc corporation software understand format common file systems concern storage system semantic knowledge file system interacts fortunately large number file systems supported cover large fraction usage population semantic storage system file system support storage system detect turn special functionality case d-graid revert normal raid layout detection simple techniques observing file system identifier partition table final concern arises processing required disk system major issue general trend increasing disk system intelligence acharya riedel processing power increases disk systems substantial computational abilities modern storage arrays exhibit fruits moore law emc symmetrix storage server configured processors ram emc corporation comparison alternative approaches semantic disk approach clear benefits detailed cost rediscovering semantic knowledge underneath modern file system entails fair amount complexity alternative approach change interface file systems storage convey richer information layers instance storage system expose failure boundaries file system denehy file system explicitly allocate blocks fault-isolated manner placing semantically related blocks alternatively file system tag write logical fault-container storage system implement fault-isolated data placement techniques conceivably complex approach drawback intrusive existing infrastructure software base requiring wide industry agreement adopted acm transactions storage vol improving storage system availability d-graid object-based storage gibson interface considered makes file boundaries visible storage layer object-based interface semantically smart technology relevant discover semantic relationships objects instance inferring directory object points set file objects single fault boundary finally approximate version fault-isolated layout implemented traditional block based storage system semantic understanding storage system simply identify sequences blocks accessed infer blocks logically related main disadvantage black-box approach fragile concurrent interleavings independent streams scheme identify critical data purposes aggressive replication hot blocks cached file system frequent reads visible storage system related work d-graid draws related work number areas including distributed file systems traditional raid systems discuss turn distributed file systems designers distributed file systems long ago realized problems arise spreading directory tree machines system walker discussed importance directory namespace replication locus distributed system popek coda mobile file system takes explicit care regard directory tree kistler satyanarayanan specifically file cached coda makes cache directory root directory tree coda guarantee file remains accessible disconnection occur interesting extension work reconsider host-based in-memory caching availability mind slice anderson route namespace operations files directory server recently work wide-area file systems reemphasized importance directory tree pangaea file system aggressively replicates entire tree root node file accessed saito island-based file system points fault isolation context wide-area storage systems island principle similar fault-isolated placement d-graid finally systems past place entire file single machine similar load balancing issues rowstron druschel problem difficult space due constraints file placement block migration simpler centralized storage array traditional raid systems draw long history research classic raid systems autoraid wilkes learned complex functionality acm transactions storage vol sivathanu embedded modern storage array background activity utilized successfully environment afraid savage wilkes learned flexible tradeoff performance reliability delaying updates raid research focused redundancy schemes early work stressed ability tolerate single-disk failures bitton gray park balasubramanian patterson research introduced notion tolerating multiple-disk failures array alvarez burkhard menon stress work complementary line research traditional techniques ensure full file system availability number failures d-graid techniques ensure graceful degradation additional failures related approach parity striping gray stripes parity data parity striping achieve primitive form fault isolation layout oblivious semantics data blocks level redundancy irrespective importance meta-data data multiple failures make entire file system inaccessible number earlier works emphasized importance hot sparing speed recovery time raid arrays holland menon mattson park balasubramanian work semantic recovery complementary approaches finally note term graceful degradation refer performance characteristics redundant disk systems failure hsiao dewitt reddy banerjee type graceful degradation 
discuss article systems continues operation unexpected number failures occurs design d-graid expectations discuss design d-graid present background information file systems data layout strategy required enable graceful degradation important design issues arise due layout process fast recovery file system background semantic knowledge system specific discuss d-graid design implementation widely differing file systems linux ext tweedie microsoft vfat microsoft corporation file systems inclusion vfat represents significant contribution compared previous research operated solely underneath unix file systems ext file system intellectual descendant berkeley fast file system ffs mckusick disk split set block groups akin cylinder groups ffs bitmaps track inode data block allocation inode blocks data blocks information file including size block pointers found file inode vfat file system descends world operating systems article linux vfat implementation fatvfat acm transactions storage vol improving storage system availability d-graid operations centered eponymous file allocation table entry allocatable block file system entries locate blocks file linked-list fashion file block address entry fat find block file entry hold end-of-file marker setting block free unlike unix file systems information file found inode vfat file system spreads information fat directory entries fat track blocks belong file directory entry information size permission type information graceful degradation ensure partial availability data multiple failures raid array d-graid employs main techniques fault-isolated data placement strategy d-graid places semantically related set blocks unit fault containment found storage array simplicity discussion assume file semantically related set blocks single disk unit fault containment generalize easily generalized failure boundaries observed scsi chains refer physical disk file belongs home site file disk fails fault-isolated data placement ensures files disk home site unavailable files remain accessible files technique selective meta-data replication inwhich d-graid replicates naming system meta-data structures file system high degree directory inodes directory data unix file system d-graid ensures live data reachable orphaned due multiple failures entire directory hierarchy remains traversable fraction missing user data proportional number failed disks d-graid lays logical file system blocks availability single file depends disks traditional raid array dependence set entire set disks group leading entire file system unavailability unexpected failure unix-centric typical layout fault-isolated data placement selective meta-data replication depicted figure note techniques d-graid work meaningful subset file system laid single d-graid array file system striped multiple d-graid arrays single array meaningful view file system scenario d-graid run logical volume manager level viewing arrays single disk techniques remain relevant d-graid treats file system block type differently traditional raid taxonomy longer adequate describing d-graid behaves fine-grained notion raid level required d-graid employ redundancy techniques types acm transactions storage vol sivathanu fig comparison layout schemes parts figure depict layouts file foo bar unix file system starting root inode directory tree file data vertical column represents disk simplicity assumes data redundancy user file data top typical file system layout non-d-graid disk system blocks pointers spread file system single fault render blocks file bar inaccessible left figure bottom fault-isolated data placement files directories scenario access inode file access data indirect pointer blocks constrained disk finally bottom selective meta-data replication replicating directory inodes directory blocks d-graid guarantee users files requisite pointers removed rightmost figure simplicity color codes white user data light shaded inodes dark shaded directory data data d-graid commonly employs n-way mirroring naming system meta-data standard redundancy techniques mirroring parity encoding raidfor user data note administrative control determines number failures d-graid degrade gracefully section explore data availability degrades varying levels namespace replication design considerations layout replication techniques required enable graceful degradation introduce number design issues highlight major challenges arise semantically related blocks fault-isolated data placement d-graid places logical unit file system data file faultisolated container disk blocks d-graid considers related acm transactions storage vol improving storage system availability d-graid determines data remains failure basic approach file-based grouping single file including data blocks inode indirect pointers treated logical unit data technique user find files directory unavailable frustration confusion groupings preserve meaningful portions file system volume failure directory-based grouping d-graid ensures files directory unit fault containment automated options allowing users arbitrary semantic groupings d-graid treats unit load balance fault-isolated placement placing blocks file disks blocks isolated single home site isolated placement improves availability introduces problem load balancing space time components terms space total utilized space disk maintained roughly level fraction disks fail roughly fraction data unavailable balancing addressed foreground data allocated background migration files directories larger amount free space single disk handled potentially expensive reorganization reserving large extents free space subset drives files larger single disk split disks pressing performance problems introduced fault-isolated data placement previous work striping data disks performance compared sophisticated file placement algorithms ganger wolf d-graid makes additional copies user data spread drives system process call access-driven diffusion standard d-graid data placement optimized availability access-driven diffusion increases performance files frequently accessed surprisingly access-driven diffusion introduces policy decisions d-graid including place replicas made performance files replicate create replicas meta-data replication level degree meta-data replication d-graid determines resilient excessive failures high degree replication desirable meta-data replication costs terms space time space overheads tradeoffs obvious replicas imply resiliency difference traditional raid d-graid amount space needed replication naming system meta-data dependent usage volume directories induces greater amount overhead time overheads higher degree replication implies lowered write performance naming system meta-data operations observed lack update activity higher levels directory tree popek lazy update propagation employed reduce costs savage wilkes acm transactions storage vol sivathanu fast recovery main design goal d-graid ensure higher availability fast recovery failure critical straightforward optimization d-graid recover live file system data assume restoring data live mirror hot spare straightforward approach d-graid simply scans source disk live blocks examining file system structures determine blocks restore process readily generalized complex redundancy encodings d-graid potentially prioritize recovery number ways restoring important files importance domain specific files orindicated users manner similar hoarding database coda kistler satyanarayanan exploring graceful degradation section simulation trace analysis evaluate potential effectiveness graceful degradation impact semantic grouping techniques quantify space overheads d-graid demonstrate ability d-graid provide continued access proportional fraction meaningful data arbitrary number failures importantly demonstrate d-graid hide failures users replicating important data simulations file system traces collected labs riedel cover days activity data 
spread logical volumes space overheads examine space overheads due selective meta-data replication typical d-graid-style redundancy calculate cost selective meta-data replication percentage overhead measured volumes trace data laid ext vfat file system running underneath ext selective meta-data replication applied superblock inode data block bitmaps inode data blocks directory files blocks replicated case vfat comprise fat directory entries calculate highest percentage selective meta-data replication overhead assuming replication user data user data mirrored overheads cut half table shows selective meta-data replication induces mild space overhead high levels meta-data redundancy linux ext vfat file systems -way redundancy meta-data space overhead incurred worst case vfat -kb blocks increasing block size ext space due internal fragmentation larger directory blocks overheads decrease vfat phenomenon due structure vfat fixed-sized file system block size grows file allocation table shrinks blocks directory data grow acm transactions storage vol improving storage system availability d-graid table space overhead selective meta-data replication table shows space overheads selective meta-data replication percentage total user data level naming system meta-data replication increases leftmost column percentage space overhead meta-data replication shown columns depict costs modest four-way paranoid -way schemes row shows overhead file system ext vfat block size set level replication -way -way -way ext ext vfat vfat fig static data availability percent entire directories shown increasing disk failures simulated system consists disks loaded trace strategies semantic grouping shown file-based directory-based line varies level replication namespace meta-data point shows average deviation trials trial randomly varies disks fail static availability examine d-graid availability degrades failure semantic grouping strategies strategy file-based grouping information single file failure boundary disk directory-based grouping allocates files directory analysis place entire files directories trace simulated -disk system remove simulated disks measure percentage directories assume user data redundancy d-graid level figure shows percent directories directory files accessible subdirectories files figure observe graceful degradation works amount data proportional number working acm transactions storage vol sivathanu fig dynamic data availability figure plots percent processes run unaffected disk failure busy hour trace degree namespace replication set aggressively line varies amount replication popular directories oneway implies directories replicated eight-way -way show modest extreme amount replication means deviations trials shown disks contrast traditional raid disk crashes lead complete data unavailability fact availability degrades slightly expected strict linear fall-off due slight imbalance data placement disks directories modest level namespace replication four-way leads good data availability failure conclude file-based grouping files directory disappear failure leading user dissatisfaction dynamic availability finally simulating dynamic availability examine users applications oblivious d-graid operating degraded mode specifically run portion trace simulator number failed disks record percent processes observed failure run experiment find namespace replication files needed processes replicated experiment set degree namespace replication full replication vary level replication contents popular directories usr bin bin lib figure shows replicating contents directories percentage processes run ill-effect lower expected results figure directories replicated percentage processes run completion disk failure expected reason clear substantial number processes require executable libraries run correctly popular directory replication excellent availability acm transactions storage vol improving storage system availability d-graid failure fortunately popular files read-only directories wide-scale replication raise write performance consistency issues space overhead due popular directory replication minimal sized file system trace directories account total file system size semantic knowledge move construction d-graid prototype underneath block-based scsi-like interface enabling technology underlying d-graid semantic knowledge sivathanu understanding file system utilizes disk enables d-graid implement graceful degradation failure quick recovery exact details acquiring semantic knowledge disk raid system sivathanu assume basic understanding file system layout structures storage system specifically assume d-graid static knowledge file system layout including regions disk block types contents specific block types fields inode describe d-graid builds basic knowledge infer detailed dynamic information file system file system behaviors article extend understanding semantically smart disks presenting techniques handle general file system behaviors previous work required file system mounted synchronously implementing complex functionality disk relax requirement describe set typical file system properties important viewpoint semantically smart disk modern file systems adhere behaviors blocks file system dynamically typed file system locate types blocks physical location disk lifetime file system unix file system block data region user-data block indirect-pointer block directory-data block file system delay updates disk delayed writes file system facilitate batching small writes memory suppressing writes files subsequently deleted consequence delayed writes order file system writes data disk arbitrary file systems order writes carefully ganger remain general make assumptions ordering note properties identified practical reasons linux ext file system exhibits aforementioned behaviors accuracy information assumptions general file system behavior imply storage system accurately classify type block block classification straightforward type block depends location disk acm transactions storage vol sivathanu berkeley fast file system ffs mckusick regions disk store inodes fixed file system creation traffic regions inodes type information spread multiple blocks block filled indirect pointers identified observing inode specifically inode indirect pointer field address indirect block formally identify indirect block semantic disk inode block indirect pointer field relevant inode block written disk disk infers indirect block observes block written information classify treat block indirect block due delayed write reordering behavior file system time disk writes block freed original inode reallocated inode type normal data block disk operations place memory reflected disk inference made semantic disk block type wrong due inherent staleness information tracked implementing correct system potentially inaccurate inferences challenges address article implementation making d-graid discuss prototype implementation d-graid alexander alexander fault-isolated data placement selective metadata replication provide graceful degradation failure employs access-driven diffusion correct performance problems introduced availability-oriented layout alexander replicates namespace system meta-data administrator-controlled stores user data raidor raidmanner refer systems d-graid levels pursuing d-graid level implementation log-structuring rosenblum ousterhout avoid small-write problem exacerbated fault-isolated data placement section present implementation graceful degradation live-block recovery complexity discussion centered graceful degradation simplicity exposition focus construction alexander underneath linux ext file system end section discuss differences implementation underneath vfat graceful degradation present overview basic operation graceful degradation alexander describe simple cases proceeding intricate aspects implementation indirection map similarly scsi-based raid system alexander presents host systems linear logical block address space acm transactions storage vol improving storage system availability d-graid internally alexander place blocks facilitate graceful degradation 
control placement alexander introduces transparent level indirection logical array file system physical placement disks indirection map imap similar structures english stepanov wang wilkes unlike systems imap maps live logical file system block replica list physical locations unmapped blocks considered free candidates d-graid reads handling block read requests d-graid level straightforward logical address block alexander imap find replica list issues read request replicas choice replica read based criteria wilkes alexander randomized selection presence access-driven diffusion diffused copy preference fault-isolated copy writes contrast reads write requests complex handle alexander handles write request depends type block written figure depicts common cases block static meta-data block inode bitmap block unmapped alexander allocates physical block disks replica reside writes copies note alexander easily detect static block types inode bitmap blocks underneath unix file systems simply observing logical block address inode block written d-graid scans block newly added inodes understand inodes d-graid compares newly written block copy process referred block differencing inode d-graid selects home site lay blocks belonging inode records inode-to-home-site hashtable selection home site balance space allocation physical disks d-graid greedy approach selects home site free space write unmapped block data region data block indirect block directory block allocation d-graid file block belongs actual home site case d-graid places block deferred block list write disk learns file block crash inode write make block inaccessible file system in-memory deferred block list reliability concern d-graid newly added block pointers inode indirect block written newly added block pointer refers unmapped block d-graid adds entry imap mapping logical block physical block home site assigned inode newly added pointer refers block deferred list d-graid removes block deferred list issues write physical block writes deferred blocks written acm transactions storage vol sivathanu fig anatomy write parts figure depicts control flow sequence write operations alexander figure inode block written alexander observes contents inode block identifies newly added inode selects home site inode creates physical mappings blocks inode home site inode block aggressively replicated part alexander observes write data block inode mapped write directly physical block part alexander write unmapped data block defers writing block alexander finally observes inode fourth part creates relevant mappings observes blocks deferred issues deferred write relevant home site owner inode blocks inode written subsequent data writes mapped disk directly block type interest d-graid data bitmap block data bitmap block written d-graid scans newly freed data blocks freed block d-graid removes logical-to-physical mapping exists frees physical blocks block deferred list freed block removed deferred list write suppressed data blocks written file system deleted inode written disk generate extra disk traffic similarly optimizations found file systems rosenblum ousterhout removing blocks deferred list important case freed blocks alexander observe owning inode deferred block stays deferred list bounded amount time inode owning block written bitmap acm transactions storage vol improving storage system availability d-graid block indicating deletion block written exact duration depends delayed write interval file system block reuse discuss intricate issues involved implementing graceful degradation issue block reuse existing files deleted truncated files created blocks part file reallocated file d-graid place blocks correct home site reuse blocks detected acted d-graid handles block reuse manner inode block indirect block written d-graid examines valid block pointer physical block mapping matches home site allocated inode d-graid mapping block correct home site write block made context file home site copied physical location location blocks copied added pending copies list background thread copies blocks homesite frees physical locations copy completes dealing imperfection difficulty arises semantically smart disks underneath typical file systems exact knowledge type dynamically typed block impossible obtain discussed section alexander handle incorrect type classification data blocks file data directory indirect blocks d-graid understand contents indirect blocks pointers place file blocks home site due lack perfect knowledge fault-isolated placement file compromised note data loss corruption issue goal dealing imperfection conservatively avoid eventually detect handle cases specifically block construed indirect block written assume valid indirect block live pointer block d-graid action cases case pointer refer unmapped logical block mentioned d-graid creates mapping home site inode indirect block belongs indirect block pointer valid mapping correct mapping indirect block misclassified pointer invalid d-graid detects block free observes data bitmap write point mapping removed block allocated file bitmap written d-graid detects reallocation inode write file creates mapping copies data contents home site discussed case potentially corrupt block pointer point mapped logical block discussed type block reuse results mapping copy block contents home site indirect block pointer valid mapping acm transactions storage vol sivathanu correct block indirect block misclassification alexander wrongly copies data home site note data accessible original file block belongs blocks incorrect home site fortunately situation transient inode file written d-graid detects reallocation creates mapping back original home site restoring correct mapping files accessed properly laid infrequent sweep inodes rare cases improper layout optimizations d-graid eventually move data correct home site preserving graceful degradation reduce number times misclassification occurs alexander makes assumption contents indirect blocks specifically number valid unique pointers null pointers alexander leverage assumption greatly reduce number misclassifications performing integrity check supposed indirect block integrity check reminiscent work conservative garbage collection boehm weiser returns true pointers -byte words block point valid data addresses volume nonnull pointers unique set blocks pass integrity check corrupt data contents happened evade conditions test run data blocks local file system small fraction data blocks pass test blocks pass test reallocated file data block indirect block misclassified access-driven diffusion issue d-graid address performance fault-isolated data placement improves availability cost performance data accesses blocks large file directorybased grouping files directory longer parallelized improve performance alexander performs access-driven diffusion monitoring block accesses determine block ranges hot diffusing blocks replication disks system enhance parallelism access-driven diffusion achieved logical physical levels disk volume logical approach access individual files monitored considered hot diffused per-file replication fails capture sequentiality multiple small files single directory pursue physical approach alexander replicates segments logical address space disks volume file systems good allocating contiguous logical blocks single file files directory replicating logical segments identify exploit common access patterns sensitive data contents semantically smart disks place requirement file system traces include user data 
blocks privacy concerns campaign encounter difficult overcome acm transactions storage vol improving storage system availability d-graid implement access-driven diffusion alexander divides logical address space multiple segments normal operation gathers information utilization segment background thread selects logical segments remain hot number consecutive epochs diffuses copy drives system subsequent reads writes replicas background updates original blocks imap entry block copy date policy deciding segments diffuse simplistic prototype implementation detailed analysis policy space access-driven diffusion left future work amount disk space allocate performance-oriented replicas presents important policy decision initial policy alexander implements reserve minimum amount space system administrator replicas opportunistically free space array additional replication approach similar autoraid mirrored data wilkes autoraid identify data considered dead file system written contrast d-graid semantic knowledge identify blocks free live-block recovery implement live-block recovery d-graid understand blocks live knowledge correct block live considered dead lead data loss alexander tracks information observing bitmap data block traffic bitmap blocks liveness state file system reflected disk due reordering delayed updates uncommon observe write data block bit set data bitmap account d-graid maintains duplicate copy bitmap blocks sees write block sets bit local copy bitmap duplicate copy synchronized file system copy data bitmap block written file system conservative bitmap table reflects superset live blocks file system perform live-block recovery note assume preallocation state bitmap written disk subsequent allocation locking linux modern systems ensures technique guarantees live block classified dead disk block live longer situation arise file system writes deleted blocks disk implement live-block recovery alexander simply conservative bitmap table build list blocks restored alexander proceeds list copies live data hot spare aspects alexander number aspects implementation required successful prototype subsection briefly describe key aspects acm transactions storage vol sivathanu physical block allocation logical array blocks exported scsi property block numbers contiguous logical address space mapped contiguous physical locations disk property empowers file systems place data contiguously disk simply allocating contiguous logical blocks data traditional raid property straightforward preserve physical blocks assigned round-robin fashion disks contiguity guarantees continue hold physical block assign logical block simple arithmetic calculation logical block number d-graid deciding physical block allocate newly written logical block straightforward decision depends file logical block belongs logical offset file fault-isolated placement set contiguous logical blocks belong single file map contiguous physical blocks disk logical block set mapped physical block block set mapped physical block order preserve contiguity expectations larger granularity d-graid balances space utilization files allocation policy large values block map physical block number disks array choice policies requires estimates file size dynamic prototype addresses issue simple technique space reservations alexander utilizes knowledge inodes indirect blocks priori estimates exact size entire file large segment file case indirect block observes inode written file size blocks reserves contiguous blocks home site assigned file actual logical blocks written subsequently reserved space note blocks deferred inodes indirect blocks observed write logical block prior reservation inodes indirect blocks written periodically size information obtained writes stable just-in-time commit space reservations depend size information extracted inode indirect blocks indirect block detection fundamentally inaccurate misclassified indirect block result spurious reservations hold physical space prevent alexander employs lazy allocation actual physical blocks committed logical block written reservation priori reservations viewed soft space reclaimed required interaction deferred writes sync alexander defers disk writes logical blocks observed owning inode arbitrary deferral potentially conflict application-level expectations sync operation issued sync returns application expects acm transactions storage vol improving storage system availability d-graid data disk preserve semantics d-graid handles inode indirect block writes specially d-graid return success write inode indirect block deferred writes blocks pointed inode indirect block reached disk sync operation complete inode block write returns deferred writes guaranteed complete sync returns argument extends fsync return writes pertaining file complete weakness approach application performs equivalent fdatasync flushes data blocks disk metadata technique preserve expected semantics inconsistent fault behavior linux ext interesting issue required change design behavior linux ext partial disk failure process read data block unavailable ext issues read returns failure process block recovery process issues read ext issue read works expected process open file inode unavailable ext marks inode suspicious issue request inode block alexander recovered block avoid change file system retain ability recover failed inodes alexander replicates inode blocks namespace meta-data collocating data blocks file persistence data structures number structures alexander maintains imap reliably committed disk preferably good performance buffered small amount nonvolatile ram note nvram serve cache actively accessed entries data structures space requirements acceptable level current prototype simply stores data structures memory complete implementation require backed persistently popular directory replication important component missing alexander prototype decision popular read-only directories usr bin replicate widely alexander proper mechanisms perform replication policy space remains unexplored initial experience simple approach based monitoring frequency inode access time updates effective alternative approach administrators directories treated manner alexander fat surprised similarities found implementing d-graid underneath ext vfat vfat overloads data blocks user data blocks directories alexander acm transactions storage vol sivathanu defer classification blocks manner similar ext implementation expected implementation basic mechanisms d-graid physical block allocation allocation home sites files tracking replicas critical blocks shared versions d-graid instances vfat implementation d-graid differed interesting ways ext version fact pointers file located file allocation table made number aspects d-graid simpler implement vfat indirect pointers worry copy fat block written version directly compared previous contents block accurate information blocks newly allocated deleted ran occasional odd behavior linux implementation vfat linux write disk blocks allocated freed avoiding obvious common file system optimization behavior vfat estimate set live blocks strict superset blocks live indicative untuned nature linux implementation served indicator semantic disks wary assumptions make file system behavior evaluating alexander present performance evaluation alexander focus primarily linux ext variant include baseline measurements vfat system answer questions alexander work correctly time overheads introduced effective access-driven diffusion fast live-block recovery benefits expect d-graid complex implementation platform alexander prototype constructed software raid driver linux kernel file systems mount pseudodevice normal disk environment excellent understanding issues involved construction real hardware d-graid system limited ways importantly alexander runs system host applications interference due competition cpu memory resources performance characteristics microprocessor memory system found actual raid system 
experiments utilize -mhz pentium iii k-rev min ibm disks acm transactions storage vol improving storage system availability d-graid fig errors placement figure plots number blocks wrongly laid alexander time running busy hour trace experiment run disks total number blocks accessed trace alexander work correctly alexander complex simple raid systems ensure alexander operates correctly put system numerous stress tests moving large amounts data system problems extensively tested corner cases system pushing situations difficult handle making system degrades gracefully recovers expected repeatedly crafted microbenchmarks stress mechanisms detecting block reuse handling imperfect information dynamically typed blocks constructed benchmarks write user data blocks disk worst-case data data appears valid directory entries indirect pointers cases alexander detect blocks indirect blocks move files directories proper fault-isolated locations verify alexander places blocks disk instrumented file system log block allocations addition alexander logs events interest assignment home site inode creation mapping logical block remapping blocks home site receipt logical writes file system evaluate behavior alexander workload ran workload alexander obtained time-ordered log events occurred file system alexander processed log off-line looked number blocks wrongly laid time ran test hours traces found hours examined number blocks misplaced temporarily low fewer blocks report detailed results hour trace observed greatest number misplaced blocks hours examined figure shows results acm transactions storage vol sivathanu fig time overheads figure plots time overheads observed d-graid level versus raid level series microbenchmarks tests run disk systems experiment operations enacted file creations operation -kb file figure parts bottom part shows normal operation alexander capability react block reuse remapping copying blocks correct homesite figure shows alexander quickly detect wrongly blocks remap appropriately number blocks misplaced temporarily total number blocks accessed trace top part figure shows number misplaced blocks experiment assuming remapping occur expected delinquent blocks remain misplaced dip end trace occurs misplaced blocks assigned file home site preceding delete accidentally correcting original misplacement time overheads introduced explore time overheads arise due semantic inference primarily occurs blocks written file system file creation figure shows performance alexander simple microbenchmark allocating writes slower due extra cpu cost involved tracking fault-isolated placement reads overwrites perform comparably raidthe high unlink times d-graid fat fat writes data pertaining deleted files processed d-graid newly allocated data implementation untuned infrastructure suffers cpu memory contention host worst-case estimates overheads cost d-graid explored overhead metadata replication purpose chose postmark katcher acm transactions storage vol improving storage system availability d-graid table performance postmark table compares performance d-graid level raidon postmark benchmark row marked d-graid specific level metadata replication column reports benchmark run-time column shows number disk writes incurred column shows number disk writes metadata blocks fourth column number unique metadata blocks written experiment run disks blocks written run-time total meta-data unique raidd-graid d-graid d-graid d-graid meta-data-intensive file system benchmark slightly modified postmark perform sync deletion phase meta-data writes accounted making pessimistic evaluation costs table shows performance alexander degrees meta-data replication table synchronous replication meta-data blocks significant effect performance meta-data-intensive workloads file sizes postmark range bytes note alexander performed default raidfor lower degrees replication physical block allocation ext contiguous free chunk blocks allocate file layout suboptimal small files pack table shows number disk writes incurred benchmark percentage extra disk writes roughly accounts difference performance replication levels extra writes meta-data blocks counted number unique physical writes meta-data blocks absolute difference replication levels small suggests lazy propagation updates meta-data block replicas idle time freeblock scheduling greatly reduce performance difference cost added complexity lazy update propagation replicas updated d-graid incur extra disk writes played back portion traces min standard raidsystem d-graid disks playback engine issued requests times trace optional speedup factor speedup implies idle time requests reduced factor speedup factors d-graid delivered per-second operation throughput raidutilizing idle time trace hide extra cpu overhead scaling factor operation throughput lagged slightly d-graid showing slowdown one-third trace execution caught due idle time acm transactions storage vol sivathanu fig access-driven diffusion figure presents performance d-graid level standard raidunder sequential workload experiment number files size read sequentially total volume data fixed d-graid performs smaller files due physical block layout effective access-driven diffusion show benefits access-driven diffusion trial experiment performed set sequential file reads files increasing size compared standard raidstriping d-graid access-driven diffusion figure shows results experiment figure access-driven diffusion sequential access larger files ran rate single disk system benefit potential parallelism access-driven diffusion performance improved reads directed diffused copies disks system note case arranged files diffused start experiment reading threshold number times investigating sophisticated policies initiate access-driven diffusion left future work fast live-block recovery explore potential improvement live-block recovery figure presents recovery time d-graid varying amount live file system data figure plots lines worst-case best-case live-block recovery worst case live data spread disk case compacted single portion volume graph live-block recovery successful reducing recovery time disk half full note difference worst-case best-case times difference suggests periodic disk reorganization ruemmler wilkes speed recovery moving live data localized portion acm transactions storage vol improving storage system availability d-graid fig live-block recovery figure shows time recover failed disk hot spare d-graid level mirrored system live-block recovery lines d-graid plotted worst case live data spread entire -mb volume case compacted smallest contiguous space plotted recovery time idealized raid level fig availability profile figure shows operation d-graid level raid failures -gb array consisted data disks hot spare failure data reconstructed hot spare d-graid recovering faster raid failures occur raid loses files d-graid continued serve files workload consisted read-modify-writes -kb files randomly picked froma -gb working set benefits expect d-graid demonstrate improved availability alexander failures figure shows availability performance observed process randomly accessing -kb files running d-graid raidto ensure fair comparison d-graid raidlimited reconstruction rate acm transactions storage vol sivathanu table iii code size alexander implementation number lines code needed implement alexander shown column shows number semicolons column shows total number lines including white spaces comments semicolons total d-graid generic setup fault-isolated placement physical block allocation access driven diffusion mirroring live block recovery internal memory management hashtable avl tree file system specific sds inferencing ext sds inferencing vfat total figure shows reconstruction -gb volume -gb live data completed faster d-graid compared raids extra 
failure occured availability raiddropped d-graid continued availability surprisingly restore raidstill failed files linux retry inode blocks fail remount required raidreturns full availability complex implementation briefly quantify implementation complexity alexander table iii shows number statements required implement components alexander table core file system inferencing module ext requires lines code counted number semicolons core mechanisms d-graid contribute lines code rest spent hash table avl tree wrappers memory management compared tens thousands lines code comprising modern array firmware added complexity d-graid significant academic prototype complexity numbers slight underestimate required production quality implementation analysis intended approximate estimate d-graid levels discussion focused implementing d-graid storage system redundancy user data raidor mirrored storage system raidhowever mentioned layout mechanisms d-graid orthogonal underlying redundancy scheme section formalize levels d-graid popular traditional raid levels present custom policies d-graid level tailored underlying redundancy mechanism acm transactions storage vol improving storage system availability d-graid note contrast traditional raid levels levels d-graid differ type redundancy normal user data system meta-data maintained raidwith configured replication degree d-graidno redundancy simplest d-graid level redundancy mechanism employed normal user data single disk failure results data loss contrast traditional raidwhere single disk failure results complete data loss d-graidensures proportional data availability failure figure shows d-graidconfiguration absence redundancy normal data additional storage required access-driven diffusion d-graidneeds separate performance reserve asdescribed section reserve fixed percentage storage volume size tunable administrator tuning parameter administrator control tradeoff performance storage efficiency issue changing size performance reserve dynamically file systems equipped deal variable volume size limitation addressed simple technique administrator creates file file system reserved diffuse size file implicitly conveys d-graid size performance reserve file system blocks assigned reserved file file d-graid free storage space file system runs short storage administrator prune size special file dynamically reducing size performance reserve d-graidmirroring mirrored d-graid system stripes data multiple mirrored pairs similar raidnote d-graid meaningful storage system comprised single mirrored pair raidbecause system fundamentally partial failure mode access-driven diffusion policy d-graidis similar d-graidwhere dynamic performance reserve hold diffused copies figure depicts configuration note diffused copies mirrored d-graidrequires half percentage space d-graidrequires order achieve level diffusion slight variant d-graidcan make access-driven diffusion effective cost slight degradation reliability disks mirrored pair physical mirrors discussed employ logical mirroring impose logical disk block copies disks relaxed definition d-graid store copy file traditional striped fashion copy file stored fault-isolated fashion figure depicts configuration file fault-isolated copy laid single disk copy striped disks single disk failure result data loss logical mirroring data achieves benefits acm transactions storage vol sivathanu fig d-graid levels figures depict data layout d-graid redundancy schemes style shading represents file d-graidfigure color shading physical raidstripe diffusion segments striped region d-graidlogical separate regions disk simplicity practice interleaved fault-isolated copies acm transactions storage vol improving storage system availability d-graid fault-isolated placement impact performance parallelism striped copies note scenario extra space required access-driven diffusion variant d-graidimproves performance efficient access-driven diffusion reduces reliability compared traditional d-graidin traditional d-graidi physical mirroring single disk failure failure mirror disk lead loss data logical mirroring failure results loss data proportionally irrespective disk incurred failure d-graidparity d-graidis counterpart traditional raidredundancy user data maintained form parity encoding small number disks resulting space efficiency fine grained block-level striping fundamental raidwould conflict fault isolated placement d-graid techniques orthogonal fine-grained striping required raidoccurs physical level actual physical disk blocks fault-isolated placement logical assignment files physical blocks d-graidwould maintain invariant kth parity block xor kth block disk difference kth block disk data pertaining file d-graid raid part file configuration shown figure blocks belonging physical raidstripe shaded color fault-isolated placement raidlike redundancy leads performance issue blocks raidstripe longer part single file logically related full stripe writes uncommon block allocation policies writes partial stripes small writes performance problem requiring disk operations block written patterson address small write problem d-graidwe customized block allocation policy allocation policies section targeted preserving logical contiguity perceived file system d-graidrequires policy minimizes impact small writes policy log-structured allocation rosenblum ousterhout wilkes blocks written place allocated empty segments invalidating locations log structured allocation d-graidwould simply divide disk multiple segments time d-graidwould operate segment stripe comprises kth segment disk write arrives fault isolation module d-graidwould decide disk block laid allocate tail physical block segment logical block typical workload writes spread multiple files graid balances space utilization disks writes acm transactions storage vol sivathanu multiple files spread segments current segment stripe resulting full stripe writes note technique effective log cleaner coordinate cleaning entire set disks set freed segments comprise full segment stripes summary summary find basic layout techniques d-graid orthogonal underlying redundancy mechanism building top physical redundancy scheme d-graid strictly improves availability storage array custom policies access-driven diffusion physical block allocation make d-graid effective redundancy mechanism discussion impact wrong section fair amount complexity identifying logical file block belongs order place correct home site graceful degradation interesting question arises light complexity d-graid makes wrong inference d-graid permanently associates block wrong file places wrong home site incorrect inferences affect parts d-graid design differently graceful degradation component d-graid robust incorrect inferences incorrect association block wrong file affect fault isolation impact correctness d-graid miscalculates large fraction associations reliability resulting storage layout strictly traditional raid level d-graid builds top existing raid redundancy incorrect association lead layout completely fault isolated alayout exhibit fault isolation compared traditional raid face incorrect inference storage system correctness affected making d-graid ideal candidate make aggressive semantic information contrast live block recovery component d-graid depend semantic information correctness requires conservative estimate set live blocks volume d-graid requires estimate strictly conservative live block inferred dead lead loss data section tracking block liveness information conservatively simple straightforward realize d-graid requires accuracy simple piece semantic information implementing fast recovery design complexity d-graid related fault isolation graceful degradation component robust incorrect inference wrong bad acm transactions storage vol improving storage system availability d-graid conclusions robust system continues operate correctly presence class errors robert hagmann hagmann d-graid turns simple binary failure model found storage systems continuum increasing availability storage continuing operation partial failure quickly restoring live data failure occur article shown potential benefits d-graid established limits semantic knowledge shown successful d-graid implementation achieved limits simulation evaluation prototype implementation found d-graid 
built underneath standard block-based interface file system modification delivers graceful degradation live-block recovery access-driven diffusion good performance conclude discussions lessons learned process implementing d-graid limited knowledge disk imply limited functionality main contributions article demonstration limits semantic knowledge proof implementation limitations interesting functionality built inside semantically smart disk system semantic disk system careful assumptions file system behavior hope work guide pursue similar semantically smart disks easier build file systems reorder delay hide operations disks reverse engineering scsi level difficult small modifications file systems substantially lessen difficulty file system inform disk believes file system structures consistent on-disk state challenges disk lessened small alterations ease burden semantic disk development semantically smart disks stress file systems unexpected ways file systems built operate top disks behave d-graid specifically behave part volume address space unavailable heritage inexpensive hardware linux file systems handle unexpected conditions fairly exact model dealing failure inconsistent data blocks missing reappear true inodes semantically smart disks push functionality storage file systems potentially evolve accommodate acknowledgments anurag acharya erik riedel yasushi saito john bent nathan burnett timothy denehy brian forney florentina popovici lakshmi bairavasundaram insightful comments earlier drafts article jack harwood helpful discussions acm transactions storage vol sivathanu richard golding excellent shepherding earlier version article anonymous reviewers thoughtful suggestions greatly improved content article finally computer systems lab providing terrific environment computer science research acharya uysal saltz active disks programming model algorithms evaluation proceedings international conference architectural support programming languages operating systems asplos viii san jose alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca denver anderson chase vahdat interposed request routing scalable network storage acmtrans comput syst feb bitton gray disk shadowing proceedings international conference large data bases vldb los angeles boehm weiser garbage collection uncooperative environment softw pract exper sep burkhard menon disk array storage system reliability proceedings international symposium fault-tolerant computing ftcstoulouse france chapin rosenblum devine lahiri teodosiu gupta hive fault containment shared-memory multiprocessors proceedings acm symposium operating systems principles sosp copper mountain resort chen lee gibson katz patterson raid highperformance reliable secondary storage acm comput surv june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix monterey dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey emc corporation symmetrix enterprise information storage systems emc corporation hopkinton web site http emc english stepanov loge self-organizing disk controller proceedings usenix winter technical conference usenix winter san francisco ganger blurring line oses storage devices tech rep cmu-cs- carnegie mellon pittsburgh ganger mckusick soules patt soft updates solution metadata update problem file systems acmtrans comput syst ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement hicss gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings international conference architectural support programming languages operating systems asplos viii san jose gray computers stop proceedings international conference reliability distributed databases gray horst walker parity striping disc arrays low-cost reliable storage acceptable throughput proceedings international conference large data bases vldb brisbane australia gribble robustness complex systems proceedings eighth workshop hot topics operating systems hotos viii schloss elmau germany acm transactions storage vol improving storage system availability d-graid hagmann reimplementing cedar file system logging group commit proceedings acm symposium operating systems principles sosp austin texas holland gibson siewiorek fast on-line failure recovery redundant disk arrays proceedings international symposium fault-tolerant computing ftcstoulouse france hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international data engineering conference ibm serveraid recovering multiple disk failures web site http ibm qtechinfo migrhtml felten wang singh archipelago island-based file system highly scalable internet services proceedings usenix windows symposium katcher postmark file system benchmark tech rep trnetwork appliance sunnyvale web site http netapp keeton wilkes automating data dependability proceedings acm-sigops european workshop saint-emilion france kistler satyanarayanan disconnected operation coda file system acm trans comput syst feb mckusick joy leffler fabry fast file system unix acmtrans comput syst aug menon mattson comparison sparing alternatives disk arrays isca gold coast australia microsoft corporation web site http microsoft hwdev orji solworth doubly distorted mirrors proceedings acm sigmod international conference management data sigmod washington park balasubramanian providing fault tolerance parallel secondary storage systems tech rep cs-tr- princeton princeton patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod chicago patterson availability maintainability greatermuch performance focus century key note speech fast popek walker chow edwards kline rudisin thiel locus network transparent high reliability distributed system proceedings acm symposium operating systems principles sosp pacific grove reddy banerjee gracefully degradable disk arrays proceedings international symposium fault-tolerant computing ftcsmontreal canada riedel gibson faloutsos active storage large-scale data mining multimedia proceedings international conference large databases vldb york riedel kallahalla swaminathan framework evaluating storage system security proceedings usenix symposium file storage technologies fast monterey rosenblum ousterhout design implementation log-structured file system acmtrans comput syst feb rowstron druschel storage management caching past large-scale persistent peer-to-peer storage utility proceedings acm symposium operating systems principles sosp banff alto canada ruemmler wilkes disk shuffling tech rep hpl- hewlett packard laboratories palo alto saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area file system proceedings symposium operating systems design implementation osdi boston acm transactions storage vol sivathanu savage wilkes afraid frequently redundant array independent disks proceedings usenix annual technical conference usenix san diego sivathanu prabhakaran popovici denehy arpaci-dusseau arpacidusseau semantically-smart disk systems fast san francisco tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans wilkes golding staelin sullivan autoraid hierarchical storage system acmtrans comput syst feb wolf placement optimization problem practical solution disk file assignment problem proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics berkeley received august revised august accepted september acm transactions storage vol 
deconstructing commodity storage clusters haryadi gunawi nitin agrawal andrea arpaci-dusseau remzi arpaci-dusseau jiri schindler computer sciences department emc corporation wisconsin madison hopkinton massachusetts abstract traditional approach characterizing complex systems run standard workloads measure resulting performance end user unique opportunities exist characterizing system constructed standardized components inside system instrumenting components paper show intra-box instrumentation understand behavior large-scale storage cluster emc centera analysis leverage standard tools tracing disk network traffic emanating node cluster correlating traffic running workload infer structure software system write update protocol policies performs caching replication load-balancing imposing variable intra-box delays network disk traffic confirm causal relationships network disk events infer semantics messages nodes examining single line source code introduction systems community long understood benefits separating architecture implementation clean separation clients assured consistent standard interface designers freedom innovate interface common result simple interface hides growing amount internal implementation complexity trend occurred implementation microprocessors instruction sets storage systems modern storage systems simple interfaces hide great deal internal complexity high-end storage servers continue scsi interface simple read write interface exporting array blocks client storage servers implement range complex functionality tens processors gigabytes memory hundreds disks emc symmetrix storage server implements redundant data paths end-to-end checksumming machinery fence portions caches failure disk scrubbing technology discover latent errors proactively standard scsi interface today fundamental change occurring storage systems past storage systems built specialized parts assembled commodity components high-end storage servers built collections commodity pcs running commodity operating system connected ethernet network modern storage systems simply instances cluster workstations domains users understand behavior systems understanding enables critical evaluation design implementation choices users build models system behaves workloads tune debug performance enables administrators identify system behaving correctly drawbacks standard interfaces hide interesting information internal behavior measure evaluate systems application-level benchmarks microbenchmarks traditional approaches assume observe behavior system external interface shift storage system design leverage commodity components greatly increases ability analyze storage system behaves effect building system commodity components opens box users directly observe occurring inside users leverage existing standardized tools perform analysis paper develop set intra-box techniques analyze structure policies commodity-based storage clusters analysis major components monitor perturb network disk traffic internal storage cluster order deduce structure main communication protocols build protocol knowledge dissect internal policy decisions caching prefetching write buffering load balancing apply techniques important instance commodity storage cluster emc centera content-addressable storage system centera designed provide low-cost easy-to-manage scalable storage fixed content data medical images electronic documents archives content addressability centera advantage massive redundancy present data attachments reduce capacity requirements main contribution paper analysis centera protocols policies note results appears proceedings international symposium computer architecture isca achieved assistance emc verify accuracy relevance include metaanalysis results emc find centera chooses simplicity reliability sophisticated performance optimization good choice early implementation analysis reveals structure write update protocol standard two-phase commit writes committed synchronously disks policies centera caching prefetching mechanisms commodity file systems storage nodes leaving complicated caching schemes client applications derive interesting properties load balancing centera storage nodes gather disperse load information locally global effects network link performance account main contribution paper development intra-box techniques applied solely centera techniques widely applicable general lesson draw power probe points system observation internal cluster network disk traffic crucial approach future systems architectural support enable type detailed analysis paper structured present methodology structural analysis centera protocols policy inference analyze results including accuracy confirmation emc engineer discuss related work conclude methodology section begin presenting general overview storage cluster test emc centera describe intra-box techniques analyzing structure policies distributed systems system overview emc centera content addressable storage cas cluster designed storing retrieving fixed content information centera handles management physical storage resources transparently applications designed highly scalable no-singlepoint-of-failure platform centera content addressing applications access data objects blobs binary large objects -bit content address derived contents object hash function applied data architecture centera cluster redundant array independent nodes rain nodes connected private lan node runs centrastar software linux kernel operates storage node access node storage nodes hold objects local disks access nodes manage read write requests external client application server storage nodes general protocol centera application contacts access node delivers data object file centera write blobwrite api centera calculates unique content address object records metadata object separate file called c-clip descriptor file cdf cdf calculated returned application copies cdf blob stored data objects retrieved contacting access node read blobread api providing c-clip evaluate centera cluster access storage nodes utilizing smaller clusters experimental purposes node runs linux centrastar version secondgeneration hardware -ghz pentium memory intel etherexpress pro ethernet ports maxtor ata disks access node connected clients ethernet intra-box analysis traditional approach understanding behavior systems run standard workloads measure resulting performance end user unique opportunities exist characterizing system constructed standardized components intra-box analysis inside system instrumenting constituent components analyze behavior distributed storage server intra-box techniques observation delay traffic nodes disks passive observation network disk traffic track correlations requests general idea protocol structure internal policies observation enable definitively conclude causality requests infer message depends specific previous event delay messages disk requests observation derive general protocol structure policies begin simply observing traffic node disk case traffic observed straightforward tools trace tcp udp communication nodes tcpdump trace disk traffic insert kernel pseudo-device driver disk mount file system driver records start time end time block number read write request collect suitable observations run simple workload client application server workload repeatedly creates objects trace traffic network disk assume workload running centera perform multiple trials filter traffic occur consistently analysis performed offline avoid interference system activity analysis determine general properties communication disk activity system appears proceedings international symposium computer architecture isca nodes send messages nodes nodes read write local disk size network message time delay network disk events infer properties specific centera nodes cluster act access nodes versus primary secondary storage nodes delay passive observation network disk events correlate events observation enable determine exact dependencies events precisely repeatedly observes event occurs event infer depends correlation derive causality successful number potential weaknesses unrelated events consistently occur order simply due performance characteristics system large number iterations required observe event orderings occur consistently filter background noise finally difficult discover event dependent multiple preceding events derive causal 
relationships events delay network disk event observe subsequent events delayed delay receipt message observe sending message delayed infer dependent indirectly delayed infer independent complicating factor determining amount event delayed important issue events dependent multiple events suppose sending message dependent receiving messages arrives delay arrive usual time event triggers incorrectly conclude independent avoid situation inject large delay exceeds durations system tuned system interest delaying dependent events delayed desired delay network traffic employ nistnet modified version sits top tcpdumpand delays incoming packets framework select amount delay define criteria packets delayed centera messages uniquely identified size size protocol type tcp udp determine messages delayed utilize fields protocol headers addresses port numbers delay disk traffic pseudo-device driver tracing requests configure duration delay requests delayed reads writes delayed requests separate queue delay time expires deducing system structure section apply intra-box analysis derive internal structural protocols employed emc centera begin passive observations infer basic protocol storing accessing objects actively delay network disk events determine relationships events storing objects passive observations step passively trace network disk traffic emc centera analyze distinct protocols events occur user stores objects writes events occur user accesses objects reads analysis focused writes complex object write protocol figure pictorial representation findings centera protocol writing objects figure shows network disk events occur client centera nodes request figure reports duration intervals tcp message analysis presented diagram infer basic centera protocol storing objects note description tend nodes perform action action send udp message disk operation completes verified causal relationship events section sequences conjectures object write protocol begins client contacts access node directly sending -byte message access node issues disk read sets tcp connection primary storage node communicates primary storage node performs disk read sets connection secondary storage node communicates time client stores object tcp connection created access node storage nodes structural analysis protocol determine access node storage nodes selected selection policy decision explore section series messages propagates back secondary storage node client client receives response sends variable number messages access node infer messages data object size varies size objects access node forwards data primary storage node forwards data secondary storage node sending transfer acknowledgment storage node writes local disk communicates udp centera storage nodes udpx udpy events storage nodes propagate series -byte -byte acknowledgments access node finally access node informs client request appears proceedings international symposium computer architecture isca commit node access primary commit commit ack setuptcp conn tcp conn abe bce req complete tcp conn transfer ack req ack write req write req req ack transfer ack transfer fin commit tear downtcp conn write req req ack transfer fin commit commit ack tear transfer fin setup client storage secondary storage disk writes sync timeout storage node secondary primary storage node storage node transfer ack data transfer transfer fin udpy udpy update ack write update udpx udpx udpx snx commit figure anatomy centera write protocol figure left pictorial representation findings shows network disk events occur client centera nodes -kb object write blobwrite api tcp messages labeled values message unique label identifying endpoints simple message initial message nodes access node primary storage node average size bytes protocol message shown message assigned label reflecting understanding semantic purpose protocol interval tcp events centera node labeled unique identifier duration intervals reported figure single block disk reads occur intervals figure top shows detail events occur intervals udp messages shown designated label identifying destination node simple message udpx udp message storage node round protocol complete centera nodes tear tcp connections abe bce timing results reported figure reveal time spent storing -kb objects note client sees object stored latency approximately roughly sum times figure shows latency occurs intervals matching intervals figure corresponds access node waiting storage nodes read disk data object transferred access node storage nodes writing object disk object read protocol reads client node communicates access node turn reads disk establishes tcp connection storage nodes storage node receives request reads disk sends data object back access node access node transmits object client exchanges acknowledgments storage node tears tcp connection storage node finally informs client request complete figure shown due space limitation delaying events observing network disk events enables learn internal protocol centera observations conclude events occur delay events infer dependencies network disk events write protocol due space constraints present similar analysis read protocol message protocol determine set events depends delaying preceding events node observing delay subsequent sending message structure discussion investigating type event turn tcp traffic udp traffic completion disk reads writes appears proceedings international symposium computer architecture isca latency a-xx period access node latency b-xx period primary storage node latency c-xx period secondary storage node figure intervals writing objects figures report average duration intervals tcp message events graphs examine access node primary storage node secondary storage node graph examine intervals node x-labels match figure send receive time access node primary storage node secondary storage node abe send receive time packet abe bce packet bcebc packet figure impact delaying tcp packets write protocol delay interesting subset tcp packets pairs graphs involved centera nodes access primary storage secondary storage nodes y-axis report send receive time packet relative start request sharp increase send receive time packet packet delayed packet depends packet stands delay tcp traffic begin investigating straightforward case dependency outgoing tcp packet depends previously-arriving tcp packets note determine outgoing message dependent multiple tcp packets node due nature tcp tcp reliable byte-stream protocol receiver guaranteed packets order sender transmitted packets delayed lost subsequently resent check dependent receiving receiving dependent receiving guaranteed arrive depends waiting sufficient figure shows impact delaying interesting subset tcp packets pairs graphs report send receive time packet centera nodes involved access primary storage secondary storage nodes note sharp increase send receive time packet packet delayed packet depends packet measurements imply dependencies initial request exchange sending primary storage node depends receiving secondary storage node serves acknowledgment primary secondary storage nodes received request sending primary storage node depends receiving acknowledges primary secondary storage nodes received data object sending primary storage node depends receiving secondary serves acknowledgment storage nodes written object disk details finally measurements show interesting set relationships request complete specifically delaying impacts 
delaying impacts delaying impact protocol packet serves acknowledgment primary storage node received packet access node secondary storage node independence confirmed delay technique passive measurements observed udp traffic isolate events dependent arriving udp traffic interesting udp traffic occurs intervals primary secondary storage nodes non-trivial amount additional udp traffic occurs measurements udp traffic filtered occur regular points write protocol found delaying protocol events impact background udp traffic conclude udp traffic related write protocol appears proceedings international symposium computer architecture isca trial time delay send udp recv udp send time sec delay udp sec send udp recv udp send time sec delay udp sec send udp recv udp send figure impact delaying udp packets write protocol graphs show time primary storage node sends receives udp packets udp udp sends tcp packet graph default case extra delay graph delay udp graph delay udp seconds experiment repeated times total data received file size understanding cas snsn- figure content addressability figure plots amount data written storage nodes storing file file size increased file filled single repeated byte candidate space-savings due content-based addressing figure addresses relationship udp traffic events measurements performed primary storage node relationships identical secondary storage node graph independent trials show time udp packets storage nodes udp packets received tcp packet back access node graph reports observed timings default centera system delays udp traffic figure shows ordering sending udp packet storage node receiving udp packet storage node fact udpx udpy parallel passive observations sufficient show dependency messages graph show udp arrives udp udp arrives confirm true dependencies delay udp messages graph delay request udp seconds results show udp delayed udp acknowledgment udp interesting property apparent udp unreliable protocol centera software implements timeout-retry policy resend messages response received graph centera implements timeout udp packets point resends packets explore impact timeout policy depth graph experiments increase delay udp seconds circumstances storage node receives acknowledgment packet udp centera protocol performs timeout-retry intervals stops retrying point storage node sends final tcp packet receiving udp depends receiving udpx udpy waiting time-out approximately seconds disk events determine tcp udp messages dependent completion disk reads disk writes disk read operations occur centera nodes request initiated intervals disk write operations occur storage nodes data object received intervals figure shows delaying disk reads writes storage nodes impact subsequent messages graphs examine access node primary storage node secondary storage node make observations measurements storage nodes tcp message immediately disk read fact dependent disk read send times increase disk read delayed confirms initial intuition related note delaying reads nodes subsequent events delayed indicating reads overlapped storage nodes delaying disk writes delays sending udpx packet send time udpx delayed seconds storage nodes perform disk writes succession conclusion delaying network disk events centera write protocol identify events dependent cases analysis passive observations correlations case occurs end write protocol primary storage node sends commit-ack message access node received an-commit message access node received commit-ack storage node relying correlations determine commit-ack primary independent commit-ack secondary occurred case occurs storage nodes written disk storage nodes send tcp message data committed disk node attempted communicate udp storage nodes udp messages succeeded relying correlations inferred storage nodes receive udp replies sending tcp commit appears proceedings international symposium computer architecture isca abe send receive time packet access node write pri write sec read pri read sec read bce udp send receive time packet primary pri storage node write pri write sec read pri read sec read bcebc udpbc send receive time packet secondary sec storage node write pri write sec read pri read sec read figure impact delaying disk activity write protocol delay reading writing disk graphs involved centera nodes access primary storage secondary storage nodes y-axis graph report send receive time packet relative start request udp sending udpx sharp increase send receive time packet disk event delayed packet depends disk event inferring policies previous section analyzed protocol structure centera write read operations section infer policy decisions protocols analysis focus important functionality expect storage system replication load balancing caching prefetching key approach utilize derived structure write figure read shown protocols fine-tune analysis analysis caching information enable fine-grained accounting disk accesses enabling filter traffic system unrelated current request object write policies begin analyzing decisions occur write protocol originally shown figure continue assume observe delay network disk traffic content addressability begin inferring basic decision large units data centera segments file multiple blobs stored replicated accessed independently blob unit granularity content hashing duplicate detection performed storage allocated storage nodes determine size blob write file byte repeated size file written blob size file internally comprised identical blobs amount traffic halved behavior observed multiples blob size figure shows amount data transferred storage nodes primary secondary network increase size file written amount data transferred climbs steadily x-axis increases point drops cyclical pattern repeats indicating unit content addressability level replication centera data replication protect data unavailability corruption face failures fundamental choice level replication number copies data objects level readily apparent previous structural analysis figure shows replicas made object written experimentation shown range object sizes reveals level replication objects load balancing dissect load balancing strategy writes write enters system centera chooses primary storage node data primary storage node chooses secondary location infer load balancing policy factors determine storage nodes selected factors influence decision nodes place data item including current performance amount space analysis focus performance factors cpu utilization disk usage network connections network delay vary factors time controlled manner cpu run high priority loop varying fraction sleep time network delay varied modified nistnet disk usage generate background traffic file copy program open varying number tcp connections primary secondary nodes observe internal message traffic determine induced load impact centera placement decisions figure plots amount data written node load experiments figure factors influence selection nodes writes heavily skewing writes unloaded nodes cpu load disk load number network connections node interestingly observe increasing network delay incoming link storage node affect load balancing performance writes decreases dramatically shown increase latency incoming link storage node centera incorporate delay load balancing strategy hypothesize centera collecting performance appears proceedings international symposium computer architecture isca net delay tcp conn disk 
load cpu ratio normalized variety pertubation snsn- figure write load balancing results experiments shown run system normal mode time labeled add load resource storage node labeled y-axis plot normalized ratio traffic primary storage nodes storage node configuration load addition expect writes roughly balanced nodes expect imbalance skewing unloaded primary node number writes ----sn- --------- sn----- time delay reaction delay sec delay sec delay sec delay figure impact delaying udp message traffic graph illustrates impact delaying distribution load information y-axis plots difference number writes snand sninitially writes served snthe bold vertical line sec marks cpu load addition snthe arrows point times writes switch loaded node snto unloaded node sndue write load balancing strategy vary udp traffic delay seconds ratio writes normalized unload-load combination storage nodes distribution writes storage nodes unloaded loaded figure write constraints graph shows percentage writes directed pair nodes system configured storage nodes pair storage nodes loaded cpu load induced unloaded load induced experiment varied x-axis pair nodes unloaded loaded pair bars y-axis plots percentage writes directed loaded unloaded pair statistics storage node distributing information system periodically basing load balancing decisions confirm belief run experiment increase load storage node cpu load case delay udp message traffic cluster protocol analysis tcp centera writes reads udp virtually inter-node communication slowing udp messages hope slow spread load information confirm hypothesis figure reveals method load information dispersed figure longer udp message traffic delayed longer takes load balancing decision affected increased cpu load storage node confirm hypothesis load information dispersal finally additional constraints determine primary secondary copies data item isolate constraints experiments identical cpu loads pairs storage nodes figure shows results cases greater cpu load pair nodes greater fraction writes unloaded storage nodes cases number writes adjust cpu load pair nodes nodes unloaded load balancing policy react writes allocated roughly evenly loaded unloaded pairs centera ensures data item copy node node copy node node inspection power distribution centera reveals reason pair nodes separate power supply centera write load balancing sensitive performance factors constrained factors influence reliability power source caching buffering important performance optimization present storage systems write buffering write transforming writes asynchronous operations application-perceived latency greatly reduced copying data in-memory buffer faster committing disk trade-off terms reliability delaying commit disk chance data loss failure increases protocol analysis shown figure revealed access client nodes notified disk write committed storage nodes conclude centera performs write operations synchronously centera developers chose safety reliability performance object read policies turn attention read protocol reads complex performance characteristics crucial applications caching begin determining caching data objects performed centera read protocol demonstrate benefits intra-box techniques begin assuming access internals centera observe performance client begin simple workload repeatedly reads file comparing difference time read subsequent reads environments determine caching present identical latency numbers row table conclude caching taking place case wrong appears proceedings international symposium computer architecture isca client latency delay delay data read an-cli sn-an disk-sn table read caching table left shows time read file centera column shows time read column shows average time subsequent reads row shows experiment large disk delay induced table shows breakdown traffic access node client storage node access node disk storage node leverage ability observe delay events inside centera insert substantial disk delay read disk storage node row table illustrates numbers show large difference time read subsequent reads file caching taking place centera presence caching observable centera mbit ethernet delivers data quickly ide disks inserting delay disks change relative ratios network disk observe caching taking place system experiment reveal system caching occurring complete read caching analysis monitor network disk traffic previously-described experiment results analysis presented table table shows data transferred access node client storage node access node disk storage node subsequent file accesses table shows requests amount data transferred access node client storage node access node client access nodes performing caching table shows data transferred disk storage node subsequent requests storage node performs in-memory caching prefetching prefetching important optimization storage systems experiments determine centera performs prefetching components perform prefetching experiment read data sequentially file small chunks time read client slow disks exacerbate difference on-disk in-memory accesses graph figure shows results experiment graph observe read takes significant amount time requests completed rapidly clientperceived timing result conclude prefetching taking place centera specifically centera time seconds iteration client-perceived prefetching disk delay byte bytes transferred request size read request sequential requests access node prefetching sn-an an-client figure prefetching graph left shows time sequential read file graph shows amount bytes transferred network storage node access node access node client workload rightmost graph run multiple tests varying size request byte prefetches block read caching unearth system prefetching occurs graph figure plots network traffic experiment x-axis vary size read file y-axis plot data transferred request graph shows amount data transferred centera nodes slightly size requested data specifically client requests bytes centera bytes storage node access node bytes access node client results draw conclusions extra bytes passed centera nodes prefetching occuring network prefetching occuring storage node extra information roughly bytes passed header storage node access node node passed client load balancing examine load balancing reads load balancing writes great deal flexibility large-scale system primary copy half nodes secondary node half reads constrained read storage nodes data located experiments seek understand factors centera determine copy data accessed examine performance factors cpu utilization disk usage network connections network delay surprisingly found centera read balancing policy completely insensitive loads induced shown node responds slowly read requests nodes read requests directed appears proceedings international symposium computer architecture isca analysis analyze design implementation centera storage server subsection present perspectives wisconsin emc adding emc perspective offer insight accuracy relevance wisconsin analysis protocol structure wisconsin protocol structure reveals basic elements centera design observe basic two-phase commit protocol writes generation secondary copy handled primary storage node implementation access node send data storage nodes trade-off clear reasonable centera latency potentially higher load access node decreased finally tcp udp purposes centera communication system tcp important aspects data transfer writes reads udp contrast traffic periodic heartbeats load balancing propagating load information observe tcp connection created data transfer large cost current generation system future centera implementations 
caching connections storage nodes avoiding costly three-way tcp handshake teardown emc analysis correctly identifies majority protocol features illuminates centera design principles workload characteristics centera designed on-line archival fixed content mostly-write operations mediumand large-sized objects reliability write-ingest important read performance lower latency extra latency writes introduced storage node relay typically small congestion occurs rarely internal network dual paths switches shielded traffic finally cost setting tearing tcp connection write initially deemed negligible targeted object sizes provided simple scalable solution clusters nodes recent centrastar versions reuse tcp connections tune number open connections based cluster size load udp messages write protocol updates distributed hash table translating content address location constant time message delivered attempts write transaction reports success logs exceptional case retries update time observed occasional disk reads observed intervals directly related write transaction priori knowledge content address write protocol performs lookup distributed hash table found data transferred cluster observed analysis read caching prefetching wisconsin analysis reveals storage nodes perform caching prefetching expect nodes runs commodity file system caching prefetching performed client access nodes decision reasonable benefit client terms latency data fetched access node storage node cases data cross network client host remember user application accesses data running host designers consume precious memory resources client node caching prefetching emc emphasis leveraging commodity components storage node file system disk drive caches emphasis access latencies extra hop internal network warrant impact complexity performance access nodes file server-like environments repeated reads objects centera offers separate gateway sits front cluster translates nfs cifs requests centera api operations implements caching avoids accesses cluster altogether design goal provide light-weight centera api library applications existing caches implement short gateway application-specific caching reduce caching access nodes write caching buffering wisconsin analysis shows centera synchronous system writes write buffering performed centera leans simplicity reliability write completes successfully means reliably committed disks storage nodes synchronous writing slow next-generation centera options improve performance nvram found higher-end emc products emc patch developed linux community centera ensures data reliably written media arguably non-commodity nvram increase complexity handling exceptional states hardware costs emc products include nvram make trade-off favor increased performance read-modify-write workloads replication load balancing wisconsin investigation centera replication reveals uniform approach objects found disks system control applications enabling create copies valuable data centera found perform load balancing storage nodes writes deappears proceedings international symposium computer architecture isca cisions based locally observable storage nodes demonstrated inducing delay incoming network link centera approach load balancing perform load measurable perspective storage node future important gather information load balancing decisions higher levels system measuring write perspective access node measured history make robust placement decisions centera takes power distribution network account system built similar orthogonal raid designs sources failure account placing data replicas disks finally clear presence load balancing machinery writes centera perform load balancing reads node performing poorly read performance system suffers future versions centera correcting oversight emc network delays issue paths node node load balanced paths load observations local node propagated nodes periodic broadcasts observations additional udp traffic piggybacked messages observations extra data transfers section access nodes information selecting storage nodes nature content addresses distribution data storage nodes reads spread equally nodes balancing load aggregate emphasis write operations fact network delays due congestion occur centrastar version analyzed employ load balancing individual read operations added versions similar load balancing writes intra-box analysis observed node balances load internal disks finally current hardware power distribution system eliminates constraints placing replicas nodes content addressability wisconsin observed centera blob size potentially missing opportunities capacity savings achieved smaller blobs smaller blobs imply metadata blob tracking desirable application wishes maximize usage content addressability expecting system find detailed content similarity objects emc implementing single-instance storage object level chunks efficient storage management hundreds millions objects applications advantage single-instance feature combine fast lookup potentially eliminate unnecessary data transfer related work intra-box techniques similar recent line work performance debugging complex systems major difference work related work level detail infer assume knowledge storage systems generally function support caching prefetching domain-specific functions discover specific structural policy details general techniques goals work related work differ specifically work seeks understand structure policies storage system approaches primarily aimed performance debugging specifically aguilera infer causal paths distributed systems message level traces techniques finding component performance bottleneck approach limited assume message dependent arrival previous message complex dependencies found storage systems similarly chen detect failures diagnose performance problems runtime path analysis unlike aguilera analysis chen assume existence message tags system track dependencies advantage approaches intra-box techniques run system interest online running real workload contrast approach applied quiesced system controlled workloads future hope extend approach operational systems previous research characterizing behavior storage systems operated domains work focused single disk worthington identify characteristics disk mapping logical block numbers physical locations size prefetch window prefetching algorithm caching policy previous work characterize traditional raid systems automatically infer number disks chunk size level redundancy layout scheme related work similar approach slowing components learn behavior system brown table locking infer dependence higher-level queries database tables comparison slow network disk traffic understand aspects storage system test communication slowdown mechanism similar presented martin approach learn aspects network performance affects application performance network slowdown infer dependencies components storage cluster appears proceedings international symposium computer architecture isca conclusion paper shown intra-box techniques applied deconstruct protocols policies modern commodity-based storage cluster emc centera analysis infer design implementation system access single line source code general study demonstrates power probe points system observing slowing system components learned structure complex system systems continue grow complexity intra-box techniques much-needed addition toolbox systems analysts techniques developed hope systems built intra-box approach mind externally visible probe points opening box systems readily understood analyzed debugged result generation higher performing robust reliable computer systems centera generation hardware centrastar releases place version observations made longer apply nonetheless work slow-down causality analysis helped emc fine-tune aspects centera protocols acknowledgments lakshmi bairavasundaram todd jones james nugent florentina popovici vijayan prabhakaran muthian sivathanu helpful discussions comments paper ana bizarro assistance setting access centera finally anonymous reviewers helpful suggestions work sponsored nsf ccrccr- ngsitr- itribm network appliance emc aguilera mogul wiener reynolds muthitacharoen performance debugging distributed systems black boxes sosp bolton landing 
amdahl blaauw brooks architecture ibm system ibm journal research development april anderson culler patterson team case networks workstations ieee micro february arpaci culler krishnamurthy steinberg yelick empirical evaluation cray-t compiler perspective isca santa margherita ligure italy bagchi kar hellerstein dependency analysis distributed systems fault injection international workshop distributed systems nancy france october barham isaacs mortier narayanan magpie real-time modeling performance-aware systems hotos lihue hawaii bohossian fan lemahieu riedel bruck computing rain reliable array independent nodes ieee transactions parallel distributed computing brown kar keller active approach characterizing dynamic dependencies problem determination distributed environment ifip ieee internationalsymposium integrated network management cantin hill cache performance selected spec cpu benchmarks computer architecture news september carson santay nist network emulation tool snad ncsl nist gov nistnet january cas-community http cascommunity chen accardi kiciman patterson fox brewer path-based failure evolution management nsdi san francisco march chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june chen patterson approach performance evaluation self-scaling benchmarks predicted performance sigmetrics pages santa clara cypher konstantinidou messina architectural requirements parallel scientific applications explicit communication isca san diego denehy bent popovici arpaci-dusseau arpaci-dusseau deconstructing storage arrays asplos pages boston massachusetts october emc emc centera content addressed storage system http emc gray reuter transaction processing concepts techniques morgan kaufmann hennessy patterson editors computer architecture quantitative approach edition morgan-kaufmann lee thekkath petal distributed virtual disks asplos vii cambridge october martin vahdat culler anderson effects communication latency overhead bandwidth cluster architecture isca denver mcvoy staelin lmbench portable tools performance analysis usenix san diego january panasas panasas active-scale storage cluster http panasas patterson gibson ginting stodolsky zelenka informed prefetching caching sosp pages copper mountain resort december policroniades pratt alternatives detecting redundancy storage systems data usenix boston june rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saavedra smith measuring cache tlb performance effect benchmark runtimes ieee trans-actions computers saito frolund veitch merchant spence fab building reliable enterprise storage systems cheap asplos boston massachusetts october schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november staelin mcvoy mhz anatomy micro-benchmark usenix pages orleans june sterling editor beowulf cluster computing linux mit press october talagala arpaci-dusseau patterson microbenchmark-based extraction local global diskcharacteristics technical report csd- california berkeley woo ohara torrie shingh gupta splashprograms characterization methodological considerations isca santa margherita ligure italy worthington ganger patt wilkes online extraction scsi disk drive parameters sigmetrics pages ottawa canada 
x-ray non-invasive exclusive caching mechanism raids lakshmi bairavasundaram muthian sivathanu andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin-madison abstract raid storage arrays possess gigabytes ram caching disk blocks raid systems lru lru-like policies manage caches array caches recognize presence system buffer caches redundantly retain blocks cached system wasting precious cache space paper introduce x-ray exclusive raid array caching mechanism x-ray achieves high degree perfect exclusivity gray-box methods observing les accessed updates system meta-data x-ray constructs approximate image contents system cache information determine exclusive set blocks cached array microbenchmarks demonstrate x-ray prediction system buffer cache contents highly accurate trace-based simulation show x-ray considerably outperforms lru performs invasive approaches main strength x-ray approach easy deploy performance gains achieved scsi protocol system introduction modern systems comprised multiple levels caching processor level rest storage hierarchy hierarchy performance secondlevel caches important important data sets rst-level caches increase effectiveness second-level caches previous work processor caching introduced concept exclusive caching avoiding duplication data levels memory hierarchy effective amount cache real estate increased potentially improving performance exclusivity studied levels storage hierarchy including distributed systems storage arrays raids problem inclusion importance modern storage systems built two-level hierarchy system rst top level storage array multiple disks beneath rst-level cache managed operating system implements lru-based replacement policy storage array hardware manages memory memory serves second-level cache managed lru fashion worsening problem fact hosts disk arrays caches similar size high-end disk arrays gigabytes memory run similarly con gured hosts cache exclusion cache space storage arrays wasted previous storage research addressed problem ways approach change second-level policy incorporate access characteristics frequency make replacement decisions approach avoids cache inclusion policies carefully tailored work beneath lru cache speci workloads workloads highly-specialized schemes function desired approach change interface systems storage wong wilkes propose scsi command demote moves block cache directly raid cache enabling manage array cache explicitly lru discipline approach readily deployed inducing instruction-set change processor level dif cult leads researchers focus micro-architectural innovations changing systems interface systems storage scsi result storage vendors implement mechanism remaining challenge storage array cache design derive second-level caching scheme lru-based work workloads require change interface systems storage broadly deployable paper introduce x-ray exclusive array cache array caching scheme designed meet goals primary dif culty building exclusive lru-based raid cache changing interface storage system activity observed raid reads cached pages raid fuzzy picture contents cache appropriately adjust contents x-ray sharpens picture gray-box methods observing updates access time eld inode x-ray infer blocks accessed build approximate view contents cache x-ray combines knowledge traditional monitoring data accesses relevant data cache approaching performance globally-managed lru cache study x-ray series simulations x-ray accurately predict contents system appears proceedings international symposium computer architecture isca cache accurate prediction enables highly exclusive cache delivering noticeably higher hit rates simple lru cache policy real workloads x-ray improves hit rates factor compared lru approaching performance perfect exclusive cache requiring system interface storage rest paper structured section overview system operation explains problem cache inclusion storage hierarchy section discusses semantic information obtained disk implications section describes x-ray cache section evaluates caching mechanism section discusses related work section summarizes work outlines future research directions background section outlines operation system viewpoint caching explains problem cache inclusion storage hierarchy proposes solution based utilizing semantic information disk array file system assumptions begin presenting assumptions system information present inode found xed location disk included inode pointers data blocks pointers direct inode indirect block pointed inode levels indirection larger les inode tracks information activity creation time access time system maintains buffer cache variable size caches disk blocks cache managed lrulike caching policies addition cache data blocks operating systems separate inode cache inodes open recently accessed les size system cache varies due pressure virtual memory system pages required address spaces fewer pages caching opened system rst identi inode number pathname traversal checks inode present inode cache present disk block inode read disk application reads system calculates block numbers desired bytes inode indirect blocks buffer cache read disk locate disk block numbers correspond requested blocks buffer cache searched block disk reads issued blocks present cache priority blocks buffer cache updated cache policy lru mostrecently read block highest priority finally access time eld in-memory inode updated inode marked dirty writing similar main differences read application-generated blocks cache marked dirty modi cation time eld inode updated system periodically ush modi dirty blocks disk modi meta-data blocks superblocks bitmap blocks inode blocks generally ushed sooner modied data blocks typical setting meta-data blocks linux ext system seconds data ushed seconds access time information periodically written observed disk problem disk arrays lru-like policies manage cache block array cache miss block potentially replacing recently accessed block policies recognize fact array cache second-level cache services accesses miss system buffer cache block read system system buffer cache subsequent requests block handled cache disk reads block evicted block recently read block array cache stay array cache signi period time wasting cache space array caches greatly affected cache inclusion size comparable size host memory ideal disk block array cache evicted system buffer cache array cache acting victim cache disk array observe disk block requests hit system cache information decide block evicted complicate situation system buffer cache xed size due varying degrees memory pressure solution question addressing storage system learn contents system cache make decisions blocks array cache system interface storage key building system semantic knowledge storage array semantically-smart storage array knowledge system structures embedded sense storage typical array semantically-smart storage system observe inode updated access time eld inode changed array infer read valuable information intelligent caching scheme section explore information obtained disk array semantic awareness inferences made information semantic information semantically-smart storage array knowledge higher level system data structures instance block identify inode data block inode block block identify individual elds inode semantic information embedded appears proceedings international symposium computer architecture isca information inferences requirements disk read request data block block system cache mru block identify data blocks disk read request previously read block block victim system cache past identify data blocks remember previously read blocks access time inode disk read observed blocks present system cache identify inode blocks note 
inode elds cache inode blocks remember disk reads table semantic inferences table presents inferences made disk array piece information capabilities required make inference disk array learned array careful observation system traf note embedding knowledge disk array reasonable on-disk system structures change modern systems great lengths preserve on-disk layout revisions order ensure backward compatibility existing systems semantic knowledge disk array observe elds inode time accessed modi information facilitate prediction ordering blocks system cache size system cache describe speci inferences disk array make based extra semantic information table summarizes inferences speci mechanisms required enable inferences disk array identify data blocks infer data block read system recently mru block system cache assumed system cache lru-based disk array infer block read replaced system disk array make stronger inference disk read observes previously read block block system cache earlier present block victim point time disk reads block make inference disk array remember block numbers previously read blocks finally updates access timestamp eld inode enable inference disk array identify inode blocks compare contents inode block written previous on-disk state identifying inodes change access time semantic knowledge enables disk array associate inode data blocks represented inode information disk array infer changed access time inode implies blocks accessed system blocks read disk access time disk array infer accessed blocks present system cache assuming time loosely synchronized host array order make inference disk array remember past disk accesses assume change access time implies accessed opposed block studies system activity reasonable explore rami cations assumption set inferences aids constructing access information blocks information block victim required system cache information derived late scheme access information block information blocks assumed system cache lru replacement policy systems conform assumption information gleaned disk array maintain list block numbers data blocks read system ordered access times blocks obtained inode writes actual disk reads list reects recency access perceived system attempts mirror ordering system cache block lru end list earliest access time mru end latest access time maintaining ordered list extend inferences disk read previously read block ordered list implies block evicted blocks block lru end list evicted system cache blocks earlier access times block read similarly system level access block generate disk read implies block blocks block mru end ordered list present system cache basic inferences driven semantic information approximating contents system cache section describe details transforming base idea working cache mechanism preserve exclusivity x-ray cache section rst describe build approximate image system cache contents semantic information discuss limitations approach finally describe cache content prediction build exclusive array cache mechanism tracking file system cache track contents system cache maintain ordered list block numbers accessed system call list recency list r-list entry r-list access time block inferred array identi denotes block belongs moving pointer called cache begin pointer appears proceedings international symposium computer architecture isca actions disk read block actions inode write lruend mru disk read block time lru end block belongsinode write file end access time lru end end inclusive region inclusive region inclusive region exclusive region exclusive region exclusive region end mru mru lru end end inclusive regionexclusive region mru block timestamp figure r-list pointer operation contents system cache tracked disk array r-list pointer gures show status r-list pointer disk read inode write block r-list disk block number access time pointer slides list demarcates set blocks presumed resident cache blocks r-list outline r-list pointer managed r-list maintained basic rules block read added moved mru end r-list access time change observed result inode write blocks belonging removed list re-inserted list ecting access time pointer called ideal point array cache placement occur number blocks pointer mru end r-list estimate size system cache set blocks range approximation contents system cache label region pointer mru end r-list inclusive region region pointer lru end list exclusive region pointer maintained read observed block r-list indicating victim block inclusive region implies system cache size overestimated pointer moved position block read shrinking inclusive region victim block present exclusive region pointer moved block mru end action required account possibility reading block result eviction lru block system cache case conservative estimate cache size system level access blocks inferred inode write set blocks r-list belong examined block earliest access time chosen block lies exclusive region implies size system cache underestimated pointer moved position accessed block expanding inclusive region accessed block present inclusive region action required figure illustrates operation system cache tracking mechanism rst disk read block time initially pointer positioned blocks indicating blocks expected present system cache blocks expected victims system cache read block implies blocks system cache victims past block read longer victim recently block moved mru end r-list timestamp updated pointer repositioned blocks ect fact block expected victim illustrates inode write initially pointer positioned blocks indicating block expected system cache inode write block observed access timestamp noted assume timestamp changed assuming disk reads observed infer block present system cache pointer moved past block ect information entry block repositioned r-list timestamp handling partial file access techniques assume les accessed entirety access time update inode infer information blocks accesses typical system workloads involve les require mechanisms robust occasional partial accesses read time block accessed initially victim moved exclusive region r-list nally block accessed receiving access time update due accessed mechanism appears proceedings international symposium computer architecture isca wrongly increase cache size position assumes fullle access handle situation adopt simple heuristic long block belonging inclusive region blocks exclusive region disregarded access time updates received improve robustness techniques occasional skews access pattern updates pointer performed suf cient evidence observed speci cally update performed threshold number accesses suggest update accesses pertaining blocks les limits accuracy mechanisms track contents system cache reasonable level accuracy fundamental limitations interval typically seconds inode writes creates window uncertainty accesses evictions blocks system interval potentially unknown x-ray mismatch actual ordering blocks system maintained x-ray blocks moved exclusive region recently blocks system access information blocks observed time pointer moved exclusive region ect access information pointer update account access blocks account blocks evicted system time interval cache size estimate ated x-ray observes victim read shrinks inclusive region inode timestamps granularity multiple les accessed access time ordering 
relevant blocks unknown x-ray lead error predicted cache size future read evicted block change access time limitation hold systems netbsd ffs maintains timestamps microsecond granularity assume worst -second granularity found linux ext system block accessed evicted system cache inode write interval case x-ray wrongly blocks accessed blocks mru end r-list present system cache error occur block system cache accessed single inode write interval show potential limitations prediction mechanism achieves high level accuracy enabling cache mechanism ideal exclusivity ray cache mechanism section describes x-ray cache built top system cache tracking mechanism size array cache blocks ideally x-ray cache rst blocks past pointer exclusive region r-list unlike simple policies lru place block cache block read system x-ray block ready system issues read block x-ray explicitly fetch block place exclusive cache decide blocks fetch cache x-ray periodically examines r-list blocks added n-block window past pointer r-list blocks removed blocks newly added window replace removed blocks one-by-one x-ray cache fetched recently blocks removed blocks cache replaced rst blocks present system cache x-ray avoid inclusion order x-ray cache organized access time ordered list blocks blocks required placement obtained source unlike mechanisms demote change interface accommodate special cache place command system supply evicted block change interface system disk blocks read disk schedule cache placement reads x-ray requires additional disk bandwidth additional bandwidth forms workload suf cient idle time requests x-ray idle time schedule disk reads placement section explore idle time required purpose workload idle time x-ray perform timely placement internal bandwidth disk array higher external bandwidth limited bandwidth single scsi bus large storage arrays internal buses substantial extra internal bandwidth perform replication migration data storage array scenarios x-ray leverage small amount extra bandwidth schedule placement reads freeblock scheduling shown capable extracting signi amount free bandwidth busy disks negligible impact foreground workload x-ray potentially freeblock scheduling cases internal bandwidth scarce evaluation section trace-based simulation evaluate x-ray rst describe simulation environment section evaluate x-ray system cache tracking mechanism terms accurately predict cache contents finally section evaluate performance improvements x-ray caching mechanism synthetic real workloads simulator built trace-driven simulator system disk system underneath simulator takes trace system requests open read write models system behaviorally similar linux ext system appears proceedings international symposium computer architecture isca simpli system model consists inode data blocks meta-data blocks including superblock bitmap blocks indirect pointer blocks modeled traf blocks minimal system cache lru replacement default investigate cache replacement policies section size cache dynamically changed model virtual memory pressure system block size writes blocks marked dirty buffer cache dirty blocks written periodically dirty inode blocks written disk seconds speci dirty data blocks written seconds inode timestamps granularity disk array simulator models simple disk constant access time model single disk simplicity results hold interesting disk arrays evaluation concentrates hit rates disk-speed insensitive access times levels hierarchy hits system cache hits array cache cost disk accesses cost prediction accuracy section quantify degree accuracy x-ray track size contents system cache explore sensitivity mechanism system workload parameters metrics error cache size prediction predicted system cache size number blocks inclusive region r-list difference predicted cache size actual system cache size measured regular intervals average error computed fraction false positives effective x-ray cache blocks recent victims system cache due prediction inaccuracy x-ray wrongly conclude blocks system cache victims measure ratio number false positives predicted cache size fraction false negatives maintain exclusion x-ray avoid wrongly identifying blocks system cache victims metric ratio number false negatives actual cache size synthetic workloads random zipf similar wong wilkes evaluate exclusive caching experiments subsection workloads warmup phase set les read fully sequentially les selected order set read fully random benchmark selection les uniform random zipf benchmark selects les based zipf distribution selection highly biased small number popular les prediction file system cache size run random benchmark system cache size initially set blocks record cache size predicted x-ray execution benchmark order evaluate reactivity x-ray cache size change cache size blocks time seconds predicted size actual size figure cache size prediction cache size prediction compared actual cache size execution random benchmark shown benchmark les blocks size performs fullle reads warmup phase actual cache size changed blocks blocks read warmup size prediction error file system cache size blocks size prediction error random file size block file size blocks file size blocks file size blocks file size blocks size prediction error file system cache size blocks size prediction error zipf file size block file size blocks file size blocks file size blocks file size blocks figure estimating cache size average percentage error cache size prediction function system cache size plotted sizes random zipf benchmarks working set size constant blocks benchmarks executed sizes total number blocks accessed benchmark constant warmup period measurements actual cache size system multiple times execution benchmark figure compares cache size prediction x-ray actual system cache size warmup phase rst seconds benchmark scans blocks working set x-ray receives information contents system cache random selection phase starts x-ray predict size system cache high degree accuracy x-ray highly responsive system cache size inaccuracies due reasons cited earlier responsible slight overestimate cache size observe obtained similar results zipf benchmark shown explore average percentage error cache size prediction random zipf benchmarks mechanism evaluated sizes system cache sizes figure shows sensitivity size prediction error sizes system cache sizes file size factor mechanism performs access time information obtained granularity appears proceedings international symposium computer architecture isca fraction false positives file system cache size blocks false positives random file size block file size blocks file size blocks file size blocks file size blocks fraction false positives file system cache size blocks false positives zipf file size block file size blocks file size blocks file size blocks file size blocks fraction false negatives file system cache size blocks false negatives random file size block file size blocks file size blocks file size blocks file size blocks fraction false negatives file system cache size blocks false negatives zipf file size block file size blocks file size blocks file size blocks file size blocks figure false positives false negatives fraction false positives fraction false negatives function system cache 
size plotted sizes random zipf benchmarks working set size constant blocks benchmarks executed sizes total number blocks accessed benchmark constant warmup period seconds measurements size prediction error percentage files accessed partially size prediction random zipf fraction false positives percentage files accessed partially false positives random zipf fraction false negatives percentage files accessed partially false negatives random zipf figure partial file access performance system cache tracking evaluated percentage partially accessed les increases working set consists les blocks size system cache size blocks random performs reads zipf performs reads varying percentages les read partially gure shows average size prediction error false positives fraction false negatives graphs observe system cache size increases percentage error decreases larger cache size fewer blocks evicted cache inode writes leading lower misclassi cation errors size prediction error depends extent size error increases size increases considerably signi fraction system cache size prediction file system cache contents cacy x-ray predicting contents cache terms metrics outlined fraction false positives false negatives important fractions low high number false positives imply x-ray ignore signi number recently accessed blocks cached system cache high number false negatives lead x-ray redundantly caching blocks present system cache figure plots fraction false positives fraction false negatives predicted x-ray system cache sizes sizes random zipf workloads graphs show fraction false positives low range parameters fraction false positives decreases increase cache size similar trend observed size prediction error cache tracking mechanism effective identifying recent victims cache quickly respect false negatives trends similar earlier graphs average fraction low x-ray cache high degree exclusivity sensitivity partial file access evaluate robust x-ray prediction mechanism partial access modi random zipf benchmarks access random number blocks percentage les rest les read fully figure shows prediction error x-ray error metrics percentage les accessed partially increased graphs x-ray tolerates partial reads studies shown accesses typical system workloads reads speci cally baker found read-only accesses read bytes sequential transfers maximum reads non-full size content predictions accurate sensitivity inode write interval x-ray obtains access information inode writes performance x-ray sensitive long inode blocks dirty written figure graphs performance x-ray inode write delay increased gures show small write delays seconds prediction error tolerable excessively long write delays size prediction error fraction false negatives increase considerably inode access time update fundamental sources information x-ray surprising reasonable inode write frequency required prediction typical systems linux ext system small write delays inode blocks appears proceedings international symposium computer architecture isca size prediction error inode write interval seconds size prediction zipf random fraction false positives inode write interval seconds false positives zipf random fraction false negatives inode write interval seconds false negatives zipf random figure inode write interval performance system cache tracking evaluated inode write interval increases working set les blocks size random performs fullle reads zipf performs fullle reads gure shows average size prediction error false positives fraction false negatives fraction inode write intervals cache workload size false false size size prediction positives negatives blocks blocks error fraction fraction table scaling behavior quality system cache tracking evaluated system cache size workload size scaled random benchmark executed size set blocks number les increases working set increases scaling behavior table shows scaling x-ray terms error metrics workload size system cache size increased random benchmark observe effect errors decreases benchmark system cache size workload scaled error sources independent system cache size percentage error reduces system cache size scaled cache performance section performance x-ray cache mechanism evaluated synthetic workloads real traces compare hit rates array cache approaches examine resulting response time read average read latency compare performance x-ray alternative approaches compare array cache managed simple lru fashion represents array caches managed today multi-queue cache policy speci cally designed second-level caches utilizes frequency access prioritize blocks compare approach demote cache mechanism achieves exclusivity modifying system explicitly supply victim blocks array cache finally compare ideal case scenario array cache added system cache upper bound array cache achieve perfect exclusivity note case relevant read latency measurements hit rate measurements separate array cache experiments section assume x-ray suf cient extra bandwidth schedule cache placement disk reads constrained scenario extra bandwidth quantify idle time needed x-ray work effectively finally compare performance x-ray approaches system cache policy lru synthetic workloads set experiments random zipf workloads section figure figure show array cache hit rates x-ray lru multi-queue demote array cache sizes random zipf workloads measurements warmup phase benchmarks gures x-ray outperforms lru multi-queue signi cantly random workload x-ray compares indistinguishably demote implying x-ray effective enforcing exclusivity explicit accurate information demote zipf workload hit rate x-ray close demote reason x-ray performs slightly worse case compared random workload impact false positives higher zipf workload important capture recent victims system cache workloads multi-queue policy performs lru due consideration frequency access figure figure compare average read latency workloads x-ray mechanisms compare x-ray ideal scenario array cache space added host system surprisingly demote performs close ideal scenario perfect information enforce exclusivity latency x-ray lru multi-queue policies close ideal scenarios higher hit rates achieved x-ray lead signi improvements read latency benchmarks real workloads evaluate performance x-ray real workloads set experiments http traces web servers evaluate x-ray convert trace requests appears proceedings international symposium computer architecture isca hit rate array cache size blocks random hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks random read latency lru multi-queue x-ray demote extra figure random workload array cache hit rate average read latency x-ray lru multi-queue demote random benchmark presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks benchmark les blocks size reads performed hit rate array cache size blocks zipf hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks zipf read latency lru multi-queue x-ray demote extra figure zipf workload array cache hit rate average read latency x-ray lru multi-queue demote zipf benchmark presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks benchmark les blocks size reads performed web servers system read operations assume objects referred trace static les generated dynamically traces requests image les requests html les 
assume bytes data returned client rst bytes issue requests times speci trace preserving idle time requests traces section -minute section http trace heavily accessed soccer world cup website section http trace nasa web server recorded august figure shows hit rate array cache average read latency worldcup workload gure shows hit rate achieved x-ray lru multi-queue entire range array cache sizes hit rate comparable demote hit rate improvements translate improvements response time x-ray improves read latency times compared lru times compared multi-queue performs similar demote maximum slowdown figure shows hit rate array cache average read latency nasa trace system cache size set blocks x-ray performs demote signi cantly lru multi-queue performance gain x-ray signi array cache small compared system cache approximate information system cache contents x-ray perform invasive methods demote require storage interface sensitivity idle time section explore idle time required x-ray timely fetch exclusive cache blocks placeappears proceedings international symposium computer architecture isca hit rate array cache size blocks worldcup trace hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks worldcup trace read latency lru multi-queue x-ray demote extra figure worldcup trace hit rate read latency array cache hit rate average read latency x-ray lru multiqueue demote worldcup trace presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks hit rate array cache size blocks nasa trace hit rate demote x-ray multi-queue lru read latency milliseconds array cache size blocks nasa trace read latency lru multi-queue x-ray demote extra figure nasa trace hit rate read latency array cache hit rate average read latency x-ray lru multi-queue demote nasa trace presented read latency graph plots line pertaining adding extra space system cache system cache size set blocks ment estimate idle time requirements x-ray implementation issues disk read time idle time assume free internal bandwidth run worldcup benchmark varying degrees idle time purpose scale inter-request times recorded trace broad range scaling factors system cache size array cache size set blocks figure shows hit rate array cache amounts idle time observe graph hit rates multi-queue lru demote independent idle time hit rate x-ray decreases idle time decreases foreground requests greater portion disk bandwidth nasa benchmark signi cantly idle time worldcup benchmark observed hit rate affected factor reductions idle time results shown suf cient idle time present workload x-ray schedule cache placement reads requiring extra internal bandwidth workloads idle time spare internal disk array bandwidth freeblock scheduling schedule reads file system cache policies x-ray designed assumption system cache managed lru fashion systems lru section evaluate performance x-ray system cache managed replacement policies clock clock widely approximation lru cache policy linux variation manage page cache figure presents array cache hit rates worldcup trace study cases ordering demote x-ray multi-queue lru remains slightly larger difference demote x-ray hinting array tuned speci caching algorithm host subject leave future investigation appears proceedings international symposium computer architecture isca hit rate idle time reduction factor x-ray demote multi-queue lru lru multi-queue demote x-ray idle time idle time figure hit rate idle time hit rate array cache worldcup benchmark factor reductions idle time shown x-ray implementation extra internal disk array bandwidth study fractional factor reduction idle time reciprocal factor increase idle time factor reduction retaining original idle time trace hit rate array cache size blocks clock demote x-ray multi-queue lru hit rate array cache size blocks demote x-ray multi-queue lru figure file system cache policy array cache hit rate x-ray lru multi-queue demote worldcup benchmark presented cases system cache managed clock system cache size set blocks related work cache replacement algorithms explored good detail years algorithms proposed single level cache mind include lru lfu fifo mru lee explored spectrum policies subsume lru lfu number earlier works distributed systems demonstrated multi-level cache hierarchy rethinking efforts investigated policies method avoid inclusion focus frequency-based policies recently zhou proposed multiqueue algorithm level caches put minimal lifetime frequency-based priority temporal frequency desirable qualities cache replacement algorithm multiqueue algorithm satis requirements multiple lru queues second-level cache recent block queue minimum frequency threshold selected replacement multi-queue eliminate cache inclusion cache placement occurs disk read block system demote array cache management mechanism considers cache exclusion central managing array cache demote system informs disk array blocks discards block present array cache system supplies block requires changing system-disk interface hinders deployment demote increases interconnect bandwidth requirement moving blocks system cache back array cache problem interconnect-constrained environments finally recent work eviction-based cache placement cache exclusion manage array cache similar work unlike demote mechanism attempts retain current interfaces requires software installed host machine change interface storage specifically eviction-based cache placement virtual memory addresses supplied system modi device driver track contents system mechanism relies idle time extra bandwidth disks read blocks array cache mechanism change interface device driver requires interface disk order communicate needed information storage server mechanism provisions detect system cache size introducing possibility misjudging contents work correctly moves cache pages original location system page migration conclusions technology trends point availability smarter disk array systems future semantic intelligence disk arrays manage large caches present systems semantic information avoids system-disk interface providing information infer contents system cache paper shown create image system cache information inferred disk traf introduced metrics evaluate tracking system cache contents viewpoint information exclusive caching image system cache helps identify set exclusive blocks array cache x-ray array cache based semantic information good cache hit rates improves execution time considerably x-ray achieves ends modi cations system storage interface readily deployed future plan explore number extensions x-ray infer occurrence deletions information remove invalid data cache x-ray possibly detect caching algorithm system assuming lru-like finally explore utility x-ray underneath classes systems windows ntfs underneath database management systems extensions lead robust deployable exclusive caching mechanism storage arrays appears proceedings international symposium computer architecture isca acknowledgments anuradha vaidyanathan involvement input early stages project saisanthosh balakrishnan nathan burnett timothy denehy florentina popovici vijayan prabhakaran insightful comments earlier drafts paper anonymous reviewers comments suggestions helped signi cantly improve paper work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation arlitt williamson web server workload characterization 
search invariants proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics martin arlitt tai jin world cup web site access logs http acm sigcomm ita august martin arlitt tai jin workload characterization world cup web site technical report hpl- hewlett packard labs andrea arpaci-dusseau remzi arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october mary baker john hartman martin kupfer ken shirriff john ousterhout measurements distributed file system proceedings acm symposium operating systems principles sosp pages paci grove california october rohit chandra scott devine ben verghese anoop gupta mendel rosenblum scheduling page migration multiprocessor compute servers proceedings international conference architectural support programming languages operating systems asplos pages san jose california october zhifeng chen yuanyuan zhou kai eviction-based placement storage caches proceedings usenix annual technical conference usenix pages san antonio texas june zarka cvetanovic dileep bhandarkar characterization alpha axp performance spec workloads proceedings international symposium computer architecture pages ian dowse david malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track emc corporation symmetrix enterprise information storage systems http emc theodore johnson dennis shasha low-overhead high performance buffer management replacement algorithm proceedings international conference large databases vldb pages santiago chile september norman jouppi improving direct-mapped cache performance addition small fully-associative cache prefetch buffers proceedings annual international symposium computer architecture isca pages seattle washington norman jouppi steven wilton tradeoffs two-level on-chip caching proceedings international symposium computer architecture pages kimberly keeton david patterson yong qiang roger raphael walter baker performance characterization quad pentium pro smp oltp workloads proceedings annual international symposium computer architecture isca pages june donghee lee jongmoo choi jun-hum kim sam noh sang lyul min yookum cho chong sang kim existence spectrum policies subsumes recently lru frequently lfu policies proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics atlanta georgia lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october makaroff derek eager disk cache performance distributed systems international conference distributed computing systems icdcs pages paris france muntz honeyman multi-level caching distributed file systems cache ain nuthin trash proceedings usenix winter technical conference usenix winter pages san francisco california january drew roselli jacob lorch thomas anderson comparison file system workloads proceedings usenix annual technical conference usenix pages san diego california june muthian sivathanu vijayan prabhakaran florentina popovici timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california march theodore stephen tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track john wilkes richard golding carl staelin tim sullivan autoraid hierarchical storage system acm transactions computer systems february darryl willick derek eager richard bunt disk cache replacement policies network fileservers international conference distributed computing systems icdcs pages pittsburgh pennsylvania theodore wong john wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june yuanyuan zhou james philbin kai multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference usenix pages boston massachusetts june 
antfarm tracking processes virtual machine environment stephen jones andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison stjones dusseau remzi wisc abstract virtualized environment vmm system primary resource manager services implemented layer scheduling kinds security monitoring naturally implemented inside vmm implementing services vmm layer complicated lack vmm paper describes techniques vmm explicit control batch-aware distributed file system john bent douglas thain andrea arpaci-dusseau remzi arpaci-dusseau miron livny computer sciences department wisconsin madison abstract present design implementation evaluation batch-aware distributed file system bad-fs system designed orchestrate large o-intensive batch workloads remote computing clusters distributed wide area bad-fs consists components storage layer exposes control traditionally fixed policies caching consistency replication scheduler exploits control workloads extracting control storage layer placing external scheduler bad-fs manages storage computation coordinated gracefully dealing cache consistency fault-tolerance space management issues workload-specific manner microbenchmarks real workloads demonstrate performance benefits explicit control delivering excellent end-to-end performance wide-area introduction traditional distributed file systems nfs afs built solid foundation empirical measurement studying expected workload patterns researchers developers long make trade-offs system design building systems work workloads interest previous distributed file systems targeted computing environment collection interactively client machines past work demonstrated workloads lead designs filenet google file system assumptions usage patterns sharing characteristics aspects workload change reexamine design decisions embedded distributed file systems area increasing interest batch workloads long popular scientific community batch computing increasingly common broad range important commercially viable application domains including genomics video production simulation document processing data mining electronic design automation financial services graphics rendering batch workloads minimally present system set jobs run ordering environments approximate run times requirements advance scheduler information dispatch jobs maximize throughput batch workloads typically run controlled localarea cluster environments organizations large workload demands increasingly ways share resources wide-area lower costs increase productivity approach accessing resources wide-area simply run localarea batch system multiple clusters spread wide-area distributed file system backplane data access approach fraught difficulty largely due handled primary problem traditional distributed file system approach control decisions caching consistency fault tolerance made implicitly file system decisions reasonable workloads file systems designed ill-suited wide-area batch computing system minimize data movement wide-area system carefully cache space remote clusters caching decisions buried deep distributed file systems preventing control mitigate problems enable utilization remote clusters o-intensive batch workloads introduce batch-aware distributed file system badfs bad-fs differs traditional distributed file systems approach control bad-fs exposes decisions commonly hidden inside distributed file system external workload-savvy scheduler bad-fs leaves consistency caching replication decisions scheduler enabling explicit workload-specific control file system behavior main reason migrate control file system scheduler information scheduler intimate knowledge workload running exploit knowledge improve performance streamline failure handling combination workload information explicit control file system leads distinct benefits traditional approaches appears usenix symposium networked systems design implementation nsdi enhanced performance carefully managing remote cluster disk caches cooperative fashion controlling needed data transported wide-area bad-fs minimizes wide-area traffic improves throughput workload knowledge bad-fs improves performance capacityaware scheduling avoid thrashing improved failure handling detailed workload information scheduler determine make replicas data based cost generating indiscriminately typical file systems data loss treated uniformly performance problem scheduler ability regenerate lost file rerunning application generated replicates cost regeneration high simplified implementation detailed workload information simpler implementation bad-fs cooperative cache implement cache consistency protocol exact knowledge data dependencies scheduler ensures proper access ordering jobs previous work demonstrated difficulties building general cooperative caching scheme demonstrate benefits explicit control prototype implementation bad-fs synthetic workloads demonstrate bad-fs reduce wide-area traffic order magnitude avoid performance faults capacity-aware scheduling proactively replicate data obtain high performance spite remote failure real workloads demonstrate practical benefits system o-intensive batch workloads run remote resources easily high performance finally bad-fs achieves ends maintaining site autonomy support unmodified legacy applications practical constraints important acceptance wide-area batch computing environments rest paper organized section describe assumptions expected environment workload section discuss architecture system section present experimental evaluation section examine related work finally section conclude background section describe setting bad-fs present expected workloads basing assumptions recent work batch workload characterization describe computing environment users workloads difficulty encounter executing workloads conventional tools workloads illustrated figure data-intensive workloads composed multiple independent vertical sequences job job job job job job job job job job job endpoint batch batchendpoint input input pipeline data pipeline data pipeline data input output job width depth figure typical batch-pipelined workload single pipeline represents logical work user wishes complete comprised series jobs users assemble pipelines batch explore variations input parameters input data processes communicate ancestors relatives private data files workload generally consists large number sequences incidentally synchronized beginning logically distinct correctly execute rate siblings refer vertical slice workload pipeline horizontal slice batch entire set batch-pipelined workload note pipeline generically processes connected unix-style pipes communicate files key differences single application batch-pipelined workload file sharing behavior instances pipeline run executable potentially input files characterize sharing occurs batch-pipelined workloads breaking activity types shown figure endpoint unique input final output pipeline-shared shared write-then-read data single pipeline batchshared input data shared multiple pipelines environment wide-area sharing untrusted arbitrary personal computers platform batch workloads platform types throughput-intensive workloads clusters managed machines spread wide area assume cluster machine processing memory local disk space remote users cluster exports resources cpu sharing system obvious bottleneck system wide-area connection managed carefully appears usenix symposium networked systems design implementation nsdi ensure high performance simplicity focus efforts case single cluster accessed remote user section present preliminary results multi-cluster environment refer organized hostile managed collection clusters cluster-to-cluster system contrast popular peer-to-peer systems environment organized effort share computing resources corporations organizations assume environments stable powerful trustworthy technologies designs directly applicable domain make practical important assumption site local autonomy resources autonomy primary implications design bad-fs workload remote resources time resources arbitrarily revoked system built exploit remote resources tolerate unexpected resource failures due physical breakdowns software failures deliberate preemptions autonomy prohibits deployment arbitrary software remote cluster designing bad-fs assume remote cluster ability dispatch well-defined job ordinary unprivileged user mandating single distributed file system viable solution finally assume jobs run systems modified experience scientific workloads product years fine-tuning complete viewed untouchable ease important work user current solutions user wishes run batchpipelined workload environment user developed debugged workload home system ready run batches hundreds thousands computing resources remote batch execution systems condor lsf pbs grid engine pipeline workload expected input data varying parameters small inputs input data begins user home storage server ftp server output data generated eventually committed home server conventional batch computing systems present 
user options running workload option remote simply submit workload remote batch system option input output occur demand back home storage device approach simple throughput data-intensive workload drastically reduced factors wide-area network bandwidth sufficient handle simultaneous batch reads data-intensive pipelines running parallel pipeline output directed back home site including temporary data needed computation completes option pre-staging user manually configure system replicate batch data sets remote environment approach requires user obtain account remote environment identify input data transfer data remote site log remote system unpack data location configure workload recognize correct directories possibly tmp temporary pipeline data submit workload manually deal failures entire process repeated data processed batch systems existing systems longer capacity offer user obvious description configuration process laborintensive error-prone additionally tmp challenging availability guaranteed limitation user made configurations independently scheduling system scheduling system correctly checkpoint pipelines workload users lengths simply run workloads traditional distributed file systems solution typically due administrative desire preserve autonomy domain boundaries systems fixed policies prevent viable batch-pipelined workloads blast commonly genomic search program consisting single stage pipeline searches large shared dataset protein string matches assume user run blast compute cluster nodes equipped conventional distributed file system afs nfs cold caches nodes individually simultaneously access home server large demands resulting poor performance dataset redundantly transferred wide area network caches loaded node run local disk speeds dataset fit cache node thrash generate enormous amount repetitive traffic back home server lacking workload information node employ mechanism protect consistency availability cached data contrast batch-aware system bad-fs global view hardware configuration workflow structure execute workloads efficiently copying dataset single time wide appears usenix symposium networked systems design implementation nsdi queries catalogscheduler home storage ssss cccccccc movementdata remote cluster remote cluster statusupdatesjob data placements complete job notices figure system architecture circles compute servers execute batch jobs squares storage servers hold cached inputs temporary outputs types servers report catalog server records state system scheduler information catalog direct system configuring storage devices submitting batch jobs gray shapes elements design white standard components found batch systems area sharing duplicating data remote cluster explicit knowledge sharing characteristics permits system dispense expense complexity consistency checks allowing nodes continue executing disconnected architecture section present architecture implementation bad-fs recall main goal design bad-fs export sufficient control remote scheduler deliver improved performance fault-handling o-intensive batch workloads run remote clusters figure summarizes architecture bad-fs elements shaded gray bad-fs structured types server processes manage local resources compute server exports ability transfer execute ordinary user program remote cpu storage server exports access disk memory resources remote procedure calls resemble standard file system operations permits remote users allocate space abstraction called volumes interposition agents bind unmodified workloads running compute servers storage servers types servers periodically report catalog server summarizes current state system scheduler periodically examines state catalog considers work assigns jobs compute servers data storage servers scheduler obtain data executables inputs number external storage sites simplicity assume user data stored single home storage server standard ftp server perspective independently overcome part semantic gap separating guest operating systems supports techniques enable vmm track existence activities operating system processes antfarm implementation techniquesthat works detailed knowledge guest internal architecture implementation evaluation antfarm virtualization environments operating systems shows accurately infer process events incurring small runtime overhead worst case demonstrate practical benefits process information vmm implement anticipatory disk scheduler vmm level case study shows significant disk throughput improvements virtualized environment exploiting process information vmm introduction virtual machine technology increasingly deployed range platforms high-end servers desktop pcs large growing list reasons virtualization diverse computing environments including server consolidation support multiple operating systems including legacy systems sandboxing security features fault tolerance optimization specialized architectures software hardwaresupport develops virtualization included dominant commercial operating systems expect virtualized computing environments ubiquitous virtualization prevalent virtual machine monitor vmm naturally supplants operating system primary resource manager machine main target innovationin systemservices oneshouldnowconsiderhow implement services vmm transition functionality vmm potential benefits implementing feature single time vmm becomesavailabletoall vmm place features introduced system operating system legacy closed-source finally vmm locale system total control system resources make informed resource management decisions pushing functionality layer software stack vmm drawbacks significant problem lack higher-levelknowledge vmm referred semantic gap previousworkin partially recognized dilemma researchers developed techniques infer higher-level hardware resource utilization techniques vmm manage resources system reallocating idle page virtual machine virtual machine addition recently proposed vmm-based services explicit information software abstractions operating systems running bridgethe semantic gap previouswork thoroughlyexplored vmm learn operatingsystemsrunning information explicitly implicitly learn operating systems vmm important guest proprietary untrusted managed entity proceedings usenix annual technical conference june boston managing vmm cases explicit information details guest memory layout implementation unavailable unreliable paper developa set techniquesthat enable virtual machine monitor implicitly discover exploit information important operating system abstractions process monitoring lowlevelinteractionsbetweenguestoperatingsystemsandthe memory management structures depend show vmm accurately determine guest operating system creates processes destroys context-switches techniques operate explicit information guest operatingsystem vendor version implementationdetails demonstrate utility efficacy vmm-level process awareness building anticipatory disk scheduler vmm virtual machine environment anticipatory disk scheduler requires information vmm layers implemented exclusively making vmm process aware overcomesthis limitation os-neutral implementation vmm layer modifications detailed knowledge implementation vmm improve throughput competing sequential streams processes virtual machines single guest operating system factor addition scheduling process information vmm applications security domain system monitoringtools malicious software identify code data sensitive processes monitored runtime modification patterns system calls process recognize process compromised additiontodetection trusions process level affecting process scheduling finally process information parent-child relationship processes identify groups related processes user applications feasible vmm process information antfarm implementation process identification techniques virtualization environments xen simics antfarm evaluated applied linux windows sparc linux guest operating systems range environments spans processor families significantly virtualmemory managementinterfacesand operating systems process management semantics antfarm imposes small runtime overhead worst case scenario common process-intensive compilation environment rest paper organized section place antfarm context related work section cover required background material relating implementation architectures virtual machines general section discussion techniques underlying antfarm section covers implementation details antfarm evaluate accuracy overhead imposed antfarm section section present anticipatory scheduling case study conclude section deploying safe user-level network services ictcp haryadi gunawi andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison haryadi dusseau remzi wisc abstract present ictcp information control tcp implementation exposes key pieces internal tcp state tcp variables set safe fashion primary benefit ictcp enables variety tcp extensions implemented user-level ensuring extensions tcp-friendly demonstrate utility ictcp collection case studies show exposing information safe control tcp congestion window readily implement user-level versions tcp vegas tcp nice congestion manager show user-level libraries safely control duplicate acknowledgment threshold make tcp robust packet reordering wireless lans show retransmission timeout adjusted dynamically finally find converting stock tcp implementation ictcp straightforward prototype requires approximately lines kernel code introduction years networking research suggested vast number modifications standard tcp protocol stack proposals eventually adopted suggested modifications tcp stack widely deployed paper address problem deployment proposing small enabling change network stack found modern operating systems specifically introduce ictcp pronounced tcp slightly modified in-kernel tcp stack exports key pieces state information safe control user-level libraries exposing state safe control tcp connections ictcp enables broad range interesting important network services built user-level user-level services built ictcp deployable services implemented tcp stack services packaged libraries easily downloaded interested parties approach inherently flexible developers tailor exact applications finally extensions composable library services build powerful functionality lego-like fashion general ictcp facilitates development services reside key advantage ictcp compared approaches upgrading network protocols simplicity implementing ictcp framework platform simplicity virtue reasons ictcp leverages entire existing tcp stack simple convert traditional tcp implementation ictcp linux-based implementation requires approximately lines code small amount code change reduces chances introducing bugs protocol previous tcp modifications property advantage ictcp safe manner user-level control safety issue time users allowed modify behavior ictcp users allowed control set limited virtual tcp variables cwnd dupthresh rto users download arbitrary code safety concern remaining concern network safety applications implement tcp extensions friendly competing flows building top extant tcp reno stack restricting virtual variables safe range values ictcp ensures extensions aggressive tcp reno friendly addition providing simplicity safeness framework ictcp address additional questions overheads implementing variants tcp ictcp reasonable measurements show services built ictcp scale incur minimal cpu overhead ictcp waiting mechanisms wide range functionality implemented conservative approach demonstrate utility ictcp implementing extensions tcp set case studies focus modifications alter behavior transport appears sixth symposium operating systems design implementation osdi regard congestion tcp vegas tcp nice congestion manager set focus tcp modifications behave differently presence duplicate acknowledgments build reodering-robust extension misinterpret packet reordering packet loss extension efficient fast retransmit efr set explore tcp eifel adjusts retransmit timeout finally services developed easily framework show amount code required build extensions user-level services ictcp similar original native implementations rest paper structured section compare ictcp related work extensible network services section present design ictcp section describe methodology section evaluate important aspects ictcp simplicity implementing ictcp platform network safety ensured user-level extensions computational overheads range tcp extensions supported complexity developing extensions conclude section related work section compare ictcp approaches provide networking extensibility upgrading tcp recent projects proposed frameworks providing limited extensions transport protocols protocols tcp evolve improve ensuring safety tcp friendliness compare ictcp proposals mogul propose applications radically set tcp state terms tcp state ictcp similar proposal greater philosophical difference arises internal tcp state set mogul arbitrary state setting suggest safety provided cryptographic signature previously exported state restricting ability super-user ictcp conservative allowing applications alter parameters restricted fashion trade-off ictcp guarantee network services behaved mogul approach enable broader range services session migration web net projects developing management interface tcp similar information component ictcp web instruments tcp export variety per-connection statistics web propose exporting detailed information ictcp web export timestamps message acknowledgment tcp-tuning daemon net similar control component ictcp daemon related work antfarm informs vmm important operating system abstraction process aboutwhich information research recognized information explicitly vmm implementing vmm features services cases information relates hardware disco determines guest executing idle loop detecting enters lowpower processor mode vmware esx server page sampling determine utilization physical memoryassigned virtualmachines antfarm differs efforts focuses inferring information processes software construct projects recognized oslevel information vmm cases detailed version-specific memory layout information semantic knowledge make information exported directly vmm vmi implement security techniques detecting malicious hidden processes guest introvirt memory layout implementation details enable host-based intrusion detection features vmm antfarm contrast enables limited inexact levelof informationto inferredbya vmm itdoesthis explicit information memory layout implementation affected guests deployed broader set environments work uhlig similar shows infer guest-level information processor managementmore intelligentlyin multiprocessor environment specifically deduce kernel locks held observing executing user versus kernel mode antfarm complementary observes virtual resource mmu infer information operating system processes proceedings usenix annual technical conference june boston finally alternative inferring os-level information knowledge passed explicitly vmm extent paravirtualized architectures explicit information supplied paravirtualized guaranteedto match inside metric paravirtual information considered gold standard information vmm important environments explicitapproachis lessvaluable paravirtualization requires os-level modification implies functionality deployed vmm running beneath legacy closed-source operating systems reasons dependence explicit interfaces forces innovation vmm requiresos-levelinformationtobe coupledwithchangesto supported operating systems inferring guest information vmm innovate independent implementation finally case security applications guest trusted report activities compromised intentionally mislead vmm background techniques describe paper based observations vmm make interactions guest virtual hardware specifically antfarm monitors guest virtual mmu implement virtual address spaces section review pertinent details intel sparc architectures antfarm discuss basic features virtual machine monitors runtime information virtual memory architecture implementation platform intel family microprocessors chose frequently virtualized processor architecture today section reviews features virtual memory architecture important inference techniques architecture two-level in-memory architecturally-defined page table page table organized tree single memory page called page directory root -byte entry page directory point page page table process page table entry pte active address physical page virtual mapping exists page protection status bits pte page writable access page restricted privileged software single address space active processor time system software informs processor mmu address space active writing physical address page directory newaddressspace processorcontrolregister access register privileged vmm virtualize behalf guest operating systems tlb entries loaded on-demand active page tables processor operating system participate handling tlb misses operatingsystem tlb ways single entry removed invlpg instruction non-persistent entries entries page table entries marked global flushed tlb writing address space process tag maintained tlb non-shared entries flushed context switch sparc virtual memory architecture section review key aspects sparc mmu differsfrom chose sparc implementation architecture significantly memory management interface system software architecturally-defined hardware-walked page tables sparc software managed tlb system software implements virtual 
address spaces explicitly managing contents hardware tlb memory made tlb entry translation processor raises exception operating system opportunity supply valid translation deliver error offending process cpu aware operating system page table organization order avoid flushing entire tlb process context switches sparc supplies tag tlb entry called context associates entry specific virtual address space memory current context supplied mmu desired virtual address order match virtual page number context tlb entry identical supplied values entries distinct address spaces exist tlb simultaneously operatingsystem tlb granularity single page granularity entire address space observes tcp statistics responds setting tcp parameters key difference ictcp net propose allowing complete set variables controlled ensure network safety net appears suitable tuning parameters set frequently ictcp frequently adjust in-kernel variables per-message statistics ability block in-kernel events occur stp addresses problem tcp deployment stp enables communicating end hosts remotely upgrade protocol stack stp authors show broad range tcp extensions deployed emphasize major differences stp ictcp stp requires invasive kernel support safe downloading extension-specific code support in-kernel extensibility fraught difficulty contrast ictcp makes minimal kernel stp requires additional machinery ensure tcp friendliness ictcp guarantees friendliness design stp powerful framework tcp extensions ictcp provided easily safely finally information component ictcp derived infotcp proposed part infokernel previous work showed infotcp enables userlevel services indirectly control tcp congestion window cwnd ictcp improves infotcp main ways ictcp exposes information complete set tcp variables ictcp services directly set cwnd inside tcp applications perform extra buffering incur sleep wake events finally ictcp tcp variables cwnd controlled ictcp tcp extensions implemented efficient accurate user-level tcp researchers found move portions conventional network stack userlevel user-level tcp simplify protocol development ictcp userlevel tcp implementation typically struggles performance due extra buffering context switching assurance network safety application-specific networking large body research investigated provide extensibility network services projects network protocols specialized applications ictcp improve performance dramatically approaches tend require radical restructuring networking stack guarantee tcp friendliness protocol languages architectures network languages structured tcp implementations simplify development network protocols ability replace specialize modules appears sixth symposium operating systems design implementation osdi operations called page demap context demap iron file systems vijayan prabhakaran lakshmi bairavasundaram nitin agrawal haryadi gunawi andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison vijayan laksh nitina haryadi dusseau remzi wisc abstract commodity file systems trust disks work fail completely modern disks exhibit complex failure modes suggest fail-partial failure model disks incorporates realistic localized faults latent sector errors block corruption develop apply failure-policy fingerprinting framework investigate commodity file systems react range realistic disk failures classify failure policies taxonomy measures internal robustness iron includes failure detection recovery techniques scheduler show compute commodity storage file servers system failure logically policies independent specialized inconsistent device run buggy type generally server inadequate process ability recover diskless workstation partial runs disk failures compute finally server design implement storage appliance evaluate runs prototype iron storage file server system linux typical ixt showing workstation cluster techniques node in-disk computing checksumming disk replication resources parity greatly runs enhance file bad-fs system robustness run incurring environment minimal time space multiple owners high failure overheads categories subject descriptors operating systems file systems management operating systems reliability general terms design experimentation reliability keywords iron file systems disks storage latent sector errors block corruption fail-partial failure model fault tolerance reliability internal redundancy introduction disks fail commodity file systems expect years file system storage system designers assumed disks operate fail stop manner classic model disks working perfectly fail absolutely easily detectable manner fault model presented modern disk drives complex modern drives exhibit latent sector faults block set blocks inaccessible worse blocks silently corrupted finally disks exhibit transient performance problems permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee sosp october brighton united kingdom copyright acm reasons complex failures disks buggy disk controller issue misdirected write placing correct data disk wrong location interestingly failures exist today simply waiting disk technology improve remove errors errors worsen time due increasing drive complexity immense cost pressures storage industry escalated reliable ata disks desktop pcs large-scale clusters storage systems developers high-end systems realized nature disk faults built mechanisms systems handle redundant storage systems incorporate background disk scrubbing process proactively detect subsequently correct latent sector errors creating copy inaccessible blocks recent storage arrays incorporate extra levels redundancy lessen potential damage undiscovered latent errors similarly highly-reliable systems tandem nonstop utilize end-to-end checksums detect block corruption occurs technology filtered realm commodity file systems including linux file systems ext reiserfs ibm jfs windows file systems ntfs file systems pervasive home environment storing valuable non-archived user data photos home movies tax returns internet services google paper question pose modern commodity file systems react failures common modern disks answer query aggregate knowledge research literature industry field experience form model disk failure label model fail-partial failure model emphasize portions disk fail block errors data corruption model place develop apply automated failure-policy fingerprinting framework inject realistic disk faults beneath file system goal fingerprinting unearth failure policy system detects recovers disk failures approach leverages gray-box knowledge file system data structures meticulously exercise file system access paths disk characterize failure policy develop internal robustness iron taxonomy catalogs broad range detection recovery techniques output fingerprinting tool broad categorization iron techniques file system constituent data structures study focuses important substantially open-source file systems ext reiserfs ibm jfs closed-source file system windows ntfs platforms find great deal illogical inconsistency failure policy due diffusion failure handling code kernel inconsistency leads substantially detection recovery strategies similar fault scenarios resulting unpredictable undesirable fault-handling strategies discover systems implement portions failure policy incorrectly presence bugs implementations demonstrates difficulty complexity correctly handling classes disk failure observe tolerance transient failures file systems assume single temporarilyinaccessible block fatal whole-disk failure finally show file systems recover partial disk failures due lack in-disk redundancy behavior realistic disk failures leads question change file systems handle modern disk failures advocate single guiding principle design file systems don trust disk file system view disk utterly reliable component blocks corrupt file system apply measures detect recover corruption running single disk approach instance end-to-end argument top storage stack file system fundamentally responsible reliable management data metadata initial efforts develop family prototype iron file systems robust variants linux ext file system iron ext ixt investigate costs checksums detect data corruption replication provide redundancy metadata structures parity protection user data show techniques incur modest space time overheads greatly increasing robustness file rate addition usual network system errors bad-fs prepared eviction failures shared resources revoked warning rapid rate change systems creates possibly stale information catalog bad-fs prepared discover servers attempts harness longer bad-fs makes standard components compute servers condor startd processes storage servers modified nest storage appliances interposition agents parrot agents catalog condor matchmaker servers advertise catalog classad resource description language storage servers storage servers responsible exporting raw storage remote sites manner efficient management remote schedulers storage server fixed policy managing space makes policies accessible external users carve space caching buffering tasks fit abstraction called volumes storage servers users allocate space lifetime type specifies policy internally manage space bad-fs storage server exports distinct volume types scratch volumes cache volumes scratch volume self-contained read-write file system typically localize access temporary data scheduler scratch volumes pipeline data passed jobs buffer endpoint output scratch volumes scheduler minimizes home server traffic localizing pipeline writing endpoint data pipeline successfully completes cache volume read-only view home server created home server path caching policy lru mru maximum storage size multiple cache volumes bound cooperative cache volume catalog server storage servers query discover peers number algorithms exist managing cooperative cache intent explore range algorithms describe reasonable algorithm system explain scheduler cooperative cache built distributed hash table keys table block addresses values server primarily responsiappears usenix symposium networked systems design implementation nsdi ble block avoid wide-area traffic primary server fetch block home server servers create secondary copies primary space needed secondary data evicted primary approximate locality initial implementation forms cooperative caches peers subnetwork initial analysis suggests sufficient future plan investigating complicated grouping algorithms failures cooperative cache including partitions easily managed slowdown cooperative cache internally partitioned primary blocks assigned missing peers reassigned long home server accessible partitioned cooperative caches refetch lost data continue noticeable disturbance running jobs approach cooperative caching important differences previous work data dependencies completely scheduler implement cache consistency scheme read data considered current scheduler invalidates volume design decision greatly simplifies implementation previous work demonstrated difficulties building general cooperative caching scheme unlike previous cooperative caching schemes manage cluster memory cooperative cache stores data local disks managing memory caches cooperatively advantageous important optimization make environment avoid data movement wide-area managing remote disk caches simplest effective local global control note volumes export degree control scheduler creating deleting volumes scheduler controls data sets reside remote cluster storage servers retain control per-block decisions important decisions made locally storage servers assignment primary blocks cooperative cache cache victim selection scheduler careful space allocation cache victimize blocks longer needed general found separation global local control suitable workloads work precisely identify balance point clear trade-off extreme complete local system control latent current sector approach errors suffers data corruption policies implementing embedded detection distributed file recovery systems techniques inappropriate iron batch taxonomy workloads system implement extreme complete well-defined global failure control policy subsequently scheduler provide makes vigorous decisions protection block broader data range require disk failures exorbitant complexity contributions scheduler paper incur excessive define network traffic realistic exert failure model fine-grained modern disks control interposition fail-partial agents model order formalize permit ordinary techniques workloads detect make recover storage disk servers errors interposition iron agent taxonomy transforms posix develop operations fingerprinting framework storage server determine calls 
agent mapping logical path names physical storage volumes provided scheduler runtime agent volume abstraction hide large number errors job end user volume longer exists due accidental failure deliberate preemption storage server returns unique volume lost error agent discovering agent forcibly terminates job indicating run correctly environment scheduler clear indication failures transparent recovery actions scheduler bad-fs scheduler directs execution workload compute storage servers combining static workload description dynamic knowledge system state specifically scheduler minimizes traffic wide-area differentiating types treating appropriately carefully managing remote storage avoid thrashing replicating output data proactively data expensive regenerate workflow language shown figure declarative workflow language describes batch-pipelined workload shows scheduler states snd nxt snd una information cwnd ssthresh msglist ack dsack seq rtt acklist timeout cwnd cwnd cnt ssthresh rcv wnd rcv nxt snd una snd nxt dupthresh rto retransmits control tcp clients ictcp vegas ictcp nice ictcp ictcp efr ictcp eifel ictcp ictcp figure ictcp architecture diagram shows ictcp architecture base stack ictcp slightly modified tcp stack exports information limited control top ictcp built number user-level libraries implement pieces functionality suggested literature libraries composed applicable enabling construction powerful services plug-and-play fashion applications sit top stack choose libraries match directly kernel transport generally easier extend existing tcp implementations ictcp design ictcp framework exposes information control key parameters tcp protocol implementation section give high-level overview user-level network services deployed ictcp describe classes information control exported ictcp system architecture figure presents schematic ictcp framework illustrated user-level libraries implementing variants tcp built top ictcp user-level libraries transparently applications 
standard interfaces tcp connections ictcp libraries design ictcp sending side ictcp deployed receivers running ictcp unmodified kernel stack simplify implementation ictcp bsd socket interface exporting information providing control socket options approach minimized implementation work imposes unnecessary run-time overhead obtaining state requires copy kernel user space evaluation shows user-level network services naively poll ictcp frequently state information incur significant increase cpu overhead minimize overhead ictcp polling interrupt-based interface tcp variables updated acknowledgment arrives end round roundtrip time elapsed applications receive interrupt condition case studies ictcp user-level libraries structured threads thread injects packets kernel performs sleep wait set operations information goal ictcp expose information traditionally internal tcp challenge determine information exposed information exposed build interesting extensions information exposed future kernel implementations tcp constrained undesirable expanded interface tcp implementations constrained adhere tcp specification internal variables required ictcp explicitly exports variables part tcp specification sequence number snd nxt oldest unacknowledged sequence number snd una congestion window cwnd slow start threshold ssthresh exposing information tcp implementation straightforward found interesting services converts description execution plan keyword job names job binds description file specifies information needed execute job parent ordering jobs volume keyword names data sources required workload volume ftp server volumes empty scratch volumes volume sizes provided scheduler allocate space appropriately themountkeyword binds volume job namespace jobs access volume mydata jobs share volume path tmp extract command files interest committed home server case pipeline produces file retrieved uniquely renamed readers accustomed working interactive environment language unusual burden point user intending execute batch-pipelined workloads exceptionally organized batch users provide information scattered shell scripts make files batch submission files addition imparting needed information bad-fs scheduler workflow language reduces user burden collecting dispersed information coherent appears usenix symposium networked systems design implementation nsdi job condor job condor job condor job condor parent child parent child volume ftp home data volume scratch volume scratch mount mydata mount mydata mount tmp mount tmp mount tmp mount tmp extract ftp home extract ftp home access information needed failure policy file system analyze popular commodity file systems discover handle disk errors build prototype version iron file system ixt analyze robustness disk failure performance characteristics bring paper close discuss related work finally conclude disk failure reasons file system errors storage system section discuss common disk failure present realistic failpartial model disks discuss aspects model storage subsystem figure presents typical layered storage subsystem file system error occur layers propagate file system generic block device driver device controller firmware media transport host disk generic file system specific file system storage subsystem electrical mechanical cache figure storage stack present schematic entire storage stack top file system beneath layers storage subsystem gray shading implies software firmware white unshaded hardware bottom storage stack disk magnetic storage media mechanical motor arm assembly electrical components busses important component firmware code embedded drive control higher-level functions including caching disk scheduling error handling firmware code substantial complex modern seagate drive roughly lines code connecting drive host transport low-end systems transport medium bus scsi networks common higher-end systems fibrechannel top stack host hardware controller communicates device software device driver controls hardware block-level software forms layer providing generic device interface implementing optimizations request reordering software file system layer split pieces high-level component common file systems specific component maps generic operations data structures file system standard interface vnode vfs positioned disks fail motivate failure model describe errors layers storage stack failures media primary errors occur magnetic media classic problem bit rot occurs magnetism single bit bits flipped type problem detected corrected low-level ecc embedded drive physical damage occur media quintessential head crash culprit drive head contacts 
surface momentarily media scratch occur particle trapped drive head media dangers well-known drive manufacturers modern disks park drive head drive reduce number head crashes scsi disks include filters remove particles media errors lead permanent failure corruption individual disk blocks mechanical wear tear eventually leads failure moving parts drive motor spin irregularly fail completely erratic arm movements head crashes media flaws inaccurate arm movement misposition drive head writes leaving blocks inaccessible corrupted subsequent reads electrical power spike surge damage in-drive circuits lead drive failure electrical problems lead entire disk failure drive firmware interesting errors arise drive controller consists thousands lines real-time concurrent firmware disks return correct data circularly shifted byte memory leaks lead intermittent failures firmware problems lead poor drive performance firmware bugs well-enough field specific names misdirected writes writes place correct data disk wrong location phantom writes writes drive reports completed reach media phantom writes caused buggy misconfigured cache write-back caching enabled summary drive firmware errors lead sticky transient block corruption lead performance problems transport transport connecting drive host problematic study large disk farm reveals systems tested interconnect problems bus timeouts parity errors occurred frequency causing requests succeed slowly fail altogether transport transient errors entire drive bus controller main bus controller problematic eide controller series motherboards incorrectly completion disk request data reached main memory host leading data corruption similar problem controllers return status bits data floppy drive time hard drive observed ide protocol version problems yield corrupt data summary controller problems lead transient block failure data corruption low-level drivers recent research shown device driver code bugs rest operating system bugs crash operating system issue disk requests bad parameters data resulting data corruption fail-partial failure model discussion root failure ready put realistic model disk failure model failures manifest ways entire disk failure entire disk longer accessible permanent classic fail-stop failure block failure blocks accessible referred latent sector errors block corruption data individual blocks altered corruption insidious silent storage subsystem simply returns bad data read term model fail-partial failure model emphasize pieces storage subsystem fail discuss key elements fail-partial model including transience locality frequency failures discuss technology market trends impact disk failures time transience failures model failures sticky permanent transient temporary behavior manifests depends root problem low-level media problem portends failure subsequent requests contrast transport higher-level software issue block failure corruption operation succeed retried locality failures multiple blocks disk fail block failures dependent root block failure suggest forms block failure exhibit spatial locality scratched surface render number contiguous blocks inaccessible failures exhibit locality corruption due misdirected write impact single block frequency failures block failures corruptions occur commercial storage system developer succinctly stated disks break lot guarantees fiction frequently errors occur modeling reliability deciding failures important handle talagala patterson point disk drive manufacturers loathe provide information disk failures people industry refer implicit industry-wide agreement publicize details surprisingly actual frequency drive errors errors disk fail well-known literature previous work latent sector errors errors occur commonly absolute disk failure recent research estimates errors occur times absolute disk failures terms relative frequency block failures occur reads writes due internal error handling common disk drives failed writes sector remapped distant sector allowing drive transparently handle problems remapping imply writes fail failure component media stuttering transport lead unsuccessful write attempt move network-attached storage serves increase frequency class failures remapping succeed free blocks large scratch render blocks unwritable quickly reserved space reads problematic media unreadable drive choice return error trends areas processor performance technology market trends combine improve aspects computer systems contrast technology trends market forces combine make storage system failures occur frequently time reasons reliability greater challenge drives made increasingly dense bits packed smaller spaces drive logic complexity increases low-end drive market cost-per-byte dominates corners cut save pennies ide ata drives low-cost class drives tend tested internal machinery prevent failures occurring result field ata drives observably reliable cost pressures serve increase usage server environments finally amount software increasing storage systems noted software root errors storage system hundreds thousands lines software present lower-level drivers firmware low-level code generally type code difficult write debug source increased errors storage stack iron taxonomy section outline strategies developing iron file system file system detects recovers range modern disk failures main focus develop strategies disks common storage arrays single disk internal robustness iron needed protection file system cope failures modern disks iron file system includes machinery detect level partial faults recover level tables present iron detection recovery taxonomies note taxonomy means complete techniques exist raid variations proposed proceedings usenix annual technical conference june boston virtual machines vmm implements hardware interface software interface includes privileged system portions microprocessor architecture peripherals disk network user interface devices note non-privileged user portion microprocessor instruction set virtualized running unprivileged instructions guest directly executes processor additional overhead key feature virtualized system environment guest operating systems execute unprivileged mode processor vmm runs full privilege guest accesses sensitive system components mmu peripherals processor trap vmm vmm virtualizesensitive system featuresby mediatingaccess feature emulating mmu virtualized attempts guest operating system establish trapped vmm vmm observe attempts similarly request virtual disk device vmm examine vmm choose service request made virtualized interface sees fit requests virtual mappings altered disk requests reordered process identification key process inference techniques logical correspondencebetween abstraction process directly visible vmm virtual address space correspondence due traditional single address space process paradigm shared modern operating systems major process events seek observe creation exit context switch extent address spaces correspond processes events approximated address space creation destruction context switch techniques track processes tracking address spaces approach tracking address spaces sparc identify vmm-visible associate specific address space call address space identifier asid tracking address space creation context switch simply observing piece vmmvisible operating system state asid asid observed infer address space created asid replaced asid conclude address space contextswitch hasoccurred thetechniquewe identify address space deallocation consists detecting asid reuse assume address space asid refers beendeallocatedif asid reuse techniques architecture physical address page directory asid page directory serves root page table tree describes address space address page directory characteristic single address space process creation context switch detect address space creation observe page directories page directory physical address resides vmm notified guest writes privileged register observe asid usedthathas notbeen inferthata addressspace hasbeencreated whenan asidis seenfor time vmm adds asid registry akin operating system process list tracking purposes writes imply addressspace contextswitch monitoring events vmm asid active process exit detect address space deallocation knowledge generic responsibilities operating system maintain address space isolation requirements lead distinctive behavior observed exploited vmm infer address space destroyed operating systems strictly control contents page tables implement virtual address spaces process isolation breached page directory page table page reused distinct processes withoutfirst cleared previousentries ensure invariant holds windows linux systematically clear non-privilegedportionsof page table pages process prior reusing privileged portions page tables implement protected kernel address space cleared shared processes map memory accessible untrusted software ensure stale entries remain tlb address space deallocated architecture provide entries multiple address spaces coexist tlb tlb completely flushed prior reusing address space structures page directory tlb proceedings usenix annual technical conference june boston flushed writing event vmm observe detect user address space deallocation vmm count number user virtual mappingspresent page tables describing address space count drops vmm infer itis simplefora vmmtomaintainsucha counterbecause vmm informed updates process page tables order updates effective requirement vmm role virtualizing mmu multi-threading introduce additional complexity updates process page tables synchronized vmm correctness monitoring tlb flushes processors vmm detect requirementfor address space deallocation met events observed asid vmm address space dead entry asid registry removed subsequent asid implies creation distinct process address space techniques sparc key aspect enable process awareness present sparc vmm-visible identifier virtual address space physical address page directory sparc virtual address space context asid making obvious substitution leads process detection technique sparc similar creation context switch sparc installing contextid privilegedoperationandsoitisalwaysvisibletoavmm byobserving operation vmm maintain registry asids asid observed asid registry vmm infers creation address space context switch detected sparc context installed processor exit requirement reuse context sparc stale entries previously address space removed processor tlbs sparc context demap operation thispurpose vmm observe contextdemap operations libraries ictcp-nice ictcp-rr examine information message ictcp exposes standard information packet message list history recent packets reporting packet sequence number round-trip time time-out fast retransmit ack list history recent acknowledgments recording packet acknowledgment number type normal ack duplicate ack sack dsack exposing per-packet per-ack information trivial tcp implementations exist tcp reno track round-trip time packet add high resolution timer ictcp record information additional complexity recording per-message information requires additional memory ictcp creates lists enabled user-level services control goal ictcp variables internal tcp externally set safe manner challenge determine variables modified values ensuring resulting behavior tcp-friendly philosophy appears sixth symposium operating systems design implementation osdi variable description safe range usage cwnd congestion window limit number packets cwnd cnt linear cwnd increase increase cwnd aggressively ssthresh slow start threshold move rcv wnd receive window size reject packet limit sender rcv nxt expected seq num vrcv wnd reject packet limit sender snd nxt seq num send vsnd una reject ack enter snd una oldest unacked seq num vsnd nxt reject ack enter frfr dupthresh duplicate threshold vcwnd enter frfr rto retransmission timeout exp backoff srtt rttvar enter retransmits number consecutive timeouts threshold postpone killing connection table safe setting tcp variables table lists tcp variables set ictcp range variable safely set ensuring result aggressive baseline tcp implementation give usage intuition control variable notation refers tcp original copy variable refers virtual copy set slow start congestion avoidance frfr fast retransmit fast recovery finally srtt rttvar joba exp backoff jobb mount represent extract smoothed extract round-trip mount time mount round-trip volume time volume variance mount rto mount exponential jobc backoff mount volume job ictcp home storage conservative control data allowed agent job scheduler catalog home storage aggressive execute transmission cleanup basic submit idea compute server variable query extract interest configure ictcp storage adds server limited virtual figure variable workflow terminology scheduler examples tcp simple variable workflow script original directed foo graph jobs introduce limited constructed virtual variable job parent vfoo file system meaning namespace presented clear jobs simply configured original volume mount restrict extract range keyword values files virtual variable committed allowed cover home storage server resulting pipeline tcp completion behavior friendly graphical representation ensure workflow tcp scheduler actions plan job aggressive scheduler original queries tcp catalog implementation current system state acceptable range decides variable place job function data fluctuating tcp scheduler variables creates volumes check storage call time server job user dispatched valid reject invalid settings years ictcp detection accepts recovery settings mechanisms employed entries context flushed sparc asid compute server job executes accessing volumes agent jobs complete scheduler extracts scheduler frees volumes scoping unlike file systems badfs aware flow data workflow language scheduler data originates needed create customized environment job minimize traffic home server refer scoping scoping minimizes traffic ways cooperative cache volumes hold read-only batch data figure volumes reused modification large number jobs scratch volumes figure localize pipeline data job executes accesses volumes explicitly created home server accessed batch data pipeline consistency management workload information expressed workflow language scheduler neatly addresses issue consistency management required dependencies jobs data directly scheduler runs jobs meet constraints implement cache consistency protocol bad-fs storage servers user make mistakes workflow description affect cache consistency correct failure recovery understanding expected workload behavior user scheduler easily detect mistakes warn user results workload compromised implemented detection features architecture readily admits capacity-aware scheduling scheduler responsible throttling running workload avoid performance faults maximize throughput carefully allocating volumes scheduler avoids overflowing storage thrashing caches disk capacity rapidly increasing size data sets growing space management remains important scheduler manages space retrieving list storage catalog server selecting ready job unfulfilled storage pipe batch scheduler allocate job volumes allocates configures volumes schedules job jobs execute space scheduler waits job complete resources arrive failure occur note due lack complete global control scheduler slightly overprovision needed volume size approaches storage capacity scheduling domains selecting smallest job result starvation domain starvation avoided workflow static entity executed scheduler smaller jobs run jobs eventually run failure handling finally scheduler makes bad-fs robust failures handling failures jobs storage servers catalog aspect batch workloads leverage job idempotency job simply rerun order regenerate output scheduler log allocations persistent storage transactional interface compute storage servers scheduler fails allocated volumes running jobs continue operate unappears usenix symposium networked systems design implementation nsdi aided scheduler recovers simply re-reads log discover resources allocated resumes normal operations recording allocations persistently re-discovered released timely manner log irretrievably lost workflow resumed beginning previously acquired leases eventually expire contrast catalog server soft state catalog coerces virtual variable valid range safe range virtual congestion window vcwnd vcwnd cwnd vcwnd rises cwnd cwnd converting variable virtual variable ictcp stack trivial simply replace instances original variable virtual ensure virtual change original variable simplest case statement cwnd cwnd replaced cwnd vcwnd complex cases control flow require careful manual inspection limit extent original variable replaced virtual variable foremost goal ictcp ensure ictcp create aggressive flows conservative virtual variables introduce interesting tcp variables set current implementation ictcp control ten variables convinced safely set analysis linux tcp implementation introduce virtual variables original variable set interfaces sysctl tcp retries user mss approximated ways set rto srtt mdev rttvar mrtt claim ten variables represent complete collection settable file values system define form failure set policy ten variables difficult safe discuss ranges failure policy summarized system table iron briefly discuss taxonomy describe range values failure policy safe file ictcp system variable describe variables cache cwnd replacement cwnd cnt file-layout ssthresh policy levels property detection level safe techniques strictly lower file system case detect sender problem directly transmits occurred data block congestion window accessed smaller corrupted cwnd cwnd simplest cnt detection slow-start strategy entered congestion file system avoidance assumes disk ssthresh works set check variables return determine codes packets acknowledgments accepted approach constraints surprisingly common variables applied complex unintentionally receiver errorcode packet accepted pragmatic detection sequence strategy number falls file inside system receive implement window check return rcv codes nxt provided rcv nxt lower rcv levels wnd storage increasing rcv system nxt sanity decreasing sanity rcv wnd checks file system effect verifies rejecting incoming data packets structures forces consistent sender check reduce sending performed rate single sender block acknowledgment blocks processed checking sequence single number block file system snd una verify snd nxt individual fields increasing snd una pointers decreasing valid snd ranges nxt verify sender type discard acks block reduce file sending system rate superblocks include magic number older file systems pilot include header data block checking block correct type information file system guard forms block corruption checking blocks involve verifying blocks discover bitmap resources corresponds allocated blocks recover involve state periodically scanning crash structures catalog determine unavailable scheduler intact continue consistent operate similar resources fsck journaling discover file systems benefit catalog periodic server full-scan recovers integrity rebuilds checks knowledge buggy compute journaling storage file servers system send periodic unknowingly updates corrupt scheduler on-disk waits structures passive running indications fsck failure background compute detect storage servers recover conducts problems active redundancy probes final verify level detection job taxonomy exits abnormally redundancy error forms indicating redundancy failure detected detect block interposition agent corruption scheduler checksumming suspects storage reliable servers systems housing years detect volumes corruption assigned recently job faulty applied improve scheduler security probes servers checksums volumes healthy number assumes reasons job encountered assist transient communication detecting problems classic bit simply rot reruns bits media volumes failed flipped in-media unreachable ecc catches period time corrects errors assumed checksums lost failure volume well-suited affects jobs detecting corruption higher design levels simplification storage scheduler system considers stack partial volume buggy failure controller failure misdirects disk entire updates volume wrong future location plan investigate write trade-offs block involved disk choice failure checksums granularities running carefully jobs implemented rely detect failed volume problems stopped specifically addition checksum failures level technique cascade comment completed dzero processes detection wrote assumes disk volume works derrorcode rolled check back return codes re-run assumes order lower level avoid lower expensive levels restarts detect errors pipeline dsanity check scheduler data structures checkpoint scratch require volumes extra pipeline consistency stages space complete block determining optimal checkpoint interval problem solution depends likelihood failure checkpoint cost create unlike systems bad-fs solve problem automatically scheduler unique position measure controlling variables scheduler performs simple cost-benefit analysis runtime determine checkpoint worthwhile algorithm works scheduler tracks average time replicate scratch volume cost initially assumed order trigger replication measurement determine benefit replication scheduler tracks number job storage failures computes mean-time-to-failure devices system benefit replicating volume sum run times jobs completed applicable pipeline multiplied probability failure benefit exceeds cost scheduler replicates volume storage server insurance failure original fails scheduler restarts pipeline saved copy due robust case modifying values effect dropping additional packets dredundancy redundancy detect corruption blocks end-to-end table levels iron detection taxonomy level technique 
comment rzero recovery assumes disk works rpropagate propagate error informs user rstop stop activity limit amount crash prevent writes damage rguess return guess wrong block contents failure hidden rretry retry read write handles failures transient rrepair repair data structs lose data rremap remaps block file assumes disk informs locale failures rredundancy block replication enables recovery forms loss corruption table levels iron recovery taxonomy stored data checksums detect misdirected phantom writes higher levels redundancy block mirroring parity error-correction codes detect corruption file system copies block reading comparing determine corrupted techniques designed correction discussed assume presence lower-overhead detection mechanism detection frequency detection techniques discussed applied lazily block access eagerly scanning disk idle time iron file systems form lazy detection additionally eager methods disk scrubbing classic eager technique raid systems scan disk discover latent sector errors disk scrubbing valuable means recovery replica exists repair nowunavailable block detect error occurred scrubbing typically leverages return codes explicitly provided disk discovers block failure corruption combined detection techniques checksums scrubbing discover block corruption levels recovery level iron taxonomy facilitates recovery block failure single disk drive techniques handle latent sector errors block corruptions simplest approach implement recovery strategy notifying clients failure occurred propagate straightforward recovery strategy propagate errors file system file system informs application error occurred assumes client program user respond appropriately problem stop recover disk failure stop current file system activity action levels granularity coarsest level crash entire machine positive feature recovery mechanism turns detected disk failures fail-stop failures preserves file system integrity crashing assumes problem transient faulty block repeatedly-accessed data script run initialization system repeatedly reboot attempt access unavailable data crash intermediate level kill process triggered disk fault subsequently mount file system read-only mode approach advantageous entire system processes continue finest level journaling file system abort current transaction approach lead system complex implement guess recently suggested rinard reaction failed block failure semantics scheduler handle network partitions differently failures partitions formed scheduler compute servers scheduler choose reschedule jobs running side partition situation partition resolved point scheduler find multiple servers executing jobs note introduce errors job writes distinct scratch volumes scheduler choose output extract discard practical issues primary obstacles deploying distributed system friendly administrator deploying operating system file system batch system vast majority software requires privileged user install oversee software requirements make forms distributed computing practical impossibility larger powerful facility difficult ordinary user obtain administrative privileges end bad-fs packaged virtual batch system deployed existing batch system special privileges technique patterned glide-in job frey similar spirit recursive virtual machines run bad-fs 
ordinary user submit jobs existing batch system bad-fs bootstraps systems relying basic ability queue run self-extracting executable program storage compute servers interposition agent deployed servers report catalog server scheduler harness resources note scheduling virtual batch jobs discretion host system jobs interleaved time space jobs submitted users technique deploy badfs existing condor pbs batch systems practical issue security bad-fs grid security infrastructure gsi public key system delegates authority remote processes time-limited proxy certificates bootstrap system submitting user enter password unlock private key home node generate proxy certificate user-settable timeout proxy certificate delegated remote system storage servers authenticate back home storage server requires users trust host system steal secrets reasonable environment appears usenix symposium networked systems design implementation nsdi pipebatch mixed home traffic traffic remote pipe-local caching bad-fs pipebatch mixed runtime runtime figure scoping traffic reduction run times graphs show total amount network traffic generated runtimes number workloads optimizations enabled experiment run synthetic pipelines depth generates total x-axis vary relative amounts batch pipeline batch workload generates batch pipeline common types workloads amount endpoint small leftmost graph shows total amount home server traffic shows total runtimes home server accessed emulated wide-area network set experimental evaluation section present experimental evaluation bad-fs variety workloads present methodology focus scoping capacity-aware scheduling failure handling synthetic workloads understand system behavior present experience running real workloads system controlled environment finally discuss initial experience bad-fs run real workloads multiple clusters wild methodology initial experiments section build environment similar section assume user input data stored home server pipelines run output data safely stored back home server workload considered complete assume workload run read manufacture response allowing system running spite failure negative artificial response desirable failing retry simple response failure retry failed operation retry appropriately handle transient errors wastes time retrying failure permanent repair iron file system detect inconsistency internal data structures repair fsck block pointed marked allocated bitmap freed discussed techniques context journaling file systems bugs lead corruption file system integrity remap iron file systems perform block remapping technique fix errors occur writing block recover failed reads specifically write block fails file system choose simply write block location sophisticated strategies remap entire semantic unit time user file preserving logical contiguity redundancy finally redundancy forms recover block loss simplest form replication block copies locations disk redundancy approach employs parity facilitate error correction similar raid adding parity block block group file system tolerate unavailability corruption block group complex encodings tornado codes subject worthy future exploration redundancy disk negative consequences replicas account spatial locality failure surface scratch corrupts sequence neighboring blocks copies allocated remote parts disk lower performance in-disk redundancy techniques incur high space cost desktop settings drives sufficient free space iron file system natural question file system implement detection recovery disk modern disks internal mechanisms detecting recovering errors sufficient view primary reason detection recovery file system found end-to-end argument lower-levels system implement forms fault tolerance file system implement guard forms failure file system place detect corruption data higher levels storage stack device driver drive controller reason implementing detection recovery file system file system exact knowledge blocks file system apply detection recovery intelligently block types file system provide higher level replication metadata leaving failure detection correction user data applications specific solution explore similarly file system provide machinery enable application-controlled replication important data enabling explicit performance reliability trade-off reason performance file systems storage systems unwritten contract file system lay blocks achieve high bandwidth unwritten contract stipulates adjacent blocks logical disk address space physically proximate disk-level recovery mechanisms remapping break unwritten contract performance problems file system assumes responsibility remap logically-related blocks file avoid problems complexities placing iron functionality file system techniques require persistent data structures track redundant copies parity blocks located mechanisms require control underlying drive mechanisms recover on-disk data modern drives attempt positioning reading strategies interface exists control low-level strategies current systems doesn raid make storage reliable question answered simply raid techniques provide reliable robust storage raid improve storage reliability complete solution reasons systems incorporate disk sine qua redundant storage systems desktop pcs ship single disk included cost driving force marketplace adding disk solely sake redundancy palatable solution raid protect failures higher storage stack shown figure layers exist storage subsystem file system errors occur layers file system ultimately responsible detecting recovering errors ironically complex raid controller consist millions lines code source faults depending raid system employed types disk faults handled lower-end raid controller cards checksums detect data corruption recently companies included machinery cope latent sector errors iron techniques file system single-disk systems multiple drives raid-like manner focus single-disk systems paper rich space left exploration iron file systems redundant storage arrays failure policy fingerprinting describe methodology uncover failure policy file systems main objective failure-policy fingerprinting determine detection recovery techniques file system assumptions makes underlying storage system fail comparing failure policies file systems learn file systems robust disk failures suggest improvements analysis helpful inferring iron techniques implemented effectively approach inject faults beneath file system observe file system reacts fault policy consistent file system simply run workload fail blocks accessed conclude reaction block failure fully demonstrates failure policy system file systems practice complex employ techniques depending operation performed type faulty block extract failure policy system trigger interesting cases challenge coerce file system code paths observe path handles failure requires run remote cluster machines accessible user home server wide-area link emulate scenario limit bandwidth home server simple network delay engine similar dummynet remotely run jobs home server traverse slow link cluster dedicated compute pool condor nodes wisconsin connected mbit ethernet switch node pentiumprocessors physical memory ibm scsi drive partition made condor jobs partitions typically half time rest awaits lazy garbage collection explore performance bad-fs range workload scenarios utilize parameterized synthetic batch-pipelined workload synthetic workload configured perform varying amounts endpoint batch pipeline compute lengths time exhibit amounts batch pipeline parallelism experiment requires parameters leave descriptions individual figure captions previous results workload analysis focus batch-intensive workloads exhibit high degree batch sharing pipeline endpoint pipeintensive perform large amounts pipeline generate batch endpoint scoping results experiment shown figure demonstrate bad-fs scoping minimize traffic wide area localizing pipeline scratch volumes reusing batch data cooperative cache volumes optimizations straightforward ability increase throughput significant experiment repeatedly run synthetic workload vary relative amount batch pipeline compare number system configurations remote configuration home node baseline compare pipeline localization caching optimizations finally optimizations combined bad-fs configuration note experiments assume copious cache space controlled environment capacity-aware scheduling failure recovery needed left-hand graph shows total transferred wide-area network surprisingly cooperative cache greatly reduces batch traffic home node ensuring batch data set retrieved cache pipeline localization optimizations work expected removing pipeline home server finally optimization isolation sufficient bad-fs configuration combines minimize network traffic entire workload range right-hand graph figure shows runtimes workloads emulated remote cluster graph direct impact wide-area traffic runtime capacity-aware scheduling examine benefits explicit storage management previous experiments run environment storage capacity increasing size batch data sets storage sharing jobs users scheduler carefully manage remote space avoid wide-area thrashing experiments compare capacity-aware bad-fs scheduler simple variants depth-first scheduler breadth-first scheduler algorithms aware data workload appears usenix symposium networked systems design implementation nsdi runtime hours batch intensive dfs bfs bad-fs normalized home traffic batch data total coop cache dfs bfs bad-fs figure batch-intensive explicit storage management graphs show benefits explicit storage management batchintensive workload workload consists -stage pipelines stage process streams shared batch file batch files total batch file size varied percentage total amount cooperative cache space nodes experiment amounts negligible nodes local storage portion cache total cache size set x-axis reflects observations storage condor pool base decisions solely job structure workflow depth-first simply assigns single pipeline cpu runs jobs pipeline completion starting conversely breadth-first attempts execute jobs batch completion descending horizontal batch slice correct types workloads lead poor storage allocations depth-first scheduling batch-intensive workload thrashing attempts simultaneously cache batch datasets similarly breadth-first scheduling pipe-intensive workload over-allocate storage creates allocations pipelines completing batch-intensive capacity-aware scheduling figure illustrates importance capacity-aware scheduling measurements batch-intensive workloads scheduled algorithms workload depth large batch data sets takes sizable fraction cooperative cache remote cluster varied x-axis upper graph shows runtime lower presents amount wide-area traffic generated normalized size batch data make number observations graphs similarity graphs validates wide-area network link bottleneck resource expected policies achieve similar results long entirety batch data sets fits caches size runtime hours pipe intensive bfs dfs bad-fs failures hundreds pipe size shared disk bfs dfs bad-fs figure pipe-intensive explicit storage management graphs depict benefits explicit storage management pipeintensive workload workload consists -stage pipelines pipe data size varied percent total storage amounts negligible compute servers storage server experiment representing set diskless clients single server storage space server constrained batch data approaches total capacity cooperative cache runtime wide-area traffic increase depth-first scheduling total batch data longer fits cache depth-first scheduling refetch batch data pipeline case results extra fetches pipelines compute servers server executes pipelines note runtime begins increase slightly reason inefficiency lack complete global control allowed current volume interface case local cooperative cache hash function perfectly distributing data peers cache nears full utilization skew overloads nodes results extra traffic home server trade-off local global control correct implication scheduler aware utilization cooperative cache utilization peer finally breadth-first bad-fs scheduling retain linear performance regime ensure total amount batch data accessed time exceed capacity cooperative cache individual batch dataset exceeds capacity cooperative cache performance breadth-first bad-fs scheduling converges depth-first note inefficiency caused depth-first deviate slightly happen slightly pipe-intensive capacity-aware scheduling set cache management experiments focus pipeline-intensive workload batchappears usenix symposium networked systems design implementation nsdi mtbf secsno failure largesmalllargesmall throughput jobs minute always-copy never-copy bad-fs figure failure handling graph shows behavior cost-benefit strategy failure scenarios shown workloads width depth minute cpu time performs small amount pipeline large amount run periods high low rates failure failures induced artificial failure 
generator formatted disks random time failures seconds total runtime single pipe intensive case expect capacity-aware approach follow depth-first strategy closely results presented figure lower graph plot number failed jobs strategy induces job failure arise workload shortage space pipeline output scenario job runs space pipeline data aborts rerun time number job failures due lack space good indicator scheduler success scheduling pipeline-intensive jobs space constraints graph observe breadth-first scheduling unable prevent thrashing contrast capacity-aware bad-fs scheduler exceed space pipelines observes aborted job careful allocation results drastically reduced runtime shown upper graph stair-step pattern runtime bad-fs results careful allocation size data pipeline total storage bad-fs schedules workload jobs cpus data exceeds bad-fs allocates single cpu time notice bad-fs achieves runtimes comparable depth-first scheduling wasted resource consumption failure handling show behavior bad-fs varying failure conditions recall unlike traditional distributed systems bad-fs scheduler re-create lost workloads exercising relevant code paths combination induced faults file system data structures describe create workloads inject faults deduce failure policy applied workload goal applying workloads exercise file system claim stress code path leaving avenue future work strive execute interesting internal cases workload suite sets programs run unix-based file systems fingerprinting ntfs requires set similar programs set programs called singlets focus single call file system api mkdir set generics stresses functionality common api path traversal table summarizes test suite file system test introduces special cases stressed ext inode imbalanced tree indirect doubly-indirect triply-indirect pointers support large 
files workloads ensure sufficiently large files created access structures file systems similar peculiarities make exercise -tree balancing code reiserfs type-aware fault injection step inject faults emulate disk adhering fail-partial failure model standard fault injectors fail disk blocks type oblivious manner block failed file system repeatedly injecting faults random blocks waiting uncover aspects failure policy laborious time-consuming process yielding insight key idea test file system efficient manner type-aware fault injection builds previous work semantically-smart disk systems type-aware fault injection failing blocks obliviously fail blocks specific type inode type information crucial reverse-engineering failure policy allowing discern strategies file system applies data structures disadvantage type-aware approach fault injector tailored file system tested requires solid understanding workload purpose singlets access chdir chroot stat statfs lstat open utimes read readlink exercise getdirentries creat posix api link mkdir rename chown symlink write truncate rmdir unlink mount chmod fsync sync umount generics path traversal traverse hierarchy recovery invoke recovery log writes update journal table workloads table presents workloads applied file systems test set workloads stresses single system call group invokes general operations span calls path traversal on-disk structures benefits typeawareness outweigh complexities block types file systems test listed table mechanism injecting faults software layer directly beneath file system pseudo-device driver layer injects block failures reads writes block corruption reads emulate block failure simply return error code issue operation underlying disk emulate corruption change bits block returning data cases inject random noise cases block similar expected corrupted fields software layer models transient sticky faults injecting failures file system emulate faults caused layers storage subsystem unlike approaches emulate faulty disks additional hardware imitate faults introduced buggy device drivers controllers drawback approach discern lower layers handle disk faults scsi drivers retry commands failure characterizing file systems react faults correct layer fault injection failure policy inference running workload injecting fault final step determine file system behaved determine fault affected file system compare results running fault perform comparison observable outputs system errors codes data returned file system api contents system log low-level traces recorded fault-injection layer human-intensive part process requires manual inspection visible outputs summary developed three-step fingerprinting methodology determine file system failure policy approach strikes good balance straightforward run exercises file system test workload suite roughly programs file system order block types block failed read write data corrupted file system amounts roughly relevant tests ext structures purpose inode info files directories directory list files directory data bitmap tracks data blocks group inode bitmap tracks inodes group indirect large files exist data holds user data super info file system group descriptor holds info block group journal super describes journal journal revoke tracks blocks replayed journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled reiserfs structures purpose leaf node items kinds stat item info files directories directory item list files directory direct item holds small files tail file indirect item large files exist data bitmap tracks data blocks data holds user data super info tree file system journal header describes journal journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled root internal node tree traversal jfs structures purpose inode info files directories directory list files directory block alloc map tracks data blocks group inode alloc map tracks inodes group internal large files exist data holds user data super info file system journal super describes journal journal data records transactions aggregate inode info disk partition bmap descriptor describes block allocation map imap control summary info imaps ntfs structures purpose mft record info files directories directory list files directory volume bitmap tracks free logical clusters mft bitmap tracks unused mft records logfile transaction log file data holds user data boot file info ntfs volume table file system data structures table presents data structures interest file systems test ext reiserfs jfs ntfs table list names major structures purpose note knowledge ntfs data structures incomplete closed-source system failure policy results present results failure policy analysis commodity file systems ext reiserfs version ibm jfs linux ntfs windows file system present basic background information discuss general failure policy uncovered bugs illogical inconsistencies source code explain problems discover due sheer volume experimental data difficult present results reader inspection file system studied depth present graphical depiction results showing workload blocktype pair detection recovery technique figure presents complex graphical depiction results caption interpretation details provide qualitative summary results presented figure linux ext linux ext similar classic unix file systems berkeley fast file system ext divides disk set block groups statically-reserved spaces bitmaps inodes data blocks major addition ext ext journaling ext includes set ondisk structures manage write-ahead log detection detect read failures ext primarily error codes derrorcode write fails ext record error code dzero write errors potentially leading file system problems checkpointing transaction final location ext performs fair amount sanity checking dsanity ext explicitly performs type checks blocks superblock journal blocks type checking important blocks directories bitmap blocks indirect blocks ext performs numerous sanity checks file-size field inode overly-large open detects reports error recovery detected errors ext propagates error user rpropagate read failures ext aborts journal rstop aborting journal leads readonly remount file system preventing future updates explicit administrator interaction ext retry rretry sparingly prefetch read fails ext retries originally requested block bugs inconsistencies found number bugs inconsistencies ext failure policy errors propagated user truncate rmdir fail silently important cases ext immediately abort journal failure implement rstop journal write fails ext writes rest transaction including commit block journal journal recovery file system easily corrupted ext perform sanity checking unlinkdoes check thelinkscount field modifying corrupted lead system crash finally ext redundant copies superblock rredundancy copies updated file system creation reiserfs reiserfs comprised vastly data structures ext virtually metadata data balanced tree similar database index key advantage tree structuring scalability allowing files coexist directory read failure write failure corruption ext detection j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ext recovery j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode reiserfs page detection directory internalroot context j-dataj-commit creation j-descj-header superdata asid indirectbitmap asid dir exit itemstat user item mappings reiserfs recovery tlb internalroot flushed context j-dataj-commit demap j-descj-header context superdata switch indirectbitmap change dir context itemstat item change table process identification techniques table lists techniques antfarm detect process event sparc architectures processorit impliesthatthe addressspace jfs detection longer valid imap-cntlbmap-desc implementation aggr-inodej-data antfarm j-supersuper datainternal implemented imapbmap 
dirinode jfs recovery imap-cntlbmap-desc aggr-inodej-data j-supersuper datainternal imapbmap dirinode figure file system failure policies tables detection recovery policies ext reiserfs jfs read write corruption faults injected block type range workloads workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery log write operations gray box workload applicable block type multiple mechanisms observed symbols superimposed key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate rredundancy rstop detection analysis reveals reiserfs pays close attention error codes reads writes derrorcode reiserfs performs great deal internal sanity checking dsanity internal leaf nodes balanced tree block header information level block tree number items free space super block journal metadata blocks magic numbers identify valid journal descriptor commit output file make replica file remote cluster depend cost generating data versus cost replicating choice varies workload system conditions figure shows bad-fs cost-benefit analysis adapts variety workloads conditions compare naive algorithms always-copy replicates pipeline volume stages completes never-copy replicate draw conclusions graph environment failure replication leads excessive overhead increases amount data case bad-fs outperforms always-copy match never-copy initial replication seed analysis environment frequent failure surprising bad-fs outperforms never-copy intuitively bad-fs outperforms always-copy case particulars workload failure rate replicating worthwhile stage bad-fs correctly avoids replicating stage always-copy naively replicates stages workload experience conclude demonstrations system running real workloads demonstration presented figure compare runtime performance badfs methods utilizing local storage resources remote configuration local storage utilized executed directly home node standalone emulates afs caching data execute nodes cooperative caching storage servers leftmost graph shows results remote workload execution bandwidth home server constrained rightmost shows local workload execution home server situated local area network execute nodes graphs draw conclusions bad-fs equals exceeds performance remote standalone caching workloads configurations workloads discussed great detail earlier profiling work large degrees batch pipeline data sharing note workloads consists endpoint data gain benefit system benefit caching cooperatively standalone mode greater batch-intensive workloads blast pipe-intensive pipe-intensive workloads important optimization pipeline localization performed bad-fs standalone cooperative caching bad-fs outperform standalone cold warm phases execution entire batch data set fits storage server cooperative caching improvement data initially paged data exceed capacity caches cooperative caching unlike standalone aggregate cache space fit working set benefit cooperative caching warm caches illustrated blast measurements graph left figure logfile analysis showed appears usenix symposium networked systems design implementation nsdi ibis cms blast amanda runtime wide-area execution remote standalone bad-fs ibis cms blast amanda runtime local-area execution remote standalone bad-fs figure workload experience graphs show runtime measurements real workloads workload submit pipelines dedicated condor pool cpus condor pool accesses local storage resources configurations remote redirected back home node standalone emulates afs-like caching home server bad-fs measurement present average runtime jobs run storage server storage cache cold subsequent jobs run cache warm graph left shows runtimes workload executed cluster separated home node emulated wide-area link set home node located local area network note y-axis shown log scale accentuate points interest detailed information workloads found profiling study storage servers slightly cache space needed total blast batch data subsequent jobs accessed servers forced refetch data refetching wide-area home server standalone case expensive refetching cooperative cache bad-fs local-area home server performance advantage disappears behavior servers explains increased variability shown measurements fourth penalty performing remote home node severe significant home node local-area network execute cluster result illustrates bad-fs improve performance bandwidth home server limiting resource finally comparing graphs make observation bad-fs performance independent connection home server caches cold independent warm scoping bad-fs achieve local performance remote environments wild evaluations conducted controlled environments conclude experimental presentation demonstration bad-fs capable operating uncontrolled real world environment created wide-area bad-fs system existing batch systems wisconsin large virtualization environments xen true condor blocks system additional thousand information cpus finally including inodes workstations clusters directory classroom blocks machines shared formats reiserfs large checks number users blocks expected values mexico unm pbs fields system manages cluster blocks checked dedicated machines carefully established bitmaps personal data scheduler blocks catalog home type storage information server wisconsin type-checked recovery submitted large prominent number aspect bad-fs recovery bootstrap policy jobs reiserfs batch systems tendency panic installing system special software detection virtually write failure rstop reiserfs calls panic file system crashes leading reboot recovery sequence reiserfs attempts ensure ondisk structures corrupted reiserfs recovers read write failures differently read failures reiserfs propagates error user rpropagate performs single retry rretry data block read fails indirect block read fails unlink truncate write operations reiserfs retries write failure bugs inconsistencies reiserfs exhibits inconsistencies bugs ordered data block write fails reiserfs journals commits transaction handling error rzero expected rstop lead corrupted data blocks metadata blocks point invalid data contents dealing indirect blocks reiserfs detects ignores read failure atruncate unlink updates bitmaps super block incorrectly leaking space reiserfs calls panic failing sanity check simply returning error code finally sanity type checking detect corrupt journal data replaying corrupted journal block make file system unusable block written super block ibm jfs jfs modern techniques manage data block allocation journaling scalable tree structures manage files directories block allocation unlike ext reiserfs jfs record-level journaling reduce journal traffic detection error codes derrorcode detect read failures ext write errors dzero exception journal superblock writes jfs employs minimal type checking superblock journal superblock magic version numbers checked sanity checks dsanity block types locations internal directed tree blocks scheduler directory execute blocks large inode workload blocks consisting number cms pipelines entries pointers resources block jfs checks figure make timeline number execution workload maximum expected block number type cpus varied equality widely check due field competition performed users block allocation maps availability verify idle workstations block corrupted vagaries recovery batch recovery scheduler strategies unm jfs consistently vary provided dramatically twenty cpus depending jumping block type forty hours error spikes occurs journal cpus superblock write hours jfs crashes due system crash rstop recovery write catalog errors server resulted rzero loss block monitoring read data failure primary running jobs superblock jfs benefits accesses cooperative alternate caching copy underscored rredundancy complete dynamic mount environment operation bottom graph corrupt cumulative primary read results traffic mount failure home node rstop explicit shown crashes rstop hills plateaus block hills allocation correspond map large inode spikes allocation map number read fails error cpus codes cpus metadata reads subnet begin executing fetch batch data home node smaller hills number cpus effect amount home read traffic server entering established cooperative cache fetch batch data peers finally figure illustrates design implementation bad-fs suitable running intensive batch-pipelined workloads multiple uncontrolled real world clusters failures disconnections bad-fs continues making steady progress removing burden user scheduling monitoring resubmitting jobs appears usenix symposium networked systems design implementation nsdi cpus total cpus cpus unm jobs running tens jobs complete writes server reads server figure wild graphs present timeline behavior large cms workload run bad-fs workload consisted cms pipelines run resources scavenged collection cpus mexico running pbs cpus wisconsin running condor topmost timeline presents total number cpus middle shows number jobs running cumulative jobs completed bottom shows cumulative traffic incurred home server related work designing bad-fs drew related work number distinct areas workflow management historically concern high-level business management problems involving multiple authorities computer systems large organizations approval loans bank customer service actions company scheduler works lower semantic level systems borrow lessons integration procedural data elements automatic management dependencies performance fault tolerance found variety tools systems managed dependencies jobs basic found unix tool make elaborate dependency tracking explored vahdat anderson work transparent result caching work authors build tool tracks process lineage file dependency automatically workflow description static encoding knowledge manner scheduler constructs private namespaces running workloads reminiscent database views private namespace simpler construct maintain views contrast present systems implementation challenges handling updates base tables propagation extant materialized views bad-fs improved prefetching batch datasets work noted difficulty correctly predicting future access patterns bad-fs explicitly supplied user declarative workflow description recent work peer-to-peer storage systems 
systems interesting solutions problem domain intended falls short applied context batch workloads reasons distributed file systems good match overlays developed environments communication clusters plan investigate future work similar work grid computing techniques designed bad-fs environments cluster-on-demand offers sophisticated resource clustering techniques badfs form cooperative cache groupings extensible systems share approach allowing application control recent work recently revisited approach extensible systems commercially successful specialized policies great greater batch workloads running systems designed interactive research mobile computing bears similarity flinn discuss process data staging untrusted surrogates ways surrogate similar bad-fs storage server major difference surrogate primarily concerned trust servers primarily concerned exposing control zap vmware checkpointing migration processes operating systems create remote virtual environment higher level batch system systems secure interposition janus complement badfs make resource owners donate resources shared pools finally bad-fs similar distributed file systems google file system motivated workloads deviate earlier file system assumptions additional similarity simplified consistency implementation gfs relax consistency semantics enable bad-fs explicit control earlier work coda afs applicable systems tcp backs-off appropriately final set variables dupthresh rto retransmits control thresholds timeouts variables set independently original values increasing decreasing dupthresh believed safe changing values increase amount traffic sender transmit packets increase congestion window appears sixth symposium operating systems design implementation osdi information loc control loc states cwnd message list dupthresh ack list rto high-resolution rtt ssthresh wakeup events cwnd cnt retransmits rcv nxt rcv wnd snd una snd nxt info total control total ictcp total table simplicity environment table reports number statements counted number semicolons needed implement current prototype ictcp linux methodology prototype ictcp implemented linux kernel experiments performed exclusively netbed network emulation environment single netbed machine mhz pentium cpu main memory intel etherexpress pro ethernet ports sending endpoints run handled generic file system code called jfs generic code attempts recover read errors retrying read single time rretry finally reaction failed sanity check propagate error rpropagate remount file system read-only rstop journal replay sanity-check failure replay abort rstop bugs inconsistencies found problems jfs failure policy jfs built-in redundancy expect jfs secondary copies aggregate inode tables special inodes describe file system error code returned aggregate inode read blank page returned user rguess design bug occurs read internal tree block pass sanity check bugs limit utility jfs recovery generic code detects read errors retries bug ictcp jfs receivers implementation run leads stock ignoring linux error corrupting experiments file dumbbell system topology windows ntfs ntfs senders non-unix routers file system interconnected study potential bottleneck link analysis requires detailed knowledge receivers on-disk experiments structures modified complete nistnet analysis router nodes figure emulate find complex ntfs behaviors error codes packet derrorcode reordering detect block experiments read vary write failures combination similar bottleneck ext bandwidth delay jfs maximum data queue write size fails ntfs intermediate records router error nodes code experiments run multiple dzero times corrupt averages file system reported ntfs variance performs low strong sanity cases checking dsanity metadata shown blocks evaluation file evaluate system unmountable ictcp reasonable metadata framework blocks deploying tcp journal extensions corrupted user-level ntfs surprisingly answer questions perform sanity easily checking existing tcp corrupted block implementation pointer converted point provide important information system structures safe control corrupt ictcp ictcp block ensure pointed updated resulting network cases flows ntfs tcp propagates friendly errors rpropagate ntfs computation aggressively overheads retry deploying rretry tcp extensions operations fail user-level processes times ictcp scale read fourth failures types writes tcp number extensions retries built varies deployed times ictcp data finally blocks difficult times mft develop tcp blocks extensions file system summary note present spend qualitative bulk summary paper addressing file fourth systems question tested table presents range summary extensions techniques implemented file discussing system employs limitations excluding ntfs approach ext set internal simplicity tcp ext variables tcp implements setsockopt simple option val reliable switch failure policy option case matching tcp design vcwnd philosophy found vcwnd val ext case family tcp set file vcwnd systems vcwnd checks error val codes check data modest level put sanity checking wire tcp recovers snd propagating test errors vcwnd aborting min operations cwnd min main vcwnd problem cwnd ext min cwnd failure cwnd handling write transmit errors tcp packets flight problems min including cwnd file system corruption reiserfs harm reiserfs concerned caching availability disconnected operation bad-fs storage servers enact similar role appears usenix symposium networked systems design implementation nsdi conclusions big bad wolf neighborhood bad meaning bad bad meaning good run dmc peter piper allowing external control long recognized powerful technique improve aspects system performance moving control external user system system user dictate policy individual nature work systems lacking mechanisms external control speculate systems proven adept speculation work majority workloads paper argued distinct nature batch workloads matched design traditional distributed file systems external control greater bad-fs distributed file system exposes internal control decisions external scheduler detailed knowledge workload characteristics scheduler carefully manages remote resources facilitates execution intensive batch jobs wide-area local-area clusters synthetic real workload measurements controlled uncontrolled environments demonstrated ability bad-fs workload specific knowledge improve throughput selecting storage policies scoping space allocation cost-benefit replication acknowledgments nate burnett nicholas coleman tim denehy barry henrichs florentina popovici muthian sivathanu vijayan prabhakaran department helpful discussions comments paper grateful excellent support provided members csl state appreciation jeff chase thoughtful analysis work development project finally anonymous reviewers helpful suggestions eric brewer excellent insightful shepherding substantially improved content presentation paper work sponsored part nsf ccrnsf ngsccr- ccritr- itrdoe de-fc wisconsin alumni research foundation emc ibm adya bolosky castro cermak chaiken douceur howell lorch theimer wattenhofer farsite federated reliable storage incompletely trusted environment proceedings symposium operating systems design implementation osdi boston dec agrawal imielinski swami database mining performance perspective ieee transactions knowledge data engineering dec altschul rules madden return schaffer return zhang figure in-kernel zhang modification miller adding vcwnd lipman tcp gapped stack blast requires psi-blast lines code generation ictcp protein applications set database search virtual programs variables nucleic acids bsd research setsockopt interface 
pages based anderson congestion dahlin window neefe tcp snd patterson test checks wang data serverless network put file systems wire show proceedings adding acm virtual cwnd symposium operating decision-making systems process principles simple sosp straightforward pages copper mountain cwnd ictcp dec minimum vcwnd arpaci-dusseau cwnd arpaci-dusseau simplicity environment burnett begin denehy addressing engle question gunawi difficult nugent convert tcp popovici implementation transforming ictcp policies initial mechanisms version infokernel ictcp proceedings implemented acm linux symposium experience operating systems implementing principles ictcp fairly sosp bolton straightforward landing requires lake adding george lines oct code avery table shows cms virtual added data statements requirements tcp kholtman create home cern ictcp kholtman tmp number cmsreqsv statements added baker perfect hartman indicator kupfer complexity shirriff ousterhout measurements non-intrusive distributed modifications file system figure proceedings partial acm symposium vcwnd operating variable systems principles added sosp ictcp stack pages network pacific safety grove investigate oct ictcp bent flows venkataramani tcp friendly leroy perform roy stanley evaluation measure arpaci-dusseau throughput arpaci-dusseau default tcp flows livny flexibility competing manageability ictcp performance flows grid measurements show storage ictcp appliance tcp proceedings friendly high-performance desired distributed default computing tcp flows obtain hpdcpages edinburgh bandwidth scotland competing jul ictcp bershad competing savage default pardyak tcp flows sirer show fiuczynski becker constraining chambers values eggers valid range extensibility safety ictcp performance illustrate spin operating created system unconstrained proceedings ictcp acm virtual symposium variables operating set systems principles sosp default tcp flows pages compete copper mountain unconstrained ictcp flows dec throughput breitbart appears deacon sixth schek symposium operating sheth systems design weikum merging implementation application-centric data-centric approaches support osdi throughput ratio cwnd cwnd cwnd cwnd cnt cwnd cnt cwnd cnt ssthresh ssthresh ssthresh throughput ratio flows set snd una snd una mss snd una mss flows set dupthresh dupthresh dupthresh flows set rto srtt var srtt var figure network safety ictcp graph shows lines line default default ictcp enforces parameters values safe range line unconstrained ictcp parameters set dupthresh graph unconstrained ictcp lines metric ratio throughput achieved default tcp flows competing ictcp flows versus competing default tcp flows graphs vary ictcp parameters set case set variable unsafe cwnd packets larger cwnd cnt times larger ssthresh times larger snd una packets lower dupthresh random values default rto remaining initial srtt rttvar packets dropped topology dumbbell senders receivers experiments rto experiments bottleneck bandwidth mbps delay rto experiments bottleneck bandwidth mbps percents drop rate default tcp flows reduced measurements shown figure graphs evaluate ictcp parameters explicitly setting parameter safe range x-axis graph increase number competing ictcp tcp flows graph shows lines line ictcp flows matching proposal virtual variables limited safe range line unconstrained ictcp flows metric ratio throughput achieved default tcp flows competing ictcp flows versus competing default tcp flows throughput ratio ictcp flows friendly ictcp flows unfriendly cwnd cwnd cnt ssthresh experiments show variables set safe range ensure friendliness expected ictcp flows allowed increase congestion window default tcp remain tcp friendly unconstrained ictcp flows larger congestion windows overly aggressive result competing tcp flows obtain fair share bandwidth evaluate variables control acknowledgments packets accepted behavior snd una shown fourth graph snd una variable represents highest unacknolwedged packet virtual snd una set safe range actual unconstrained ictcp over-estimates number bytes acknowledged increases congestion window aggressively ictcp correctly constrains snd una flow remains friendly results variables rcv wnd rcv nxt snd nxt shown cases ictcp flows remain friendly desired unconstrained ictcp flows fail completely increasing rcv wnd variable safe range receive buffer overflow final graphs explore dupthresh rto thresholds experiment retransmits variable decide connection terminated expected dupthresh decreasing increasing default unfriendliness dupthresh constrained case rto graph shows rto set exp backoff srtt rttvar resulting flow aggressive graphs represent small subset experiments conducted investigate tcp friendliness experimented setting ictcp variables random values safe range controlled ictcp parameters isolation sets parameters simultaneously cases tcp reno flows competing ictcp obtain appears sixth symposium operating systems design implementation osdi cpu utilization number connections scaling cpu per-ack intr per-round intr mstlist per-round intr reno throughput number connections scaling throughput reno per-round intr per-round intr msglist per-ack intr figure cpu overhead throughput scaling ictcp connect sender host receiver hosts network interfaces links mbps delay links aggregrate sender host send data outward mbps x-axis increase number connections sender host connections spread evenly receivers figure compares cpu utilization reno ictcp per-ack per-round interrupt figure shows ictcp throughput degradation sender load high bandwidth competing tcp reno flows desired summary results empirically demonstrate ictcp flows require safe variable settings tcp friendly experiments prove ictcp ensures network safety measurements combined analysis give confidence ictcp safely deployed cpu overhead evaluate overhead imposed ictcp framework ways explore scalability ictcp synthetic user-level libraries experiments explore ways user-level library reduce cpu overhead minimizing interactions kernel implement tcp vegas user-level top ictcp experiments directly compare ictcp infotcp scaling ictcp evaluate ictcp scales number connections increased host user-level extensions built ictcp expected set pieces tcp information rates factors determine amount overhead user process requires per-ack per-round interrupts user process ictcp message list ack list data structures show scaling properties user libraries built ictcp construct synthetic libraries mimic behavior case studies synthetic library per-ack interrupts representing ictcp-efr ictcp-eifel library per-round interrupts iccm final library per-round interrupts message ack list data structures ictcp-vegas ictcp-nice ictcp-rr graphs figure show ictcp tcp reno scale number flows increased host figure reports cpu utilization figure reports throughput figure shows ictcp per-ack per-round interrupts reaches cpu utilization connections additional cpu overhead ictcp message list negligible comparison tcp reno reaches roughly utilization connections slowly increases roughly connections figure shows throughput ictcp starts degrade connections depending per-ack per-round interrupts flows ictcp throughput per-ack per-round interrupts lower tcp reno ictcp cpu overhead noticeable prohibitive measure extent user-level library accurately implement tcp functionality measure interrupt miss rate defined frequently user misses interrupt ack end round scaling experiments connections observed worst-case miss rate per-ack interrupts per-round interrupts low miss rates imply functionality userlevel responsive current network conditions ictcp-vegas evaluate ictcp implement tcp vegas congestion avoidance user-level library tcp vegas reduces latency increases throughput relative tcp reno carefully matching sending rate rate packets drained network avoiding packet loss specifically sender sees measured throughput differs expected throughput fixed threshold increases appears sixth symposium operating systems design implementation transaction-oriented multi-system workflows sigmod record cantin hill cache performance selected spec cpu benchmarks computer architecture news sep chandra dahlin richards wang anderson larus experience language writing coherence protocols proceedings usenix conference domain-specific languages santa barbara oct chang gibson automatic hint generation speculative execution proceedings symposium operating systems design implementation osdi pages orleans louisiana feb chase grit irwin moore sprenkle dynamic virtual clusters grid site manager proceedings ieee international symposium high performance distributed computing hpdc seattle june dabek kaashoek karger morris stoica wide-area cooperative storage cfs proceedings acm symposium operating systems principles sosp banff canada oct dahlin wang anderson patterson cooperative caching remote client memory improve file system performance proceedings symposium operating systems design implementation osdi monterey nov eda industry working group eda resource http eda edwards mckendry exploiting read-mostly workloads filenet file system proceedings acm symposium operating systems principles sosp pages litchfield park arizona dec engler kaashoek toole exokernel operating system architecture application-level resource management proceedings acm symposium operating systems principles sosp pages copper mountain dec appears usenix symposium networked systems design implementation nsdi feeley morgan pighin karlin levy implementing global memory management workstation cluster proceedings acm symposium operating systems principles sosp pages copper mountain dec flinn sinnamohideen tolia satyanarayanan data staging untrusted surrogates proceedings usenix symposium file storage technologies fast san francisco apr ford hibler lepreau tullman back clawson microkernels meet recursive virtual machines proceed-ings symposium operating systems design implementation osdi seattle oct foster avery petascale virtual data grids data intensive science griphyn white paper foster kesselman tsudik tuecke security architecture computational grids proceedings acm conference computer communications security conference pages foster kesselman tuecke anatomy grid enabling scalable virtual organizations international journal supercomputer applications frey tannenbaum foster livny tuecke condor-g computation management agent multiinstitu- tional grids proceedings ieee international symposium high performance distributed computing hpdc san francisco aug gelenbe optimal checkpoint interval journal acm apr georgakopoulos hornick sheth overview workflow management process modeling workflow automation infrastructure distributed parallel databases ghemawat gobioff leung google file system proceedings acm symposium operating systems principles sosp bolton landing lake george oct goldberg wagner thomas brewer secure environment untrusted helper applications proceedings sixth usenix security symposium july gribble brewer hellerstein culler scalable distributed data structures internet service construction proceedings symposium operating systems design implementation osdi san diego oct gupta mumick maintenance materialized views problems techniques applications ieee quarterly bulletin data engineering special issue materialized views data warehousing jones interposition agents transparently interposing user code system interface proceedings acm symposium operating systems principles sosp pages asheville north carolina dec kistler satyanarayanan disconnected operation coda file system acm transactions computer systems feb kubiatowicz bindel eaton chen geels gummadi rhea weimer wells weatherspoon zhao oceanstore architecture global-scale persis-tent storage proceedings international conference architectural support 
programming languages operating systems asplos pages cambridge nov lancaster renderman web site http renderman litwin neimat schneider family order preserving scalable distributed data structures proceedings international conference large databases vldb pages santiago chile sep litzkow livny mutka condor hunter idle workstations proceedings acm computer network performance symposium pages june muthitacharoen morris gil chen ivy read write peer-to-peer file system proceedings symposium operating systems design implementation osdi boston dec osman subhraveti nieh design implementation zap system migrating computing environments proceedings symposium operating systems design implementation osdi boston dec ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system proceedings acm symposium operating system principles sosp pages orcas island dec platform computing improving business capacity distributed computing platform industry financial raman matchmaking frameworks distributed resource management phd thesis wisconsin-madison oct rizzo dummynet simple approach evaluation network protocols acm computer communication review roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference usenix pages san diego june rowstron druschel storage management caching past large-scale persistent peer-to-peer storage utility proceedings acm symposium operating systems principles sosp banff canada oct rusinkiewicz sheth specification execution transactional workflows modern database systems object model interoperability pages saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area file system proceedings symposium operating systems design implementation osdi boston dec sapuntzakis chandra pfaff chow lam rosenblum optimizing migration virtual computers proceedings symposium operating systems design implementation osdi boston dec satyanarayanan study file sizes functional lifetimes proceedings acm symposium operating systems principles sosp pages pacific grove dec seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating systems design implementation osdi pages seattle oct soderbergh mac lies videotape apple hotnews articles fullfrontal sullivan werthimer bowyer cobb gedye anderson major seti project based project serendip data personal computers proceedings international conference bioastronomy thain bent arpaci-dusseau arpaci-dusseau livny pipeline batch sharing grid workloads proceedings high-performance distributed computing hpdcpages seattle june thain livny parrot transparent user-level middleware data-intensive computing workshop adaptive grid middleware orleans louisiana sep vahdat anderson transparent result caching proceedings usenix annual technical conference usenix orleans louisiana june vogels file system usage windows proceedings acm symposium operating systems principles sosp pages kiawah island resort south carolina dec 
vmm low-level system simulator called simics architectures supported xen antfarm xen xen open source virtual machine monitor intel architecture xen paravirtualized processor interface enables lower overhead virtualization expense porting system software explicitly make feature xen mechanisms describe equally applicable conventional virtual machine monitor vmware operating systems ported run xen proprietary commercial operating systems microsoft windows supported antfarm xen implemented set patches xen hypervisor concentrated handlers events page faults page table updates privileged register access additional hooks added xen back-end block device driver antfarm patches xen including debugging measurement infrastructure total approximately lines files antfarm simics simics isafullsystem unmodified commercial operating systems applications variety processor architectures simics virtual machine monitor strict sense direct execution user instructions play proceedings usenix annual technical conference june boston role vmm allowing antfarm observe interpose operating system application hardware requests vmm simics explore process awareness techniques sparc linux windows xen-only implementation antfarm simics implemented simics extension module simics extension modules shared libraries dynamically linked main simics executable extension modules read write application memory registers vmm simics hooks called haps hardwareeventsforwhichextensionmodulescanregistercall- back functions antfarm simics hap detect writes antfarm simics sparc hap detect processor context changed invocation callback akin exception raised guest accesses privileged processor registers true vmm memory write breakpoint installed antfarm simics pages page tables page table updates detected vmm xen marks page tables read-only detect event antfarm simics consists lines code simics sparc total approximately lines process awareness evaluation section explorethe accuracyof antfarmin implementation environments characterize runtime overhead antfarm xen analysis accuracy decomposed components thefirstis abilityto correctlydetectprocess creations exits context switches call aspect completeness component time difference lag process events occur operating 
system detected vmm evaluation evaluation xen version version linux kernel xen privileged control linux kernel version unprivileged vms noted evaluation hardware consists ghz pentium ram virtual machines allocated ram environment microsoft windows windows run xen simics purpose simics virtual machines configured ghz pentium ram completeness quantify completeness guest operating system exit andcontext switch event records include asid time event obtained processor cycle counter traces compared similar traces generated antfarm guest traces functionally equivalent information provided paravirtualized included process event interface evaluation implicitly compares accuracy antfarm ideal represented paravirtual interface addition process creation exit context switch guests report address space creation destruction events discriminate errors caused mismatch processes address spaces errors induced inaccurate address space inferences made antfarm categorize incorrect inferences false negatives false positives false negative occurs true process event missed antfarm false positive occurs antfarm incorrectly infers events exist determine false negatives occurred one-to-one matches found os-reported event pair traces required matching event asid occur range event plausible match process-creation event inferred event occur previous os-reported process os-reported process creation events asid table reports process address space event countsgatheredby ourguest oses antfarmduring experiment utilizing process intensive workloads workload synthetic creates processes runs seconds exits process creation rate processes linux synthetic workload variants creates processes fork fork exec employs vfork exec windows processes created createprocess api workload parallel compile bash shell sources command make clean object directory compilation workload chosen creates large number short-lived processes stressing antfarm ability track concurrent processes varying runtimes antfarm incurs false negatives tested proceedings usenix annual technical conference june boston process addr spc inferred process addr spc inferred context create create create exit exit exit switch inferred linux fork fork exec vfork osdi cpu utilization bandwidth delay ictcp-vegas cpu overhead infovegas ictcp-vegas per-ack intr ictcp-vegas polling ictcp-vegas per-round intr reno figure ictcp-vegas cpu overhead figure compares cpu utilization reno infovegas versions ictcp-vegas vary bottleneck-link bandwidth x-axis decreases congestion control window cwnd implementation implementation vegas congestion control algorithm ictcp-vegas structured operation vegas user-level exec compile linux fork fork exec vfork exec compile windows create compile table completeness table shows total number creations exits processes address spaces reported operating system total number process creations exits inferred antfarm shown comparison antfarm detects process creates exits false positives false negatives linux windows fork exec lead false positives linux bold face values false positives due mismatch address spaces processes matching counts address space creates inferred creates actual inferred context switch counts shown completeness accurate expected cases process-related events reported instrumented oses detected vmm fact inferred counts greater equal reported counts suggests verified os-reported event properly matched vmm-inferred event linux windows false positives occur indicatingantfarmcan precisely detectaddressspace events one-to-one match address spaces processes operating systems linux false positives occur indicatedin table bythe os-reported counts discrepancy due implementationof linux fork exec system calls unix programscreate user processes invoking theforksystemcallwhich amongotherthings constructs address space child process child address space copy parent address space cases newly created child process immediately invokes exec system call replaces child virtual memory image program read disk linux exec invoked existing process address space cleared reused newly loaded program contrast linux destroys releases address space process invoking exec address space allocated newly exec program linux process library library simply passes messages directly ictcp buffering layer implement versions vary point poll ictcp information time send packet time acknowledgment received round ends library relevant tcp state calculates target congestion window vcwnd vcwnd ictcp-vegas sets explicitly inside ictcp note implementation ictcp-vegas similar infovegas part infokernel primary difference infotcp manage vcwnd provide control tcp variables infovegas 
calculates vcwnd actual cwnd infovegas buffer packets transfer tcp layer infovegas blocks acknowledgment arrives point recalculates vcwnd send messages evaluation verified ictcp-vegas behaves in-kernel implementation vegas due space constraints show results focus evaluation cpu overhead figure shows total user system cpu utilization function network bandwidth tcp reno versions ictcp-vegas infovegas network bandwidth increases cpu utilization increases implementation cpu utilization system utilization increases significantly infovegas due frequent user-kernel crossings extra overhead reduced ictcp-vegas polls ictcp message send wakes arrival acknowledgment document latency bottleneck link bandwidth mbps link capacity latency reno tcp nice ictcp-nice figure ictcp-nice link capacity latency foreground flow competes background flows line corresponds run experiment protocol background flows ictcp tcp nice reno vegas y-axis shows average document transfer latency foreground traffic foreground traffic consists -minute section squid proxy trace logged berkeley background traffic consists long-running flows topology dumbbell sending nodes receiving nodes foreground flow sender receiver pairs background flows distributed remaining sender receiver pairs bottleneck link bandwidth varied x-axis noticeable ictcp information getsockopt interface incurs significant overhead ictcp-vegas greatly reduce overhead information frequently vegas adjusts cwnd end round ictcp-vegas behave accurately waking round optimization results cpu utilization higher ictcp-vegas in-kernel reno tcp extensions fourth axis evaluating ictcp concerns range tcp extensions importance issue spend remaining paper topic address question demonstrating tcp variants built top ictcp case studies explicitly meant exhaustive illustrate flexibility simplicity ictcp briefly discuss ictcp implement wider set tcp extensions ictcp-nice case study show tcp nice implemented user-level ictcp study demonstrates algorithm differs radically base ictcp reno algorithm implemented ictcp-nice requires access internal state ictcp complete message list overview tcp nice zero-cost background transfer tcp nice background flow interferes foreground flows reaps large fraction spare network bandwidth tcp nice appears sixth symposium operating systems design implementation osdi similar tcp vegas additional components multiplicative window reduction response increasing round-trip times ability reduce congestion window discuss components turn tcp nice halves current congestion window long round-trip times measured unlike vegas reduces window halves window packets lost determine window size halved tcp nice algorithm monitors round-trip delays estimates total queue size bottleneck router signals congestion estimated queue size exceeds fraction estimated maximum queue capacity specifically tcp nice counts number packets delay exceeds minrtt maxrtt minrtt fraction delayed packets round exceeds tcp nice signals congestion decreases window multiplicatively tcp nice window effect congestion window tcp nice adds timer waits number rtts sending packets implementation implementation ictcp-nice similar ictcp-vegas slightly complex ictcp-nice requires information packet summary statistics ictcp-nice obtains full message list sequence number seqno round trip time usrtt packet implementation windows tricky vcwnd mechanism case window ictcp-nice sets vcwnd single rtt period periods evaluation demonstrate effectiveness ictcp approach replicate experiments original tcp nice paper figures results show ictcp-nice performs identically in-kernel tcp nice desired figure shows latency foreground connections competes background connections spare capacity network varied results ictcp-nice tcp nice background connections latency foreground connections order magnitude faster tcp reno background connections desired ictcp-nice tcp nice perform similarly graphs figure show latency foreground connections throughput background connections number background connections increases graph top shows background flows added document latency remains essentially constant ictcp-nice tcp nice background flows graph bottom shows ictcp-nice tcp nice obtain invokes exec distinct address spaces overlap time words runtime process partitioned segments segment corresponds period fork exec corresponds period exec process exit antfarm based address space tracking concludes processes created leading inferred process creations exits occurred due idiomatic fork exec process partitioned distinctive linux case figure depicts temporal relationship inferred pseudo-processes duration pseudo-process small case compilation workload average time fork exec compared average lifetime pseudoprocess seconds difference orders magnitude pseudo-processesare separated shorttime period active interval corresponds time original address space destroyed address space created compilation workload interval averaged larger user instructions executed absence user address space combination pseudo-processes detected antfarm encompasses user activity true process conventional fork exec imply proceedings usenix annual technical conference june boston figure effects error figure shows type process identification error occurs tested platform error lag true event occurs vmm detects figure consists falsely partitioning single process multiple inferred processes linux occurs exec typically immediately fork sparc partitioning process calls fork exec create lag linux avg max exit lag linux concurrent processes windows figure lag system load figure shows average maximum create exit lag time measurements variety system load levels evaluation environments average worst case create lag affected system load linux windows small constant linux large exit lag competing processes linux exit lag sensitive system load substantive activity true user process captured pseudo-process lag aspect process identification accuracy consideristhe timedifferencebetweena processevent eventis detected vmm define process exist instant fork equivalent system call invoked exit defined start exit system call definitions maximally conservative figure create lag labeled exit lag labeled lag similar nature response time expect sensitive system load evaluate sensitivity conduct experiment measures lag times levels system load linux linux windows experiment cpubound processes created additional test processes created create exit lag time computed test process creationswere separated test process slept exiting results experiments presented figure graph x-axis shows number concurrent cpu-bound processes y-axis shows lag time create lag sensitive system load linux windows steadily increasing lag time increasing system load result intuitive call scheduler occur invocation create process api parent process begins child process runs vmm detects linux exhibits process creation policy leads small constant creation lag antfarm detects process creation process runs vmm informed process existence user instructions executed exception idle linux shows large exit lag average reason anomaly linux kernel tasks including idle task user address space borrow previously active user address space run mechanism kernel task run incurring expense tlb flush case experiment test processes started intervals process sleeps processes ready run approximately mselapse processexitand process begins interval linux idle task active preventsthe previousaddress space released leads observed delay big picture figure shows set timelines depicting antfarm tracksprocessactivity overtime parallelcompilation proceedings usenix annual technical conference june boston process count vmm linux bash compile time diff process count vmm linux bash compile time diff process count vmm bash compile time diff figure compilation workload timelines linux linux windows process count timeline shown timeline depicts os-reported process count vmm-inferred process count difference versus time lag larger impact accuracy false positives linux exhibits significantly smaller lag linux track process counts accurately workload platforms top curve eachgraphshowsthetrue reported operating system middle curve shows current process count inferred antfarm bottom 
curve shows difference curves calculated inferred actual result large creation lag linux apparent larger negative process count differencescomparedto linux workload metriccombination false positives experienced linux environmentsuch lightly loaded system tend reduce lag metric total cumulative process count false positives incurred linux problematic exit lag prominent graphs large persistent exit lag effects show significant positive deviations difference curves fact errors due fork exec accumulate time linux apparent increasing inaccuracy trend present overhead evaluate overhead process awareness techniqueswe measure comparethe runtime workloads antfarm pristine build xen workload microbenchmark represents worst case performance scenario antfarm experiments performed linux guests vmm extensions affect code paths page tables updated microbenchmark focuses execution paths program allocates memory touches page ensure page table entry allocated page created exits causing page tables cleared released program run times total elapsed time computed experiment repeated times average duration reported negligible variance experiments unmodified version xen experiment required average seconds complete antfarm xen experiment average seconds complete average slowdown worst case runtime configuringand buildingbash compared modified unmodified versions xen unmodifiedcase average measured runtime trials average runtime experiment modified xen variance experiments negligible yielding slowdown process-intensive application workload proceedings usenix annual technical conference june boston process addr spc inferred process addr spc inferred context create create create exit exit exit switch inferred sparc linux fork fork exec vfork compile table completeness sparc table shows results experiments reported table sparc linux false positives occur fork due implementation copy-on-write antfarm infers additional non-existent exit create event pair exec error due multiple address spaces process stems flush occurs clear caller address space exec sparc evaluation implementation process tracking sparc simics virtual machine configured mhzultrasparc iiprocessorand mbofram sparc linux version guest operating system tests guest operating system instrumented report information completeness criteria evaluate process awareness sparc table lists total event counts process creation micro-benchmark bash compilation workload false negatives occur contrast fork-only variant microbenchmark incurs false positives reason copy-on-write implementation fork linux fork writable portionsof parent saddress space marked read-only copy-on-write shared child entries parent page tables updated tlb entries flushed sparc linux accomplishes efficiently flushing parent current tlb entries context demap operation context demap incorrectlyinterpretedbyantfarmasa processexit assoonas parent scheduled run detect address space signal matching spurious process creation false positives caused fork sparc character caused exec errors limited convention tiny time interval fork exec fork invoked processes user shell occur repeatedly process lifetime linux sparc case figure depicts process repeatedly invokes fork partitioned inferred pseudo-processesby antfarm execis additionalfalse positives reason linux case process inference technique falsely reports creation address spaces don treally exist behavior tlb demap operation occurswhena error mode differentthan observed errors due faulty assumption single address space process sparc error occurs chosen indicator context demap happen correspondingaddress space deallocated sources false positives expect compilation workload experience approximately multiple false positives fork exec synthetic benchmark fewer false positives expect due vfork gnu make gcc vfork creates process duplicate parent address space parent page tables changed flush required exec invoked detect creation single address space vfork exec linux antfarmexperiences false positives build process consists processes created make gcc processes created calls external shell process creations induce false positives observe lag lag os-recorded vmm-inferred process events sparc linux comparable linux average maximum lag values sparc linux system loads shown figure create lag sensitive system load exit lag unaffected load proceedings usenix annual technical conference june boston create lag sparc-linux avg max exit lag concurrent processes figure lag system load sparc figure shows average maximum create exit lag time measurements experiments figure create lag grows system load exit lag small constant independent load process count vmm sparc linux bash compile time diff figure compilation workload timeline compilation timeline comparable figure sparc linux limitations sparc inferencetechnique simple suffers drawbacks relative shown technique incurs false positives techniques spite additional false positives figure shows technique track process events parallel compilation workload accurately linux unlike assume page directory page shared multiple runnable processes make assumption context ids sparc reason vastly smaller space unique context ids sparc bits field distinct contexts represented concurrently system exceeds number active processes context ids necessarily recycled cases system softprocess count vmm sparc context overflow time diff figure context overflow processes exist represented sparc context ids techniques fail detect context reuse ware limit number concurrent contexts supports linux sparc architectures context bits concurrent address spaces supported recycling figure shows behavior sparc process detection techniques processes exist distinguished context ids limit reached technique fails detect additional process creations importance limitation reduced busy servers rarely active processes fact doubt influenced selection context field size discussion process event detection techniques antfarm based mechanisms provided cpu architecture implement manage virtual address spaces responsibilities general-purpose operating systems maintain process isolation techniques assume follow address space conventions suggested mmu features architecture deviates convention detection accuracy differ reported evaluation shows widely operatingsystems adhereto ourassumptions antfarmprecisely identifies desired process events windows linux false positives occur linux andsparc linux thefalsepositivesare stylized affectthe ability antfarmto accurate process count architectures devoted hardware-assisted virtualization configurations reduce eliminate vmm track guest page taproceedings usenix annual technical conference june boston ble updates context switches amd private guest-cr options secure virtual machine svm architecture fact prevent vmm observing guest operating systems shadow page tables explicitly supported architectures increase performancepenalty exacted techniques antfarm case study anticipatory scheduling order disk requests serviced make large difference disk performance requests adjacent locations disk serviced consecutively time spent moving disk head unproductively minimized primary performance goal disk scheduling algorithms case study explores application innovative scheduling algorithm called anticipatoryscheduling virtualmachine environment ofantfarmforxen background iyer demonstrated phenomenon call deceptive idleness disk access patterns generated competing processes performing synchronous sequential reads deceptive idleness leads excessive seeking locations disk solution called anticipatory scheduling introduces small amount waiting time completion request initiation process disk request completed issue request nearby location strategy leads substantial seek savings throughput gains concurrent disk access streams exhibit spatial locality anticipatory scheduling makes process-specific information decides wait process issue read request long wait based statistics disk scheduler processes recent disk accesses average distance request stored estimate process access distance large sense waiting process issue request nearby statistics long 
process waits request completes issues order determine long make sense wait request issued anticipatory scheduling work virtual machine environment system-wide information disk requests required estimate disk head located essential deciding request nearby obehavior requiredto determine whetherand howlong wait information completely single guest requests vmm distinguish guestlevel processes guests vmm cooperate implement anticipatory scheduling requires introduction additional specialized vmm-to-guest interfaces interfaces case legacy binary-only components case interfaces exist today information implement anticipatory scheduling effectively vmm vmm distinguish guest processes additionally associate disk read requests specific guest processes pieces information vmm implementation anticipatory scheduling maintain average seek distance inter-request waiting time processes guests antfarm inform implementation anticipatory scheduling inside xen associate disk read requests processes employ simple context association strategy associates read request process active simple strategy potential asynchrony operating system account due request queuing inside read issued vmm process originated blocked context switched processor leads association error researched accurate ways associating reads true originating process tracking movement data disk memory requesting process methods proven effective overcoming association error due queuing limited space present techniques implementation anticipatory scheduling paper simple context association implementation xen implements device driver virtual machines ddvm ddvm virtual machine allowed unrestricted access physical devices ddvms logically part xen vmm operationally guests runningin disk requests ddvm idealized disk device interface ddvm carries behalf current versions xen driver vms run linux advantage broad device support offers device back-end driver services requests submitted instance front-end driver located proceedings usenix annual technical conference june boston vmas aggregate throughput sec scheduler configuration comparison layer schedulers processes vms process vms processes figure benefit process awareness anticipatory scheduling graph shows theaggregate throughput forvarious configurations scheduler number virtual machines number processes virtual machine experiment linux deadline scheduler standard anticipatory scheduler vmm-level anticipatory scheduler vmas adding process awareness enables vmas achieve single process sequential read performance aggregate competing sequential streams running guest layer effective process case global disk request information normal vms standard linux kernel includes implementation anticipatory scheduling implement anticipatory scheduling vmm layer enabling linux anticipatory scheduler xen ddvm manages disk drive make existing implementationprocessaware resents processes running vms disk request arrives foreign virtual machine xen hypervisorabout process active foreign virtual machine ability distinguish processes expect vmm-level anticipatory scheduler vmas competing processes exist vms evaluation vmas repeat experiments original anticipatory scheduling paper virtual machine environment experiment consists running multiple instances program sequentially reads segment private file vary number processes assignment processes virtual machines disk scheduler guests vmm explore process awareness influences effectiveness anticipatory scheduling vmm make linux deadline scheduler nonanticipatory baseline results scheduler configurations combined workloads shown figure workloads virtual machine processes virtual machines process virtual machines processes experiment shows results configuration anticipatory scheduling demonstrates expected performance anticipation workloads test system results aggregate throughputof sec configurationenables anticipatory scheduling guest deadline scheduler xen virtual machine process case guest complete information processes actively reading disk expect anticipatory scheduler guest level effective figure shows fact case anticipatory scheduling improve aggregate throughput sec sec cases deadline scheduler due lack information processes virtual machines experiment demonstrates performance unmodified anticipatory scheduling vmm layer similar case anticipatory scheduling running guest layer expect performance improvement two-virtual-machine one-process-each case good vmm distinguish virtual machines operating system distinguish processes improvement occur implementation detail xen ddvmback-enddriver requestsin thecontextofa single dedicatedtask anticipatory scheduler interprets presented stream single process making alternating requests parts disk performance comparable configuration anticipation workloads final configuration shows benefit process awareness anticipatory scheduling implemented vmm layer workload configurations anticipatory scheduling works improving aggregate throughput factor sec sec implemented atthevmm layer anticipatoryschedulingin thisconfiguration complete information requests reaching disk process awareness extensions track statistics individual process enabling make effective anticipation decisions proceedings usenix annual technical conference june boston conclusion widespread adoption virtual machines brings interesting research opportunities reevaluate operating system services implemented implementing os-like services vmm made challenging lack high-level application information techniquesdevelopedin paperand implementation antfarm explicit information important operating system abstraction process inside vmm observing interaction guest virtual hardware method alternative explicitly exporting required information vmm directly enabling vmm independently infer information vmm decoupled specific vendor version correctness guests supports acknowledgments work sponsored sandia national laboratories doctoral studies program nsf ccritr- cnsand generous donations network appliance emc amd amd programmer manual volume system programming december ballmer keynote address microsoft management summit april bressoud schneider hypervisor-based fault tolerance proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december bugnion devine rosenblum disco running commodity operating systems scalable multiprocessors proceedings acm symposium operating systems principles sosp pages saint-malo france october chen noble virtual real hotos proceedings eighth workshop hot topics inoperating systems page ieeecomputer society dragovic fraser hand harris pratt warfield barham neugebauer xen art virtualization proceedings acm symposium operating systems principles sosp bolton landing lake george york october fraser hand neugebauer pratt warfield williamson safe hardware access xenvirtual machine monitor oasis asplos workshop gao reiter song gray-box program tracking foranomaly detection proceedings usenix security symposium pages san diego usa august garfinkel pfaff chow rosenblum boneh terra virtual machine-based platform fortrusted computing proceedings acm symposium operating systems principles sosp bolton landing lake george york october garfinkel rosenblum virtual machine introspection based architecture intrusion detection proc network distributed systems security symposium february goldberg survey virtual machine research ieee computer gum system extended architecture facilities virtual machines ibm journal research development november intel intel virtualization technology specification iaintel architecture april iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous proceedings acm symposium operating systems principles sosp pages banff canada october joshi king dunlap chen detecting past present intrusions vulnerabilityspecific predicates proceedings acm symposium operating systems principles sosp pages brighton united kingdom october king chen backtracking intrusions proceedings acm symposium operating systems principles sosp banff canada october magnusson christensson eskilson forsgren allberg ogberg larsson moestedt werner simics full system simulation platform ieee computer february popek goldberg formal requirements virtualizable generation architectures communications acm sekar bowen segal preventing intrusions process behavior monitoring proc workshop intrusion detection network monitoring pages berkeley usa usenixassociation sivathanu bairavasundaram arpacidusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december somayaji forrest automated response system-call 
delays proceedings usenix annual technical conference usenix san diego california june sugerman venkitachalam lim virtualizing devices vmware workstation hosted virtual machine monitor proceedings usenix annual technical conference usenix boston massachusetts june uhlig levasseur skoglund dannowski scalable multiprocessor virtual machines proceedings virtual machine research technology symposium pages san jose california waldspurger memory resource management vmware esx server proceedings symposium operating systems design implementation osdi boston massachusetts december whitaker shaw gribble scale performance denali isolation kernel proceedings symposium operating systems design implementation osdi boston massachusetts december 
document latency sec number background latency reno tcp nice ictcp-nice throughput number background throughput reno disk failure concern evident write failures induce panic reiserfs takes action ensure file system corrupted reiserfs great deal sanity type checking behaviors combine form hippocratic failure policy harm jfs kitchen sink jfs consistent diverse failure detection recovery techniques detection jfs sanity checks error codes recovery jfs redundancy crashes system retries operations depending tcp nice ictcp-nice figure ictcp-nice impact background flows graphs correspond experiment graph shows average document latency foreground traffic graph shows number bytes background flows manage transfer minutes period line corresponds protocol background flows tcp reno ictcp-nice tcp nice number background flows varied x-axis bottleneck link bandwidth set kbps delay experimental setup identical figure throughput number flows increases desired ictcp-nice tcp nice achieve similar results iccm show important components congestion manager built ictcp main contribution study show information shared ictcp flows multiple ictcp flows sender cooperate overview congestion manager architecture motivated types problematic behavior exhibited emerging applications applications employ multiple concurrent flows sender receiver flows compete resources prove overly aggressive share network information applications udp-based flows sound congestion control adapt changing network conditions addresses problems inserting module sender receiver layer appears sixth symposium operating systems design implementation osdi maintains network statistics flows orchestrates data transmissions hybrid congestion control algorithm obtains feedback receiver implementation primary difference iccm location iccm built top ictcp layer top iccm leverages congestion control algorithm statistics present tcp iccm considerably simpler implement iccm guarantees congestion control algorithm stable friendly existing tcp traffic iccm approach drawback non-cooperative applications bypass iccm tcp directly iccm guarantee fairness flows aware iccm architecture running sending endpoint components iccm clients individual flow iccm server component receiving endpoint iccm server roles identify macroflows flows endpoint destination track aggregate statistics macroflow identify macroflows client flow registers process destination address iccm server track statistics client flow periodically obtains network state ictcp number outstanding bytes snd nxt snd una shares state block type fails error iccm server iccm server periodically updates statistics macroflow sums outstanding bytes flow macroflow client flow obtain aggregate statistics macroflow time intervals implement bandwidth sharing clients macroflow client calculates window limit number outstanding bytes specifically iccm client obtains server number flows macroflow total number outstanding bytes flow statistics client calculates number bytes send obtain fair share bandwidth client tcp transport simply sets vcwnd ictcp number iccm clients macroflow compete share bandwidth evenly evaluation demonstrate effectiveness ictcp build congestion manager replicating experiments performed figure experiments shown figure place flows macroflow shown graph tcp reno flows macroflow share bandwidth fairly performance connections varies standard deviation contrast shown graph iccm flows macroflow connections progress similar consistent rates iccm flows achieve throughputs sequence number time seconds reno sequence number time seconds iccm figure iccm fairness graphs compare performance concurrent transfers sender receiver bottleneck link set delay graph stock reno graph iccm manages tcp flows roughly standard deviation ictcp-rr tcp fast retransmit optimization fairly sensitive presence duplicate acknowledgments specifically tcp detects duplicate acks arrived assumes loss occurred triggers retransmission recent research packet reordering common internet earlier designers suspected frequent reordering occurs tcp sender receives rash duplicate acks wrongly concludes loss occurred result segments unnecessarily retransmitted wasting bandwidth congestion window needlessly reduced lowering client performance overview 
number solutions handling detection api called level ext reiser jfs dzero derrorcode dsanity dredundancy rzero rpropagate rstop rguess rretry rrepair rremap rredundancy table iron techniques summary table depicts summary iron techniques file systems test check marks higher relative frequency usage technique ntfs persistence virtue compared linux file systems ntfs persistent retrying failed requests times giving propagate errors user reliably testing ntfs needed order broaden conclusions part ongoing work technique summary finally present broad analysis techniques applied file systems detect recover disk failures concentrate techniques underused overused inappropriate manner detection recovery illogical inconsistency common found high degree illogical inconsistency failure policy file systems observable patterns figure reiserfs performs great deal sanity checking important case journal replay result single corrupted block journal corrupt entire file system jfs illogically inconsistent employing techniques scenarios similar note inconsistency problematic logically inconsistent good idea file system provide higher level redundancy data structures deems important root directory criticizing inconsistencies undesirable unintentional jfs attempt read alternate superblock read failure occurs reading primary superblock attempt read alternate deems primary corrupted estimation root illogical inconsistency failure policy diffusion code implements failure policy spread kernel diffusion encouraged architectural features modern file systems split generic specific file systems observed cases developers implement portions code implement failure policies cases reiserfs panic write failure arises due inconsistency indicative lack attention paid failure policy detection recovery bugs common found numerous bugs file systems tested found sophisticated techniques generally indicative difficulty implementing correct failure policy hints effort put testing debugging code suggestion literature helpful periodically inject faults 
normal operation part fire drill method reveals testing broad cover code paths testing indirect-block handling reiserfs observe classes fault mishandling detection error codes amazingly error codes file system common jfs found occasionally file systems testing framework part file system developer toolkit tools class error easily discovered detection sanity checking limited utility file systems sanity checking ensure metadata meets expectations code modern disk failure modes misdirected phantom writes lead cases file system receive properly formatted incorrect block bad block passes sanity checks corrupt file system file systems tested exhibit behavior stronger tests checksums recovery stop correctly file systems employed form rstop order limit damage file system types errors arose reiserfs calls panic virtually write error prevent corruption structures careful techniques write failure ext abort transaction correctly squelch writes file system leading corruption fine-grained rebooting difficult apply practice recovery stop overused downside halting file system activity reaction failure inconvenience recovery takes time requires administrative involvement fix file systems form rstop innocuous read failure occurred simply returning error requesting process entire system stops draconian reactions possibly temporary failures avoided recovery retry underutilized file systems assume failures transient lower layers system handle failures retry requests time systems employ retry generally assume read retry write retry transient faults due device drivers transport issues equally occur reads writes retry applied uniformly ntfs lone file system embraces retry issue higher number requests block failure observed recovery automatic repair rare automatic repair rarely file systems rstop technique file systems require manual intervention attempt fix observed problem running fsck detection recovery redundancy finally importantly virtually file systems include machinery detect disk failures apply redundancy enable recovery failures lone exception minimal amount superblock redundancy found jfs redundancy inconsistently jfs places copies close proximity making vulnerable spatiallylocal errors explored potentially handling failures common drives today investigate inclusion forms redundancy failure policy file system read failure write failure corruption ixt detection j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ixt recovery j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode duplicate acknowledgments suggested literature high level algorithms detect presence reordering dsack increase duplicate threshold dupthresh avoid triggering fast retransmit base implementation blanton allman work limits maximum dupthresh window appears sixth symposium operating systems design implementation osdi false fast retransmits packet delay rate false fast retransmits linux linux dsack ictcp-rr throughput packet delay rate throughput linux dsack ictcp-rr linux figure avoiding false retransmissions ictcp-rr top number false retransmissions bottom throughput vary fraction packets delayed reordered modified nistnet router compare implementations text experimental setup includes single sender receiver bottleneck link set delay nistnet router runs router introducing distributed packet delay standard deviation size timeout occurs sets dupthresh back original implementation user-level library implementation ictcp-rr straight-forward library history acks received list larger kernel exported ack list kernel aggressive pruning size losing potentially valuable information dsack arrives ictcp places sequence number falsely retransmitted packet ack list library consults ack history frequently occurrences found library searches past history measure reordering length sets dupthresh evaluation figure shows effects packet reordering compare implementations stock linux dsack enhancement linux dsack reordering avoidance built kernel user-level ictcp-rr implementation graph show number false fast retransmisretransmitted packets loss rate retransmissions ictcp-efr reno reno ictcp-efr throughput loss rate throughput ictcp-efr reno figure aggressive fast retransmits ictcp-efr top number retransmitted packets reno ictcp-efr due retransmission timeouts fast retransmits bottom achieved bandwidth x-axis vary loss rate mimic wireless lan single sender single receiver bottleneck link set delay sions occur false retransmission caused reordering stock kernel issues false retransmits incorrectly believes reordering actual packet loss graph observe resulting bandwidth dsack in-kernel ictcp-rr versions perform essentially ignoring duplicate acks achieving higher bandwidth ictcp-efr previous case study showed increasing dupthresh contrast environments wireless lans loss common duplicate acks strong signal packet loss window size small case opposite solution desired dupthresh lowered invoking fast retransmit aggressively avoid costly retransmission timeouts overview discuss ictcp-efr user-level library implementation efr efficient fast retransmit observation underlying efr simple appears sixth symposium operating systems design implementation osdi rto sec time eifel rto ictcp user-level karn-patridge ul-kp ul-kp lines spike fixed ictcp-eifel measured rtt rto time self-trained eifel rto rto measured rtt figure adjusting rto ictcp-eifel graph top shows versions ictcp-eifel experiment measured round-trip time identical calculated rto differs line shows karn-partridge rto algorithm disabled kernel implemented user-level ictcp experiment remove lines tcp code added fix rto spike show fix easily provided user-level experiment implement full eifel rto algorithm user-level experiments emulate bandwidth kbps delay queue size graph bottom shows full adaptive eifel rto algorithm bandwidth kbps delay queue size sender adjust dupthresh match number duplicate acks receive implementation ictcp-efr implementation straightforward simplicity modify dupthresh window small efr scheme relevant window small library frequently checks message list duplicate acks sees computes sets dupthresh evaluation figure shows behavior ictcpefr versus in-kernel reno function loss rate emulated wireless network ictcp-efr interprets duplicate acknowledgments signs loss number fast retransmits increases shown graph top importantly number costly retransmission timeouts reduced graph bottom shows bandwidth increases result ictcp-eifel retransmission timeout rto determines time elapse packet sender considers lost retransmits rto prediction upper limit measured round-trip time mrtt correctly setting rto greatly influence performance overly aggressive rto expire prematurely forcing unnecessary spurious retransmission overly-conservative rto long idle times lost packets retransmitted overview eifel rto corrects problems traditional karn-partridge rto immediately mrtt decreases rto incorrectly increased period time rto decay correct magic numbers rto calculation assume low mrtt sampling rate sender load assumptions figure ixt failure policy tables plot incorrect rto incorrectly collapses mrtt implementation implemented eifel rto algorithm user-level library ictcp-eifel library access ictcp variables mrtt ssthresh cwnd mrtt calculates values srtt smoothed round-trip rttvar round-trip variance ictcp-eifel library operates wakes acknowledgment arrives polls ictcp mrtt mrtt changed calculates rto sets ictcp library requires safe control rto evaluation graph figure shows progression improvements ictcp-eifel experiments approximately match eifel rto paper figure implementation disable karn-partridge rto algorithm kernel implement ictcp-eifel expected version incorrectly increases rto mrtt decreases implementation corrects problem additional lines code user-level rto eventually collapses mrtt finally version ictcp-eifel adjusts rto conservative avoids spurious retransmissions graph figure similar figure eifel paper shows implemented full eifel rto algorithm user-level algorithm rto increasingly aggressive spurious timeout occurs point backs conservative summary case studies number strengths ictcp approach ictcp easily enables tcp variants aggressive reno implemented simply efficiently user-level tcp vegas tcp nice push kernel ictcp ideally appears sixth symposium operating systems design implementation osdi suited tuning parameters optimal values depend environment workload dupthresh ictcp correcting errors parameter values behavior rto case studies illustrated limitations ictcp iccm assemble framework shares information flows information shared flows voluntarily congestion state learned previous flows directly inherited flows limitation arises ictcp reliance inkernel tcp stack forcibly set starting congestion state implementing extensions evaluate ability ictcp implement wider range tcp extensions list discussed stp extensions standardized linux sack dsack fack tcp high performance ecn reno syn cookies implemented ictcp rr-tcp vegas nice discuss challenges implementing remaining extensions place extensions categories introduce algorithms existing variables modify packet format modify tcp algorithm structure mechanisms existing variables classify extensions changing behavior existing variables byte counting abc tcp westwood equation-based tcp tfrc recently proposed tcp extensions fall category include fast tcp limited slow-start highspeed tcp extensions natural match ictcp implemented extent aggressive tcp reno equationbased tcp specifies congestion window increase decrease gradually reno ictcpeqn cwnd increase gradually desired forces cwnd decrease usual reno rate conservative implementations extensions beneficial abc implemented ictcp aggressively increase cwnd receiver delays ack ictcp-abc correct ack division case highspeed tcp extension supported manner strictly aggressive cwnd decreased smaller amount tcp reno issue arises extensions ictcp enforces tcp friendliness ictcp constrains tcp virtual variable safe range overly conservative ictcp small increases tcp initial congestion window long time period flows generally considered tcp friendly alternatively stp separate module enforce tcp friendliness module monitors sending rate verifies upper-bound determined state connection packet size loss event rate round-trip time retransmission timeout ictcp similar modular approach equation-based enforcer important drawback non-conforming flows terminated packets buffered bounded size tcp-friendly rate terminate flows ictcp naturally modulates agressive flows manner efficient space time packet format classify extensions changing 
format contents packets extensions put bits tcp reserved field eifel algorithm robust congestion signaling extensions implemented easily ictcp current form compelling expand ictcp variables packet header set difficult ensure safely approximate behavior encapsulating extra information application data requiring sender receiver ictcp-enabled kernel library technique extra information passed protocol stacks remaining transparent applications technique implemented functionality similar dccp implementation user-level library transmits packets udp obtains network information ictcp flow sender receiver investigating approach detail structure mechanism approximately extensions modify fundamental aspects tcp algorithm extensions follow existing tcp states tcp limited transmit define mechanisms sctp checksum extensions deviate substantially base tcp reno algorithm ictcp implement behavior approach addressing limitation modifying packet headers ictcp provide control underneath kernel stack packet filter users exert control packets changing timing ordering altogether suppressing duplicating subset packets pass filter control meted caution ensuring remain tcp friendly central challenge summary ictcp powerful stp implement smaller range tcp extensions appears sixth symposium operating systems design implementation osdi throughput kbps packet delay rate composing ictcp-rr ictcp-vegas ictcp-rr ictcp-vegas ictcp-rr ictcp-vegas reno figure composing ictcp-vegas ictcp-rr figure shows strength composing multiple ictcp libraries environment reordering occurs space bottleneck queue low libraries time environment throughput higher compared libraries experimental setup includes single sender receiver bottleneck queue size set link set delay nistnet router runs router introducing distributed packet delay standard deviation x-axis vary percentage delayed packets simplicity providing ictcp layer real system outweigh drawback ease development final question address complexity ictcp framework develop tcp extensions answer question showing ease user-level libraries ictcp combined perform functionality directly compare complexity building tcp extensions user-level building directly kernel ictcp framework enables functional composition user-level library exports interface ictcp library services stacked build powerful functionality simplest case stacked libraries control disjoint sets ictcp variables ictcp-vegas ictcp-rr libaries stacked combination controls values cwnd dupthresh figure shows advantage stacking libraries flows running environment packet reordering small bottleneck queues exhibit higher throughput libraries libary alternatively stacked libaries control overlapping sets ictcp variables case layer constrains range safe values virtual variable quantify complexity building functionality top ictcp kernel count number statements implementation number semicolons removing printing debugging table shows number case study ictcp native ictcp-vegas ictcp-nice iccm ictcp-rr table ease development ictcp table reports number statements counted number semicolons needed implement case studies ictcp compared native implementation native vegas implementation count entire patch linux tcp nice count statements changing core transport layer algorithm quantifying number needed statements complicated fact authors provide complete linux kernel modifications distributed count transport layer comparison fair functionality iccm count number lines linux calculate amount reordering in-kernel sack dsack ictcp-rr traverses ack list statements required case studies implementations vegas nice comparing ictcp user-level libraries native implementations number statements comparable conclude developing services ictcp complex building natively advantage debugging analysis performed user-level conclusions presented design implementation ictcp slightly modified version linux tcp exposes information control applications userlevel libraries evaluated ictcp axes findings converting tcp stack ictcp requires small amount additional code determining precisely limited virtual parameters place original tcp parameters nontrivial exercise ictcp ten internal tcp variables safely set user-level processes values chosen user resulting flow tcp friendly ictcp incurs minimal additional cpu overhead relative in-kernel implementations long ictcp polled excessively information reduce overhead ictcp processes block acknowledgment arrives end round fourth ictcp enables range tcp extensions implemented user-level found ictcp framework suited extensions implement congestion control algorithms aggressive reno adjusting parameters match workload environment conditions support radical tcp extensions ictcp developed allowing tcp headers safely set packets acknowledgments reordered delayed finally developing tcp extensions appears sixth symposium operating systems design implementation osdi top ictcp complex implementing directly kernel easier debug exposing information control layers network stack similarity tcp sctp sctp extended straight-forward manner icsctp icsctp framework user-level libraries deal problems spurious retransmission implement functionality network failure detection recovery conclusion ictcp powerful proposals extending tcp networking protocols advantage ictcp simplicity pragmatism easy implement ictcp flows built ictcp remain tcp friendly computational overheads reasonable systems ictcp practice theory reap benefits userlevel tcp extensions acknowledgments experiments paper performed exclusively netbed network emulation environment utah greatly indebted robert ticci tim stack leigh stoller kirk webb jay lepreau providing superb environment networking research nitin agrawal lakshmi bairavasundaram nathan burnett vijayan prabhakaran muthian sivathanu helpful discussions comments paper jeffrey mogul excellent shepherding substantially improved content presentation paper finally anonymous reviewers helpful suggestions work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation abbott peterson language-based approach protocol implementation ieee acm transactions networking feb allman tcp congestion control byte counting rfc feb allman balakrishnan floyd enhancing tcp loss recovery limited transmit jan rfc allman floyd patridge increasing tcp initial window rfc internet engineering task force allman paxson stevens tcp congestion control rfc internet engineering task force apr armando caro iyengar amer ladha gerard heinz shah sctp proposed standard robust internet data transport ieee computer november arpaci-dusseau arpaci-dusseau burnett denehy engle gunawi nugent popovici transforming policies mechanisms infokernel sosp balakrishnan rahul seshan integrated congestion management architecture internet hosts sigcomm pages bellardo savage measuring packet reordering proceedings acm usenix internet measurement workshop marseille france nov biagioni structured tcp standard proceedings sigcomm pages london united kingdom aug blanton allman making tcp robust packet reordering acm computer communication review jan blanton allman tcp dsacks sctp duplicate tsns detect spurious retransmissions rfc internet engineering task force february braden tcp tcp extensions transactions rfc internet engineering task force brakmo malley peterson tcp vegas techniques congestion detection avoidance proceedings sigcomm pages london united kingdom aug cardwell bak tcp vegas implementation linux http flophouse neal linux-vegas carson santay nist network emulation tool snad ncsl nist gov nistnet january cheng jin low fast tcp motivation architecture algorithms performance infocom dunigan mathis tierney tcp tuning daemon nov edwards muir experiences implementing high-performance tcp user-space sigcomm pages cambridge massachusetts aug ely savage wetherall alpine user-level infrastructure network protocol development proceedings usenix symposium internet technologies systems usits pages san francisco california mar ely spring wetherall savage robust congestion signaling icnp nov fiuczynski bershad extensible protocol architecture application-specific networking proceedings usenix annual technical conference usenix san diego california jan floyd reno modification tcp fast 
recovery algorithm rfc internet engineering task force floyd highspeed tcp large congestion windows rfc internet engineering task force floyd limited slow-start tcp large congestion windows rfc internet engineering task force floyd handley padhye widmer equationbased congestion control unicast applications proceedings sigcomm pages stockholm sweden aug floyd mahdavi mathis podolsky extension selective acknowledgment sack option tcp rfc internet engineering task force ganger engler kaashoek briceno hunt pinckney fast flexible application-level networking exokernel systems acm tocs feb appears sixth symposium operating systems design implementation osdi isi usc transmission control protocol rfc sept jacobson congestion avoidance control proceedings sigcomm pages stanford california aug jacobson braden borman tcp extensions high performance rfc internet engineering task force iyengar amer heinz two-level threshold recovery mechanism sctp sci karn partridge improving round-trip time estimates reliable transport protocols proceedings sigcomm aug kohler handley floyd designing dccp congestion control reliability icir kohler dcp dccp-icnp pdf kohler kaashoek montgomery readable tcp prolac protocol language proceedings sigcomm pages cambridge massachusetts aug ludwig sklower eifel retransmission timer acm computer communications review july maeda bershad protocol service decomposition high-performance networking proceedings acm symposium operating systems principles sosp pages asheville north carolina dec mahdavi floyd tcp-friendly unicast ratebased flow control end end-interest mailing list http psc networking papers tcp friendly html jan mathis heffner reddy web extended tcp instrumentation acm computer communications review july mathis mahdavi floyd romanow tcp selective acknowledgment options rfc internet engineering task force mogul brakmo lowell subhraveti moore unveiling transport hotnets mogul rashid accetta packet filter efficient mechanism user-level networkcode proceedings acm symposium operating systems principles sosp austin texas november padhye floyd inferring tcp behavior sigcomm pages august patel whitaker wetherall lepreau stack upgrading transport protocols untrusted mobile code sosp paxson allman dawson fenner griner heavens lahey semke volz tcp implementation problems rfc internet engineering task force mar pradhan kandula shaikh nahum daytona user-level tcp stack http nms lcs mit kandula data daytona pdf ramakrishnan floyd black addition explicit congestion notification ecn rfc internet engineering task force seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating systems design implementation osdi pages seattle washington oct stone stewart otis stream control transmission protocol rfc sept tamura tobe tokuda efr retransmit scheme tcp wireless lans ieee conference local area networks pages thekkath nguyen moy lazowska implementing network protocols user level ieee acm transactions networking venkataramani kokku dahlin tcp-nice mechanism background transfers proceedings symposium operating systems design implementation osdi pages boston massachusetts dec von eicken basu buch vogels u-net user-level network interface parallel distributedcomputing proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado dec wallach engler kaashoek ashs application-specific handlers high-performance messaging ieee acm transactions networking aug wang valla sanadidi gerla adaptive bandwidth share estimation tcp westwood ieeeglobecom white lepreau stoller ricci guruprasad newbold hibler barb joglekar integrated experimental environment distributed systems networks proceedings symposium operating systems design implementation osdi pages boston massachusetts dec zhang karp floyd peterson rr-tcp reordering-robust tcp dsack international conference network protocols icnp june 
detection recovery policies ixt read write corruption faults injected block type range workloads workloads varied columns figure block types ixt file system varied rows workloads grouped manner figure key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate dredundancy rredundancy rstop iron file system describe implementation evaluation iron ext ixt ixt implement family recovery techniques commodity file systems provide increase robustness ixt applies checksums metadata data blocks pure replication metadata employs parity-based redundancy protect user data section describe implementation demonstrate robust broad class partial disk failures investigate time space costs ixt showing time costs small modest space costs reasonable performance measurements activate deactivate iron features independently understand cost approach implementation briefly describe ixt implementation explain add checksumming metadata replication user parity performance enhancement transactional checksums existing ext file system framework checksumming implement checksumming ixt borrow techniques recent research checksumming file systems specifically place checksums journal checkpoint checksums final location distant blocks checksum checksums small cached read verification current implementation shato compute checksums incorporating checksumming existing transactional machinery ixt cleanly integrates ext framework metadata replication apply similar approach adding metadata replication ixt metadata blocks written separate replica log checkpointed fixed location block group distant original metadata transactions ensure copies reach disk consistently parity implement simple parity-based redundancy scheme data blocks parity block allocated file simple design enables recover data-block failure file modify inode structure ext associate file parity block data blocks parity blocks allocated files created file modified parity block read updated respect contents improve performance file creates preallocate parity blocks assign files created transactional checksums explore idea leveraging checksums journaling file system specifically checksums relax ordering constraints improve performance updating journal standard ext ensures previous journal data reaches disk commit block enforce ordering standard ext induces extra wait writing commit block incurs extra rotational delay avoid wait ixt implements call transactional checksum checksum contents transaction placing checksum journal commit block ixt safely issue blocks transaction concurrently crash occurs commit recovery procedure reliably detect crash replay transaction checksum journal data match checksum commit block note transactional checksum crash semantics original ext iron extensions cleaning overheads note cleaning overhead large problem pure log-structured file systems major performance issue journaling file systems ixt -style checksumming replication journaling file systems incorporate cleaning on-line maintenance costs ext writes metadata journal cleans journal checkpointing data final fixed location additional cleaning performed ixt increases total traffic small amount evaluation evaluate prototype implementation ixt focus major axes assessment robustness ixt modern disk failures time space overhead additional redundancy mechanisms employed ixt robustness test robustness ixt harness fault injection framework running partial-failure experiments ixt results shown figure ixt detects read failures ext error codes lower level derrorcode metadata block read fails ixt reads replica copy rredundancy replica read fails behaves ext propagating error rpropagate stopping file system activity rstop data block read fails parity block data blocks file read compute failed data block contents rredundancy ixt detects write failures error codes derrorcode aborts journal mounts file system read-only stop writes disk rstop data metadata block read checksum contents computed compared checksum block dredundancy checksums match read error generated rpropagate read errors contents failed block read replica computed parity block rredundancy process building ixt fixed numerous bugs ext avoided cases ext commit failed transactions disk potentially corrupt file system employing checksumming detect corruption replication parity recover lost blocks ixt robust file service spite partial disk failures quantitatively ixt detects recovers partial-error scenarios induced result logical well-defined failure policy time overhead assess performance overhead ixt isolate overhead iron mechanism enabling checksumming metadata data metadata replication parity user data transactional checksumming separately combinations standard file system benchmarks ssh-build unpacks compiles ssh source distribution web server benchmark responds set static http requests postmark emulates file system traffic server tpc-b runs series debit-credit transactions simple database run experiment times present average results benchmarks exhibit broad set behaviors specifically ssh-build good albeit simple model typical action developer administrator web server read intensive concurrency postmark metadata intensive file creations deletions tpc-b induces great deal synchronous update traffic file system table reports relative performance variants ixt 
workloads compared stock linux ext numbers draw main conclusions ssh-build web server workload time overhead iron techniques enabled ssh-build indicative typical activity checksumming replication parity incurs cost similarly web server benchmark conclude read-intensive workloads suffer addition iron techniques ssh web post tpcb baseline ext table overheads ixt file system variants results running variants ixt ssh-build ssh web server web postmark post tpc-b tpcb benchmarks presented ssh-build time measures time unpack configure build ssh source tree tar source size web server benchmark transfers data http requests postmark run transactions file sizes ranging subdirectories files tpc-b run randomly generated debitcredit transactions rows vary redundancy technique implemented combinations implies metadata checksumming enabled data checksumming enabled replication metadata turned parity data blocks enabled transactional checksums results normalized performance standard linux ext interested reader running times standard ext ssh-build web postmark tpc-b seconds slowdowns greater marked bold speedups relative base ext marked brackets testing linux kernel ghz intel memory western digital wdc bbdaa disk metadata intensive workloads postmark tpc-b overhead noticeable postmark tpc-b row workloads metadata intensive results represent worst-case performance expect observe implementation metadata replication row incurs substantial cost data checksumming row user parity metadata checksums contrast incur cost rows untuned implementation ixt results demonstrate worst case costs robustness prohibitive finally performance synchronous tpc-b workload demonstrates benefits transactional checksum base case technique improves standard ext performance row combination parity checksumming replication parity reduces overhead roughly row row additional robustness checksums applied improve performance journaling file systems space overhead evaluate space overhead measured number local file systems computed increase space required metadata replicated room checksums included extra block parity allocated found space overhead checksumming metadata replication small range found parityblock overhead user files bit substantial range depending volume analyzed summary investigated family redundancy techniques found ixt greatly increases robustness file system partial failures incurring modest time space overheads work left designs implementation techniques explored understand benefits costs iron approach related work effort builds related work bodies literature file system analysis related efforts inject faults test robustness systems failure prototype iron file system draws recent efforts building software robust hardware failure discuss turn fault injection robustness testing fault-tolerance community worked years techniques injecting faults system determine robustness fiat simulates occurrence hardware errors altering contents memory registers similarly fine inject software faults operating system major difference previous work approach focuses file systems handle broad class modern disk failure modes previous work approach assumes implicit knowledge file-system block types ensure test paths file system code previous work inserts faults blind fashion uncover problems found work similar brown patterson work raid failure analysis authors suggest hidden policies raid systems worth understanding demonstrate fault injection software raid systems qualitatively failure-handling recovery policies discover failure policy target file system raid requiring complex type-aware approach recent work yang model-checking find host file system bugs techniques well-suited finding classes bugs approach aimed discovery file system failure policy interestingly approach uncovers file system bugs yang reason testing scale model-checking limited small file systems reduce run-time approach applied large file systems work builds earlier work failure injection underneath file systems work developed approach test file systems handle write failures journal updates current work extends data types read write corruption failures iron file systems work iron file systems partially inspired work google acharya suggests cheap hardware paranoid assume fail unpredictable ways google good reason treats application-level problem builds checksumming top file system disk-level redundancy drives machines drive extend approach incorporating techniques file system applications benefit note techniques complimentary application-level approaches file system metadata block inaccessible user-level checksums replicas enable recovery now-corrupted volume related approach driver hardening effort linux stated hardened driver extends realm well-written include professional paranoia features detect hardware software problems page drivers generally improve system reliability faults handled file system end-to-end argument fail-partial failure model disks understood high-end storage high-availability systems communities network appliance introduced row-diagonal parity tolerate disk faults continue operate order ensure recovery presence latent sector errors virtually network appliance products checksumming detect block corruption similarly systems tandem nonstop kernel include end-to-end checksums handle problems misdirected writes interestingly redundancy single disk instances ffs internal replication limited fashion specifically making copies superblock platters drive noted earlier commodity file systems similar provisions suggest making replicas disk raid array reduce rotational latency primary intention copies recovery storage array difficult apply techniques selective manner metadata work replication improving performance fault-tolerance future investigation iron strategies checksumming commonplace improve system security patil stein suggest implement evaluate methods incorporating checksums file systems systems aim make corruption file system data attacker difficult finally dynamic file system sun good file system iron techniques dfs checksums detect block corruption employs redundancy multiple drives ensure recoverability contrast emphasize utility replication drive suggest evaluate techniques implementing redundancy show embellish existing commodity file system dfs written scratch limiting impact conclusions commodity operating systems grown assume presence reliable hardware result case file systems commodity file systems include requisite machinery handle types partial faults expect modern disk drives time reexamine file systems handle failure excellent model operating system kernel networking subsystem network hardware long considered unreliable hardware medium software stacks designed well-defined policies cope common failure modes disks viewed fully reliable mistrust woven storage system framework challenges remain failures disks expose layers file system software architecture redesigned enable consistent well-defined failure policy kind controls exposed applications users low-overhead detection recovery techniques iron file systems employ answers questions lead understanding effectively implement generation robust reliable iron file systems acknowledgments extend steve kleiman network appliance dave anderson jim dykes seagate insights disks work fail liuba shrira shepherd dave dewitt mark hill jiri schindler mike swift anonymous reviewers members adsl excellent suggestions comments himani apte meenali rungta invaluable work implementing parity ext finally computer systems lab csl providing terrific computing environment systems research work sponsored nsf ccrccr- ngsitr- ibm network appliance emc acharya reliability cheap learned stop worrying love cheap pcs easy workshop october altaparmakov linux-ntfs project http linuxntfs sourceforge net ntfs august alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca pages denver colorado anderson drive manufacturers typically don 
talk disk failures personal communication dave anderson seagate anderson dykes riedel interface scsi ata proceedings usenix symposium file storage technologies fast san francisco california april arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance eighth workshop hot topics operating systems hotos viii pages schloss elmau germany bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca pages munich germany june bartlett spainhower commercial fault tolerance tale systems ieee transactions dependable secure computing january barton czeck segall siewiorek fault injection experiments fiat ieee transactions computers april jfs overview ibm developerworks library jfs html bitton gray disk shadowing proceedings international conference large data bases vldb pages los angeles california august brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june candea kawamoto fujiki friedman fox microreboot technique cheap recovery proceedings symposium operating systems design implementation osdi pages san francisco california december chou yang chelf hallem engler empirical study operating system errors proceedings acm symposium operating systems principles sosp pages banff canada october corbett english goel grcanac kleiman leong sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april devale koopman performance evaluation exception handling libraries proceedings international conference dependable systems networks dsngoteborg sweden june douceur bolosky large-scale study file-system contents proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages atlanta georgia dykes modern disk roughly lines code personal communication james dykes seagate august emc emc centera content addressed storage system http emc emerson essays english traits self-reliance harvard classics edited charles eliot york collier son volume foolish consistency hobgoblin minds adored statesmen philosophers divines engler chen hallem chou chelf bugs deviant behavior general approach inferring errors systems code proceedings acm symposium operating systems principles sosp pages banff canada october ghemawat gobioff leung google file system proceedings acm symposium operating systems principles sosp pages bolton landing lake george york october gibson rochberg zelenka nagle amiri chang feinberg gobioff lee ozceri riedel file server scaling network-attached secure disks proceedings joint international conference measurement modeling computer systems sigmetrics performance pages seattle washington june gray census tandem system availability technical report tandem computers green eide controller flaws version http mindprod eideflaw html february kalbarczyk iyer yang characterization linux kernel behavior error proceedings international conference dependable systems networks dsnpages san francisco california june gunawi agrawal arpaci-dusseau arpacidusseau schindler deconstructing commodity storage clusters proceedings annual international symposium computer architecture isca pages madison wisconsin june henson history unix file systems http infohost nmt val slides pdf hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference usenix winter san francisco california january hughes murray reliability security raid storage systems archives sata disk drives acm transactions storage february intel corp ibm corp device driver hardening http hardeneddrivers sourceforge net kari latent sector faults reliability disk arrays phd thesis helsinki technology september kari saikkonen lombardi detection defective media disks ieee international workshop defect fault tolerance vlsi systems pages venice italy october katcher postmark file system benchmark technical report trnetwork appliance october kleiman vnodes architecture multiple file system types sun unix proceedings usenix summer technical conference usenix summer pages atlanta georgia june lewis smart filers dumb disks nsic osd working group meeting april luby mitzenmacher shokrollahi spielman stemann practical loss-resilient codes proceedings twenty-ninth annual acm symposium theory computing stoc pages paso texas lun kao iyer tang fine fault injection monitoring environment tracing unix system behavior faults ieee transactions software engineering pages mckusick joy leffler fabry fast file system unix acm transactions computer systems august mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- department computer science princeton november patil kashyap sivathanu zadok in-kernel integrity checker intrusion detection file system proceedings annual large installation system administration conference lisa atlanta georgia november patterson brown broadwell candea chen cutler enriquez fox kiciman merzbacher oppenheimer sastry tetzlaff traupman treuhaft recovery oriented computing roc motivation definition techniques case studies technical report csd- berkeley march patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june postel rfc transmission control protocol september ftp ftp rfc-editor in-notes rfc txt august prabhakaran arpaci-dusseau arpaci-dusseau model-based failure analysis journaling file systems proceedings international conference dependable systems networks dsnyokohama japan june redell dalal horsley lauer lynch mcjones murray purcell pilot operating system personal computer communications acm february reiser reiserfs namesys ridge field book scsi starch june rinard cadar dumitran roy leu william beebe enhancing server availability security failure-oblivious computing proceedings symposium operating systems design implementation osdi san francisco california december rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saltzer reed clark end-to-end arguments system design acm transactions computer systems november schindler experienced severe performance degradation identified problem disk firmware disk drives reprogrammed fix problem personal communication schindler emc july schlosser ganger mems-based storage devices standard disk interfaces square peg round hole proceedings usenix symposium file storage technologies fast pages san francisco california april schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december schwarz xin miller long hospodor disk scrubbing large archival storage systems proceedings annual meeting ieee international symposium modeling analysis simulation computer telecommunication systems mascots volendam netherlands october seltzer bostic mckusick staelin implementation log-structured file system unix proceedings usenix winter technical conference usenix winter pages san diego california january siewiorek hudak suh segal development benchmark measure system robustness proceedings international symposium fault-tolerant computing ftcstoulouse france june sivathanu bairavasundaram arpaci-dusseau arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid proceedings usenix symposium file storage technologies fast pages san francisco california april sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california 
april solomon inside windows microsoft programming series microsoft press edition stein howard seltzer unifying file system protection proceedings usenix annual technical conference usenix boston massachusetts june sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january swift bershad levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing lake george york october talagala patterson analysis error behaviour large storage system ieee workshop fault tolerance parallel distributed systems san juan puerto rico april data clinic hard disk failure http dataclinic harddisk-failures htm transaction processing council tpc benchmark standard specification revision technical report tsai iyer measuring fault tolerance ftape fault injection tool international conference modeling techniques tools computer performance evaluation pages september tweedie journaling linux ext file system fourth annual linux expo durham north carolina wehman den haan enhanced ide fast-ata faq http thef-nym sci kun cgi-pieterh atazip atafq html weinberg solaris dynamic file system http members visi net thedave sun dynfs pdf wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february yang twohey engler musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october 
analysis evolution journaling file systems vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison vijayan dusseau remzi wisc abstract develop apply methods analyzing file system behavior evaluating file system semantic block-level analysis sba combines knowledge on-disk data structures trace disk traffic infer file system behavior contrast standard benchmarking approaches sba enables users understand file system behaves semantic trace playback stp enables traces disk traffic easily modified represent file system implementation contrast directly modifying file system stp enables users rapidly gauge benefits policies sba analyze linux ext reiserfs jfs windows ntfs process uncover strengths weaknesses journaling file systems apply stp evaluate modifications ext demonstrating benefits optimizations incurring costs real implementation introduction modern file systems journaling file systems writing information pending updates write-ahead log committing updates disk journaling enables fast file system recovery crash basic techniques existed years cedar episode journaling increased popularity importance recent years due ever-increasing disk capacities scan-based recovery fsck prohibitively slow modern drives raid volumes popularity importance journaling file systems ext reiserfs jfs ntfs internal policies understanding file systems behave important developers administrators application writers time perform detailed analysis journaling file systems previous work analyzed file systems writing userlevel programs measuring time file system operations elicit salient aspects file system performance difficult discover underlying reasons observed performance approach paper employ benchmarking methodology called semantic block-level analysis sba trace analyze file systems sba induce controlled workload patterns file system focus analysis time operations resulting stream read write requests file system analysis semantic leverage information block type block request journal inode analysis block-level interposes block interface storage analyzing low-level block stream semantically meaningful understand file system behaves analysis hints file system improved reveal change worth implementing traditionally potential improvement file system implement change measure performance workloads change improvement implementation effort wasted paper introduce apply complementary technique sba called semantic trace playback stp stp enables rapidly suggest evaluate file system modifications large implementation simulation effort real workloads traces show stp effectively applied detailed analysis linux ext reiserfs preliminary analysis linux jfs windows ntfs case focus journaling aspects file system determine events data metadata written journal fixed locations examine characteristics workload configuration parameters size journal values commit timers impact behavior analysis uncovered design flaws performance problems correctness bugs file systems ext reiserfs make design decision group unrelated traffic compound transaction result tangled synchrony single disk-intensive process forces write traffic disk affecting performance asynchronous writers find ext reiserfs artificially limit parallelism preventing overlap pre-commit journal writes fixed-place updates analysis reveals ordered data journaling modes ext exhibits eager writing forcing data blocks disk sooner typical -second delay addition find jfs proceedings usenix annual technical conference april anaheim infinite write delay utilize commit timers indefinitely postpones journal writes trigger forces writes occur memory pressure finally identify previously unknown bugs reiserfs fixed subsequent releases main contributions paper methodology semantic block analysis sba understanding internal behavior file systems methodology semantic trace playback stp rapidly gauging benefits file system modifications heavy implementation effort detailed analysis sba important journaling file systems ext reiserfs preliminary analysis jfs ntfs evaluation stp design implementation alternatives ext rest paper organized describe techniques sba stp apply techniques ext reiserfs jfs ntfs discuss related work conclude methodology introduce techniques evaluating file systems semantic block analysis sba enables users understand internal behavior policies file system semantic trace playback stp users quantify changing file system impact performance real workloads semantic block-level analysis file systems traditionally evaluated approaches applies synthetic real workloads measures resulting file system performance collects traces understand file systems performing isolation misses interesting opportunity correlating observed disk traffic running workload performance answer workload behaves block-level tracing disk traffic analyze number interesting properties file system workload coarsest granularity record quantity disk traffic divided reads writes information understanding file system caching write buffering affect performance detailed level track block number block read written analyzing block numbers extent traffic sequential random finally analyze timing block timing information understand file system initiates burst traffic combining block-level analysis semantic information blocks infer ext reiserfs jfs ntfs sba generic sba specific sba total table code size sba drivers number statements counted number semicolons needed implement sba ext reiserfs preliminary sba jfs ntfs behavior file system main difference semantic block analysis sba standard block-level tracing sba analysis understands on-disk format file system test sba enables understand properties file system sba distinguish traffic journal versus in-place data track individual transactions journal implementation infrastructure performing sba straightforward places pseudo-device driver kernel associates underlying disk mounts file system interest ext pseudo device refer sba driver runs controlled microbenchmarks generate disk traffic sba driver passes traffic disk efficiently tracks request response storing small record fixed-sized circular buffer note tracking ordering requests responses pseudo-device driver infer order requests scheduled lower levels system sba requires interpret contents disk block traffic interpret contents journal infer type journal block descriptor commit block interpret journal descriptor block data blocks journaled result efficient semantically interpret block-level traces on-line performing analysis off-line require exporting contents blocks greatly inflating size trace sba driver customized file system test concern amount information embedded sba driver file system focus paper understanding journaling file systems sba drivers embedded information interpret placement contents journal blocks metadata data blocks analyze complexity sba driver journaling file systems ext reiserfs jfs ntfs journaling file systems journal transactions temporarily recorded fixed-location data structures data permanently reside sba driver distinguishes traffic journal fixed-location data structures traffic simple distinguish reiserfs jfs ntfs journal set contiguous blocks separate rest file system backward proceedings usenix annual technical conference april anaheim compatible ext ext treat journal regular file determine blocks belong journal sba knowledge inodes indirect blocks journal change location created classification remains efficient run-time sba classify types journal blocks descriptor block journal data block commit block perform analysis journaling file systems sba driver understand details file system driver understand directory blocks superblock ext tree structure reiserfs jfs wishes infer additional file system properties embed sba driver knowledge sba driver policies parameters file system fact sba infer policies parameters table reports number statements required implement sba driver numbers show code sba driver statements general infrastructure approximately 
statements needed support journaling file systems ext specific code file systems ext journal created file span multiple block groups order find blocks belonging journal file parse journal inode journal indirect blocks reiserfs jfs ntfs journal contiguous finding blocks trivial journal file ntfs small journals contiguously allocated workloads sba analysis gather information workload focus paper understanding internal policies behavior file system result construct synthetic workloads uncover decisions made file system realistic workloads considered apply semantic trace playback constructing synthetic workloads stress file system previous research revealed range parameters impact performance created synthetic workloads varying parameters amount data written sequential versus random accesses interval calls fsync amount concurrency focus exclusively write-based workloads reads directed fixed-place location impact journal analyze file system report results workloads revealed file system policies parameters overhead sba processing memory overheads sba minimal workloads ran generate high rates request sba driver performs operations collect detailed traces gettimeofday call start end block number comparison block journal fixed-location block check magic number journal blocks distinguish journal metadata journal data sba stores trace records details read write block number block type time issue completion internal circular buffer operations performed detailed traces analyses sufficient cumulative statistics total number journal writes fixedlocation writes numbers easy collect require processing sba driver alternative approaches directly instrumenting file system obtain timing information disk traces equivalent superior performing sba analysis case reasons directly instrument file system source code file system re-instrument versions released contrast sba analysis require file system source sba driver code reused file systems versions directly instrumenting file system accidentally miss conditions disk blocks written sba driver guaranteed disk traffic finally instrumenting existing code accidentally change behavior code efficient sba driver impact file system behavior semantic trace playback section describe semantic trace playback stp stp rapidly evaluate kinds file system designs heavy implementation investment detailed file system simulator describe stp functions stp built user-level process takes input trace parses issues requests disk raw disk interface multiple threads employed concurrency ideally stp function taking blocklevel trace input generated sba driver sufficient types file system modifications straightforward model layout schemes simply mapping blocks on-disk locations desire enable powerful emulations stp issue explore effect byte differences journal storing entire blocks complication arises changing contents journal proceedings usenix annual technical conference april anaheim timing block thresholds initiate triggered time handle emulations alter timing disk information needed readily low-level block trace specifically stp observe high-level activities stp observe file-system level operations create dirty buffers memory reason requirement found number uncommitted buffers reaches threshold ext journal size commit enacted similarly interval timers expires blocks flushed disk stp observe application-level calls fsync stp understand operation sba trace due fsync call due normal file system behavior thresholds crossed timers differentiation stp emulate behaviors timing sensitive requirements met giving filesystem level trace input stp addition sbagenerated block-level trace library-level interpositioning trace application interest qualitatively compare stp standard approaches file system evolution approach idea improving file system simply implements idea file system measures performance real system approach attractive reliable answer idea real improvement assuming workload applied relevant time consuming modification file system non-trivial approach builds accurate simulation file system evaluates idea domain file system migrating real system approach attractive avoid details building real implementation quickly understand idea good requires detailed accurate simulator construction maintenance challenging endeavor stp avoids difficulties approaches low-level traces truth file system behaves modifying file system output block stream based simple internal models file system behavior models based empirical analysis found advantages traditional implementation simulation stp limited important ways stp suited evaluating design alternatives simpler benchmarks workload exhibits complex virtual memory behavior interactions file system modeled results meaningful stp limited evaluating file system radical basic operation file system remain intact finally stp provide means evaluate implement change understand modification improves performance environment measurements machine running linux mhz pentium iii processor main memory file system test created single ibm lzx disk separate root disk data point reports average trials cases variance low ext file system section analyze popular linux filesystem ext begin giving overview ext apply semantic block-level analysis semantic trace playback understand internal behavior background linux ext journaling file system built extension ext file system ext data metadata eventually standard ext structures fixed-location structures organization loosely based ffs disk split number block groups block group bitmaps inode blocks data blocks ext journal log commonly stored file file system stored separate device partition figure depicts ext on-disk layout information pending file system updates written journal forcing journal updates disk updating complex file system structures writeahead logging technique enables efficient crash recovery simple scan journal redo incomplete committed operations bring file system consistent state normal operation journal treated circular buffer information propagated fixed location ext structures journal space reclaimed journaling modes linux ext includes flavors journaling writeback mode ordered mode data journaling mode figure illustrates differences modes choice mode made mount time changed remount writeback mode file system metadata journaled data blocks written directly fixed location mode enforce ordering journal fixed-location data writes writeback mode weakest consistency semantics modes guarantees consistent file system metadata provide guarantee consistency data blocks ordered journaling mode metadata writes journaled data writes fixed location ordered journal writes metadata proceedings usenix annual technical conference april anaheim jcib inode groupscylinder group journal descriptor block inode bitmap journal commit blockdb data bitmap journal superblock figure ext on-disk layout picture shows layout ext file system disk address space broken series block groups akin ffs cylinder groups bitmaps track allocations regions inodes data blocks ext journal depicted file block group file system superblock descriptor blocks describe contents commit blocks denote ends transactions journal commit journal inode sync fixed data fixed data fixed inode fixed data journal inode sync journal commit fixed data sync fixed inode sync writeback mode data write happen time writeback ordered journal inode data fixed inode data journal commit data journal write journal commit checkpoint write figure ext journaling modes diagram depicts journaling modes ext writeback ordered data diagram time flows downward boxes represent updates file system journal inode implies write inode journal destination writes labeled fixed write fixed in-place ext structures 
arrow labeled sync implies blocks written succession synchronously guaranteeing completes curved arrow ordering succession write happen time finally writeback mode dashed box fixed data block happen time sequence data block write inode updates propagated file system diagrams show data flow ext journaling modes contrast writeback mode mode consistency semantics data metadata guaranteed consistent recovery full data journaling mode ext logs metadata data journal decision implies process writes data block typically written disk journal fixed ext location data journaling mode strong consistency guarantees ordered journaling mode performance characteristics cases worse surprisingly cases explore topic transactions file system update separate transaction ext groups updates single compound transaction periodically committed disk approach simple implement compound transactions performance fine-grained transactions structure frequently updated short period time free space bitmap inode file constantly extended journal structure ext additional metadata structures track list journaled blocks journal superblock tracks summary information journal block size head tail pointers journal descriptor block marks beginning transaction describes subsequent journaled blocks including final fixed on-disk location data journaling mode descriptor block data metadata blocks ordered writeback mode descriptor block metadata blocks modes ext logs full blocks opposed differences versions single bit change bitmap results entire bitmap block logged depending size transaction multiple descriptor blocks data metadata blocks logged finally journal commit block written journal end transaction commit block written journaled data recovered loss checkpointing process writing journaled metadata data fixed-locations checkpointing checkpointing triggered thresholds crossed file system buffer space low free space left journal timer expires crash recovery crash recovery straightforward ext journaling file systems basic form redo logging updates data metadata written log process restoring in-place file system structures easy recovery file system scans log committed complete transactions incomplete transactions discarded update completed transaction simply replayed fixed-place ext structures analysis ext sba perform detailed analysis ext sba framework analysis divided categories analyze basic behavior ext function workload journaling modes isolate factors control data committed journal isolate factors control data checkpointed fixed-place location proceedings usenix annual technical conference april anaheim bandwidth amount data written bandwidth data ordered writeback ext journal data amount data written amount journal writes data ordered writeback ext fixed-location data amount data written amount fixed-location writes data ordered writeback ext figure basic behavior sequential workloads ext graph evaluate ext ext journaling modes increase size written file x-axis workload writes single file sequentially performs fsync graph examines metric top graph shows achieved bandwidth middle graph sba report amount journal traffic bottom graph sba report amount fixed-location traffic journal size set basic behavior modes workload begin analyzing basic behavior ext function workload journaling mode writeback ordered full data journaling goal understand workload conditions trigger ext write data metadata journal fixed locations explored range workloads varying amount data written sequentiality writes synchronization interval writes number concurrent writers sequential random workloads begin showing results basic workloads workload writes single file sequentially performs fsync flush data disk figure workload issues writes random locations single file calls fsync writes figure workload issues random writes calls fsync write figure workload increase total amount data bandwidth amount data written random write bandwidth data ordered writeback ext journal data amount data written amount journal writes data ordered writeback ext fixed-location data amount data written amount fixed-location writes data ordered writeback ext figure basic behavior random workloads ext figure similar figure workload issues writes random locations single file calls fsynconce writes top graph shows bandwidth middle graph shows journal traffic bottom graph reports fixed-location traffic journal size set writes observe behavior ext top graphs figures plot achieved bandwidth workloads graph compare journaling modes ext bandwidth graphs make observations achieved bandwidth extremely sensitive workload expected sequential workload achieves higher throughput random workload calling fsync frequently reduces throughput random workloads sequential traffic ext performs slightly highest performing ext mode small noticeable cost journaling sequential streams workloads ordered mode writeback mode achieve bandwidths similar ext finally performance data journaling irregular varying sawtooth pattern amount data written graphs file system throughput compare performance workloads journaling modes enable infer differences infer internal behavior file system apply semantic analysis underlying block stream proceedings usenix annual technical conference april anaheim bandwidth amount data written random write bandwidth data ordered writeback ext journal data amount data written amount journal writes data ordered writeback ext fixed-location data amount data written amount fixed-location writes data ordered writeback ext figure basic behavior random workloads ext figure similar figure workload issues random writes calls fsync write bandwidth shown graph journal writes fixed-location writes reported graph sba journal size set record amount journal fixedlocation traffic accounting shown bottom graphs figures row graphs figures quantify amount traffic flushed journal infer events traffic data journaling mode total amount data written journal high proportional amount data written application expected data metadata journaled modes metadata journaled amount traffic journal small row figures shows traffic fixed location writeback ordered mode amount traffic written fixed location equal amount data written application data journaling mode observe stair-stepped pattern amount data written fixed location file size process called fsync force data disk data written fixed location time application terminates data logged expected consistency semantics preserved consistency application writes data checkpointing occur regular intervals extra traffic leads sawtooth bandwidth measured graph experiment sequential traffic journal size checkpoint occurs data written explore relationship checkpoints journal size carefully sba graphs reveal data journaling mode performs modes asynchronous random writes data journaling mode data written log random writes logically sequential achieve sequential bandwidth journal filled checkpointing extra disk traffic reduces bandwidth experiment checkpointing occurs finally sba analysis reveals synchronous writes perform data journaling mode forcing small write log logical sequence incurs delay sequential writes shown write incurs disk rotation concurrency report results running workloads multiple processes construct workload diverse classes traffic asynchronous foreground process competition background process foreground process writes file calling fsync background process repeatedly writes block random location optionally calls fsync sleeps period time sync interval focus data journaling mode effect holds ordered journaling mode shown figure show impact varying sync interval background process performance foreground process graph plots bandwidth achieved foreground asynchronous process depending competes asynchronous synchronous background process expected 
foreground process runs asynchronous background process bandwidth uniformly high matches in-memory speeds foreground process competes synchronous background process bandwidth drops disk speeds sba analysis graph reports amount journal data revealing frequently background process calls fsync traffic journal fact amount journal traffic equal sum foreground background process traffic written interval background process effect due implementation compound transactions ext file system updates add global transaction eventually committed disk workload reveals potentially disastrous consequences grouping unrelated updates comproceedings usenix annual technical conference april anaheim bandwidth sync interval milliseconds bandwidth background process call fsync background process calling fsync journal data sync interval milliseconds amount journal writes background process call fsync background process calls fsync figure basic behavior concurrent writes ext processes compete workload foreground process writing sequential file size background process writing optionally calling fsync sleeping sync interval repeating x-axis increase sync interval top graph plot bandwidth achieved foreground process scenarios background process calling calling fsync write bottom graph amount data written disk sets experiments shown pound transaction traffic committed disk rate asynchronous traffic wait synchronous updates complete refer negative effect tangled synchrony explore benefits untangling transactions stp journal commit policy explore conditions ext commits transactions on-disk journal factors influence event size journal settings commit timers experiments focus data journaling mode mode writes metadata data journal traffic journal easily mode writeback ordered modes commit transactions policies exercise log commits examine workloads data explicitly forced disk application process call fsync minimize amount metadata overhead write single file impact journal size size journal configurable parameter ext contributes updates committed varying size journal amount data written workload infer amount data triggers log commit figure shows resulting bandwidth amount journal traffic function file size journal size graph shows amount data writbandwidth amount data written bandwidth journal size journal size journal size journal size journal data amount data written amount journal writes journal size journal size figure impact journal size commit policy ext topmost figure plots bandwidth data journaling mode different-sized file writes lines plotted representing journal sizes graph shows amount log traffic generated experiments clarity journal sizes shown ten application precise number dirty uncommitted buffers includes data metadata reaches size journal bandwidth drops considerably fact performance regime observed bandwidth equal in-memory speeds semantic analysis shown graph reports amount traffic journal graph reveals metadata data forced journal equal journal size inspection linux ext code confirms threshold note threshold ordered writeback modes shown triggered frequently metadata logged impact timers linux ext timers control data written metadata commit timer data commit timer managed kupdate daemon commit timer managed kjournal daemon system-wide kupdate daemon responsible flushing dirty buffers disk kjournal daemon specialized ext responsible committing ext transactions strategy ext flush metadata frequently seconds delaying data writes longer time seconds flushing metadata frequently advantage file system approach ffs-like consistency severe performance penalty delaying data writes advantage files deleted quickly tax disk mapping ext goals ext timers leads default values seconds kupdate metadata timer seconds kjournal timer proceedings usenix annual technical conference april anaheim journal write time seconds kupdated metadata timer seconds sensitivity kupdated metadata timer journal write time seconds kupdated data timer seconds sensitivity kupdated data timer journal write time seconds kjournald timer seconds sensitivity kjournald timer figure impact timers commit policy ext graph timer varied x-axis time write journal recorded y-axis measuring impact timer set timers seconds journal size affect measurements seconds kupdate data timer measure timers affect transactions committed journal ensure specific timer influences journal commits set journal size sufficiently large set timers large analysis observe write appears journal figure plots results varying timers x-axis plotting time log write occurs y-axis graph graph show kupdate daemon metadata commit timer kjournal daemon commit timer control timing log writes data points log write occurred precisely timer expired traffic log minimum timers graph shows kupdate daemon data timer influence timing log writes data points correlated x-axis timer influences data written fixed location interaction journal fixed-location traffic timing writes journal fixedrequest queue blocks time seconds write ordering ext fixed location journal figure interaction journal fixed-location traffic ext figure plots number outstanding writes journal fixed-location disks experiment run processes issues random synchronous writes file system journal running ordered mode journal configured run separate disk location data managed carefully consistency fact difference writeback ordered mode timing writeback mode enforce ordering ordered mode ensures data written fixed location commit block transaction written journal performed sba analysis found performance deficiency ordered mode implemented workload synchronously writes large number random blocks sba driver separate journal fixed-location data figure plots number concurrent writes data type time figure shows writes journal fixed-place data overlap specifically ext issues data writes fixed location waits completion issues journal writes journal waits completion finally issues final commit block waits completion observe behavior irrespective journal separate device device file system inspection ext code confirms observation wait needed correctness cases journal configured separate device extra wait severely limit concurrency performance ext falsely limited parallelism stp fix timing problem checkpoint policy turn attention checkpointing process writing data fixed location ext structures show checkpointing ext function journal size commit timers synchronization interval workload focus data journaling mode sensitive journal size understand checkpointing occurs construct workloads periodically force data journal call fsync observe data subsequently written fixed location impact journal size figure shows sba results proceedings usenix annual technical conference april anaheim fixed location data amount data written amount fixed location writes sync size sync size sync size free space amount data written checkpointing sync size sync size sync size journal size journal size figure impact journal size checkpoint policy ext workload amount data x-axis written sequentially fsync issued graph sba plot amount fixed-location traffic graph sba plot amount free space journal function file size synchronization interval single journal size graph shows amount data written fixed ext location end experiment point checkpointing occurs varies sync intervals sync interval data forced disk worth writes checkpoints occur approximately committed log sync interval checkpoints occur illustrate triggers checkpoint graph plot amount journal free space immediately preceding checkpoint correlating graphs checkpointing occurs amount free space -th -th journal size precise fractiondepends synchronization interval smaller sync 
amounts checkpointing postponed free space journal confirmed relationship journal sizes shown impact timers examine system timers impact timing checkpoint writes fixed locathe exact amount free space triggers checkpoint straightforward derive reasons ext reserves amount journal space overhead descriptor commit blocks ext reserves space journal committing transaction synchronization interval derived free space function precisely feel detailed information enlightening simply checkpointing occurs free space -th -th journal size write time seconds kupdated data timer seconds sensitivity kupdated data timer log writes fixed-location writes figure impact timers checkpoint policy ext figure plots relationship time data written log checkpointed dependent kupdate data timer scatter plot shows results multiple runs process running writes data fsync data journaling mode timers set seconds journal size tions workload vary kupdate data timer setting timers seconds figure shows kupdate data timer impacts data written fixed location previously figure log updated timers expire checkpoint write occurs amount kupdate data timer granularity experiments shown reveal granularity controlled kupdate metadata timer analysis reveals ext timers lead timing data metadata traffic ext ordered data journaling modes force data disk time metadata writes data metadata flushed disk frequently timing behavior largest potential performance differentiator ordered writeback modes interestingly frequent flushing potential advantage forcing data disk timely manner large disk queues avoided performance improved disadvantage early flushing temporary files written disk subsequent deletion increasing load system summary ext sba isolated number features ext strong impact performance journaling mode delivers performance depends strongly workload random workloads perform logging relationship size journal amount data written application larger impact performance ext implements compound transactions unrelated concurrent updates transaction result tangled synchrony traffic transaction committed disk rate results disastrous performance asynchronous traffic combined synchronous traffic proceedings usenix annual technical conference april anaheim bandwidth file number bandwidth ordered journaling mode default ext journal beginning modified ext journal middle stp journal middle figure improved journal placement stp compare placements journal beginning partition ext default modeled middle file system stp middle file system files created file system file chosen number x-axis workload issues synchronous writes file ordered mode ext overlap writes journal fixed-place data specifically ext issues data writes fixed location waits completion issues journal writes journal waits completion finally issues final commit block waits completion wait needed correctness journal separate device falsely limited parallelism harm performance ordered data journaling modes timer flushes meta-data disk data flushed disadvantage eager writing temporary files written disk increasing load evolving ext stp section apply stp wider range workloads traces evaluate modifications ext demonstrate accuracy stp approach begin simple modification varies placement journal sba analysis pointed number improvements ext quantify stp journaling modes depending workload separate transactions update overlapping pre-commit journal writes data updates ordered mode finally stp evaluate differential journaling block differences written journal journal location experiment stp quantifies impact changing simple policy placement journal default ext creates journal regular file beginning partition start policy validate stp results obtain stp similar implement change ext construct workload stresses placement journal partition filled files benchmark process issues random synchronous bandwidth sync interval milliseconds bandwidth untangled standard figure untangling transaction groups stp experiment identical figure addition show performance foreground process untangled transactions emulated stp writes chosen file figure vary file chosen x-axis line graph shows performance ordered mode default ext bandwidth drops file located journal sba analysis shown confirms performance drop occurs seek distance increases writes file journal evaluate benefit placing journal middle disk stp remap blocks validation coerce ext allocate journal middle disk compare results figure shows stp predicted performance identical version ext worst-case behavior avoided placing journal middle file system beginning longest seeks entire volume avoided synchronous workloads workloads frequently seek journal ext structures journaling mode shown workloads perform journaling modes random writes perform data journaling mode random writes written sequentially journal large sequential writes perform ordered mode avoids extra traffic generated data journaling mode journaling mode ext set mount time remains fixed mount stp evaluate adaptive journaling mode chooses journaling mode transaction writes transaction transaction sequential ordered journaling data journaling demonstrate potential performance benefits adaptive journaling run portion trace labs removing inter-arrival times calls compare ordered mode data journaling mode adaptive approach trace completes seconds seconds ordered data journaling modes stp adaptive journaling trace completes seconds trace sequential random write proceedings usenix annual technical conference april anaheim request queue blocks time seconds modified write ordering fixed location journal figure changing interaction journal fixedlocation traffic stp experiment run figure run stp issue precommit journal writes data writes concurrently plot stp emulated performance made change ext directly obtaining resultant performance phases adaptive journaling performs single-mode approach transaction grouping linux ext groups updates system-wide compound transactions commits disk periodically shown single update stream synchronous dramatic impact performance asynchronous streams transforming in-memory updates disk-bound stp show performance file system untangles traffic streams forcing process issues fsync commit data disk figure plots performance asynchronous sequential stream presence random synchronous stream vary interval updates synchronous process graph segregated transaction grouping effective asynchronous stream unaffected synchronous traffic timing show stp quantify cost falsely limited parallelism discovered pre-commit journal writes overlapped data updates ordered mode stp modify timing journal fixed-location writes initiated simultaneously commit transaction written previous writes complete workload processes issuing random synchronous writes journal separate disk figure shows stp model implementation change modifying timing requests workload stp predicts improvement prediction matches achieve ext changed directly expected increasing amount concurrency improves performance journal separate device journal contents ext physical logging writes blocks entirety log blocks journaled irrespective bytes changed block journal space fills quickly increasing commit checkpoint frequency stp investigate differential journaling file system writes block differences journal blocks entirety approach potentially reduce disk traffic noticeably dirty blocks substantially previous versions focus data journaling mode generates journal traffic differential journaling modes evaluate differential journaling matters real workloads analyze sba traces underneath database workloads modeled tpc-b tpcc simple application-level implementation debit-credit benchmark realistic implementation order-entry built top postgres data journaling mode amount data written journal reduced factor tpc-b factor tpc-c contrast ordered writeback modes difference minimal modes metadata written log applying differential journaling metadata blocks makes difference total volume reiserfs focus linux journaling 
filesystem reiserfs section focus chief differences ext reiserfs due time constraints stp explore reiserfs background general behavior reiserfs similar ext file systems journaling modes compound transactions reiserfs differs ext primary ways file systems on-disk structures track fixed-location data ext structures ext improved scalability reiserfs tree data stored leaves tree metadata stored internal nodes impact fixed-location data structures focus paper difference largely irrelevant format journal slightly ext journal file partition contiguous reiserfs journal file contiguous sequence blocks beginning file system ext reiserfs journal put device reiserfs limits journal maximum ext reiserfs differ slightly journal contents reiserfs fixed locations blocks transaction stored descriptor block commit block unlike ext reiserfs descriptor block compound proceedings usenix annual technical conference april anaheim bandwidth amount data written bandwidth data ordered writeback journal data amount data written amount journal writes data ordered writeback fixed-location data amount data written amount fixed-location writes data ordered writeback figure basic behavior sequential workloads reiserfs graph evaluate reiserfs journaling modes single workload size sequentially written file increased x-axis graph examines metric hows achieved bandwidth sba report amount journal traffic sba report amount fixed-location traffic journal size set transaction limits number blocks grouped transaction semantic analysis reiserfs performed identical experiments reiserfs ext due space constraints present results reveal significantly behavior file systems basic behavior modes workload qualitatively performance journaling modes reiserfs similar ext random workloads infrequent synchronization perform data journaling sequential workloads generally perform random writeback ordered modes generally perform data journaling reiserfs groups concurrent transactions single compound transaction ext primary difference file systems occurs sequential workloads data journaling shown graph figure fixed-location data amount data written amount fixed-location writes sync size sync size sync size sync size fixed location data number transactions amount fixed-location writes sync size sync size sync size figure impact journal size transactions checkpoint policy reiserfs workloads data sequentially written anfsyncis issued amount data sba report amount fixedlocation traffic graph vary amount data written graph vary number transactions defined number calls fsync throughput data journaling mode reiserfs follow sawtooth pattern initial reason found sba analysis graphs figure data written journal checkpointed inplace location reiserfs appears checkpoint data aggressively ext explore journal commit policy explore factors impact reiserfs commits transactions log focus data journaling sensitive postpone exploring impact timers previously ext commits data log approximately log filled timer expires running workload force data disk call fsync reiserfs performing sba analysis find reiserfs threshold depending journal size reiserfs commits data blocks blocks written reiserfs limits journal size fixed thresholds sufficient finally note reiserfs falsely limited parallelism ordered mode ext reiserfs forces data flushed fixed location issues writes journal proceedings usenix annual technical conference april anaheim write time seconds kreiserfsd timer seconds sensitivity kreiserfsd journal timer log writes fixed-location writes figure impact timers reiserfs figure plots relationship time data written kreiserfs timer scatter plot shows results multiple runs process running writes data fsync data journaling mode timers set seconds journal size checkpoint policy investigate conditions trigger reiserfs checkpoint data fixed-place location policy complex reiserfs ext found data checkpointed journal full reiserfs point data checkpointed depends free space journal number concurrent transactions workloads periodically force data journal calling fsync intervals results shown figure graph shows amount data checkpointed function amount data written cases data checkpointed journal filled graph shows amount data checkpointed function number transactions graph shows data checkpointed intervals transactions running similar workload ext reveals relationship number transactions checkpointing reiserfs checkpoints data journal free space drops transactions journal ext timers control data written journal fixed locations differences ext kjournal daemon responsible committing transactions reiserfs kreiserfs daemon role figure shows time data written journal fixed location kreiserfs timer increased make conclusions log writes occur seconds data write application timer fixed-location writes occur elapsed time greater seconds multiple kreiserfs timer reiserfs timer policy simpler ext finding bugs sba analysis inferring policies filesystems finding cases implemented correctly sba analysis found number problems reiserfs implementation reported case identified problem sba driver observe disk traffic expected verify problems examined code find suggested fixes reiserfs developers transaction mount fsync call returns data written tracked aberrant behavior incorrect initialization file block overwritten writeback mode stat information updated error occurs due failure update inode transaction information committing transactions dirty data flushed tracked erroneously applying condition prevent data flushing journal replay irrespective changing journal thread wake interval dirty data flushed problem occurs due simple coding error ibm journaled file system section describe experience performing preliminary sba analysis journaled file system jfs began rudimentary understanding jfs obtain documentation knew journal located default end partition treated contiguous sequence blocks journaling mode due fact knew file system began found needed apply analysis technique cases filtered traffic rebooted system infer filtered traffic consistency technique understand journaling mode jfs basic starting point examining jfs code learn number interesting properties jfs inferred jfs ordered journaling mode due small amount traffic journal obvious employing data journaling differentiate writeback ordered modes observed ordering writes matched ordered mode data block written application jfs orders write data block written successfully metadata writes issued determined jfs logging record level inode index tree directory tree structure structure logged entire block structure result jfs writes fewer journal blocks ext reiserfs operations jfs default group concurrent updates single compound transaction running experiment performed figure proceedings usenix annual technical conference april anaheim bandwidth asynchronous traffic high irrespective synchronous traffic background circumstances transactions grouped write commit records log page finally commit timers jfs fixedlocation writes happen kupdate daemon timer expires journal writes triggered timer journal writes indefinitely postponed trigger memory pressure unmount operation infinite write delay limits reliability crash result data loss data written minutes hours windows ntfs section explain analysis ntfs ntfs journaling file system default file system windows operating systems source code documentation ntfs publicly tools finding ntfs file layout exist ran windows operating system top vmware linux machine pseudo device driver exported scsi disk windows ntfs file system constructed top pseudo device ran simple workloads ntfs observed traffic sba driver analysis object ntfs file metadata stored terms files journal file located center file system ntfsprogs tools discover journal file boundaries journal boundaries distinguish journal 
traffic fixed-location traffic analysis found ntfs data journaling easily verified amount data traffic observed sba driver found ntfs similar jfs block-level journaling journals metadata terms records verified blocks journaled ntfs matching contents fixed-location traffic contents journal traffic inferred ntfs performs ordered journaling data writes ntfs waits data block writes fixed-location complete writing metadata blocks journal confirmed ordering sba driver delay data block writes upto seconds found metadata writes journal delayed amount related work journaling studies journaling file systems studied detail notably seltzer compare variants journaling ffs soft updates technique managing metadata consistency file systems authors present direct observation low-level traffic familiar systems implementors explain behavior make semantic inferences explain journaling performance drops delete benchmark authors report file system forced read indirect block order reclaim disk blocks section tool sba makes expert observations readily recent study compares range linux file systems including ext ext reiserfs xfs jfs work evaluates file systems fastest benchmarks explanation workload file system benchmarks popular file system benchmarks iozone bonnie lmbench modified andrew benchmark postmark iozone bonnie lmbench perform synthetic read write tests determine throughput andrew postmark intended model realistic application workloads uniformly measure throughput runtime draw high-level conclusions file system contrast sba intended yield low-level insights internal policies file system related work chen patterson self-scaling benchmark work benchmarking framework conducts search space workload parameters sequentiality request size total workload size concurrency hones interesting parts workload space interestingly conclusions file system behavior drawn resultant output size file cache approach automated construct benchmarks exercise file system behaviors controlled manner file system tracing previous studies traced file system activity zhou ousterhout baker roselli record file system operations deduce file-level access patterns vogels performs similar study inside file system driver framework information mapped missed studies recent tracing infrastructure tracefs traces file systems vfs layer tracefs enable low-level tracing sba finally blaze ellard show low-level packet tracing nfs environment recording network-level protocol activity network file system behavior carefully analyzed type packet analysis analogous sba positioned low level reconstruct higher-level behaviors obtain complete view proceedings usenix annual technical conference april anaheim conclusions systems grow complexity techniques approaches enable users system architects understand detail systems operate presented semantic block-level analysis sba methodology file system benchmarking block-level tracing provide insight internal behavior file system block stream annotated semantic information block belongs journal data structure excellent source information paper focused behavior journaling file systems understood sba case sba straightforward user journal allocated disk sba analyzed detail linux journaling file systems ext reiserfs performed preliminary analysis linux jfs windows ntfs cases uncovered behaviors difficult discover conventional approaches developed presented semantic trace playback stp enables rapid evaluation ideas file systems stp demonstrated potential benefits numerous modifications current ext implementation real workloads traces modifications transaction grouping mechanism ext reevaluated untangled approach enables asynchronous processes obtain in-memory bandwidth presence synchronous streams system acknowledgments theodore jiri schindler members adsl research group insightful comments mustafa uysal excellent shepherding anonymous reviewers thoughtful suggestions work sponsored nsf ccrccr- ccrngs- itritr- ibm emc aranya wright zadok tracefs file system trace fast san francisco april baker hartman kupfer shirriff ousterhout measurements distributed file system sosp pages pacific grove october jfs log journaled file system performs logging proceedings annual linux showcase conference pages atlanta jfs overview ibm developerworks library l-jfs html blaze nfs tracing passive network monitoring usenix winter pages san francisco january bray bonnie file system benchmark http textuality bonnie bryant forester hawkes filesystem performance scalability linux freenix monterey june chen patterson approach performance evaluation self-scaling benchmarks predicted performance sigmetrics pages santa clara chutani anderson kazar leverett mason sidebotham episode file system usenix winter pages san francisco january ellard seltzer nfs tracing tools techniques system analysis lisa pages san diego california october ganger patt metadata update performance file systems osdi pages monterey november gray reuter transaction processing concepts techniques morgan kaufmann hagmann reimplementing cedar file system logging group commit sosp austin texas november katcher postmark file system benchmark technical report trnetwork appliance october mckusick joy leffler fabry fast file system unix acm transactions computer systems august mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april mcvoy staelin lmbench portable tools performance analysis usenix san diego january mogul update policy usenix summer boston june norcutt iozone filesystem benchmark http iozone ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference anaheim june ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system sosp pages orcas island december reiser reiserfs namesys riedel kallahalla swaminathan framework evaluating storage system security fast pages monterey january roselli lorch anderson comparison file system workloads usenix pages san diego california june rosenblum ousterhout design implementation logstructured file system acm transactions computer systems february seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems usenix pages san diego california june solomon inside windows microsoft programming series microsoft press sourceforge linux ntfs project http linux-ntfs net sweeney doucette anderson nishimoto peck scalability xfs file system usenix san diego january transaction processing council tpc benchmark standard specification revision technical report transaction processing council tpc benchmark standard specification revision technical report tweedie future directions ext filesystem freenix monterey june tweedie journaling linux ext file system fourth annual linux expo durham north carolina tweedie ext journaling file system olstrans sourceforge net release ols -ext ols -ext html july vogels file system usage windows sosp pages kiawah island resort december yang twohey engler musuvathi model checking find file system errors osdi san francisco december zhou costa smith file system tracing package berkeley unix usenix summer pages salt lake city june 
pipeline batch sharing grid workloads douglas thain john bent andrea arpaci-dusseau remzi arpaci-dusseau miron livny computer sciences department wisconsin madison abstract present study batch-pipelined scientific workloads candidates execution computational grids studies focus behavior single applications study characterizes workloads composed pipelines sequential processes file storage communication share significant data batch study includes measurements memory cpu requirements individual components analyses sharing complete batches conclude discussion ramifications workloads end-to-end scalability system design introduction years researchers understood importance studying workload characteristics order evaluate impact current future systems architecture previous application studies focused detailed behavior single applications sequential parallel caching behavior spec workloads long topic intense scrutiny communication characteristics parallel applications similarly documented applications isolation production settings computational science desired end-result product group applications run hundreds thousands times varied inputs applications executed high throughput computing system condor managed high-level workflow software chimera refer workloads batch-pipelined illustrated figure batch-pipelined workload composed independent pipelines pipeline sequential processes communicate preceding succeeding processes private data files shared input files pipelines process pipeline sharing pipeline sharing batch batch width figure batch-pipelined workload stages figure suggests workload generally submitted large batches pipelines incidentally synchronized beginning pipeline logically distinct correctly execute faster slower siblings key difference studying behavior single application batch-pipelined workload sharing behavior batch-pipelined workload understood instances application run executable potentially input files realistically capture full diversity production workloads study behavior entire pipeline account effects sharing paper present study production scientific workloads collected application pipelines diverse fields computational science including astronomy biology geology physics applications representative broad class important workloads present basic characterization computational memory demands workloads find individually single pipeline place tremendous load system resources combination loads overwhelming focus behavior workloads primary source sharing characterize sharing occurs workloads breaking activity categories endpoint represents input final output pipeline-shared shared write-then-read fashion single pipeline batch-shared comprised input shared pipelines characterization show shared dominant component traffic importantly study implications systems design find wide-area network bandwidth scalability problem applications attempts made eliminate shared successful systems workloads segregate types traffic order scale successfully submit pipeline-shared data significant problem batch-shared data elucidate traditional file systems workloads rest paper organized section describe general characteristics batchpipelined workloads specific application pipelines section describe experimental method sections analyze data discuss implications discuss related work section conclude section applications applications characterize chosen range scientific disciplines selection criteria applications attacking major scientific objective composed sequential applications require scalable computing environment accomplish high throughput focus applications measurements include seti home point guidance users chose workloads input parameters correspond production descriptions applications found figure applications variable granularity cms amanda process variable number small independently generated events applications chose pipeline sizes events cms showers amanda typical production cases cpu resources consumed pipeline scale linearly number events ibis multiple datasets differing resolutions granularity resolution reflects size dataset experiments medium sized dataset ibis nautilus perform single simulations variable length seti blast operate work unit fixed size applications observed characteristic behaviors diamond-shaped storage profile small initial inputs generally created humans initialization tools expanded early stages large intermediate results intermediates reduced stages small results interpreted humans incorporated database intermediate data serves checkpoint cached values ephemeral nature multi-level working sets users easily identify large logical collections data needed application calibration tables physical constants execution applications tend select small working set users aware significant consequences data replication caching techniques significant data sharing application large configuration space users submit large numbers similar jobs access similar working sets analysis condor logs shows usual batch size thousand amanda cms blast property exploited efficient widearea distribution modest communication links method application capture cpu memory behavior cpu memory behavior tracked hardware counters statistics instrument behavior make shared-library interposition agent replaces routines standard library explicit event requested application library records event marking start end operation instruction count details request technique applied application dynamically linked care avoid additional overheads due tracing access memory-mapped files traced userlevel paging technique posix mprotect feature access memory-mapped regions generates user-level page fault sigsegv handled traced shared library application blast memory-mapped analysis page faults considered equivalent explicit read operations page size non-sequential access memorymapped pages recorded explicit seek operation workload analysis overview resources consumed application figure applications wide variance run times current hardware ranging minute blast day ibis considered individually applications spend majority time consuming cpu memory requirements program sizes modest comparison total volume matches search string database mbmips blastp blast inputs climate data climate forecast analyze mips ibis triggered events raw events geometry configuration configuration cmkin mips mips cmsim cms problem solution initial state integral mips scf mips argos mips setup initial state intermediate states initial state coordinate files visualization physics mips nautilus bin coord mips mips rasmol nautilus inputs ice tables raw events standard events noisy events triggered events geometry physics corsika corama amasim kbmips mips mips mmc mips amanda figure application schematics schematics summarize structure application pipeline circles individual processes labeled instruction counts rounded boxes data private pipeline double boxes data shared pipelines batch arrows data flow blast searches genomic databases matching proteins nucleotides queries archived data include errors gaps acceptable match similarity parameterized exhaustive search single executable blastp reads query sequence searches shared database outputs matches ibis global-scale simulation earth systems ibis simulates effects human activity global environment global warming ibis performs simulation emits series snapshots global state cms high-energy physics experiment begin operation cms testing software two-stage pipeline stage cmkin random seed generates models behavior particles accelerated ring output set events fed cmsim simulates response detector final output represents events exceed triggering threshold detector messkit hartree-fock simulation non-relativistic interactions atomic nuclei electrons allowing computation properties bond strengths reaction energies distinct executables comprise calculation setup initializes data files input parameters argos computes writes integrals atomic configuration scf iteratively solves self-consistent field equations nautilus simulation molecular dynamics input configuration describes molecules threedimensional space newton equation solved particle incremental snapshots periodically capture particle coordinates final snapshot passed back program initial configuration simulation eventually snapshots converted standard format bin coord consolidated images rasmol amanda astrophysics experiment designed observe cosmic events gamma-ray bursts collecting resulting neutrinos interaction earth mass stage calibration software corsika simulates production neutrinos primary interaction creates showers muons corama translates 
output standard high-energy physics format mmc propagates muons earth ice introducing noise atmospheric sources finally amasim simulates response detector incident muons real millions instructions memory traffic application time integer float burst text data share ops seti seti blast blastp ibis ibis cms cmkin cmsim total setup argos scf total nautilus nautilus bin coord rasmol total amanda corsika corama mmc amasim total figure resources consumed shown total amounts resources consumed subsequent tables shading differentiate application pipelines real time refers total wall-clock time applications run instrumentation overhead burst average number instructions executed operations instruction counts obtained performance monitoring counters pmcs -class processors notice exception application pipelines modest bandwidth requirements total reads writes application files traffic unique static files traffic unique static files traffic unique static seti seti blast blastp ibis ibis cms cmkin cmsim total setup argos scf total nautilus nautilus bin coord rasmol total amanda corsika corama mmc amasim total figure volume shown total amounts performed traffic number bytes flow process unique considers unique byte ranges total traffic notice cms perform large proportions reread traffic indicating caching important static refers total size files accessed unique applications read portions files notice blast reads total data files accesses suggests systems prestage data sets performing unnecessary work appl open dup close read write seek stat seti blastp ibis cmkin cmsim total setup argos scf total nautilus bin coord rasmol total corsika corama mmc amasim total figure instruction mix shown total number type instructions executed applications seek column includes non-sequential access memory-mapped pages ignores lseek operations change file offset column sums number generally uncommon operations ioctl access high numbers column reflect fact bin coord rasmol driven shell scripts perform readdir operations notice applications high degrees random access shown ratio seeks reads writes contradicts previous file system studies dominance sequential endpoint pipeline batch appl files traffic unique static files traffic unique static files traffic unique static seti blastp ibis cmkin cmsim total setup argos scf total nautilus bin coord rasmol total corsika corama mmc amasim total figure roles shown total amounts type performed endpoint traffic consists initial inputs final outputs unique application pipeline traffic intermediate data passed pipeline stages intermediate data passed phases single stage batch traffic input data shared instances pipeline traffic number bytes flow process unique considers unique byte ranges total traffic static refers total size files accessed unique applications read portions files notice applications exception ibis endpoint traffic relative total traffic scalability systems run applications depend ability differentiate types hit rate blast ibis cms hit rate cache size cache size nautilus cache size amanda figure batch cache simulation hit rate blast ibis cms hit rate cache size cache size nautilus cache size amanda figure pipeline cache simulation figure details volume produced pipeline stage applications conceived pipeline multiple stages connected simple data streams makes complex read write file system number files accesses seti cms lesser degree blast read input data multiples times overwriting output data found pipelines exception amanda output over-writing update application-level checkpoints place alarmed observe checkpoints unsafely written directly existing data written file atomically replaced renaming pipelines distributed large collections data runs typical run accesses small portion common similar runs static size blast dataset exceeds unique amount read application distribution operations figure notice applications high degree random access shown ratio seeks reads writes results nature data files accessed programs generally complex self-referencing internal structure contradicts file system studies dominance sequential characterize types sharing batchpipelined workloads divided traffic roles endpoint traffic consists initial inputs final outputs unique pipeline read written central site system design pipeline traffic consists intermediate data passed pipeline stages intermediate data passed phases single stage batch traffic input data identical pipelines understanding application identified file accessed endpoint pipeline batch computed traffic performed category shown figure immediately comparatively traffic needed endpoints bulk pipeline batch depending application examining figures note large number opens issued relative number files accessed typically designed standalone workstations applications optimized realities distributed computing opening file access times expensive issuing read write figures show working set sizes batchshared pipeline-shared data workload values computed simulations performed trace data batch width varying lru cache size blocks executable files implicitly included batch-shared data general types sharing cache sizes small respect volume sizes typical main memories today outliers amanda large amount batch shared data half read cache effective large sizes amanda high pipeline hit rate small cache sizes due large number singlebyte requests due high degree re-reading output overwriting cms small cache sizes effectively maximize hit rates blast pipeline data ibis stage pipeline data form checkpoints written read multiple times figure shows applications relate amdahl long standing system balance ratios recently amended gray workloads cpu-io ratios measured mips mbps exceeding amdahl ideal indicating reliance computation ratio memory cpu speed alpha amdahl exception component amanda close gray reliance computation memory finally ratio cpu instructions instructions orders magnitude larger respect single instance pipeline commodity computing node engineered amdahl metrics considerably overprovisioned bandwidth memory capacity cpu mem cpu cpu appl mips mbps mips instr seti blastp ibis cmkin cmsim total setup argos scf total nautilus bin coord rasmol total corsika corama mmc amasim total amdahl gray figure amdahl ratios system implications workloads potentially infinite problem domains ability harness computing power enables higher resolution parameters lower statistical uncertainties current users applications scale throughput running hundreds thousands simultaneously scale applications considered cpu-bound bound considered aggregate give idea growing envelope current scientific computing spring cms pipeline simulate million events divided pipelined jobs consuming cpu-years producing terabyte output batch small fraction attempted test run full production begins successive yearly workloads planned grow code data published authoritative form experiment central site likewise simulation outputs eventually moved back archival storage section explore general properties computing storage systems built satisfy workloads explore detailed algorithms data management provisioning resources endpoint scalability capacity individual computing nodes ultimate scalability workloads limited competition shared resources assume workload relies central site authenticity archival input output data demonstrated actual endpoint traffic small fraction total applications eliminate non-endpoint traffic endpoint server techniques caching replication significant gains scalability traffic elimination carried carefully pipeline-shared traffic eliminated end user intermediate data return debugging 
archival ability reproduce questionable batchshared eliminated constraints maintaining consistency authenticity potentially changing input data traffic elimination blindly consideration data computing system limits system executing workloads based ability eliminate shared traffic figure shows selected applications scale systems eliminating category traffic assume presence buffering structure sufficient completely overlap cpu figures assume mips cpu show cpu time horizontal lines show milestones bandwidth lower represents capable commodity hard disk upper represents aggressive storage server network leftmost graph shows scalability system carries traffic endpoint server discipline high end storage device needed systems modest size overwhelmed applications ibis seti scale batch-shared traffic eliminated make significant improvements cms nautilus shown graph hand pipelineshared traffic eliminated observe significant gains seti nautilus shown endpoint performed reach limit shown rightmost graph applications shown scale workers modest storage high-end storage seti potentially scale million cpus indicator specialized design wide-area deployment valuable limits workload scalability cpu hardware improve performance time limits space prevent detailed discussion found technical report rate batch width batch width batch width storage center commodity disk rate batch width blast cms amanda nautilus ibis setiathome data endpoint pipeline endpoint batch endpoint figure scalability roles graphs show scalability applications improved orders magnitude batch-shared pipeline-shared performed endpoint server horizontal lines show milestones bandwidth upper represents high-end storage center lower represents current commodity disk software architecture order scale large sizes software architectures workloads strive eliminate batch-shared pipeline-shared data endpoint interactions constraints security persistence performance traditional file systems serve applications naming consistency requirements targeted interactive cooperating users applications require data management system specialized requirements workload analysis failure recovery resource management issue batch input sharing received significant attention grid computing community deployed systems srb gdmp manage widely-distributed well-known batch shared data techniques discovering replicating batchshared data proposed diminishing importance batch sharing submit issue pipeline sharing problem neglected figure shows localization types achieve high scalability treatment pipelineshared data necessarily batch shared data writer reader discarded pipeline-shared outputs require facility discovery reader data advertised degree batch-shared data loss pipeline-shared output require re-execution previous computation stage solutions pipeline batch sharing problems require application classified roles degree accuracy custom applications seti succeeded wide scalability virtue manual division endpoint explicit network communication expect valuable applications re-written distributed environment ideally roles detected automatically approach trec system deduces program dependencies behavior user provide hints roles system modifying applications directly number file systems account conventional wisdom quickly-deleted data significant source traffic general-purpose workload recognition limited application due requirements reliability consistency interactive systems nfs permits delay application writes data movement server delay made minutes hours order accommodate pipeline sharing reduction unnecessary writes accompanied increased danger data loss crash unusual consistency semantics session semantics afs worse closing file blocking operation forces write-back dirty data vertically shared data written back numerous close operations cpu held idle pipelines offering possibility cpu-i overlap general-purpose file systems operate assumption data eventually flow back archival site workloads require opposite assumption created data remain created explicit operation writer system user forces archival storage improves overlap eliminates unnecessary writes increases danger operations waiting written back fail due permissions disconnection sources error distributed file system acceptable batch system long failed detected matched process issued force re-execution job suggest problem attacked coupling workflow manager condor dagman globus chimera tracks dependencies general graphs jobs systems activity presumed reliable centralized side effect execution creation positioning pipeline-shared data integrated workflow data efficiently shared maintaining possibility error recovery related work cpu memory communication characteristics applications studied years research community roughly categorized type workloads generalpurpose workloads applications sequential applications examined isolation parallel applications isolation summarize work categories focusing examined file system activity file system activity examined range general-purpose workloads studies greatly influenced file system design years focused academic research workloads studies found files short lifetimes access patterns exhibit high degree locality read-write sharing rare missing broad studies traffic linkage applications generate traffic similar work studies focused behavior individual applications commercial workloads domain interaction pipeline behavior sequential applications examined interesting study detailed memory-system behavior applications opportunities sharing fundamentally studies parallel applications ways similar pipelined batch applications cpu memory communication behavior parallel vector applications quantified number studies impact explicit study complements works studying sharing behavior important class workload studies demonstrate drastic differences behavior parallel applications compared general-purpose workloads parallel scientific workloads high bursty rates constant behavior runs input parameters parallel workloads tend dominated storage retrieval costs large files check-point files finally quick deletion uncommon conclusions applications run isolation production settings scripting workflow tools glue series applications pipelines pipeline run thousands times varied inputs achieve goals users term workloads batch-pipelined batches pipelines run instant paper characterize collection scientific batch-pipelined workloads typical characterizations processing memory demands bring sharing characteristics workloads demonstrate importance scalability key managing workloads classification segregating traffic type aggressively exploiting sharing characteristics scalability improved orders magnitude acknowledgements gratefully acknowledge people helped install operate understand applications behavior including paolo desiati zach miller amadeu sum juan pablo jon foley dierk polzin daniel reed alan smet brian forney muthian sivathanu florentina popovici timothy denehy helpful discussion comments paper work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation douglas thain supported lawrence landweber ncr fellowship distributed systems acharya uysal bennett mendelson beynon hollingsworth saltz sussman tuning performance intensive parallel applications proceedings fourth workshop input output parallel distributed systems iopads pages philadelphia pennsylvania altschul madden schaffer zhang zhang miller lipman gapped blast psi-blast generation protein database search programs nucleic acids research pages amdahl storage parameters system potential ieee computer group conference pages june baker hartmann kupfer shirriff ousterhout measurements distributed file system proceedings acm symposium operating systems principles sosp july barroso gharachorloo bugnion memory system characterization commercial workloads proceedings international symposium computer architecture isca pages cantin hill cache performance selected spec cpu benchmarks computer architecture news september crandall aydt chien reed input output characteristics scalable parallel applications proceedings ieee acm conference supercomputing san diego california cypher konstantinidou messina architectural requirements parallel scientific applications explicit communication proceedings annual international symposium computer architecture pages san diego california acm sigarch ieee computer society tcca computer architecture news foley integrated biosphere model land surface processes terrestrial carbon balance vegetation dynamics global biogeochemical cycles foster voeckler 
wilde zhou chimera virtual data system representing querying automating data derivation proceedings conference scientific statistical database management edinburgh scotland july gray shenoy rules thumb data engineering proceedings sixteenth ieee international conference data engineering icde pages holtman cms data grid system overview requirements cms note cern july hulith amanda experiment proceedings xvii international conference neutrino physics astrophysics helsinki finland june kuo winslett cho lee chen efficient input output scientific simulations proceedings parallel distributed systems iopads pages lee crowley bear anderson bershad execution characteristics desktop applications windows proceedings international symposium computer architecture isca pages litzkow livny mutka condor hunter idle workstations proceedings international conference distributed computing systems june miller katz input output behavior supercomputing applications proceedings acm ieee conference supercomputing pages ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system procedings acm symposium operating systems principles sosp pages december pasquale polyzos static analysis characteristics scientific applications production workload proceedings acm ieee conference supercomputing pages november rajasekar wan moore mysrb srb components data grid proceedings eleventh ieee symposium high performance distributed computing hpdc edinburgh scotland july ranganathan foster decoupling computation data scheduling distributed data-intensive applications proceedings eleventh ieee symposium high performance distributed computing hpdc edinburgh scotland july roselli lorch anderson comparison file system workloads usenix annual technical conference rosti serazzi smirni squillante impact program behavior parallel scheduling proceedings joint international conference measurement modelling computer systems sigmetrics pages samar stockinger grid data management pilot proceedings iasted international conference applied informatics february satyanarayanan study file sizes functional lifetimes proceedings symposium operating systems principles sosp pages sullivan werthimer bowyer cobb gedye anderson major seti project based project serendip data personal computers proceedings international conference bioastronomy sum pablo nautilus molecular simulations code technical report wisconsin madison dept chemical engineering thain bent arpaci-dusseau arpaci-dusseau livny architectural implications pipeline batch sharing scientific workloads technical report cstr- wisconsin computer sciences department january thain livny multiple bypass interposition agents distributed computing journal cluster computing vahdat anderson transparent result caching technical report csd- computer science division california-berkeley vazhkudai tuecke foster replica selection globus data grid ieee international symposium cluster computing grid ccgrid wong martin arpaci-dusseau culler architectural requirements scalability nas parallel benchmarks supercomputing portland oregon nov woo ohara torrie shingh gupta splashprograms characterization methodological considerations proceedings annual international symposium computer architecture pages santa margherita ligure italy june acm sigarch ieee computer society tcca computer architecture news 
improving storage system availability d-graid muthian sivathanu vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison fmuthian vijayan dusseau remzig wisc abstract present design implementation evaluation d-graid gracefully-degrading quickly-recovering raid storage array d-graid ensures les system remain unexpectedly high number faults occur d-graid recovers failures quickly restoring live system data hot spare graceful degradation live-block recovery implemented prototype scsibased storage system underneath unmodi systems demonstrating powerful le-system functionality implemented narrow block-based interface introduction tree falls forest hears make sound george berkeley storage systems comprised multiple disks backbone modern computing centers storage system entire center grind halt downtime expensive on-line business world millions dollars hour lost systems storage system availability formally ned time failure mtbf divided sum mtbf time recovery mttr mtbf mtbf mttr improve availability onecan increase mtbf decrease mttr surprisingly researchers studied components availability increase time failures large storage array data redundancy techniques applied keeping multiple copies blocks sophisticated redundancy schemes parity-encoding storage systems tolerate small xed number faults decrease time recovery hot spares employed failure occurs spare disk activated lled reconstructed data returning system normal operating mode quickly narrow interface systems storage curtailed opportunities improving mtbf mttr raidstorage array disk fails repaired entire array corrupted availability cliff result storage system laying blocks oblivious semantic importance relationship les corrupted inaccessible extra disk failure time-consuming restore backup entire array remains unavailable disks operational storage array information blocks live system recovery process restore blocks disk unnecessary work slows recovery reduces availability ideal storage array fails gracefully disks system data unavailable ideal array recovers intelligently restoring live data effect important data disappear failure data restored earlier recovery strategy data availability stems berkeley observation falling trees isn process access recovered failure explore concepts provide storage array graceful failure semantics present design implementation evaluation d-graid raid system degrades gracefully recovers quickly d-graid exploits semantic intelligence disk array place system structures disks fault-contained manner analogous fault containment techniques found hive operating system distributed systems unexpected double failure occurs d-graid continues operation serving les accessed d-graid utilizes semantic knowledge recovery speci cally blocks system considers live restored hot spare aspects d-graid combine improve effective availability storage array note graid techniques complementary existing redunappears usenix symposium file storage technologies fast dancy schemes storage administrator con gures d-graid array utilize raid level single disk fail data loss additional failures lead proportional fraction unavailable data paper present prototype implementation d-graid refer alexander alexander semantically-smart disk system built transforming policies mechanisms infokernel andrea arpaci dusseau remzi arpaci dusseau nathan burnett timothy denehy thomas engle haryadi gunawi james nugent florentina popovici department computer sciences wisconsin madison fdusseau remzi ncb tedenehy englet haryadi damion popovicig wisc abstract describe evolutionary path operating systems exible manner higher-level services infokernel exposes key pieces information algorithms internal state default policies mechanisms controlled user-level implemented prototype infokernels based linux netbsd kernels called infolinux infobsd infokernels export key abstractions basic information primitives infolinux implemented case studies showing policies linux manipulated kernel speci cally show default cache replacement algorithm layout policy disk scheduling algorithm tcp congestion control algorithm turned base mechanisms case study found infokernel abstractions implemented code overhead accuracy synthesizing policies user-level acceptable categories subject descriptors operating systems organization design general terms design experimentation performance keywords policy mechanism information introduction separating policy mechanism long goal operating system design informal nition view policy scheme deciding mechanism underneath narrow block-based scsi storage interface disk system understands system data structures including super block allocation bitmaps inodes directories important structures knowledge central implementing graceful degradation quick recovery intricate understanding system structures operations semantically-smart arrays tailored systems alexander functions underneath unmodi linux ext vfat systems make important contributions semantic disk technology deepen understanding build semantically-smart disk systems operate correctly imperfect system knowledge demonstrate technology applied underneath widely varying systems demonstrate semantic knowledge raid system apply redundancy techniques based type data improving availability key aspects alexander implementation graceful degradation rst selective metadata replication alexander replicates naming system meta-data structures system high degree standard redundancy techniques data small amount overhead excess failures render entire array unavailable entire directory hierarchy traversed fraction les missing proportional number missing disks fault-isolated data placement strategy ensure semantically meaningful data units failure alexander places semantically-related blocks blocks storage array unit fault-containment disk observing natural failure boundaries found array failures make semantically-related groups blocks unavailable leaving rest system intact fault-isolated data placement improves availability cost related blocks longer striped drives reducing natural bene parallelism found raid techniques remedy alexander implements access-driven diffusion improve throughput frequently-accessed les spreading copy blocks hot les drives system alexander monitors access data determine les replicate fashion nds space replicas pre-con gured performance reserve opportunistically unused portions storage system evaluate availability improvements d-graid trace analysis simulation d-graid excellent job masking arbitrary number failures processes enabling continued access important data evaluate prototype alexander microbenchmarks trace-driven workloads construction d-graid feasible imperfect semantic knowledge powerful functionality implemented block-based storage array run-time overheads d-graid small cpu costs compared standard array high show access-driven diffusion crucial performance live-block recovery effective disks under-utilized combination replication data placement recovery techniques results storage system improves availability maintaining high level performance rest paper structured section present extended motivation section discuss design principles d-graid section present trace analysis simulations discuss semantic knowledge section section present prototype implementation evaluate prototype section discuss alternative methods implementing d-graid commercial feasibility semantic disk based approach section section present related work conclude section extended motivation case graceful degradation raid redundancy techniques typically export simple failure model fewer disks fail raid continues operate correctly degraded performance disks fail raid unavailable problem corrected restore tape raid schemes small disks working users observe failed disk system graceful degradation raid system absolutely tolerate xed number faults excess failures catastrophic data amount proportional number disks system continues allowing access data failed data restored matter users applications entire contents volume present matters set les question realistic expect catastrophic failure scenario raid system raidsystem high mtbf reported disk manufacturers appears usenix symposium file storage technologies fast disk failure highly occur rst failed disk repaired multiple disk failures occur primary reasons correlated faults common systems expected raid carefully designed orthogonal manner single controller fault component error render fair number disks unavailable redundant designs expensive found higher end storage arrays gray points system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure page evidence suggests multiple failures occur ibm serveraid array controller product includes directions attempt data recovery multiple disk failures occur raidstorage array organization data stored servers raidin servers single disk failed indicator informed administrators problem problem discovered disk array failed full restore backup ran days scenario graceful degradation enabled access large fraction user data long restore approach dealing multiple failures employ higher level redundancy enabling storage array tolerate greater number failures loss data techniques expensive three-way data mirroring bandwidth-intensive write redundant store graceful degradation complementary techniques storage administrators choose level redundancy common case faults graceful degradation enacted worse expected fault occurs mitigating ill 
tool implementing set policies conceptually design simpler view distinct minimum separating policy mechanism build modular managing processes cpu traditional view dispatcher performs low-level context-switch mechanism scheduler decides process run policy conceptual division ectively isolates code change permission make digital hard copies part work personal classroom granted fee provided copies made distributed pro commercial advantage copies bear notice full citation rst page copy republish post servers redistribute lists requires prior speci permission fee sosp october bolton landing york usa copyright acm ported dispatcher change handle erent workloads scheduler ambitious goals separation enables extensible systems kernel implements mechanisms processes implement policies user-level suit alternatively design kernel processes download ideal policies directly practice cleanly separate policies mechanisms simplest mechanisms decisions embedded placing waiting process queue makes policy decision process inserted queue presence speci mechanisms dictates policies ciently pragmatically implemented words levin decision exclude policies lower-level kernel policy research explicitly strives push nition mechanism extreme safe protected image hardware operation implement policies ensure fairness competitors result mechanism policy viewed continuum policies implemented component act mechanisms layer system words features policy mechanisms depends perspective important challenge next-generation systems determine policies implemented traditional converted mechanisms higher-level services large amount functionality hundreds millions dollars invested thousands developer years spent commodity operating systems view existing policies mechanisms higher-level services built top radical restructuring operating system advocate evolutionary approach thesis paper key transforming policies implemented mechanisms export information internals operating system enhanced expose internal information infokernel work show functionality system signi cantly enhanced existing modi provide key pieces information internal state algorithms infokernel higher-level services require policies implement functionality services leverage default policies demonstrate feasibility approach modify linux kernel create infolinux implementation infolinux exposes values key data structures simple descriptions algorithms employs illustrate power information investigate case studies user-level services convert default linux policies controllable mechanisms illustrate succinctness approach show tens hundreds lines code required export information linux case studies focus major components cache management placement disk scheduling networking rst case study show user-level service convert linux q-based page replacement algorithm building block replacement algorithms mru lru lfu fifo show applications turn range system allocation policies controllable placement mechanism applications les directories disk show linux c-look disk scheduling policy transformed building block algorithms idle queue free-bandwidth scheduler finally demonstrate exporting information tcp reno altered controllable transport mechanism enabling user-level policies tcp vegas experience reveals fundamental abstractions infokernel support exporting comprehensive information infokernel retains level secrecy implementation enable innovation infokernel interface provide portability applications infokernel strives export fundamental abstractions expected hold policies represent page replacement algorithm infokernel report direct state page bits frequency bits dirty bits relevant state depends exact algorithm generalized abstraction desired list pages ordered expected evicted level detail export fundamental tension infokernel design case studies describe abstraction prioritized lists provide ective means exporting requisite information prototype infobsd derived netbsd veri abstractions easily implemented erent systems case studies uncover limitations effect semantically-smart storage implementing functionality semantically-smart disk system key bene enabling wide-scale deployment underneath unmodi scsi interface modi cation working smoothly existing systems software base desire evolve interface systems storage reality current interfaces survive longer anticipated bill joy systems protocols live forever mechanism d-graid deployed non-intrusive existing infrastructure semantic disks ensure design d-graid expectations section discuss design d-graid present background information systems data layout strategy required enable graceful degradation important design issues arise due layout process fast recovery file system background semantic knowledge system speci discuss d-graid design implementation widely differing systems linux ext microsoft vfat system inclusion vfat represents signi contribution compared previous research operated solely underneath unix systems ext system intellectual descendant berkeley fast file system ffs disk split set block groups akin cylinder groups ffs bitmaps track inode data block allocation inode blocks data blocks information including size block pointers found inode vfat system descends world operating systems paper linux vfat implementation fatalthough work general applies variants vfat operations centered eponymous allocation table entry allocatable block system entries locate blocks linked-list fashion rst block address entry fat block entry hold end-ofle marker setting block free unlike unix systems information found inode vfat system spreads information fat directory entries fat track blocks belong directory entry information size permission type information graceful degradation ensure partial availability data multiple failures raid array d-graid employs main techniques rst fault-isolated data placement strategy d-graid places semanticallyrelated set blocks unit fault containment found storage array simplicity discussion assume semantically-related set blocks single disk unit fault containment generalize easily generalized failure boundaries observed scsi chains refer physical disk belongs home site disk fails fault-isolated data placement ensures les disk appears usenix symposium file storage technologies fast foo bar inode foo inode bar data bar data bar data root data foo inode root foo bar inode foo inode bar data bar data bar data root data foo inode root inode foo inode bar data bar data bar foo data root bar data foo inode root inode root inode root inode root inode fooinode fooinode foo foo data root foo data root foo data root bar data foo bar data foo bar data foo figure comparison layout schemes gures depict layouts foo bar unix system starting root inode directory tree data vertical column represents disk simplicity assumes data redundancy user data left typical system layout non-d-graid disk system blocks pointers spread system single fault render blocks bar inaccessible middle fault-isolated data placement les directories scenario access inode access data indirect pointer blocks constrained disk finally selective meta-data replication replicating directory inodes directory blocks d-graid guarantee users les requisite pointers removed rightmost gure simplicity color codes white user data light shaded inodes dark shaded directory data home site unavailable les remain accessible les technique selective meta-data replication d-graid replicates naming system meta-data structures system high degree directory inodes directory data unix system d-graid ensures live data reachable orphaned due failure entire directory hierarchy remains traversable fraction missing user data proportional number failed disks d-graid lays logical system blocks availability single depends disks traditional raid array dependence set entire set disks group leading entire system unavailability unexpected failure unix-centric typical layout fault-isolated data placement selective meta-data replication depicted figure note techniques d-graid work infokernel approach meaningful subset system laid single d-graid array system striped multiple d-graid arrays single array meaningful view system scenario d-graid run logical volume manager level viewing arrays single disk techniques remain relevant d-graid treats system block type differently traditional raid taxonomy longer adequate describing d-graid behaves ner-grained notion raid level required graid employ redundancy techniques types data d-graid commonly employs n-way mirroring naming system meta-data standard redundancy techniques mirroring parity encoding raidfor user data note administrative control determines number failures d-graid degrade gracefully section explore data availability degrades varying levels namespace replication design considerations layout replication techniques required enable graceful degradation introduce host design issues highlight major challenges arise semantically-related blocks fault-isolated data placement d-graid places logical unit system data fault-isolated container disk blocks d-graid considers related determines data remains failure basic approach le-based grouping single including data blocks inode indirect pointers treated logical unit data technique user les directory unavailable frustration confusion groupings preserve meaningful portions system volume failure directory-based grouping d-graid ensures les directory unit fault containment automated options allowing users arbitrary semantic groupings d-graid treats unit load balance fault-isolated placement placing blocks disks blocks isolated single home site isolated placement improves availability introduces problem load balancing space time components terms space total utilized space disk maintained roughly level fraction disks fail roughly fraction data unavailable balancing addressed foreground data rst allocated background migration files directories larger amount free space single disk handled potentially appears usenix symposium file storage technologies fast expensive reorganization reserving large extents free space subset drives files larger single disk split disks pressing performance problems introduced fault-isolated data placement previous work striping data disks performance compared sophisticated placement algorithms d-graid makes additional copies user data spread drives system process call access-driven diffusion standard d-graid data placement optimized availability access-driven diffusion increases performance les frequently accessed surprisingly access-driven diffusion introduces policy decisions d-graid including place replicas made performance les replicate create replicas meta-data replication level degree meta-data replication d-graid determines resilient excessive failures high degree replication desirable meta-data replication costs terms space time space overheads trade-offs obvious replicas imply resiliency difference traditional raid d-graid amount space needed replication naming system meta-data dependent usage volume directories induces greater amount overhead time overheads higher degree replication implies lowered write performance naming system meta-data operations observed lack update activity higher levels directory tree lazy update propagation employed reduce costs fast recovery main design goal d-graid ensure higher availability fast recovery failure critical straightforward optimization d-graid recover live system data assume restoring data live mirror hot spare straightforward approach d-graid simply scans source disk live blocks examining system structures determine blocks restore process readily generalized complex redundancy encodings d-graid potentially prioritize recovery number ways restoring important les rst importance domain speci les users manner similar hoarding database coda exploring graceful degradation section simulation trace analysis 
evaluate potential effectiveness graceful degradation impact semantic grouping techniques rst quantify space overheads level replication -way -way -way ext ext vfat vfat table space overhead selective meta-data replication table shows space overheads selective metadata replication percentage total user data level naming system meta-data replication increases leftmost column percentage space overhead meta-data replication shown columns depict costs modest -way paranoid -way schemes row shows overhead system ext vfat block size set graid demonstrate ability d-graid provide continued access proportional fraction meaningful data arbitrary number failures importantly demonstrate d-graid hide failures users replicating important data simulations system traces collected labs cover days activity data spread logical volumes space overheads relevant kernel state expressed ordered list user-level service directly manipulate ordering touching page increase priority primitives types services implemented limited disk scheduling networking case studies target policy substantially ers underlying kernel policy cult accurately mimic target policy behavior arises emulating mru top default linux algorithm finally list-reordering operations expensive involve disk accesses achieving control user-level prohibitively costly controlled placement case study exhibits property suggesting additional policy-manipulation machinery bene cial implementing infolinux discovered number information primitives streamline interactions user-level services infokernel application obtain information memorymapped read-only pages system calls interface depends frequency information needed internal structure application obtain information polling blocking kernel informs information changed case studies illustrate primitives structure remainder paper begin section addressing primary issues building infokernel section compare infokernels previous work section describe details primary implementation infolinux section describe case studies section discuss preliminary experience infobsd conclude section infokernel issues section discuss general issues infokernel begin presenting bene exposing information policies state higher-level services applications discuss fundamental infokernel tensions information exposed bene information transform policy implemented mechanism user-level process understand behavior policy erent conditions behavior policy function algorithms current state export information captures aspects providing information user-level processes manipulate underlying policy adapt behavior manipulate policy exposing algorithms internal state higher-level services running implement policies tailored knowledge services predict decisions make set inputs current state change decisions speci cally service implemented user-level library probes normal inputs reacts controlled manner application system performs prefetching observing sequential access pattern application blocks squelch prefetching intervening read random block policy limits preferences limits ensure measure fairness competing processes preferences workload behavior improve system performance converting source infokernel policy target user-level policy implies limits imposed source policy circumvented preferences biased general service bias preferences source policy overhead incurs disincentive obtaining control system-wide goals system allocation policy takes advantage higher bandwidth outer tracks multi-zone disk giving preference large les outer zone fairness system limits space allocated user zone process wishes allocate small outer tracks exceeded user per-zone quota process bias behavior system padding larger size case process pays time space overhead create large actions match default preferences system enable adaptation secondary bene providing information applications adapt behavior improved performance memory-intensive application amount physical memory process data multiple passes limiting working set avoid thrashing extreme process amount time remaining time slice decide acquire 
contentious lock expects preempted nishing critical section straightforward services directly adapt behavior indirectly manipulate behavior focus challenging issue controlling policy paper tensions design infokernel number design decisions made discuss issues pertaining amount information exposed exposing information process boundaries exposing information adding mechanisms amount information tension designing infokernel decide information exported hand exporting information bene cial priori information higher-level services hand exposing information greatly expands api presented applications destroys encapsulation put simply knowledge power ignorance bliss unfortunate implications application api expanded user-level service control page replacement algorithm page evicted service developed infokernel clock replacement application examines clock hand position bits service moved infokernel pure lru replacement service examine position page lru list perspective user-level service api implies service longer operates correctly signi cantly rewritten perspective xed api discourages developers implementing algorithms constrains evolution application portability infokernel information hidden provide abstractions sake innovation abstractions ciently high level operating system easily convert internal representations abstractions precisely determining correct infokernel abstractions requires experience large number case studies operating systems paper important rst step ning abstractions examining major components operating system cache management placement disk scheduling networking case studies describe abstraction infokernel export represent cache replacement algorithm prioritized list resident pages user-level services ciently determine pages evicted implementation illustrates implementing abstractions existing simple involves lines code process boundaries tension designing infokernel determine information competing processes exposed hand information processes exposed process optimize behavior relative entire system hand information processes process learn secrets harm performance process information processes hidden security privacy data read written contents memory pages information resource usage processes increase prevalence covert channels information higher cost resident page list curious process infer process accessing speci timing open system call curious process infer fast time inode cache infokernel hide information process boundaries performing work resident page list block number removed pages belong calling process issue addresses suitability competing applications running infokernel concern infokernel services encouraged game control harm processes acquire locks performing control potentially competes processes greedy process avoid advisory lock information greedy process acquire fair share resources greedy service pages memory touching evicted steal frames processes infokernel provide mechanisms behavior original albeit costly achieve infokernel support greedy process continually touch pages blindly imposing additional overhead entire system summary infokernel stresses role arbitrate resources competing applications limits existing policies impart responsibilities infokernel adequate policy limits suited non-competitive server workloads adding mechanisms nal issue determine kernel add mechanisms control simply exposing information question cult answer general requires side-by-side comparison desired piece functionality leave future work adding mechanism complex exposing information reasons consistent existing policy mechanism preferences policy violate limits policy mechanism explicitly check current invocation violate policy limits system user-level policy implemented infokernel performs check automatically complexity arises notifying user reasons mechanism failure cient level detail user submit request cient information request succeed future exposing details mechanism invocation violated policy similar exposing basic policy information task infokernel related philosophies infokernel extensible systems goal tailoring operating system workloads services user-speci policies primary erence infokernel strives evolutionary design realistic discard great body code contained current operating systems infokernel transforms existing operating system suitable building block infokernel approach erence extensible systems application-speci code run protected environment disadvantages advantages disadvantages infokernel exible range policies provide higher overhead indirectly controlling policies userlevel policies voluntarily processes advantage approach infokernel require advanced techniques dealing safety downloaded code software-fault isolation type-safe languages in-kernel transactions open question address simple control provided infokernel cient implement range policies idea exposing information explored speci components instance bene knowing cost accessing erent pages state network connections demonstrated infokernel generalizes concepts compare infokernel philosophy related philosophies detail exokernel open implementation gray-box systems goal exposing information stated exokernels exokernel takes strong position xed high-level abstractions avoided information page numbers free lists cached tlb entries exposed directly exokernel sacri ces portability applications erent exokernels information standard interfaces supplied library operating systems alternately infokernel emphasizes importance allowing operating systems evolve maintaining application portability exposes internal state abstractions systems map data structures philosophy open implementation project similar infokernel philosophy states part implementation details hidden interface mere details details bias performance resulting implementation authors propose ways changing interface clients modules allowing clients anticipated usage outline requirements download code module clients choose module implementation btree linkedlist hashtable approach exposes algorithm employed infokernel address importance exposing state finally relationship infokernels authors work gray-box systems philosophy gray-box systems acknowledges information applications existing operating systems leveraged graybox approach takes extreme position modi applications assume infer information number limitations implementing user-level services gray-box system removed infokernel gray-box system user-level services make key assumptions incorrect ignore important parameters operations performed service infer internal state impose signi overhead web server simulate cache replacement algorithm on-line infer current contents memory finally make correct inference circumstances service observe inputs outputs infokernel retains advantages leveraging commodity operating system user-level services built infokernel robust powerful gray-box system implementation infolinux section describe experience building prototype infokernel based linux infolinux strict superset linux interfaces added expose information control mechanisms policies modi point exercise demonstrate traditional operating system easily converted infokernel result prototype infolinux cient functionality demonstrate higher-level services extend existing linux policies abstractions policy information structure initial version infolinux abstractions key policies linux abstraction composed data structures algorithms enable portability user-level services erent infokernels data structures standardized data structures exported infolinux system calls user-level libraries user-level library accesses kernel memory directly key pages case study abstraction description inforeplace pagelist prioritized list in-memory pages victimlist list pending victim pages infoplace fsregionlist prioritized list disk groups dir allocation infosched diskrequestlist queue disk requests fileblocks list blk numbers inode data infovegas msglist list message segments table case studies infolinux abstractions case study paper present names abstractions employs short description mapped read-only address space memory-mapped interface processes avoid overhead system call rarely interesting data structures scattered kernel memory disk scheduling queue signi restructuring kernel place related information page case studies ned number fundamental infokernel abstractions summarized table detail section found commonality exists abstractions needed disparate policies case essential state information expressed terms prioritized list cases version list exists kernel cases infokernel construct list varied sources information abstract list disk scheduling policy simply 
existing scheduling queue separated device abstract list allocation policy cylinder groups group selected allocation head list constructed combining knowledge cylinder groups picked current state group represent algorithm infolinux prototype exports logical disk scheduling algorithm represented c-look sstf sptf naming method primitive sufcient initial demonstration existing policies infolinux controlled investigating general representation key aspects policy infokernel exports rules determine items list abstraction moved priority user processes predict item inserted list information primitives converting internal format data structures linux general representation required infokernel interface requires careful implementation choices found number information primitives making conversion simpler developer cient run time ers application periodically poll state infokernel polling performed frequently application miss important state infolinux mechanism recording data structure circular ers amortizing overhead system call values noti ers poll state service noti key data structure infolinux mechanism process block speci abstraction changed timers amount time operation takes reveal information infokernel time valuable inferring properties resource autonomy network disk infolinux mechanism add timers kernel return results user processes procedure counters infokernel export estimate state kernel count number times procedure called piece code executed infolinux mechanism add counters speci locations code primitives implemented dynamic kernel instrumentation tool dynamic instrumentation infokernel developer easily trace variables incur overhead tracing activated user process preliminary experience tool overhead enabling information primitives low ering variable typical system operations infolinux routine ext getblk incurs negligible overhead case rst examine space overheads due selective meta-data replication typical d-graidstyle redundancy calculate cost selective meta-data replication percentage overhead measured volumes trace data calculate highest selective meta-data replication overhead percentage assuming replication user data user data mirrored overheads cut half table shows selective meta-data replication induces mild space overhead high levels meta-data redundancy linux ext vfat systems -way redundancy meta-data space overhead incurred worst case vfat blocks increasing block size ext space due internal fragmentation larger directory blocks overheads decrease vfat phenomenon due structure vfat xed-sized system block size grows allocation table shrinks blocks directory data grow static availability examine d-graid availability degrades failure semantic grouping strategies rst strategy le-based grouping information single failure boundary disk directory-based grouping allocates les directory analysis place entire les directories trace simulated -disk sysappears usenix symposium file storage technologies fast percent directories completely number failed disks static data availability directory-based -way directory-based -way file-based -way directory-based -way figure static data availability percent entire directories shown increasing disk failures simulated system consists disks loaded trace strategies semantic grouping shown le-based directory-based line varies level replication namespace meta-data point shows average deviation trials trial randomly varies disks fail tem remove simulated disks measure percentage directories assume user data redundancy d-graid level figure shows percent directories directory les accessible subdirectories les gure observe graceful degradation works amount data proportional number working disks contrast traditional raid disk crashes lead complete data unavailability fact availability degrades slightly expected strict linear fall-off due slight imbalance data placement disks directories modest level namespace replication -way leads good data availability failure conclude le-based grouping les directory disappear failure leading user dissatisfaction dynamic availability finally simulating dynamic availability examine users applications oblivious graid operating degraded mode speci cally run portion trace simulator number failed disks record percent processes observed failure run experiment namespace replication les needed processes replicated experiment set degree namespace replication full replication vary level replication contents popular directories usr bin bin lib figure percent unaffected processes number failed disks dynamic per-process availability popular replication -way -way -way figure dynamic data availability gure plots percent processes run unaffected disk failure busy hour trace degree namespace replication set aggressively line varies amount replication popular directories -way implies directories replicated -way -way show modest extreme amount replication means deviations trials shown shows replicating contents directories percent processes run ill-effect lower expected results figure directories replicated percentage processes run completion disk failure expected reason clear substantial number processes require executable libraries run correctly popular directory replication excellent availability failure fortunately popular les read directories wide-scale replication raise write performance consistency issues space overhead due popular directory replication minimal sized system trace directories account total system size semantic knowledge move construction d-graid prototype underneath block-based scsi-like interface enabling technology underlying d-graid semantic knowledge understanding system utilizes disk enables d-graid implement graceful degradation failure quick recovery exact details acquiring semantic knowledge disk raid system assume basic understanding system layout structures storage system speci cally assume d-graid static knowledge system layout including regions disk block types contents speci block types elds inode appears usenix symposium file storage technologies fast file system behaviors paper extend understanding semanticallysmart disks presenting techniques handle general system studies demonstrate power infokernel approach implemented number case studies show policies infolinux converted controllable mechanisms examples focus major policies linux cache management placement disk scheduling networking case studies emphasize erent aspects infokernel exibility control provided range internal policies mapped general abstraction rate state information converting cache replacement policy mechanism show wide range target policies fifo lru mru lfu implemented user-level transforming system placement policy mechanism show infokernel abstraction ciently general capture important details variety policies directory allocation ext ffs temporal locality manipulations disk scheduling tcp congestion control algorithms show user-level policies frequent noti cation state implemented case study present infokernel abstraction suitably represents underlying policy describe export abstraction ciently infolinux present user-level library code implements policy top infokernel abstraction quantify accuracy overhead controlling policies comparing infolinux result modeled expectations in-kernel implementation cases approach perfect accuracy incur additional overhead common theme case studies overhead controlling policies user-level directly depends user desired control meshes preferences biases policy finally case study demonstrate usefulness user-level policy showing performance improvement workload compared default linux policy experimental environment experiments section employ number erent machine con gurations machine ghz pentium processor main memory -gb -rpm western digital ata ide hard drives machine mhz pentium processor main memory -gb -rpm ibm lzx scsi hard drives machine mhz pentium main memory -gb -rpm ibm gxp ata ide hard drive intel etherexpress pro ethernet ports netbed emulation environment multi-disk machines disk noted stress erent components system machines booted memory experiments run multiple times averages reported variance low cases shown file cache replacement erent applications bene erent cache replacement algorithms modifying replacement policy demonstrate exibility extensible systems functionality approximated infokernel environment rst case study user-level library inforeplace demonstrates variety replacement algorithms fifo lru mru lfu implemented top unmodi linux replacement algorithm begin describing intuition cache replacement policy treated mechanism giving replacement control applications case application wishes hot list pages resident memory target policy supports simple lru-replacement policy source policy ensure hot list remains resident user process pages evicted user process accesses page number times source replacement policy increase priority page generally replacement policy converted accessing pages evicted source policy evicted target policy infokernel abstractions support inforeplace user-level library infolinux export information applications determine victim pages operations move pages priority state linux converted form low overhead linux uni page cache q-like replacement policy rst referenced page active queue managed two-handed clock evicted page inactive queue managed fifo provide general representation prioritized list kernel task statements memory-map counter setup track page movement reset counter export victimlist total victimlist abstraction user-level task statements setup simulation framework target policies fifo lru mru lfu check victimlist refresh total inforeplace library table code size cache replacement case study number statements counted number semicolons needed implement victimlist abstraction infolinux inforeplace library user-level shown physical pages pagelist infolinux exports concatenation queues system call information inforeplace examine end queue pages interest drawback pagelist abstraction large number elements imposes signi overhead copying queue user space call made infrequently queue checked infrequently pages evicted user-level library notices infolinux victimlist abstraction pages full queue mechanism quickly determine pages added list infolinux exports estimate rapidly queues changing reporting times items moved inactive queue ciently counting number times key procedures called counter activated service registers interest fast access user-space mapped address space user process counter approximately equal process performs expensive call state pages inactive queue shown top half table victimlist abstraction implemented statements fact half code needed setup memory-mapped counter user-level policies victimlist abstraction user-level inforeplace library frequently poll pages eviction obtain list pages pages evicted target policy inforeplace accesses move active list roles inforeplace track pages resident target policy simplicity inforeplace library exports set wrappers applications call open read write lseek linux procedures shrink cache macro del page inactive list lfumrulrufifo percent missing pages target replacement algorithm inforeplace inaccuracy lfumrulrufifo time read target replacement algorithm inforeplace overheads misc sim refresh check figure accuracy overhead inforeplace fifo lru mru lfu implemented top q-based replacement algorithm linux bar graph left shows inaccuracy inforeplace inaccuracy percentage pages memory workload ends bar graph shows average overhead incurred read write time divided time check victimlist abstraction refresh pages evicted simulate target replacement algorithm perform miscellaneous setup experiments run machine close system calls library tracks pages accessed explicit calls infolinux expanded return access information page process address space read write inforeplace library rst performs simulation target replacement algorithm determine speci page belongs page queue inforeplace victimlist pages high priority eviction accesses nally library wrapper performs requested read write returns basic steps implemented fifo lru mru lfu top linux q-based replacement algorithm bottom half table shows amount code needed implement inforeplace thousand statements required code straightforward bulk simulation erent replacement policies overhead accuracy evaluate overhead accuracy 
infokernel approach run synthetic workload speci cally crafted stress choices made replacement algorithms workload accesses large times size memory touching blocks initial access order recency frequency attributes block blocks evicted depends attributes replacement policy considers measure accuracy target policy end run comparing actual contents memory expected contents figure shows accuracy overhead implementing algorithms infolinux graph run time workload run time pinrange hit rate level index hit rate level index pinrange figure workload bene inforeplace graph left depicts run-time synthetic database index lookup workloads systems bars labeled show run time index lookups stock linux kernel bars labeled pinrange show run time specialized pinrange policy infolinux x-axis varies workload speci cally depth fan-out index implies index depth fan-out graph shows details pinrange speeds performance workload showing hit rate erent levels -level index experiments run machine left shows inaccuracy inforeplace ned percentage pages resident memory target replacement algorithm metric pages memory target policy pages resident inaccuracy general inaccuracy inforeplace low inaccuracy mru highest roughly resident pages preferences mru highly con ict emulating mru inforeplace constantly probe pages memory graph figure shows overhead implementing policy terms increase time read write operation time broken time check victimlist abstraction probe pages evicted simulate target replacement algorithm perform miscellaneous setup overhead inforeplace generally low read write call exception pure lfu incurs high simulation overhead roughly call due logarithmic number operations required read write order pages frequency assuming cost missing cache high overhead emulating lfu pays miss rate reduced workload bene database researchers observed policies provided general-purpose operating systems deliver suboptimal performance database management systems demonstrate utility inforeplace provide cache replacement policy inspired dbmin suited database index lookups indices dbms systems typically organized trees replacement policy nodes root tree memory pages higher probability accessed simplicity policy pinrange assumes index allocated root head leaves end pinrange pages preference based set pages rst bytes large lru queue remaining pages smaller queue pinrange simple implement requiring roughly statements inforeplace library demonstrate bene inforeplace repeated index lookups compare workload run-time pinrange versus default linux replacement policy note fairly sophisticated policy introduced database community speci cally handle types access patterns result preference pages top tree experiments run synthetic workloads emulating lookups index trees levels fan-out machine memory pinrange con gured prefer rst main memory graph left figure shows pinrange improves run-time erent trees illustrate pinrange improves performance graph figure plots hit rate function index level tree levels fan-out graph shows pinrange noticeably improves hit rate sixth level tree slightly reducing hit rate lowest seventh level tree improvement total hit rate results decrease run-time includes approximately seconds overhead inforeplace library summary case study shows replacement policies implemented information exposed victimlist abstraction ciently exible build variety classic replacement algorithms compares favorably direct in-kernel implementations cao work applications easily invoke policies combination lru mru strategies system difculty emulating behavior wider range policies lfu case study illustrates care ciently perform conversion internal state general victimlist abstraction inforeplace demonstrates target replacement algorithms similar source algorithm implemented accuracy overhead file directory placement o-intensive applications database systems web servers bene controlling layout data disk systems provide type control applications placement les demonstrate power extensible systems gray-box systems ability group related objects nextgeneration storage systems describe infoplace placement service case study demonstrate points placement functionality implemented lower kernel task statements collect region stats convert stats fsregionlist ext temporal data ffs export fsregionlist total fsregionlist abstraction user-level task statements framework setup directory allocation file allocation fill regions cache directories total infoplace library table code size placement case study number statements counted number semicolons needed implement fsregionlist abstraction infolinux shown kernel layout policies ext temporal data block ffs infoplace library overhead infokernel gray-box techniques erent kernel policies mapped common infokernel representation enabling innovation infokernel abstractions describe placement policy abstraction prioritized list regions disk fsregionlist list ordered beginning region allocation data operations performed reduce priority region infokernels freedom disk regions erently demonstrate generality abstraction explore erent kernel placement policies map representation begin default placement policy ext linux ffs-based systems maintain locality disk allocation cylinder group block group natural mapping region cylinder group abbreviated simply group placement policy ext les group parent directory directory group free data blocks average number free inodes simplicity focus abstracting directory placement policy placement exporting directory fsregionlist straightforward previous case study case priority group derived directory placement algorithm current state group speci cally free inodes free data block count map parameters precise ordering infolinux rst places cylinder groups categories average number free inodes average infolinux sorts category number free data blocks concatenates nal ordered list step infolinux describe erent system operations decrease priority group fsregionlist altering free inode data block counts ext placement algorithm decrease priority group future allocations infolinux considers cases groups average number free inodes infolinux reports creations zero-length les performed number free inodes average average infolinux reports creation block created number additional free data blocks group compared list understand challenges controlling directory placement top erent placement policies implemented variety policies linux kernel original ffs algorithm group fewest directories average number free inodes chosen case mapping prioritized list identical ext number directories data blocks simple placement algorithm selects group free data blocks case prioritized list simply orders groups number free data blocks finally temporal allocation cylinder group chosen previous group full algorithm prioritized list simply distance group hot group temporal allocation operation lowers group priority create empty les block number free inodes number free data blocks group choice made infolinux min amount code required implement fsregionlist abstraction shown top half table exporting system abstraction involves non-trivial amount code list explicitly behaviors exist previous linux work required expected creating system fsregionlist mounted synchronously complex directory implementing placement complex policies functionality ext ffs disk requires relax code straightforward policies requirement temporal describe data user-level assumptions policies general steps system performed behavior infoplace similar gray-box version modern place systems adhere brie behavioral describe guidelines place place blocks assumes system running dynamically ext typed system system locate group types blocks parent directory physical location disk simple trick lifetime allocate system named group unix place system creates block data region directory user-data block previously indirect-pointer allocated block group place directory-data block renames system speci delay updates user disk infoplace delayed writes steps system les facilitate controlling batching placement small writes directories memory complicated suppressing writes les subsequently deleted consequence delayed writes order system writes data disk arbitrary systems order writes carefully remain general make assumptions ordering note assumptions made practical reasons linux ext system exhibits aforementioned behaviors accuracy information assumptions general system behavior imply storage system accurately classify type block block classi cation straightforward type block depends location disk berkeley fast file system ffs regions disk store inodes xed system creation traf regions inodes type information spread multiple blocks block lled indirect pointers identi observing inode speci cally inode indirect pointer eld address indirect block formally identify indirect block semantic disk inode block indirect pointer eld relevant inode block written disk disk infers indirect block observes block written information classify treat block indirect block due delayed write reordering behavior system time disk writes block freed original inode reallocated inode type normal data block disk operations place memory ected disk inference made semantic disk block type wrong due inherent staleness information tracked implementing correct system potentially inaccurate inferences challenges address paper implementation making d-graid discuss prototype implementation graid alexander alexander faultisolated data placement selective meta-data replication provide graceful degradation failure employs access-driven diffusion correct performance problems introduced availability-oriented layout alexander replicates namespace system meta-data administrator-controlled stores user data raidor raidmanner refer systems d-graid levels pursuing d-graid level implementation logstructuring avoid small-write problem exacerbated fault-isolated data placement section present implementation graceful degradation live-block recovery complexity discussion centered graceful degradation simplicity exposition focus construction alexander underneath linux ext system end section discuss differences implementation underneath vfat graceful degradation present overview basic operation graceful degradation alexander indirection map similar scsi-based raid system alexander presents host systems linear logical block address space internally alexander place blocks facilitate place graceful degradation control placement alexander introduces transparent level indirection logical array infoplace operation place repeatedly creates directory checks inode number directory allocated directory correct system physical placement disks indirection map imap similar structures unlike systems imap maps live logical system block replica list physical 
locations unmapped blocks considered free candidates d-graid reads handling block read requests d-graid level straightforward logical address block alexander imap replica list issues read request replicas choice replica read based criteria alexander randomized selection appears usenix symposium file storage technologies fast figure anatomy write gure depicts control sequence write operations alexander rst gure inode block written alexander observes contents inode block identi newly added inode selects home site inode creates physical mappings blocks inode home site inode block aggressively replicated gure alexander observes write data block inode mapped write directly physical block gure alexander write unmapped data block defers block alexander nally observes inode fourth gure creates relevant mappings observes blocks deferred issues deferred write relevant home site writes contrast reads write requests complex handle alexander handles write request depends type block written figure depicts common cases block static meta-data block inode bitmap block unmapped alexander allocates physical block disks replica reside writes copies note alexander easily detect static block types inode bitmap blocks underneath unix systems simply observing logical block address inode block written d-graid scans block newly added inodes understand inodes d-graid compares newly written block copy process referred block differencing inode d-graid selects home site lay blocks belonging inode records inode-to-homesite hashtable selection home site balance space allocation physical disks d-graid greedy approach selects home site disk space utilization write unmapped block data region data block indirect block directory block allocation d-graid block belongs actual home site case d-graid places block deferred block list write disk learns block crash inode write make block inaccessible system in-memory deferred block list reliability concern d-graid newly added block pointers inode indirect block written newly added block pointer refers unmapped block graid adds entry imap mapping logical block physical block home site assigned inode newly added pointer refers block deferred list d-graid removes block deferred list issues write physical block writes deferred blocks written owner inode blocks inode written rst subsequent data writes mapped disk directly block type interest d-graid data bitmap block data bitmap block written d-graid scans newly freed data blocks freed block graid removes logical-to-physical mapping exists frees physical blocks block deferred list freed block removed deferred list write suppressed data blocks written system deleted inode written disk generate extra disk traf similar optimizations found systems removing blocks deferred list important case freed blocks alexander observe owning inode deferred block stays deferred list bounded amount time inode owning block written bitmap block indicating deletion block written exact duration depends delayed write interval system block reuse discuss intricate issues involved implementing graceful degradation rst issue block reuse existing les deleted truncated les created blocks part reallocated graid place blocks correct home site reuse blocks detected acted d-graid handles block reuse manner inode block indirect block written d-graid examines valid block pointer physical block mapping matches home site allocated inode d-graid mapping block correct home site write block made context home site copied physical location location blocks copied added pending copies list background thread copies appears usenix symposium file storage technologies fast blocks home site frees physical locations copy completes dealing imperfection dif culty arises semantically-smart disks underneath typical systems exact knowledge type dynamically-typed block impossible obtain discussed section alexander handle incorrect type classi cation data blocks data directory indirect blocks d-graid understand contents indirect blocks pointers place blocks home site due lack perfect knowledge fault-isolated placement compromised note data loss corruption issue goal dealing imperfection conservatively avoid eventually detect handle cases speci cally block construed indirect block written assume valid indirect block live pointer block d-graid action cases rst case pointer refer unmapped group steps move groups closer state target group chosen eventually place succeeds comparison infoplace begins 
obtaining fsregionlist priority list target group rst infoplace allocates directory veri infolinux directory created desired group race occur activity system target list infoplace performs designated time imbalance thousands inodes controlling ext place infoplace time percentage imbalance comparing allocation algorithms data ext ffs temporal figure overhead placement infoplace graph left compares overhead controlling allocation infoplace versus place information current state system time perform allocation target group shown y-axis target group fewest free inodes inode imbalance ned number inodes allocated groups identical number inodes graph compares overhead erent directory allocation policies linux data chooses group free data blocks ext default ext policy ffs original ffs policy temporal allocates directories hot group group completely lled nition imbalance varies allocation policies x-axis scenarios percentage maximum imbalance cylinder groups algorithm experiments run machine number type operations creating zero-length dummy les groups preceding re-obtains list groups repeating process target head successfully complete infoplace cleans removing zero-length dummy les place infoplace expensive directory allocation algorithm time user speci location directory common operation retain cache directories erent groups user speci target group libraries simply rename existing directories group target group cache empty explicit control needed analysis server traces typical day directories created upper bound entries added directory cache nighttime cron job amount code required implement infoplace library shown bottom half table overhead accuracy rst experiment shows information state system helps infoplace perform controlled placement ciently gray-box version place rst graph figure show time overhead place directory target group function imbalance imbalance ned number items inodes data blocks lled non-target groups target group move front list show accuracy versions provide complete accuracy expected versions overhead control increases inode imbalance increases representing amount ght placement preferences ext policy graph dramatically shows bene information overhead place times higher infoplace experiments shown graph figure illustrate cost infoplace depends directory placement algorithm key predicting cost correctly imbalance terms inodes data blocks overhead function imbalance cost creating needed items placement policies non-target groups primarily inodes ext ffs temporal cost policies data blocks data block algorithm cost graph show imbalance expected occur workload running placement policy expect imbalance ext ffs data block policies tend low algorithms balance usage disk imbalance temporal allocation tend high allocations group overhead controlling layout constant imbalance policies typical imbalance vary polices workload bene demonstrate utility infoplace library show bene results les accessed time reorganized disk shown bene general block-level le-level reorganization show simply standard tool modi advantage placement control provided infoplace speci cally modify tar program place les directories unpacked single localized portion disk demonstrate bene optimization unpack large archive case entire tree linux documentation project size unpacked directory tree roughly les directories run experiment machine memory infoplace initialized pre-built cache directories unmodi tar utility running linux takes seconds logical block mentioned d-graid creates mapping home site inode indirect block belongs indirect block pointer valid mapping correct mapping indirect block misclassi pointer invalid d-graid detects block free observes data bitmap write point mapping removed block allocated bitmap written d-graid detects reallocation inode write creates mapping copies data contents home site discussed case potentially corrupt block pointer point mapped logical block discussed type block reuse results mapping copy block contents home site indirect block pointer valid mapping correct block indirect block misclassi cation alexander wrongly copies data home site note data accessible original block belongs blocks incorrect home site fortunately situation transient inode written d-graid detects reallocation creates mapping back original home site restoring correct mapping files accessed properly laid infrequent sweep inodes rare cases improper layout optimizations d-graid eventually move data correct home site preserving graceful degradation reduce number times misclassi cation occurs alexander makes assumption contents indirect blocks speci cally number valid unique pointers null pointers alexander leverage assumption greatly reduce number misclassi cations performing integrity check supposed indirect block integrity check reminiscent work conservative garbage collection returns true pointers -byte words block point valid data addresses volume non-null pointers unique set blocks pass integrity check corrupt data contents happened evade conditions test run data blocks system small fraction data blocks pass test blocks pass test reallocated data block indirect block misclassi access-driven diffusion issue d-graid address performance fault-isolated data placement improves availability cost performance data accesses blocks large directory-based grouping les directory longer parallelized improve performance alexander performs access-driven diffusion monitoring block accesses determine hot diffusing blocks replication disks system enhance parallelism access-driven diffusion achieved logical physical levels disk volume logical approach access individual les monitored considered hot diffused perle replication fails capture sequentiality multiple small les single directory pursue physical approach alexander replicates segments logical address space disks volume systems good allocating contiguous logical blocks single les directory replicating logical segments identify exploit common access patterns implement access-driven diffusion alexander divides logical address space multiple segments normal operation gathers statistics utilization access patterns segment background thread selects logical segments bene access-driven diffusion diffuses copy drives system subsequent reads writes rst replicas background updates original blocks imap entry block copy date amount disk space allocate performanceoriented replicas presents important policy decision initial policy alexander implements reserve minimum amount space speci sysappears usenix symposium file storage technologies fast tem administrator replicas opportunistically free space array additional replication approach similar autoraid mirrored data autoraid identify 
data considered dead system written contrast d-graid semantic knowledge identify blocks free live-block recovery implement live-block recovery d-graid understand blocks live knowledge correct block live considered dead lead data loss alexander tracks information observing bitmap data block traf bitmap blocks liveness state system reected disk due reordering delayed updates uncommon observe write data block bit set data bitmap account d-graid maintains duplicate copy bitmap blocks sees write block sets bit local copy bitmap duplicate copy synchronized system copy data bitmap block written system conservative bitmap table ects superset live blocks system perform live-block recovery note assume pre-allocation state bitmap written disk subsequent allocation locking linux modern systems ensures technique guarantees live block classi dead disk block live longer situation arise system writes deleted blocks disk implement live-block recovery alexander simply conservative bitmap table build list blocks restored alexander proceeds list copies live data hot spare aspects alexander host aspects implementation required successful prototype discuss length due space limitations found preserving logical contiguity system important block allocation developed mechanisms enable placement directory-based grouping requires sophistication implementation handle deferral writes parent directory block written complete average enhanced tar completes unpacking roughly faster seconds bene achieved slightest modi cations tar statements added call infoplace library summary case study demonstrates erent directory placement policies implemented mapped infokernel abstraction infoplace takes initial steps showing abstract algorithm expressing operations lower priority group prioritized list case study shows overhead control strict function target end state ers desired native system placement policy finally study demonstrates standard utilities bene control provided infoplace library kernel task statements setup export diskrequestlist wait diskrequestlist change total diskrequestlist user-level task statements setup misc diskrequestlist issue request total infoidlesched library setup misc disk model pick background request total infofreesched library table code size disk scheduling case study number statements counted number semicolons needed implement diskrequestlist abstraction infolinux shown counts user-level libraries infoidlesched infofreesched disk scheduling researchers demonstrated applications bene advanced disk scheduling algorithms traditional sstf c-look algorithms case study demonstrate exposing information disk scheduler implement scheduling policies top infolinux speci cally show idle disk scheduler infoidlesched limited freeblock disk scheduler infofreesched implemented user-level libraries top infolinux infokernel abstractions describe disk scheduling policy infokernel exports requests disk scheduling queue cient detail disk scheduling algorithm predict queue request infolinux system calls obtain diskrequestlist processes block diskrequestlist exports scheduling algorithm c-look amount code needed export diskrequestlist reported table addition infofreesched detailed information overheads erent disk operations infofreesched timing primitives infolinux obtain coarse disk model times successive requests disk observed recorded linear block distance requests key index model shown capture seek head switch costs probabilistically capture rotational latency small enhancements needed capture aspects modern disks zoning user-level policies amount code scheduling policies shown table infoidlesched simple disk scheduling algorithm process schedule requests disk idle infoidlesched simply checks diskrequestlist scheduling queue remains empty threshold amount time infoidlesched issues single request queue empty infoidlesched waits state queue change waking items removed recheck queue infofreesched complex freeblock scheduler freeblock scheduling periods rotational latency disk lled data transfers words background tra serviced disk head moving requests impacting foreground tra lumb implement freeblock scheduling disk rmware simpli service time prediction subsequent work freeblock scheduling implemented freebsd kernel user-level scheduling testbed implementing freeblock scheduling top infolinux presents challenges user speci set infofreesched convert disk addresses conversion performed fileblocks infokernel interface predicting newly written blocks allocated disk complex infofreesched schedules read trafc limited setup tasks raid scrubbing virus detection backup infofreesched complete control requests disk scheduling queue requests ordered infofreesched choose background request inserted describe step detail list background requests infofreesched knowledge scheduling algorithm c-look predict background request inserted scheduling queue infofreesched determined request inserted requests infofreesched calculates background request harm harm determined indexing disk timing table linear block distance requests background request impact time foreground request allowed proceed optimization infofreesched schedules background request impact foreground tra infofreesched blocks waiting noti infolinux state disk queue changed infofreesched wakes rechecks background requests serviced overhead accuracy evaluate overhead placing disk scheduling policy user-level complicated infofreesched policy stress infofreesched random-i workload disk idle foreground tra consists processes continuously reading small les chosen uniformly random single background process reads random blocks disk keeping requests outstanding read performed infofreesched incurs overheads roughly obtain diskrequestlist infolinux approximately background request examined determine issued disk infofreesched examines requests background queue overhead induced negligible sum compared multi-millisecond disk latencies workload bene demonstrate utility infoidlesched show ability expose idle queue system activity traces labs foreground tra run minutes 
trace starting december rst created directory structure les trace run issue background tra stream blocks disk sequentially shown figure infoidlesched support background requests idlesched bandwidth infoidlesched background foreground freesched bandwidth infofreesched background foreground figure workload bene infoidlesched infofreesched graph left shows performance foreground background tra infoidlesched leftmost bar shows foreground tra competing background tra middle bar competing tra standard linux rightmost bar infoidlesched graph shows similar graph infofreesched workloads erent graphs text experiments run machine signi cantly degrade foreground performance read requests trace achieve background tra grabs infoidlesched support background tra limited desired foreground read requests achieve bandwidth background tra obtains background request stream induce small decrease foreground performance infoidlesched library aggressive idle scheduler reduce overhead reduce background bandwidth achieved demonstrate ability infofreesched free bandwidth random-i workload overhead experiment figure random foreground tra isolation achieves background requests added support infofreesched foreground tra harmed proportionately achieving background tra achieves background requests infofreesched foreground tra receives background tra obtains free bandwidth summary case studies stress infokernel approach infolinux user-level processes uence decisions disk scheduler ordering requests disk queue result user-level policies decide perform disk request time cult implement freeblock scheduler top system due culty predicting write tra system investigate future work case study shows importance infolinux primitives blocking state abstraction timing duration operations kernel task statements rtt timers wait wake export msglist total msglist abstraction user-level task statements setup main algorithm error handling total infovegas library table code size networking case study number statements counted number semicolons needed implement tcp msglist abstraction infolinux shown upper table lower table presents code size implementing congestion-control policy infovegas user-level networking networking research shown variations tcp algorithm superior erent circumstances erent workloads implemented small variations sending algorithm case study show tcp congestion control algorithm exported user-level processes manipulate behavior speci cally infovegas show tcp vegas implemented top tcp reno algorithm linux similar research network protocols userlevel infokernel infrastructure enables bene shortened development cycle easier debugging improved stability infokernel abstractions manipulate congestion control algorithm main abstraction infokernel msglist list packet packet infolinux exports state waiting acknowledged dropped round-trip time acknowledged variables snd una snd nxt speci tcp rfc exported derived message list tcp reno record round-trip time message high resolution timer times gathered infolinux timing primitives user-level policies basic intuition infovegas calculates target congestion window vcwnd msglist important parameters tcp vegas minrtt basertt derived information infovegas ensures forwards target vcwnd message segments time underlying infokernel finally infovegas blocks state message queue message acknowledged point infovegas send segment adjust calculation target vcwnd amount code implement functionality shown table overhead accuracy experiments verify infovegas behaves similarly in-kernel implementabandwidth queue size packets macroscopic behavior vegasinfovegas reno figure accuracy infovegas macroscopic behavior emulation environment figure experiments emulate network bandwidth delay vary router queue size x-axis y-axis reports bandwidth achieved linux vegas infovegas reno tion vegas linux macroscopic microscopic levels figure shows infovegas achieves bandwidth similar vegas variety network con gurations router queue size changed numbers illustrate space queue decreases reno unable obtain full bandwidth due packet loss infovegas vegas achieve full bandwidth queue space desired figure illustrates behavior infovegas time compared reno in-kernel vegas desired cwnd derived infovegas time closely matches vegas ering signi cantly reno finally infovegas accurately implements vegas functionality user-level low overhead measurements show cpu utilization increases infovegas compared approximately in-kernel vegas workload bene illustrate bene infovegas prototype clustered server specifically nfs storage server con gured front-end node handles client requests backend storage units machines connected switch runs infolinux back-end storage units continuously time block allocation prevents misclassi indirect blocks causing spurious physical block allocation deferred list management introduces tricky issues memory alexander preserves sync semantics returning success inode block writes deferred block writes waiting inode complete number structures alexander maintains imap reliably committed disk preferably good performance buffered small amount non-volatile ram important component missing alexander decision popular read-only directories usr bin replicate widely alexander proper mechanisms perform replication policy space remains unexplored initial experience simple approach based monitoring frequency inode access time updates effective alternative approach administrators perform work background replicating important les disks performing checksums data reorganizing les disk ideally background tasks interfere foreground requests front-end workload stresses network front-end handles nfs requests les cached memory back-end node background replicator process replicates les back-end node back-end node results figure show contention network controlled infovegas streams contend network link bandwidth shared approximately equally foreground tra achieves replication process infovegas background tra interferes minimally foreground tra background tra obtains packets time reno cwnd time infovegas vcwnd cwnd diff time vegas cwnd diff figure accuracy infovegas microscopic behavior behavior reno infovegas in-kernel vegas compared time network con guration sender running infolinux receiver running stock linux machines acting routers single network exists source destination machine passes emulated bottleneck delay maximum queue size packets rst graph shows cwnd calculated reno graph shows infovegas cwnd exported reno derived target vcwnd derived parameter graph shows cwnd calculated linux native vegas implementation experiments run machines netbed testbed foreground tra achieves full line rate gure shows cpu utilization machines experiments revealing small additional cost infovegas service summary infovegas stresses limits infokernel control service react quickly frequent events occur inside kernel receiving acknowledgment overhead handling event circumstances infovegas pays overhead reduces bandwidth generally congestion control algorithm implemented infokernel viewed base sending mechanism aggressive policy allowed algorithm speci limit preference network resource congestion control policies built top infolinux send lower rate exposed primitive tcp friendly discussion brie compare user-level policies explored case studies discussion centers fact case study categories depending user-level process reorder relevant items infokernel prioritized list rst category libraries control policy changing order items related infokernel list inforeplace touches page increase priority infoplace allocates inodes data blocks group decrease priority libraries uence current future requests handled relative existing items list category overhead implementing policy user-level direct function overhead reorganizing list case studies shown infoplace inforeplace provide performance bene applications extreme circumstances overhead performing probes high inforeplace category libraries reorder items related lists libraries exert preferences limiting requests inserted related lists infoidlesched infofreesched infovegas libraries maintain order queue requests issue requests well-controlled times process retract decision insert item policies conservative initiating requests adapt quickly changing conditions infoidlesched send background requests disk queue increasing chances background request interfere foreground request arrives likewise infovegas react immediately network conditions change group messages issued experience infobsd section describe initial experience building prototype infobsd netbsd discussion focus main erences infobsd infolinux implementations date implemented memory management disk scheduling networking abstractions leave placement future work pagelist abstraction infobsd similar infolinux netbsd xed-sized cache primary erence infokernels infolinux pagelist page memory infobsd pages cache netbsd cache managed pure 
lru replacement infobsd simply exports lru list pagelist elements victimlist enable processes quickly determine elements moving lists infobsd tracks number evictions occurred lru list statements needed export abstractions infobsd primary savings compared infolinux requires statements infobsd provide memory-mapped interface eviction count diskrequestlist abstraction straightforward export infobsd chief erence infolinux infobsd relates layer system responsible maintaining device scheduling bgfg throughput cluster throughput background foreground bgfg cpu utilization bandwidth normalized cpu utilization figure workload bene infovegas graph left shows impact contention network infovegas background replication stream y-axis plots bandwidth delivered bar represents experiment combination foreground background tra infovegas graph depicts normalized cpu utilizations experiments cpu utilization bandwidth observe additional cpu overhead running infovegas experiments run machines type queues linux generic level maintains queues block devices infolinux generic level exports diskrequestlist abstraction netbsd generic level exists device type scsi ide export diskrequestlist abstraction independently lines code needed provide information infobsd infobsd requires statements infolinux linux code required access queue check finally infobsd implementation msglist requires relative infolinux version primary erence tcp linux skb ers network packet tcp netbsd mbuf ers multiple packets data structure netbsd cult add time-stamp packet needed msglist infobsd creates maintains queue packets time unacknowledged packet result infolinux statements msglist infobsd requires statements signi increase nal amount code small exercise shown abstractions exported infolinux straight-forward implement infobsd hopeful list-based abstractions ciently general capture behavior unix-based operating systems creating infokernels cult note infokernels export interfaces directly leverage user-level libraries created case studies majority code resides conclusions layering technique long building computer systems breaking larger system constituent components layering makes process building system manageable resultant separation modules increases maintainability facilitating testing debugging layering negative side-e ects traditional arguments layering implementationoriented observation layers network protocol stacks induce extra data copies insidious impact design architects layer encouraged hide details information layer system concealed layers paper argued operating systems avoid pitfall design export general abstractions describe internal state abstractions list memory pages eviction disk requests scheduled user-level services control policies implemented surprising ways information policies implemented transformed mechanisms usable services case studies stressing erent components cache management placement disk scheduling networking explored issues infokernel design ned abstractions infokernel export experience shown abstractions represented prioritized lists found number information primitives implementing abstractions procedure counters timers ability block infokernel state general found power infokernel approach depends closely desired control matches policy kernel infokernel user-level policies operate limits underlying kernel policy user-level policies bias preferences policy result target policies mesh inherent preferences policy implemented high accuracy low overhead found ability user-level processes ciently manipulate internal lists touching page increase priority enables powerful services built top infokernel knowledge serve guide developing future infokernels infokernels export operations ciently reorder retract items in-kernel prioritized lists exible building blocks implementing user-level policies acknowledgments john bent brian forney muthian sivathanu vijayan prabhakaran doug thain helpful discussions comments paper john bent storage server implementation brian forney trace analysis michael marty jacob kretz initial gray-box implementation inforeplace kirk webb netbed team con guring linux netbed computer systems lab providing superb research environment john wilkes excellent demanding shepherding substantially improved content presentation directories treated manner interesting issue required change design behavior linux ext partial disk failure process read data block unavailable ext issues read returns failure process block recovery process issues read ext issue read works expected process open inode unavailable ext marks inode suspicious issue request inode block alexander recovered block avoid change system retain ability recover failed inodes alexander replicates inode blocks namespace meta-data collocating data blocks alexander fat surprised similarities found implementing d-graid underneath numerous aspects paper anonymous reviewers helpful suggestions finally give special grandparents arpaci niara vedat grandparents dusseau anita richard traveling madison taking care anna weeks submission due authors worked requisite long nights nal weeks work sponsored part nsf ccrngs- ccrccr- itrthe wisconsin alumni research foundation ibm faculty award ndseg fellowship department defense akyurek salem adaptive block rearrangement acm transactions computer systems allman balakrishnan floyd rfc enhancing tcp loss recovery limited transmit august ftp ftp rfc-editor in-notes rfc txt august arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages ban canada october bershad savage pardyak sirer fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december brakmo malley peterson tcp vegas techniques congestion detection avoidance proceedings sigcomm pages london united kingdom august burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge ercache contents proceedings usenix annual technical conference usenix pages monterey california june cao felten implementation performance application-controlled file caching proceedings symposium operating systems design implementation osdi pages monterey california november cardwell bak tcp vegas implementation linux http flophouse neal linux-vegas august cheriton zwaenepoel distributed kernel performance diskless workstations proceedings acm symposium operating system principles sosp pages bretton woods hampshire october chou dewitt evaluation management strategies relational database systems proceedings international conference large data bases vldb pages stockholm sweden august dijkstra structure multiprogramming system communications acm druschel pai zwaenepoel extensible kernels leading research astray proceedings workshop workstation operating systems wwos-vi pages cape codd massachusetts ely savage wetherall alpine user-level infrastructure network protocol development proceedings usenix symposium internet technologies systems usits pages san francisco california march engler kaashoek toole exokernel operating system architecture applicationlevel resource management proceedings acm symposium operating systems principles sosp pages copper mountain resort colorado december gibson nagle amiri chang gobio riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon hoe improving start-up behavior congestion control sheme tcp proceedings sigcomm pages stanford california august iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous proceedings acm symposium operating systems principles sosp pages ban canada october jacobson wilkes disk scheduling algorithms based rotational position technical report hpl-csp- hewlett packard laboratories jacobson congestion avoidance control proceedings sigcomm pages stanford california august johnson shasha low-overhead high performance management replacement algorithm proceedings international conference large databases vldb pages santiago chile september kaashoek engler ganger brice hunt mazi eres pinckney grimm jannotti mackenzie application performance flexibility exokernel systems proceedings acm symposium operating systems ext vfat vfat overloads data blocks user data blocks directories alexander defer classi cation blocks manner similar ext implementation instances vfat implementation d-graid differed interesting ways ext version fact pointers located allocation table made number aspects d-graid simpler implement vfat indirect pointers worry ran occasional odd behavior linux implementation vfat linux write disk data blocks allocated freed avoiding obvious common system optimization indicative untuned nature linux implementation served indicator semantic disks wary assumptions make system behavior evaluating alexander present performance evaluation alexander focus primarily linux ext variant appears usenix symposium file storage technologies fast misplaced blocks time sec misplaced blocks remapping remapping close-up close-up figure errors placement gure plots number blocks wrongly laid alexander time running busy hour trace experiment run disks total number blocks accessed trace include baseline measurements vfat system answer questions alexander work correctly time overheads introduced effective access-driven diffusion fast live-block recovery bene expect d-graid complex implementation platform alexander prototype constructed software raid driver linux kernel file systems mount pseudo-device normal disk environment excellent understanding issues involved construction real hardware d-graid system limited ways importantly alexander runs system host applications interference due competition resources performance characteristics microprocessor memory system found actual raid system experiments utilize mhz pentium iii k-rpm ibm disks alexander work correctly alexander complex simple raid systems ensure alexander operates correctly put system numerous stress tests moving large amounts data system problems extensively tested corner cases system pushing situations dif cult handle making system degrades gracefully recovers expected repeatedly crafted microbenchmarks stress mechanisms detecting block reuse handling imperfect information dynamically-typed blocks constructed benchmarks write user data blocks disk slowdown versus raidoperational overheads ext fat create read overwrite unlink figure time overheads gure plots time overheads observed d-graid level versus raid level series microbenchmarks tests run disk systems experiment operations enacted creations operation worst case data data appears valid directory entries indirect pointers cases alexander detect blocks indirect blocks move les directories proper fault-isolated locations verify alexander places blocks disk instrumented system log block allocations addition alexander logs events interest assignment home site inode creation mapping logical block re-mapping blocks homesite receipt logical writes system evaluate behavior alexander workload run workload alexander obtain time-ordered log events occurred system alexander process log off-line principles sosp pages saint-malo france october kiczales lamping lopes maeda mendhekar murphy open implementation design guidelines international conference software engineering icse pages boston massachusetts kiczales lamping maeda keppel mcnamee customizable operating systems proceedings workshop workstation operating systems wwos-iv pages napa california october lampson hints computer system design proceedings acm symposium operating system principles sosp pages bretton woods hampshire october levin cohen corwin pollack wulf policy mechanism separation hydra proceedings acm symposium operating systems principles sosp pages texas austin november liedtke micro-kernel construction proceedings acm symposium operating 
systems principles sosp pages copper mountain resort colorado december lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october lumb schindler ganger freeblock scheduling disk firmware proceedings usenix symposium file storage technologies fast pages monterey california january matthews roselli costello wang anderson improving performance logstructured file systems adaptive methods proceedings acm symposium operating systems principles sosp pages saint-malo france october mckusick joy fabry fast file system unix acm transactions computer systems august nugent arpaci-dusseau arpaci-dusseau controlling place file system gray-box techniques proceedings usenix annual technical conference usenix pages san antonio texas june neil neil weikum lru-k page replacement algorithm number database blocks disk wrongly ering laid proceedings time acm ran sigmod test international conference hours management data traces sigmod found pages washington hours examined number blocks peacock kamaraju misplaced agrawal temporarily fast low consistency checking blocks solaris file report system detailed results proceedings usenix hour annual technical trace conference observed usenix greatest pages number misplaced orleans blocks louisiana hours june examined pearce figure kelly shows harder results gure field gilk parts dynamic bottom instrumentation part tool shows normal linux operation kernel alexander proceedings capability international react conference block reuse modeling tools remapping techniques copying computer blocks communication correct system homesite performance evaluation gure shows tools alexander pages quickly london detect united wrongly kingdom blocks april remap appropriately popovici number arpaci-dusseau blocks misplaced arpacidusseau temporarily robust portable total scheduling number disk blocks mimic accessed proceedings trace top usenix part annual technical gure conference shows usenix number misplaced pages blocks san antonio texas experiment june assuming postel remapping rfc transmission occur control protocol expected september delinquent blocks remain ftp ftp misplaced rfc-editor dip in-notes end rfc txt trace occurs appears august riedel usenix kallahalla symposium file swaminathan storage framework technologies evaluating fast storage run-time system blocks security written seconds proceedings total meta usenix uniquedata symposium file storage technologies raidd-graid fast pages monterey d-graid california january d-graid rosu rosu kernel d-graid support table faster performance web proxies postmark proceedings table compares usenix performance annual d-graid technical level conference raidon usenix postmark pages san benchmark antonio row texas marked d-graid june speci ruemmler level wilkes metadata disk replication shu ing rst technical column report reports benchmark hpl- run-time hewlett packard column shows laboratories number schindler disk writes ganger incurred automated disk drive column shows characterization number technical disk report writes cmu-cs- carnegie metadata blocks mellon fourth column november number seltzer unique chen metadata blocks ousterhout disk written scheduling revisited experiment run proceedings disks usenix winter technical misplaced conference blocks usenix winter assigned pages washington homesite accidentally correcting january original seltzer misplacement endo time small overheads introduced smith dealing explore disaster time surviving overheads misbehaved arise due kernel semantic extensions inference proceedings primarily occurs symposium operating blocks systems written design implementation system osdi creation pages seattle figure washington shows performance october alexander shenoy simple vin cello microbenchmark disk scheduling allocating framework writes slower next-generation due operating systems extra cpu cost proceedings involved tracking joint fault-isolated international placement conference reads measurement overwrites perform modeling comparably computer systems raidthe high sigmetrics unlink times performance graid pages fat madison fat wisconsin writes data june pertaining smaragdakis deleted les kaplan processed wilson eelru d-graid simple ective newly adaptive allocated page data replacement proceedings implementation untuned acm sigmetrics infrastructure conference suffers measurement cpu modeling memory contention computer systems host sigmetrics worst pages case atlanta estimates georgia overheads cost staelin d-graid garcia-mollina explore smart filesystems overhead proceedings metadata replication usenix winter purpose technical choose conference usenix postmark winter metadata intensive pages system dallas texas benchmark slightly january modi postmark stonebraker operating perform system sync support database management communications acm july tamches miller fine-grained dynamic instrumentation commodity operating system kernels proceedings symposium operating systems design implementation osdi pages orleans louisiana february van meter gao latency management storage systems proceedings symposium operating systems design implementation osdi pages san diego california october van renesse masking overhead protocol layering proceedings sigcomm pages stanford california august venkataramani kokku dahlin tcp-nice mechanism background transfers proceedings symposium operating systems design implementation osdi pages boston massachusetts december wahbe lucco anderson graham cient software-based fault isolation proceedings acm symposium operating systems principles sosp pages asheville north carolina december white lepreau stoller ricci guruprasad newbold hibler barb joglekar integrated experimental environment distributed systems networks proceedings symposium operating systems design implementation osdi pages boston massachusetts december young tevanian rashid golub eppinger chew bolosky black baron duality memory communication implementation multiprocessor operating system proceedings acm symposium operating systems principles sosp pages austin texas november 
deletion phase metadata writes accounted making pessimistic evaluation costs table shows performance alexander degrees metadata replication table synchronous replication metadata blocks signi effect performance metadata intensive workloads sizes postmark range bytes note alexander performs default raidfor lower degrees replication physical block allocation ext contiguous free chunk 
blocks allocate layout sub-optimal small les table shows number disk writes incurred benchmark percentage extra disk writes roughly accounts difference perbandwidth file size access-driven diffusion raidd-graid file-based access-driven diffusion d-graid directory-based access-driven diffusion d-graid file-based d-graid directory-based figure access-driven diffusion gure presents performance d-graid level standard raidunder sequential workload experiment number les size read sequentially total volume data xed d-graid performs smaller les due physical block layout formance replication levels extra writes metadata blocks count number unique physical writes metadata blocks absolute difference replication levels small suggests lazy propagation updates metadata block replicas idle time freeblock scheduling greatly reduce performance difference cost added complexity lazy update propagation replicas updated d-graid incur extra disk writes played back portion traces minutes standard raidsystem d-graid disks playback engine issues requests times speci trace optional speedup factor speedup implies idle time requests reduced factor speedup factors d-graid delivered persecond operation throughput raidutilizing idle time trace hide extra cpu overhead scaling factor operation throughput lagged slightly d-graid showing slowdown rst one-third trace execution caught due idle time effective access-driven diffusion show bene access-driven diffusion trial experiment perform set sequential reads les increasing size compare standard raidstriping d-graid access-driven diffusion figure shows results experiment gure access-driven diffusion sequential access larger les run rate single disk system bene potential parallelism access-driven diffusion performance improved reads directed appears usenix symposium file storage technologies fast reconstruction time live volume percentage costs reconstruction d-graid level worst case d-graid level case idealized raid level figure live-block recovery gure shows time recover failed disk hot spare d-graid level mirrored system live-block recovery lines graid plotted worst case live data spread entire volume case compacted smallest contiguous space plotted recovery time idealized raid level diffused copies disks system note case arrange les diffused start experiment reading threshold number times investigating sophisticated policies initiate access-driven diffusion left future work fast live-block recovery explore potential improvement live-block recovery figure presents recovery time d-graid varying amount live system data gure plots lines worst case case live-block recovery worst case live data spread disk case compacted single portion volume graph live-block recovery successful reducing recovery time disk half full note difference worst case case times difference suggests periodic disk reorganization speed recovery moving live data localized portion bene expect d-graid demonstrate improved availability alexander failures figure shows availability performance observed process randomly accessing les running d-graid raidto ensure fair comparison d-graid raidlimit reconstruction rate gure shows reconstruction volume live data completes faster graid compared raids extra failure occurs availability raiddrops d-graid continues availability surprisingly restore raidstill fails les linux retry inode blocks fail remount required availability ops succeed file throughput files sec time sec raid availability raid throughput d-graid availability d-graid throughput operation failure firstfailure reconcomplete failureafter recon secondfailure restore offailed disk re-mount reconcomplete figure availability pro gure shows operation d-graid level raid failures array consists data disks hot spare rst failure data reconstructed hot spare d-graid recovering faster raid failures occur raid loses les d-graid continues serve les workload consists read-modify-writes les randomly picked working set raidreturns full availability complex implementation brie quantify implementation complexity alexander table shows number statements required implement components alexander table core system inferencing module ext requires lines code counted number semicolons core mechanisms d-graid contribute lines code rest spent hash table avl tree wrappers memory management compared tens lines code comprising modern array rmware added complexity d-graid signi discussion section rst compare semantic-disk based approach alternative methods implementing graid discuss concerns commercial feasibility semantic disk systems alternative approaches semantic disk based approach ways implementing d-graid trade-offs similar modern processors innovate beneath unchanged instruction sets semantic disk level implementation facilitates ease deployment inter-operability unchanged client infrastructure making pragmatic cost approach complexity rediscovering semantic knowledge tolerant inaccuracies alternative approach change interface systems storage convey richer information layers instance storage system expose failure boundaries system appears usenix symposium file storage technologies fast semicolons total d-graid generic setup fault-isolated placement physical block allocation access driven diffusion mirroring live block recovery internal memory management hashtable avl tree file system speci sds inferencing ext sds inferencing vfat total table code size alexander implementation number lines code needed implement alexander shown rst column shows number semicolons column shows total number lines including white-spaces comments system explicitly allocate blocks fault-isolated manner alternatively system tag write logical fault-container storage system implement faultisolated data placement techniques intrusive existing infrastructure software base conceivably complex approach object-based storage interface considered makes boundaries visible storage layer objectbased interface semantically-smart technology relevant discover relationships objects instance inferring directory object points set objects collocated commercial feasibility nition d-graid semantically-smart storage systems detailed knowledge system embedding higher degree functionality storage system leads concerns commercial feasibility systems rst concern arises placing semantic knowledge disk system ties disk system intimately system system on-disk structure storage system change issue problematic on-disk formats evolve slowly reasons backwards compatibility basic structure ffs-based systems changed introduction period twenty years linux ext system introduced roughly exact layout lifetime nally ext journaling system backwards compatible ext on-disk layout extensions freebsd system backwards compatible evidence storage vendors maintain support software speci system emc symmetrix storage system software understand format common systems concern storage system semantic knowledge system interacts fortunately large number systems supported cover large fraction usage population semantic storage system system support storage system detect system conform expectations turn special functionality case d-graid revert normal raid layout detection simple techniques observing system identi partition table nal concern arises processing required disk system major issue general trend increasing disk system intelligence processing power increases disk systems substantial computational abilities modern storage arrays exhibit fruits moore law emc symmetrix storage server congured processors ram related work d-graid draws related work number areas including distributed systems traditional raid systems discuss turn distributed file systems designers distributed systems long ago realized problems arise spreading directory tree machines system walker discuss importance directory namespace replication locus distributed system coda mobile system takes explicit care regard directory tree speci cally cached coda makes cache directory root directory tree coda guarantee remains accessible disconnection occur interesting extension work reconsider host-based inmemory caching 
availability mind slice route namespace operations les directory server recently work wide-area systems re-emphasized importance directory tree pangaea system aggressively replicates entire tree root node accessed island-based system points fault isolation context widearea storage systems island principle similar fault-isolated placement d-graid finally systems past place entire single machine similar load balancing issues problem dif cult space due constraints placement block appears usenix symposium file storage technologies fast migration simpler centralized storage array traditional raid systems draw long history research classic raid systems autoraid learned complex functionality embedded modern storage array background activity utilized successfully environment afraid learned exible trade-off performance reliability delaying updates raid research focused redundancy schemes early work stressed ability tolerate single-disk failures research introduced notion tolerating multiple-disk failures array stress work complementary line research traditional techniques ensure full system availability number failures d-graid techniques ensure graceful degradation additional failures related approach parity striping stripes parity data achieve fault isolation layout oblivious semantics data blocks level redundancy irrespective importance meta-data data multiple failures make entire system inaccessible number earlier works emphasize importance hot sparing speed recovery time raid arrays work semantic recovery complementary approaches finally note term graceful degradation refer performance characteristics redundant disk systems failure type graceful degradation discuss paper systems continues operation unexpected number failures occurs conclusions robust system continues operate correctly presence class errors robert hagmann d-graid turns simple binary failure model found storage systems continuum increasing availability storage continuing operation partial failure quickly restoring live data failure occur paper shown potential bene d-graid established limits semantic knowledge shown successful d-graid implementation achieved limits simulation evaluation prototype implementation found d-graid built underneath standard block-based interface system modi cation delivers graceful degradation live-block recovery access-driven diffusion good performance conclude discussions lessons learned process implementing d-graid limited knowledge disk imply limited functionality main contributions paper demonstration limits semantic knowledge proof implementation limitations interesting functionality built inside semantically-smart disk system semantic disk system careful assumptions system behavior hope work guide pursue similar semantically-smart disks easier build systems reorder delay hide operations disks reverse engineering scsi level dif cult small modi cations systems substantially lessen dif culty system inform disk believes system structures consistent ondisk state challenges disk lessened small alterations ease burden semantic disk development semantically-smart disks stress systems unexpected ways file systems built operate top disks behave d-graid speci cally behave part volume address space unavailable heritage inexpensive hardware linux systems handle unexpected conditions fairly exact model dealing failure inconsistent data blocks missing reappear true inodes semantically-smart disks push functionality storage systems evolve accommodate detailed traces workload behavior invaluable excellent level detail traces simulate analyze potential d-graid realistic settings traces per-process information anonymize extent pathnames included trace utilize study remaining challenge tracing include user data blocks semantically-smart disks sensitive contents privacy concerns campaign encounter dif cult overcome acknowledgments anurag acharya erik riedel yasushi saito john bent nathan burnett timothy denehy brian forney florentina popovici lakshmi bairavasundaram insightful comments earlier drafts paper richard golding excellent shepherding anonymous reviewers thoughtful suggestions greatly improved content paper finally appears usenix symposium file storage technologies fast computer systems lab providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation acharya uysal saltz active disks programming model algorithms evaluation asplos viii san jose october alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering isca pages anderson chase vahdat interposed request routing scalable network storage acm transactions computer systems february bitton gray disk shadowing vldb pages los angeles august boehm weiser garbage collection uncooperative environment software practice experience september burkhard menon disk array storage system reliability ftcspages toulouse france june chapin rosenblum devine lahiri teodosiu gupta hive fault containment shared-memory multiprocessors sosp december chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks usenix june dowse malone recent filesystem optimisations freebsd freenix monterey june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge self-organizing disk controller usenix winter january ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december ganger mckusick soules patt soft updates solution metadata update problem file systems acm tocs ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement hicss gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka costeffective high-bandwidth storage architecture asplos viii october gray computers stop international conference reliability distributed databases june gray horst walker parity striping disc arrays low-cost reliable storage acceptable throughput proceedings international conference large data bases vldb pages brisbane australia august gribble robustness complex systems hotos viii schloss elmau germany hagmann reimplementing cedar file system logging group commit sosp november holland gibson siewiorek fast on-line failure recovery redundant disk arrays ftcsfrance hsiao dewitt chained declustering availability strategy multiprocessor database machines international data engineering conference ibm serveraid recovering multiple disk failures http ibm qtechinfo migrhtml felten wang singh archipelago islandbased file system highly scalable internet services usenix windows symposium august katcher postmark file system benchmark technical report trnetwork appliance oct keeton wilkes automating data dependability proceedings acm-sigops european workshop pages saint-emilion france september kistler satyanarayanan disconnected operation coda file system acm tocs february mckusick joy lef fabry fast file system unix acm tocs august menon mattson comparison sparing alternatives disk arrays isca gold coast australia microsoft corporation http microsoft hwdev december orji solworth doubly distorted mirrors sigmod washington park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- princeton november patterson gibson katz case redundant arrays inexpensive disks raid sigmod june patterson availability maintainability performance focus century key note fast january popek walker chow edwards kline rudisin thiel locus network transparent high reliability distributed system sosp december reddy banerjee gracefully degradable disk arrays ftcspages montreal canada june riedel gibson faloutsos active storage largescale data mining multimedia proceedings international conference large databases vldb york york august riedel kallahalla swaminathan framework evaluating storage system security fast pages monterey january rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february rowstron druschel storage management caching past large-scale persistent peer-to-peer storage 
utility sosp banff canada october ruemmler wilkes disk shuf ing technical report hpl- hewlett packard laboratories saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area system osdi boston december savage wilkes afraid frequently redundant array independent disks usenix pages san diego january sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems fast san francisco march tweedie future directions ext filesystem freenix monterey june wang anderson patterson virtual log-based file systems programmable disk osdi orleans february wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wolf placement optimization problem practical solution disk file assignment problem sigmetrics pages berkeley 
flexibility manageability performance grid storage appliance john bent venkateshwaran venkataramani nick leroy alain roy joseph stanley andrea arpaci-dusseau remzi arpaci-dusseau miron livny department computer sciences wisconsin-madison abstract present nest flexible software-only storage appliance designed meet storage grid nest key features make well-suited deployment grid environment nest generic data transfer architecture supports multiple data transfer protocols including gridftp nfs easy addition protocols nest dynamic adapting on-the-fly runs effectively wide range hardware software platforms nest grid-aware implying features integration grid storage space guarantees mechanisms resource data discovery user authentication quality service part nest infrastructure introduction data storage movement increasing importance grid time scientific applications evolved process larger volumes data throughput inextricably tied timely delivery data usage grid evolves include commercial applications data management central today data management aspects performance long focus storage systems research recent trends factors including reliability availability manageability relevant argue manageability dominant criterion evaluating storage solutions cost storage management outweighs cost storage devices factor potential solution storage management problem specialized storage devices appliances pioneering products filers network appliance reduce burden management specialization specifically storage appliances designed solely serve files clients toaster designed solely toast results convincing field testing network appliance filers shown easier manage traditional systems reducing operator error increasing system uptime considerably storage appliances natural match storage grid easy manage provide high performance number obstacles prevent direct application commercial filers grid environment commercial storage appliances inflexible protocols support defaulting common local area unix windows environments nfs cifs filers readily mix world-wide shared distributed computing infrastructure non-standard specialized grid protocols data transfer commercial filers expensive increasing cost raw cost disks factor ten greater storage appliances missing features crucial integration grid environment ability interact larger-scale global scheduling resource management tools overcome problems bring appliance technology grid introduce nest open-source user-level software-only storage appliance compared current commercial storage appliances nest primary advantages flexibility cost grid-aware functionality briefly discuss advantages detail nest flexible commercial storage appliances nest generic data transfer architecture concurrently supports multiple data transfer protocols including gridftp nfs nest framework protocols added grid evolves nest open-source softwareonly appliance low-cost alternative commercial storage appliances expenses incurred raw hardware costs disks nest software-based appliance introduces problems traditional appliances encounter nest run hardware tailored tested nest ability adapt characteristics underlying hardware operating system allowing nest deliver control flow data flow dispatcher transfer managerstorage manager concurrency models physical storage chirp http nfsgrid ftpftp common protocol layer physical network figure nest software design diagram depicts nest major components protocol layer storage manager transfer manager dispatcher control data flow paths depicted high performance retaining ease management benefits storage appliances finally nest grid-aware key features storage space guarantees mechanisms resource data discovery user authentication quality service fundamental part nest infrastructure functionality enables nest integrated smoothly higher-level job schedulers distributed computing systems rest paper organized section describes design nest protocol layer mediates interaction clients section section describes transfer manager responsible monitoring scheduling concurrency section describes storage layer manages actual physical storage system usage nest traced section evaluation presented section comparisons related work section conclusions drawn section design overview grid storage appliance nest mechanisms file directory operations resource management implementation provide mechanisms heavily dependent nest modular design shown figure major components nest protocol layer storage manager transfer manager dispatcher briefly examine components separately show work tracing client interaction component descriptions protocol layer nest connectivity network client interactions mediated clients communicate nest supported file transfer protocols including http restricted subset nfs ftp gridftp chirp native protocol nest role protocol layer transform specific protocol client common request interface understood components nest refer virtual protocol connection describe motivation multiple protocol support section dispatcher main scheduler macrorequest router system responsible controlling flow information components examines client request received protocol layer routes appropriately storage transfer manager data movement requests transfer manager requests resource management directory operation requests handled storage manager dispatcher periodically consolidates information resource data availability nest publish information classad global scheduling system storage manager main responsibilities virtualizing controlling physical storage machine underlying local filesystem raw disk physical memory storage system directly executing non-transfer requests implementing enforcing access control managing guaranteed storage space form lots lots discussed detail section storage operations execute quickly order milliseconds chosen simplify design storage manager requests execute synchronously responsibility dispatcher ensure storage requests serialized executed storage manager thread-safe schedule transfer manager controls data flow nest specifically transfers data protocol connections allowing transparent threeand four-party transfers file data transfer operations managed asynchronously transfer manager synchronously approved storage manager transfer manager concurrency models threads processes events schedules transfer models scheduling policies preferential scheduling scheduling optimizations responsibility transfer manager discussed section client interaction examine components function tracing sequence events interacting client case client creates directory nontransfer request inserts file directory transfer request client initially connects nest request create directory dispatcher wakes asks protocol layer receive connection depending connecting port protocol layer invokes handler protocol handler authenticates client parses incoming request common request format returns virtual protocol connection dispatcher dispatcher asks storage manager create directory checking access permissions storage manager synchronously creates directory sends acknowledgment back client dispatcher virtual protocol connection point dispatcher assumes responsibility client listens requests channel client sees directory created successfully requests permission send file dispatcher invokes virtual protocol connection receive request queries storage manager storage manager transfer returns virtual protocol connection transfer written dispatcher passes connections transfer manager stops listening client channel sleeps waiting client request transfer manager free schedule queue request request scheduled transfer manager past information predict concurrency model provide service passes connection selected model transfer continues chosen concurrency model transfers data client connection storage connection performing acknowledgment client desired finally transfer status returned transfer manager dispatcher sections describe important aspects nest motivate importance supporting multiple communication protocols virtual protocol layer describe transfer manager adapts client workload underlying system pick concurrency model performance show transfer manager apply scheduling policies connections fourth explain role storage guarantees nest explain storage manager implements functionality protocol layer supporting multiple protocols fundamental requirement storage appliances grid standardization common protocols globus toolkit diversity reign community widespread fastmoving grid wide-area transfers conducted gridftp local-area file access dominated nfs afs cifs protocols multiple protocols 
supported nest virtual protocol layer design implementation virtual protocol layer clients communicate nest preferred file transfer protocol shields components nest detail protocol allowing bulk nest code shared protocols virtual protocol layer nest virtual file system vfs layer operating systems alternative approach single nest server virtual protocol layer implement separate servers understand individual protocol run simultaneously refer approach bunch servers jbos relative advantage jbos servers added upgraded easily immediately implementation protocol nest incorporating upgraded protocol effort protocol operations mapped nest common framework advantages single server outweigh implementation penalty number reasons single server enables complete control underlying system server give preferential service requests protocols users protocols single interface tasks administering configuring nest simplified line storage appliance philosophy single server optimizations part system transfer manager concurrency model applied protocols fourth single server memory footprint considerably smaller finally implementation penalty reduced protocol implementation nest leverage existing implementations implement gridftp server-side libraries provided globus toolkit sun rpc package rpc communication nfs point implemented file transfer protocols nest http subset nfs ftp gridftp nest native protocol chirp experience request types protocols similar directory operations create remove read file operations read write put remove query fit easily virtual protocol abstraction interesting exceptions instance nfs protocol lookup mount request chirp protocol supports lot management plan include grid-relevant protocols nest including data movement protocols ibp resource reservation protocols developed part global grid forum expect protocols added implementation effort focused mapping specifics protocol common request object format protocols require additions common internal interface authentication mechanism protocol specific protocol handler performs authentication clients drawback approach devious protocol handler falsify client authenticated grid security infrastructure gsi authentication chirp gridftp connections protocols allowed anonymous access transfer manager heart data flow nest transfer manager transfer manager responsible moving data disk network request transfer manager protocol agnostic machinery developed manager generic moves data protocols highlighting advantages nest design mount technically part nfs protocol nest mount handled nfs handler multiple concurrency models inclusion grid environment mandates support multiple on-going requests nest provide means supporting concurrent transfers single standard concurrency operating systems platforms choice threads processes cases events making decision difficult fact choice vary depending workload requests hit cache perform events bound perform threads processes avoid leaving decision administrator avoid choosing single alternative perform poorly workloads nest implements flexible concurrency architecture nest supports models concurrency threads processes events future plan investigate advanced concurrency architectures seda crovella experimental server deliver high performance nest dynamically chooses architectures choice enabled distributing requests architectures equally monitoring progress slowly biasing requests effective choice scheduling multiple outstanding requests nest nest selectively reorder requests implement scheduling policies scheduling multiple concurrent transfers server decide resources dedicate request basic strategy service requests first-come first-served fcfs manner nest configured employ transfer manager control on-going requests scheduling policies nest supports proportional share cache-aware scheduling addition fcfs proportional-share scheduling deterministic algorithm fine-grained proportional resource allocation previously cpu scheduling network routers current implementation nest administrator proportional preferences protocol class nfs requests bandwidth gridftp requests future plan extend provide preferences per-user basis byte-based strides scheduling policy accounts fact requests transfer amounts data nfs client reads large file entirety issues multiple requests http client reading file issues give equal bandwidth nfs requests http requests transfer manager schedules nfs requests times frequently ratio average file size nfs block size nest proportional-share scheduling similar bandwidth request throttling module apache proportional-share scheduling nest offers flexibility schedule multiple protocols apache requestthrottling applies http requests apache server processes applied traffic streams jbos environment cache-aware scheduling utilized nest improve average client perceived response time server throughput modeling kernel buffer cache gray-box techniques nest predict requested files cache resident schedule requests files fetched secondary storage addition improving client response time approximating shortest-job scheduling scheduling policy improves server throughput reducing contention secondary storage earlier work examined cache-aware scheduling focus web workloads independence transfer manager virtual protocol layer clear policy works protocols illustrates major advantage nest jbos optimizations transfer code immediately realized protocols reimplemented multiple servers storage manager protocol layer multiple types network connections channeled single flow storage manager designed virtualize types physical storage provide enhanced functionality properly integrate grid environment specific roles fulfilled storage manager implement access control virtualize storage namespace provide mechanisms guaranteeing storage space access control provided nest generic framework built top collections classad afs-style access control lists determine read write modify insert privileges typical notions users groups maintained nest support access control generic policies enforced protocols nest supports clients communicate native chirp protocol supported protocol access control semantics set nest virtualizes physical namespace underlying storage enabling nest run wide variety storage elements current implementation local filesystem underlying storage layer nest plan physical storage layers raw disk future running remote location grid users higher-level scheduling systems assured exists sufficient storage space save data produced computation stage input data subsequent access address problem nest interface guarantee storage space called lot requests made space allocations similar reservations network bandwidth lot defined characteristics owner capacity duration files owner client entity allowed lot individual owners allowed group lots included release capacity total amount data stored lot duration amount time lot guaranteed exist finally lot set files number files lot bounded file span multiple lots fit single duration lot expires files contained lot immediately deleted allowed remain indefinitely space reclaimed creation lot refer behavior best-effort lots investigating selection policies reclaiming space create files nest user access lot file transfer protocols support creating lots environment lot obtained ways system administrators grant access nest simultaneously make set default lots users client directly chirp protocol create lot accessing server alternative data-transfer protocol lots implemented current implementation relies quota mechanism underlying filesystem nest limit total amount disk space allocated user utilizing quota system affords number benefits direct access file system nest observe quota restrictions allowing clients utilize nest make space guarantee bypass nest transfer data directly local file system existing software file system nest implementation simplified approach benefits lots user overfill single lot fill lot capacity future plan investigate costs benefits nest-managed lot enforcement nest global execution manager nest argonne cluster chirp gftp nfs submit madison figure nest grid diagram illustrates information flow scenario multiple nest servers 
utilized grid nest grid basic understanding nest place illustrate multiple nest servers global grid environment figure depicts scenario major events labeled sequence numbers defined description figure user input data permanently stored home site case nest madison wisconsin step user submits number jobs remote execution global execution manager manager aware remote cluster labeled argonne cluster large number cycles nest gateway appliance argonne previously published resource data availability global grid discovery system manager aware argonne nest sufficient amount storage manager decides run user jobs argonne site staging user input data step manager chirp create lot user files argonne guaranteeing sufficient space input output files step manager orchestrates gridftp third-party transfer madison nest nest argonne cluster data movement protocols kangaroo utilized move data site site step manager begins execution jobs argonne jobs access user input files nest local file system protocol case nfs jobs execute output files generate stored nest note ability give preference users protocols harnessed local administrators ensure preference jobs global manager ensure timely data movement finally step jobs begin complete point manager moves output data back madison utilizing gridftp wide area movement manager free chirp terminate lot nest nest wu-ftpd nest apache nest linux nfsd nest jbos server bandwidth nest jbos chirp gridftp http nfs total figure multiple protocols experiment measures bandwidth clients request files protocol sets bars single protocol workload single server jbos set bars workload protocols pair bar shows performance nest bar jbos step inform user output files local nest note steps guaranteeing space moving input data executing jobs moving output data terminating reservations encapsulated request execution manager condor directedacyclic-graph manager dagman higherlevel storage resource managers srm nest services synchronize access globallyshared storage resources experiments section perform evaluation key components nest experiments performed cluster pentium-based machines running linux ibm lzx disks connected gigabit ethernet solaris-based runs performed cluster netra machines running solaris connected mbit ethernet support multiple protocols illustrate supporting multiple protocols nest framework incurs overhead compared native implementations individual protocol figure compares bandwidth delivered clients nest server delivered native servers implementing individual protocol sets bars graph evaluate workloads requests protocol fifo server bandwidth scheduling configuration proportional protocol scheduling total chirp gftp http nfs desired figure proportional protocol scheduling workload identical figure results shown nest set bars bar represents total delivered bandwidth protocols remaining bars show bandwidth protocol labels sets bars show proportional ratios desired lines show ideal proportions note nest achieve close desired ratios case right-most time jbos single server running make observations results delivered bandwidth varies widely protocols chirp http deliver in-cache files peak bandwidth determined network gridftp nfs achieve approximately half bandwidth importantly performance nest protocols similar native server rightmost pair bars show delivered bandwidth workload requests multiple protocols jbos multiple servers running simultaneously total delivered bandwidth nest jbos similar roughly allocation bandwidth protocols bandwidth delivered nfs clients lower nest jbos nfs block-based protocol protocols file-based default transfer manager nest ends disfavoring nfs schedules requests fifo order quality service advantage nest relative jbos transfer manager nest easily extended scheduling policies implemented simple stride scheduler nest proportional share server bandwidth delivered types requests results shown figure set bars shows base case nest transfer manager simple fifo scheduler sets bars adjust desired ratio bandwidth protocol make conclusions graph proportional share scheduler imposes slight performance penalty fifo scheduling delivering total approximately proportional-share scheduler achieves close desired ratios cases specifically jain metric fairness represents ideal allocation achieve values greater ratios exception allocating additional bandwidth nfs chirp gridftp http nfs extremely difficult jain fairness case drops challenge sufficient number nfs requests transfer manager schedule interval case nfs request current implementation work-conserving schedules competing request server idle implementing non-work-conserving policy idle server waits period time scheduling competitor policy pay slight penalty average response time improved allocation control concurrency architecture adaptation show benefits automatically adapting concurrency architecture platform workload run simple experiments results shown figure experiment shown left run nest solaris server clients small files cache results illustrate workload event-based model lower average response time threaded model nest adaptive scheme performing experiment shown run nest linux server clients larger files case threaded model higher bandwidth event model adaptive scheme close performance model experiments discern cost adaptation nest models periodically order find current workload components receive proportional shares fairness allocation defined fairness ratio delivered allocation desired allocation components ideal allocation events threads adaptive average time request model solaris events threads adaptive bandwidth model linux figure adaptive concurrency graph left experiment measures average request latency solaris requests events threads adaptive nest approach graph experiment measures bandwidth linux requests models cases nest adaptively picks model overhead note process model disabled experiments sake clarity lot management overhead measured overhead quota mechanism implement lots nest found quotas enabled write performance disk decreases roughly worst case single sequential write stream shown figure read performance unaffected surprisingly cost writing quotas enabled hidden server network-bound concurrent write streams investigating additional complexity implementing lots directly monitoring write operations nest worth performance improvement ability distinguish lots correctly related work storage appliance nest relates closely filers network appliance enterprise storage platforms emc nest attempt compete commercial offerings terms raw performance primarily intended target domain nest offers low-cost softwareonly alternative offers protocol flexibility grid-aware features needed enable scientific computations grid server bandwidth write size performance overhead lots quotas disabled quotas enabled figure overhead lots graph shows overhead imposed implementing lots kernel quota system notice small files cost negligible increases quickly file size grid community number projects related nest gara architecture advance reservations variety resources including computers networks storage devices nest gara reservations similar nest lots users make advance gara provide best-effort lots sophisticated user management nest disk resource managers srm storage depots ibp legionfs servers provide grid storage services projects designed provide local storage management global scheduling middleware conversely nest local storage management solution designed integrate number global scheduling systems distinction account key difference nest storage servers systems designed work primarily self-contained middleware projects protocol independence servers unique feature nest dynamic concurrency adaptation note intrinsic design nest incorporated systems srm ibp provide space guarantees manners similar nest lots difference srm srm guarantees space allocations 
multiple related files two-phased pinning lots nest provide functionality client flexibility control implementation complexity comparing nest lots ibp space guarantees difference ibp reservations allocations byte arrays makes extremely difficult multiple files contained allocation client build file system byte array difference ibp permanent volatile allocations nest permanent lots users allowed indefinitely renew best-effort lots analogous volatile allocations mechanism ibp switching allocation permanent volatile lots nest switch automatically best-effort duration expires nest legionfs recognizes importance supporting nfs protocol order unmodified applications benefit grid storage resources legionfs builds support client side nest server side legionfs client-based nfs easier server implementation makes deployment difficult legion-modified nfs module deployed client locations nest grid storage system supports multiple protocols server pfs srb middleware client side approaches complementary enable middleware server negotiate choose protocol transfer nfs locally gridftp remotely conclusion presented nest open-source user-level software-only storage appliance nest specifically intended grid designed concepts flexibility adaptivity grid-awareness flexibility achieved virtual protocol layer insulates transfer architecture particulars file transfer protocols dynamic adaptation transfer manager additional flexibility enabling nestto run effectively wide range hardware software platforms supporting key grid functionality storage space guarantees mechanisms resource data discovery user authentication quality service nest grid-aware integrate cleanly distributed computing systems experimental results demonstrated inclusion multiple protocols single storage appliances enables proportional-share scheduling jbos model presented experimental results showing nest adjusts workload solaris linux systems adjusts highest performing concurrency model finally illustrate vision nest role grid computation scenario utilizes grid middleware multiple nests coordinate reservation scheduling cpus reservation scheduling storage resources nest development release runs linux solaris version support nfs protocol operational supported download http wisc condor nest production release released end acknowledgments members condor team numerous list members controlling place file system gray-box techniques james nugent andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract present design implementation place gray-box library controlling layout top ffslike systems place exploits knowledge ffs layout policies users place les directories speci localized portions disk applications place collocate les exhibit temporal locality access improving performance series microbenchmarks analyze overheads controlling layout top system showing overheads prohibitive discuss limitations approach finally demonstrate utility place case studies demonstrate potential layout rearrangement web-server environment build benchmarking tool exploits control placement quickly extract low-level details disk system traditional gray-box manner place library achieves ends user level changing single line operating system source code introduction creators high-performance o-intensive applications including database management systems web servers long yearned control placement data disk proper data allocation exploit locality access workload increasing disk ciency improving performance systems provide explicit controls needed applications affect desired layouts unix systems based berkeley fast file system ffs group les set heuristics speci cally group inodes data blocks les reside directory applications full control layout traditionally avoided systems altogether relinquishing convenience control gray-box techniques promising approach gather information exert control systems export interfaces treating system gray box assumes general knowledge system behaves implemented knowledge combined run-time observations system enables construction powerful services exported base system paper explore application gray-box techniques placement problem speci cally retain convenience system regaining control placement introduce place positional layout controller system exploits gray-box techniques give applications improved control placement system depicted figure important component place place information control layer icl place icl applications group les directories localized portions disk speci cally group proper placement data improve read write performance collocating les accessed time applications improve performance short-stroking disk reducing cost seeks limiting arm movement portion disk applications place library operate expected key place implementation shadow directory tree sdt sdt hidden control structure place icl control les disk carefully creating structure exploiting gray-box knowledge system behavior sdt enables place icl place les user preferences correct cient manner creating maintaining structure central challenges implementing place rst evaluate place icl set microbenchmarks understand basic costs potential bene place costs place reasonable controlled directory creation costly standard versions proceedings usenix annual technical conference june san antonio texas unmodified ffs file system place icl place tools command line place application pmkfs group group group figure place system place system consists components highlighted gray gure important component place information control layer icl gray-box techniques discover information system exploits knowledge enable applications link control directory layout components place tool pmkfs initialize place on-disk structures set place command-line tools place works top ffs-like systems learning internal group structure exposing structure place icl operation prototype implementation potential bene substantial random performance improves dramatically related data items grouped small portion disk large les outer tracks disk improve throughput due zoning effects demonstrate utility place separate application studies rst show web server place wind group group les exhibit nathan temporal access burnett locality timothy denehy utilizing brian simple forney placement florentina heuristics popovici collocation muthian place improves sivathanu web server throughput specially noticeably mention erik paulson show douglas thain peter high-speed couvares user-level todd benchmarking tool tannenbaum called fast condor team place rapidly construct people testing infrastructure anonymous reviewers controlled placement contributed les fast suggestions extract specifically important disk paper characteristics development seek time nest bandwidth project seconds general rest paper extend organized gratitude members section describe computer systems design lab implementation place outstanding job keeping section measure computers running costs show networks bene work sponsored section nsf present case studies ccrccr- place usage ngsccr- describe related work section future directions section itrand wisconsin alumni research foundation allcock bester bresnahan chervenak liming tuecke grid ftp protocol extensions ftp grid http www-fp mcs anl gov dsl gridftpprotocol-rfc-draft pdf march arpaci-dusseau arpaci-dusseau information control gray-box systems symposium operating systems principles sosp oct baru moore rajasekar wan sdsc storage resource broker proceedings cascon toronto canada burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache management usenix chervenak foster kesselman tuecke protocols services distributed data-intensive science proceedings acat chiu jain analysis increase decrease algorithms congestion avoidance computer networks journal computer networks isdn volume pages june condor condor directed-acyclic-graph manager dagman http wisc condor dagman crovella frangioso harchol-balter connection scheduling web servers usenix symposium internet technologies systems emc corporation http emc fielding gettys mogul frystyk berners-lee rfchttp hypertext transfer protocol specification version network working group requests comments january foster kesselman globus metacomputing intrastructure toolkit international journal supercomputer applications foster kesselman tsudik tuecke security architecture computational grids proceedings acm conference computer communications security conference pages frey tannenbaum livny tuecke condor-g computation managament agent multiinstitutional grids proceedings tenth ieee symposium high performance distributed computing hpdc pages san francisco california august grimshaw wulf legion vision worldwide virtual computer communications acm january hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference pages san fransisco usa howe bandwidth request throttling apache http snert software throttle iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous acm symposium operating systems principles october kleiman vnodes architecture multiple file system types sun unix usenix conference proceedings pages kohler morris chen jannotti kaashoek click modular router acm transactions computer systems august lancaster rowe measuring real world data availability proceedings lisa systems administration conference pages san diego california december lohr supercomputing business move closer york times business financial desk february pai druschel zwaenepoel flash efficient portable web server proceedings usenix technical conference patterson availability maintainability performance focus century key note lecture usenix conference file storage technologies fast january plank bassi beck moore swany wolski managing data storage network ieee internet computing september october postel rfcftp file transfer protocol specification june raman matchmaking frameworks distributed resource management phd thesis wisconsin october raman livny solomon matchmaking distributed resource conclude section place design implementation section describe place system controlling layout rst provide background describe goals implementing place describe api exposed place icl presenting programming interface discuss place implementation including system initialization shadow structures control placement discuss general operation issues concurrency limitations current implementation background modern unix systems based berkeley fast file system including direct descendants found bsd solaris families intellectual descendants linux ext main innovations ffs emphasis locality placing related data objects disk ffs provided quantum leap performance systems scattered data disk oblivious manner primary construct ffs manage disk locality cylinder group block group ext terms interchangeably simplicity cylinder group divides disk number contiguous regions consists inodes data blocks bitmaps tracking inode block usage small number blocks store implementation-speci information placing related data objects cylinder group conversely spreading unrelated objects groups locality access achieved dif culty deciding objects related typically simple heuristics based system namespace speci cally group related objects implementations place inodes data blocks les directory group assuming locality access les conversely directories groups spread unrelated les disk leaving room grow group original ffs implementation descendants spread large les groups avoid lling group single large designing place icl seek exploit gray-box knowledge ffs-like systems perform layout order users control les disk understand limits gray-box control including types functionality realized top modern systems proceedings usenix annual technical conference june san antonio texas design goals designing place goals simple intuitive control layout applications straightforward representation disk locality exploit application-speci knowledge improve performance easy place easy substantial code modi cations required programming apis commandline tools provided compatible non-place applications applications place top system operate basic system structure usage unmodi applications change unaffected system namespace applications users les conventions desire layout dependent specialized naming schemes goals impact design implementation place abstractions api basic abstraction place exposes underlying groups ffs-like systems applications link place library applications knowledge access patterns place related les directories speci group exploiting spatial locality improved performance number group applications pieces information applications safely assume les proximate groups close object group close object group close object group lower group numbers located outer tracks disk higher group numbers located tracks applications utilize zone-sensitive placement large les improve throughput note abstract virtual groupings group hierarchies layered top physical group interface desired simplicity focus solely lowest level abstraction rest paper applications place les directories speci groups place basic functions applications place createfile char pathname mode mode int group creates speci pathname mode set mode group number group rst arguments identical creat system call place createdir char pathname mode mode int group creates directory speci bypathnamewith mode set mode group number group rst arguments identical mkdir system call theplace createfilecall ne-grained placement les groups place createdir function applications create directory controlled manner subsequent allocations directory place collocated due standard ffs policy place allocate directory group 
due insuf cient resources free data blocks inodes left group case standard behavior routine return error indicating object created alternative interface routines search nearby group failure place directory utility convenience functions provided applications discover number groups system current utilization level group number group utilized user re-write application place api set command-line tools utilized tools users move directories les speci groups create speci groups subsequent data access unmodi applications enjoy bene rearrangement basic operation place exploits ffs tendency namespace hint placement gain control layout place rst create structure les directories created controlled fashion created place library renames les moving back proper location system namespace system structure shadow directory tree sdt central place implementation initialization process performed management high throughput computing proceedings seventh ieee international symposium high performance distributed computing hpdc july roy end-to-end quality service high-end applications phd thesis chicago satyanarayanan digest seventh ieee workshop hot topics operating systems rice conferences hotos digest digesthtml html march sharpe smb samba cifs docs whatis-smb html september shoshani sim storage resource managers middleware components grid storage nineteenth ieee symposium mass storage systems mss sun microsystems rfcnfs network file system protocol specification network working group requests comments march thain basney son livny kangaroo approach data movement grid proceedings tenth ieee symposium high performance distributed computing hpdc san francisco california august thain bent arpaci-dusseau arpaci-dusseau livny gathering creating communities grid proceedings supercomputing denver colorado november thain livny pluggable file system http wisc condor pfs vazhkudai tuecke foster replica selection globus data grid ieee international symposium cluster computing grid ccgrid waldspurger weihl stride scheduling deterministic proportional-share resource mangement technical report mit lcs tmmassachusetts institute technology mit laboratory computer science june walsh lyon sager chang goldberg kleiman lyon sandberg weiss overview sun network file system proceedings usenix winter conference pages jan welsh culler brewer seda architecture well-conditioned scalable internet services proceedings eighteenth symposium operating systems principles sospbanff canada october white walker humphrey grimshaw legionfs secure scalable file system supporting cross-domain high-performance applications proceedings supercomputing denver colorado november zhang deering estrin shenker zappala rsvp resource reservation protocol ieee networks magazine september 
system place produces sdt structure appears system namespace shown figure important entities found sdt superblock persistent inproceedings usenix annual technical conference june san antonio texas hidden superblock hidden concurrency hidden hidden hidden figure shadow directory tree hidden shadow directory tree structure presented superblock persistent information needed place concurrency manage multi-user access finally directories control placement formation place concurrency manage concurrent access les place api les discussed detail interesting set directories namedd throughdn number cylinder groups system initialization procedure detail ensures directory cylinder group note structures hidden directory applications traversing directory tree controlling file creation sdt place creating group straightforward application calls place createfile passing pathname created mode bits desired group place internally place icl creates shadow directory simply calls rename put proper location namespace place checks make allocated group user requested i-number newly allocated initialization place learns records number group mapping information determine allocation successful controlling directory creation placing directory proper group place createdir challenging creating directory proper shadow directory suf ffs-like systems place child directory cylinder group parent approach required shown algorithm algorithm works creating temporary directory checking desired group i-number repeat tmp picknewname mkdir tmp indesiredgroup tmp break end fillothergroups forever rename tmp dirname algorithm directory creation algorithm basic algorithm create directory speci group disk presented repeating process temporary directory created correct group directory created renamed proper location namespace complication arises due directory allocation policies ffs-like systems linux ext policy searches group above-average number free inodes fewest allocated data blocks netbsd ffs picks group above-average number free inodes fewest allocated directories algorithm create temporary les directories coerce system creating directory desired group process referred algorithm fillothergroups creates number les non-target groups ensure les spread groups uncontrolled manner place creates small les les utilize indirect pointers basic algorithm slow demonstrate section speed process common case build shadow cache directories group numbers sdt attempting create directory group directory creation algorithm rst consults shadow cache directory group exists place simply renames directory nished avoiding expensive directory creation algorithm place directory cache performs fulledged algorithm case directories created algorithm added cache repopulating shadow cache periodically sdt initialization discuss initialization process required place encapsulated tool call pmkfs place mkfs steps pmkfs proceedings usenix annual technical conference june san antonio texas pmkfs discovers system parameters algorithms pmkfs creates sdt on-disk data structures populates shadow cache parameter discovery place requires pieces information create on-disk structures support controlled allocation number groups system ngrp number blocks bgrp inodes igrp group total number blocks inodes system readily statfs system call finding number groups slightly challenging current algorithm calculates number allocating directories recording difference inode numbers subsequently allocated directories directory group common difference number inodes group place detects allocation wrapped fact directory i-number close previously allocated directory igrp calculate group number gnum object inode number inum computing gnum inum igrp system calculates number direct pointers inode size small required directory creation algorithm work multiple ffs platforms discovered synchronously writing blocks monitoring number free blocks system statfs small size discovered point single allocating block write decreases free block count blocks indicating indirect block allocated current implementation place requires exclusive access empty system initialization reason restriction igrp exported system procedure previously determine number made source system administrator place initialized top system sdt creation step pmkfs stores information superblock creates directory tree directoriesd throughdn assuming groups process creating directories identical directory creation algorithm found algorithm typical directory creation procedure excess directories created added shadow cache general place maintain minimal threshold shadow directories group avoid costly directory creation algorithm obtain understanding threshold examined system traces labs typical busy day found thousand long-lived directories created giving rough upper bound number shadow directories place maintain absorb day worth controlled directory creation environment issues crash recovery concurrency directory creation place create les directories sdt potential data accrue time occur created sdt system crashes rename place worse job killed midst place library call place include basic crash recovery mechanism order periodically remove les refer process sdt cleaning current implementation sdt cleaner scans sdt directory structures removes data objects left system crashes run cleaner alternatives current implementation invokes cleaner invocations place set conservative longer directory-allocation process run alternatives include running cleaner time interval day background process issues arise place usage multiple processes users concurrent place processes problem current implementation basic algorithm allocate directory situation competing controlled directory creations groups lead signi dif culty creating directory desired location avoid problem place acquires advisory lock concurrency mode lock signify usage basic algorithm practice usage basic algorithm repopulates shadow cache reducing mode operation cooperative approach processes share work gaining control introduce signi complexity multiple users introduce issue sdt shared private user sharing requires level trust applications sdt writable location shared sdt vulnerable types attacks changing structures place lead poor allocations lling sdt causing denial service proceedings usenix annual technical conference june san antonio texas environments problem single user application sole access system trustworthy settings sdt replicated per-user basis increases space utilization duplicates effort circumvents security issues arise due sharing limitations primary limitation place implemented ffs-like systems modern unix systems ffs-like recent features including journaling ext soft updates found bsd family ffs implementations affect ability control placement techniques limitation arises due internal implementation ffs implementations spread larger les cylinder groups order avoid lling single group quickly ffs behavior prevents place controlling large les laid disk provide interface query place largest size allocation guaranteed controllable notable exception standard 
ffs implementation strategy occurs ext spread larger les groups implementation strategy hints graybox implementors inside systems build top behavior simple understand easy control place directly ne-grained placement les group applications modify order creation pack les group controlled fashion alternative initially explored overcomes limitations mesh applications place alternative approach place initially lls target system set dummy les discovering exact locations place free space applications request space data allocations controlled deemed approach unacceptable unmodi applications work correctly applications system appears full analysis section analyze behavior place demonstrating functionality basic overheads rst discuss experimental environment proceed series microbenchmarks demonstrating effectiveness layout control revealing approach group number group utilization figure controlled allocation graph depicts experiments creates -kb les rst standard system interfaces number directories les created varied experiments labeled fourth experiment place api create les directories place single group middle disk labeled graph group number varied x-axis shaded bar data group darker bars indicating data debugfs command gather needed information costs system creation usage show improvement expected reorganizing data controlling layout account zonedbit recording finally discuss experience broader range platforms experimental environment present results place top linux ext system experiments platform performed mhz pentium-iii processor main memory ibm lzx disk default ext system built disk consists block groups report experience systems end section layout control begin simple experiment demonstrate place effectively collocate les speci group disk speci cally compare methods creating directory tree allocated uniformly-sized les rst standard system interfaces alter number directories place les fourth place icl create les underneath directories direct system proceedings usenix annual technical conference june san antonio texas time number shadow directories system creation time place figure system initialization system initialization time plotted x-axis vary number shadow directories created group y-axis plots total time initialization complete place les directories single group middle disk figure shows group utilization approach directory trees data gure directories standard layout algorithms data les scattered disk contrast place data located middle group system desired system creation demonstrated basic control layout seek understand costs system rst cost present system initialization performed pmkfs tool figure presents system initialization time dominant cost system initialization number shadow directories created shadow cache present sensitivity initialization time number shadow directories created group gure cost scale increasing number directories linux ext system increasing amount data created order allocate directories groups successfully api overheads present overheads controlled directory creation place goal understand costs gray-box control data placement table breaks cost creating different-sized les place createfile interface costs presented table broken time percentage base state alloc ren misc total table file allocation overheads result shows average controlled creations place icl variance runs time percentage shadow cache shadow cache min median max base state alloc ren clean misc total table directory allocation overheads result shows average controlled directory creations place icl note times milliseconds rightmost column max shows time seconds time shown appears due rounding categories creation tests categories base time create standard interfaces state time read superblock access system statistics con guration information alloc time control allocation case stat system call check inode number ren time rename correct namespace misc additional software processing overhead table place api creation adds roughly overhead creation cost due place initialization amortized multiple calls place library signi overhead allocation rename software overheads finally size increases overheads unsurprisingly amortized explore overheads directory creation place createdir api table presents cost breakdown controlled directory allocation shadow cache note category included labeled cleanup includes time spent cleaning sdt directory-allocation process run note alloc case refers proceedings usenix annual technical conference june san antonio texas time number directories directory tree create performance place naive place directories place directories shadow cache standard figure create performance cost moving directory tree speci group presented varying number sub-directories structure x-axis xed amount data spread evenly les approaches creating structure compared text y-axis presents total time bulk collocation log scale costs creating les directories required directory-allocation algorithm table make number observations shadow cache time controlled directory creation reasonable roughly substantially higher base directory creation cost approximately factor faster shadow cache times higher median cost column lists maximum time shadow cache potential cost running full directorycreation process worst case takes seconds create directory correct group dif culty arises controlled creation smaller group ext allocates directories based free bytes remaining takes excessively long time groups coerce directory allocation group base alloc state times essentially constant constitute negligible part total maximum case shadow cache basic algorithm clean afterward bulk collocation costs common usage place move entire directory tree speci group disk accomplished place command-line tools interested strategy tool moving large amount data source nal destination group small number groups time number directories directory tree read performance standard random place random standard optimal place optimal figure smallle reads time read -kb les data shown settings varying number directories data settings standard system apis create les settings place collocate data single group orders shown random reads les random order optimal reads single scan disk figure presents time perform bulk collocation data spread evenly -kb les varying number sub-directories schemes compared rst place naive fashion creating directories les target group recursively assuming shadow cache exists approach dramatically slow directory creation algorithm nds increasingly dif cult force data target group approach creates directories rst performance improves tremendously ext allocation policy number bytes allocated group-selection policy creating les target group easier coerce system choosing scheme shows time approach assuming directories allocated shadow cache improves performance bulk collocation seconds finally traditional directory-tree copy shown comparison point fast overhead spread data disk localized manner bene collocation quantify potential read performance improvement place perform nal set microbenchmarks figure shows performance rst set tests present time takes read set les collocated disk gure application reads proceedings usenix annual technical conference june san antonio texas bandwidth group number exploiting zoned-bit recording ext ffs figure largele reads performance reading shown ext ffs varying group created x-axis cache ushed umount mount cycle read ensure disk bandwidth properly measured point average trials variance trials low set les random order collocating localized portion disk improves performance factor random lines graph les read optimal order essentially scanning 
disk single sweep bene collocation small case spreading data disk results additional seeks makes difference performance demonstrate place advantage zoned bandwidth characteristics modern disks figure plots performance large sequential scans les speci groups gure depicts performance place systems standard ext modi ext acts traditional bsd ffs gure ext platform placing data lower-numbered groups corresponds directly placing data outer zones disk improving performance observe experiment run top ffs system zoning nature disks hidden ffs spreads blocks large les disk place control placement blocks experience systems primary focus ext system modern implementation ffs concepts popular system linux community designed aspects place general ffslike systems mind interested studying behavior place platforms rst test generality run place top ext system journaling version ext ext great lengths preserve backwardscompatibility ext on-disk structures utilized surprised place works issue top ext tested place top implementation ffs allocation algorithms linux kernel place worked modi cation environment limitations discussed section shown directly figure relating placement large les case studies section describe place library rst demonstrate web server reorganize les place improve server throughput response time describe high-speed system benchmarking infrastructure place quickly extract characteristics underlying system improving web server throughput rst apply place order understand potential performance improvement web server environment reorganizing les popularly accessed les close disk seek costs reduced web service good target place structure typical web directory tree necessarily match locality assumptions encoded ffs-like systems change source code web server reorganization performed off-line command-line tools study potential bene simpli trace-based approach utilize web trace wisconsin-madison web server trace rst preprocessed remove requests induce system activity errors redirects requests remain transfer data http replies reply cache coherence check trace roughly million requests accesses total directory tree size understand potential gains collocation run trace system request generator trace entry request generator invokes system call records response time specifically model http requests generator performs stat system call requests transfer data maps memory touches page approach capture full complexity web environment give proceedings usenix annual technical conference june san antonio texas time memory size standard place figure web server performance time play back component web trace shown standard line plots performance typical layout place approach packs data small portion disk point represents trials variance low size directory tree served baseline potential performance improvement system reorganization utilize place command-line tools collocate directory tree outer-most tracks disk compare organization directory tree spread drive determined typical system heuristics figure time replay trace function amount memory range memory sizes collocation place improves performance roughly bene result directly reduction seek costs demonstrated instrumentation testing apparatus speci cally recording group number access compute average group distance traversed requests standard layout average number groups requests place performance gains limited due access patterns found web trace trace requests les single image directory les collocated standard ffs policy reducing placeassisted placement environment place improve performance simple direct manner greater bene expected environments access patterns match directory structure closely rapid file system microbenchmarking examine place context fast discovery performance characteristics tools developed time extract performance characteristics underlying system benchmarking tools run root run uncontrolled potentially lengthy amount time chen patterson self-scaling benchmark runs hours days reporting results back user settings system benchmarking tool ran quickly trading accuracy shorter run-time running application foreign computing environment seti home wide-area shared computing system condor globus mobile application quickly extract characteristics underlying system parameterize properly system benchmark run user-level requiring special privileges discover system parameters develop benchmarking tool fast fast accurate system extraction mobile application extract performance characteristics underlying system xed time budget user level fast extract information system memory system system component utilizes place mobile application examine single processor version now-sort worldrecord-breaking sorting application traditionally thought database contexts sorting commonly found scienti computation pipelines reasonable candidate mobile execution scienti peer-to-peer shared computing systems now-sort requires parameters tune host system rst parameters bandwidth expected local disk worst-case seek time numbers sort estimate large buffers merge phase order amortize seek costs size caches memory-hierarchy sorting data cachesized chunks sorting proceeds faster rate dif cult parameters generally extract maximum seek cost place api fast tool create les disk issue synchronous update rst start timer issue synchronous update record elapsed time write giving coarse estimate full-stroke seek nements made time order remove rotational costs desired table presents costs running fast test system mode operation fast runs quickly garnering coarse estimates required system parameters table observe proceedings usenix annual technical conference june san antonio texas time cache bandwidth max seek pmkfs total table fast performance table presents time fast takes discover system parameters mode fast congured run quickly extracting coarse estimates consuming time total time extract needed information sorting roughly seconds sorts massive data sets spending extra seconds con gure application worth time finally note pmkfs specialized task hand giving command-line options prevent creation shadow directories initialization time reduced small xed overhead related work work directly related place gray-box file layout detector controller fldc original gray-box paper fldc components rst decide order access set les re-write directory improve accesses components place fldc exposing ne-grained control directory layout applications applications long sought control underlying operating system policies mechanisms response demand previous research developed operating systems including spin exokernel vino much-improved control operating system behavior gray-box approach route improved control underlying exploiting knowledge behavior place demonstrates directory layout realized user-level place method exposing group numbers conceptually similar exokernel philosophy exposing physical names place treats system underlying entity exposes internal structure ext block groups exokernel exposes details hardware physical sector numbers moving data blocks spatial arrangement web server case study explored contexts work disk shuf ing ruemmler wilkes track frequency block accesses reorder disk blocks reduce seek times higher level staelin garcia-molina rearrange les system major difference approaches place performed transparently users applications control exposed sophisticated tracking blocks les accessed temporal succession hope develop access-tracking tool future work improving web server performance similar work improving web proxy performance hummingbird library-based system designed web proxies place features hummingbird users collocate les tie locality naming contrast place implemented top ffs-like system hummingbird performs functions library runs raw disk hummingbird specialized web-proxy environment place 
general-purpose tool finally fast tool bears similarity recent work database management systems online aggregation dbms returns approximate result selection query user immediately includes statistical estimate accuracy result user query running system nes result time data sampled answer precise fast tool applies philosophy benchmarking system future work number avenues exist future research plan explore breadth applicability place top systems platform interested bsd family challenges domain recent bsd implementations ffs utilize dirprefs algorithm directory group selection algorithm places directories parents attempt increase performance common operations unpacking large directory tree building gray-box controller place top dirprefs require extra care spread directories groups building place top log-structured system lfs interesting grouping related les generally straightforward user wishes group les les written time aspects make lfs challenging including grouping les span multiple segments controlling off-line behavior cleaner generally interested developing techniques control alloproceedings usenix annual technical conference june san antonio texas cation broader range systems investigate utility methods range storage devices specifically determine controlled placement top modern disk arrays finally tool place low-level mechanism placing les controlled manner disk les higher-level policy decision requires detailed knowledge les accessed time similar previous work data rearrangement plan develop tool track les blocks accessed generate inputs placement conclusions classic paper hints computer system design lampson tells don hide power higherlevel abstractions hide undesirable properties functionality contrast exposed client unix systems expose explicit controls laying les user demands standard layout heuristics workloads conform locality assumptions set stone years ago perform poorly paper present design implementation evaluation place gray-box information control layer exposes directory information applications exploiting knowledge internal algorithms common ffs-like systems place control directory allocations microbenchmarks shown costs gray-box control overly burdensome potential bene controlled allocation substantial case studies demonstrated place system realistic diverse application settings discussed limitations place gray-box approach controlled allocation highlighting features system allocation policies make simple dif cult build control top gray-box approach alternative path innovation requiring underlying operating system dif cult implement maintain distribute gray-box icl embeds knowledge underlying system exploits knowledge implement functionality portable manner important question remains full range functionality implemented gray-box manner ultimate limitations icl small step nal answer acknowledgments nathan burnett tim denehy florentina popovici vijayan prabhakaran muthian sivathanu feedback paper special muthian assistance traces geoffrey kuenning excellent shepherding anonymous reviewers thoughtful suggestions combination greatly improved content paper john heim staff doit providing recent web trace included path names tom engle implementation ffs allocation algorithm linux kernel finally computer systems lab providing superb environment computer science research work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau culler hellerstein patterson highperformance sorting networks workstations proceedings acm sigmod conference management data sigmod tucson arizona bershad savage przemyslaw pardyak fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles december burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey june chen patterson approach performance evaluation self-scaling benchmarks predicted performance inproceedings acm sigmetrics conference pages dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey california june engler kaashoek toole exokernel operating system architecture proceedings usenix annual technical conference june san antonio texas application-level resource management proceedings acm symposium operating systems principles december foster kesselman globus metacomputing infrastructure toolkit international journal supercomputer applications hellerstein haas wang online aggregation sigmod international conference management data sigmod pages tucson arizona holtman cms data grid system overview requirements cms note cern july lampson hints computer system design proceedings acm symposium operating system principles pages bretton woods december acm litzkow livny mutka condor hunter idle workstations proceedings acm computer network performance symposium pages june mckusick joy lef fabry fast file system unix acm transactions computer systems august mcvoy staelin lmbench portable tools performance analysis proceedings usenix winter technical conference january meter observing effects multi-zone disks proceedings usenix conference january nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference riedel kallahalla swaminathan aframework evaluating storage system security proceedings usenix symposium file storage technologies fast pages monterey california january rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february ruemmler wilkes disk shuf ing technical report hpl- hewlett packard laboratories oct saavedra smith measuring cache tlb performance effect bench-mark runtimes ieee transactions computers schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings usenix symposium operating systems design implementation osdi october seltzer ganger mckusick smith soules stein journal-ing versus soft updates asynchronous meta-data protection file systems proceedings usenix annual technical conference pages san diego june shriver gabber huang stein storage management web proxies proceedings usenix annual technical conference usenix pages boston massachussetts june staelin garcia-molina smart filesystems proceedings usenix winter technical conference dallas texas january stonebraker operating system support database management communications acm july sullivan werthimer bowyer cobb gedye anderson major seti project based project serendip data personal computers proceedings international conference bioastronomy talagala arpaci-dusseau patterson microbenchmark-based extraction localand global disk characteristics technical report csd- california berkeley tweedie future directions theext filesystem proceedings usenix annual technical conference freenix track monterey california june 
life death block-level muthian sivathanu lakshmi bairavasundaram andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison muthian laksh dusseau remzi wisc abstract fundamental piece information required intelligent storage systems liveness data formalize notion liveness storage present classes techniques making storage systems liveness-aware explicit notification approach present robust techniques file system impart liveness information storage free block command implicit detection approach show information inferred storage system efficiently underneath range file systems storage interface demonstrate techniques prototype implementation secure deleting disk find explicit interface approach desirable due simplicity implicit approach easy deploy enables quick demonstration functionality facilitating rapid migration explicit interface introduction life pleasant death peaceful transition troublesome isaac asimov smarter storage systems understand blocks live dead previous work demonstrated utility knowledge dead blocks store rotationally optimal replicas data provide zero-cost writes failure recovery time reduced restoring live blocks liveness information modern storage systems due narrow blockbased interface file systems storage storage systems simply observe block-level reads writes aware logical operations deletes issued file system limitation precludes storage level optimizations makes effective paper address limitation presenting techniques storage systems imparted liveness information perform qualitative quantitative comparison approaches explicit notification augment interface storage free block command file systems modified properly implicit detection develop techniques enable storage system infer liveness information change interface evaluate approaches formalize notion liveness storage specifically identify classes liveness content block generation liveness present techniques explicit implicit tracking type techniques imparting liveness information dependent characteristics file system study range file systems including ext ext vfat identify key file system properties impact feasibility complexity techniques gain direct experience liveness-tracking methods design implement evaluate prototype secure deleting disk shreds blocks logically deleted file system making deleted data irrecoverable implement secure delete due extreme requirements type accuracy liveness information surface explicit implicit approaches obvious benefits drawbacks explicit notification promises simplicity implementation requires broad industry consensus implicit detection suggests ease deployment cost complexity analysis reveals complex trade-offs find qualitatively explicit approach complicated design implement straightforward modify file systems issue free block commands accurate notification presence crashes entails careful integration file system consistency management schemes noticeably increasing complexity find implicit liveness detection feasible underneath range modern file systems file system behaviors prohibit classes liveness inference identify properties hold order enable simplify implicit liveness inference propose implement minor modifications file systems conform properties finally show implicit liveness detection accurate underneath modern asynchronous file systems secure delete prototype utilizes implicit liveness shred blocks inferred dead proving correct operation implicit secure delete demonstrate implicit liveness storage applications appears sixth symposium operating systems design implementation osdi extreme correctness requirements evaluating performance implicit liveness tracking find comparable explicit approach conclude storage systems easily implement explicit approach interface embellished support implicit approach complementary competitive technology industry consensus interface change slow-moving implicit techniques complex specifically deploying technology explicit interface change implicit techniques readily demonstrate benefits move industry rapidly explicit change paper organized present extended motivation taxonomy liveness list file system properties impact techniques imparting liveness information proceed discussing explicit notification implicit detection describe secure deletion describe initial experience implicit detection ntfs closed-source file system finally present discussion relative merits implicit explicit approaches finish discussing related work concluding appendix includes proof correctness implicit secure delete extended motivation section present examples functionality enabled liveness information motivate alternative approaches gathering information utility liveness liveness information enables variety functionality performance enhancements storage system enhancements implemented higher layers require low-level control storage system eager writing workloads write-intensive run faster storage system capable eager writing writing free block closest disk arm traditional in-place write order select closest block storage system information blocks live existing proposals function long exist blocks written file system writes block storage system identify subsequent death block result delete disk empowered liveness information effective eager writing adaptive raid information block liveness storage system facilitate dynamic adaptive raid schemes autoraid system autoraid utilizes free space store data raidlayout migrates data raidwhen runs short free space knowledge block death make schemes effective optimized layout techniques optimize on-disk layout transparently storage system explored adaptive reorganization blocks disk replication blocks rotationally optimal locations examples knowing blocks free greatly facilitate techniques live blocks collocated minimize seeks free space dead blocks hold rotational replicas smarter nvram caching buffering writes nvram common optimization storage systems synchronous write workloads benefit in-memory delayed writes file system nvram buffering improves performance absorbing multiple overwrites block deleteintensive workloads unnecessary disk writes occur absence liveness information deleted blocks occupy space nvram written disk nvram fills real file system traces found writes deleted typical delayed write interval seconds unnecessarily written disk knowledge block death storage removes overhead intelligent prefetching modern disks perform aggressive prefetching block read entire track block resides prefetched cached internal disk cache aged fragmented file system subset blocks track live caching track result suboptimal cache space utilization reading track efficient disk knowledge liveness enable disk selectively cache blocks live faster recovery liveness information enables faster recovery storage arrays storage system reduce reconstruction time disk failure reconstructing blocks live file system self-securing storage liveness information storage build intelligent security functionality storage systems storage level intrusion detection system ids perimeter security monitoring traffic suspicious access patterns deletes truncates log files detecting patterns requires liveness information secure delete ability delete data manner makes recovery impossible important component data security government regulations require strong guarantees sensitive data forgotten requirements expected widespread government industry future secure deletion requires low-level control block placement storage appears sixth symposium operating systems design implementation osdi system implementing storage level secure delete requires liveness information storage system explore secure deletion section acquiring liveness information clear benefits liveness information storage systems information natural question arises convey liveness information storage systems discuss approaches explicit notification implicit detection explicit notification explicit notification involves augmenting existing storage interface allocate block free block commands modifying file systems commands explicitly convey liveness information storage system main benefit explicit approach potential simplicity interface deployed conveying liveness information seemingly straightforward appearing natural achieve goal problems approach changing interface storage raises legacy issues requires broad industry consensus demand interface requires agreement clear benefits interface difficult achieve deployment interface chicken-and-egg problem implicit detection implicit detection 
intended solve bootstrapping problem explicit interface evolution approach storage system monitors block-level reads writes issued file system underneath unmodified interface infers liveness information implicitly ideally change file system implicit approach enables demonstration benefits due proposed interface change making evolutionary step eventual interface modification previous work semantically-smart storage systems explored implicit detection forms file system information storage system storage-level enhancements degree accuracy required implicit detection techniques case depends nature application information x-ray storage system utilizes implicit information file accesses implement exclusive storage array cache inaccurate information x-ray simply reduces potential performance gain d-graid storage system utilizes implicit information file block belongs order place blocks fault-isolated fashion improving availability storage system multiple disk failures inaccurate information graid leads poor fault isolation impact correctness array exhibits strictly liveness description type utility content data block versioning block block holds eager write valid data fast recovery generation block lifetime secure delete context file storage ids table forms liveness availability traditional raid paper investigate limits implicit detection applications utilize implicit liveness information directly impacts correctness primary concern implicit interface evolution ties interacting layers file system storage system change issue problematic on-disk formats evolve slowly reasons backwards compatibility linux ext file system introduced roughly layout lifetime ext journaling file system backwards compatible on-disk layout ext extensions freebsd file system backwards compatible evidence commercial storage vendors maintain support software specific file system emc symmetrix storage system software understand common file systems trends point commercial viability implicit detection approach liveness storage taxonomy discussed utility liveness information storage system present taxonomy forms liveness information relevant storage liveness information classified dimensions granularity accuracy timeliness granularity liveness depending specific storage-level enhancement utilizes liveness information logical unit liveness tracked vary identify granularities liveness information meaningful content block generation summary presented table content liveness content liveness simplest form liveness unit liveness actual data context block death granularity occurs overwrite block block overwritten data storage system infer contents dead approximate form content liveness readily existing storage systems explored previous work wang appears sixth symposium operating systems design implementation osdi virtual log disk frees past location block block overwritten contents tracking liveness granularity on-disk versioning self-securing storage systems completely accurate storage system block freed file system contents stored block dead overwritten block liveness block liveness tracks disk block valid data data accessible file system unit interest case container contents block liveness granularity required applications intelligent caching prefetching eager writing deciding propagate block nvram disk storage system block live granularity form liveness information tracked traditional storage systems storage system unaware blocks file system thinks live weak form liveness tracked block written inferred dead generation liveness generation disk block lifetime block context file death generation block written disk context file free reallocated file tracking generation liveness ensures disk detect logical file system delete block contents reached disk context deleted file storage level functionality requires generation liveness secure delete track block live contained data belonged file generation longer alive application requires generation liveness information storage-based intrusion detection generation liveness tracked existing storage systems accuracy liveness information dimension liveness accuracy refer degree trust disk place liveness information inaccuracy liveness information lead disk overestimating underestimating set live entities blocks generations degree accuracy required varies specific storage application deletesquashing nvram acceptable storage system slightly overestimate set live blocks performance issue correctness issue hand underestimating set live blocks catastrophic disk lose valid data similarly generation liveness detection secure delete acceptable miss intermediate generation deaths block long latest generation death block timeliness information final axis liveness timeliness defines time death occurring file system disk learning death explicit notification approach file system delays free notifications similar delayed writes time lag disk learns block generation death similarly implicit approach periodicity file system writes metadata blocks imposes bound timeliness liveness information inferred applications eager writing delete-aware caching delayed knowledge liveness acceptable long information changed meantime applications secure delete timely detection provide stronger guarantees file system properties explicit implicit methods imparting liveness information storage dependent characteristics file system storage system study range techniques required liveness notification detection experimenting underneath file systems ext ext vfat experimented ntfs limited scale due lack source code access ntfs experience section ext modes operation synchronous asynchronous modes ext modes writeback ordered data journaling modes update behaviors form rich set file systems begin background file systems outline high level behavioral properties file system relevant context liveness information sections discuss properties influence techniques storage-level liveness tracking file system background subsection provide background information file systems study discuss key on-disk data structures update behavior common properties begin properties common file systems viewpoint liveness tracking basic level file systems track kinds on-disk metadata structure tracks allocation blocks bitmap freelist index structures inodes map logical file groups blocks common aspect update behavior modern file systems asynchrony data metadata appears sixth symposium operating systems design implementation osdi block updated contents block immediately flushed disk buffered memory interval delayed write interval blocks dirty longer delayed write interval periodically flushed disk order delayed writes committed potentially arbitrary file systems enforce ordering constraints linux ext ext file system intellectual descendant berkeley fast file system ffs disk split set block groups akin cylinder groups ffs inode data blocks allocation status live dead data blocks tracked bitmap blocks information file including size block pointers found file inode accommodate large files pointers inode point indirect blocks turn block pointers committing delayed writes ext enforces ordering whatsoever crash recovery requires running tool fsck restore metadata integrity data inconsistency persist ext synchronous mode operation metadata updates synchronously flushed disk similar early ffs linux ext ext file system journaling file system evolved ext basic on-disk structures ext ensures metadata consistency write-ahead logging metadata updates avoiding perform fsck-like scan crash ext employs coarse-grained model transactions operations performed epoch grouped single transaction ext decides commit transaction takes in-memory copy-on-write snapshot dirty metadata blocks belonged transaction subsequent updates metadata blocks result in-memory copy ext supports modes operation ordered data mode ext ensures transaction commits data blocks dirtied transaction written disk data journaling mode ext journals data blocks metadata modes ensure data integrity crash mode data writeback order data writes data integrity 
guaranteed mode vfat vfat file system descends world operating systems paper linux implementation vfat vfat operations centered file allocation table fat entry allocatable block file system entries locate blocks file linkedlist fashion file block address property syn reuse ordering block exclusivity generation marking delete suppression consistent metadata data-metadata coupling table file system properties table summarizes properties exhibited file systems study entry fat find block file entry hold endof-file marker setting block free unlike unix file systems information file found inode vfat file system spreads information fat directory entries fat track blocks belong file directory entry information size type information pointer start block file similar ext vfat preserve ordering delayed updates properties update behavior file system direct influence techniques liveness information imparted storage system based experience aforementioned file systems identify high-level file system properties relevant liveness tracking table summarizes properties reuse ordering file system guarantees reuse disk blocks freed status block bitmaps metadata pointed block reaches disk file system exhibits reuse ordering property sufficient ensure data integrity absence property file end partial contents deleted file crash journaling file system vfat asynchronous mode ext reuse ordering modes ext ext synchronous mode exhibit reuse ordering block exclusivity block exclusivity requires disk block dirty copy block file system cache requires file system employ adequate locking prevent update in-memory copy dirty copy written disk property holds file systems ext vfat ext conform property snapshot-based journaling dirty copies metadata block previous transaction committed current transaction generation marking generation marking property requires file system track reuse file pointer obappears sixth symposium operating systems design implementation osdi jects inodes version numbers ext ext file systems conform property inode deleted reused file version number inode incremented vfat exhibit property delete suppression basic optimization found file systems suppress writes deleted blocks file systems discuss obey property data blocks vfat obey property directory blocks consistent metadata property file system conveys consistent metadata state storage system journaling file systems exhibit consistent metadata property transaction boundaries on-disk log implicitly convey information ext vfat exhibit property data-metadata coupling data-metadata coupling builds consistent metadata property requires notion consistency extended data blocks words file system conforming property conveys consistent metadata state set data blocks dirtied context transaction file systems ext data journaling mode conforms property explicit liveness notification proceed techniques imparting forms liveness information storage systems section discuss explicit notification approach assume special allocate free commands added scsi optimization obviate explicit allocate command treating write previously freed block implicit allocate modifying file systems interface trivial find supporting free command ramifications consistency management file system crashes modified linux ext ext file systems free command communicate liveness information discuss issues free command implemented ioctl pseudo-device driver serves enhanced disk prototype granularity free notification issue arises explicit notification exact semantics free command granularities liveness outlined section block liveness content liveness tracked file system lazy initiating free commands suppressingfreeto blocks subsequently reused generation liveness file system notify disk delete block contents reached disk context deleted file multiple intermediate layers buffering file system contents block reached disk context file simplify file system implementation file system concerned form liveness disk functionality requires approach file system invokes free command logical delete receiving free command block disk marks block dead internal allocation structure bitmap write marks block live responsibility mapping thesefreecommands form liveness information lies disk disk track generation deaths interested free command block thinks live internal bitmaps redundant free block free disk block deleted written disk viewed generation death correct operation file system guarantee write block disk prior allocation write treated implicit allocate guarantee delete suppression property write freed block allocation result incorrect conclusion generation liveness disk note free issued block disk safely block possibly erasing contents timeliness free notification important issue arises explicit notification free file system issues notification option notification file system issues free immediately block deleted memory solution result loss data integrity crash scenarios crash occurs immediately free notification block metadata indicating delete reaches disk disk considers block dead recovery file system views block live delete reached disk live file freed block scenario violation data integrity violations acceptable file systems ext weak data integrity guarantees file systems preserve data integrity ext delay notification effect delete reaches disk delayed notification requires file system conform reuse ordering property block reused live file system effect previous delete reaches disk delayed free command suppressed means disk miss generation death orphan allocations finally explicit notification handle case orphan allocations file system considers block dead disk considers live assume block newly allocated file written disk conappears sixth symposium operating systems design implementation osdi text file crash occurs point metadata indicating allocation written disk disk assume block live restart file system views block dead on-disk contents block belong file longer extant file system block suffered generation death disk free notification mechanism enable accurate tracking liveness orphan allocations handling orphan allocations file system specific describe explicit notification ext mentioned ext provide data integrity guarantees crash notification deletes ext invokes free command synchronously block freed memory dealing orphan allocations ext requires simple expensive operation recovery fsck utility conservatively issuesfreenotifications block dead file system explicit notification ext ext guarantees data integrity ordered data journaling modes free notification ext delayed effect delete reaches disk words notification delayed transaction performed delete commits record in-memory list blocks deleted part transaction issue free notifications blocks transaction commits ext conforms reuse ordering property delayed notification feasible crash occur invocation free commands immediately commit transaction free operations redo-able recovery purpose log special free records journal replayed recovery part delete transaction recovery multiple committed transactions propagated on-disk locations block deleted transaction reallocated subsequent committed transaction replay loggedfree commands guarantee completing free commands transaction committing transaction replay free commands successfully committed transaction log earlier committed transactions replayed deal orphan allocations log block numbers data blocks written written disk recovery ext issuefree commands set orphan data blocks part uncommitted transaction implicit liveness detection section analyze issues implicit detection liveness storage system implicit liveness inference requires storage system semantic understanding on-disk format file system running coupled careful observation file system traffic implicit liveness detection file system dependent discuss feasibility generality 
implicit liveness detection file systems ext ext vfat section discuss initial experience implicit detection underneath windows ntfs file system forms liveness address granularity accuracy axes mentioned section accuracy axis accurate approximate inferences approximate instance refers strict over-estimate set live entities timeliness axis address common complex case lack timely information modern file systems delay metadata updates timeliness guaranteed guarantees timeliness synchronously mounted file system implicit inference liveness trivial content liveness discussed section disk observes write contents live data block infer previous contents stored block suffered content death completely accurate content liveness inference requires information block liveness block liveness block liveness information enables storage system block valid data time track block liveness storage system monitors updates structures tracking allocation ext ext specific data bitmap blocks convey information vfat information embedded fat entry fat block free file system writes allocation structure storage system examines entry concludes relevant block dead live allocation bitmaps buffered file system written periodically liveness information storage system stale account allocations deletes occurred interval table depicts time line operations leads incorrect inference storage system bitmap block tracking liveness written step indicating dead subsequently allocated file written disk indicating live buffered memory point disk wrongly believes dead on-disk contents valid appears sixth symposium operating systems design implementation osdi operation in-memory on-disk initial free write disk free alloc alloc write disk written liveness belief live free table naive block liveness detection table depicts time line events leads incorrect liveness inference problem solved shadow bitmap technique address inaccuracy disk tracks shadow copy bitmaps internally file system writes bitmap block disk updates shadow copy copy written addition data block written disk disk pro-actively sets bit shadow bitmap copy block live write leads disk live preventing incorrect conclusion drawn file system properties block liveness shadow bitmap technique tracks block liveness accurately underneath file systems obey block exclusivity data-metadata coupling property block exclusivity guarantees bitmap block written reflects current liveness state relevant blocks file system tracks multiple snapshots bitmap block ext write version bitmap block indicating dead subsequent allocation write ofb disk wrongly infer dead fact on-disk contents valid belongs newer snapshot uncertainty complicates block liveness inference file system exhibit block exclusivity block liveness tracking requires file system exhibit data-metadata coupling group metadata blocks bitmaps actual data block contents single consistent group file systems typically enforce consistent groups transactions observing transaction boundaries disk reacquire temporal information lost due lack block exclusivity ext data journaling mode transaction newly allocated data blocks bitmap blocks indicating allocation part consistent group commit point disk conclusively infers liveness state state bitmap blocks transaction data writes actual in-place locations occur transaction commits disk guaranteed transaction commit blocks marked dead previous transaction remain dead absence data-metadata coupling newly allocated data block reach in-place location transaction commits live disk disk detects operation in-memory on-disk initial alloc live write disk written delete free alloc alloc write disk live liveness belief missed gen death table missed generation death block liveness table shows scenario illustrate simply tracking block liveness insufficient track generation deaths accuracy block liveness requires file system conform delete suppression property delete suppression hold write block imply file system views block live shadow bitmap technique overestimate set live blocks bitmap write table ext vfat ext data journaling mode readily facilitate block liveness detection generation liveness generation liveness stronger form liveness block liveness builds shadow bitmap technique generation liveness goal find on-disk block generation data file stored block dead block liveness special case generation liveness block dead latest generation stored dead conversely block liveness information sufficient detect generation liveness block live stored dead generation past table depicts case block initially stores generation inode disk thinks block live deleted freeing immediately reallocated file written time continues marked live disk missed generation death occurred bitmap writes generation liveness reuse ordering tracking generation liveness general challenging file system reuse ordering property makes simple track reuse ordering block reused file deleted status block reaches disk reused bitmap block written disk detect dead presence reuse ordering tracking block liveness accurately implies accurate tracking generation liveness file systems ext conform reuse ordering facilitate accurate tracking generation liveness generation liveness reuse ordering underneath file systems ext vfat exhibit reuse ordering property tracking generation appears sixth symposium operating systems design implementation osdi liveness requires disk detailed information specifically disk monitor writes metadata objects link blocks single logical file inode indirect blocks ext directory fat entries vfat disk explicitly track generation block belongs inode written disk records block pointers belong specific inode extra knowledge file block belongs disk identify generation deaths ownership table disk tracked belongs eventually written disk observe change ownership owns block owned past disk conclude generation death occurred complication arises reused reused representing file belongs generation scenario detected generation death ownership change monitor miss detect case require file system track reuse inodes generation marking property ext maintains version number enables detection cases generation deaths version numbers disk tracks block generation belonged generation number combination inode number version number disk observes inode written incremented version number concludes blocks belonged previous version inode incurred generation death call technique generation change monitoring finally pertinent note generation liveness detection generation change monitoring approximate assume disk observes block belongs generation time observes belongs generationg generation change monitoring disk conclude generation death occurred disk generation deaths occurred relevant period freed allocated freed reallocated disk owningb due delayed write show case study weaker form generation liveness summary file system properties required forms implicit liveness inference presented table case study secure delete demonstrate techniques imparting liveness storage present design implementation evaluation secure deleting disk explicit imliveness type properties blockapprox block exclusivity data-metadata coupling blockaccurate blockapprox delete suppression generationapprox blockapprox generation marking generationaccurate blockaccurate reuse ordering table properties implicit liveness detection approx set live entities over-estimated plicit approaches describe implicit secure delete detail briefly discuss explicit secure delete primary reasons chose secure deletion case study secure delete requires tracking generation liveness challenging track secure delete liveness information context correctness paramount false positive detecting delete lead irrevocable deletion valid data false negative result long-term recoverability deleted data violation secure deletion guarantees compared 
previous work functioned simplistic assumption synchronously mounted file system demonstrate accurate inference liveness feasible underneath variety modern file system behaviors implicit secure deletion prototype called faded file-aware data-erasing disk faded works underneath file systems ext vfat ext complete lack ordering guarantees ext presented challenges specifically ext reuse ordering property detecting generation liveness requires tracking generation information disk section focus implementation faded underneath ext finally discuss key differences implementation file systems goals faded desired behavior faded block reaches disk context file delete file trigger secure overwrite shred block behavior corresponds notion generation liveness defined section shred involves multiple overwrites block specific patterns erase remnant magnetic effects past layers recovered techniques magnetic scanning tunneling microscopy recent work suggests overwrites sufficient ensure non-recoverability modern disks traditionally secure deletion implemented file system implementations unreliable modern storage systems high security overwrites off-track writes writes straggling physical track boundaries external erase programs file system perform storage system buffers writes nvram multiple overwrites file system appears sixth symposium operating systems design implementation osdi collapsed single write physical disk making overwrites ineffective finally presence block migration storage system overwrite file system overwrite current block location stray copies deleted data remain storage system proper locale implement secure deletion note faded operates granularity entire volume control individual files shredded limitation dealt storing sensitive files separate volume secure delete functionality enabled basic operation discussed section faded monitors writes inode indirect blocks tracks inode generation block belongs augments information block liveness information collects shadow bitmap technique note ext obeys block exclusivity delete suppression properties block liveness detection reliable block death detected faded safely shred block hand faded detects generation death ownership change generation change monitors block live block liveness module faded simply shred block faded current contents block belong generation deleted generation subsequently allocated block due block reuse current contents block valid shredding block catastrophic deal uncertainty conservative approach generation-death inference conservative convert apparent correctness problem performance problem end performing overwrites required fundamental approach notion conservative overwrite conservative overwrites conservative overwrite block erases past layers data block leaves current contents intact faded subsequent valid write occurred predicted generation death conservative overwrite block safe shred valid data perform conservative overwrite block faded reads block non-volatile ram performs normal secure overwrite block specific pattern ultimately restores original data back block problem conservative overwrite block contents restored conservative overwrite fact data shredded conservative overwrite ineffective case faded guaranteed observe things block reused file system file valid data written eventually delayed write interval file system faded receives write buffers write writing data disk faded performs shred concerned block time faded restore data recent contents block identify writes treat special manner faded tracks list blocks subjected conservative overwrite suspicious blocks list write block list committed secure overwrite block overwrite block removed suspicious list note suspicious list stored persistently nvram order survive crashes block reused file system immediately faded guaranteed observe bitmap reset block flagged block death block liveness detector block liveness tracking reliable faded shred block destroying data cases wrongful restore data faded guaranteed opportunity make error cost conservatism conservative overwrites performance cost conservative overwrite results concerned block treated suspicious data restored conservative overwrite data faded information find stage uncertainty data restored data overwritten subsequent write block context file lead redundant shredding block performance cost faded pays circumvent lack perfect information coverage deletes previous subsection showed generation deaths detected faded ensures block version overwritten compromising valid data faded achieve goals detection techniques sufficient identify cases deletes file system level shredded section show faded detect deletes requires minor modifications ext undetectable deletes weak properties ext deletes missed faded present specific situations identification deletes impossible propose minor ext fix scenarios file truncates generation change monitor assumes version number inode incremented inode reused version number ext appears sixth symposium operating systems design implementation osdi operation in-memory on-disk initial bind bind delete free alloc write disk bind wrong type table misclassified indirect block table shows scenario normal data block misclassified indirect block bind treated indirect block reuse ordering indirect blocks prevents problem incremented complete delete reuse partial truncates affect version number block freed due partial truncate reassigned file faded misses generation death reuse partial truncate argued logical overwrite file delete adopt complex conservative interpretation treating delete handle deletes propose small change ext incrementing version number reallocation inode increment truncate alternatively introduce separate field inode tracks version information non-intrusive change effective providing disk requisite information technique result extra overwrites rare case partial truncates correctness guaranteed spurious overwrites conservative leave data intact reuse indirect blocks subtle problem arises due presence indirect pointer blocks indirect blocks share data region file system user data blocks file system reuse normal user data block indirect block vice versa presence dynamic typing disk reliably identify indirect block faded identify block indirect block observes inodei indirect pointer field faded records fact indirect block observes write faded contents indirect block deleted reused user data block inode scenario illustrated table faded trust block pointers suspected indirect block uncertainty lead missed deletes cases prevent occurrence data block misclassified indirect block ensure file system allocates immediately file system frees indirect block bind concerned data bitmap blockmbind flushed disk disk block freed note weak form reuse ordering indirect blocks show change operation in-memory on-disk initial free free alloc write disk written delete free alloc write disk missed delete table missed delete due orphan write table illustrates delete missed orphan block treated carefully block initially free allocated memory written disk written deleted reallocated written faded associate miss overwrite impact performance indirect blocks tend small fraction set data blocks practicality discussed minimal non-intrusive required modification lines code ext required weak ordering guarantees ext file systems ext exhibit reuse ordering required study ext aimed limit study minimal set file system properties required reliably implement secure deletion disk orphan allocations implicit block liveness tracking faded addresses orphan allocation issue discussed ext recovers crash fsck utility writes copy bitmap blocks block liveness monitor faded detect death orphan allocations orphan writes due arbitrary ordering ext faded observe write newly allocated data block observes owning inode orphan writes treated carefully owning 
inode deleted written disk faded block belonged inode block reused inode faded miss overwriting concerned block written context inode table depicts scenario address problem defer orphan block writes faded observes owning inode potentially memory-intensive solution suspicious block list conservative overwrites track orphan blocks faded observes write orphan block marks suspicious subsequent write arrives contents shredded inode owning block deleted reaching disk write block context file trigger shred block reused bitmap reset delete technique results redundant secure overwrite anytime orphaned block overwritten file sysappears sixth symposium operating systems design implementation osdi persistent liveness monitor block block inode mapping overwrite thread datadelayed overwrites suspicious list shadow bitmaps monitor generation change figure key components faded tem context file cost pay conservatism note overhead incurred time orphan block overwritten guaranteed detection deletes techniques prove block deleted file system reached disk faded overwrites deleted contents proof presented appendix delayed overwrites multiple overwrites block additional disk hurt performance incurred critical path performance faded delays overwrites idle time workload optionally minutes detection faded decides shred block queues low priority thread services queue faded observed foreground traffic duration delayed overwrites faded present writes disk sequential ordering reducing impact foreground performance delaying reduces number overwrites block deleted multiple times notion conservative overwrites crucial delaying overwrites arbitrarily block overwritten written context file note shredding required user perform sync summary key data structures components faded presented figure faded file systems implemented faded underneath file systems case validated implementation testing methodology section due space constraints point key differences observed relative ext faded vfat ext vfat conform reuse ordering faded track generation information block order detect deletes key difference vfat compared ext pre-allocated uniquely addressable inodes version information dynamically allocated directory blocks pointer start block file fat chains start block blocks file detecting deletes reliably underneath unmodified vfat impossible introduced additional field vfat directory entry tracks globally unique generation number generation number incremented create delete file system newly created file assigned current generation number small change lines code vfat generation change monitor accurately detects deletes interest faded ext ext exhibits reuse ordering tracking generation liveness ext tracking block liveness ext obey block exclusivity property tracking block liveness accurately impossible data journaling mode property data-metadata coupling ordered writeback modes make small change metadata transaction logged made ext log list data blocks allocated transaction change lines code coupled reuse ordering property enables accurate tracking deletes explicit secure delete built secure deletion explicit notification framework modified ext ext file systems notify disk logical delete file system modifications accounted lines code receiving notification disk decides shred block similar faded disk delays overwrites idle time minimize impact foreground performance evaluation section evaluate implicit explicit implementations secure delete enhanced disk implemented pseudo-device driver linux kernel driver observes information hardware prototype suffers contention cpu memory host ghz pentiumwith ram rpm ibm lzx disk due space constraints provide results ext version correctness accuracy test faded implementation detected deletes interest instrument file system log delete correlate log writes overwrites faded capture cases unnecessary missed overwrites tested system workloads technique including busy hours file system traces table presents results study trace hour experiment ran faded versions linux ext marked default appears sixth symposium operating systems design implementation osdi config delete overwrite excess miss indirect version table correctness accuracy table shows number overwrites performed faded configurations ext columns order number blocks deleted file system total number logical overwrites performed faded number unnecessary overwrites number overwrites missed faded note deletes occurred data write require overwrite config reads writes run-time version table impact performance performance file system configurations busy hour trace shown configuration show number blocks read written trace run-time ext file system indirect ext modified obey reuse ordering indirect blocks version ext modified increment inode version number truncate configuration represents correct file system implementation required faded column measure extra work faded order cope inaccurate information column number missed overwrites correct system fourth column cost inaccuracy reasonable faded performs roughly overwrites minimal amount note version number modification ext faded misses deletes reason missed overwrites reported version configuration rarity case involving misclassified indirect block performance impact evaluate performance impact made ext running trace versions ext table shows results performance reduction number blocks written marginally higher due synchronous bitmap writes indirect block reuse ordering conclude practical performance secure delete explore foreground performance implicit explicit secure delete cost overwrites foreground performance impact tracking block generation liveness requires faded perform extra processing cost reverse engineering directly impacts application performance incurred critrun-time system implicit explicit postmark trace trace default securedelete securedelete securedelete table foreground impact postmark trace run-times postmark trace shown faded overwrite passes comparison run-time explicit secure delete trace shown postmark configured files transactions ical path disk operation quantify impact extra processing required faded foreground performance software prototype competes cpu memory resources host worst case estimates overheads run postmark file system benchmark trace file system running top faded postmark metadata intensive small-file benchmark heavily exercises inferencing mechanisms faded arrive pessimistic estimate perform sync end phase postmark causing disk writes complete account time results note wait completion delayed overwrites numbers performance perceived foreground task table compares performance faded default disk explicit secure delete table overwrite passes foreground performance affected extra cpu processing faded lower performance compared modified file system running normal disk explicit implementation performs incur overhead inference require file system modifications reported table corresponds row table note model cost sending free command scsi bus overheads explicit case optimistic idle time required quantify cost performing overwrites shredding microbenchmarks verified overwrites obtained sequential bandwidth due delayed ordered issue found block reuse occurs file system resulting multiple deletes block delaying overwrites significantly reduces overwrite traffic omit results due space constraints explore time required overwrites postmark configuration measure time benchmark complete including delayed overwrites postmark deletes files end run face worst case scenario entire working set benchmark overwritappears sixth symposium operating systems design implementation osdi run-time overwrites system implicit explicit postmark trace trace default securedelete securedelete securedelete table idle time requirement table shows total run-time benchmarks postmark trace time reported includes completion delayed overwrites ten accounting large overwrite times reported table hp-trace overwrite times 
reasonable blocks deleted trace reused subsequent writes overwrites performed conservative accounts steep increase overwrite passes implicit case explicit implementation incurs lower overwrite times compared faded perfect information deletes avoids extra overwrites incurred due conservatism implicit detection ntfs section present experience building support implicit liveness detection underneath windows ntfs file system main challenge faced underneath ntfs absence source code file system basic on-disk format ntfs details update semantics journaling behavior publicly result implementation tracks block liveness requires knowledge on-disk layout generation liveness tracking implemented details ntfs journaling mechanism fundamental piece metadata ntfs master file table mft record mft information unique file piece metadata ntfs treated regular file file mft file recovery log allocation status blocks volume maintained file called cluster bitmap similar block bitmap tracked ext block allocations deletions ntfs regularly writes modified bitmap blocks prototype implementation runs device driver linux similar setup earlier file systems virtual disk interpose exported logical disk virtual machine instance windows running vmware workstation track block liveness implementation shadow bitmap technique mentioned section detailed empirical observation long-running workloads found ntfs exhibit violation block exclusivity delete suppression properties mentioned section due absence source code assert ntfs conforms properties limitation points general difficulty implicit techniques underneath closed-source file systems file system conforms properties guaranteed file system vendor absence guarantees utility implicit techniques limited optimizations afford occasionally wrong implicit inference experience ntfs points utility characterizing precise set file system properties required forms liveness inference set properties constitutes minimal interface communication file system storage vendors ntfs confirmed conformance block exclusivity delete suppression properties storage system safely implement aggressive optimizations rely implicit inference discussion section reflect lessons learned case study refine comparison strengths weaknesses explicit implicit approaches ideal scenario implicit approach required storage system file system interface practice accurate liveness detection requires file system properties means file system modified conform requisite properties face storage system file system implicit approach pragmatic explicit approach changing interface main reasons implicit approach file system required file system conforms requisite properties file systems ext vfat ext -data journaling ntfs amenable block liveness detection change file system ext file system data journaling mode conforms properties required generation liveness detection cases implicit approach enables non-intrusive deployment functionality modifying file system conform set well-defined properties general modifying file system interface convey specific piece information discussed file system properties viewpoint implicit liveness detection properties enable richer information inferred association block owning inode required applications file-aware layout tracked accurately file system obeys reuse ordering consistent metadata properties ultimate goal arrive set properties enable wide variety information tracked implicitly outlining file systems designed enable appears sixth symposium operating systems design implementation osdi transparent extension storage system contrast approach changing interface requires introducing interface time piece information required related work liveness information storage systems recognized previous work existing proposals interface communicate liveness part radical set existing storage interface logical disks list-based interface storage includes command delete block list recent work suggests object-like interface storage moves responsibilities low-level storage management liveness tracking file system drives contrast wide-scale explicit notification approach imparting liveness intrusive large body file systems utilize existing block-based interface storage work implementing smarts storage system interface change similar implicit approach systems utilize limited form liveness inference autoraid requires information free space decide amount data stored raidautoraid infers blocks written dead inference weak form liveness block written subsequent deletes detected systems programmable disk make similar inferences existence proposals liveness information important storage systems systematic techniques acquiring information missing related implicit techniques work previous work semantically-smart disks work presented techniques blockbased storage system infer file system level information implemented set case studies trackaligned extents journaling secure delete correctness-sensitive case studies implemented required file system synchronously mounted synchronous file systems implicit information tracking trivial recent work d-graid considered asynchronous file systems layout mechanisms d-graid depend accuracy correctness acceptable d-graid predictions wrong fast recovery d-graid utilized block liveness easier property track generation liveness specific assumptions file system behavior work previous work generalizing techniques inference underneath wide range realistic file system behaviors demonstrating storage-level functionality correctness paramount utilize information reliably conclusion system layers evolve explicit implicit file system storage system interface change change change time interfaces layers obsolete sub-optimal necessitating evolution presented approaches interface evolution explicit implicit context embedding liveness information storage qualitative summary complexity approaches axes presented figure shown explicit approach appearing straightforward entails fair amount file system change practice requiring minimal support storage system factors explicit approach results simpler systems implicit case main strength implicit approach permits demonstration functionality interface enabling seamless deployment catalyzing rapid interface evolution acknowledgments nitin agrawal john bent timothy denehy todd jones james nugent florentina popovici vinod yegneswaran helpful comments mendel rosenblum excellent shepherding anonymous reviewers thoughtful feedback gordon hughes comments secure delete work sponsored nsf ccrccr- ccrngs- itritr- ibm emc agrawal kiernan srikant hippocratic databases vldb bairavasundaram sivathanu arpaci-dusseau arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids isca bauer priyantha secure data deletion linux file systems usenix security august jonge kaashoek hsieh logical disk approach improving file systems sosp denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks usenix monterey june dowse malone recent filesystem optimisations freebsd freenix june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge self-organizing disk controller usenix jan ganger blurring line oses storage devices scs cmu-cs- dec ganger mckusick soules patt soft updates solution metadata update problem file systems acm tocs golding bosch staelin sullivan wilkes idleness sloth usenix winter pages gutmann secure deletion data magnetic solidstate memory usenix security july hughes personal communication appears sixth symposium operating systems design implementation osdi hughes coughlin secure erase disk drive data idema insight magazine katcher postmark file system benchmark netapp troctober mckusick joy leffler fabry fast file system unix tocs aug mesnier ganger riedel object-based storage ieee communications magazine august pennington strunk griffin soules goodson ganger storage-based intrusion detection watching storage activity suspicious behavior usenix security riedel kallahalla swaminathan framework evaluating storage system security fast roselli lorch anderson comparison file system workloads usenix ruemmler wilkes disk shuffling technical report hpl- laboratories schindler griffin lumb ganger trackaligned extents matching access patterns disk drive characteristics fast january 
sivathanu prabhakaran arpaci-dusseau arpaci-dusseau improving storage system availability graid fast mar sivathanu prabhakaran popovici denehy arpaci-dusseau arpaci-dusseau semantically-smart disk systems fast sourceforge srm secure file deletion posix systems http srm sourceforge net sourceforge wipe secure file deletion http wipe sourceforge net sourceforge linux ntfs project http linux-ntfs net strunk goodson scheinholtz soules ganger self-securing storage protecting data compromised systems osdi tweedie future directions ext filesystem freenix june vmware vmware workstation http vmware products wang anderson patterson virtual log-based file systems programmable disk osdi wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february gum chen wang krishnamurthy anderson trading capacity performance disk array osdi guaranteed detection deletes prove techniques faded ext guarantee shredding deletes blocks contents reached disk delete inode occurs ext set blocks freed file results increment version number reset relevant bits data bitmap block pertaining freed blocks block freed assume written disk context written disk disk perform overwrite case bitmap block status block inode possibilities reused file system written disk reused write case block reused reused immediately file bitmap block dirtied eventually written disk disk immediately delete block liveness module overwrite case block reused case whereb reused inode possibilities case point receiving write disk thinks belongs thinks free belongs inode case disk thinks disk knew disk tracked previous version number eventually observes write dirtied version number increment disk note version number increased overwrite blocks thought belonged case includesb thusb overwritten restoring newer discussed section conservative overwrite contents guaranteed shredded case disk thinks free disk thinksb free treat orphan block written mark suspicious written context inode contents shredded case disk thinks disk observed pointing point current write disk observed allocated file system case c-i disk observed allocated thinks written context means disk case block deleted time past order allocated led version number incrementing disk observes written perform overwrite thinks belong case c-ii occurs means written disk owning deleted written case written context live overwritten discussed section holds block exclusivity property ext note case block deleted file quickly reallocated file special case case cases block written disk context file delete block file lead shred deleted contents indirect block detection uncertain disk wrongly corrupt pointer false indirect block file system change reuse ordering indirect blocks prevents case 
evolving rpc active storage muthian sivathanu andrea arpaci dusseau remzi arpaci dusseau department computer sciences wisconsin madison fmuthian dusseau remzig wisc abstract introduce scriptable rpc srpc rpc-based framework enables distributed system services advantage active components technology trends point world component system disk network interface memory substantial computational capabilities traditional methods building distributed services designed advantage architectures mandating wholesale change software base exploit powerful hardware contrast srpc direct simple migration path traditional services active environment demonstrate power exibility srpc framework series case studies focus active storage servers speci cally advantages approach srpc improves performance distributed servers reducing latency combining execution operations server srpc enables ready addition functionality powerful cache consistency models realized top server exports simple nfs-like interface srpc simpli construction distributed services operations dif cult coordinate client server co-executed server avoiding costly agreement crash-recovery protocols introduction remote procedure call rpc long standard implementing distributed services rpc simple extends well-known paradigm procedure call client server setting powerful serve substrate distributed services sun nfs development rpc changed functionality offers modernized versions rpc java rmi significantly alter basic rpc paradigm simply provide features language run-time context rpc stagnating architecture distributed systems changing rapidly technology trends point world active components commonplace intelligence form additional processing capabilities permission make digital hard copies part work personal classroom granted fee provided copies made distributed pro commercial advantage copies bear notice full citation rst page copy republish post servers redistribute lists requires prior speci permission fee asplos san jose california copyright acm found active disks smart network interfaces intelligent memories importance active storage potential greatly improve distributed storage services terms performance functionality simplicity design early research active storage demonstrated potential performance bene previous systems require programming environments target limited class applications parallel applications database primitives existing distributed services clear migration path coming active world paper present design implementation evaluation scriptable rpc srpc extensible framework developing active distributed services key difference srpc traditional rpc clients send scripts server implement server-side functionality srpc designed assist developers evolving traditional rpc-based services rst-class active services srpc shares features rpc automates service development process increase ease-of-use srpc automatically embeds script interpreter server providing safe execution environment client-generated scripts demonstrate primary bene srpc number case studies distributed system called scfs scfs improves performance combining operations server avoiding costs extra round-trip latencies scfs ready addition functionality evolving base system protocol enhanced virtual protocol speci cally scripts applied implement afs sprite cache consistency top base protocol simple nfs-like consistency finally scfs simple implement functionality complex execute reliably client server scfs directory update operations performed script directly server obviate multi-client coordination introducing scripting server number issues arise concern safety client kernels partially trusted induce server crash accidentally malicious intent monopolize resources performance issue script interpretation overhead negate advantages srpc finally ease-of-use important criterion language enable development short powerful scripts prototype implementation srpc tcl base scripting language similar languages exist feel tcl interesting choice reasons tcl addresses safety concerns providing limited execution environment ne-grained control operations script execute loops disabled previous studies tcl interpreter thousand times slower recent versions tcl interpreter sophisticated cient internal representations boost performance time ripe reexamining strengths weaknesses tcl performance tcl fairly high-level language ease-of-use main advantages srpc effective framework extending scfs distributed service performance functionality enhancements readily designed deployed system framework case studies scripts reduce latency factor strengthen consistency semantics system server functionality readily embellished srpc framework similar extension methodology client-side kernels evaluation tcl language extensibility powerful remaining easy complex extension requires tens lines code performance tcl improved warrant consideration language extension numerous safety features sophisticated extensions afford slightest overheads specialized domain-speci language active storage worth investigating rest paper structured present overview srpc system section describe experimental environment including details scfs empirical methodology section section section section illustrate manner srpc improve performance functionality simplicity active services section perform in-depth analysis extension scripts tcl cover related work section conclude section overview srpc srpc framework building active extensible distributed services srpc extends widely understood paradigm rpc exible scripting capability clients harness execute customized extensions server srpc designed meet goals important environment rst goal provide smooth migration path existing distributed services traditional rpc-based server active server goal accomplished making process developing srpc-based service similar developing service traditional rpc paradigm automating steps goal ensure script execution overhead severe compared traditional rpc call improve performance srpc caches scripts supports concurrency cient implementations performancesensitive commands srpc standard library goal provide safe environment script execution choice tcl scripting language enables meet goal migration path rst goal srpc enable developers easily move distributed services traditional rpc-based servers powerful active servers srpc engineered backwardly compatible rpc allowing unmodi clients send scripts server co-exist clients altered functionality srpc automatically generates code interface rpc server scripting language interpreter traditional rpc developer distributed service nes interface server exports speci interface interface nition language idl interface nition parsed idl compiler generate source code client server stubs call rpc library developer implement procedures speci interface compiled automatically generated stubs produce distributed service developing active distributed service srpc involves similar set steps basic interface exported server speci idl functionality idl compiler augmented ways rpc procedure added interface called scriptexec takes parameters script data buffer returns data buffer result implementation scriptexec generated automatically idl compiler automatically generates tcl wrappers interface procedures client scripts directly call routines simplify srpc programming interpreter server commands extract speci parameters character buffer pass arguments compiler generates code initialize set tcl interpreters registers interface procedures interpreter increasing ease-ofuse reducing burden programmers assist programmers development scripts additional pieces functionality supported srpc infrastructure scripts maintain state accessed scripts clients script store state script access state state variables uniquely named scripts inadvertently con ict current srpc implementation responsibility namespace management rests clients follow pre-de ned conventions clients easily access state variables piece functionality ability server invoke functionality client perform callback srpc supports allowing client run rpc server high performance order srpc appealing platform distributed systems performance comparable traditional rpc service script interpretation imposes overhead time script executed potentially negating bene executing operations server srpc simple techniques improve performance caching concurrency standard library primitives performance initial versions tcl shown 
signi overhead overhead occurred line script re-interpreted execution cient repeated execution code recent versions tcl translate interpreted code cient internal form leverage behavior srpc identi scripts executed previously reuses cached procedures subsequent invocations arguments sending script invocation clients register scripts server back script passed server script arguments subsequent invocations caching mechanism advantage reducing size messages client server improving observed network latency reducing amount network traf case studies section quantify performance bene srpc script caching improve performance srpc framework executes multiple tcl interpreters concurrently multiple interpreters scripts execute parallel server greatly improving system throughput concurrent execution complicates development scripts discuss describing safety issues performance optimization srpc implement performance-sensitive operations inside srpc standard library directly script tcl supports calling functions written language providing functionality straightforward issue identify operations included standard library general operations popular scripts costly implement tcl current implementation srpc includes standard library routines manipulate data buffers search buffers string send messages create manage lists access shared script state acquire release locks importance routines enable scripts avoid touching data inside tcl ensuring data buffers solely manipulated substructure avoid primary source additional overhead implementing server routines directly tcl bypasses security issues scripting language anticipate trusted administrators add primitive routines safety goal srpc design ensure scripts server easily corrupt server state consume undue resources problem arises extensible operating systems research srpc arbitrary user applications insert code server kernel clients trust boundary relaxed boundary advantageous enforce limited execution context scripts fortunately tcl needed functionality required limit script actions safetcl extensions server control functions script call control constructs implemented tcl procedures prevent execution whileand loops ensure scripts terminate nite amount time scripts require iteration provide simple safe callback interface procedure executed nite number times complication safety model scripts manipulate buffer pointers directly ensure invalid memory generated srpc implements run-time checks inside wrapper standard-library routines pointers passed arguments run-time type checking automatically generated wrappers ensures argument addresses correspond correct type preventing illegal dereferencing arbitrary memory addresses additional level safety cost worthwhile complication arises due concurrency concurrent execution mandates locks scripts access shared state execute correctly locking introduces problems realm safe extensibility client misbehave release lock negatively impacting well-behaved clients prevent problem occurring srpc automatically releases locks acquired released executing script script completion safeguard restricts scope locks found burdensome limitation access control locks prohibiting misbehaving clients acquiring locks privy implemented functionality proc spritecb arg res state lock global variable global load cacheable flag set cache srpc load state state cache add client callback list set list srpc load state list set list srpc makelist srpc install state list srpc add list list args invoke read set rdarg scfs make rdarg arg rdoff set rdres scfs read rdarg extract components result set cnt scfs rdres getlen rdres set data scfs rdres getdata rdres pack state results result srpc putint res cacheoff cache srpc putint res cntoff cnt srpc memcpy res dataoff data cnt return expr cnt cachesz cntsize figure srpc script script performs read presence write-sharing scripts enforce spritely cache consistency augmenting stateless protocol stateful functionality involves handful tcl commands figure present srpc script script part sprite consistency implementation executed client reads marked uncacheable explored detail case study section rpc interface server exports read procedure idl compiler automatically generates tcl wrapper scfs read commands extract individual elds scfs rdres getdata make arguments type character buffer scfs make rdarg automatically generated type checking performed ensure script access invalid memory addresses examples standard library routines include srpc loadstate srpc putint script begins loading cacheable object interest set address client added object callback list script creates valid read argument data passed script accessed arg variable invokes read argument results read cacheable form result script packed result variable res returned caller evaluation environment evaluate srpc framework context distributed system scfs section begin presenting general overview scfs describe details experimental platform finally describe mechanism systematically increase network delay order understand performance srpc environments scripted file system scfs distributed systems built network-attached disks potential delivering high bandwidth additional cost servers evaluate bene srpc introduce scfs scripted distributed system networkattached storage base version scfs supports multiple clients single disk acts repository system data scfs exports hierarchical namespace applications performs basic caching multiple outstanding requests tolerate latency implements nfs-like weak cache consistency scheme paper demonstrate scripting capability provided srpc enhance performance functionality scfs prototype network-attached disk con gured export interfaces clients rst commonly object-based interface similar proposed gibson exported object namespace nonhierarchical distinguish directories normal les read write requests arbitrarily ne-grained object identi offset length blockbased interface explore capabilities srpc restricted legacy environment mode disk reads writes size clients communicate disk protocol similar nfs disk server maintaining state behalf clients slight difference nfs calls directory operations exist clients perform operations client runs in-kernel rpc server receive messages scripts executing disk experimental platform system runs testbed intel-based machines running linux operating system scfs clientside code developed kernel standard linux vnode interface mhz pentium iii memory ibm ultrastar lzx disk machines connected mbit ethernet gigabit ethernet experiments primarily mbit ethernet environment network-attached disk emulated exact capabilities match network-attached disk processor network-attached disk engineered low power consumption absolute performance reasonable approximation network-attached storage unit disks high-end processing capabilities rivaling cpu power commodity network emulation methodology potential performance bene active disk servers depend exact characteristics environment perceived network latency low smaller performance bene combining operations script contrast high-latency scenario wide-area dial-up link home performance bene larger study impact network performance scfs implemented framework increasing network latency controlled amount approach similar introduced martin study effects latency overhead bandwidth parallel applications speci cally delay packet server queues packet internally records time packet received thread server removes messages queue designated delay passed services experiments shown validate delay mechanism behaves desired performance enhancements achieve single logical task network system series dependent interactions client server required dependencies impose synchrony clientserver interactions operation initiated previous returned clients incur multiple network round-trips accomplish single logical task result signi performance loss high-latency networks severe server load scfs srpc framework group dependent operations script executed disk single network round-trip illustrate cacy scripting infrastructure case studies rst study combine client 
lookup operation dependent getattr read calls single script merge nfs-like consistency check read modi page similar http get-if-modi ed-since request avoid readmodify-write cycle sending partial write script server motivation discuss case studies rst examine effects server load perceived client latency network latency concern wide-area dial-up connections round-trip times milliseconds uncommon tightly-coupled cluster high-performance communication subsystem latency considerable factor due server load illustrate effects server load perceived round-trip time perform simple experiment experiment measure round-trip times observed single client communicating server small byte messages vary number competing clients continually sends stream large requests server competing processes perceived average round-trip time reasonable roughly gigabit ethernet network traf c-inducing competitors added perceived round-trip time client increases dramatically competing processes note effect strictly due queuing network interface server touch data cpu under-utilized problem illustrated obvious inspection overlooked design distributed systems clusters fact messages transmitted quickly idle system avoid convoy effect small messages queue large network interface promptly reach destination response time performance metric throughput server load easily transform low-latency network perceived authors discuss assumptions made implementation distributed cluster-based service cluster low-latency san latency wide-area internet means two-phase commits prohibitively expensive made similar assumptions design cluster-based systems page section average cost readdir getattr additional network delay performance combined read-getattr separate combined tcl uncached combined tcl cached combined figure combining operations read getattr gure shows performance combining read getattr operations single script gure plots average cost performing directory read subsequent attribute request increase perceived round-trip latency request x-axis separate line shows performance network round-trips required lines graph show performance combining operations server combined tcl uncached line shows tcl performance procedure installed request combined tcl cached line shows performance script pre-installed finally combined line baseline performance implementation script data point represents average trials variance shown low differently clients operation load experiments section evaluate performance scfs function network latency read getattr optimization rst case study illustrate performance bene combining directory read attributes directory disk object model basis scfs lookup client involves reading directory path search path component worst case path components local cache round-trip network operations required component path rst read directory page parent attributes child object initialize vfs inode scfs srpc infrastructure disk enables client send script reads directory page searches speci lename object performs getattr object script returns results read getattr calls client completes lookup operation path component network round-trip client system designed performs lookups components pathname single script avoids network round-trips approach integrate easily linux in-kernel framework evaluate trade-offs scripts read-getattr operation function network latency compare perceived latency implementations figure rst implementation labeled separate gure standard implementation synchronously issues separate requests rpc labeled combined tcl uncached tcl script combine operations script installed interpreter invocation labeled combined tcl cached considers case pre-installed script fourth implementation labeled combined combines operations language version serves baseline ideal combined performance utilized understand overhead scripting gure draw conclusions fast interpreted language approaches speeds combining read getattr operations single script leads lower overhead base implementation amount network latency caching tcl procedures greatly reduces cost operation removing half millisecond overhead operation finally higher latencies combining operations slow interpreter uncached tcl procedures strictly standard implementation note latencies higher end tightly-coupled cluster environment low dial-up link latencies milliseconds uncommon demonstrate utility read-getattr optimization realistic benchmark utilize postmark benchmark postmark system benchmark constructed mimic workload typical mail server consists create phase transaction phase les randomly created deleted read appended generate realistic workload make postmark benchmark separate create phase transaction phase present results transaction phase left unmodi creation phase warms cache arti cially speeds transaction phase change make results realistic layout directories default behavior postmark creates directories level directory hierarchy measurements reveal read-getattroptimization removes round-trip messages combining readandgetattr pairs combined read-getattrscript calls benchmark total messages simple optimization reduces number round-trip latencies incurred client read modi optimization case study illustrate scripting interface make nfs-style consistency checks cient nfs primitive form consistency periodically validating cached copy client server time validation threshold cached copy considered suspect read request suspect copy client sends getattr request server check changed cached changed client invalidates cached copy fetches pages server operations dependent client issue read knowing cached copy stale wastes network bandwidth re-fetch valid pages srpc framework operations merged single script getattr checks modi cation time higher speci modi cation time cached copy sends desired page attributes refer combined functionality read-if-modi ed-since operation srpc support average time file read percentage modified data read-if-modified-since delay separate combined tcl cached combined average time file read percentage modified data read-if-modified-since delay separate combined tcl cached combined figure nfs read-if-modi ed-since graphs plot performance ofgetattrfollowed conditional readof data data changed check graph top additional network latency graph bottom add xed network latency x-axis increase percentage les changed check data point average trials system performs logical task incurs fewer synchronous network round-trip delays figure shows performance read-if-modi ed-since compared standard getattr conditional read x-axis graphs vary percentage data modi read re-fetched server graph top plot performance assuming additional network delay graph bottom assume additional network delay compare performance standard implementation cached tcl script combined functionality implemented gure draw conclusions lowlatency environment illustrated graph top bene combination script small realized highperformance interpreter cached tcl implementation adds overhead higher-latency environment illustrated graph bottom large fraction les changed tcl implementation perform standard implementation greater average cost partial write additional network delay performance partial writes read-modify-write partial write tcl cached partial write figure partial writes gure shows performance performing synchronous partial write script reading data server modifying writing data server gure plots average cost performing synchronous -byte write increasing perceived latency request x-axis read-modifywrite line shows performance standard read-modify-write case lines show performance scripted approach partial write tcl cached line shows tcl performance script pre-installed partial write line baseline performance implementation script point average trials les optimizations combine operations limited circumstances overhead interpretation dominant factor higher performance interpreter worth implementing kinds features partial write optimization writes size page problematic traditional systems partial-page write translates read-modify-write cycle read relevant page cache modi cation affected portion page write complete update read-modify-write cycle occurs blockbased systems export read write operations page granularity srpc framework avoid read-modifywrite 
cycle send data written script performs partial write server removing synchronous page-sized read critical path figure displays performance server-side execution partial-write compared standard read-modify-write cycle gure avoiding synchronous page read great bene additional network delay avoiding excessive data movement read-modifywrite cycle pays dividends performing single round-trip factor reduced latency easily achieved evaluate utility partial-write optimization realistic benchmark setting addition postmark benchmark investigate traf savings debit-credit benchmark tpc-b benchmark intended model workload database server manages bank transactions measurements reveal partial-write script reduces bandwidth considerably postmark debit-credit benchmarks postmark total message traf bytes reduced roughly debit-credit benchmark savings dramatic full traf removed reason substantial reductions network bandwidth straightforward benchmarks perform small writes avoiding page-level data transmissions required readmodify-write cycle network load considerably reduced discussion case studies power combine arbitrary operations server helps overcome limitations exported rpc interface enabling clients build customized performance optimizations important bene approach arises server-side srpc designer distributed service focus simpler task providing highly-tuned primitives allowing clients compose primitives full scope required functionality noted srpc optimize performance network systems ways case studies presented speci cally client performs predictable series dependent operations server incurs multiple network round-trips operations combined single script additional case system ensure set writes reach disk xed order crash recovery traditional system client ensure ordering perform write synchronously label type synchronous operation false synchrony client synchronous operations enforce order ensure operation reached stable storage srpc framework false synchrony avoided client perform writes asynchronously guaranteed writes reach disk desired order functionality enhancements traditional servers implement single xed protocol size solution limits functionality clients expect server exible approach wrongly assumes single protocol meets requirements clients respect les consistency semantics client system implement strongly constrained protocol interface exported server server exports nfs-like interface restricts clients weak level consistency server stronger consistency model forces clients incur consequent overhead srpc clients enhance physical protocol provided server implement enhanced virtual protocols srpc enable clients implement sophisticated consistency semantics top nfs-like physical protocol case studies implement afs sprite consistency semantics cases scfs interface remains existing clients nfs-like semantics continue operate smoothly key feature examples demonstration framework state easily added previously stateless protocols nfs earlier work spritely nfs demonstrated capabilities rewriting server client extensively note consistency semantics functionality enhancement scripts utilized implement object-based disk interface top block-based server hinted section average time close client callbacks broken afs close script overhead tcl-loop broadcast serial std lib broadcast serial std lib broadcast parallel figure afs close script overhead average cost executing afs close dirty script shown number callbacks broken increases afs consistency afs write-on-close consistency model clients consistent image open close operation open client installs callback server reads entire local disk cache subsequent reads writes performed local copy closed client writes back modi server client stores modi version back server callbacks broken clients invalidate cached copies forcing fetch server time open server actively involved ensuring afs consistency server exports xed nfs protocol support afs semantics srpc infrastructure disk implementing afs consistency semantics feasible servers track state variables live invocation single script afs relevant state disk object callback list list client machines cached scripts required afs open afs close dirty open client sends afs open script installs client address per-object callback list client reads object standard read interface caches local disk modi closed client client sends afs close dirty script server script loads callback list object invokes send rpc list library routine send callback break listed clients performance consistency model dif cult measure present time takes execute afs close dirty script number callbacks broken increased figure plots performance implementations script rst labeled tcl-loop broadcast serial script sends message synchronously clients callback list case labeled std lib broadcast serial tcl script makes single call standard library routine synchronously issues rpc callbacks clients performance improves version due fewer number crossings tcl boundary finally case labeled std lib broadcast parallel multiple threads standard library issue callexperiment avg read cost overhead direct read tcl cached table read write-sharing overhead average cost executing sprite read callback script shown rst row table labeled direct read shows cost reading page synchronously serves lower bound execution time rows table show cost executing tcl implementations script establishing overhead reads experience write-sharing experiment read cost calculated average reads occur large copy back requests asynchronously issuing rpc callbacks parallel improves performance slightly experiment demonstrates performance bene including primitives standard library sprite consistency sprite stronger consistency model afs semantics referred perfect consistency close approximation unix local system semantics sprite model clients cache les long write sharing write sharing occurs client open opened write mode write-sharing occurs sprite turns caching clients send reads directly server finding write-shared requires server track state unmodi nfs-based system major design concern implementing sprite semantics scfs ensure reads writes slowed common case write sharing occurs executing scripts negative performance impact script part read write operation meet goal developed design object state variables read callback list list clients open cached reading writers list tracks clients open writing designating cacheable on-going write-sharing client state believes non-cacheable read request client sends sprite read callback script sprite read callback script checks cacheable set script adds address client read callback list returned client client mark state local inode cacheable subsequent reads scripted issued normal rpc read requests write sharing overhead paid scripting client opens write mode client sends sprite write open script script sends invalidate rpc members read callback list marks non-cacheable adds address writers list object registered writer client marks cacheable local inode in-memory cache satisfy read requests client closes sends sprite write close script disk removes client writers list marks cacheable writers list empty registered writers subsequent scripted-reads experiment avg update cost overhead base tcl cached table directory update script overhead average cost executing directory update script shown tcl implementations compared version concurrency control experiment object created inserted directory tcl implementations directory information server contrast base entry shows performance updating directory directly concurrency protocol case directory page written server update clients notice cacheable status cache overhead scripts reading shown table direct read entry shows average cost read copy assuming write-sharing case experiment read takes roughly entry shows cost executing 
version sprite read callback script adds roughly overhead finally tcl version script runs approximately slowly non-scripted read experiment importance calling sprite read callback script discussion general ability extend interface server scripts expands types functionality client systems implement case studies shown functionality requires state object server brie discuss examples stateful scripts implement scfs functionality client system implement ne-grained copy-onwrite system state required track byte ranges object copied script required client reads writes write script transparently redirects write copied version object read script directs read version object imagine optimizations similar sprite case study avoid invoking scripts read write operation clients associate arbitrary type meta-data virtual meta-data interesting piece extensible meta-data general access control list acl provide exible sharing permission bits provided server system read write operation client send script check credentials user fully ensure clients bypass protection check simply calling existing rpcbased read write calls permission bits disabled potential security hole illustrates general principle scripts assume cooperation set mutuallytrusting clients client call scripts enforce sprite consistency semantics clients desired behavior simplicity enhancements srpc arbitrary operations grouped executed server greatly simpli implementation atomic sets operations isolated respect read partial afs sprite directory operation attr modi write open close read callback write open write close update sec sec sec sec sec sec arithmetic ops control list ops state mgmt type checking library locks communication native calls total lines table tcl functional breakdown table categorizes static operation counts script script minimum maximal path costs shown left-most column presents categories arithmetic operations control statements list operations create add iterate state management stateful protocols type checking library searches copies utility functions locks lock unlock communication extra rpcs mandated rpc reply native calls calls raw rpc-exported functions total bottom column lists total number lines script note number lower higher sum previous operations cases statements accounted separately end brace simple assignment statements cases line consist multiple statements arguments math expressions script identi column header section number concurrent operations clients demonstrate case study srpc simplify implementation seemingly complex distributed concurrency problem concurrent directory updates disk object model directories considered equivalent objects disk directory operations create delete translate reads writes server create client rst reads directory page inserts directory entry pertaining vacant slot writes page back proper concurrency control simultaneous creates directory clients lead lost create scfs ensure directory readmodify-write sequence performed atomically traditional server common ensure atomicity distributed locks client rst acquires lock directory object performs read-modify-write nally unlocks directory approach result sub-optimal performance due multiple network roundtrip operations required phases importantly makes system signi cantly complex speci cally server track locks multiple client machines handle distributed failure scenarios client crashing momentarily losing network connectivity holding lock srpc framework greatly simpli implementation atomic operations co-locating server operations distributed machines concurrent directory update case study client sends script disk acquires in-memory lock server performs read-modify-write releases srpc reduce complex distributed concurrency problem simpler challenge ensuring mutual exclusion threads server address space table shows performance tcl implementation directory update compared cost simply sending directory page server approach provide concurrency control represents cost simply writing page perform directory update table overhead simple concurrent directory update satisfactory providing functionality dif culties encountered implementing robust three-phase protocol discussion case study brie illustrated srpc simplify client server code centralized script server simplify functionality cases traditional distributed algorithms required multiupdate atomic transactions natural provide scripting framework transactional capabilities mandate additional functionality srpc standard library including ability roll back perform crash recovery future plan investigate utility transactional support srpc framework analysis tcl section explore costs script execution tcl environment provide detailed accounting scripts implemented including functional cost breakdowns end discussion ndings functional breakdown table shows breakdown tcl commands scripts case studies grouped categories arithmetic operations control statements statements list operations number scripts lists basic data structure state management routines saving restoring longlived server state type checking ensure illegal memory dereferences occur library routines utility functions copies string searches locks communication single reply mandated rpc native calls rpc-exported routines table make number general observations line column total number lines script observe scripts implement powerful functionality small amount code scripts lines code regime ranging low high -line range low end afs open afs close dirty scripts simply manage state required track clients les open partial write script performs server-side read-modify-write expected manner complex script sprite write open script straight-forward lines consist simple state list management routines learn table scripts broken constituent commands scripts controlow decisions complex sprite write open sprite write close scripts statements largely composed straight-line code scripts require state afs sprite consistency scripts tcl command count consists retrieving state manipulating form list operations storing state finally type checking library operations comprise substantial component scripts cost breakdown functional breakdown instructive timebased analysis dif cult pinpoint location bottlenecks garner insight instrumented scripts case studies allowing collect detailed information time spent scripts figure presents results investigation gure make number observations scripts invocation overhead accounts substantial portion time execute scripts time consists statements set-up relevant environment script call tcl eval invoke script percentage varies scripts invocation overhead fairly constant varying detailed instrumentation reveals cost roughly two-thirds attributed tcl eval call type checking buffer pointers library commands combine signi amount time scripts current system type check requires call tcl substructure calls expensive future versions cost reduced batching type checks library commands routines primarily called data copy routines manipulate input parameters construct return results overheads dif cult avoid additional communication expensive dwar costs effect observed afs close dirty script experiment breaks single callback client cached copy relevant cost communication script dominates costs finally native routines account reasonable amount time scripts unlike components bar higher percentage native portion bar portion represents direct calls underlying service real work script performs native portion read-if-modi ed-since optimization consists getattr conditional read functions copy data perform type checking pure overhead discussion small code size demonstrated case studies arguments scripting approach extensibility small code segments easier write maintain fewer lines code implies fewer bugs leading cost breakdown tcl scripts time invocation math list ops state mgmt type check library locks comm native read-getattr read-if-mod partial-write afs open afs close sprite 
read sprite open sprite close dir update figure tcl cost breakdown graph depicts percentage time spent script typical scenario bar represents single script labeled x-axis broken categories based operational breakdown table additional item breakdown invocation cost time spent infrastructure tcl script control statements low cost accounted graph absolute execution times script read-getattr read-if-modi ed-since partial-write afs open afs close sprite read callback sprite write open sprite write close directory update measurements ect average runs robust reliable systems experience brought downsides scripting performance conscious tcl programming dif cult analogous process language environments costs arise unexpected sources making dif cult programmers optimize code simple math expression set expr executes roughly pentium-based platforms slight variant set expr extra spaces executes factor slower speci case parsing single argument passed exprcommand faster passing separate arguments cases found subtle differences programming style lead non-trivial differences performance general problem illustrated dif culty programming top system high-level tcl interpreter complex virtual machine dif cult process programming high-performance makes times slower expected related work active storage active storage forms existing literature previous work context programming environments existing rpc-based services easily earliest work found database literature researchers sought exploit processing capability disk arm increase database performance recent efforts active storage termed active disks studied independently acharya riedel acharya proposed specialized stream-based programming model parallel applications model applications re-partitioned host disk portions disk runs disklet small piece java code lters data per-record granularity riedel studied parallel applications focusing scan-intensive codes applications partitioned host disks disk runs small portion code lter requests reduce total bandwidth host active disk systems effective supporting variety user-level parallel applications developing general distributed service recently amiri introduced abacus object-oriented framework developing active storage systems abacus similar srpc authors developed distributed object storage system top abacus demonstrated performance bene abacus differs distributed object store built scratch distributed object environment code exists user-level abacus well-integrated kernel mount abacusbased system system calls redirected inside kernel user-level proxy inef cient main strengths abacus approach work dynamically migrated client server depending system workload characteristics similar adaptive framework utilized srpc extensibility srpc related long line work extensible systems pioneered systems spin exokernel vino systems sought enable extensibility operating systems seek enable extensibility rpcbased services lessons learned systems apply srpc techniques vino survive misbehaving kernel extensions directly applicable framework slice virtual service extends services client side interposition introducing client-side packet lters slice build virtual service top existing protocols nfs transparently system clients slice interposition srpc server-side scripting complimentary approaches slice adding activity client srpc adding server suggested utility scripting languages extensible systems direct found choices operating system authors suggest tcl-like scripting language extensions argue exibility safety provided interpreted languages outweigh potential performance loss hypothesis topics investigate paper similarly swarm scalable storage system sends tcl scripts servers order read write data authors state swarm makes feature debugging rpc realm rpc aware system highly similar srpc recent work area concentrates increasing performance exibility rpc substrate reducing code size automatically-generated stubs excellent found work flick exible infrastructure building optimizing rpc layer main goal flick separate cient stub generation speci interface-de nition language idl underlying communication layer work scriptable rpc extended flick framework conclusions current technology trends expect core building blocks future systems signi processing capabilities imperative future distributed services ability effectively leverage active components paper introduced srpc scriptable rpc layer enables system developers migrate existing distributed services active servers designed srpc meet goals essential environment srpc designed provide smooth migration path existing distributed services traditional rpc-based server active server goal accomplished making process developing srpc-based service similar developing service traditional rpc paradigm automating steps enhanced idl compiler srpc engineered high performance srpc caches scripts support concurrent script execution key operations implemented directly efciency srpc targets safe extensibility tcl base scripting language crucial factor meeting goal number case studies demonstrated general bene scfs traditional rpc-based system shown srpc improve client performance scripting client merge operations dependencies single operation client disk reducing number network round-trips demonstrated srpc enables functionality easily integrated system cases original designers foresee bene functionality cases clients desire functionality les finally shown scripts permit operations co-located disk distributed multiple clients acquiring releasing locks avoiding complex code crash recovery distributed failure scenarios evaluation tcl tcl enables development short powerful scripts complex case studies required thirty lines tcl code performance tcl past tcl system extensions scenarios higher performance required specialized domain-speci language active disks worth investigating desirable feature language predictable cost model allowing programmers optimize code direct obvious manner future interesting examine srpc context multiple network-attached disks single server active framework disks communicate cooperate directly implementing advanced features snap-shots lazy redundancy key challenge system provide proper primitives distributed coordination server-side scripts removing burden implementing complex distributed systems protocols ideal storage system future simple collection srpc-enabled disks base primitives distributed computation srpc standard library higherlevel le-system functionality built top exible cient scripting substrate acknowledgments venkateshwaran venkataramani work project early stages skepticism led nement ideas members wind research group input ideas presentation paper csclass lively feedback finally anonymous reviewers excellent thoughtful suggestions greatly improved content paper work sponsored nsf ccrccr- ngsccr- itrand wisconsin alumni research foundation acharya uysal saltz active disks proceedings conference architectural support programming languages operating systems asplos viii san jose october amiri petrou ganger gibson dynamic function placement data-intensive cluster computing proceedings usenix annual technical conference pages june anderson chase vahdat interposed request routing scalable network storage transactions computer systems tocs february berners-lee fielding nielsen gettys mogul hypertext transfer protocol http technical report internet engineering task force january bershad savage przemyslaw pardyak fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles december birrell nelson implementing remote procedure calls acm transactions computer systems february boden cohen felderman kulawik seitz seizovic myrinet gigabitper-second local-area network ieee micro february campbell tan choices objectoriented multimedia operating system workshop hot topics operating systems hotos-v orcas island chen bershad impact operating system structure memory system performance proceedings acm symposium operating systems principles sosp pages asheville december dahlin wang anderson patterson cooperative caching remote client memory improve file system performance proceedings usenix symposium operating systems design implementation 
pages november davis principles software development mcgrawhill denehy arpaci-dusseau arpacidusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey june dewitt hawthorn performance evaluation data base machine architectures proceedings seventh annual conference large data bases vldb pages eide frei ford lepreau lindstrom flick flexible optimizing idl compiler pldi las vegas june engler kaashoek toole exokernel operating system architecture application-level resource management proceedings acm symposium operating systems principles december fiuczynski martin bershad culler spine operating system intelligent network adapters technical report tr- washington department computer science engineering august gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings conference architectural support programming languages operating systems asplos viii october gibson nagle amiri chang feinberg gobioff lee ozceri riedel rochberg zelenka file server scaling network-attached secure disks proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle june gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie-mellon gokhale schmidt measuring performance corba internet inter-orb protocol atm technical report wucs- washington louis gray storage bricks arrived invited talk usenix conference file storage technologies fast gribble brewer hellerstein culler scalable distributed data structures internet service construction proceedings fourth symposium operating systems design implementation osdi san diego october hartman murdock spalink swarm scalable storage system proceedings ieee international conference distributed computing systems icdcs austin texas june howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems tocs february katcher postmark file system benchmark technical report trnetwork appliance october kistler satyanarayanan disconnected operation coda file system acm transactions computer systems tocs february lee thekkath petal distributed virtual disks proceedings seventh conference architectural support programming languages operating systems asplos vii pages cambridge october lowell chen free transactions rio vista proceedings acm symposium operating systems principles sosp pages saintmalo france october mai paaske jayasena dally horowitz smart memories modular recon gurable architecture proceedings annual international symposium computer architecture pages june martin vahdat culler anderson effects communication latency overhead bandwidth cluster architecture proceedings annual international symposium computer architecture pages denver colorado june acm sigarch ieee computer society tcca muthitacharoen chen mazi eres lowbandwidth network file system proceedings acm symposium operating systems principles sosppages banff canada october nelson welch ousterhout caching sprite network file system acm transactions computer systems february nester philippsen haumacher cient rmi java proceedings acm java grande conference san francisco california june malley proebsting montz usc universal stub compiler proceedings conference communications architectures protocols applications sigcomm london august ousterhout tcl embedable command language proceedings usenix association winter conference patterson anderson cardwell fromm keeton kozyrakis thomas yelick intelligent ram iram chips remember compute ieee international solid-state circuits conference san francisco february riedel gibson faloutsos active storage large-scale data mining multimedia proc international conference large databases vldb august romer lee voelker wolman wong baer bershad levy structure performance interpreters proceedings seventh international conference architectural support programming languages operating systems pages cambridge massachusetts october acm sigarch sigops sigplan seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings usenix symposium operating systems design implementation october small seltzer comparison extension technologies proceedings usenix annual technical conference january srinivasan mogul spritely nfs experiments cache-consistency protocols proceedings twelfth acm symposium operating systems principles pages acm december order thekkath mann lee frangipani scalable distributed file system proceedings acm symposium operating systems principles sosp pages saint-malo france october transaction processing council tpc benchmark standard speci cation revision technical report von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings fifteenth acm symposium operating systems principles pages copper mountain resort usa von eicken culler goldstein schauser active messages mechanism integrated communication computation proceedings annual symposium computer architecture gold coast australia welsh culler achieving robust scalable cluster java lcr workshop languages compilers run-time systems scalable computers rochester 
robust portable scheduling disk mimic florentina popovici andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract propose approach scheduling performs on-line simulation underlying disk simulation integrated system key challenges addressed rst simulator portable full range devices conguration automatic computation memory overheads low simulator disk mimic achieves goals building table-based model disk observes times previous requests show shortest-mimicked-timerst smtf scheduler performs approach perfect knowledge underlying device superior traditional scheduling algorithms c-look sstf results hold seek rotational characteristics disk varied introduction high-performance disk schedulers explored research literature progressively tuned performance characteristics underlying disks generation disk schedulers accounted behavior storage devices time disk schedulers analyzed focused minimizing seek time seek time order magnitude greater expected rotational delay early focus disk schedulers shifted rotational delay account rotational delays seek costs balanced level sophistication disk scheduler takes aspects underlying disk account track cylinder switch costs cache replacement policies mappings logical block number physical block number zero-latency writes worthington demonstrate algorithms effectively utilize prefetching disk cache perform intricate knowledge scheduler disk barriers realization operating system kernels speci cally obstacles overcome scheduler discover detailed knowledge underlying disk variety tools automatically acquire portions knowledge embedded disk model employed scheduler resulting scheduler con gured handle single disk speci characteristics disk scheduler knowledge current state disk exact position disk head head position exposed current disk controllers position predictable due low-level disk techniques wear leveling predictive failure analysis log updates scheduler control current position non-trivial techniques finally computational costs detailed modeling high uncommon time model request time larger time service request due dif culties disk schedulers leverage basic seek costs implemented real disks rotational position previous work performed simulation environments schedulers recently implemented researchers contained substantial simpli cations painstakingly tuned small group disks surprisingly disk schedulers found modern operating systems linux netbsd solaris attempt minimize seek time approach promising alternative approach embedding detailed knowledge disk scheduler embed on-line simulator disk scheduler scheduler on-line simulation underlying storage device predict request queue shortest positioning time variety disk simulators exist targeted performing traditional off-line simulations infrastructure performing on-line simulation fundamentally respects requirements on-line simulator stringent off-line simulator on-line simulator portable simulator model behavior disk drive practice on-line simulator automatic run-time con guration precise characteristics proceedings usenix annual technical conference june san antonio texas underlying device constructing simulator highly undesirable human administrator interact simulator finally on-line simulator low overhead computation memory overheads on-line simulator minimized simulator adversely impact system performance addition complexity introduces on-line simulator ample opportunities simpli cation on-line simulator opportunity observe run-time behavior device simulator con gure simulator adjust behavior device time on-line simulator specialized problem domain question finally on-line simulator parameterizable on-line simulator exploring versions device simulator functional model device contributions address implement scheduler aware underlying disk technology simple portable robust manner achieve goal introduce disk mimic meets requirements on-line simulator disk scheduling disk mimic based simple table-based approach input parameters simulated device index table entry table predicted output device table-based approach on-line simulation portably capture behavior variety devices requires manual con guration performed computational overhead signi challenge size table tractable identify input parameters signi cantly impact desired outputs method reducing input space depends largely domain on-line simulator deployed show disk scheduling input parameters suf cient predicting positioning time logical distance requests request type inter-request distance prediction issues resolved inter-request distance fairly coarse predictor positioning time result high variability times requests distance implication disk mimic observe instances distance semantically-smart disk systems muthian sivathanu vijayan prabhakaran florentina popovici timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison abstract propose evaluate concept semantically-smart disk system sds opposed traditional smart disk sds detailed knowledge system disk system including information on-disk data structures system sds exploits knowledge transparently improve performance enhance functionality beneath standard block read write interface automatically acquire knowledge introduce tool eof discover le-system structure types systems show sds exploit knowledge on-line understand le-system behavior quantify space time overheads common sds showing excessive study issues surrounding sds construction designing implementing number prototypes case studies case study exploits knowledge aspect system implement powerful functionality beneath standard scsi interface surprising amount functionality embedded sds hinting future disk manufacturers compete enhanced functionality simply cost-per-byte performance introduction true knowledge confucius microprocessors memory chips smaller faster cheaper embedding processing memory peripheral devices increasingly attractive proposition placing processing power memory capacity smart disk system functionality migrated system disk raid providing number potential advantages traditional system computation takes place data improve performance reducing traf host processor disk disk system exploit low-level information typically le-system level including exact head position blockmapping information finally unmodi systems leverage optimizations enabling deployment broad range systems smart disk systems great promise realizing full potential proven dif cult causative reason shortfall narrow interface systems disks disk subsystem receives series block read write requests inherent meaning data structures system bitmaps tracking free space inodes data blocks directories indirect blocks exposed research efforts limited applying disk-system intelligence manner oblivious nature meaning system traf improving write performance writing blocks closest free space disk ful potential retain utility smart disk systems smarter interface storage remains system acquire knowledge system exploit understanding order enhance functionality increase performance storage system understands blocks constitute perform intelligent prefetching perle basis storage system blocks unused system utilize space additional copies blocks improved performance reliability storage system detailed knowledge system structures policies semantically-smart disk system sds understands meaning operations enacted important problem solved sds information discovery disk learn details system on-disk data structures straight-forward approach assume disk exact white-box knowledge system structures access relevant header les cases information unavailable cumbersome maintain paper explore gray-box approach attempting proceedings usenix conference file storage technologies fast san francisco california automatically obtain le-system speci knowledge storage system develop present ngerprinting tool eof automatically discovers le-system layout probes observations show eof smart disk system automatically discover layout class systems similar berkeley fast file system ffs show exploit layout information infer higher-level le-system behavior processes classi cation association operation inferencing refer ability categorize disk block data inode bitmaps superblock detect precise type data block directory indirect pointer associate data block inode relevant information identify higher-level operations creation deletion sds techniques implement desired functionality prototype smart disk system software infrastructure in-kernel driver interposes read write requests system disk prototype environment explore challenges adding functionality sds adhering existing interfaces running underneath stock system paper focus linux ext ext systems netbsd ffs understand performance characteristics sds study overheads involved ngerprinting classi cation association operation inferencing microbenchmarks quantify costs terms space time demonstrating common overheads excessive finally illustrate potential semantically-smart storage systems implemented number case studies sds framework aligning les track boundaries increase performance smallle operations information lesystem structures implement effective secondlevel caching schemes volatile non-volatile memory secure-deleting disk system ensures non-recoverability deleted les journaling storage system improve crash recovery time case studies demonstrate broad range functionality implemented semantically-smart disk system cases demonstrate sds tolerate imperfect information system key building robust semantically-smart disk systems rest paper organized section discuss related work discuss lesystem ngerprinting section classi cation association section operation inferencing section evaluate system section present case studies section conclude section related work related work smart disks grouped categories rst group assumes interface storage systems xed changed category sds belongs research group proposes storage interface requiring systems modi leverage interface finally group proposes interface programming model applications fixed interfaces focus paper integration smart disks traditional system environment environment system narrow scsi-like interface storage disk persistent store data structures early smart disk controller loge harnessed processing capabilities improve performance writing blocks current disk-head position wang log-based programmable disk extended approach number ways quick crash-recovery free-space compaction systems assume require knowledge system structures storage system interfaces developed provided local setting opportunities functionality network packet lter slice virtual service slice interpose nfs traf clients implement range optimizations preferential treatment small les interposing nfs traf stream simpler scsi-disk block stream contents nfs packets well-de ned high-end raid products perfect place semantic smartness typical enterprise storage summary metric distribution experimentally found summarizing small number samples works large number inter-request distances modern disk drive disk mimic record distances table reasonable size show simple linear interpolation represent ranges missing distances long number interpolations range checked measured values propose disk scheduling algorithm shortestmimicked-time- rst smtf picks request predicted disk mimic shortest positioning time demonstrate smtf scheduler utilize disk mimic ways specifically disk mimic con gured off-line on-line approaches performed automatically disk mimic con gured off-line performs series probes disk inter-request distances records resulting times scenario disk mimic complete control inter-request distances observed interpolated disk mimic con gured on-line records requests running workload resulting times note disk mimic con gured off-line on-line simulation performed on-line active system show disk mimic significantly improve throughput disks high system substantial processing capabilities memory capacity emc utilization speci cally variety simulated real disks c-look sstf perform slower smtf demonstrate disk mimic successfully con gured on-line show disk mimic learns storage device smtf performs worse base scheduling algorithm c-look sstf quickly performs close off-line con guration approximately requests rest paper organized section describe smtf scheduler detail section describe disk mimic describe basic methodology evaluation section investigate issues con guring disk mimic offline section describe additional complexities con guring disk mimic on-line show performance section finally describe related work section conclude section scheduler modern disks implement scheduling device suggest system scheduling obsolete reasons system perform scheduling disks schedule limited number simultaneous requests restrictive space computational power constraints instances increased functionality requires scheduling system level iyer druschel introduce short waiting times scheduler proceedings usenix annual technical conference june san antonio texas preserve continuity stream requests single process interleaving streams processes shenoy vin implement service requirements applications implementing scheduling framework system brie describe approach system scheduler leverages disk mimic refer algorithm implemented scheduler shortest-mimicked-timerst smtf basic function smtf performs order queue requests request shortest positioning time determined disk mimic scheduled basic role optimizations made assumptions paper assume goal scheduler optimize throughput storage system fairness scheduler techniques achieving fairness weighting request age added smtf assume scheduler operating environment heavy disk traf queues disk hundreds thousands requests computational complexity scheduling algorithm important issue large queue lengths feasible perform optimal scheduling decision considers combinations requests greedy approach time request minimized evaluate performance smtf compare algorithms practice rst-comerst-served fcfs shortest-seek-timerst sstf c-look fcfs simply schedules requests order issued sstf selects request smallest difference logical block number lbn accessed disk c-look variation sstf requests serviced lbn proximity request serviced scheduler picks requests ascending lbn order requests serviced algorithm picks request queue lowest lbn continues service requests ascending order compare performance case implemented best-case-greedy scheduler simulated disks best-case scheduler long request simulated disk greedily picks request shortest positioning time refer scheduler greedyoptimal scheduler disk 
mimic disk mimic capture behavior disk drive portable robust cient manner predict performance disk disk mimic simple table indexed relevant input parameters disk disk mimic attempt simulate mechanisms components internal disk simply reproduces output function inputs observed reducing input parameters disk mimic table-driven approach predict time request function observable inputs fundamental issue reducing number inputs table tractable number device treated true black box internal behavior device disk mimic assume service time request function previous requests request ned parameters read write block number size time request data leads prohibitively large number input parameters indices table tractable approach make assumptions behavior device problem domain interest goal scheduler portable realistic range disk drives necessarily work hypothetical storage device high-level assumptions disks behave eliminate signi number input parameters disk mimic make assumptions current implementation disk mimic predicts time request input parameters request type inter-request distance interrequest distance logical distance rst block current request block previous request conclusion request type inter-request distance key parameters agrees previous researchers brie argue inter-request distance request type suitable parameters domain begin summarizing characteristics modern disk drives discussion classic paper ruemmler wilkes interested reader referred paper details background disk drive platters platter surface disk head reading writing surface data stored series concentric circles tracks single stack tracks common distance spindle called cylinder modern disks ram perform caching proceedings usenix annual technical conference june san antonio texas caching algorithm dif cult aspects disk capture model accessing block data requires moving disk head desired block time dominant components rst component seek time moving disk head desired track seek time reads writes reads performed aggressively read performed block read repeated performed wrong sector write rst verify sector avoid overwriting data component rotation latency waiting desired block rotate disk head time platter rotate roughly constant vary nominal rate result dif cult predict location disk head disk idle revolutions important positioning components mechanical movements accounted head track switch time head switch time takes mechanisms disk activate disk head access platter surface track switch time takes move disk head track cylinder rst disk appears client linear array logical blocks logical blocks mapped physical sectors platters indirection advantage disk reorganize blocks avoid bad sectors improve performance disadvantage client logical block located client derive mapping multiple sources complexity tracks numbers sectors specifically due zoning tracks platter sectors subsequently deliver higher bandwidth tracks spindle consecutive sectors track cylinder boundaries skewed adjust head track switch times skewing factor differs zones awed sectors remapped sparing sparing remapping bad sector track xed alternate location slipping sector track subsequent sector track input parameters previously explained read write operations times execute addition type operation issued uences service time account factors table-based model record request type read write current previous requests input parameters input parameter inter-request distance logical block addresses captures aforementioned underlying characteristics disk missing note ordering requests based time distance signi cantly distance due complexity disk geometry requests separated larger logical distance positioned rapidly relationship logical block address distance positioning time linear opinion ruemmler wilkes aspects disk modeled accuracy seek time calculated separate functions depending seek distance current nal cylinder position disk head reads writes head track switches rotation latency data layout including reserved sparing areas zoning track cylinder skew data caching read-ahead write-behind brie discuss extent components captured approach approach accounts combined costs seek time head track switches rotation layout probabilistic manner inter-request distance probability request crosses track cylinder boundaries requests distance cross number boundaries total positioning time number track seeks number head track switches amount rotation note table-based method tracking positioning time accurate advocated ruemmler wilkes expressing positioning time computed sum functions seek time rotation time caching disk mimic records precise positioning time distance cost incurred rotation disk components rotational distance previous current request elapsed time requests amount rotation occurred inter-request distance probabilistically captures rotational distance disk mimic record amount time elapsed request omission issue disk scheduling presence full queue requests case inter-arrival time requests disk negligible ignoring time inaccuracies scheduling rst request idle period disk idle scheduling important problem data layout incorporated fairly disk mimic number sectors track number cylinders impact measured values sizes determine probability request inter-request distance crosses boundary sizes impact probability observed time distriproceedings usenix annual technical conference june san antonio texas time number requests time number requests time number requests figure distribution off-line probe times inter-request distances graph shows inter-request distance x-axis show probes performed sorted time y-axis show time probe times ibm lzx disk bution zoning behavior bad sectors tracked model previous research shown level detail scheduling symmetrix server eighty mhz motorola microprocessors con gured memory high-end raid systems leverage resources perform bare minimum semanticallysmart behavior storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically paper explore acquisition exploitation detailed knowledge system behavior expressive interfaces primary factors limits addition functionality smart disk narrow interface systems storage surprising research investigates changing interface brie highlight projects mime investigates enhanced interface context intelligent raid controller speci cally mime adds primitives proceedings usenix conference file storage technologies fast san francisco california clients control updates storage visible traf streams commit order operations logical disks expand interface allowing system express grouping preferences lists systems simpli maintain information raid exposes per-disk information informed system lfs providing performance optimizations control redundancy improved manageability storage finally ganger suggests reevaluation interface needed outlines relevant case studies track-aligned extents explore paper freeblock scheduling recent work storage community suggests evolution storage place disks general-purpose network standard scsi bus suggested network disks export higher-level object-like interface moving responsibilities low-level storage management system drives speci challenges context xed object-based interface systems storage provide interesting avenue research utility semantic awareness programming environments contrast integration underneath traditional system work focused incorporating active storage parallel programming environments recent work active disks includes acharya riedel amiri research focuses partition applications host disk cpus minimize data transferred system busses inferring on-disk structures fingerprinting file system semantically smart disk implement interesting functionality interpret types blocks read written disk speci characteristics blocks sds practical information obtained robust manner require human 
involvement alternatives obtaining information rst approach directly embeds knowledge system sds onus understanding target system developer sds obvious drawbacks sds rmware updated system upgraded sds robust target system approach target system informs sds data structures run-time case responsibilities target system numerous disadvantages approach importantly target system changed system process access information directly communicate sds communication channel existing protocols added target system sds finally dif cult ensure speci cation communicated sds matches actual system implementation approach sds automatically infers system data structures bene approach speci knowledge target system required sds developed assumptions made sds target system checked deployed additional work required con gure sds installed sds deployed environments difculty approach potential semantically-smart storage system explore sds automatically acquire layout information ngerprinting software automatically inferring system structures bears similarity research efforts reverseengineering researchers shown bit-level machine instruction encodings semantic meaning assembly instructions deduced developed techniques identify parameters tcp protocol extract lowlevel characteristics disks determine buffer-cache policies understand behavior real-time cpu scheduler assumptions automatically inferring layout information arbitrary system challenging problem important rst step developed utility called eof extraction filesystems extract layout information ffs-like systems journaling capabilities veri eof identify data structures employed linux ext ext netbsd ffs eof understand future ffs-like system adheres assumptions layout data structures disk general disk blocks statically exclusively assigned categories data inodes bitmaps free allocated data blocks inodes summary information superblock group descriptors log data eof identi block addresses disk allocated category data blocks data block dynamically data directory listings pointers data blocks indirect block data blocks shared les eof identi structure directory data proceedings usenix conference file storage technologies fast san francisco california eof assumes record directory data block length record entry length entry inode number entry eld directory entry assumed multiple bits eof assumes indirect blocks -bit pointers inode blocks inode block inodes inode consumes n-th block eof assumes nition inode eld static time eof identi location absence elds inode size blocks number data blocks allocated inode ctime time inode changed mtime time data changed dtime deletion time links number links inode generation number data pointers number combination direct pointers single double triple indirect pointers dir bits bits change directory inodes exception dir bits elds identify default assumed multiple bits multiple elds identi bits blocks links elds size eld assumed largest multiple bits lead overlapping elds bitmap blocks bitmaps data inodes share single block separate blocks bits data inode bitmap blocks one-to-one linear mapping data blocks inodes bitmap block valid log data log data journaling system managed circular contiguous buffer make assumptions contents log future feasibility inferring on-disk data structures depends assumption production systems change slowly time assumption hold system developers strong motivation on-disk structures legacy systems continue operate examining systems past present corroborates belief on-disk structure ffs system changed years linux ext system layout conception ext journaling system backward compatible ext extensions freebsd ffs designed avoid on-disk algorithm overview eof software system made sds partition eof run partition sds understands context deployed basic structure eof user-level probe process performs operations system generating controlled traf streams disk sds high-level operations performed disk traf result observing blocks written bytes blocks change sds infers blocks type system data structures offsets block type eld sds knowledge con gure simultaneously verifying target system behaves expected sds correlate traf observes system operations performed probe process correlation requires pieces functionality probe process ensure blocks operation ushed system cache written sds ensure probe process unmounts system unmounting re-mounting sparingly increases running time eof probe process occasionally inform sds speci operation ended probe process communicates sds writing distinct pattern fencepost sds pattern resulting traf message probe process general techniques eof identify blocks inode elds identify data blocks sds pattern probe process writes test les classify blocks elds sds attempts isolate unique unclassied block written aspect model directly general caching disk mimic capture effects simple prefetching important aspect caching scheduling read sector entire track cached disk mimic observe faster performance accesses distances track respect con guring disk mimic on-line observing actual workload accurate con guring off-line locality workload captured complexity inter-request distance concentrate issues related input parameter values request type output disk mimic characteristics explore combinations input parameters discussions refer inter-request distance assume request type xed results illustrate complexity inter-request distance predictor request time show distribution times observed experiments con gure disk mimic off-line disk mimic con gures probing device xed-size requests inter-request distances covering disk negative positive disk mimic samples number points distance accesses block speci distance previous block avoid caching prefetching performed disk disk mimic accesses random location probe required distance observed times recorded table indexed inter-request distance operation type figure show small subset data collected ibm lzx disk gure shows distribution samples inter-request distances case y-axis shows request time sample points axis represent sample sorted increasing request time make important observations sampled times inter-request distance observed request time constant distance requests require require require multi-modal behavior time single request reliably predicted interrequest distance predict request distance faster slower request distance make reasonable predictions based probabilities data conclude request distance longer examining distributions inter-request distances observe number transitions percentage samples time varies inter-request distances number transitions graph corresponds roughly number track cylinder boundaries crossed inter-request distance data shows number important issues remain con guration disk mimic signi variation request times single inter-request distance summary metric summarize distribution samples required adequately capture 
behavior distribution inter-request distance sampled interpolate intermediate distances investigate issues section methodology evaluate performance smtf scheduling range disk drive technology presented proceedings usenix annual technical conference june san antonio texas con guration rotation seek head cyl track cyl sectors num time cyl switch switch skew skew track heads base fast seek slow seek fast rotate slow rotate fast seek rot capacity capacity table disk characteristics con gurations simulated disks times rotation seek head cylinder switch milliseconds cylinder track skews expressed sectors experiments base disk table implemented disk simulator accurately models seek time xed rotation latency track cylinder skewing simple segmented cache rst disk named base disk simulates disk performance characteristics similar ibm lzx disk seek times cache size number segments head cylinder switch times track cylinder skewing rotation times measured issuing scsi commands measuring elapsed time directly querying disk similar approach schindler ganger values provided manufacturer curve seek time modeled probing ibm lzx disk range seek distances measured distance cylinders previous cylinder position current curve tting values two-function equation proposed ruemmler wilkes short seek distances seek time proportional square root cylinder distance longer distances seek time proportional cylinder distance middle seek column represents cylinder distance switch functions occurs base disk seek distance smaller cylinders square root function disk con gurations simulate start base disk vary parameters inuence positioning time disk con guration number fast seek represents disk fast seek time numbers compute seek curve adjusted number sectors constitute cylinder skew similarly disk conguration number fast rotate time execute rotation decreased factor number track cylinder skew sectors increased disk con gurations account disks slower seek time slower rotation time faster seek time faster rotation time capacity base disk addition simulated disks run experiments ibm lzx disk time scaling factor fcfs sstf smtf min smtf probabilistic smtf max smtf median smtf greedy-optimal figure sensitivity summary metrics graph compares performance variety scheduling algorithms base simulated disk week-long trace smtf schedulers interpolation performed samples obtained data point x-axis shows compression factor applied workload y-axis reports time spent disk evaluate scheduling performance show results set traces collected labs cases focus trace busiest disk week performance metric report time workload spent disk impact heavier workloads longer queue lengths compress inter-arrival time requests scaling time attempt preserve dependencies requests workload observing blocks requested assume request repeated block serviced request dependent previous request rst completing hold repeated requests subsequent requests previous identical request completes proceedings usenix annual technical conference june san antonio texas time ordered requests summary metric probability percentage error probabildisk time ordered requests summary metric percentage error meandisk time ordered requests summary metric maximum percentage error maxdisk figure demerit figures smtf probability maximum summary metrics graph shows demerit gure summary metric distributions correspond day experiments shown figure compression factor off-line con guration smtf scheduler con gured on-line off-line explore case disk mimic con gured off-line disk mimic con gured off-line simulation predictions required scheduler performed on-line system previously con guring disk mimic off-line involves probing underlying disk requests range inter-request distances note model con gured off-line process con guring smtf remains automatic portable range disk drives main drawback con guring disk mimic offline longer installation time device added system disk probed workload traf summary data enable smtf scheduler easily compare expected time requests queue disk mimic supply summary distribution function inter-request distance multi-modal characteristics distributions choice summary metric obvious evaluate summary metrics median maximum minimum probabilistic randomly picks sampled distribution probability results summary metrics base simulated disk shown figure workload week-long trace scaled compression factor noted x-axis graph shows fcfs sstf c-look perform worse smtf schedulers expected smtf schedulers perform worse greedy-optimal scheduler approach workload results show inter-request distance predict positioning time merits attention comparing performance smtf approaches summary metric performs time samples fcfs sstf smtf greedy-optimal figure sensitivity number samples graph shows performance smtf improves samples results simulated disk week-long trace compression factor x-axis number samples smtf y-axis shows time spent disk differently ordering performance worse median maximum probabilistic minimum interesting note scheduling performance summary metric correlated accuracy accuracy disk models evaluated demerit gure ned root square horizontal distance time distributions model real disk point brie illustrated figure shows distribution actual times versus predicted times metrics probabilistic maximum expected probabilistic model demerit gure requests distribution predicts expected match real device probabilistic model performs poorly smtf time predicts request differ signi cantly acproceedings usenix annual technical conference june san antonio texas time request inter-request distance full range time request inter-request distance close-up figure values samples function inter-request distance graph left shows time entire set inter-request distances simulated disk graph shows close-up inter-request distances distances qualitatively similar saw-tooth behavior tual time request conversely maximum results poor demerit gure performs adequately scheduling fact smtf maximum performs signi cantly minimum similar demerit gures finally summary distribution achieves performance result demerit gure found performs days traces examined remainder experiments observed samples summary data inter-request distance number samples large variation times single inter-request distance disk mimic perform large number probe samples true distribution reduce time required con gure disk mimic off-line perform samples evaluate impact number samples smtf performance figure compares performance smtf function number samples performance fcfs c-look sstf optimal expected performance smtf increases samples workload disk performance smtf continues improve approximately samples interestingly single sample inter-request distance disk mimic performs fcfs c-look sstf interpolation number samples performed interrequest distance impacts running time off-line probe process greater issue distance explicitly probed intere time percent error check checks checks interpolation sstf figure sensitivity interpolation graph shows performance interpolation function percent allowable error lines correspond numbers check points x-axis percent allowable error y-axis time spent disk results base simulated disk week-long trace compression factor polated distances due large number potential inter-request distances modern storage device times number sectors negative positive distances performing probes signi amount time storing values prohibitive disk size amount memory required table exceed explore distances interpolated making detailed assumptions underlying disk illustrate potential performing simple interpolations show function inter-request distance figure graph left proceedings usenix annual technical conference june san antonio texas check points acceptable error table allowable error 
interpolation table summarizes percentage interpolated relative probed order infer interpolation successful check points performed inter-request distances allowable error increases numbers gathered running number workloads simulated disks observing point performance interpolation degrades relative interpolation shows values inter-request distances simulated disk curve bands emanating middle point corresponds seek curve disk short seeks time proportional square root distance long time linear distance width bands constant corresponds rotation latency disk graph shows close-up inter-request distances graph shows times follow distinct saw-tooth pattern result simple linear model interpolate distances care ensure model applied short distances length linear regions varies disks function track cylinder size goal determine distances interpolated successfully challenge determine interpolated close actual scheduling performance impacted negligibly basic off-line interpolation algorithm disk mimic performs samples interrequest distances left chooses random distance middle left linearly interpolates middle means left interpolated middle error percent probed middle interpolation considered successful distances left interpolated interpolation successful disk mimic recursively checks smaller ranges distances left middle middle intermediate points successfully interpolated points probed additional con dence linear interpolation valid region slight variation points left interpolated checked points predicted desired level accuracy interpolation considered successful intuition performing check points higher error rate interpolation successful figure shows performance smtf distances interpolated graph shows effect increasing number intermediate points checked increasing acceptable error error interpolation make observations graph smtf performance decreases allowable error check points increases result expected note performance decreases dramatically error error checked distances increased interpolated distances inaccurate single check point error level found interpolated values accurate level average error interpolated values increases shown summary error increases signi cantly linear relationship distances left interpolation performed smtf performance xed error increases number intermediate check points effect performing checks con linear interpolation distances valid check points error interpolated points accurate level average error shown table summarizes ndings wider number check points table shows allowable error percentage function number check points achieve scheduling performance similar probes nal probe process operate interpolation distance left error deemed successful distances left errors interpolation successful progressively check points made higher error rates successful approach distances disk interpolated probed scheduling performance virtually unchanged interpolation leads fold memory savings disk characteristics demonstrate robustness portability disk mimic smtf scheduling full range simulated disks table performance fcfs c-look sstf smtf relative proceedings usenix annual technical conference june san antonio texas disk configuration slowdown fcfs sstf smtf figure sensitivity disk characteristics ure explores sensitivity scheduling performance disk characteristics shown table performance shown relative greedy-optimal report values smtf interpolation performance smtf interpolation probes similar greedy-optimal disks summarized figure show performance smtf interpolation performance smtf interpolation identical expected fcfs performs worst entire range disks performing factor slower greedy-optimal c-look sstf perform seek time dominates performance disks sstf performs c-look cases finally smtf performs rotational latency signi component request positioning disks summary range disks smtf performs c-look sstf scheduling greedy-optimal algorithm show smtf handle performance variation real disks compare performance implementation smtf c-look run ibm lzx disk week trace achieve performance improvement smtf compared c-look improvement idle time removed trace performance improvement signi reasons ibm lzx disk high ratio seek rotation time performance improvement smtf relative c-look greater rotation time signi component positioning trace exercises operation set operations operations algorithm phases eof composed phases eof isolates summary blocks log eof identi data blocks data bitmaps eof inodes inode bitmaps blocks classi eof isolates inode elds finally eof identi elds directory entries bootstrapping phase goal bootstrapping isolate blocks frequently written phases tered blocks interest phase isolates summary blocks log inode data blocks fencepost test directory test les probe process creates fencepost number test les test directory sds identies data blocks searching patterns eof identi blocks belonging log exists step probe process synchronously appends data pattern proceedings usenix conference file storage technologies fast san francisco california test les sds observes blocks meta-data blocks written circular pattern belong log block traf matches pattern eof infers system perform journaling eof identi summary blocks probe process unmounts system written blocks classi log data identi summary blocks isolate inode blocks repeatedly written probe process performs chmod fencepost test directory test les case inode written allowing classied data blocks belonging test directory identi changing test blocks previously unidenti blocks written finally determine separate bitmap blocks data inode blocks linux ext ext single bitmap block shared netbsd ffs eof creates sds observes unclassi blocks determine bitmap blocks shared separate data inodes simplify presentation remainder discussion case data inode bitmaps separate blocks eof correctly handles shared case case eof isolates speci bits shared bitmap block devoted inode data block state data data-bitmap blocks phase eof continues identifying blocks disk data data bitmaps isolate blocks probe process appends blocks data pattern test les blocks match pattern classi assumed data-bitmap blocks indirect-pointer blocks eof differentiates inferring blocks written les data-bitmap blocks care create small les single lls bitmap block bitmap block special case smaller expected completely cleanup phase test les deleted inodes inode-bitmap blocks phase identifying inodes bitmaps requires creating les distinct steps required probe process creates les inodes inode bitmaps modi probe process performs chmod les inodes inode bitmaps written inodes inode bitmaps distinguished phase calculates size inode performed recording number times block identi inode dividing block size observed number inodes block inode fields phase point eof classi blocks disk belonging categories data structures phase identi elds inodes observing elds change operations brevity describe eof infers blocks links generation number elds rst inode elds eof identi size times requires steps probe process creates sds 
stores inode data compare inode data written steps probe process overwrites data inode elds change related time probe process appends small amount data data pointer added point size eld identi data changed step step fourth probe process performs operation change inode changing data adding link changing permissions sds isolate mtime changed step ctime changed finally deleted deletion-time eld observed eof identi location level data pointers inode probe process repeatedly appends sds observes bytes inode change changed previous step eof infers location indirect pointers observing additional data block written additional pointer updated inode improve performance write block probe process seeks progressively larger amount seek distance starts block increases size handled detected indirection level finally eof isolates inode bit elds designate directories probe process alternately creates les directories sds histograms directory inodes histogram eof records count times bit inode type determine directory elds eof isolates bits les directories vice versa bits values considered identify les versus directories soft link bits identi similar manner directory entries phase nal phase eof identi structure entries directory eof infers offsets entry length probe proproceedings usenix conference file storage technologies fast san francisco california cess creates sds searches directory data block eld designating length validation deleted step repeated numerous times lenames lengths eof nds location record length assumption length record remaining space directory data block length reduced record added probe process creates additional les sds simply records offsets change previous entries finally offset inode number found assumption directory entry step probe process creates empty directories sds isolates inode offset recording differences data blocks directories assertion assumptions major challenge automatic inferencing ensure sds correctly identi behavior target system robust system meeting assumptions eof mechanisms detect assumption fails case system identi non-supported sds operates correctly semantic knowledge blocks expected written speci step speci blocks observed eof detects violation veri violations identi appropriately eof run non-ffs systems msdos vfat reiserfs additional bene eof con gure sds system bugs identi running eof ext linux isolated bugs sds observed incomplete traf key steps problem tracked back ext bug data written seconds prior unmount ushed disk probe process noted error inodes allocated case ext incorrectly marks system dirty eof enables checks system easily obtained methods exploiting structural knowledge classi cation association key advantage sds ability identify utilize important properties block disk properties determined direct indirect classi cation association direct classi cation blocks easily identi location disk indirect classi cation blocks identi additional information identify directory data indirect blocks inode examined finally association data block inode connected cases sds requires functionality identify change occurred block functionality implemented block differencing infer data block allocated single-bit change data bitmap observed change detection potentially costly operations sds reasons compare current block version block sds fetch version block disk avoid overhead cache blocks employed comparison expensive location difference byte block compared byte block quantify costs section direct classi cation direct classi cation simplest cient form on-line block identi cation sds sds determines type block performing simple bounds check calculate set block ranges block falls ffs-like system superblock bitmaps inodes data blocks identi technique indirect classi cation indirect classi cation required type block vary dynamically simple direct classi cation precisely determine type block ffs-like systems indirect classi cation determine data block data directory data form indirect pointer block single double triple indirect block illustrate concepts focus directory data differentiated data steps identifying indirect blocks versus pure data similar identifying directory data basic challenge identifying data block belongs directory track inode points data check type directory perform tracking sds snoops inode traf disk directory inode observed data block numbers inserted hash table sds removes data blocks hash table observing blocks freed block differencing bitmaps sds identify block directory block presence table directory data discuss complications approach proceedings usenix conference file storage technologies fast san francisco california sds guarantee correctly identify blocks les directories speci cally data block present hash table sds infers data corresponds cases directory inode sds result hash table situation occur directory created blocks allocated existing directories system guarantee inode blocks written data blocks sds incorrectly classify newly written data blocks problem occur classifying data blocks read case system read inode block data block data block number sds inode rst correctly identify subsequent data blocks transient misclassi cation problem depends functionality provided sds instance sds simply caches directory blocks performance tolerate temporary inaccuracy sds requires accurate information correctness ways ensured rst option guarantee system writes inode blocks data blocks true default ffs soft updates linux ext mounted synchronous mode option buffer writes time classi cation made deferred classi cation occurs inode written disk data large amount data disk locality workload low trace seek time dominates positioning time explore effect workload locality create synthetic workload random reads writes idle time maximum inter-request distance varied speci x-axis figure graph shows performance improvement smtf relaslowdown maximum inter-request distance real disk comparison smtf off-line figure real disk performance graph shows slowdown c-look compared smtf congured off-line workload synthetically generated trace numbers averages runs standard deviation reported x-axis shows maximum inter-request distance existent trace y-axis reports percentage slowdown algorithm tive c-look varies interrequest distance varies systems linux ext optimize locality placing related les cylinder group smtf optimize accesses c-look practice smtf viable option scheduling real disks on-line con guration explore smtf scheduler con guration performed on-line approach overhead installation time probe disk drive disk mimic observes behavior disk workload runs off-line version disk mimic records observed disk times function inter-request distance case control inter-request distances observes general approach on-line version assume lessons learned off-line con guration hold continue represent distribution times inter-request distance continue rely interpolation note disk mimic con gured on-line interpolation saving space providing information distances observed primary challenge smtf address situation schedule requests inter-request distances unknown times inter-request distance observed proceedings usenix annual technical conference june san antonio texas sstf slowdown base-line pri interp pri interp set interp set interp percentage slowdown day performance hybrid versions sstf set sstf set sstf interp set set interp figure performance on-line smtf rst graph compares performance variations online smtf performance day week-long trace shown relative off-line smtf graph shows performance online-set improves time inter-request distances observed disk mimic disk mimic unable con interpolated successfully algorithms comparison algorithms assume base scheduler c-look sstf disk mimic suf cient information rst algorithm online-priority schedules requests disk mimic information speci cally online-priority strict priority requests queue inter-request distance time requests times request minimum time picked online-priority base scheduler c-look sstf inter-request distances current queue problems approach preference scheduling inter-request distances online-priority perform worse base scheduler schedules diversity distances produced disk mimic observe cient distances algorithm online-set improves limitations decision base scheduler starting point scheduling request disk mimic knowledge performance improved speci cally online-set rst considers request base scheduler pick time distance disk mimic request scheduled time requests inter-request distances considered fastest chosen online-set improve performance base scheduler schedule variety inter-request distances learning experimental results evaluate performance on-line algorithms return base simulated disk left-most graph figure compares performance online-priority online-set c-look sstf baseline algorithm interpolation performance expressed terms slowdown relative off-line version smtf make observations graph surprising c-look performs sstf workload disk smtf performs noticeably sstf c-look base c-look disk mimic observe inter-request distances negative backward discover distances close online-set performs onlinepriority sstf base scheduler interpolation signi cantly improve performance online-priority online-set c-look leads small improvement online-set sstf off-line con guration primary bene interpolation reduce memory requirements disk mimic opposed improving performance right-most graph figure illustrates performance online-set improves time inter-request distances observed performance 
online-set algorithms interpolation base-line schedulers sstf c-look day original trace approximately requests performance online-set sstf converges off-line version days requests point feel opportunities proceedings usenix annual technical conference june san antonio texas improving performance on-line smtf relative off-line smtf current on-line implementations slow time distance observed initially scheduler avoid distance faster address requiring distance minimum number samples classi current algorithm leverage idle time perform probes unknown inter-request distances idle times disk mimic learn characteristics disk related work approach propose brings areas study disk modeling disk scheduling present related work areas compare method disk modeling classic paper describing models disk drives ruemmler wilkes main focus work enable informed trade-off simulation effort resulting accuracy model ruemmler wilkes evaluate aspects disk modeled high level accuracy demerit gure researchers noted additional non-trivial assumptions made model disks desired accuracy level modeling cache behavior challenging aspect detailed knowledge modeling disks documentation researchers developed innovative methods acquire information worthington describe techniques scsi drives extract time parameters seek curve rotation speed command overheads information data layout disk caching prefetching characteristics techniques automated work modeling storage devices tables past performance explored previous work previous work high-level system parameters load number disks operation type indices table anderson results online assist recon guration disk arrays approach similar thornock work authors stochastic methods build model underlying drive application model standard off-line simulation specifically authors study block reorganization similar earlier work ruemmler wilkes higher level seltzer small suggest situ simulation method building adaptive operating systems work authors suggest operating systems utilize in-kernel monitoring adaptation make informed policy decisions tracing application activity vino system determine current policy behaving expected policy switched place actual simulations system behavior performed offline resort poor performance detected disk scheduling disk scheduling long topic study computer science rotationally-aware schedulers existence early work seltzer jacobson wilkes due dif culty implementation early works focused solely simulation explore basic ideas recently implementations rotationally-aware schedulers literature crafted extreme care recently worthington examine bene detailed knowledge disk drives os-level disk schedulers algorithms mesh modern prefetching caches perform detailed logical-to-physical mapping information anticipatory scheduling recent scheduling development complementary on-line simulationbased approach anticipatory scheduler makes assumption locality stream requests process waiting request servicing request process performance improved authors note dif culty building rotationally-aware scheduler empirically-generated curvetted estimate disk access-time costs disk mimic yield performance bene simpli approach conclusions paper explored issues simulation system make run-time scheduling decisions focused disk simulator automatically model range disks human intervention shown disk mimic model time request simply observing request type logical distance previous request predicting behave similarly past requests parameters disk mimic con gure disk probing disk off-line slight performance cost observing requests disk on-line demonstrated shortest-mimicked-timerst smtf disk scheduler signi cantly improve disk performance relative fcfs sstf c-look range disk characteristics future plan show smtf scheduling range storage devices disk drives raid systems networkproceedings usenix annual technical conference june san antonio texas attached storage devices mems-based devices tapes non-volatile memory building blocks storage system devices complex performance characteristics ideal scheduler automatically adapt devices acknowledgments nathan burnett timothy denehy brian forney muthian sivathanu feedback paper vern paxson shepherd anonymous reviewers thoughtful suggestions greatly improved content paper finally computer systems lab tireless assistance providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation anderson simple table-based modeling storage devices technical report hpl-ssp- laboratories july andrews bender zhang algorithms disk scheduling problem ieee symposium foundations computer science focs pages arpaci-dusseau arpaci-dusseau information control gray-box systems symposium operating systems principles sosp pages block freed inferred monitoring data bitmap traf sds perform excess work obliviously inserts data blocks hash table directory inode read written inode recently passed sds causing hash table updated optimize performance sds infer block added modi deleted time directory inode observed ensure blocks added deleted hash table process operation inferencing detail section identifying indirect blocks process identifying indirect blocks identical identifying directory data blocks case sds tracks indirect block pointers inodes read written maintaining hash table single double triple indirect block addresses sds determine data block indirect block association association connect data blocks inodes size creation date sds association achieved simple space-consuming approach similar indirect classi cation sds snoops inode traf inserts data pointers addressto-inode hash table concern table size accurate association table grows proportion number unique data blocks read written storage system system booted approximate information tolerated sds size table bounded detecting high-level behavior operation inferencing block classi cation association provide sds cient identifying special kinds blocks operation inferencing understand semantic meaning observed blocks outline sds identify system operations observing key challenge operation inferencing sds distinguish blocks valid version instance newly allocated directory block written compared contents block block contained arbitrary data identify versions sds simple insight metadata block written read contents block relevant detect situation sds maintains hash table meta-data block addresses read past meta-data block read added list block freed block bitmap reset removed list block allocated data freed reallocated directory block address present hash table sds contents illustrative purposes section 
examine sds infer create delete operations discussion speci ext similar techniques applied ffs-like systems file creates deletes steps identifying creates deletes rst actual detection create delete determining inode affected describe detection mechanisms logic determining inode rst detection mechanism involves inode block inode block written sds examines determine inode created deleted valid inode non-zero modi cation time proceedings usenix conference file storage technologies fast san francisco california deletion time modi cation time non-zero deletion time non-zero means inode newly made valid created similarly reverse change newly freed inode deleted indication change version number valid inode delete create occurred cases inode number calculated physical position inode disk on-disk inodes inode numbers detection mechanism involves inode bitmap block bit set inode bitmap created inode number represented bit position similarly newly reset bit deleted update directory block indication newly created deleted directory data block written sds examines block previous version directory entry dentry added inode number obtained dentry case removed dentry contents dentry inode number deleted newly created deleted choice mechanism combinations thereof depends functionality implemented sds sds identify deletion immediately creation inode number inode bitmap mechanism sds observe change bitmap operations grouped due delayed write system case modi cation times version numbers similarly newly created deleted directory block-based solution cient file system operations general technique inferring logical operations observing blocks versions detect system operations note cases conclusive inference speci logical operation sds observe correlated multiple meta-data blocks semantically-smart disk system infer renamed observes change directory block entry inode number stays note version number inode stay similarly distinguish creation hard link normal directory entry inode examined evaluation section answer important questions sds framework cost ngerprinting system time overheads classi cation association operation inferencing space overheads proceeding evaluation rst describe experimental environment platform prototype sds employ software-based infrastructure implementation inserts pseudo-device driver kernel interpose traf system disk similar software raid prototype appears systems device system mounted primary advantage prototype observes information traf stream actual sds system current infrastructure differs important ways true sds importantly prototype direct access low-level drive internals information made dif cult sds runs system host interference due competition resources initial case studies prime importance performance characteristics microprocessor memory system actual sds high-end storage arrays signi processing power processing capacity trickle lower-end storage systems experimented prototype sds linux linux netbsd operating systems underneath ext ext ffs systems experiments paper performed processor slow modern standards mhz pentium iii processor k-rpm ibm lzx k-rpm quantum atlas iii disk experiments employ fast system comprised ghz pentium k-rpm seagate cheetah disk gauge effects technology trends off-line layout discovery subsection show time run ngerprinting tool eof reasonable modern disks eof run system runtime eof determine common case performance sds runtime eof prohibitive disks larger potential solution parallelism proceedings usenix conference file storage technologies fast san francisco california time partition size costs fingerprinting eof phase phase slow system fast system figure costs fingerprinting gure presents time breakdown ngerprinting slow system ibm disk fast system running underneath linux ext x-axis vary size partition ngerprinted y-axis shows time phase time-consuming components eof parallelizable reduce run-time disk arrays figure presents graph time run eof single-disk partition size partition increased show performance results slow system ibm disk fast system graph shows phase determines locations data blocks data bitmaps phase determines locations inode blocks inode bitmaps dominate total cost ngerprinting time phases increases linearly size partition requiring approximately seconds slow system seconds fast system comparing performance systems conclude increases sequential disk performance directly improve eof ngerprinting time phases require small amount time partition size on-line time overheads classi cation association operation inferencing potentially costly operations sds subsection employ series microbenchmarks illustrate costs actions results experiments sds underneath linux ext presented table action microbenchmark cases rst case system mounted synchronously ensuring meta-data operations reach sds order allowing sds guarantee correct classi cation additional effort synchronous mounting linux ext similar traditional ffs handling meta-data updates case system mounted asynchronously case guarantee correct classi cation association sds perform operation inferencing microbenchmarks perform basic system operations including directory creates deletes report perle per-directory overhead action test experiments make number observations operations tend cost order tens microseconds directory operations require complete cost due per-block cost operation inferencing synchronous mode create workload takes roughly corresponds base cost create workload cost approximately block costs rise size increases sds incurs small per-block overhead compared actual disk writes number milliseconds complete cases overheads ext system run asynchronous mode lower run synchronous mode asynchronous mode numerous updates meta-data blocks batched costs block differencing amortized synchronous mode meta-data operation ected disk system incurring higher overhead sds observe synchronous mode classi cation expensive association expensive inferencing sds care employ actions needed implement desired functionality on-line space overheads sds require additional memory perform classi cation association operation inferencing speci cally hash tables required track mappings data blocks inodes caches needed implement cient block differencing quantify memory overheads variety workloads table presents number bytes hash table support classi cation association operation inferencing sizes maximum reached run workload netnews postmark modi andrew benchmark netnews postmark vary workload size caption table dominant memory overhead occurs sds performing block-inode association classi cation operation inferencing require table sizes proportional number unique meta-data blocks pass sds association requires information unique data block passes worst case entry required proceedings usenix conference file storage technologies fast san francisco california indirect block-inode operation classi cation association inferencing sync async sync async sync async create create delete delete mkdir rmdir table sds time overheads table breaks costs indirect classi cation block-inode association operation inferencing microbenchmarks row stress aspects action create benchmark creates les size delete benchmark similarly deletes les mkdir rmdir benchmarks create remove directories result presents average overhead operation extra time sds takes perform classi cation association inferencing experiments run slow system ibm lzx disk linux ext mounted synchronously sync asynchronously async data block disk memory disk space space costs tracking association information high prohibitive memory resources scarce sds choose tolerate imperfect information swap portions table disk addition hash tables needed perform classi cation october association ganger operation inferencing worthington cache data patt blocks disksim simulation perform block environment differencing version effectively recall manual http differencing 
observe pointers allocated freed inode indirect block check time elds inode changed detect bitwise bitmap monitor directory data creations deletions performance system sensitive size cache cache small difference calculation rst fetch version block disk avoid extra size cache roughly proportional active meta-data working set postmark workload found sds cache approximately blocks hold working set cache smaller block differencing operations disk retrieve older copy block increasing run-time benchmark roughly case studies section describe case studies implementing functionality sds implement drive raid semantic knowledge case studies indirect block-inode operation classi cation association inferencing netnews netnews netnews postmark postmark postmark andrew table sds space overheads table presents space overheads structures performing classi cation association operation inferencing workloads netnews postmark modi andrew benchmark workloads netnews postmark run amounts input correspond roughly number transactions generates netnews implies transactions run number table represents maximum number bytes stored requisite hash table benchmark run hash entry bytes size experiment run slow system linux ext asynchronous mode ibm lzx disk built system proper implementing le-system functionality storage system advantages semantic intelligence storage-system manufacturers augment products broader range capabilities due space limitations fully describe case studies paper highlight functionality case study implements present performance evaluation conclude analyzing complexity implementing functionality sds performance evaluation included demonstrate interesting functionality implemented effectively sds leave detailed performance studies future work theme explore section usage approximate information scenarios sds wrong understanding system case studies track-aligned extents proposed schindler track-aligned extents traxtents improve disk access times placing medium-sized les tracks avoiding track-switch costs detailed level knowledge traxtents-enabled system requires underlying disk mapping logical block numbers physical tracks traxtents natural candidate implementation sds information readily obtained fundamental challenge implementing traxtents sds system adapting policies system system speci cally traxtent sds uence system allocation prefetching mid-sized les proceedings usenix conference file storage technologies fast san francisco california prefetching prefetching ext traxtent sds table track-aligned extents table shows bandwidth obtained reading les randomized order roughly size track case examine default track-aligned allocation varying track-sized prefetching enabled sds experiment run slow system running linux ext system mounted asynchronously quantum atlas iii disk allocated consecutive data blocks span track boundaries accesses track-sized units components interest traxtent sds implementation bitmap blocks rst read system sds marks bitmap block track allocated similar technique schindler wastes small portion disk fake allocation uences system allocate les span tracks system decides allocate tracks sds dynamically remaps blocks track-aligned locale similar block remapping loge smart disks major difference sds remaps blocks part mid-sized les benet track-alignment non-semantically aware disks make distinction traxtent sds performs additional prefetching ensure accesses smaller track linux ext ffs prefetches blocks initially read traxtent sds observes read rst block track-aligned requests remainder track places data blocks cache traxtent sds relies piece exact information correctness location bitmap blocks marks trick system track-aligned allocation information static obtained reliably eof performance cost runtime indirect classi cation data belonging medium-sized les occasionally incorrect citeseer nec article ganger disksim html gibson nagle amiri chang feinberg gobioff lee ozceri riedel rochberg zelenka file server scaling network-attached secure disks proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle june golding bosch staelin sullivan wilkes idleness sloth proceedings winter usenix technical conference pages orleans louisiana january gotlieb macewen performance movable-head disk storage devices journal association computing machinery grif schindler schlosser bucy ganger timing-accurate storage emulation proceedings usenix conference file storage technologies fast pages monterey january hillyer silberschatz modeling performance characteristics serpentine tape drive proceedings sigmetrics conference measurement modeling computer systems pages hofri disk scheduling fcfs sstf revisited communications acm huang chiueh implementation rotation latency sensitive disk scheduler technical report ecsl-tr suny stony brook march iyer druschel anticipatory scheduling disk scheduling framework overcome deceptive idleness synchronous acm symposium operating systems principles pages october jacobson wilkes disk scheduling algorithms based rotational position technical report hpl-csp- laboratories kotz toh radhakrishnan detailed simulation model disk drive technical report dartmouth college patterson gibson katz case redundant arrays inexpensive disks raid sigmod record acm special interest group management data september ruemmler wilkes disk shuf ing technical report hpl- hewlett packard laboratories october ruemmler wilkes unix disk access patterns proceedings usenix winter technical conference pages ruemmler wilkes introduction disk drive modeling ieee computer proceedings usenix annual technical conference june san antonio texas schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november schlosser grif nagle ganger designing computer systems memsbased storage architectural support programming languages operating systems pages seltzer chen ousterhout disk scheduling revisited proceedings usenix winter technical conference pages berkeley seltzer small self-monitoring self-adapting systems proceedings workshop hot topics operating systems pages chatham shenoy vin cello disk scheduling framework next-generation operating systems proceedings sigmetrics conference measurement modeling computer systems pages june shriver merchant wilkes analytic behavior model disk drives readahead caches request reordering proceedings sigmetrics conference measurement modeling computer systems pages talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley teorey pinkerton comparative analysis disk scheduling policies communications acm thornock flanagan stochastic disk simulation technique proceedings winter simulation conference pages wang reiher popek kuenning conquest performance disk persistent-ram hybrid system proceedings usenix annual technical conference usenix pages monterey june wilhelm anomaly disk scheduling comparison fcfs sstf seek scheduling empirical model disk accesses communications acm wilkes pantheon storage-system simulator technical report hpl-ssp- laboratories palo alto december worthington ganger patt scheduling algorithms modern disk drives proceedings acm sigmetrics conference measurement modeling computer systems pages nashville usa worthington ganger patt 
wilkes on-line extraction scsi disk drive parameters technical report cse-tr- carnegie mellon gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation pages san diego usenix association 
remapping performance correctness table shows traxtent sds prefetching results roughly improvement bandwidth medium-sized les structural caching discuss semantic information caching sds simple lru management disk cache duplicate contents system cache wastes memory storage system waste onerous storage arrays due large amounts memory contrast sds structural undertpc-b tpc-b ffs lru sds file-aware caching sds table file-aware caching table shows time seconds takes execute tpc-b transactions experiments transactions rst run warm system large scan run series transactions timed table compares netbsd ffs standard disk sds lru-managed cache sds le-aware cache experiments run slow system ibm lzx disk standing system cache blocks intelligently avoid wasteful replication explore caching blocks volatile memory dram non-volatile memory nvram presents unique opportunities optimization rst examine simple optimization avoids worst-case lru behavior file-aware caching sds fac sds exploits knowledge size selectively cache blocks les small cache les accessed sequentially strategy avoids caching blocks large les scanned ush cache blocks implement le-aware caching fac sds identi cacheable blocks indirect classi cation association case hash table holds block addresses correspond les meet caching criteria previously sds misclassify blocks cases inode written disk data blocks fac sds small amount state active order detect sequential access patterns table shows performance fac sds database workload scenario run tpc-b transactions periodically intersperse large scans system emulating system running mixed interactive batch transactions large scan ushes contents traditional lru-managed cache degrades performance subsequent transactions le-aware cache cache blocks large scans keeping transactional tables sds memory improving performance examine sds semantic knowledge store important structures non-volatile memory explore possibilities rst exploit semantic knowledge store ext journal nvram implement journal caching sds sds sds recognize traf journal redirect nvram straightforward eof tool determines blocks belong journal classifying caching data reads proceedings usenix conference file storage technologies fast san francisco california create create sync ext lru sds lru sds journal caching sds table journal caching table shows time create -kb les ext sds sds performs lru nvram cache management cache journal caching sds storing journal nvram create benchmark performs single sync les created create sync benchmark performs sync creation inducing journaling-intensive workload experiments run slow system running linux utilizing ibm lzx disk writes journal sds implement desired functionality place meta-data bitmaps inodes indirect blocks directories netbsd ffs nvram inodes bitmaps identi location disk pointer blocks directory data blocks identi indirect classi cation occasionally miss blocks exploit fact approximate information adequate sds writes unclassi blocks disk nvram observes inode track meta-data blocks meta-data caching sds mdc sds additional map record in-core location tables show performance sds mdc sds cases simple nvram caching structures journal system meta-data effective reducing run times dramatically greatly reducing time write blocks stable storage lru-managed cache effective case cache large working set main bene structural caching nvram size cached structures sds guarantees effective cache utilization hybrid combine worlds storing important structures journal meta-data nvram managing rest cache space lru fashion future plan investigate ways semantic information improve storagesystem cache management sds types meta-data updates last-accessedtime updates inode order ascertain les system cache prefetching sds intelligent system awareness make guess block read finally blocks deleted removed cache freeing space live blocks create read delete postmark ffs lru sds lru sds 
mdc sds table meta-data caching left columns table show time seconds complete phase lfs microbenchmark experiment lfs benchmark creates reads deletes -kb les column shows total time seconds postmark benchmark run les transactions directories rows compare performance netbsd ffs slow system ibm disk sds sds performs lru nvram cache management cache mdc sds strategy secure deletion advanced magnetic force scanning tunneling microscopy stm person physical access drive lot time potentially extract sensitive data user deleted case study explore secure-deleting sds disk guarantees data deleted les unrecoverable previous approaches incorrectly functionality system over-writing deleted blocks multiple times patterns guarantee data removed disk copies data blocks exist due bad-block remapping storage system optimizations multiple consecutive le-system writes reach disk media due nvram buffering sds locale secure delete implemented ensure stray copies data exist over-writes performed disk nature case study approximate incorrect information blocks deleted acceptable secure-deleting sds recognizes deleted blocks operation inferencing overwrites blocks data patterns speci number times system reallocate blocks possibly write block fresh contents meantime sds tracks deleted blocks queues writes blocks overwrite nished note mount ext system synchronously secure deletion operate correctly investigating techniques relax requirement part future work table shows overhead incurred sds function number over-writes overwrites performed data recoverable noticeable price paid securedelete functionality loss acceptable highlysensitive applications requiring security performance improved delaying secureproceedings usenix conference file storage technologies fast san francisco california delete postmark ext secure-deleting sds secure-deleting sds secure-deleting sds table secure deletion table shows time seconds complete delete microbenchmark postmark benchmark secure-deleting sds delete benchmark deletes -kb les postmark benchmark performs transactions row secure-deleting sds shows performance number over-writes experiment place slow system running linux ext mounted synchronously ibm lzx disk overwrite disk idle performing immediately freeblock scheduling minimize performance impact journaling nal case study complex sds implements journaling underneath unsuspecting system view journaling sds extreme case helps understand amount information obtain disk level unlike case studies journaling sds requires great deal precise information system due space limitations present summary implementation fundamental dif culty implementing journaling sds arises fact disk transaction boundaries blurred instance system create system inode block parent directory block inode bitmap block updated part single logical create operation block writes grouped single transaction straight-forward fashion sds sees stream meta-data writes potentially interleaved logical system operations challenge lies identifying dependencies blocks handling updates atomic transactions result journaling sds maintains transactions coarser granularity journaling system basic approach buffer meta-data writes memory write disk in-memory state meta-data blocks constitute consistent metadata state logically equivalent performing incremental in-memory fsck current set dirty meta-data blocks writing disk check succeeds current set dirty meta-data blocks form consistent state treated single atomic transaction ensuring on-disk meta-data contents remain previous consistent state fully updated consistent state bene coarse-grained transactions batching commits performance improved traditional journaling systems create read delete ext sync ext async ext ext sync journaling sds table journaling table shows time complete phase lfs microbenchmark seconds -kb les con gurations compared ext linux mounted synchronously mounted asynchronously journaling ext linux journaling sds synchronously mounted ext linux experiment place slow system ibm lzx disk guarantee bounded loss data crash journaling sds limits time elapse successive journal transaction commits journaling daemon wakes periodically con gurable interval takes copy-on-write snapshot dirty blocks cache dependency information point subsequent meta-data operations update copy cache introduce additional dependencies current epoch similar secure-deleting sds current journaling sds implementation assumes system mounted synchronously robust sds requires verify assumption holds turn journaling meta-data state written disk journaling sds consistent synchronous asynchronous mount problem imposed asynchronous mount sds miss operations reversed create delete lead dependencies resolved inde nite delays journal transaction commit process avoid problem journaling sds suspicious sequence meta-data blocks single change expected multiple inode bitmap bits change part single write turns journaling cases fall-back journaling sds monitors elapsed time commit dependencies prolong commit time threshold suspects asynchronous mount aborts evaluate correctness performance journaling sds check correctness crashed system numerous times ran fsck verify inconsistencies reported performance journaling sds summarized table sds requires system mounted synchronously performance similar asynchronous versions semantically-smart disk system delays writing meta-data disk read test sds similar performance base system ext delete test similar performance journalproceedings usenix conference file storage technologies fast san francisco california eof fingerprinting probe process sds sds infrastructure case studies initialization traxtents hash table cache file-aware cache direct classi cation journal cache indirect classi cation meta-data cache association secure delete operation inferencing journaling table code complexity number lines code required implement aspects sds presented sds component eof tool lines code roughly lines shared tiv siva ndrea system types rest le-system speci ing system ext creation sds pays signi cost relative ext overhead block differencing hash table operations noticeable impact purpose case study demonstrate sds implement complex functionality small overhead acceptable complexity analysis brie explore complexity implementing software sds table shows number lines code components system case studies table complexity found eof tool basic cache hash tables operation inferencing code case studies trivial implement top base infrastructure traxtent sds journaling sds require thousand lines code conclude including type functionality sds pragmatic conclusions beware false knowledge dangerous ignorance george bernard shaw recent article wise drives gordon hughes associate director center magnetic recording research writes favor smarter drives stressing great potential improving storage system performance functionality believes interface systems storage required widespread drive input output command requirements interface speci cation short industry consensus task general interest offers market opportunities multiple computer drive companies hughes comments illustrate dif culty interfaces require wide-scale industry agreement eventually limits creativity inventions existing interface framework information system disk low-level knowledge drive internals sds sits ideal location implement powerful pieces functionality disk system implement enabling innovations existing interfaces storage system manufacturers embed optimizations previously relegated domain systems enabling vendors compete axes rpaci-dusseau remzi arpaci-dusseauuniversity cost wisconsin-madison performance ite paper demonstrated underneath ryth class tiv ffs-like systems cheaper le-system faster information processing power exam automatically gathered ple sma isks exploited implement ryth functionality drives distributed heretofore etw ork betw een implemented processors devices system exam ple etw implemented ork-attached storage shown ffe costs ctive ftw reverse-engineering system structure leverage power behavior active reasonable components challenges remain including understanding generality oftw robustness tem semantic inference future ools build active istrib systems pragmatic easy system developers powerful xploit active nature system active systems permit extensibility download code device tailor applications system simplicity maintainability provide primitives clients compose interface traditional distributed syste ilt rpc simple easy-to-use communication paradigm designed active world build distributed systems active components scriptable rpc srpc paradigm extensible distributed systems pragmatic rpc-like development process powerful exploit active components easily case study active storage high performance efficient composition primitives rapid addition functionality powerful advanced consistency semantics nfs sim ple substantial functionality lines code simplicity design obviate distributed locking crash recovery llin syste utline tiva tio crip cas tudy tiv torage performance functionality simp licity summary script evolve remote procedure call rpc rfa server scripting capability client script limited executioncontext active disk script interpreter result prototype tcl scripting language migration path fficie tio scrip igration path make transition paradigm tru code embed scripting serverautomatically generated existin ifie clie xist ith scrip tin clie development process interfacedefinition interface implementation idl compiler stubs native compiler distributed service interfacedefinition interface implementation idl compiler stubs native compiler distributed service tcl wrappers interpreter efficient execution scripts hide script interpretation overhead script caching exploit efficient bytecode representation concurrency multiple interpreters run simultaneously fast standard library primitives imp safety guard misbehaving client scripts lim ited execution environm ent afetcl eve ile tim typ -ch ckin prevent illegal ory automatic tracking locks safe concurrent execution utline tiva tio crip cas tudy tiv torage performance functionality simp licity summary utilize power disks clientspecific processing previous approaches demonstrate performance benefits require radically architectures migration path existing services limited class applications parallel database primitives iro platform p-iii mhz machines mem net linux kernel case studies enhance scfs srpc applica tion vfs scfs ctiv disk nfs protocol linux ernel client cfs erform ance enhancem ents combine dependent sequence ofo tio sin scrip reduction network round-trips needed fora logical operation benefit sensitive network delay ignificant savings dialup ide-area overloaded fast netw orks ctio tra ffic helps overcome limitations interface pathname lookup read dir page client disk rpc foo pathname lookup find inodenumber read dir page page data client disk rpcabc bar foo foo pathname lookup find inodenumber page data getattr inode client disk rpc foo read dir page pathname lookup find inodenumber page data getattr inode attributes client disk rpc foo read dir page pathname lookup find inodenumber page data getattr inode attributes client disk client disk execscript read-getattr find inode number getattributes rpc srpc foo foo read dir page pathname lookup find inodenumber page data getattr inode attributes client disk client disk execscript read-getattr find inode number getattributes attributes rpc srpc foo foo read dir page fits fits fits fits rfo illu tra tiv compositions micro-benchmarks benefit due reduced network roundtrips macro-benchmarks postm ark network traffic tpc netw ork traffic cilita rkin min ima rfa utline tiva tio crip cas tudy tiv torage performance functionality simp licity summary scfs functionality enhancements virtual protocols physical protocols state added stateless protocols system primitives clients compose desired functionality siste tics sprite consistency semantics nfs consistency semantics nfs afs tateless server lie cks rio ica lly write -clo tics server tracks clients caching file notifies clients modified file written requires server-side state participation implement existing paradigms rip afs consistency client client rip afs consistency client client execscript afs open open rip afs consistency client client file data execscript afs open llba list cac hed file rip afs consistency client client execscript afs open open llba list cac hed file rip afs consistency client client file data execscript afs open llba list cac hed file rip afs consistency client client execscript afs close dirt file close llba list cac hed file rip afs consistency client client invalidate cache execscript afs close dirt llba list cac hed file lit rfu add complex functionality requiring augmenting server state srpc simple afs consistency scripts lines sprite consistency scripts lines ple base system pact scripts extend lic ity ility tio rve simplifies implementation atomic sets ofoperations obviates distributed locks distributed crash recovery rre ire cto concurrent directory updates client client abc concurrent directory updates client client read abc abc crea concurrent directory updates client client read abc abc crea create bar abc concurrent directory updates client client abc abc foo abc bar concurrent directory updates client client write abc foo abc foo abc bar concurrent directory updates client client write abc bar abc foo abc bar rre ire non-scripting distributed locking distributed crash recovery clients acquire locks read-modify-write recover client failures holding locks scrip ire -me rve rce xclu sio sin srpc high performance rapid extensibility simplicity makes effective active architecture scripts lines code implement non-trivial functionality fewer lines code fewer bugs robust systems ease building systems active components complex system catering client requirements provide primitives enhance compact scripts wisconsin network disks group http wisc wind tio 
broader range systems sophisticated systems wider range platforms probed reveal workings approximate information exploited implement interesting functionality techniques tools developed assure correct operation 
semantic technology answer questions research experimentation nal answer elicited acknowledgments members wind research group feedback ideas presented paper keith smith excellent shepherding anonymous reviewers thoughtful suggestions greatly improved content paper finally computer systems lab tireless assistance providing terri environment computer science research work sponsored nsf ccrccr- ccrngs- itran ibm faculty award wisconsin alumni research foundation timothy denehy sponsored ndseg fellowship department defense proceedings usenix conference file storage technologies fast san francisco california acharya uysal saltz active disks proceedings conference architectural support programming languages operating systems asplos viii san jose october amiri petrou ganger gibson dynamic function placement dataintensive cluster computing proceedings usenix annual technical conference pages june anderson chase vahdat interposed request routing scalable network storage transactions computer systems tocs february arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october baker asami deprit ousterhout seltzer non-volatile memory fast reliable file systems proceedings international conference architectural support programming languages operating systems pages boston massachusetts october acm sigarch sigops sigplan bauer priyantha secure data deletion linux file systems tenth usenix security symposium washington august brown yamaguchi oracle hardware assisted resilient data oracle technical bulletin note burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey june chao english jacobson stepanov wilkes mime high performance parallel storage device strong recovery guarantees technical report hpl-csp- rev laboratories november collberg reverse interpretation mutation analysis automatic retargeting conference programming language design implementation pldi las vegas nevada june jonge kaashoek hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles pages asheville december denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey june dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey california june emc corporation symmetrix enterprise information storage systems http emc english stepanov loge selforganizing disk controller proceedings usenix winter technical conference pages san francisco january ganger blurring line oses storage devices technical report cmu-cs- carnegie mellon december gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proceedings conference architectural support programming languages operating systems asplos viii october gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon gray storage bricks arrived invited talk usenix conference file storage technologies fast gutmann secure deletion data magnetic solid-state memory sixth usenix security symposium san jose california july hagmann reimplementing cedar file system logging group commit proceedings acm symposium operating systems principles november proceedings usenix conference file storage technologies fast san francisco california hsieh engler back reverseengineering instruction encodings proceedings usenix annual technical conference usenix boston massachusetts june hughes wise drives ieee spectrum august katcher postmark file system benchmark technical report trnetwork appliance october king dirty lesystem bug ext https listman redhat pipermail ext users -april html march lumb schindler ganger freeblock scheduling disk firmware proceedings usenix conference file storage technologies fast monterey january mckusick joy lef fabry fast file system unix acm transactions computer systems august morton data corrupting bug ext http uwsg hypermail linux kernel html dec ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference anaheim june padhye floyd inferring tcp behavior sigcomm san deigo august regehr inferring scheduling behavior hourglass proceedings usenix annual technical conference freenix track monterey june riedel gibson faloutsos active storage large-scale data mining multimedia vldb york august rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler grif lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey january seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems proceedings usenix annual technical conference pages san diego june swartz brave toaster meets usenet lisa pages chicago illinois october talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans february wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey june gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings fourth symposium operating systems design implementation osdi san diego october zhou philbin multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference boston june 
bridging information gap storage protocol stacks timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison ftedenehy dusseau remzig wisc abstract functionality performance innovations file systems storage systems proceeded largely independently past years result information gap information designed implemented result high cost maintenance poor performance duplication features limitations functionality bridge gap introduce evaluate division labor storage system file system develop enhanced storage layer exposed raid raid reveals information file systems built specifically raid exports parallelism failure-isolation boundaries storage layer tracks performance failure characteristics fine-grained basis advantage information made raid develop informed log-structured file system lfs lfs extension standard logstructured file system lfs altered advantage performance failure information exposed raid experiments reveal prototype implementation yields benefits management flexibility reliability performance storage system small increase file system complexity lfs raid incorporate disks system on-the-fly dynamically balance workloads disks system user control file replication delay replication files increased performance functionality difficult impossible implement traditional division labor file systems storage introduction chasm exists world file storage management hierarchical file system directories byte-accessible files norm years internals file systems underlying storage systems evolved substantially improving performance functionality file systems approaches developed improve performance including read-optimized inode file placement logging writes improved meta-data update methods scalable internal data structures off-line reorganization strategies techniques developed assumption file system run single traditional disk recently storage systems received attention smart disks improve read write performance block remapping techniques o-intensive workloads multiple-disk storage systems studied research community achieved success storage industry high-end storage systems provide illusion single fast disk unsuspecting file systems internally manage parallelism redundancy optimize performance capacity analogous file systems storage systems developed single ffs-like file system mind file systems parallel disk systems substantial separate result information gap file system understand true nature storage system runs storage system comprehend semantic relations blocks stores addition unaware state tracks optimizations performs gap arose historical source hardware software boundary file systems traditionally expected block-based read write interface storage interface similar single disk exports advent hardware-based raid systems storage vendors advantage freedom innovate interface developed high-performance high-capacity systems appeared single large fast disk file system software modifications required host operating system file systems continued operate correctly spite fact optimized single-disk system case ignorance bliss arrangement simple worked boundary file system storage system changing migrating softwarestructuring technique interface necessitated hardware software raid drivers platforms advent networkattached storage client-side striping software replace hardware-based raid systems software-based raids attractive due low cost linux-based system incurs cost machine disks term arrangement file system layer top software storage layer storage protocol stack akin networking protocol stacks prominent communication networks similarities layering simplify system design potentially cost performance crucial difference exists layers comprise network protocol stacks derived design architects carefully deciding specific element storage protocol stack developed single coherent manner end result poor performance potential duplication implementation limitations functionality performance suffer model file system storage layer accurate layout optimizations work single traditional disk logical-block physical-block mapping unknown feature duplication potential pitfall log-structured file system layered top disk array performs logging duplicating work increasing system complexity unnecessarily finally functionality limited pieces information live layer system storage system blocks constitute file perform per-file operations block longer live file deletion optimize system ways knowledge time ripe re-examine division labor file system storage system layers attempt understand structure storage protocol stack specifically piece storage functionality understand easily effectively implemented problem germane time move network-attached storage proposed higher-level disk interfaces paper step goal exploring single point spectrum designs bridge file system storage system information gap develop evaluate division labor file system storage realignment storage layer exposes parallelism failure isolation boundaries part full file systems built on-line performance failure characteristics call layer exposed raid layer raid advantage information provided raid introduce informed lfs lfs enhancement log-structured file system combining performance failure information presented raid file-system specific knowledge lfs flexible manageable traditional file system deliver higher performance availability adding disk lfs on-line easily accomplished lfs accounts potential heterogeneity introduced disk dynamically balances load disks system rates lfs increases flexibility storage enabling user control redundancy per-file basis implements lazy mirroring defer replication time potentially increasing performance system slight decrease reliability crucial lfs raid implementation aforementioned benefits significant increase complexity maintainability storage protocol stack careful design functionality mentioned implemented increase code size compared traditional system lfs raid panacea find managing redundancy file system onerous requiring careful placement inodes data blocks ensure efficient operation failure extending traditional file system structure support enhanced functionality lfs arduous task redesign age-old vnode layer support informed file systems warranted rest paper structured begin discussion related work section section give overview approach describe raid lfs sections section present evaluation system present discussion section future work section conclude section related work part motivation informing file system nature storage system reminiscent work berkeley fast file system ffs ffs early demonstration benefits low-level understanding disk technology colocating correlated inodes data blocks performance improved compared unix file system work goal multidisk storage systems mind file system base decisions reliablyobtained information characteristics storage relying assumptions hold time seek costs dominate rotational costs roselli discuss file system storage system gap talk file system fingerprinting solution enrich interface file systems storage systems giving storage system information blocks related blocks accessed future approach storage system information file system collected presumes storage layer make good information potential problem approach require agreement set interfaces cooperating storage vendors file-system implementors benefits low-level knowledge disk characteristics found schindler recent work track-aligned extents authors explore range performance improvements allocating accessing data disktrack boundaries avoiding rotational latency track-crossing overheads single-disk setting contrast raid exposes disk boundaries raid file systems detailed lower-level information future interesting investigate benefits lower-level knowledge specifics raid-based storage system network appliance pioneered ideas discuss work file server appliances development wafl write-anywhere file layout technique hitz hint information hidden inside raid layer advantage file system ensure writes raidlayer occur full-stripe-sized units avoid small-write penalty manifests raidand raidsystems step formalizing raid layer showing traditional file system easily modified advantage information provided raid demonstrating broader range optimizations attainable framework volume managers long ease management storage multiple devices raid layer simply type volume manager exposes information file 
systems specifically on-line performance failure information raid built presupposition single mounted file system utilize multiple volumes data volume managers assume one-to-one mapping mounted file system volume volume manager similar raid pool driver volume manager sans sub-pool concept file system group related data work gfs file system sub-pools separate journaled meta-data normal user data exposing disk storage system file system extension arguments made engler kaashoek authors argue software abstractions made operating systems fundamentally problematic high-level limit power functionality authors advocate solution exposing hardware features user missing argument minimalism observation hardware abstractions users operating systems change apropos data storage abstraction put raid systems high-level raid breaks revealing information hidden file system distributed file systems zebra xfs manage disk system individually manner similar lfs systems traditional storage management techniques raidstriping advantage potential possibilities raid layer makes future hope extend ideas distributed arena direct comparison recently nasd object interface introduced higher-level data repository sanbased distributed file systems interface advanced functionality storage layer raid designed functionality file system earlier work datamesh proposes sophisticated interfaces network-attached storage informed approach similar large body work parallel file systems parallel file systems expose disk parallelism application file system manage control redundancy parallel file system proposed work computation parity put user control user avoid well-known performance penalty raidand raidunder small writes overview sections present design implementation raid lfs primary goal designing system exploit information made raid allowing lfs implement functionality difficult impossible achieve traditional layering aim increase ease storage management performance multiple heterogeneous disks functionality meet demands diverse set applications primary goal implementing raid facilitate information provided raid simplest non-informed legacy file systems built top raid primary goal implementing lfs minimize impact transforming file system utilize storage interface require re-design vnode layer ruled mandate file systems changed order function system implementation effort integrate lfs highly localized modular fashion fewer lines code changed question addressed decision modify lfs traditional popular ffs-like journaling file system reason chose lfs natural flexibility data placement lfs modern write storage system write-anywhere systems provide extra level indirection writes location storage medium exploit aspect lfs part implementation mind number implementation techniques general applied file systems hope investigate future interested general lfs file system performance issues consult work rosenblum ousterhout subsequent research seltzer software developed context netbsd operating system raid implemented set hooks lower-level blockdriver calls detail section lfs implemented extending netbsd version lfs based original lfs bsd unix detail section chose netbsd version lfs stable solid implementation raid describe raid storage interface consists major components segmented address space exposes parallelism storage system file system functions inform file system dynamic state storage system mirror pairmirror pair linear address space blocks region region figure raid configuration diagram depicts raid configuration disks combined mirrored pair regions half size total address space presented client file system region layout performed mirror hidden file system segmented address space traditional raid array presents storage subsystem file system linear array blocks underneath true complexity raid scheme hidden file systems interact raid systems reading writing blocks keeping desire minimize change preserve backwards compatibility raid linear array blocks read written basic interface expose information storage system file system address space segmented specifically organized series contiguous regions mapped directly single disk set disks region boundaries made file system desires four-disk storage system disk capable storing blocks address space raid presents segmented blocks throughn map disk blocksn map disk exposing information raid enables file system understand performance failure boundaries storage system sections file system advantage place data region intelligently potentially improving performance reliability aspects storage system raid region represent single disk region configured represent mirrored pair disks raidcollection region viewed configurable software-based raid entire raid address space single representation conglomeration raid subsystems scenario information hidden file system cross-region optimizations region exists raid configuration mirrored pairs shown figure allowing region represent single disk primary benefits region configured raid mirrored pair disks file system forced manage redundancy choose desired arrangement backwards compatibility raid configured single striped mirrored raidregion allowing unmodified file systems change dynamic information segmented address space exposes nature underlying disk system file system part full knowledge make intelligent decisions data placement replication raid layer exposes dynamic information state region file system raid distinguishes traditional volume managers pieces information needed file system desire performance information per-region basis raid layer tracks queue lengths current throughput levels makes pieces information file system historical tracking information left file system file system resilience region failures occur failures region tolerate raid presents information file system figure file system mirror pair tolerate single disk failure informed failure occurs file system action directing subsequent writes regions moving important data bad region reliable portions raid address space implementation current implementation raid implemented thin layer file system storage system order implement striped mirrored raidregion simply utilize standard software raid layer provided netbsd prototype raid layer completely generalized date current form require effort file system lfs utilize segmented address space built interposing vnode strategy call remap requests logical block number virtual address space presented raid physical disk number block offset issued underlying disk raid dynamic performance information collected monitoring current performance levels reads writes prototype region boundaries failure information performance levels throughput queue length tracked low-levels file system complete implementation make information ioctl interface raid device note focus primarily utilizing performance information paper lfs describe lfs file system current design major pieces additional functionality compared standard lfs on-line expandability storage system dynamic parallelism account performance heterogeneity flexible user-managed redundancy lazy mirroring writes sum total added features make system manageable administrator easily add disk worry configuration flexible users control replication occurs higher performance lfs delivers full bandwidth system heterogeneous configurations flexible mirroring avoids costs rigid redundancy schemes discussion focus case separates lfs raid traditional raid raid layer exposes disk storage system separate region lfs on-line expansion contraction design ability upgrade storage system incrementally crucial performance capacity demands site increase administrator add disks ideally addition simple perform single command issued administrator automatic addition disk detected hardware require down-time keeping availability storage high immediately make extra performance capacity disk older systems on-line expansion storage system add disk on-thefly case administrator unmount partition expand 
tool similar re-mount file system worse systems require file system built forcing administrator restore data tape modern volume managers on-line expansion file system support lfs design includes ability incorporate disks raid regions on-line single command file system complicated support necessitated layers system hardware supports hot-plug detection disks power-cycle lfs add disks time reduction data availability amount work administrator put expand system small contraction important removal region simple addition incorporate ability remove region fly file system configured non-redundant manner data lost difference lfs traditional system scenario lfs files deliver applications implementation on-line expansion contraction storage file system views regions added extant fully utilized region added system blocks disk made allocation file system immediately begin write data conversely region removed viewed fully allocated technique general applied file systems similar ideas specifically log-structured file system composed collection lfs segments natural expand capacity lfs adding free segments implement functionality newfs ilfs program creates expanded lfs segment table file system entries segment table record current state segment raid region added file system pertinent information added superblock additional portion segment table activated approach limits number regions added fixed number flexible growth segment table file expanded dynamic parallelism design problem introduced flexibility administrator growing system increased potential performance heterogeneity disk subsystem disk raid segment performance characteristics disks system case traditional striping raid schemes work assume disks run identical rates traditionally presence multiple disks hidden storage layer file system current systems handle disk performance heterogeneity storage layer file system information research community proposed schemes deal static disk heterogeneity solutions require careful tuning administrator van jacobsen notes experience shows configured misconfigured complicating issue delivered performance device change time result workload imbalances fail-stutter nature modern devices present correct operation degraded performance clients advanced heterogeneous data layout schemes utilized work dynamic shifts performance handle static dynamic performance differences disks include dynamic segment placement mechanism lfs segment logically written free space file system exploit writing segments raid regions proportion current rate performance exploiting dynamic state presented file system raid dynamically balance write load system account static dynamic heterogeneity disk subsystem note performance disks roughly equivalent dynamic scheme degenerate standard raidstriping segments disks style dynamic placement performed traditional storage system autoraid basic mechanisms place unduly adds complexity system file system storage system track blocks pushing dynamic segment placement file system complexity reduced file system tracks blocks file located implementation original version lfs allocates segments sequentially based availability words free segments treated equally manage parallelism disks lfs develop segment indirection technique specifically modify ilfs newseg routine invoke data placement strategy ilfs newseg routine find free segment write alter region aware informed segment-placement decision choosing disks accordance performance levels information provided raid load set regions balanced major advantage decision implement functionality ilfs newseg routine localizes knowledge multiple disks small portion file system vast majority code file system aware region boundaries disk address space remains unchanged slight drawback decision region place segment made early segment written performance level disk segment fills significant placement decision potentially poor practice found performance problem flexible redundancy design typically redundancy implemented one-size-fits-all manner single raid scheme autoraid applied blocks storage system file system typically involved aware details data replication storage layer traditional approach limiting semantic information file system smart users applications exploited improve performance utilize capacity lfs explore management redundancy strictly file system managing redundancy file system greater flexibility control users current design users applications select file made redundant mirrored file mirrored users pay cost terms performance capacity file mirrored performance increases writes file capacity saved chances losing file increased turning redundancy well-suited temporary files files easily regenerated swap files lfs performs replication accounting system files users physical blocks contrast traditional file system mounted top advanced storage system autoraid users charged based logical capacity true usage storage depends access patterns usage frequency redundancy schemes implemented raid storage system notion file exists scheme easily implemented traditionally-layered system storage system wholly unaware blocks constitute file receive input user blocks replicate file system block block inode inode file file figure crossed pointer problem figure illustrates problem separate file means redundancy specifically element file inode data block replicated single lost disk make difficult find data block due extra requirement block pointer chain block live file inode number mirror inode consist single data block block disk crashes find data block copy exists remaining working disk storage system altered functionality realized future interesting investigate range policies top redundancy mechanisms automatically apply redundancy strategies class file akin elephant file system segregates files versioning techniques implementation accomplish goal per-file redundancy decided utilize separate unique meta-data original redundant files approach natural file system require on-disk data structures implementation straight-forward scheme assigns inode numbers original files odd inode numbers redundant copies method advantages original redundant files unique inodes data blocks distributed arbitrarily disks constraints allowing redundancy combination file system features number lfs inodes unlimited written log inode map stored regular file expanded prime disadvantage approach limits redundancy copy easily extended n-way mirroring scheme reserving i-numbers file problem introduced decision utilize separate inodes track primary mirrored copy file refer crossed pointer problem figure illustrates difficulty arise simply requiring component file inode indirect blocks data blocks replicated sufficient guarantee data recovered easily single disk failure ensure data block reachable disk failure block reachable implies pointer chain exists figure file inode number replicated inode number inode located disk data block mirror copy file inode disk data block primary copy file disk fails data block easily recovered inode surviving disk points data block failed disk file systems fatal flaw data block unrecoverable lfs performance issue extra information found segment summary blocks full recovery disk crash mandate full scan disk recover data blocks number remedies problem perform explicit replication inode pointer-carrying structures indirect blocks doubly-indirect blocks require on-disk format change inefficient usage disk space inode indirect block logical copies file system simpler approach divide conquer disks system divided sets writing redundant file disk lfs decides set primary copy redundant copy set pointers cross set guarantee single failure harm fact tolerate number failures disks set finally incorporating redundancy lfs presents difficult implementation challenge replicate data inodes file 
system re-writing routine creates modifies data disk develop apply recursive vnode invocation ease task embellish lfs vnode operations short recursive tail routine invoked recursively arguments routine operating i-number primary copy data file designated redundancy user instance file created ilfs create recursive call ilfs create create redundant file recursion broken call perform identical operation redundant file lazy mirroring design user-controlled replication users control replication occurs shown previous work potential benefits arise allowing flexible control redundant copies made parity updated delaying parity updates shown beneficial raidschemes avoid small-write problem reduce load mirrored schemes implementing feature file system level user decide window vulnerability file losing data files tolerable note enhancements difficult implement traditional system information required resides file system raid necessitating non-trivial lfs incorporate lazy mirroring usercontrolled replication scheme users designate file non-replicated immediately replicated lazily replicated choosing lazy replica user increase chance data loss improved performance lazy mirroring improve performance reasons delaying file replication file system reduce load burst traffic defer work replication period lower system load file written disk deleted replication occurs cost replication removed systems buffer files memory short period time seconds file lifetimes recently shown longer average scenario common previously thought implementation lazy mirroring implemented lfs embellishment file-system cleaner files designated lazy replicas extra bit set segment usage table indicating status cleaner scans segment finds blocks replicated simply performs replication directly making place replicated blocks avoid crossed pointer problem associates mirrored inode replication complete bit cleared file system replicates files -minute delay future set directly user application evaluation section present evaluation raid lfs experiments performed intelbased physical memory main processor -ghz intel pentium iii xeon system houses rpm seagate seq write seq read rand write rand read throughput access pattern baseline performance slow disksfast disks figure baseline performance comparison figure plots performance lfs raid sequential writes sequential reads random writes random reads tests run disks varying disks slow disks fast cases requests generated tests size total data-set size cheetah disks refer fast disks rpm seagate barracuda disks slow disks fast disks deliver data roughly slow disks approximately apiece experiments perform trials show average standard deviation experiments compare performance lfs raid standard raidstriping stripe sizes chosen maximize performance raidgiven workload hand making comparison fair slightly unfair lfs raid baseline performance experiment demonstrate baseline performance lfs raid top homogeneous storage configurations slow disks fast disks experiment consists sequential write sequential read random write random read phases based patterns generated bonnie iozone benchmarks perform experiment demonstrate unexpected overhead implementation scales higher-performance disks effectively figure sequential write sequential read random writes perform excellently achieving high bandwidth disk configurations surprisingly log-based file system random reads perform poorly achieving roughly slow disks fast disks line expect disks typical raid configuration throughput amount written performance expansion disk added disk added disk added figure storage expansion graph plots performance lfs storage expansion experiment begins lfs writing single disk time written disk brought on-line lfs immediately begins writing increased performance disk expansion accomplished simple command adds disk region file system time on-line expansion demonstrate performance system writes disks added system on-line experiment disks present expansion stresses software infrastructure hardware capabilities figure plots performance sequential writes time disks added system x-axis amount data written disk shown y-axis plots rate recent committed disk graph lfs immediately starts disks write traffic added system read traffic continue directed original disks older data lfs cleaner redistribute existing data newly-added disks explicitly cleaning explored possibility dynamic parallelism explore ability lfs place segments dynamically regions based current performance characteristics system order demonstrate ability lfs react static dynamic performance differences devices reasons performance variation drives disks added faster older unexpected dynamic performance variations due bad-block remapping hot spots workload uncommon lead performance random writes perform similarly due nature lfs throughput heterogeneity configuration fast disks slow disks performance static heterogeneity lfs exraid ffs ccd figure static storage heterogeneity figure plots performance lfs versus ffs ccd standard raidstriping series disk configurations x-axis number fast slow disks varied implies fast disks slow adjusting segments written dynamically lfs raid deliver full bandwidth disks contrast standard striping performs rate slowest disk system test written disk heterogeneity disks ability expand disk system on-line shown induces workload imbalance read traffic directed newly-added disks cleaner reorganized data disks system experiment static dynamic performance variations subsection figure shows results static heterogeneity test sequential write performance lfs dynamic segment placement scheme plotted ffs top netbsd concatenated disk driver ccd configured stripe data raidfashion experiments data written disks x-axis increase number slow disks system extreme left disks fast slow middle heterogeneous configurations figure writing segments dynamically proportion delivered disk performance lfs raid deliver full bandwidth underlying storage system applications performance degrades gracefully slow disks replace fast storage system raidstriping performs rate slowest disk performs poorly heterogeneous configuration perform misconfiguration test experiment configure storage system utilize partitions disk emulating misconfiguration administrator similar spirit tests performed brown patterson disk system appears separate disks case lfs raid throughput amount written performance dynamic heterogeneity lfs exraid ffs ccd figure dynamic storage heterogeneity figure plots performance lfs raid ffs ccd dynamic performance variation experiment performance single disk temporarily degraded faulty disk delays requests fixed time reducing throughput disk adaptively writing data disks lfs raid dynamic segment placement adjust imbalance deliver higher throughput writes data disk standard striping delivers dynamic segment striping lfs successfully balance load disks case properly assigning load partition accidentally over-burdened disk final heterogeneity experiment introduce artificial performance fault storage system consisting fast disks order confirm load balancing works face dynamic performance variations figure shows performance write lfs raid dynamic segment placement ffs ccd raidstriping case single disk exhibits performance degradation data written kernel-based utility temporarily delay completed requests disks delay effect reducing throughput impaired disk returned normal operation additional data written figure lfs raid job tolerating fluctuations induced phase experiment improving performance factor compared ffs ccd flexible redundancy redundancy experiment verify operation system face failure figure plots performance set processes performing random reads redundant files lfs initially bandwidth disks utilized balancing read load mirrored copies data throughput amount read performance failure disk failed disk failed figure storage failure figure 
plots random read performance set mirrored files disks lfs labeled points graph disk offline performance decreases lfs longer balance read load replicas note lfs raid survive single disk failure failure lfs raid tolerate loss disk set experiment progresses disk failure simulated disabling reads disks lfs continues providing data replicas performance reduced demonstrate flexibility per-file redundancy redundancy managed file system total files written concurrently system consisting fast disks percentage files mirrored increased x-axis results shown figure expected net throughput system decreases linearly files mirrored mirrored throughput roughly halved per-file redundancy users pay users file redundant performance cost replication paid write performance write reflects full bandwidth underlying disks lazy mirroring final experiment demonstrate performance characteristics lazy mirroring figure plots write performance set lazily mirrored files delay seconds cleaner begins replicating data normal file system traffic suffers small decline performance default replication delay system minutes length abbreviated delay reduce time experiments figure potential benefits lazy mirroring potential costs lazily mirrored files deleted replication bethroughput percent files written redundantly cost redundancy figure per-file redundancy figure plots performance writes separate files percent files mirrored increases files mirrored net bandwidth system drops roughly half peak rate expected peak bandwidth achieved lower previous experiments due increased number files subsequent meta-data operations experiment written disk gins full throughput storage layer realized lazily mirrored files deleted replication system incurs extra penalty files read back disk replicated affect subsequent file system traffic lazy mirroring carefully systems highly bursty traffic idle time lazy replicas created files easily distinguishable short-lived discussion implementing lfs raid concerned pushing functionality file system code unmanageably complex primary goals minimize code complexity achieve goal integrating major pieces functionality additional lines code increase original size lfs implementation additional code roughly half due redundancy management design standpoint find managing redundancy file system benefits difficulties solve crossed-pointer problem applied divide-andconquer technique placing primary copy file sets mirror enable fast operation failure solution limits data placement flexibility file assigned set subsequent writes file written set limitation affects performance heterogeneous configuthroughput time performance lazy redundancy lfs exraid cleaner figure lazy mirroring figure plots write performance set lazy redundant files lfs replication delay seconds peak performance achieved initial portion test performance reduced slightly cleaner begins replicating data write test completes cleaner continues replicate data background rations set significantly performance characteristics relax placement restrictions choosing disks constitute set per-file basis problem fundamental approach file-system management redundancy implementation standpoint file-system managed redundancy problematic vnode layer designed single underlying disk mind recursive invocation technique successful stretched limits current framework additions modifications code straightforward implement support file-system managed redundancy redesign vnode layer beneficial future work number avenues exist future research generally organizations storage protocol stack explored pieces functionality implemented trade-offs natural followon incorporate lower-level information raid main challenge exposing information file system find pieces information file system readily exploit file service today spans client server machines important functionality split machines portion traditional storage protocol stack reside clients portion reside servers researchers distributed file systems opposing points view systems zebra xfs letting clients work frangipani petal system places functionality storage servers cooperative approaches file system storage system found implementing redundancy file system vexing approach shared responsibility redundancy file system storage layer improvement storage layer file system block mirror block file system decide perform replication decide storage interface difficult convince storage vendors move tried-and-true standard scsi interface storage pragmatic approach treat raid layer gray box inferring characteristics exploiting file system modification underlying raid layer tools automatically extract low-level information disk drives dixtrac skippy steps goal extensions needed understand parallel aspects storage systems finally envision optimizations arrangement storage protocol stack exploring notion intelligent reconstruction basic idea simple disk region fails lfs duplicated data disk lfs begin reconstruction process key difference lfs reconstruct live data disk entire disk blindly storage system substantially lowering time perform operation fringe benefit intelligent reconstruction lfs give preference files reconstructing higher-priorityfiles increasing availability files failure imagine optimizations lfs cleaner data laid disk current performance characteristics access patterns meet subsequent potentially non-sequential reads applications similarly disks added cleaner run order lay older data disks cleaner reorganize data drives read performance presence heterogeneity drives similar work neefe generalized operate heterogeneous multi-disk setting conclusions terms abstractions block-level storage systems scsi successful disks hide low-level details file systems exact mechanics arm movement head positioning export simple performance model file systems optimize lampson interface combine simplicity flexibility high performance solving problem leaving rest client early single-disk systems balance struck perfectly storage systems evolved single drive raid multiple disks interface remained simple raid result system full misinformation file system longer accurate model disk behavior now-complex storage system good understanding expect file system raid lfs bridge information gap design presence multiple regions exposed directly file system enabling functionality paper explored implementation on-line expansion dynamic parallelism flexible redundancy lazy mirroring lfs implemented straight-forward manner file system increasing system manageability performance functionality maintaining reasonable level system complexity aspects lfs difficult impossible build traditional storage protocol stack highlighting importance implementing functionality correct layer system chosen single point design space storage protocol stacks arrangements preferable hope explored conclusion research division labor file storage systems proper division arrived design implementation experimentation historical artifact acknowledgements shepherd elizabeth shriver john bent nathan burnett brian forney florentina popovici muthian sivathanu anonymous reviewers excellent feedback work sponsored nsf ccrccr- ngsccr- itrand wisconsin alumni research foundation timothy denehy sponsored ndseg fellowship department defense anderson dahlin neefe patterson wang serverless network file systems proceedings acm symposium operating systems principles sosp pages copper mountain resort december arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations proceedings acm sigmod conference management data sigmod pages tucson arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common workshop input output parallel distributed systems iopads atlanta arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance eighth workshop hot topics operating systems hotos viii pages schloss elmau germany bray bonnie file system benchmark http textuality bonnie brown patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference pages san 
diego june comer internetworking tcp vol principles protocols architecture prentice hall london edition cormen kotz integrating theory practice parallel file systems proceedings dags symposium dartmouth institute advanced graduate studies pages hanover june cortes labarta extending heterogeneity raid level proceedings usenix annual technical conference boston june jonge kaashoek hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles sosp pages asheville december engler kaashoek exterminate operating system abstractions workshop hot topics operating systems hotos orcas island english stepanov loge self-organizing disk controller proceedings usenix winter technical conference pages san francisco january gibson nagle amiri chang feinberg gobioff lee ozceri riedel rochberg zelenka file server scaling network-attached secure disks proceedings acm sigmetrics international conference measurement modeling computer systems pages seattle june hartman ousterhout zebra striped network file system proceedings acm symposium operating systems principles sosp pages asheville december hitz lau malcolm file system design nfs file server appliance proceedings usenix winter technical conference berkeley january huber elford reed chien blumenthal ppfs high performance portable parallel file system proceedings acm international conference supercomputing pages barcelona spain july jacobson kill internet ftp ftp lbl gov talks vj-webflame kilburn edwards lanigan summer one-level storage system ire transactions electronic computers ecapril lampson hints computer system design proceedings acm symposium operating system principles pages bretton woods december acm lee thekkath petal distributed virtual disks proceedings seventh conference architectural support programming languages operating systems asplos vii pages cambridge october matthews roselli costello wang anderson improving performance log-structured file systems adaptive methods proceedings acm symposium operating systems principles sosp pages saint-malo france october mckusick joy leffler fabry fast file system unix acm transactionson computer systems august nieuwejaar kotz galley parallel file system proceedings acm international conference supercomputing pages philadelphia acm press norcutt iozone filesystem benchmark http iozone patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago june ritchie thompson unix time-sharing system comm assoc comp mach july roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference pages san diego june roselli matthews anderson file system fingerprinting works-in-progress symposium operating systems design implementation osdi february rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february rosenthal evolving vnode interface proceedings usenix summer technical conference pages anaheim santos muntz performance analysis rio multimedia storage system heterogeneous disk configurations acm multimedia december santry feeley hutchinson veitch carton ofir deciding forget elephant file system proceedings acm symposium operating systems principles sosp pages kiawah island resort december savage wilkes afraid frequently redundant array independent disks proceedings usenix technical conference pages san diego january schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler griffin lumb ganger trackaligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey january seltzer bostic mckusick staelin implementation log-structured file system unix proceedings usenix winter technical conference pages san diego january seltzer smith balakrishnan chang mcmains padmanabhan file system logging versus clustering performance comparison proceedings usenix annual technical conference pages orleans january seltzer ganger mckusick smith soules stein journaling versus soft updates asynchronous meta-data protection file systems proceedings usenix annual technical conference pages san diego june stodolsky holland courtright gibson parity-logging disk arrays acm transactions computer systems august sweeney doucette anderson nishimoto peck scalability xfs file system proceedings usenix annual technical conference san diego january talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley teigland pool driver volume driver sans master thesis minnesota december teigland mauelshagen volume managers linux freenix track usenix annual technical conference boston june thekkath mann lee frangipani scalable distributed file system proceedings acm symposium operating systems principles sosp pages saint-malo france october http fsprogs sourceforge net ext html june van renesse masking overhead protocol layering proceedings acm sigcomm conference pages palo alto veritas http veritas june wang anderson patterson virtual logbased file systems programmable disk proceedings symposium operating systems design implementation osdi orleans february wilkes datamesh research project phase proceedings usenix file systems workshop pages wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february zimmermann ghandeharizadeh hera heterogeneous extension raid technical report usc-cs-tr southern california 
storage-aware caching revisiting caching heterogeneous storage systems brian forney andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison bforney dusseau remzi wisc abstract modern storage environments composed variety devices performance characteristics paper explore storage-aware caching algorithms file buffer replacement algorithm explicitly accounts differences performance devices introduce family storageaware caching algorithms partition cache partition device algorithms set partition sizes dynamically balance work devices simulation show storageaware policies perform similarly landlord costaware algorithm previously shown perform web caching environments demonstrate partitions easily incorporated clock replacement algorithm increasing likelihood deploying cost-aware algorithms modern operating systems introduction modern computer systems interact broad diverse set storage devices including local disks remote file servers nfs afs archival storage tapes read-only media compact discs dvds storage sites accessible internet storage components introduced behaviors properties divergent today set devices disparate commonality pervades time access high compared cpu cache memory latencies due cost fetching blocks storage media caching blocks main memory reduces execution time individual applications increases system performance orders magnitude storage technology dramatically changed past decades important aspects caching architectures modern operating systems remained unchanged innovations mechanism including integration file cache virtual memory page cache copyon-write techniques software emulation bits change policy operating systems employing lru lru-like algorithms decide block replace problem lru related caching algorithms cost-oblivious blocks treated fetched identically performing devices re-fetched replacement cost blocks assumption increasingly problematic manifold device types correspondingly rich set performance characteristics simple block fetched local disk compared fetched remote highly contended file server case operating system prefer block file server heterogeneous environments file systems require caching algorithms aware replacement costs file blocks slowest device roughly determines throughput system storage-aware caching seeks balance work devices adjusting stream block requests heterogeneous environment storage-aware cache considers workload behavior device characteristics filter requests paper explore integration costaware algorithms operating system page cache simulation simulation accounts real-world factors integrated page cache simplicity design build previous work cost-aware caching web-cache theory communities demonstrating separate set partitioned algorithms effective simpler proposals research areas study set context network-attached disk system network-attached disks increasingly important storage paradigm present clients static dynamic forms performance heterogeneity algorithms develop general applied broader range storage devices main results show storageaware caching significantly performance robust cost-oblivious caching robust leading web-caching algorithm operating systems specific implementation develop evaluate version storage-aware caching extends commonly implemented clock algorithm rest paper organized section give overview algorithms investigate paper describe algorithm selecting partition sizes section section describes assumptions environment detail explains simulation framework simulation results section compare contrast work existing work section section future work finally conclude section algorithm overview section overview algorithmic space explore describe existing cost-aware algorithms basis comparison present caching algorithms based partitioning cache replacement cost existing cost-aware algorithms theoretical community studied cost-aware algorithms k-server problems restricted class k-server problems weighted caching closely related cost-aware caching landlord significant algorithm literature comparison landlord closely related leading web caching algorithm landlord combines replacement cost cache object size locality extending lru fifo include cost variable cache object sizes cache configured landlord lru describe lru version landlord associates cost object called object enters cache landlord sets retrieval cost object divided size object object eviction needed landlord finds object lowest removes ages remaining objects landlord ages pages decrementing remaining objects evicted object object cache landlord restores landlord degenerates strict lru values landlord attractive theoretical experimental properties shown theoretical analysis size cache objects landlord k-competitive size cache fixed object size cache landlord performs factor optimal off-line algorithm request sequences overview aggregate partitioned algorithms cost-aware algorithms literature place-anywhere place-anywhere algorithm characteristics blocks occupy logical location cache independent original source cost costs recorded page granularity advantage place-anywhere algorithms calculate single trade-off locality cost replacement time algorithms bias eviction pages low retrieval cost contrast place-anywhere algorithm aggregate partitioned algorithm divides cache logical partitions blocks logical partition device share replacement cost algorithm aggregates replacement cost function device performance aggregate partitioned algorithm benefits aggregation blocks cost metadata ways amount metadata reduced metadata closely reflects current replacement cost block device space overhead proportional number devices blocks replaced replacement cost low conversely place-anywhere algorithms record cost page brought cache cache large number pages common today place-anywhere algorithm susceptible inconsistent cost values aggregate partitioned algorithms avoid problem aggregating cost metadata per-device basis performance device cost metadata rarely inconsistent period time placeanywhere algorithm recognize change cost device propagate cost pages cache cost update requires significant number pages updated increasing overhead implementation complexity aggregate partitioned algorithms strive set relative size partition balance work devices define work balanced cumulative delay device period time equal balance work size partition reflects relative cost blocks simple efficient manner storage system slow disk fast disk cache divided partitions slow disk receiving larger partition describe precisely relative sizes configured section choose victim block storage-aware algorithm selects victim partition victim block partition victim partition chosen resulting size relative partitions maintains desired proportions individual victim partition selected replacement algorithm lru lfu fifo distinctions prior work virtual memory systems noted unified partitioned virtual memory systems traditional sense partitioned virtual memory systems distinguish file system pages virtual memory pages managed separately storage-aware algorithms explicitly distinguish file system pages virtual memory pages order balance work algorithms distinguish pages based device supplied page additionally storage-aware caching algorithms change size partitions dynamically partitioned virtual memory systems change size file system cache virtual memory partitions local global page replacement local page replacement eviction time considers processes isolation global page replacement applies replacement processes storage-aware algorithms make per-partition replacement decisions similar traditional notion local page replacement decisions based cost locality solely locality local page replacement schemes taxonomy aggregate partitioned algorithms work investigate taxonomy aggregate partitioned algorithms show dynamic aggregate partitioning needed taxonomy subsection basic approaches aggregate partitioning static dynamic static scheme ratio partitions selected one-time notion costs knowledge workload resulting miss rates cache size priori determine relative sizes lead balanced work dynamic partitioning needed ratio partition sizes adjusts requests monitored dynamic partitioning benefits dynamic partitioning adjust dynamic performance variations faults common modern devices dynamic partitioning react contention devices due hotspots workloads finally dynamic partitioning compensate fact performance ratios devices change function access patterns dynamic partitioning divided eager partitioning 
lazy partitioning eager partitioning partition sizes desired algorithm immediately reallocates pages cost information algorithm lazy partitioning scheme gradually reallocates pages demand desired size response workload eager partitioning simplifies choosing victim partition location page cost removing pages conversely lazy partitioning scheme removes pages partitions needed partition lazy partitioning block replace block long blocks replaced proper frequency maintain desired partition size ratios replacement explicitly choose victim partition investigate strategy based inverse lottery previously proposed resource allocation idea partition number tickets inverse proportion desired size replacement needed lottery held selecting random ticket partition holding ticket picked victim victim valuable page lazy partitioning algorithm allocates page logical partition selecting partition sizes main challenge partitioned approaches determining relative sizes partitions configured storage-aware caching viewed performing selective filtering requests devices assuming slowest device limits system throughput goal storage-aware caching set partition sizes equal amount work device formally device number cache misses multiplied average cost miss equal algorithmic details basic approach dynamic repartitioning algorithm algorithm storage-aware cache observes amount work performed device fixed interval past predicts relative sizes partitions adjusted work equal algorithm work metric cumulative delay period time delay related total number requests includes request service time variation device devices algorithm streaming accesses fit cache problematic algorithm detect type access algorithm measures time spent waiting device past device requests window size records wait time device wait times approximately equal current partition ratios deemed adequate remain wait times size partitions large wait times increased size partitions small wait times decreased ratio wait times devices considered simultaneously selecting amount increment decrement partition non-trivial search problem change partition size affects future miss rate presence dynamically changing workloads initial approach employ simplest algorithm found meet challenge find algorithm adjusts partition sizes quickly find proportions quickly algorithm overshoots correct proportions meet goals simultaneously approach aggressively increases size partition wait time device increasing reacts conservative manner algorithm makes observations wait time device epoch action based observation epoch begins device requests complete epochs cache repartitioned repartitioning occurs steps algorithm computes per-device wait time wait time devices epoch algorithm computes relative wait time partition dividing per-device wait time wait time algorithm determines partitions page consumers pages give consumer page consumers partitions relative wait time threshold threshold filter normal variations wait time due workload device characteristics page consumers found repartitioning stops epoch begins finally algorithm finds partitions below-average wait times called page suppliers reallocates pages consumers consumers reach desired size repartitioning cache algorithm classifies figure corrective actions repartitioning algorithm figure shows actions algorithm response states graph shows observation per-device wait time trend relative wait time time progresses dotted line shows wait time graph graphs actions state shown fixed clarity threshold constant multiplied wait time partition states corrective action change partition size states shown figure cool wait time threshold wait time normal operating regime corrective action needed cool partitions page suppliers page consumers warming wait time threshold increasing algorithm infers increasing wait time due workload device characteristics initially cache size increased pages base correction amount partition continues warm subsequent epochs increase cache size grows exponentially reclassification partition warming state restarts exponential correction cooling wait time threshold decreasing corrective action set epochs halted increase wait time partition started decline wait time algorithm acts conservatively cooling state change partition size aggressive approach continues increase cache size state over-correct unstable warm wait time threshold constant based experimental evidence partitions classified cool warming cooling constant wait time occur partition moves state small change partition size algorithm increases partition size step algorithm reallocates pages page suppliers page consumers algorithm biases collection pages partitions lowest relative wait times determine number pages removed page supplier algorithm computes inverse relative wait time irwt relative wait time partition sums inverse relative wait times finally number pages partition supply computed irwtj sum irwtsforsuppliers consumed pages note parameters algorithm window size threshold base increment amount set care large smooth wait time variations sample sufficient number requests determine accurately effect corrections found sufficient smoothing feedback small exponential correction found cache works practice threshold large filter normal device performance fluctuation seek time found detects wait time warrant correction fixed algorithm compute threshold dynamically statistical variance wait times sum variance threshold discuss adaptive approach plan investigate future modifying existing replacement algorithms desire cost-aware cache performs easily implemented modern operating systems attention paid make computationally efficient landlord priority queue efficiently find lowest cost object mesh common virtual memory hardware easy combine landlord existing code base modern operating systems including solaris variant clock page replacement policy unified page cache desire algorithm incorporated easily clock structure introduce extension clock takes partitions account partitioned-clock base algorithm partitioned-clock assumes page bit set page referenced victim page needed clock arm successive pages bit set clearing bits sweeps partitioned-clock page tracks partition number belongs page selected victim bit cleared partition number match partition number chosen replacement chosen lottery note additional bits single bit consistent variations clock examine dirty bits history multiple bits optimizations improve performance partitioned-clock approximation lazy partitions searching replacement pages belonging victim partition bits cleared clearing bits pages unnecessarily removes usage history separate clock hand partition improves performance helps maintain usage history partition previously lazy partitions simpler implement eager partitions ratio sizes dynamic focus clock algorithm applied lazy partitions version termed lazy clock inverse lottery scheduling pick victims partitions evaluation environment section describes methodology evaluating storage-aware caching specifically overview simulator describes simulated storage environments simulator developed trace-driven storage-system simulator study behavior storage-aware caching configured simulated environment single client connected sixteen network-attached storage devices simulator explore performance impact client workloads data layout caching algorithms network characteristics disk characteristics storage-system heterogeneity simulator driven workload client trace file trace file represents data block requests striped raidacross full set disks request specifies starting offset length data read write simulate system high demand closed workload model completion disk request immediately triggers request client local cache replacement policies focus investigation model time cache hit small real system dwarfed cost remote-block access time cache miss sum network transit time remote disk service time trace trace trace request distribution uniform exponential exponential disk distribution uniform uniform gaussian locality random random random request size 
working set size requests table characteristics synthetic traces table summarizes synthetic traces set experiments gaussian distribution disk standard deviation storage device model roughly matches ruemmler wilkes model cylinders seek time rotational delay bandwidth calculating transfer time request specifically disk request falls cylinder previous request model sequential seek rotational delay set transfer time determined bandwidth nonsequential requests rotational delay chosen uniformly random full rotation time seek time non-linear model depends cylinder distance current request previous request network model based loggp endpoint contention loggp designed model communication large parallel computers depends parameters message latency network endpoint overhead minimum time message sends seconds byte network number endpoints workloads fully understand impact storage-aware caching algorithms study sets workloads variety synthetic traces web server trace collected analyzed roselli simply refer roselli web server trace roselli trace synthetic workloads control request size distributions working set size locality distributions distribution request disks synthetic traces summarized table traces read traces variety request sizes stress small large read requests trace adds request imbalance disks roselli trace image server california berkeley january age bandwidth seek rotation years avg avg table aging ibm lzx model bandwidth seek rotation time family disks based ibm lzx manufactured progressively older years assume bandwidth improves factor year seek rotation time year image server ran web server postgres database stored images trace alternates large reads files database tables small reads writes storage-system characteristics goals configuring set disks simulated environment goal understand full sensitivity storage-aware caching algorithms device heterogeneity requires diverse range configurations goal understand algorithms perform realistic scenarios requires focused set tests meet goals simultaneously employ device aging performance-fault injection idea device aging choose base device case ibm lzx age performance range years collections disks create configurations key performance base disk scaled fixed amount component bandwidth seek rotation time scaled expected yearly improvement historical data suggests year improvement bandwidth roughly year reduction seek time rotational latency realistic aggressive side table shows performance characteristics aged devices experiments note progressively older disks backwards aging newer disks based current year similar manner forward aging performance-fault injection dynamically throughput age slow disk years slow disk lru clock cache throughput number slow disks multiple slow disks lru clock cache figure performance lru clock caching figures show throughput storage system trace figure left varies age single disk x-axis figure increases number year disks system change performance drive experiment earlier represent disk stuttering absolute failure unexpected network traffic client drive sudden workload imbalance environment configuration section describes details simulator configuration configure network bottleneck system choose parameters similar ethernet set bandwidth future hope investigate network performance caching interact distributed storage systems configured simulator separately synthetic roselli traces synthetic traces choose sufficient number requests mitigate effects cold-start misses set client cache size roselli trace set cache size hit rate hit rate high requests disks heterogeneity device performance issue traces include disk layout file path information created simple layout policy layout policy assumes raidstriping policy lays blocks order access aged disks scenarios scenarios represent cases storage-system incrementally updated newer faster devices added time scenario single heterogeneous disk performance aged entire range years scenario groups heterogeneous disks group age age years relative size groups varied scenarios cover real-world situations provide insight common configurations scenario mimics stuttering disks increased workloads clients scenario closely incremental upgrades disk array incremental upgrades occur due cost constraints prohibit replacement entire array small number disks fail experiments section presents progression experiments demonstrating effectiveness storage-aware caching begin motivating cost-aware caching algorithms heterogeneous devices show partitioned approaches mask performance differences configuring fixed partition ratios correctly difficult static environment demonstrate mask performance heterogeneity adjusting ratio partition sizes on-line observations amount work performed device finally show partitioned approaches easily incorporated operating system replacement policies perform explore performance robustness trace web server motivating set experiments motivates storage-aware caching algorithms storage system heterogeneous devices figure shows throughput obtained trace common replacement policies cost-aware lru throughput age slow disk years static lru cache figure potential partitioned approaches slow disk aged shown x-axis approaches cache lru caching static partitioning cache disk performance trace figure clock caching graph left illustrates disks aged seek rotation time bandwidth decline throughput system drops dramatically performance benefit file cache decreases lru replacement throughput drops disks equally fast single disk performance -year disk similarly graph shows entire storage system runs rate slowest disk system throughput slow disk fast disks poor slow disks contrast storage-aware caching algorithm mask performance slow disks allocating cache slow disks slow disk fewer requests handle harm performance system dramatically configuring partition sizes set experiments show partitioned caching algorithms potential mask heterogeneous performance care selecting ratio partition sizes begin examining static partitioning algorithm simply named static figure show performance static trace experiments ratio partition sizes statically set directly proportional ratio expected service time disk trace request size directly compute expected transfer time disk function seek time rotation delay peak bandwidth graph static throughput partition ratio trace figure sensitivity static partitioning partition ratios workload graph shows performance workloads run disks twoyear disk experiment varies ratio slow disk partition fast disk partitions x-axis lower lines requests vary ratio requests slow disk versus ratio partition strategy significantly improves performance relative cost-oblivious algorithms lru static performs priori request size per-disk miss rate function cache size real world information advance difficulty correctly configuring static partition strategy illustrated figure experiment examine single storage configuration slow disk years older disks x-axis graph varies ratio partition sizes slow disk disks system greater slow disk correspondingly larger partition lines graph correspond workloads optimal partition size ratios top line workload examined verify highest throughput workload approximately matches shown figure two-year disk lines distribution requests disks changed slow disk receives ten times requests disks graph shows workloads optimal partition ratio ratio top workload ratio bottom workload performance workload varies greatly partition ratio performance workload varies throughput age slow disk years trace lazy lru eager lru landlord lru cache throughput age slow disk years trace lazy lru eager lru landlord lru cache figure dynamic partitioning algorithms figure shows performance eager lru lazy lru landlord disk aged left graph trace graph trace approach select partition ratios dynamically 
function workload disk performance partitioning balance work set experiments shows dynamically adjusting size partition algorithms balance amount work performed disk effectively hide heterogeneity classes dynamic partitioning eager partitioning lazy partitioning lazy partitioning inverse lottery scheduling pick victim partitions replacement time eager lazy lru partitions simplicity refer approach eager lru approach lazy lru experiments investigate trace trace realistic evaluation continuing understood workload parameters figure compares performance eager lru lazy lru storage-aware algorithms lru landlord left-most graph figure examine workload uniform number requests disks setup throughput lru degrades dramatically performance slow disk aged specifically throughput drops approximately eager lru lazy lru maintain throughput system slow disk aged specifically performance algorithms similar lru disks speed ten-year-old disk mask impact slow disk throughput graph figure shows challenges non-uniform number requests disks interestingly disks identical cost-aware algorithms perform lru workload popular disks suffer contention queueing delays make blocks disks costly fetch monitoring replacement cost cost-aware algorithms devote cache popular disks balance load disks previous workload performance benefits cost-aware caching improve disks aged -year disk eager lru lru differ factor comparing performance eager lru lazy lru landlord sees performance algorithms similar identical graph shows difference eager lru performance robust lazy lru landlord lazy lru devotes entire cache slow disk eager lru continues allocate small amount cache fast disks repartitioning eager lru aggravates efforts find good partition size trace clock-based replacement noted section operating systems clock algorithm clock cost-aware section evaluate lazy partitioned algorithm called lazy clock practical virtual memory page replacement algorithm traces compare lazy clock clock experimental results found figures figure lazy clock performs desired lazy clock greater proportion cache slower devices devices requests lazy clock mask performance differences speed slow disk degrades significantly imbalanced workload lazy clock begins throughput approximately disks identical degrades slow disk full -years older throughput age slow disk years trace lazy clock clock cache throughput age slow disk years trace lazy clock clock cache figure clock-based replacement algorithms figure shows performance lazy clock clock disk aged workloads investigated figure throughput number slow disks trace lazy clock clock cache figure clock-based multiple disks figure shows performance lazy clock clock number disks age years increased trace workload left graph figure throughput compares favorably landlord lazy lru figure figure shows lazy clock gracefully masks increasing number two-year disks clock affected heterogeneity performance lazy clock slowly degrades performance match clock system homogeneous two-year disks performance fairly close experience shown smaller base correction size devices homogeneous remove discrepancy dynamic performance evaluate tolerance performance faults show partitioned caching algorithms react relative performance storage devices cases effectively landlord experiments begin cluster homogeneous disks trace inject performance fault disks disk simulated time seconds approximately half simulation performance fault effect slowing disk factor graph figure show eager lru adjusts partition ratios change performance partitions initially equal performance fault window disk requests passed observation algorithm observes waiting time disk significantly higher average waiting time partition disk increased small amount partitions disks decreased number cache entries algorithm continues measuring wait time disk increasing partition size disk wait times approximately equal time-line shows correct partition ratio found quickly graph figure summarize results comparing performance landlord eager lru clock lazy clock plot throughput workload performance fault occurs disk disks disks simultaneously number affected disks small relative total number disks system aggregating replacement-cost information beneficial specifically eager lru achieves throughput approximately performance fault landlord maintains lazy clock performs finally costoblivious clock algorithm performs number faulty disks increases partition ratio simulation time seconds single fault injection disk fault injected disks eager lru landlord lazy clock figure performance dynamic faults cluster initially homogeneous disks performance fault time seconds slows disk factor graph left performance fault occurs single disk show partition sizes chosen eager lru algorithm graph performance fault occurs disks cases trace throughput age slow disk years lru-based algorithms lru landlord eager lru lazy lru cache throughput age slow disk years clock-based algorithms clock lazy clock cache figure web server workload figure shows performance lruand clock-based algorithms run file system trace web server providing art images results lru-based algorithms graph left side clock-based algorithms graph results change eager lru mask performance faults landlord groups disks similar performance characteristics correction algorithm detect severity heterogeneity making smaller corrections needed adaptive threshold real-world performance conclude experiments examination partitioned algorithms web workload web server received modest number requests trace shorter synthetic traces partitioned algorithms partially penalized shortness trace move partition sizes initial state similar experiments investigate aging single disk results shown figure expected cost-oblivious algorithms show sharp drop performance age slow disk increases lru performance falls peak performance age range realistic range falls aged years clock shows similar slightly dramatic decline expected adaptive algorithms show robust performance performance degrades lazy lru eager lru landlord lazy lru performs poorly eager lru landlord lazy lru poor interaction repartitioning algorithm devotes entire cache slow disk disks homogeneous eager lru distributes pages evenly pages gradually distribution disk ages lazy clock shows small decrease performance sharp drop slow disk years drop due significant change partition ratios lazy clock strongly favors slow disk age weak preference age performance landlord decreases age increases trace bimodal distribution request sizes due postgres table reads interleaved small web server reads writes introduce anomalous behavior based experiments shown due space constraints performance dynamic partitioning algorithms sensitive base correction amount experiment figure fixed base correction amount range slow disk ages base correction amount starts small increases disk ages performance improves matches base cost-oblivious algorithm homogeneous system adaptive base correction amount needed performance related work work cost-aware caching occurred web cache database communities web cache community extensively studied cost-aware caching addition document size included algorithms web caching work differs storage-aware caching ways performance wide area varies common storage systems web caching document caching differs fixed-size blocks storage systems finally web caching replacement cost web page strongly correlated replacement costs pages broadcast disks continuously deliver data clients asymmetric link broadcast schedule meet client client met broadcast schedule client cache strives manage cache contents mask non-ideal broadcast schedule knowledge broadcast schedule probability access cache manages contents algorithm generalizes lru storage-aware caching differs ways partitions cache 
pages device broadcast disks page device granularity track replacement costs broadcast disks assume infrequently changing broadcast schedule storage-aware caching react frequent workload device performance recently researchers studied allocation pages classes prefetching compiler-controlled memory management resizeable file buffer caches prefetching page allocation occurs applications hinted unhinted compiler-controlled building mpi multi-programming systems implicit information frederick wong andrea arpaci-dusseau david culler computer science division california berkeley fredwong culler berkeley computer systems laboratory stanford dusseau stanford abstract growing importance fast system area networks parallel community common message passing programs run multi-programming environments competing sequential parallel jobs distort global coordination communicating processes paper describe implementation mpi implicit information global coscheduling results show mpi program performance sensitive local scheduling variations integration implicit co-scheduling mpi runtime system achieves robust performance multi-programming environment compromising performance dedicated introduction emergence fast system area networks low-overhead communication interfaces common parallel mpi programs run cluster environments offer high performance communication multi-programming parallel program run time systems utilize independent operating systems schedule sequential processes interleaved parallel program core question addressed paper design mpi runtime system realistic applications good performance robust multi-programming studies shown shared address space programs tend sensitive scheduling cases perform co-scheduled fortunately work shown interesting programs co-ordinate scheduling implicitly making simple observations reacting spinning sleeping question answer mpi program performance sensitive multi-programming commonly believed message passing tolerate scheduling variations programming model inherently loosely coupled programs typically send large messages infrequently distributed systems pvm-style environments local scheduling variations communication events out-of-sync impact global schedule paper show mpi performance sensitive scheduling common intuition misplaced programs running times longer running sequence competing sequential applications similar slowdown result demonstrated nas parallel benchmarks fast robust mpi layer developed active messages large high-speed cluster show simple techniques implicit co-scheduling integrated mpi runtime library make application performance robust multi-programming loss dedicated performance paper organized briefly describe experimental environment initial mpi implementation section section examine sensitivity message passing programs multi-programming show influence global uncoordination application execution time section describes solution problem detailed description mpi implementation implicit co-scheduling application performance results discussed section background section briefly describes experimental environment initial implementation mpi standard serves basis study sensitivity multiprogramming experimental environment measurements paper performed berkeley cluster ultrasparc model workstations connected -port myrinet switches ultrasparc unified cache main memory nodes running solaris active messages firmware glunix parallel execution tool start processes cluster mpi-am mpi implementation based mpich implementation realizing abstract device interface active message operations approach achieves good performance portable active message platforms active messages active messages widely low-level communication abstraction closely resembles network transactions underlie modern parallel programming models constitute request response transactions form restricted remote procedure calls implementation active messages api supports multi-user multi-programming environment efficient user-level communication access implementation current implementation mpi eager protocol transport messages abstract device mpi message transferred medium request communication group rank tag message size adi specific information passed request arguments messages larger fragmented transferred multiple active messages destination buffer sender transfers message fragments destination process dynamically reorders copies message fragments buffer temporary buffer allocated receive posted mpi message considered delivered requests handled receiving node performance dedicated environment figure shows one-way bandwidth mpi-am dongarra echo test one-way bandwidth calculated reciprocal one-way message latency half average round-trip time start-up cost mpi-am raw performance maximum bandwidth achievable sec kink shows performance single medium request increase bandwidth message sizes larger due streaming multiple medium requests figure shows speedup nas benchmarks class data sizes dedicated processors cluster performance limited aggregate bandwidth benchmarks obtain perfect speedup benchmarks speedup slightly super-linear machine sizes due cache effects evaluate impact multi-program scheduling chose applications representing broad range communication computation patterns encounter real workloads fig figures show micro-benchmark application performance mpi-am dedicated environment figure shows one-way bandwidth mpi-am message sizes figure shows speedup nas benchmarks processors message size bytes one-way bandwidt processors speedup sensitivity multi-programming multi-programming environment local operating system scheduler responsible multiplexing processes physical resources execution time sequential process inversely proportional percentage cpu time allocated context switching overheads parallel applications spend additional communication waiting time message passing library parallel application running non-dedicated environment waiting time increase processes scheduled simultaneously section examine sensitivity parallel applications multi-programming investigate performance nas benchmarks multiple copies parallel jobs competing multiple sequential jobs competing parallel job figure shows slowdown benchmarks multiple copies program running rest experiments performed set workstations slowdown calculated dividing running time program multi-programming environment execution time program run sequentially dedicated environment slowdown workload jobs equal workload runs long single job dedicated environment significant drop performance slowdown copies competing execution slowdown shows performance nas benchmarks noticeably worse co-scheduled figure shows effect nas benchmarks competing sequential processes benchmark slowdown slowdown slowdown copies sequential job run competing parallel jobs performance benchmarks drops significantly actively sharing sequential jobs fig figures show slowdown nas benchmarks multi-programming environment figure shows slowdown copy dedicated copies -pp copies -pp benchmark running cluster figure shows slowdown benchmarks competing copy -seq copies -seq copies -seq sequential job slowdown dedicated -seq -seq -seq slowdown dedicated -pp -pp understand performance impact multi-programming environment parallel jobs profile execution time benchmark figure shows execution time breakdown nas benchmarks sharing copies sequential job expected time execution time benchmark dedicated environment context switch time overhead context switching processes cache penalty extra time spent memory hierarchy due cache misses caused processes waiting time additional time spent processes waiting communication events multi-programming environment context switching cost cache penalty substantial constitute percent execution time excessive slowdown explained tremendous increase waiting communication events percent execution time spent performing work percent time spent spin-waiting message sends receives shown figures parallel jobs highly sensitive global co-scheduling traditional solution problem mpps explicit gang scheduling operating system unattractive general-purpose clusters mixed workloads sections present elegant solution obtained modifying implementation mpi local information achieve global co-scheduling incorporating implicit co-scheduling idea implicit co-scheduling simple observations communication events coordinated processes running block execution un-coordinated processes release resources processes round-trip time message significantly higher expected dedicated environment sending process infer receiving process scheduled relinquishing processor sending process local processes proceed hand timely message response means sender scheduled receiving process remain scheduled fig figure shows execution time breakdown nas benchmarks copies sequential process running percent age runt ime waiting cache penalty context switch expected two-phase spin-blocking mechanism embodies ideas consists active spinning phase block-on-message phase active spinning phase sender actively polls network reply fixed amount time block-on-message phase caller blocks incoming message received order active processes co-scheduled amount time sender spin 
wait sum round-trip time cost process context switch hand receiver wait time one-way message latency perfect co-scheduling utilize ideas modify mpi-am runtime library incorporate two-phase spin-block mechanism spin-waiting occur places trivial two-phase spin-block sender waiting reply receiver confirm delivery message mpid complete send receiver waiting delivery message mpid complete recv spin-waiting occur posting receive mpid post recv mpi receive receive handle carries receive information posting receive application abstract device ensure network device received message checking unexpected receive queue match found receive handle posted expected receive queue message received network device unexpected handle returned large messages fragmented message middle transmission receiving process retrieves unexpected handle implementation spin waits entire message buffered two-phase spin-block mechanism needed relinquish processor sender delivering message fragments fast finally ensure layer message passing system spin wait active messages implementation request operation blocked due running flow control credits order avoid spin wait layer mpi-am outstanding requests counter two-phase spin-block counter reaches pre-defined flow control limit layer fig figure compares slowdown nas benchmarks implicit coscheduling copies benchmark figure shows execution time breakdown benchmarks copies sequential process time-shared benchmarks implicit co-scheduling slowdown implicit implicit entage runti waiting cache penalty context switch expected results extra complexity introduced implicit co-scheduling mpi library increases one-way latency small message echo test execution time single copy nas benchmarks essentially unchanged figure compares slowdown running copies nas benchmarks implicit co-scheduling significant improvement slowdown figure shows execution time breakdown benchmarks running copies sequential process waiting time reduced seconds figure seconds reduced seconds seconds completely eliminated two-phase spin-block mechanism effectively reduces spin-waiting time benchmarks substantially increasing context-switch overhead figure repeats study figure implicit co-scheduling demonstrate scalability workload sizes experience moderate increase slowdown multiple copies run increase occurs application perform frequent fine-grained communications hand experiences speedup copies run sends large messages performance limited aggregate bandwidth network processes spin wait messages drain network dedicated environment spends percent execution time spin waiting communication events multiple copies implicitly co-scheduled waiting process eventually blocks allowing processes continue total execution time processes reduced interleaving computation communication competing processes speedup achieved benefit interleaving outweighs cost multi-programming conclusion previous studies shown implicit co-scheduling building parallel fig figure shows slowdown nas benchmarks copy dedicated copies -pp copies -pp benchmark run slowdown dedicated -pp -pp applications fine-grain shared memory programming model study loosely coupled message passing programs highly sensitive multi-programming computationally intensive bulk synchronous programs application slowdown caused global uncoordination high times coordination processes waste percent execution time spin-waiting communication paper presented implementation mpi standard distributed multi-programming environment leveraging implicit information global co-scheduling effectively reduce communication waiting time message passing applications caused communication uncoordination performance message passing applications improved factor reducing application slowdown worst case studied case implicit co-scheduling helps coarse grain message passing application yield performance interleaving communication computation parallel processes acknowledgments shirley chiu brent chun richard martin alan mainwaring remzi arpaci-dusseau helpful comments discussions work research supported part darpa nsf cda doe asci djb california micro arpaci dusseau vahdat liu anderson patterson interaction parallel sequential workloads network workstations proceedings acm joint intr conf measurement modeling computer systems arpaci-dusseau culler mainwaring scheduling implicit information distributed systems acm sigmetrics performance bailey barszcz barton browning carter dagum fatoohi fineberg frederickson lasinski schreiber simon venkatakrishnan weeratunga nas parallel benchmarks intr supercomputer applications boden cohen felderman kulawik seitz seizovic myrinet gigabet-per-second local-area network ieee micro feb dongarra dunnigan message passing performance computers tennessee technical report cs- von eicken culler goldstein schauser active messages mechanism integrated communication computation proc isca memory management compiler application memory usage information operating system global replacement policies hints reintegrating elements local page replacement global page replacement finally nelson work resizeable file buffer caches evaluates tradeoff file buffer caches virtual memory system loaded work areas closely related work directly address storage device heterogeneity future work storage-aware caching increases storage system performance robustness adapting performance differences devices areas improvement application domain addition study real implementation partitioning algorithm presented significant limitations sophisticated informed cost-benefit algorithms limitations linear relationship assumption cache size hit rate reliance proper values window size base increment amount threshold limitation evident considers access patterns locality working set larger cache intuitively algorithm recognize instances increasing cache size decrease wait time general framework storageaware caching existing cost-oblivious policies manage individual partitions studied framework approach modularity primary strength existing non-cache aware policies lru clock mru eelru minimal effort work concentrated non-cooperative client caching combination cooperative caching cost-aware caching lead performance robustness disk arrays individual cache sizes small fourth storage-aware caching applied lowpower environments storage-aware caching extended include power retrieval cost devices higher power accessed frequently stay low-power mode longer frequently finally caching algorithms affected prefetching layout decisions explore advantages tradeoffs integrated prefetching layout caching decisions light device heterogeneity previous caching prefetching work homogeneous environments shown benefits integration benefit extends heterogenous environments conclusions diverse characteristics modern storage devices time ripe re-investigate caching algorithms optimize performance task costaware cache control blocks cached amount work performed storage device roughly equal paper presented family cost-aware caching algorithms based notion explicitly partitioning cache size partition configured directly corresponds relative cost usefulness data partition approaches advantages partitions aggregate replacement-cost information entries cache reducing amount information tracked allowing recent cost information blocks device important virtual partition approach easily implemented clock replacement policy increasing likelihood adoption real systems acknowledgements members wisconsin network disks research group helpful discussions contributions paper comments florentina popovici provided ibm lzx disk profile information simulations leslie cheung wrote layout code berkeley trace experiments omer zaki contributed early versions work comments anonymous reviewers tireless shepherding jeff chase greatly improved quality paper condor distributed execution system run simulations members condor project todd tannenbaum supported condor work sponsored nsf ccrccr- itrand wisconsin alumni research foundation acharya alonso franklin zdonik broadcast disks data management asymmetric communications environments proceedings acm sigmod international conference management data san jose pages acm press acharya franklin zdonik disseminationbased data delivery broadcast disks ieee personal communications december alexandrov ionescu schauser scheiman loggp incorporating long messages logp model step closer realistic model parallel computation papers annual acm symposium parallel algorithms architectures santa barbara june pages acm press apple computer corporation idisk http itools mac arpaci-dusseau arpaci-dusseau bent forney muthukrishnan popovici zaki manageable storage adaptation wind proceedings ieee int symposium cluster computing grid ccgrid pages arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance workshop hot topics operating systems hotos schloss elmau germany babaoglu joy converting swap-based system paging architecture lacking page-referenced bits proceedings acm symposium operating system principles pages pacific grove december bertoni understanding solaris filesystems paging technical report tr- sun microsystems cao felten karlin implementation performance integrated application-controlled file caching prefetching disk scheduling acm transactions computer systems november cao irani cost-aware proxy caching algorithms usenix symposium internet technologies systems proceedings monterey california december pages berkeley usa carley ganger nagle mems-based integrated-circuit mass-storage systems communications acm november chrobak karloff payne vishwanathan results server problems proceedings annual acmsiam symposium discrete algorithms soda pages san francisco usa jan cortes labarta extending heterogeneity raid level usenix annual technical conference june demke brown mowry taming memory hogs compiler-inserted releases manage physical memory intelligently proceedings symposium operating systems design implementation ghormley petrou rodrigues vahdat anderson glunix global layer unix network workstations software practice experience gropp lusk doss skjellum high-performance portable implementation mpi message passing interface standard parallel computing sept mainwaring active message application programming interface communication subsystem organization california berkeley ucb csd- 
osdipages berkeley october grochowski ibm leadership disk stroage technology ibm corporation howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems february jin bestavros popularity-aware greedydual-size algorithms web access proceedings international conference distributed computing systems icdcs april kelly chan jamin mackie-mason biased replacement policies web caches differential quality-ofservice aggregate user fourth international web caching workshop san diego california march april kubiatowicz bindel eaton chen geels gummadi rhea weimer wells weatherspoon zhao oceanstore architecture global-scale persistent storage proceedings ninth international conference architectural support programming languages operating systems asplos november manasse mcgeoch sleator competitive algorithms on-line problems proceedings twentieth annual acm symposium theory computing chicago illinois pages york usa nelson welch ousterhout caching sprite network file system acm transactions computer systems february nelson virtual memory file system technical report compaq computer corporation nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference patterson gibson ginting stodolsky zelenka informed prefetching caching proceedings fifteenth acm symposium operating systems principles pages copper mountain december acm press pro softnet corporation ibackup http ibackup rashid tevanian young golub baron black bolosky chew machine-independent virtual memory management paged uniprocessor multiprocessor architectures proceedings international conference architectural support programming languages operating systems asplos pages palo alto october association computing machinery ieee rizzo vicisano replacement policies proxy cache ieee acm transactions networking roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference usenixpages berkeley june ruemmler wilkes introduction disk drive modeling ieee computer march sandberg design implementation sun network file system proceedings usenix summer conference pages berkeley usa june usenix association sanford greier yang olyha narayan hoffnagle alt melcher one-megapixel reflective spatial light modulator system holographic storage ibm journal research development smaragdakis kaplan wilson eelru simple effective adaptive page replacement proceedings acm sigmetrics international conference measurement modeling computing systems sigmetricsvolume sigmetrics performance evaluation review pages york acm press sullivan seltzer isolation flexibility resource management framework central servers proceedings usenix annual technical conference san diego california infiniband trade association http infinibandta june tomkins patterson gibson informed multiprocess prefetching caching proceedings acm sigmetrics conference measurement modeling computer systems pages acm press june vahdat anderson dahlin belani culler eastham yoshikawa webos operating system services wide area applications proceedings seventh symposium 
high performance distributed computing july waldspurger weihl lottery scheduling flexible proportional-share resource management proceedings usenix symposium operating systems design implementation nov wooster abrams proxy caching estimates page load delays proceedings international conference april young k-server dual loose competitiveness paging algorithmica june young on-line file caching proceedings ninth annual acm-siam symposium discrete algorithms balitmore january pages acm press 
datamation sorting odyssey florentina popovici john bent brian forney andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin-madison abstract present experience turning linux cluster high-performance parallel sorting system implementation wind-sort broke datamation record roughly factor sorting million -byte records seconds identi keys success developing fast remote execution service con guring cluster properly avoiding potential ill-effects occasionally faulty hardware introduction datamation introduced sorting mbyte data time consuming endeavor advances computing power software algorithms reduced time benchmark orders magnitude figure considerably altering nature datamation sort originally intended memory-intensive benchmark datamation considered benchmark interactive performance datamation importance database benchmark diminished benchmark remains relevant reasons datamation stresses importance start-up time start-up time noted factors limiting performance parallel systems problem clusters smps datamation interactive parallel application parallelism commonplace interactive applications reality interactive parallel technical report describes work completed spring time seconds year datamation results past years tsukerman weinberger nyberg arpaci-dusseau buonadonna azuma popovici bent forney figure years datamation jobs sensitive performance uctuations occur large-scale systems datamation sort runs consistently platform conclusions drawn ability platform avoid tolerate presence perturbations tuning sort linux-based cluster-ofpcs platform found elements crucial achieving consistent high-performance developed lean parallel remote execution layer order minimize start-up time con gured linux properly enabling scsi tagged command queueing altering overly-pessimistic arp caching scheme increasing socket buffer sizes avoid messagebuffer ows cases default values desirable led performance problems unexpected dif cult discover altered communication layer aggresively resend packets order overcome occasionally faulty network switch module proactive re-sends important jobs run extremely short periods time remainder paper organized section discuss base sorting code section hardware supporting software efforts understand optimize performance startup disk network section integration efforts record breaking sort occur section conclusions section background section describes sort algorithm implementation derived now-sort assumes input records evenly distributed nodes numbered sorted output range-partitioned node lowest-valued keys node highest-valued keys node number records node approximately equal depends exact distribution key values input les algorithm consists steps rst step nodes opens input reads xed-sized buffers buffer read copy record send-buffer allocated destination node now-sort destination node determined top somany bits key approach assumes key values uniformly distributed distribution phase node sends records read node sort records range-partition receiver site keys separated records buckets based highorder bits key pointer full byte record stored memory step ends synchronization nodes ensure records received sort step node performs in-core sort records conventional quicksort algorithm sorts data nal phase node sorted keys pointers gather records contiguous buffer sorted keys accompanying records written local concatenated output les nodes represent sorted version concatenated input les main differences now-sort fall-outs emphasis datamation sort now-sort sorts previously overlap read distribution phases discovered due small amount data node bene overlapping steps algorithm cost starting extra threads render technique undesirable small amounts data utilize quicksort highly specialized partial radix bubble sort hybrid differences algorithms small data sizes negligable chose conceptually simpler approach environment hardware wind-sort runs node linux cluster node intel mhz pentium iii processors mbyte memory ibm lzx disks disks con ured ultra scsi buses rated mbyte disks including operating system root disk bus remaining connected bus wind-sort data les reside non-root disks nodes networked mbit gigabit ethernet gigabit ethernet private network cluster mbit network job launching external system link departmental network gigabit ethernet cards intel pro server adapters -port intel netstructure gigabit ethernet switch connects gigabit nics system software cluster system software primarily commodity-based additions research community development job launching software node cluster runs linux ext system linux raid software jobs started parallel software detail section sorting code depends pieces research software split-c udp-based implementation active messages split-c parallel extension supports cient access global address space distributed memory machines communication split-c active messages restricted lightweight version remote procedure calls active messages cluster layered udp implementation cient previous highly-tuned timplementations startup cost launch time parallel program cluster add considerable overhead program run time found job launch packages impose considerable overhead rexec ssh common packages initially considered launching null job exits immediately remote node rexec ssh measurements shown found rexec ssh scale jobs launched numbers reveal high performance launcher sort developed special launcher called ice interactive cluster execution ice daemon runs node cluster responsible receiving messages clients spawning jobs response launch parallel job user simply type ice command contacts ice daemon speci node nodes requesting launch job request includes environment variables ice command response request ice daemon creates default connections back requesting ice command redirect program output control program creating connections ice daemon forks process waits completion job completion exit status remote job back spawning node addition basic operation ice optimizations program output connection turned creation connection expensive tcp wind-sort optimization debugging completed control connection udp default tcp finally ice daemon cache environment variables hostname translations caching scheme works static environment results optimizations shown figure ice optimizations minimizes launch time wind-sort node cluster time spawn nodes ice scalability control tcp data tcp hname env control udp data tcp hname env control udp data hname env control udp data hname env control udp data tcp hname env control udp data hname env control udp data hname env local fork exec performance figure ice scaling comparison graph plots start-up performance ice remote execution layer line parameters control protocol tcp udp data protocol tcp program output hostname cache environment cache numbers parentheses parameters bestt line data points disk understand disk cluster gathered disk performance data range sizes data gathered software raid stripes disks reads performed byte chunks writes performed byte increments sizes chosen coding convience determining linux insensitive chunk sizes system caches ushed measurement results shown figure reads figure writes gures show data mbyte data area interest datamation node cluster made observations data collected read bandwidth disks reaches aggregate rated performance mbyte small amount transfered data figure show due software hardware overheads measurements shown found read bandwidth peaks mbyte amount data read exceeds mbyte observed addition disks increases performance small amount data transfered wind-sort approximately mbyte node concerned adding disks reduce performance performance decrease due overhead striping due variations perforaggregate disk bandwidth aggregate data transfered disk read scaling read disks disks disks disk figure disk 
read performance scaling graph shows disk performance scales data transfered number disk read performance scales point represents average trials mance characteristics disks due seemingly minor factor location disk arm read begins case wind-sort small transfers figures amount data transfered mbyte terms writes found write performance disks lower expected manufacturer rates disks mbyte sustained transfers outer tracks disks originally delivered mbyte bandwidth attempts made improve performance enabling scsi tagged command queuing scsi driver tagged command queuing outstanding scsi request device request scheduling performance increased single disk mbyte mbyte figure tagged command queuing enabled tagged command queuing enabled write performance scale disks added write bandwidth disks mbyte user-level striping show signi differences performance kernel-level raid software additionally efforts determine disk write limitation due kernel structure policies revealed unlike now-sort disk performance running netbsd cluster node produced results common components netbsd linux run cluster core scsi driver hardware suspicion limitation due components aggregate disk bandwidth aggregate data transfered disk write scaling write disks disks disks disk figure disk write performance scaling write performance scsi tagged command queuing enabled shown graph scales half expected bandwidth mbyte point represents average trials improved implicit wind-sort explicit system calls performing implicit measurement found aggregate read performance sustained half bandwidth linux implementation welltuned unix implementations lacks systems provide hints virtual memory system trigger page prefetching network distribution time largest component sort times ideal case nodeto-node transfer udp network delivered mbyte shown figure all-toall communication occurs udpam-based performance test network delivered roughly half rate per-node nodeto-node all-to-all theoretically mbyte peak bandwidth discovered bottleneck receiver overhead receiver utilizes processor completely sending processors idle figure shows scaling sort all-to-all distribution phase lowest trials point shown gure gure shows cluster gigabit ethernet nics support jumbo frames jumbo frames improve performance cluster network bandwidth message size bytes one-way udp bandwidth nodes figure one-way udp performance graph shows achieved bandwidth sending messages node cluster size udp message increases performance reaches mbyte signi cantly theoretical peak bandwidth mbyte margin dimishing returns number utilized nodes grows amount data transferred decreases xed overhead dominates distribution time uenced factors gigabit switch faulty module observed drop packets ocassionally heavy load overcome problem proactive retransmits transport protocol udpam insures reliable delivery timeout thread resends dropped packets thread checks regular time intervals packets acknowledged receiver resends packets reliable found waiting timeouts aggressive provide good distribution time reason added function udpam library resend immediately unacknowledged packets retransmit packets proactively distribute phase sort complete figure shows difference proactive retransmits default udpam reactive technique timeouts proactive sends reduced average sort time sort scales one-eight reactive time nodes factor originally adverse affect communication time arp address resolution protocol traf default arp timeout values caused arp traf occasionally ood average sort time seconds number machines effect proactive retransmissions reactive proactive figure proactive retransmissions graph shows proactive retransmissions constrain variability imposed faulty hardware reactive strategy dropped packets requested timeout expires incurs greater penalty average trials shown network order renew cached entries found seemingly minor disruption signi impact sort performance due sensitivity switch static nature environment frequent arp traf unnecessary turned resulting performance variation tighter distribution sort times finally avoid dropped packets lack buffer space kernel increased size kernel network receive buffer accommodate amount data received node putting developing fast remote execution layer conguring cluster properly disk network performance developing simple mechanisms deal occasionally faulty hardware transform cluster highperformance parallel sorting system rate scale decreases nodes added post minimal gains limits cluster capacity nodes eventually carefully tuning disks networks patience set record seconds datamation challenge figure shows phase sort scales nodes utilized graph undertime seconds number machines sort phases write phase sort phase distribute phase read storage-aware caching revisiting caching heterogeneous systems brian forney andrea arpaci-duseau rmzi arpaci-duseau wisconsin network disk wiscnsin madison circa ospolicy intense rsearch period policies wide variety developed usd today exmples clock lru simple torage environment focus workload asmption consistent retrieval cost bufer cache appappapp today rich storage environmnt deices attached ways dvice incrase dvice sophistication mismatch revaluate lan wan policybufer cache appappapp problem illustration unifor workload disks lru policy slow disk bottleneck problem plicy oblivious filter fastslow genral solution integrate workload device prformnce balance work devices work cumulative delay canot hrow existing non-cost aware policy research xisting cahing software solution overiew generic partitioning framework idea n-to-ne maping device partition partition cost-oblivious policy adjust prtitio size dvantages agregates performance information easily quickly dpts workload device prforance integrates existin software key pick partition sizes outline motivation slutio overiew taxonmy dyic partitioning algorithm evalution sumary partioning algorithms staic pro simple con wasteful dynamic adapts workload hotspots ace patern handles devic performance faults dynamic algorithm overiew observ detrine pr-device cumlati dlay reprtion che sav reset clear lst request observ act save reset algorithm observe acurate sytem balnce view record per-dvic ultiv dlay completed disk request client inludes etwork time algorithm act categorize partition cnsumrs cumulative delay threshold bottleneck page supliers cumulative delay lose pages decreasing performance page supliers ther pge conumr page consumerpage supplierneither page consuerpage supplierneither page consumers pages depnds stae warming cumlative dlay increasing agresivly pag reduce queing warm cumlative dlay constant onservativly pge cooling umlative dlay decrasing nturlly decrase dynamic partitioning eager immediately change partition sizes pro matches observation som pag temporaily unsed lazy change partition sizes demand pro easier implement con ovr correction outline motivation slutio overiew taxonmy dyic partitioning algorithm evalution sumary evaluation methodology simulator workloads synthetic web caching raidclient loggp network endpoint contetion disks ibm lzx first-order model queuing seek time rotational time evaluation methodology introduced performance hterogenity disk agin current technology trends sek rotation decrease yar bandwidth increase year scenarios single disk degradtion single disk multiple ages incremental upgrades multiple diss aes fault injection understand dynamic device performance change device sharing effects talk shows single disk degradation evaluated policies cost-oblivious lru clock storage-aware eager lru lazy lru lazy clock clock-lotry comparisn landord st-ware on-partitioned lru cahing algorithm integration prolems dern oses synthetic workload read requests exponetially distributed uniform acros disk single slow disk greatly impacts performance eager lru lazy lru lazy clock landlord robust slow disk performance degrades lru-based age slow disk years throughput eager lru landlord lazy lru lru cache clock-based age slow disk years throughput clock lazy clock cache web workload day image server trace berkeley reads writes eager lru landlord robust age slow disk years throughput eager lru lazy lru age slow disk years throughput eager lru landlord lazy lru lru cache sumary problem mismatch storage environent cahe policy current buffer cache policies lack device information policies include storage environmnt information solution generic partitioning framework agregates performance information dapts quickly existing policies questions information wisc wind future work implentaion linux cost-befit lgrithm study intration preftching laut problems landlord mesh unified bufr cach asumes lru lru-bsed lway desirable database sufers memory efect uch slower adapt disk aging ibm lzx avg rotational delay avg seek time bandwidth age years web writes workload web workload writes replaced reads eager lru landlord robust age slow disk years throughput eager lru lazy lru age slow disk years throughput eager lru landlord lazy lru lru cache problem illustration aplications fastslow disks problem illustration aplications fastslow disks lack information bufer cache aplications disks dfses nas dvices drivers solution overiew partition cahe device cost-oblivious policies lru partitions aggregate device perf dynamically rellocte pges bufer cachepolicy forms dynamic partioning eager change sizes tomorrow devices pardigm incresingly rich storae envirnmnt misatch revlution nde lan wan bufer cachepolicy wind project isconsi network disk building mangable distributed store foc local-ra netwrked storge isus imilar wid-are 
phase start phase figure sort scaling graph shows performance sort phases increasing number machines point time observed run thirty iterations lines drawn stacked lowest trials plotted point shown scores disproportionate amount time sort program spends distribution phase operation validation validate correctness sort developed additional tools noticed additionally aspects correctness size sequencing validity correct sort sorted correctly key corrupting values verify check program collect distributed input les machine concatenates les veri validity size checksum -byte record sequencing checked ensuring key previous concatenated output stress importance regularly checking factors writing sort programs found likliest opportunity bugs enter code disabled validation phase sort sort results presented ated overhead incurred gathering timing data sort phases timing phases added extra global communication compute maximum time spent speci phase nodes conclusion presented experiences tuning cluster optimize performance datamation benchmark datamation benchmark changed bit time tool measuring understanding system performance diminished stresses aspects systems sorting benchmarks careful engineering remote execution layer proper con guration simple techniques overcome hardware misbehaviors commonplace large-scale clusters demonstrated linux-based clusters potential basis interactive parallelism years acknowledgements computer systems lab paul beebe david parter keeping cluster running david dewitt encouraging start project motivating nish finally jim gray excellent sustained encouragement advice anonymous measure transaction processing power datamation readings database systems stonebraker morgan kaufmann san mateo andrea arpaci-dusseau remzi arpacidusseau david culler joseph hellerstein david patterson high-performance sorting networks workstations proceedings acm sigmod conference pages remzi arpaci-dusseau andrea arpaci-dusseau fail-stutter fault tolerance workshop hot topics operating systems hotos-viii brent chun udpam active messages udp network workstations project retreat january http berkeley retreat winter udpamslides ibm corporation ultrastar lzx hardware functional speci cation december david culler andrea dusseau seth copen goldstein arvind krishnamurthy steven lumetta thorsten von eicken katherine yelick parallel programming split-c proceedings supercomputing pages david dewitt jim gray parallel database systems future high performance database systems communications acm chris nyberg tom barclay zarka cvetanovic jim gray dave lomet alphasort risc machine sort proceedings acm sigmod conference thorsten von eicken david culler seth copen goldstein klaus erik schauser active messages mechanism integrated communication computation proceedings international symposium computer architecture gold coast australia 
storage-aware caching revisiting caching heterogeneous storage systems brian forney andrea arpaci-dusseau remzi arpaci-dusseau computer sciences department wisconsin madison bforney dusseau remzi wisc abstract modern storage environments composed variety devices performance characteristics paper explore storage-aware caching algorithms file buffer replacement algorithm explicitly accounts differences performance devices introduce family storageaware caching algorithms partition cache partition device algorithms set partition sizes dynamically balance work devices simulation show storageaware policies perform similarly landlord costaware algorithm previously shown perform web caching environments demonstrate partitions easily incorporated clock replacement algorithm increasing likelihood deploying storage-aware algorithms modern operating systems introduction modern computer systems interact broad diverse set storage devices including local disks remote file servers nfs afs archival storage tapes read-only media compact discs dvds storage sites accessible internet storage components introduced behaviors properties divergent today set devices disparate commonality pervades time access high compared cpu cache memory latencies due cost fetching blocks storage media caching blocks main memory reduces execution time individual applications increases system performance orders magnitude storage technology dramatically changed past decades important aspects caching architectures modern operating systems remained unchanged innovations mechanism including integration file cache virtual memory page cache copyon-write techniques software emulation bits change policy operating systems employing lru lru-like algorithms decide block replace problem lru related caching algorithms cost-oblivious blocks treated fetched identically performing devices re-fetched replacement cost blocks assumption increasingly problematic manifold device types correspondingly rich set performance characteristics simple block fetched local disk compared fetched remote highly contended file server case operating system prefer block file server heterogeneous environments file systems require caching algorithms aware replacement costs file blocks slowest device roughly determines throughput system storage-aware caching seeks balance work devices adjusting stream block requests heterogeneous environment storage-aware cache considers workload behavior device characteristics filter requests paper explore integration costaware algorithms operating system page cache simulation simulation accounts real-world factors integrated page cache simplicity design build previous work cost-aware caching web-cache theory communities demonstrating separate set partitioned algorithms effective simpler proposals research areas study set context network-attached disk system network-attached disks increasingly important storage paradigm present clients static dynamic forms performance heterogeneity algorithms develop general applied broader range storage devices main results show storageaware caching significantly performance robust cost-oblivious caching robust leading web-caching algorithm operating systems specific implementation develop evaluate version storage-aware caching extends commonly implemented clock algorithm rest paper organized section give overview algorithms investigate paper describe algorithm selecting partition sizes section section describes assumptions environment detail explains simulation framework simulation results section compare contrast work existing work section section future work finally conclude section algorithm overview section overview algorithmic space explore describe existing cost-aware algorithms basis comparison present caching algorithms based partitioning cache replacement cost existing cost-aware algorithms theoretical community studied cost-aware algorithms k-server problems restricted class k-server problems weighted caching closely related cost-aware caching landlord significant algorithm literature comparison landlord closely related leading web caching algorithm landlord combines replacement cost cache object size locality extending lru fifo include cost variable cache object sizes cache configured landlord lru describe lru version landlord associates cost object called object enters cache landlord sets retrieval cost object divided size object object eviction needed landlord finds object lowest removes ages remaining objects landlord ages pages decrementing remaining objects evicted object cache landlord restores object landlord degenerates strict lru values landlord attractive theoretical experimental properties shown theoretical analysis size cache objects landlord k-competitive size cache fixed object size cache landlord performs factor optimal off-line algorithm request sequences overview aggregate partitioned algorithms cost-aware algorithms literature place-anywhere place-anywhere algorithm characteristics blocks occupy logical location cache independent original source cost costs recorded page granularity advantage place-anywhere algorithms calculate single trade-off locality cost replacement time algorithms bias eviction pages low retrieval cost contrast place-anywhere algorithm aggregate partitioned algorithm divides cache logical partitions blocks logical partition device share replacement cost algorithm aggregates replacement cost function device performance aggregate partitioned algorithm benefits aggregation blocks cost metadata ways amount metadata reduced metadata closely reflects current replacement cost block device space overhead proportional number devices blocks replaced replacement cost low conversely place-anywhere algorithms record cost page brought cache cache large number pages common today place-anywhere algorithm susceptible inconsistent cost values aggregate partitioned algorithms avoid problem aggregating cost metadata per-device basis performance device cost metadata rarely inconsistent period time placeanywhere algorithm recognize change cost device propagate cost pages cache cost update requires significant number pages updated increasing overhead implementation complexity aggregate partitioned algorithms strive set relative size partition balance work devices define work balanced cumulative delay device period time equal balance work size partition reflects relative cost blocks simple efficient manner storage system slow disk fast disk cache divided partitions slow disk receiving larger partition describe precisely relative sizes configured section choose victim block storage-aware algorithm selects victim partition victim block partition victim partition chosen resulting size relative partitions maintains desired proportions individual victim partition selected replacement algorithm lru lfu fifo distinctions prior work virtual memory systems noted unified partitioned virtual memory systems traditional sense partitioned virtual memory systems distinguish file system pages virtual memory pages managed separately storage-aware algorithms explicitly distinguish file system pages virtual memory pages order balance work algorithms distinguish pages based device supplied page additionally storage-aware caching algorithms change size partitions dynamically partitioned virtual memory systems change size file system cache virtual memory partitions local global page replacement local page replacement eviction time considers processes isolation global page replacement applies replacement processes storage-aware algorithms make per-partition replacement decisions similar traditional notion local page replacement decisions based cost locality solely locality local page replacement schemes taxonomy aggregate partitioned algorithms work investigate taxonomy aggregate partitioned algorithms show dynamic aggregate partitioning needed taxonomy subsection basic approaches aggregate partitioning static dynamic static scheme ratio partitions selected one-time notion costs knowledge workload resulting miss rates cache size priori determine relative sizes lead balanced work dynamic partitioning needed ratio partition sizes adjusts requests monitored dynamic partitioning benefits dynamic partitioning adjust dynamic performance variations faults common modern devices dynamic partitioning react contention devices due hotspots workloads finally dynamic partitioning compensate fact performance ratios devices change function access patterns dynamic partitioning divided eager partitioning 
lazy partitioning eager partitioning partition sizes desired algorithm immediately reallocates pages cost information algorithm lazy partitioning scheme gradually reallocates pages demand desired size response workload eager partitioning simplifies choosing victim partition location page cost removing pages conversely lazy partitioning scheme removes pages partitions needed partition lazy partitioning block replace block long blocks replaced proper frequency maintain desired partition size ratios replacement explicitly choose victim partition investigate strategy based inverse lottery previously proposed resource allocation idea partition number tickets inverse proportion desired size replacement needed lottery held selecting random ticket partition holding ticket picked victim victim valuable page lazy partitioning algorithm allocates page logical partition selecting partition sizes main challenge partitioned approaches determining relative sizes partitions configured storage-aware caching viewed performing selective filtering requests devices assuming slowest device limits system throughput goal storage-aware caching set partition sizes equal amount work device formally device number cache misses multiplied average cost miss equal algorithmic details basic approach dynamic repartitioning algorithm algorithm storage-aware cache observes amount work performed device fixed interval past predicts relative sizes partitions adjusted work equal algorithm work metric cumulative delay period time delay related total number requests includes request service time variation device devices algorithm streaming accesses fit cache problematic algorithm detect type access algorithm measures time spent waiting device past device requests window size records wait time device periodically relative wait times device computed wait time device relative wait times approximately equal current partition ratios deemed adequate remain relative wait times size partitions large relative wait times increased size partitions small wait times decreased ratio wait times devices considered simultaneously selecting amount increment decrement partition non-trivial search problem change partition size affects future miss rate presence dynamically changing workloads initial approach employ simplest algorithm found meet challenge find algorithm adjusts partition sizes quickly find proportions quickly algorithm over-corrects meet goals simultaneously approach aggressively increases size partition relative wait time device increasing reacts conservative manner algorithm makes observations wait time device epoch action based observation epoch begins device requests complete epochs cache repartitioned repartitioning occurs steps algorithm computes per-device wait time wait time devices epoch algorithm computes relative wait time partition dividing per-device wait time wait time algorithm determines partitions page consumers pages give consumer page consumers partitions relative wait time threshold threshold filter normal variations relative wait time due workload device characteristics page consumers found repartitioning stops epoch begins finally algorithm finds partitions below-average relative wait times called page suppliers reallocates pages consumers consumers reach desired size figure corrective actions repartitioning algorithm figure shows actions algorithm response states graph shows observation per-device relative wait time trend relative wait time time progresses dotted line shows wait time graph graphs actions state shown fixed clarity threshold constant multiplied wait time repartitioning cache algorithm classifies partition states corrective action change partition size states shown figure cool relative wait time threshold relative wait time normal operating regime corrective action needed cool partitions page suppliers page consumers warming relative wait time threshold increasing algorithm infers increasing relative wait time due workload device characteristics initially cache size increased pages base correction amount partition continues warm subsequent epochs increase cache size grows exponentially reclassification partition warming state restarts exponential correction cooling relative wait time threshold decreasing corrective action set epochs halted increase relative wait time partition started decline relative wait time algorithm acts conservatively cooling state change partition size aggressive approach continues increase cache size state over-correct unstable warm relative wait time threshold constant based experimental evidence partitions classified cool warming cooling constant relative wait time occur partition moves state small change partition size algorithm increases partition size step algorithm reallocates pages page suppliers page consumers algorithm biases collection pages partitions lowest relative wait times determine number pages removed page supplier algorithm computes inverse relative wait time irwt relative wait time partition sums inverse relative wait times finally number pages partition supply computed irwtj sum irwtsforsuppliers consumed pages note parameters algorithm window size threshold base increment amount set care large smooth relative wait time variations sample sufficient number requests determine accurately effect corrections found sufficient smoothing feedback small exponential correction found cache works practice threshold large filter normal device performance fluctuation seek time found detects relative wait time warrant correction fixed algorithm compute threshold dynamically statistical variance relative wait times sum variance threshold discuss adaptive approach plan investigate future modifying existing replacement algorithms desire cost-aware cache performs easily implemented modern operating systems attention paid make computationally efficient landlord priority queue efficiently find lowest cost object mesh common virtual memory hardware easy combine landlord existing code base modern operating systems including solaris variant clock page replacement policy unified page cache desire algorithm incorporated easily clock structure introduce extension clock takes partitions account partitioned-clock base algorithm partitioned-clock assumes page bit set page referenced victim page needed clock arm successive pages bit set clearing bits sweeps partitioned-clock page tracks partition number belongs page selected victim bit cleared partition number match partition number chosen replacement chosen lottery note additional bits single bit consistent variations clock examine dirty bits history multiple bits optimizations improve performance partitioned-clock approximation lazy partitions searching replacement pages belonging victim partition bits cleared clearing bits pages unnecessarily removes usage history separate clock hand partition improves performance helps maintain usage history partition previously lazy partitions simpler implement eager partitions ratio sizes dynamic focus clock algorithm applied lazy partitions version termed lazy clock inverse lottery scheduling pick victims partitions provide comparison algorithm eager clock combines eager partitions clock algorithm complex evaluation environment section describes methodology evaluating storage-aware caching specifically overview simulator describes simulated storage environments simulator developed trace-driven storage-system simulator study behavior storage-aware caching configured simulated environment single client connected network-attached storage devices simulator explore performance impact client workloads data layout caching algorithms network characteristics disk characteristics storage-system heterogeneity simulator driven workload client trace file trace file represents data block requests striped raidacross full set disks request specifies starting offset length data read write simulate system high demand closed workload model completion disk request immediately triggers request client local cache replacement policies focus investigation model trace trace trace request 
distribution uniform exponential exponential disk distribution uniform uniform gaussian locality random random random request size working set size number requests table characteristics synthetic traces table summarizes synthetic traces set experiments gaussian distribution disk standard deviation time cache hit small real system dwarfed cost remote-block access time cache miss sum network transit time remote disk service time storage device model roughly matches ruemmler wilkes model cylinders seek time rotational delay bandwidth calculating transfer time request specifically disk request falls cylinder previous request model sequential seek rotational delay set transfer time determined bandwidth nonsequential requests rotational delay chosen uniformly random full rotation time seek time non-linear model depends cylinder distance current request previous request network model based loggp endpoint contention loggp designed model communication large parallel computers depends parameters message latency network endpoint overhead minimum time message sends seconds byte network number endpoints workloads fully understand impact storage-aware caching algorithms study sets workloads variety synthetic traces web server trace collected analyzed roselli simply refer roselli web server trace roselli trace synthetic workloads control request size distributions working set size locality distributions distribution request disks synthetic traces summarized table traces read traces variety request sizes stress small large read requests trace adds request imbalance disks roselli trace image server univerage bandwidth avg seek avg rotation years table aging ibm lzx model bandwidth seek rotation time family disks based ibm lzx manufactured progressively older years assume bandwidth improves factor year seek rotation time year sity california berkeley january image server ran web server postgres database stored images trace alternates large reads files database tables small reads writes storage-system characteristics goals configuring set disks simulated environment goal understand full sensitivity storage-aware caching algorithms device heterogeneity requires diverse range configurations goal understand algorithms perform realistic scenarios requires focused set tests meet goals simultaneously employ device aging performance-fault injection idea device aging choose base device case ibm lzx age performance range years collections disks create configurations key performance base disk scaled fixed amount component bandwidth seek rotation time scaled expected yearly improvement historical data suggests year improvement bandwidth roughly year reduction seek time rotational latency realistic aggressive side table shows performance characteristics aged devices experiments note progressively older disks backwards aging newer disks based current year similar manner forward aging throughput age slow disk years lru clock cache throughput number slow disks lru clock cache aging single disk increasing number slow disks figure performance lru clock caching figures show throughput storage system trace graph varies age single disk x-axis graph increases number year disks system performance-fault injection dynamically change performance drive experiment earlier represent disk stuttering absolute failure unexpected network traffic client drive sudden workload imbalance environment configuration section describes details simulator configuration configure network bottleneck system choose parameters similar ethernet set bandwidth future hope investigate network performance caching interact distributed storage systems configured simulator separately synthetic roselli traces synthetic traces choose sufficient number requests mitigate effects cold-start misses set client cache size roselli trace set cache size hit rate hit rate high requests disks heterogeneity device performance issue traces include disk layout file path information created simple layout policy layout policy assumes raidstriping policy lays blocks order access aged disks scenarios scenarios represent cases storage-system incrementally updated newer faster devices added time scenario single heterogeneous disk performance aged entire range years scenario groups heterogeneous disks group age age years relative size groups varied scenarios cover real-world situations provide insight common configurations scenario mimics stuttering disks increased workloads clients scenario closely incremental upgrades disk array incremental upgrades occur due cost constraints prohibit replacement entire array small number disks fail experiments section presents progression experiments demonstrating effectiveness storage-aware caching begin motivating cost-aware caching algorithms heterogeneous devices show partitioned approaches mask performance differences configuring fixed partition ratios correctly difficult static environment demonstrate mask performance heterogeneity adjusting ratio partition sizes on-line observations amount work performed device finally show partitioned approaches easily incorporated operating system replacement policies perform explore performance robustness trace web server motivating set experiments motivates storage-aware caching algorithms storage system heterogeneous devices figure shows throughput obtained trace common replacement policies cost-aware lru clock caching figure illustrates throughput age slow disk years static lru cache figure potential partitioned approaches slow disk aged shown x-axis approaches cache lru caching static partitioning cache disk performance trace figure disks aged seek rotation time bandwidth decline throughput system drops dramatically performance benefit file cache decreases lru replacement throughput drops disks equally fast single disk performance -year disk similarly figure shows entire storage system runs rate slowest disk system throughput slow disk fast disks poor slow disks contrast storage-aware caching algorithm mask performance slow disks allocating cache slow disks slow disk fewer requests handle harm performance system dramatically configuring partition sizes set experiments show partitioned caching algorithms potential mask heterogeneous performance care selecting ratio partition sizes begin examining static partitioning algorithm simply named static figure show performance static trace experiments ratio partition sizes statically set directly proportional ratio expected service time disk trace request size directly compute expected transfer time disk function seek time rotation delay peak bandwidth graph static partition strategy significantly improves performance relative cost-oblivious algorithms lru static performs priori request size per-disk miss rate function cache throughput partition ratio trace figure sensitivity static partitioning partition ratios workload graph shows performance workloads run disks two-year disk experiment varies ratio slow disk partition fast disk partitions x-axis lower lines requests vary ratio requests slow disk versus ratio size real world information advance difficulty correctly configuring static partition strategy illustrated figure experiment examine single storage configuration slow disk years older disks x-axis graph varies ratio partition sizes slow disk disks system greater slow disk correspondingly larger partition lines graph correspond workloads optimal partition size ratios top line workload examined verify highest throughput workload approximately matches shown figure two-year disk lines distribution requests disks changed slow disk receives ten times requests disks graph shows workloads optimal partition ratio ratio top workload ratio bottom workload performance workload varies greatly partition ratio performance trace workload varies approach select partition ratios dynamically function workload disk performance throughput age 
slow disk years landlord eager lru lazy lru lru cache throughput age slow disk years landlord eager lru lazy lru lru cache trace trace figure lru-based dynamic partitioning algorithms figure shows performance eager lru lazy lru landlord disk aged graph trace graph trace partitioning balance work set experiments show dynamically adjusting size partition algorithms balance amount work performed disk effectively hide heterogeneity classes dynamic partitioning eager partitioning lazy partitioning lazy partitioning inverse lottery scheduling pick victim partitions replacement time eager lazy lru partitions simplicity refer approach eager lru approach lazy lru experiments investigate trace trace realistic evaluation continuing understood workload parameters figure compares performance eager lru lazy lru storage-aware algorithms lru landlord figure examine workload uniform number requests disks setup throughput lru degrades dramatically performance slow disk aged specifically throughput drops approximately eager lru lazy lru maintain throughput system slow disk aged specifically performance algorithms similar lru disks speed ten-year-old disk mask impact slow disk throughput figure shows challenges non-uniform number requests disks interestingly disks identical cost-aware algorithms perform lru workload popular disks suffer contention queuing delays make blocks disks costly fetch monitoring replacement cost cost-aware algorithms devote cache popular disks balance load disks previous workload performance benefits costaware caching improve disks aged -year disk eager lru lru differ factor eager lru performance anomaly figure slow disk ages years eager lru over-allocates pages slow disk early trace decreasing performance comparing performance eager lru lazy lru landlord sees performance algorithms similar identical figure shows difference eager lru performance robust lazy lru landlord lazy lru devotes entire cache slow disk eager lru continues allocate small amount cache fast disks repartitioning eager lru aggravates efforts find good partition size trace clock-based replacement noted section operating systems clock algorithm clock cost-aware section evaluate lazy clock eager clock section traces compare lazy clock eager clock clock experimental results found figures figure lazy clock eager clock perform desired lazy clock eager clock give greater proportion cache slower devices devices requests lazy clock eager clock mask performance differences speed slow disk degrades significantly trace lazy clock begins throughput age slow disk years lazy clock eager clock clock cache throughput age slow disk years lazy clock eager clock clock cache trace trace figure clock-based replacement algorithms figure shows performance lazy clock eager clock clock disk aged workloads investigated figure throughput number slow disks eager clock lazy clock clock cache figure clock-based multiple disks figure shows performance eager clock lazy clock clock number disks age years increased trace workload figure throughput approximately disks identical degrades slow disk full -years older throughput compares favorably landlord lazy lru figure figure shows lazy clock eager clock gracefully mask increasing number two-year disks clock affected heterogeneity performance lazy clock eager clock slowly degrade performance lazy clock match clock system homogeneous two-year disks performance fairly close experience shown smaller base correction size devices homogeneous remove discrepancy dynamic performance evaluate tolerance performance faults show partitioned caching algorithms react relative performance storage devices experiments begin cluster homogeneous disks trace inject performance fault disks disk simulated time seconds approximately half simulation performance fault effect slowing disk factor figure show eager lru adjusts partition ratios change performance partitions sizes similar variation partition sizes occurs due normal variation wait times correspondingly relative wait times performance fault window disk requests passed observation relative wait time disk significantly higher average waiting time crossing threshold partition disk increased small amount partitions disks decreased number cache entries algorithm continues measuring wait time disk increasing partition size disk wait times approximately equal time-line shows stable partition sizes found seconds figure shows bandwidth range caching algorithms single client fault injected disks simultaneously trace workload experiments relative bandwidth results similar caching algorithm number faults cache-aware algorithms tolerating fault injection eager lru eager clock highest bandwidth fraction cache time seconds partition disk fault injected partitions disks number faults throughput cache clock lru landlord eager lru eager clock lazy lru lazy clock partition sizes single dynamic fault throughput dynamic fault scenarios figure performance dynamic faults graphs figure show results injecting performance fault slows disk factor initially homogeneous disk cluster fault injected seconds experiment graph performance fault occurs single disk graph shows partition sizes chosen eager lru algorithm graph shows throughput disk cluster fault occurs disks range caching algorithms cases trace cost-aware algorithms real-world performance conclude experiments examination partitioned algorithms web workload web server received modest number requests trace shorter synthetic traces partitioned algorithms partially penalized shortness trace move partition sizes initial state similar experiments investigate aging single disk results shown figure expected cost-oblivious algorithms show sharp drop performance age slow disk increases lru performance falls peak performance age range realistic range falls aged years clock shows similar performance decline similar previous experiments adaptive algorithms show robust performance range slow disk ages performance eager lru lazy lru degrades landlord degrades smaller percentage degradation landlord shows lower bandwidth disks homogeneous lru eager lru lazy lru specifically landlord performance lru lazy lru eager lru hard time adapting extreme heterogeneity roselli workload bandwidth lazy lru falls age slow disk reaches years increases disk ages ten years anomalous behavior due poor page allocation partitions lazy partitioning implemented inverse lottery makes probabilistic guarantees proportions partitions observations made epoch inaccurate poor partition sizes achieved roselli trace alternates smaller reads writes web server substantially larger reads postgres database exacerbates problem lazy clock exhibits behavior lazy lru experiment performance falls bit earlier lazy clock suffers probabilistic guarantee problem lazy lru performance eager clock robust lazy clock range slow disk ages similar eager lru related work work cost-aware caching occurred web cache database communities web cache community extensively studied cost-aware caching addition document size included algorithms web caching work differs storage-aware caching ways performance wide area varies common storage systems web caching document caching differs fixed-size blocks storage systems finally web caching replacement cost web page strongly correlated replacement costs pages throughput age slow disk years eager lru lazy lru landlord lru cache throughput age slow disk years eager clock lazy clock clock cache lru-based clock-based figure web server workload figure shows performance lruand clock-based algorithms run file 
system trace web server providing art images graph shows results lru-based algorithms clock-based algorithms graph broadcast disks continuously deliver data clients asymmetric link broadcast schedule meet client client met broadcast schedule client cache strives manage cache contents mask non-ideal broadcast schedule knowledge broadcast schedule probability access cache manages contents algorithm generalizes lru storage-aware caching differs ways partitions cache pages device broadcast disks page device granularity track replacement costs broadcast disks assume infrequently changing broadcast schedule storage-aware caching react frequent workload device performance recently researchers studied allocation pages classes prefetching compiler-controlled memory management resizeable file buffer caches prefetching page allocation occurs applications hinted unhinted compiler-controlled memory management compiler application memory usage information operating system global replacement policies hints reintegrating elements local page replacement global page replacement finally nelson work resizeable file buffer caches evaluates tradeoff file buffer caches virtual memory system loaded work areas closely related work directly address storage device heterogeneity future work storage-aware caching increases storage system performance robustness adapting performance differences devices areas improvement application domain addition study real implementation partitioning algorithm presented significant limitations sophisticated informed cost-benefit algorithms limitations linear relationship assumption cache size hit rate reliance proper values window size base increment amount threshold limitation evident considers access patterns locality working set larger cache intuitively algorithm recognize instances increasing cache size decrease wait time general framework storageaware caching existing cost-oblivious policies manage individual partitions studied framework approach modularity primary strength existing non-cache aware policies lru clock mru eelru minimal effort work concentrated non-cooperative client caching combination cooperative caching cost-aware caching lead performance robustness disk arrays individual cache sizes small fourth storage-aware caching applied lowpower environments storage-aware caching extended include power retrieval cost devices higher power accessed frequently stay low-power mode longer frequently finally caching algorithms affected prefetching layout decisions explore advantages tradeoffs integrated prefetching layout caching decisions light device heterogeneity previous caching prefetching work homogeneous environments shown benefits integration benefit extends heterogeneous environments conclusions diverse characteristics modern storage devices time ripe re-investigate caching algorithms optimize performance task costaware cache control blocks cached amount work performed storage device roughly equal paper presented family cost-aware caching algorithms based notion explicitly partitioning cache size partition configured directly corresponds relative cost usefulness data partition approaches advantages partitions aggregate replacement-cost information entries cache reducing amount information tracked allowing recent cost information blocks device importantly virtual partition approach easily implemented clock replacement policy increasing likelihood adoption real systems acknowledgments comments jeff chase members wisconsin network disks research group including john bent nathan burnett timothy denehy florentina popovici muthian sivathanu shai rubin anonymous reviewers improved quality paper people made technical contributions paper florentina popovici provided ibm lzx disk profile information simulations leslie cheung wrote layout code roselli trace experiments omer zaki contributed early versions work condor distributed execution system run simulations members condor project todd tannenbaum supported condor work sponsored nsf ccrccr- itrand wisconsin alumni research foundation acharya alonso franklin zdonik broadcast disks data management asymmetric communications environments proceedings acm sigmod international conference management data san jose pages acm press acharya franklin zdonik disseminationbased data delivery broadcast disks ieee personal communications december alexandrov ionescu schauser scheiman loggp incorporating long messages logp model step closer realistic model parallel computation papers annual acm symposium parallel algorithms architectures santa barbara june pages acm press apple computer corporation idisk http itools mac arpaci-dusseau arpaci-dusseau bent forney muthukrishnan popovici zaki manageable storage adaptation wind proceedings ieee int symposium cluster computing grid ccgrid pages arpaci-dusseau arpaci-dusseau fail-stutter fault tolerance workshop hot topics operating systems hotos schloss elmau germany babaoglu joy converting swap-based system paging architecture lacking page-referenced bits proceedings acm symposium operating system principles pages pacific grove december bertoni understanding solaris filesystems paging technical report tr- sun microsystems cao felten karlin implementation performance integrated application-controlled file caching prefetching disk scheduling acm transactions computer systems november cao irani cost-aware proxy caching algorithms usenix symposium internet technologies systems proceedings monterey california december pages berkeley usa carley ganger nagle mems-based integrated-circuit mass-storage systems communications acm november chrobak karloff payne vishwanathan results server problems proceedings annual acmsiam symposium discrete algorithms soda pages san francisco usa january cortes labarta extending heterogeneity raid level proceedings usenix annual technical conference boston june demke brown mowry taming memory hogs compiler-inserted releases manage physical memory intelligently proceedings symposium operating systems design implementation osdipages berkeley october grochowski ibm leadership disk storage technology ibm corporation howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems february jin bestavros popularity-aware greedydual-size algorithms web access proceedings international conference distributed computing systems icdcs april kelly chan jamin mackie-mason biased replacement policies web caches differential quality-ofservice aggregate user fourth international web caching workshop san diego california march april kubiatowicz bindel eaton chen geels gummadi rhea weimer wells weatherspoon zhao oceanstore architecture global-scale persistent storage proceedings ninth international conference architectural support programming languages operating systems asplos november manasse mcgeoch sleator competitive algorithms on-line problems proceedings twentieth annual acm symposium theory computing chicago illinois pages york usa nelson welch ousterhout caching sprite network file system acm transactions computer systems february nelson virtual memory file system technical report compaq computer corporation nyberg barclay cvetanovic gray lomet alphasort risc machine sort acm sigmod conference patterson gibson ginting stodolsky zelenka informed prefetching caching proceedings fifteenth acm symposium operating systems principles pages copper mountain december acm press pro softnet corporation ibackup http ibackup rashid tevanian young golub baron black bolosky chew machine-independent virtual memory management paged uniprocessor multiprocessor architectures proceedings international conference architectural support programming languages operating systems asplos pages palo alto october association computing machinery ieee rizzo vicisano replacement policies proxy cache ieee acm transactions networking roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference usenixpages berkeley june ruemmler wilkes introduction disk drive modeling ieee computer march sandberg design implementation sun network file system proceedings usenix summer technical conference pages berkeley june sanford greier yang olyha narayan hoffnagle alt melcher one-megapixel reflective spatial light modulator system holographic storage ibm journal research development smaragdakis kaplan wilson eelru simple effective adaptive page replacement proceedings acm sigmetrics international conference measurement modeling computing systems 
sigmetricsvolume sigmetrics performance evaluation review pages york acm press sullivan seltzer isolation flexibility resource management framework central servers proceedings usenix annual technical conference san diego california infiniband trade association http infinibandta june tomkins patterson gibson informed multiprocess prefetching caching proceedings acm sigmetrics conference measurement modeling computer systems pages acm press june vahdat anderson dahlin belani culler eastham yoshikawa webos operating system services wide area applications proceedings seventh symposium high performance distributed computing july waldspurger weihl lottery scheduling flexible proportional-share resource management proceedings usenix symposium operating systems design implementation nov wooster abrams proxy caching estimates page load delays proceedings international conference april young k-server dual loose competitiveness paging algorithmica june young on-line file caching proceedings ninth annual acm-siam symposium discrete algorithms balitmore january pages acm press 
information control gray-box systems andrea arpaci-dusseau department computer sciences wisconsin madison dusseau wisc remzi arpaci-dusseau department computer sciences wisconsin madison remzi wisc abstract modern systems developers unable modify underlying operating system build services environment advocate gray-box techniques treating operating system gray-box recognizes changing restricts completely obviate information acquire internal state control impose paper develop investigate gray-box information control layers icls determining contents le-cache controlling layout les local disk limiting process execution based memory gray-box icl sits client combination algorithmic knowledge observations inferences garner information control behavior gray-box system summarize set techniques helpful building gray-box icls begun organize gray toolbox ease construction icls case studies demonstrate utility gray-box techniques implementing os-like services modi cation single line source code introduction modern operating systems large complex bodies code hundreds programmer-years invested result modifying operating system difcult costly impractical endeavor extreme realistic view researchers noted traditional operating systems rigid simply hardware masquerading software viewing operating system immutable object odds bulk operating systems research seeks develop integrate ideas operating systems reduce orts required change large body research investigated operating system restructured extensible systems functionalto symposium operating systems principles sosp october chateau lake louise banff canada permission make digital hard copies part work personal classroom granted fee provided copies made distributed profit commercial advantage copies bear notice full citation page copy republish post servers redistribute lists requires prior specific permission fee copyright acm ity performance improvements easily added tailored desires applications limitation approaches require operating system orts minimize modi cations require altered minor requiring change single line code make deployment innovation commercial operating systems problem obvious non-technical hurdles overcome persuade large company incorporate idea accepted single vendor open-source base wide-spread adoption innovations unused applications run cross-platform existing interfaces systems transactional database manages raw disk obtain high performance implements optimized database-oriented system incentive system single platform complicates database source code rare idea incorporated widely large range good ideas orphaned remaining challenge disseminate research ideas requiring underlying projects distributed computing addressed building system services top unmodi commodity operating systems approach constricting seemingly sti implementation functionality thesis paper surprisingly large class os-like services provided applications modi cation speci cally acquire information state control behavior unexpectedly powerful ways explicit interfaces exist approach treated gray box general characteristics algorithms employed combining knowledge run-time observations reacts commands queries services implemented term software layer interfaces gather information control gray-box system graybox information control layer gray-box icl icl residing clients applications graybox system presents clients traditional enhanced interfaces interfaces icl clients learn state underlying system data cache control behavior place les disk internally obtain information icl observe existing client interactions gray-box system insert probes system case combining observations statistical analyses priori knowledge behaves icl infer current state experienced programmers tend exploit knowledge behavior underlying system knowledge encapsulated icls techniques programmers gray-box systems step combining knowledge measurements observations technique commonly found microbenchmarks exists strong duality microbenchmarks gray-box techniques icls require underlying components benchmarked con gure internal thresholds parameters understanding behavior icls requires understanding behavior icls reveal surprising behavior microbenchmark paper explore challenges building graybox icls developing studying services rst le-cache content detector fccd determines contents cache applications re-order operations rst access data cache service functionality similar proposed modi cations layout detector controller fldc discerns controls layout blocks disk applications schedule accesses reduce seek time memory-based admission controller mac detects amount memory multi-programmed system limits number contending processes cases anticipate modifying applications interfaces provided icls show cases unmodi applications icls obtain bene demonstrate utility icls simple benchmarks real applications cases observe substantial performance improvements relative versions applications information control underlying cases improve performance order magnitude limitations gray-box approach discuss fundamental advantage building services icl library examples middleware distributed environment service receive wide-spread adoption related advantage approach icls easy port demonstrate relative ease icl porting running codes erent unix platforms linux netbsd solaris implementations found overlapping set required functionality gray-box icls begun formalize gray toolbox common repository routines ease construction erent operating systems fast platformspeci timers statistical routines envision toolbox grow icls developed similar spirit interposition toolkit jones rest paper organized begin summarizing techniques building icls section discuss previous gray-box systems section section give overview experimental environment cover icl case studies le-cache content detector layout detector controller memory-based admission controller describe beginnings gray toolbox section cover related work section conclude section gray-box techniques encapsulation helps simplify design large complex systems allowing system designers ignore unnecessary details simpli cation high cost viewing black box make assumptions implementation behavior internal state speci interface internal-state information acquired behavior controlled explicit mechanisms designed implementors interface provided information hidden control prevented practice systems black boxes savvy programmers idea component implemented gray box users knowledge acts speci interface knowledge toe-hold gaining information state key controlling behavior note focus paper treatment operating system gray box component layer module objectbased system treated section discuss techniques found developing case studies existing systems information techniques information internal state optimize system services applications scheduler activations key piece state information passed kernel user-level scheduling library number processors application running information threads library job scheduling active threads summarize ways determine internal state interface exists acquire algorithmic knowledge developers interfacing knowledge algorithms employed developer access source code internal design documentation simply familiarity common implementation techniques lru-like caching algorithmic knowledge exists levels detail extreme designer caching performed system extreme designer full understanding source code cost hitting missing cache algorithmic knowledge gray-box icl interact cient manner determining interact component general knowledge behaves studied extensively theoretical work game theory decision theory practical side exists tension optimizations makes icl portability algorithmic knowledge assumed optimizations make fewer systems assumptions apply monitor outputs algorithmic knowledge icl infer internal state improve quality inferences icl make found combine knowledge observations output observed output speci interface measurable characteristic external interface covert channel examples covert channels gray-box systems include elapsed time power consumed presence dropped messages outputs black-box observed make predictions infer behaves infer internal state powerful aspect gray-box techniques combination observations algorithmic knowledge allowing designers build icls portable cient portable assume high-level algorithmic knowledge cient tuned speci platform observations infer current state algorithmic knowledge simplistic inaccurate icls built robust observations 
verify true state note icl observe inputs infer state models simulations drawback requires participation processes investigate icls assume visibility inputs statistical methods infer internal state icl observe output correlated state interest infer speci code path executed data item cached observe response fast slow draw robust inferences potentially noisy data advocate statistical methods microbenchmarks parameterize system icls system parameters order operate properly speed sequential disk access suite microbenchmarks icls care executing benchmarks require dedicated system time run insert probes cases client icl make cient requests icl observe outputs icl insert probes speci requests generated solely observe resulting output probe icl generate requests desired inputs desired time desired context challenge probes describe case studies presence change state system refer heisenberg ect challenge probes add signi overhead system cases adding probes icl improve application performance prefetching disk blocks control techniques responsibility icl control ways speci existing interfaces assume designer icl level algorithmic knowledge control algorithmic knowledge perform actions side-e ects operations read interface afs icl read single byte prefetch entire server describe techniques exerting control move system state inferring information arbitrary unknown state cult state control technique icls move system simpler state easier gauge contents page cache periodically ushes monitors models subsequent activity reinforce behavior feedback application icl interactions strongly determined behavior icl icl reinforce desired behavior controlling manner behaves contents cache determined order accesses icl direct client interactions make cache contents predictable repeated access icl erent runs erent applications act positive feedback stabilizing system behavior previous gray-box approaches illustrate techniques brie survey literature systems assume exploit gray-box knowledge rst examine microbenchmarks assume knowledge system test examine existing systems gray-box techniques tcp congestion control implicit coscheduling manners note services developed implementors modify existing part system microbenchmarks applications obtain performance detailed characteristics underlying hardware systems interfaces microbenchmarks developed exploit gray-box knowledge user infer characteristics measuring completion time memory accesses erent patterns determine parameters memory hierarchy nding greatest common divisor execution time erent expressions determine processor cycle time measuring access time carefully designed requests low-level characteristics disk geometry inferred gray-box icls bear similarity microbenchmarks number important ways microbenchmarks acquire information control system microbenchmarks gather static information component characteristics current state microbenchmarks run controlled environment microbenchmarks arbitrarily long time run make inferences tcp congestion control goal tcp congestion control algorithm distributed clients send data amounts congestion viewing network gray-box clients combine general knowledge network behaves measurements ongoing communication infer current state network congestion knowledge tcp implicit coscheduling manners knowledge message dropped congestion dest scheduled send msg symmetric performance impact outputs time ack arrives arrival requests reported progress process time response statistics variance linear regression exponential avg paired-sample sign test benchmarks round-trip time probes state required benchmarks slow convergence feedback routers drop msgs signal react observations table summary gray-box techniques existing systems network drops packets congestion clients observe existing communication acknowledged infer congestion exists routers network turn control sending rate clients dropping packets congestion occurs misbehaving clients identi observing unresponsive gray-box control tcp congestion control algorithm labeled black-box scheme due assumption packet loss caused congestion gray-box scheme fact recognizing gray-box knowledge led problems environments wireless setting dropped message longer congestion due simply lossy medium result unmodi tcp congestion control algorithm behave wireless settings correctly identifying gray-box knowledge problems avoided implicit coscheduling time-shared ne-grain parallel jobs achieve acceptable performance communicating processes scheduled simultaneously implicit coscheduling technique achieving coordinated multiprocess scheduling modifying implicit coscheduling combines gray-box knowledge communication interacts scheduling remote nodes observations on-going communication parallel job speci cally hard-wired algorithm process waiting response knowledge receiving message remote process means remote process scheduled recent past likewise receiving prompt response request means remote process scheduled infer scheduling state remote nodes process simply observes message arrivals waiting time manners running low-importance processes idle time feature missing modern operating systems manners functionality suspending low-importance jobs resource contention detected implemented modi cation manners gray-box knowledge process competing degrades progress symmetrically combining knowledge measurements progress low-importance process manners infer low-importance process suspended authors number simple statistical techniques calculating expected level performance uncontended environment required time frame order hours summary summarized table services touch number gray-box techniques revisit case studies services combine algorithmic knowledge observations time required existing operations infer state system services statistical techniques run time priori benchmarking controlled state addition techniques presented case studies demonstrate utility probing case studies section explore icls speci cally develop experiment le-cache content detector fccd layout detector controller fldc memory-based admission controller mac due space limitations describe fccd detail present subset issues fldc mac section discuss basic problem icl addresses gray-box knowledge explain implementation perform experiments show capabilities layer discuss limitations summary gray-box techniques found shown table experiments run machine intel pentium-iii processors physical memory ibm lzx disks machine large amount memory stresses icls determine contents cache amount memory experiments performed top linux evaluate gray-box libraries netbsd solaris fact easily deploy icls platforms illustrates major advantages gray-box approaches file-cache contents knowledge contents cache applications re-order data accesses potentially improve performance application repeatedly accesses set les erent arguments grep arg system total amount data exceeds size cache operating system performs lru-like replacement case performance improves dramatically application rst processes data cache small fraction data fetched disk application access cached data rst operates lru worst-case mode fetching data disk run section describes gray-box file-cache content defile-cache content file layout detector memory-based admission detector fccd controller fldc controller mac knowledge lru-replacement groups directory replacement algorithm locality app groups i-nodes data-blocks outputs time read byte i-number time write byte statistics sort cluster sort discard outliers benchmarks measure seek overhead memory disk threshold probes read byte stat i-number write byte page state refresh directory contents write rst make resident feedback order determines access pattern table summary gray-box techniques case studies tector fccd applications gauge contents cache act pursuit gray-box fccd inspired recent work storage latency estimation descriptors sleds discussed van meter gao work van meter gao propose interface returns predicted access times sections interface determine parts fast access based combination knowledge storage hierarchy static estimates storage device latencies main limitation work requires modi cations linux kernel gather information show great deal utility proposed 
system obtained modi cation operating system gray-box knowledge begin exploring algorithmic knowledge le-cache manager knowledge develop gray-box fccd extreme approach complete algorithmic knowledge le-cache manager access inputs extreme approach basic algorithmic knowledge combined observations outputs complete knowledge behavior le-cache page-replacement algorithm ability observe input model simulate pages cache approach complex inaccurate detailed model page-replacement algorithm order correctly simulate contents cache due interactions memory pages observe accesses memory accesses finally applications interested state le-cache provide inputs simulation single process obey rules knowledge accessed incomplete simulation inaccurate explore infer internal state le-cache observing outputs begin assuming coarsest level algorithmic knowledge cache les full page replaced order page hypothesis predict presence systems provide information contents cache mincore routine interface broadly relied part cache timing carefully selected le-cache probes probe read single byte page read returns quickly conclude probed page cache probe returns slowly page disk probes sparingly reasons probing page memory high cost time probe page essentially identical application access page disk probing page disk destructive state cache heisenberg ect probe page entire page brought cache page evicted selective probes relative cost low avoid changing state system probes accurately ect state cache goals inherently odds order probes successful presence page cache highly correlated presence pages nearby order correlation exist system tend adjacent pages cache evict ect occurs systems applications access les spatial locality page replacement algorithms designed mind operating system approximation lru clock algorithm tend evict pages signi cantly long chunks figure demonstrate relationship plotting correlation presence random page prediction unit contiguous region percentage unit cache line designates erent access unit application random-access sequential-access access unit amount data application reads sequentially randomly picking set test run ush cache run program accesses speci access unit query cache determine contents query contents cache modi linux kernel return bit-map presence bits page graph prediction unit equal access size presence probed page highly correlated presence entire prediction unit prediction unit large relative access unit application corindeed interface existed platforms require gray-box fccd correlation prediction unit size prediction units access size access size access size figure probe correlation graph plots correlation presence single random page prediction unit percentage unit cache size prediction unit increased x-axis correlation plotted y-axis sets points plotted vary access pattern test program running test program explicit probe determines pages present cache accessed roughly size cache measurements times means standard deviations shown relation falls noticeably implementation section discuss fccd prediction unit made smaller access unit desired implementation details envision common usage template applications fccd application speci set les interested reading library returns list offset length pairs data thinks cache based probes performs application information re-order accesses rst accessing pages memory applications examined modi cations straight-forward involved lines code provide method application fccd requiring modi cation users call utility gbp set les returns list les predicted access order implying grep foo replaced grep foo gbp -mem utilize data reordering single option gbp probe read data blocks probe order copying reads stdout gbp -mem -out infile app unmodi application reads stdin utilize intrale re-ordering describe make inferences probes working library simple cient portable manner goal library work operating system performs replacement similar based time access underlying hardware technology parameters speed memory disks implementation address problems erentiate probe times cache cache amount data application access unit number pages state predicted single probe describe issues turn cache-di erentiation threshold conceptually determine probed page memory erentiate time er-cache hit versus er-cache miss approach simple threshold time probe threshold page considered cache greater page considered disk library work variety platforms approach requires priori benchmarking kernel-to-user memory copy time storage subsystem painful erent types disks present arrived solution requires differentiation threshold sorting prediction units time required probe method simple robust erentiates entities multiple-level store memory disk tape case closest items accessed rst closest access unit gray-box interface order accesses application previously read large sequential order read random order amortize seek overhead reading arbitrary sets library return offset length pairs large length elds determine default access unit delivers near-peak performance disk performing simple microbenchmark platform found default access unit works application access units obey boundaries ensure records cross multiple access units advanced applications exact manner data returned passing list offset length pairs prediction unit shown picking prediction unit smaller access unit application cient high prediction accuracy similarly fccd access size access unit simply set prediction unit access unit obtain reasonable predictor found performing probes access unit slightly robust prediction unit gray-box layer probes points default access unit measures time probe sorts access units total time probes overhead probes negligible measurements reveal probe time in-cache data realm microseconds milliseconds probe out-of-cache data amortized entire access time files smaller size probed found method choosing probe point prediction unit important approach select bytes predetermined sets process terminates probe phase access phase processes probe le-cache time set probes return bad information indicating pages cache solution probe random byte prediction unit method robust runs added bene application probe cache repeatedly increased con dence time file size single-file scan traditional gray-box worst-case disk ideal in-cache ideal out-of-cache figure single-file scan graph plots total access time repeated runs warm cache traditional linear scan gray-box scan gray-box scan fccd ascertain parts cache accesses accessing rest data point average runs includes standard deviation bars simple models plotted predicted worst-case time data retrieved disk predicted ideal data cache retrieved memory-copy rates data fetched disk experiments perform experiments demonstrate utility cacy gray-box fccd begin showing software obtains good performance reordering accesses single large reordering accesses les examine bene applications modi interfaces fastsort grep finally demonstrate techniques work erent unix-based operating systems single-file scan perform simple experiment modify scan single utilize library gray-box scan library probe state cache accesses rst pages library predicts in-cache rest result access pattern gray-box scan longer purely sequential scan sequentially accesses segments size directly determined access unit ect running application multiple times control technique positive feedback accessing access-unit sized chunks access-unit sized chunks present cache figure plots time access single varying size gray-box scan traditional linear scan experiment begin ushing cache running application times note graph similar spirit style graphs 
presented van meter gao text gure traditional scan ers large performance decrease size exceeds size cache point entire retrieved disk due lru-like page replacement algorithm gray-box scan consistently perform accesses disk frequently total amount performed proportional size minus size cache multiple-file scan applications easily modi process single arbitrary order exibly process set input les arbitrary order experiments shown due lack space utilize fccd determine ordering group les processing sequentially performance similar shown singlele scan application experiments set experiments incorporate gray-box library real applications rst examine versions grep rst unmodi standard gnu utility searches string set les version gb-grep modify grep internally reorder les speci command line gray-box library change straight-forward transforming lines code roughly lines version output gbp utility input unmodi grep grep foo gbp -mem figure shows time versions grep -mb text les warm cache time application normalized time unmodi version gray-box knowledge repeated runs access les order run rate disk gray-box version gb-grep runs factor faster data cache traditional grep combined gbp exhibits bene slight additional overhead incurred due extra fork exec redundant opens closes application fastsort highly tuned twopass disk-to-disk sort similar agarwal rst pass creates multiple sorted runs records size run determined records memory run reads records sorts keys writes sorted records disk pass reads sorted runs disk merges single sorted list writes nal output disk experiments sort roughly data -byte records report performance rst read phase simulate pipeline creating records sorting refresh cache contents run versions sort unmodi sort sort modi gray-box library unmodi sort gbp -mem -out input transformation traditional sort graybox version slightly involved grep application read parts single input erent order required replacing read code lines code adding probe phase main sorting loop lines note gbp informed -byte alignment restrictions sort returns chunks record-aligned figure shows performance read-phase versions gray-box versions substantially improve performance bene large grep erence occurs sort copies memory gray-box versions grep follow exact semantics grep output ordered erently semantics preserved output grep re-ordered application thrash outputting large number matches grep sort normalized execution time application application performance grep gb-grep grep gbp -mem fastsort gb-fastsort gbp -mem -out infile sort figure application performance performance grep fastsort shown leftmost bar group shows normalized performance unmodi application repeated runs roughly total data grep scans -mb les seconds average fastsort completes read-phase input seconds bar group shows relative improvement gray-box version application finally bar group gray-box command line tool unmodi application advantage gray-box knowledge data item reads eventually writes data disk contention memory read-only application grep pages heap pages write-bu ering purge parts input memory prematurely unmodi sort gbp -out input experiences bene extra copy data required operating system pipe mechanism copy palatable sort cpu acceptable situations multiple-platform tests demonstrate gray-box approach works range operating systems examine fccd linux netbsd solaris experiments compare performance microbenchmarks scan multile search measuring unmodi performance cold warm cache modi gray-box performance warm cache figure plots relative execution times normalized platform time cold-cache run platform actual times caption examining scan results rst linux repeated runs gray-box fccd exhibit signi improvement relative unmodi scan expected slightly surprised performance repeated scans -gb netbsd solaris linux solaris entire physical memory caching throwback early unix implementations netbsd xed amount memory caching case note recent overhaul netbsd netbsd repeated scan -gb runs near-disk rate gray-box knowledge illustrate bestcase gray-box performance netbsd report linux bsd solaris linux bsd solaris normalized execution time scan search multi-platform experiments traditional cold cache traditional warm cache gray-box warm cache solaris re-reads hit file cache figure multi-platform experiments gure plots performance repeated largele scans multile searches bars plotted runs experiment average time cold-cache warm-cache run traditional approach average time warm-cache gray-box run group bars normalized cold-cache time linux bsd solaris scans les seconds cold-cache case searches -mb les -mb les -mb les average seconds performance repeated -mb scans surprisingly solaris repeated scans warmcache perform gray-box knowledge case le-cache manager lrubased replacement algorithm single portion cache repeated accesses hit cache testing revealed portion solaris cache cult dislodge repeated scans erent les approach works benchmark solaris cache manager holds pages rst accessed persistently behavior implementors intended investigation warranted search benchmark demonstrates nonlru replacement policies bene gray-box techniques performing search match string set les match found cached gray-box search nish quickly traditional search mercy ordering speci user scenario similar grep experiment reported experiment set arbitrary manner con gure illustrates maximum bene graybox approach matching string located cached speci command-line figure shows unmodi search advantage cache searches les order nding match gray-box search nds match quickly cache discussion investigating multiple platforms revealed level algorithmic knowledge assumed fccd largely unix-based operating systems relying primarily measurements probes determine state cache requiring detailed knowledge study highlights duality gray-box systems microbenchmarks tend unveil inner-workings systems fccd panacea major limitation techniques limited heisenberg ect gauge presence small page size cache bringing entire cache fccd probe small les returns fake high probe-time analogous heisenberg ect arises distributed system afs reading single byte force fetch entire local disk cache file layout accessing les disk exact layout les strong ect performance section investigate treat le-system layout algorithm gray-box developing layout detector controller fldc fldc layer applications order accesses improved performance based probable layout disk discussed earlier applications re-order accesses time improve disk performance purposes discussion focus smallle accesses scans large les amortize arm-movement overheads obviate re-ordering gray-box knowledge information exact layout inode le-block disk application re-order accesses reduce seek time rotational delay superuser privileges knowledge le-system structures reconstruct exact layout les raw-disk reads information hidden users applications fortunately experienced programmers gray-box knowledge les allocated disk modern unix le-systems direct intellectual descendents berkeley fast file system ffs ffs attempts lay les subsequent read performance optimized basic premise blocks meta-data les directory accessed ffs place cylinder group consecutive cylinders disk based algorithmic knowledge ffs simple heuristic reduce seek time group set les directory access order access order matter directory clean le-system small les created directory creation order matches data-block layout disk determine creation order option creation time resolution creation time cient multiple les created simultaneously option inode number i-number probe stat system 
call account ects le-system aging attempt discover layout predictor arbitrary creations deletions follow control technique gray-box systems moving system state refreshing directory writing les directory order hypothesize system state i-number order highly correlated data-block layout note directory refreshed small les rst small les assigned rst i-nodes directory large les presence lower correlation i-numbering data layout assigned i-nodes data blocks impact implementation approach implementation layout detector controller straight-forward application wishes access set les calls fldc layer desired set les fldc layer performs stat returns les i-number sorted order note sorting i-number essentially obviates sort directory verify low-overhead performing stat measured operation requires milliseconds disk access fact accessing group les single directory rst calling stat accessing data improves performance slightly inodes data blocks located separate regions cylinder group control component fldc layer refresh directory requires steps create temporary directory level hierarchy sort les original directory size user-speci cation copy les original directory sorted order update access modi cation times match original les make time-dependent programs operate correctly delete directory rename temporary directory directory experiments explore bene fldc clean aged le-systems experiments examine simple microbenchmarks ensure data meta-data cache begin reporting performance newly created le-system operating system platforms examine total time read small les evenly divided directories erent access patterns random ordering les les sorted directory les sorted i-number figure shows sorting directory improves performance relative random order sorting i-number leads dramatic improvements factor linux netbsd factor solaris hypothesize solaris pack data blocks small les tightly issues atomicity refresh operation crash occurs delete midst rename envision nightly script directory signature patches problems implemented linux bsd solaris time platform sensitivity file ordering random sort directory sort i-number figure file ordering matters gure plots total access time scan -kb les split equally directories experiment varies platform order access random bar ects access time les random order trial sort directory bar rst groups les directory accesses nally sort i-number bar rst sorts collection les i-number reads bars ect average measurements standard deviations shown spends time rotation measure ects le-system aging fldc create series epochs epoch random les deleted les created experiment les directory compare performance application reads les random order versus i-number ordering figure plots time application function increasing epochs epoch explicitly refresh directory graph shows random ordering performs poorly expected i-number ordering performs excellently fresh directory degrades epoch i-number ordering random epochs performance worse fresh directory performance factor directory refreshed epoch i-number sorting returns performance original level composition applications utilize fldc layer utilize fccd layer modifying grep process les order probable layout disk passing gbp -file commandline unmodi grep improves performance manner similar speed-ups figure shown due space constraints similarity fccd fldc interfaces leads natural question compose icls ordering les application rst access les cache access rest i-number ordering culty approach fccd explicitly identify les cache orders les probe time reliably discern in-cache out-of-cache les apply standard statistical clustering time aging epoch effects aging random sorted i-number refresh brings performance back excellent level figure undoing ects aging gure plots total time access -kb les directory linux platform upper line plots time access les random order lower line sorts les i-number rst x-axis age directory increased epoch random les deleted les created epochs directory refreshed copying contents temporary directory deleting directory renaming temporary directory probe times clustered groups minimizing intragroup variance maximizing intergroup variance form clusters clustering algorithm fast rst group predicted cache group predictions incorrect les disk group sorted i-number incorporated approach grep initial experiments performs rst accessing les cache picking order on-disk accesses provide avenue unmodi applications compose gbp utility discussion limitation gray-box fldc highly unix-centric utilizes i-number approach work platforms expose low-level information current implementation work non-ffs-based systems porting prove cult lfs icl advantage knowledge writes occur time lead proximity space open question refresh directory simple heuristic refresh directory randomly accesses refresh important directories nightly script ideally ascertain i-number ordering performing historical tracking perform refresh additional danger refresh operation applications i-number les directly applications active time refresh cease operate correctly safe refreshes invoked system start-up user guarantee processes running memory-based admission control virtual memory systems provide applications illusion unlimited amount memory memory resources heavily overcommitted illusion breaks system page parts memory swap entire processes disk desire service ensures running processes actively memory physically present section describe gray-box memory-based admission controller mac limits total amount memory allocated service components control mac ensures set processes allocate memory physically present admissioncontrol processes forced wait cient memory information mac noties applications amount memory applications adjust memory usage operating multiple passes mac atomically identi allocates memory avoid race conditions gray-box knowledge investigate approach process independently determines amount memory probing measuring time increasingly large memory accesses approach naturally leverages nition working set employed page-replacement algorithm mac observes memory accessed triggering replacement technique special conditions needed account memory erent competing processes erent purposes heap stack cache directly observe paging activity systems vmstat observe time order explore environments limited interfaces basic algorithm employed mac probe chunk memory page time sequential loops recording time page access note probes write page copy-on-write systems reads force pages allocated rst loop control technique moving system state directly infer accesses amount memory space access time include costs allocating zeroing re-fetching page disk loop access page fast mac infers chunk memory space pages selected replacement accesses slow mac infers amount memory large paged disk probing progressively larger chunks memory mac determine amount space assumption algorithm rate probes access memory approximately matches access rate application stable working set competing processes mac application pages resident probes write single byte page moving page application touches pages slowly result application allocated pages resident active competing process make mac slightly aggressive approach application rate mac probe pages order match rate subsequent access patterns approach cult cumbersome approach mac assume memory resident competing process active working set result cient mac wait access page determine paging occurring mac notices consecutive slow data points rst access loop predicts increasing working set activated page daemon size exceed amount suspicions mac immediately skips loop verify contents memory implementation mac low-level interface applications informed memory speci cally mac 
exports interface dynamic memory allocation alloc takes minimum maximum multiple bytes allocate returns pointer allocated space actual number bytes allocated null pointer returned minimum amount memory application adapt memory requirements speci identical minimum maximum amounts exposing control information low level processes respond application-speci manner lack memory cases anticipate applications simply allocate memory previous invocation fails waiting period time naive interface applications deadlock applications allocate half memory allocate memory releasing initial memory complete classic solutions deadlock prevention allocating required memory releasing memory allocation fails solve problem future plan investigate higher-level interfaces hide complexity provide fair allocation competing processes discuss challenges implementing mac erentiating in-memory out-of-memory probe times incrementing amount memory test iteration memory-di erentiation threshold determine paging memory disk occurring mac erentiate time write memory versus disk platform-independent manner unlike fccd collect probe times sort cluster times erentiate groups mac determine page-by-page basis probing reveals page memory approaches determining thresholds rst method values calculated simple microbenchmark run controlled environment advertised method microbenchmark run rst time mac contacted process measures time repeated accesses pages memory time non-resident page simply considered signi cantly larger erentiate paging scheduling activity found experimentally observing slow data points succession reliable indicator paging increment unit repeatedly accessing large amounts pages time-consuming mac probe substantially larger chunk memory iteration recovering large increment paging costly processes increase aggressive found good compromise initially increment search size conservative amount slowly double increment amount allocated memory found space xed maximum increment unit back completely original increment size problem detected analogous conservative tcp congestion-control scheme experiments linux platform extensively veri mac returns expected amount memory experiments show process allocates data accesses variety patterns mac reliably returns competing application applications repeatedly access allocated memory paging show mac behaves multiple competing processes simultaneously demanding application investigate copies fastsort sorting data sort adapt amount memory reading sorting writing sets records passes size pass memory determining number records memory complicated due fact linux shared virtual memory cache amount devoted virtual memory system records read written disk investigate traditional fastsort size pass speci command-line gb-fastsort modi mac icl gb-fastsort frees chunk memory allocating memory pass meshes interface deadlock minimum allocatable amount ensure arbitrarily small passes performed maximum equal total amount sorted gb-fastsort processes allocate memory simultaneously grab large chunks acquire minimum wait memory freed figure shows sorting extremely sensitive amount memory allocated pass overestimating amount severely increases amount time required complete workload perfect knowledge workload user determine amount correctly machine anticipate sorted pass processes paging passes paging signi slowdown compared runs robustness mac layer illustrated fact gb-fastsort exhibits paging activity read sort write phases experiments gb-fastsort average pass size close best-performing static version gb-fastsort time data sorted run gray-box sort performance write sort read overhead note gray-box version sort overhead component figure performance sort mac execute rst phase competing copies fastsort sorts million -byte records execute merge phase performance sensitive amount memory avoid contention disks process reads writes disk fth disk paging cache ushed test performance mac degrades rapidly memory allocated sorting pass size shown requires average minutes data point represents average experiments performs worse due alloc overhead signi approximately equal overheads present gb-fastsort designated overhead graph rst component time spent mac layer repeatedly probing memory increase number probed pages iteration operation proportional square number pages component time spent waiting memory compute-bound workload waiting time hidden increased throughput processes bound workload fastsort waiting time increases total time workload note performing admission control workload accounts improved performance write phase contention cache discussion mac important limitations implementation sensitive behavior underlying page-replacement process parameters mac tuned work linux interface assumes applications call pairs alloc free memory requirements change inform application memory call-backs cult programmers deal ciently ectively allocate memory application mac allocated memory malloc approach prevent thrashing competing application subsequently calls malloc amount larger memory gray toolbox support gray-box systems assembling collection tools operations performed frequently gray-box layers common operations implemented optimized implemented number utilities current toolbox imagine added time developers gain experience gray-box systems observing systems literature case studies tools building gray-box icls microbenchmarks con guration graybox icls require knowledge performance parameters underlying components amortize overheads set access unit le-cache content detector erentiate erent states determine page memory disk multiple icls parameters share information common repository microbenchmarks report performance numbers expected disk seek time expected disk bandwidth time allocate page time access page memory time access page disk common format persistent storage microbenchmark run performance suspected changed icl search desired information exist icl determine acquire information update common repository measuring output acquire information gray-box component icls studied measure time operations complete overhead obtaining elapsed time added operations important overhead low time operations complete quickly timer resolution issue provide fast timer speci current platform intel machines rdtsc instruction interpreting measurements observations output icl manipulate raw data infer current state found common data manipulations calculating simple statistics standard deviation median maximum minimum performing slightly sophisticated operations correlations clusters discarding outliers nally sorting due frequency important operations performed low time space overhead data collected time results continually monitored operations performed incrementally note douceur bolosky statistical sampler good candidate inclusion related work uential research projects investigated restructure operating systems extensible adopted systems solve problems incorporate functionality commercial operating systems restructured work investigated developers add functionality ways minimal interpositioning developers write code extensions invoked events cross interface boundaries implementing protected interposition agents requires small presence enhances number gray-box techniques applied speci cally interpositioning easily observe inputs outputs model simulate infer current state future plan investigate interpositioning gray-box icls disco virtual machine monitor modify run multiprocessor additional layer software inserted hardware multiple operating systems disco occasionally gray-box knowledge achieve desired information control disco developers irix enters low-power mode idle signal switch virtual processor finally visual proxies treat applications gray boxes changed insight gui-based applications ect internal state visual interface visual proxy snoop gui mirror internal state application visual proxy control application generating synthetic window events simulate user input conclusions systems longer developed isolation building local distributed service utilize interact software components control situation contrast past small group researchers implement operating system include compiler shell tools gray-box techniques 
building systems services live constraints gray-box icls encapsulate knowledge behavior observation statistical methods inference clients gain information state control behavior paper demonstrated utility gray-box techniques os-like services lecache content detector layout detector controller memory-based admission controller applications improve performance substantially icls source-code modi cation gray-box techniques broadly applicable local operating system distributed environments plan develop explore gray-box icls settings services developed gray-box approach revolutionary ideas require parts system remaining challenge determine types services implemented gray-box icl incorporated operating system acknowledgements bent burnett denehy forney popovici sivathanu anonymous reviewers shepherd monica lam excellent feedback work sponsored nsf ccrccr- itrand wisconsin alumni research foundation agarwal super scalar sort algorithm risc processors proceedings acm sigmod conference pages june anderson bershad lazowska levy scheduler activations ective kernel support user-level management parallelism acm transactions computer systems pages february arpaci culler krishnamurthy steinberg yelick empirical evaluation cray-t compiler perspective annual international symposium computer architecture iscapages june arpaci-dusseau implicit coscheduling coordinated scheduling implicit information distributed system acm transactions computer systems tocs aug babaoglu joy converting swap-based system paging architecture lacking pagereferenced bits proceedings acm symposium operating system principles pages paci grove december baker hartman kupfer shirri ousterhout measurements distributed file system proceedings acm symposium operating systems principles pages october balakrishnan padmanabhan seshan katz comparison mechanisms improving tcp performance wireless links ieee acm transactions networking december bershad savage sirer fiuczynski becker chambers eggers extensibility safety performance spin operating system proceedings acm symposium operating systems principles december blackwell girshick theory games statistical decisions john wiley sons york bugnion devine govil rosenblum disco running commodity operating systems scalable multiprocessors acm transactions computer systems november cranor parulkar uvm virtual memory system proceedings usenix annual technical conference monterey california june douceur bolosky progress-based regulating low-importance processes procedings acm symposium operating systems principles december druschel peterson hutchinson micro-kernel design decoupling modularity protection lipto proceedings international conference distributed computing systems pages june engler kaashoek exterminate operating system abstractions proceedings workshop hot topics operating systems engler kaashoek toole exokernel operating system architecture application-level resource management proceedings acm symposium operating systems principles december floyd jacobson random early detection gateways congestion avoidance ieee acm transactions networking aug ghormley petrou rodrigues anderson slic extensibility system commodity operating systems usenix annual technical conference pages june ghormley petrou rodrigues vahdat anderson global layer unix network workstations software practice experience july howard kazar menees nichols satyanarayanan sidebotham west scale performance distributed file system acm transactions computer systems february jacobson congestion avoidance control sigcomm symposium communications architectures protocols pages aug jain delay-based approach congestion avoidance interconnected heterogeneous computer networks technical report dec-trdigital equipment corporation april jones interposition agents transparently interposing user code system interface proceedings symposium operating systems principles pages december kocher jun erential power analysis advances cryptology crypto annual international cryptology conference lecture notes computer science pages lampson note con nement problem communications acm october lampson hints computer system design operating systems review october litzkow livny mutka condor hunter idle workstations proceedings international conference distributed computing systems pages june mckusick joy fabry fast file system unix acm transactions computer systems aug meter gao latency management storage systems osdi october ousterhout scheduling techniques concurrent systems international conference distributed computing systems pages pang carey livny memory-adaptive external sorting proceedings international conference large data bases aug ritchie thompson unix time-sharing system communications acm july rosenblum ousterhout design implementation log-structured file system acm transactions computer systems february saavedra smith measuring cache tlb performance ect benchmark runtimes ieee transactions computers satyanarayanan flinn walker visual proxy exploiting customizations application source code acm operating systems review july seltzer endo small smith dealing disaster surviving misbehaved kernel extensions proceedings symposium operating system design implementation osdi smaragdakis kaplan wilson eelru simple ective adaptive page replacement sigmetrics conference measurement modeling computer systems atlanta smith seltzer file system aging proceedings sigmetrics conference seattle june smith seltzer comparison ffs disk allocation policies usenix annual technical conference pages staelin mcvoy mhz anatomy microbenchmark proceedings usenix annual technical conference pages berkeley usa june usenix association talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley von neumann morgenstern theory games economic behavior princeton press princeton jersey edition worthington ganger patt wilkes on-line extraction scsi disk drive parameters proceedings acm sigmetrics performance conference measurement modeling computer systems pages zeller gray adaptive hash join algorithm multiuser envronments proceedings international conference vldb pages 
deconstructing storage arrays timothy denehy john bent florentina popovici andrea arpaci dusseau remzi arpaci dusseau department computer sciences wisconsin madison abstract introduce shear user-level software tool characterizes raid storage arrays shear employs set controlled algorithms combined statistical techniques automatically determine important properties raid system including number disks chunk size level redundancy layout scheme illustrate correctness shear running numerous simulated con gurations verify real-world applicability running shear software-based hardware-based raid systems finally demonstrate utility shear case studies show shear storage management environment verify raid construction detect failures demonstrate shear extract detailed characteristics individual disks array show operating system shear automatically tune storage subsystems speci raid con gurations categories subject descriptors storage management storage hierarchies general terms measurement performance keywords storage raid introduction modern storage systems complex highend storage array tens processors hundreds disks array con gured ways commonly raidraid- raidhowever internal complexity raid arrays expose simple interface consisting linear array blocks internal complexity hidden large array exports interface single disk encapsulation advantages important transparent operation unmodi systems top storage device transparency cost users applications easily obtain information storage system storage systems reveal data blocks mapped underlying disks permission make digital hard copies part work personal classroom granted fee provided copies made distributed pro commercial advantage copies bear notice full citation rst page copy republish post servers redistribute lists requires prior speci permission fee asplos october boston massachusetts usa copyright acm raid con guration large impact performance reliability fact con guring modern array dif cult error-prone administrators verifying correctness setup paper describe shear user-level software tool automatically identi important properties raid tool characterize raid developers higher-level software including systems database management systems tailor implementations speci array run administrators shear understand details arrays verifying con gured raid expected observing disk failure occurred common microbenchmarking general approach shear generate controlled request patterns disk measure time requests complete applied generally similar techniques single-disk storage systems carefully constructing patterns shear derive broad range raid array characteristics including details block layout strategy redundancy scheme building shear applied number general techniques critical successful realization important application randomness generating random requests disk shear control experimental environment avoiding multitude optimizations common storage systems crucial shear inclusion variety statistical clustering techniques techniques shear automatically conclusions avoid human interpretation demonstrate effectiveness shear running simulated real raid con gurations simulation demonstrate breadth shear running variety con gurations verifying correct behavior show shear discover interesting properties real systems running shear linux software raid driver uncover poor method parity updates raidmode running shear adaptec raid controller card unusual left-asymmetric parity scheme finally demonstrate utility shear tool case studies rst show administrators shear verify correctness con guration determine disk failure occurred raid array demonstrate shear enables existing tools extract detailed information individual disks array show system knowledge underlying raid improve performance speci cally show modi linux ext system performs stripe-aware writes improves sequential performance hardware raid factor pppp striping raid stripe size pattern size pppp ppp pppp pppp parity raid left symmetric stripe size pattern size striping zig zag stripe size pattern size mirroring raid stripe size pattern size mirroring chained declustering stripe pattern pppp parity raid stripe size pattern size pppp pppp pppp parity raid left asymmetric stripe pattern pppp pppp pppp pppp qqqqpppp qqqpppp qqqqpppp qqqqpppp parity stripe size pattern size figure examples terminology picture displays number disk arrays layout patterns discussed paper numbers represent blocks parity blocks redundant data denoted italics case chunk size blocks stripe size pattern size blocks listed array depicts full patterns layout scheme rst shaded gray rest paper organized section describe shear illustrating output variety simulated congurations redundancy schemes section show results running shear software hardware raid systems section show shear improve storage administration system performance case studies finally discuss related work section conclude section shear describe shear software identifying characteristics storage system multiple disks begin describing assumptions underlying storage system present details raid simulator verify shear give intuition behavior finally describe algorithms compose shear assumptions paper focus characterizing block-based storage systems composed multiple disks speci cally assumptions shear determine mapping logical block numbers individual disks disks mirrored copies parity blocks model storage system captures common raid levels variants chained declustering assume storage system properties data allocated disks block level block minimal unit data system reads writes storage system chunk set blocks allocated contiguously disk assume constant chunk size stripe set chunks data disks shear assumes mapping logical blocks individual disks repeatable unknown pattern pattern minimum sequence data blocks block offset pattern located disk likewise pattern mirror parity blocks disks note con gurations pattern size identical stripe size raidand raidleft-symmetric pattern size larger raidleft-asymmetric based assumption shear detect complex schemes autoraid migrate logical blocks physical locations redundancy levels figure illustrates number layout con gurations analyze paper con guration disks chunk size blocks vary layout algorithm level redundancy raid systems typically signi amounts memory caching shear attempt identify amount storage memory policy replacement techniques developed applicable due random accesses steady-state behavior shear operates correctly presence cache long cache small relative storage array assumption shear initiate read requests cached perform writes overwhelm capacity cache framework makes additional assumptions assume disks homogeneous performance capacity random accesses makes shear robust heterogeneity detail assume shear access raw device access blocks directly storage system bypassing system buffer cache finally assume traf processes system found shear robust small perturbations techniques basic idea shear accessing sets disk blocks timing accesses detect blocks located disks infer basic properties block layout intuitively sets reads slow assumed located disk sets reads fast assumed located disks basic approach shear employs number techniques key operation randomness key insight employed shear random accesses storage device random accesses important number reasons random accesses increase likelihood request disk cached prefetched raid performance random access dominated number disk heads servicing requests shear easily identify number disks involved random accesses saturate interconnects hide performance differences finally random accesses tend homogenize performance slightly heterogeneous disks historical data disk bandwidth improves year seek time rotational latency improve year result disks generations similar terms random performance sequential performance steady-state shear measures steady-state performance storage system issuing large number random reads 
writes approximately outstanding requests examining steadystate performance ensures storage system prefetch cache requests important write operations temporarily buffered writeback raid cache statistical inferences shear automatically identi parameters storage system statistical techniques shear graphical presentations results veri cation human user required interpret results automatic identi cation performed clustering observed access times k-means x-means clustering shear determine access times similar blocks correlated safe operations operations shear performs storage system safe accesses read operations writes performed rst reading existing data memory writing data result shear run storage systems live data shear inspect raids disk failures performance anomalies time simulation framework demonstrate correct operation shear developed storage system simulator simulate storage arrays variety striping mirroring parity con gurations simulate raidraid- raidraid- left-symmetric left-asymmetric right-symmetric rightasymmetric layouts redundancy chained declustering con gure number disks chunk size disk storage array include cache disks storage array con gured perform similarly ibm lzx disk simulation disk storage array fairly detailed accurately modeling seek time rotation latency track cylinder skewing simple segmented cache con gured disk simulator combination methods issuing scsi commands measuring elapsed time directly querying disk values provided manufacturer speci cally simulate rotation time head switch time cylinder switch time track skew sectors cylinder skew sectors sectors track disk heads seek time curve modeled two-function equation proposed ruemmler wilkes short seek distances cylinders seek time proportional square root cylinder distance endpoints longer distances seek time proportional cylinder distance endpoints algorithm shear steps step parameter storage system identi shear determines pattern size shear identi boundaries disks chunk size shear extracts detailed information actual layout blocks disks finally shear identi level redundancy shear behaves correctly striping mirroring parity examples section begin assuming storage system redundancy show shear operates redundancy additional simulations section describe algorithmic steps detail pattern size rst step shear identi pattern size pattern size ned minimum distance blocks located disk shear operates testing assumed pattern size varying assumed size single block prede ned maximum slight unimplemented nement simply continue desired output results shear divides storage device series non-overlapping consecutive segments size shear selects random segment offset random segments issues parallel reads offset segment workload random requests repeated times completion times averaged increasing effect concurrently examining segments disk increasing conducts trials random offsets intuition algorithm nition match actual pattern size requests disks equal requests disk requests serviced parallel disks response time storage system expected requests serviced disk illustrate behavior disk raidarray block size chunk size blocks blocks blocks blocks blocks blocks blocks blocks blocks figure pattern size detection sample execution disks chunk size blocks shaded blocks read shear increments assumed pattern size compactness gure starts assumed pattern size blocks increases time blocks gure highlights blocks stride reality random blocks read pattern size detection raid disks chunks pattern size assumed time pattern size detection raid disks chunks pattern size assumed time pattern size detection raid disks chunks pattern size assumed time figure pattern size detection simulations graphs show results running pattern size detection algorithm raidwith chunks disks actual pattern size blocks figure shows location reads assumed pattern size increased sample execution top graph figure shows timings workload run simulator sample execution shows assumed pattern blocks requests disks result timings stride minimum sample execution shows assumed pattern blocks requests disks result timing slightly higher finally assumed pattern size blocks requests disk stride results highest time detect pattern size automatically shear clusters observed completion times variant x-means cluster algorithm clustering algorithm require number clusters priori shear selects cluster greatest completion time correct pattern size calculated greatest common divisor pattern size assumptions cluster demonstrate shear detect pattern sizes con gure simulator disks remaining graphs figure desired blocks stride disks disks located disk mark length pattern boundaries chunk size step shear identi data boundaries disks chunk size data boundary occurs blocks block allocated disk block chunk size ned amount data allocated contiguously single disk shear operates assuming data boundary occurs offset pattern shear varies pattern size determined previous step shear selects patterns random creates read request offset pattern shear selects random patterns creates read request offset mod requests issued parallel completion times recorded workload repeated trials times averaged intuition correspond disk boundary requests disk workload completes slowly correspond disk boundary requests split disks complete quickly due parallelism illustrate disk raidarray figure shows portion sample execution chunk size detection algorithm top graph figure shows timings sample execution shows equal requests disks values requests disk timing data validates result requests offset faster shear automatically determines chunk size dividing observed completion times clusters k-means algorithm selecting cluster smallest completion time data points cluster correspond disk boundaries raid chunk size calculated difference boundaries show shear detect chunk sizes striping variants begin raidand constant pattern size examine disks chunks disks chunks graphs figure desired accesses slow intervals stress boundary detection zigzag striping alternating stripes allocated reverse direction scheme shown figure graph shows rst chunks stripe large expected layout previous steps shear determine pattern size chunk size step shear infers chunks repeating pattern fall disk determine chunks allocated disk shear block block block block block block block block figure chunk size detection sample execution disks block chunks shaded blocks read shear increments offset pattern requests shown accessing pattern selected random chunk size detection raid disks chunks boundary offset assumed time chunk size detection raid disks chunks boundary offset assumed time chunk size detection raid disks chunks boundary offset assumed time chunk size detection zig zag disks chunks boundary offset assumed time figure chunk size detection simulations rst graphs raidcon gurations disks chunks disks chunks disks chunks graph zig-zag striping con guration alternating stripes allocated reverse direction disks chunks examines turn pair chunks pattern shear randomly selects patterns creates read requests chunk pattern shear selects patterns creates read requests pattern requests pair issued parallel completion times recorded workload repeated trials results averaged shear examines pair figure shows results visualized interesting experiments con gure simulator model raidand zig-zag disks chunks point graph corresponds pair light points slow access times fall disk diagonal line graph corresponds pairs fall disk raidno chunks pattern allocated disk pairs shown chunk chunk chunk chunk figure read 
layout detection simulations rst graph raidthe graph zig-zag con gurations disks chunks points graph correspond pairs chunks pattern accessed simultaneously lighter points workload nished slowly chunks reside disk con ict zig-zag half pattern con icts blocks rst half shown upper-left lower-right diagonal line automatically determine chunks disk shear divides completion times clusters k-means selects cluster largest completion time shear infers chunk pairs cluster physical disk dividing chunks associative sets shear infer number primary data disks system algorithm elicits read dependencies pairs chunks running algorithm writes reads shear identify write dependencies occur due rotating mirrors chained declustering shared parity block raidor raidfor raidleft-asymmetric array figure writing blocks time result short response time operations spread disks writing blocks result longer response time share parity disk similarly writing blocks longer parity block block resides disk block write layout results reinforce conclusions read layout results distinguish raidraid- raidand chained declustering discuss write layouts provide results section redundancy fourth step shear identi redundancy managed array generally ratio random read bandwidth random write bandwidth determined disk array manages redundancy detect redundancy managed shear compares bandwidth random reads writes shear creates block-sized random reads issues parallel times pattern size detection raid pattern size assumed time chunk size detection raid boundary offset assumed time figure pattern size chunk size detection raidwe simulate raidwith disks chunks rst graph con rms pattern size graph con rms chunk size completion shear times random writes issued parallel writes performed safely needed rst reading data storage system writing values extra intervening traf ush caches ratio read write bandwidth compared expectations determine amount type redundancy storage arrays redundancy raidthe read write bandwidths expected approximately equal storage systems single mirror raidthe read bandwidth expected write bandwidth reads balanced mirrored disks writes propagate disks generally ratio read bandwidth write bandwidth exposes number mirrors systems raidparity write bandwidth roughly fourth read bandwidth small write requires reading existing disk contents parity writing values back disk raidarrays bandwidth ratio varies number disks single parity disk bottleneck makes raidmore dif cult identify discuss section problem arises redundancy detection algorithm solely reads shear writes writes conjunction reads essential shear observe difference case block read case block parity mirrors committed disk depending speci storage system test writes buffered time written stable storage systems risk data loss desktop drive reporting enabled higher-end arrays amount non-volatile ram safely delay writes acknowledged case shear avoid effects buffering move steady-state domain inducing disk writes issued manner shear achieves simple adaptive technique basic idea redundancy detection algorithm shear monitors write bandwidth write phase write performance fast previously observed read performance shear concludes pattern size detection raid left symmetric pattern size assumed time pattern size detection raid left asymmetric pattern size assumed time pattern size detection raid symmetric pattern size assumed time pattern size detection raid asymmetric pattern size assumed time figure pattern size detection raidwe simulate raidwith left-symmetric left-asymmetric right-symmetric right-asymmetric layouts con guration disks chunk size pattern size raidleft-symmetric rest writes buffered written disk round writes initiated eventually writes ood write cache induce storage system desired steadystate behavior writing data disk shear detects transition observing writes longer faster reads slower explore issue experimentation section identifying layouts finally shear pattern size chunk size read layout write layout redundancy information attempt match observations schemes raidleft-asym- metric match found shear rst re-evaluates number disks system instance number disks doubled raidand incremented raidshear completes reporting total number disks array chunk size layout observed match found shear reports discovered chunk size number disks reports speci algorithm unknown assuming chunks allocated sequentially disks shear produce suspected layout based observations figure read write layout detection raidwe simulate left raidleft-symmetric left-asymmetric right-symmetric right-asymmetric disks rst row displays read layouts row shows write layout graphs pattern size detection raid pattern size assumed time read layout chunk size detection raid boundary offset assumed time write layout figure pattern size chunk size layout detection raidwe simulate raidwith disks chunks rst graph con rms pattern size detected graph shows chunk size detected read layout graph resembles raidbut write layout graph uniquely distinguishes raidfrom parity-based schemes redundancy simulations section describe shear handles storage systems redundancy begin showing results systems parity speci cally raidraid- mirroring variants raidand chained declustering simulations storage array disks chunk size purpose comparison present base case raidin figure parity shear handles storage systems parity blocks form redundancy demonstrate variants raidraid- redundancy raidraid- calculates parity block stripe data location parity block rotated disks raidcan number layouts data parity blocks disks left-symmetric left-asymmetric right-symmetric right-asymmetric left-symmetric deliver bandwidth layout pattern size equal stripe size raidin raidlayouts pattern size times stripe size pattern size detection pattern size assumed time read layout chunk size detection boundary offset assumed time write layout figure pattern size chunk size layout detection present simulated results redundancy disks chunk size rst graph con rms pattern size detected graph shows chunk size detected read layout graph resembles raidbut write layout graph distinguishes schemes pattern size results raidsystems shown figure rst graph shows pattern size leftsymmetric identical raidthe graphs show left-asymmetric right-symmetric right-asymmetric pattern sizes chunks expected note apparent noise graphs x-means clustering algorithm correctly identify pattern sizes chunk size algorithm behave differently raidversus raidtherefore omit results figure shows read layout write layout graphs raidnote raidvariants leads distinct visual image light points correspond dependent chunk pairs slow points dark correspond independent chunk pairs offer fast concurrent access read dependence occurs chunks located disk write dependencies occur chunks reside disk share parity disk interference parity disk instances result overburdened disk longer response time graph depicts pattern-sized grid accounts pairs chunks raidleft-asymmetric read layout graph chunk chunk grid points pair chunk chunks light color chunks located disk knowledge shear identify storage system standard raidvariants calculate number disks raidraid- calculates single parity block stripe data parity blocks reside single disk pattern size chunk size read layout write layout results raidare shown figure pattern size parity disk invisible read-based workload read layout graph resembles raidresult pattern size equal stripe size disk occurs pattern hand write layout graph raidis unique parity disk bottleneck writes pairs chunks limited single disk exhibit similar pattern 
size detection raid pattern size assumed time read layout chunk size detection raid boundary offset assumed time write layout figure pattern size chunk size layout detection raidwe present simulated results raidwith disks chunk size rst graph con rms pattern size detected graph shows chunk size detected read layout write layout graphs resemble raidcompletion times bottleneck produces raidwrite layout graph allowing distinguish raidfrom parity schemes demonstrate shear handles parity schemes show results detecting pattern size chunk size redundancy raidin parity scheme stripe parity blocks calculated reed-solomon codes layout left-symmetric raidin figure rst graph shows pattern size detected graph shows chunk size figure shows read layout write layout graphs read layout graph resembles raidthe write layout graph exhibits distinct performance regions slowest time occurs requests chunk disk repeating pattern fastest time occurs requests parity updates spread evenly disks instance pairing chunks middle performance region occurs parity blocks chunk con ict data blocks chunk testing chunks half parity updates chunk fall disk chunk unique write layout distinguish parity-based schemes mirroring algorithms shear handle storage systems mirrors impact mirrors greater parity blocks read traf directed mirrors key assumption make reads balanced mirrors reads primary copy shear detect presence mirrored copies demonstrate shear handles mirroring simple raidand chained declustering raidthe results running shear disk raidsys- tem shown figure note pattern size raidis half raidgiven chunk size number disks rst graph shows raidpattern size inferred shear reads offsets pattern requests mirrors desired worst performance occurs request offset equal real pattern size case requests serviced pattern size detection chained declustering pattern size assumed time read layout chunk size detection chained declustering boundary offset assumed time write layout figure pattern size chunk size layout detection chained declustering present simulated results chained declustering disks chunk size rst graph con rms pattern size graph shows chunk size detected wider bands read layout write layout graphs show neighboring chunks mirrored total disks uniquely identi chained declustering disks illustrated fact worstcase time workload raidis half raidi seconds graph figure shows chunk size inferred shear boundary disks requests mirrors shear automatically detects disk boundary workload time increases requests disks disks mapping chunks disks single pattern con icts read layout write layout graphs figure resemble raidchained declustering chained declustering redundancy scheme disks exact mirrors disk primary instance block copy block neighbor results running shear disk system chained declustering shown figure rst graph shows pattern size detected desired raideach read request serviced disks pattern size identi requests disks system note chained declustering pattern size raidsince disk unique set data blocks graph figure shows block chunks detected ratio worstcase performance differs case raidand raidin chained declustering ratio raidand raidthe ratio chained declustering adjacent requests located disk boundary requests serviced disks raidwhen requests located chunk requests serviced disks mapping con icts chained declustering interesting shown remaining graphs figure chained declustering pair chunks located disks results distinct performance regimes case shared disks occurs chunks cyclically adjacent chunks resulting wider bands read write layout graphs raid-ls raidraid- operations millions shear overhead redundancy read pattern write pattern chunk size pattern size total raid-la figure shear overhead graph shows number generated phase shear simulated redundancy schemes shown raidraid- raidleft- symmetric raidleft-asymmetric numbers disks chunks bar plots number phase shear rightmost bar shows total raidleft-asymmetric results plotted log scale y-axis overhead examine overhead shear showing scales disks added system figure plots total number shear generates simulation variety disk con gurations x-axis vary con guration y-axis plot number generated tool note raidleft-asymmetric results shown log scale y-axis graphs make observations total number issued simple schemes raidraid- raidleft-symmetric low millions scales slowly disks added system raid schemes shear scales larger arrays run raidwith leftasymmetric layout shear generates redundancy schemes total number scale reason poor scaling behavior read layout write layout detection bars account traf illustrated figure raidleft- asymmetric pattern size grows square number disks layout algorithms issue requests pairs chunks pattern large patterns lead large numbers requests serviced parallel raidleft-asymmetric represents extreme case shear current form shear roughly days complete read layout write layout detection raidleft-asymmetric disks reduce factor ten issuing fewer disk pairwise trial reducing run time decreasing con dence layout results real platforms section present results applying shear real platforms rst linux software raid device driver adaptec hardware raid controller understand behavior shear real systems ran large variety software hardware con urations varying number disks chunk size redundancy scheme results expected revealed slightly surprising properties systems test raidmode hardware controller employs left-asymmetric parity read write ratio region size effect region size quantum atlas ibm ultrastar lzx seagate cheetah figure sensitivity region size gure plots bandwidth ratio series random read requests compared series random write requests x-axis varies size region experiment run run sector-sized read write requests issued lines plotted disks quantum atlas wls ibm lzx seagate cheetah bandwidth amount written effect write buffering figure avoiding write buffer gure plots performance writes top raidhardware writebuffering enabled x-axis varies number writes issued y-axis plots achieved bandwidth placement due space constraints concentrate challenging aspect shear redundancy detection experimenting redundancy detection uncovered issues addressed produce robust algorithm rst size region test run figure plots read write ratio single disk size region varied gure size region test conducted strongly uence outcome tests quantum disk desired ratio roughly achieved small region sizes ratio grows disk reason undesirable ation large settling time quantum disk conclude redundancy detection algorithm run small portion disk odds desire run small portion disk issue presence writeback cache raid adaptec card congured perform write buffering host writes complete quickly disk time note presence buffer affect data integrity buffer non-volatile redundancy detection algorithm issue write requests disk compare read request timings shear raid-la raidraid- raidr ratio read write bandwidth ratios software raid hardware raid figure redundancy detection gure plots ratio read write bandwidth variety disk con gurations x-axis varies number disks con guration raidraid- raidor raidleft-asymmetric software hardware raid circumvent caching effects recall shear simple adaptive scheme detect bypass buffering issuing successive rounds write requests monitoring performance point write bandwidth decreases indicating raid system moved steady-state writing data disk memory reliable result generated figure demonstrates technique 
adaptec hardware raid adapter write caching enabled enhancements place study redundancy detection software hardware raid systems figure plots read bandwidth write bandwidth ratio number con gurations recall read write ratio key differentiating redundancy scheme ratio redundancy ratio mirrored scheme ratio raidstyle parity encoding note hardware raid card support raidand con gure raidon disks gure shows shear redundancy detection good job identifying scheme expected read write ratios approximately raidnear raidand raidthere points make bandwidth ratios raidscale number disks due parity disk bottleneck makes dif cult identify raidarrays rely write layout test previously exhibits bottleneck write performance unique results write layout test distinguish raidfrom parity-based schemes note performance software raidon disks expected read write ratio measure ratio tracing disk activity inspecting source code revealed linux software raid controller utilize usual raidsmall write optimization reading block parity writing block parity read entire stripe blocks write block parity finally graph shows raidwith disks -disk mirrored system distinguishable disks raidand mirroring converge shear applications section illustrate bene shear begin showing shear detect raid conguration errors disk failures show shear discover information individual disks array finally present storage system paramefigure detecting miscon gured layouts raidleft-symmetric left-asymmetric right-symmetric rightasymmetric upper graph shows read layout graph raid ibm disks correctly con gured lower graphs show read layout logical partitions miscon ured physical device chunk size detection raid boundary offset assumed time figure detecting heterogeneity rst graph shows output chunk size detection algorithm run array single heterogeneous fast rotating disk row gures shows results read layout algorithm simulated disk con gurations con guration single disk capability fast rotating slow rotating fast seeking slow seeking disk depicted gures ters uncovered shear tune system speci cally show system improve sequential bandwidth writing data full stripes shear management intended shear administrative utility discover con guration performance safety problems figure shows failure identify scheme suggest storage miscon guration upper set graphs expected read layout graphs common raidlevels lower resulting read layout graphs disk array miscon gured logical partitions reside physical disk graphs generated disk arrays comprised logical disks built linux software raid ibm disks visualization makes obvious manual inspection shear automatically determines results match existing schemes shear detect unexpected performance heterogeneity disks experiment run shear range simulated heterogeneous disk con gurations exchunk size detection raid left symmetric boundary offset assumed time chunk size detection raid left symmetric boundary offset assumed time figure detecting failure chunk size detection algorithm shear discover failed devices raid system upper graph shows initial chunk size detection results collected building disk software raid system ibm disks lower graph system fth disk removed periments disk slower faster rest figure shows results run heterogeneous con gurations gure faster slower disk makes presence obvious ways read layout graphs chunk size detection output pattern size detection unaffected administrator view outputs observe unexpected performance differential disks action correct problem finally chunk size detection algorithm shear identify safety hazards determining redundant array operating degraded mode figure shows chunk size detection results ten disk software raid system ibm disks upper graph shows chunk size detection correctly working array rst built lower graph shows chunk size detection changed physically remove fth disk array recall chunk size detection works guessing boundaries timing sets requests sides boundary vertical downward spikes half height plateaus guessed boundary correct requests serviced parallel disks plateaus false boundaries requests sides guessed boundary incurred disk lower graph identi array operating degraded mode boundary points missing disk disappear plateau higher due extra overhead performing on-they reconstruction shear disk characterization related projects concentrated extracting speci properties individual disk drives techniques built top characteristic knowledge aligning les track boundaries free-block scheduling shear enables optimizations context storage arrays shear expose boundaries disks existing tools determine speci properties individual disks demonstrate ability skippy disk characterization tool skippy sequence write operations inwrite time request number skippy disk write time request number skippy raiddisks write time request number skippy raiddisks figure skippy gures plot results running skippy disk characterization tool single quantum disk disk raidarray disk raidarray creasing strides determine disk sector track ratio rotation time head positioning time head switch time cylinder switch time number recording surfaces rst graph figure shows pattern generated skippy single quantum disk graph figure shows results running modi version skippy raidarray disks version skippy array information provided shear map block stream logical blocks residing rst disk array results pattern identical running single disk allowing extract individual disk parameters nal graph figure shows results technique applied disk raidarray results identical single disk pattern small perturbations affect analysis limitations approach case raidthe skippy write workload performs expected read workload produces spurious results due fact reads balanced disks conversely reads work raidwhereas writes due update parity information additionally parity blocks raidcannot directly accessed characterization tools obtain incomplete set data limitations tested read-based version skippy raidand successfully extracted parameters individual disks shear performance stripe size disk array large impact performance effect important raidstorage writes complete stripe require additional previous work focused selecting optimal stripe size workload show system adapt size alignment writes function stripe size bandwidth average file size effects stripe-alignment stripe-aligned default figure bene stripe alignment gure plots bandwidth series creations average size varied x-axis variants shown system generates stripe-sized writes default linux workload consists creating les x-axis size les uniformly distributed basic idea system adjust writes stripe aligned optimization occur multiple places modi linux device scheduler properly coalesces divides individual requests raid stripe-sized units modi cation straight-forward lines code added kernel simple change make system stripe-aware leads tremendous performance improvements experiments shown figure run hardware raidcon guration quantum disks chunk size results show stripe-aware system noticeably improves bandwidth moderately-sized les improves bandwidth larger les factor related work idea providing software automatically uncover behavior underlying software hardware layers explored number domains earliest work area targeted memory subsystem measuring time reads amounts strides saavedra smith reveal interesting aspects memory hierarchy including details caches tlbs similar techniques applied identify aspects tcp protocol stack determine processor cycle time cpu scheduling policies work related targeted characterizing single disk storage system worthington identify characteristics disks mapping logical block numbers physical locations costs low-level operations 
size prefetch window prefetching algorithm caching policy schindler talagala build similar portable tools achieve similar ends shown shear conjunction low-level tools discover properties single disks inside arrays evaluations storage systems focused measuring performance workload uncovering underlying properties interesting synthetic benchmark adapts behavior underlying storage system benchmark examines sensitivity parameters size requests read write ratio amount concurrency finally idea detailed storage-systems knowledge system storage client investigated schindler investigate concept track-aligned placement single disk systems work modi system allocates medium-sized les track boundaries avoid head switches deliver low-latency access les systems lfs atropos augment array interface provide information individual disks lfs knowledge disk boundaries dynamically allocate writes based performance control redundancy perle basis atropos volume manager extends storage interface expose disk boundary track information provide cient semi-sequential access two-dimensional data structures shear enables information multiple disk systems enhanced interface conclusions presented shear tool automatically detects important characteristics modern storage arrays including number disks chunk size level redundancy layout scheme keys shear randomness extract steady-state performance statistical techniques deliver automated reliable detection veri shear works desired series simulations variety layout redundancy schemes subsequently applied shear software hardware raid systems revealing properties speci cally found linux software raid exhibits poor performance raidparity updates adaptec raid adapter implements raidleft-asymmetric layout shown shear case studies storage administrators shear verify properties storage arrays monitor performance detect disk failures shear extract individual parameters disks array enabling performance enhancements previously limited single disk systems finally shown factor improvement performance system tuning writes stripe size raid storage storage systems computer systems general complex layers interacting components remain concealed veil simplicity hope techniques developed shear reveal true power future systems subsequently make manageable composable cient acknowledgements bradford beckmann nathan burnett vijayan prabhakaran muthian sivathanu anonymous reviewers excellent feedback work sponsored nsf ccrccr- ccrngs- itritr- ibm emc wisconsin alumni research foundation bray bonnie file system benchmark http textuality bonnie burnett bent arpaci-dusseau arpaci-dusseau exploiting gray-box knowledge buffer-cache contents proceedings usenix annual technical conference usenix pages monterey california june chen lee striping raid level disk array proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages ottawa canada chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june chen patterson maximizing performance striped disk array proceedings annual international symposium computer architecture isca pages seattle washington chen patterson approach performance evaluation self-scaling benchmarks predicted performance proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages santa clara california denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june emc corporation symmetrix enterprise information storage systems http emc glaser tcp stack fingerprinting principles http sans newlook resources idfaq tcp ngerprinting htm october grochowski emerging trends data storage magnetic hard disk drives datatech september hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings international conference data engineering icde pages los angeles california february katcher postmark file system benchmark technical report trnetwork appliance october lee katz performance consequences parity placement disk arrays proceedings international conference architectural support programming languages operating systems asplos pages santa clara california april lumb schindler ganger nagle riedel higher disk head utilization extracting free bandwidth busy disk drives proceedings symposium operating systems design implementation osdi pages san diego california october norcutt iozone filesystem benchmark http iozone padhye floyd identifying tcp behavior web servers proceedings sigcomm san diego california august patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june pelleg moore x-means extending k-means cient estimation number clusters proceedings international conference machine learning june regehr inferring scheduling behavior hourglass proceedings usenix annual technical conference freenix track monterey california june ruemmler wilkes introduction disk drive modeling ieee computer march saavedra smith measuring cache tlb performance effect benchmark runtimes ieee transactions computers savage wilkes afraid frequently redundant array independent disks proceedings usenix annual technical conference usenix pages san diego california january schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon november schindler grif lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix symposium file storage technologies fast monterey california january schindler schlosser shao ailamaki ganger atropos disk array volume manager orchestrated disks proceedings usenix symposium file storage technologies fast san francisco california april staelin mcvoy mhz anatomy micro-benchmark proceedings usenix annual technical conference usenix pages orleans louisiana june talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley varki merchant qiu issues challenges performance analysis real disk arrays ieee transactions parallel distributed systems june wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february worthington ganger patt wilkes on-line extraction scsi disk drive parameters proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages ottawa canada 

operating stockton gaines systems editor time clocks ordering events distributed system leslie lamport massachusetts computer associates concept event happening distributed system examined shown define partial ordering events distributed algorithm synchronizing system logical clocks totally order events total ordering illustrated method solving synchronization problems algorithm specialized synchronizing physical clocks bound derived synchrony clocks key words phrases distributed systems computer networks clock synchronization multiprocess systems categories introduction concept time fundamental thinking derived basic concept order events occur happened occurred clock read read concept temporal ordering events pervades thinking systems airline reservation system request reservation granted made flight filled concept carefully reexamined events distributed system general permission make fair teaching research part material granted individual readers nonprofit libraries acting provided acm copyright notice made date issue fact reprinting privileges granted permission association computing machinery reprint figure table substantial excerpt entire work requires specific permission republication systematic multiple reproduction work supported advanced research projects agency department defense rome air development center monitored rome air development center contract number -cauthor address computer science laboratory sri international ravenswood ave menlo park acm distributed system consists collection distinct processes spatially separated communicate exchanging messages network interconnected computers arpa net distributed system single computer viewed distributed system central control unit memory units input-output channels separate processes system distributed message transmission delay negligible compared time events single process concern primarily systems spatially separated computers remarks apply generally multiprocessing system single computer involves problems similar distributed system unpredictable order events occur distributed system impossible events occurred relation happened partial ordering events system found problems arise people fully aware fact implications paper discuss partial ordering defined happened relation give distributed algorithm extending consistent total ordering events algorithm provide mechanism implementing distributed system illustrate simple method solving synchronization problems unexpected anomalous behavior occur ordering obtained algorithm differs perceived user avoided introducing real physical clocks describe simple method synchronizing clocks derive upper bound synchrony drift partial ordering people event happened event happened earlier time justify definition terms physical theories time system meet specification correctly specification terms events observable system specification terms physical time system real clocks real clocks problem clocks perfectly accurate precise physical time define happened relation physical clocks begin defining system precisely assume system composed collection processes process consists sequence events depending application execution subprogram computer event execution single machine instruction communications july volume acm number fig event assuming events process form sequence occurs sequence words single process defined set events priori total ordering generally meant process trivial extend definition process split distinct subprocesses bother assume sending receiving message event process define happened relation denoted --as definition relation ---on set events system smallest relation satisfying conditions events process sending message process receipt message process --c distinct events concurrent assume event systems event happen physically meaningful implies irreflexive partial ordering set events system helpful view definition terms space-time diagram figure horizontal direction represents space vertical direction represents time--later times higher earlier dots denote events vertical lines denote processes wavy lines denote messagesfl easy means choice constitutes event affects ordering events process receipt message denote setting interrupt bit computer execution subprogram handle interrupt interrupts handled order occur choice affect ordering process message-receiving events observe messages received order sending messages single event convenience assume receipt single message coincide sending receipt message fig diagram moving forward time process message lines figure viewing definition means event causally affect event events concurrent causally affect events figure concurrent drawn diagram imply occurs earlier physical time process process receives message event planning definition natural reader familiar invariant space-time formulation special relativity chapter relativity ordering events defined terms messages pragmatic approach messages determine system performed correctly knowing events occur knowing events occurred logical clocks introduce clocks system begin abstract point view clock assigning number event number thought time event occurred precisely define clock process function assigns number event process entire system ofclbcks represented function assigns event number ifb event process make assumption relation numbers physical time clocks logical physical clocks implemented counters actual timing mechanism communications july volume acm number fig ilql means system clocks correct base definition correctness physical time require introducing clocks physical time definition based order events occur strongest reasonable condition event occurs event happen earlier time state condition formally clock condition events a--b note expect converse condition hold imply concurrent events occur time figure concurrent occur time contradict clock condition ----it easy definition relation --that clock condition satisfied conditions hold events process sending message process receipt message process clocks terms space-time diagram imagine process clock ticks number ticks occurring process events consecutive events process clock ticks occur events draw dashed tick line likenumbered ticks processes spacetime diagram figure yield picture figure condition means tick line events process line condition means message line cross tick line pictorial meaning of--it easy conditions imply clock condition tick lines time coordinate lines cartesian coordinate system spacetime redraw figure straighten coordinate lines obtaining figure figure valid alternate representing system events figure introducing concept physical time system requires introducing physical clocks decide pictures representation reader find helpful visualize twodimensional spatial network processes yields three-dimensional space-time diagram processes messages represented lines tick lines two-dimensional surfaces assume processes algorithms events represent actions execution show introduce clocks processes satisfy clock condition process clock represented register contained event change events changing constitute event guarantee system clocks satisfies clock condition insure satisfies conditions condition simple processes obey implementation rule process increments successive events meet condition require message timestamp equals time message receiving message timestamped tin process advance clock tin precisely rule event sending message process message timestamp receiving message process sets greater equal present greater tin event represents receipt message occur setting notational nuisance irrelevant actual implementation insures satisfied simple implementation rules imply clock condition satisfied guarantee correct system logical clocks ordering events totally system clocks satisfying clock condition place total ordering set system events simply order events times communications july volume acm number occur break ties arbitrary total ordering processes precisely define relation event process event process ---cj easy defines total ordering clock condition implies ---b words relation completing happened partial ordering total ordering ordering depends 
system clocks unique choices clocks satisfy clock condition yield relations total ordering relation extends --there system clocks satisfying clock condition yields relation partial ordering uniquely determined system events totally order events implementing distributed system fact reason implementing correct system logical clocks obtain total ordering illustrate total ordering events solving version mutual exclusion problem system composed fixed collection processes share single resource process resource time processes synchronize avoid conflict find algorithm granting resource process satisfies conditions process granted resource release granted process requests resource granted order made iii process granted resource eventually releases request eventually granted assume resource initially granted process perfectly natural requirements precisely means solution correct observe conditions involve ordering events condition concurrently issued requests granted important realize nontrivial problem central scheduling process grants requests order received work additional assumptions made scheduling process suppose sends request sends message receiving message sends request request reach request condition violated request granted solve problem implement system ordering establishes priority processes fairer method desired made function clock andj ifj mod total number processes term eventually made precise require long diversion main topic clocks rules define total ordering events total ordering request release operations ordering finding solution straightforward exercise involves making process learns processes operations simplify problem make assumptions essential introduced avoid distracting implementation details assume processes messages received order assume message eventually received assumptions avoided introducing message numbers message acknowledgment protocols assume process send messages directly process process maintains request queue process assume request queues initially single message requests resource process initially granted resource initial clock algorithm defined rules convenience actions defined rule assumed form single event request resource process sends message tin requests resource process puts message request queue timestamp message process receives message requests resource places request queue sends timestamped acknowledgment message release resource process removes requests resource message request queue sends timestamped releases resource message process process receives releases resource message removes requests resource message request queue process granted resource conditions satisfied requests resource message request queue ordered request queue relation define relation messages identify message event sending received message process timestamped tin note conditions rule tested locally easy verify algorithm defined rules satisfies conditions i-iii observe condition rule assumption messages received order guarantees learned requests preceded current acknowledgment message message timestamped received message timestamped communications july volume acm number request rules delete messages request queue easy condition holds condition fact total ordering extends partial ordering --rule guarantees requests resource condition rule eventually hold rules imply process granted resource eventually releases condition rule eventually hold proving condition iii distributed algorithm process independently rules central synchronizing process central storage approach generalized implement desired synchronization distributed multiprocess system synchronization terms state machine consisting set commands set states function s-s relation means executing command machine state machine state change set consists commands requests resource releases resource state consists queue waiting request commands request head queue granted executing request command adds request tail queue executing release command removes command queue process independently simulates execution state machine commands issued processes synchronization achieved processes order commands timestamps relation process sequence commands process execute command timestamped learned commands issued processes timestamps equal precise algorithm straightforward bother describe method implement desired form multiprocess synchronization distributed system resulting algorithm requires active participation processes process commands issued processes failure single process make impossible process execute state machine commands halting system problem failure difficult scope paper discuss detail observe entire concept failure meaningful context physical time physical time distinguish failed process pausing events user system crashed waiting long response method works failure individual processes communication lines process strictly alternate request release commands executing release command delete request queue anomalous behavior resource scheduling algorithm ordered requests total ordering permits type anomalous behavior nationwide system interconnected computers suppose person issues request computer telephones friend city issue request computer request receive lower timestamp ordered request happen system knowing preceded precedence informatiori based messages external system examine source problem closely set system events introduce set events events relevant external events calls denote happened relation obvious algorithm based events relate events events guarantee request ordered request ways avoid anomalous behavior explicitly introduce system information ordering person issuing request receive timestamp request system issuing request friend timestamp user responsibility avoiding anomalous behavior approach construct system clocks satisfies condition strong clock condition events ifa stronger ordinary clock condition stronger relation --it general satisfied logical clocks identify set real events physical space-time partial ordering events defined special relativity mysteries universe construct system physical clocks running independently satisfy strong clock condition physical clocks eliminate anomalous behavior turn attention clocks physical clocks introduce physical time coordinate space-time picture denote reading clock physical time mathematical conwe assume newtonian space-time relative motion clocks gravitational effects negligible deduced actual clock reading transforming proper time arbitrarily chosen time coordinate communications july volume acm number venience assume clocks run continuously discrete ticks discrete clock thought continuous error tick reading precisely assume continuous differentiable function isolated jump discontinuities clock reset dcg represents rate clock running time order clock true physical clock run approximately correct rate dci precisely assume condition satisfied pci exists constant dcg typical crystal controlled clocks clocks individually run approximately correct rate synchronized precisely sufficiently small constant condition holds vertical distance figure represent physical time states variation height single tick line clocks run rate tend drift devise algorithm insure holds examine small prevent anomalous behavior insure system relevant physical events satisfies strong clock condition assume clocks satisfy ordinary clock condition require strong clock condition holds events events occurring processes number event occurs physical time event process satisfies occurs physical time words shortest transmission time interprocess messages choose equal shortest distance processes divided speed light depending messages transmitted significantly larger avoid anomalous behavior make cat combining relate required smallness assume clock reset set forward back setting back violated pci implies easy deduce inequality holds inequality implies anomalous behavior impossible describe algorithm insuring holds message physical time received time define total delay message delay process receives assume receiving process minimum delay tzm pro call unpredictable delay message specialize rules iri physical clocks receive message physical time differentiable 
dcg sends message physical time timestamp receiving message time process sets equal maximum rules formally terms physical time parameter process clock reading timestamps messages receives mathematical convenience assuming event occurs precise instant physical time events process occur times rules specializations rules system clocks satisfies clock condition fact real events finite duration difficulty implementing algorithm real concern implementation making discrete clock ticks frequent maintained show clock synchronizing algorithm satisfy condition assume system processes directed graph arc process process represents communication line messages directly message arc seconds sends message physical times diameter directed graph smallest number pair distinct processes path arcs addition establishing theorem bounds length time clocks synchronized system started theorem assume strongly connected graph processes diameter obeys rules assume message constant holds constants seconds message unpredictable delay arc satisfied approximations assume proof theorem surprisingly difficult appendix great deal work problem synchronizing physical clocks refer reader introc lim communications july volume acm number duction subject methods literature estimating message delays ktm adjusting clock frequencies dci clocks permit adjustment requirement clocks set backwards distinguish situation previously studied theorem result conclusion concept happening defines invariant partial ordering events distributed multiprocess system algorithm extending partial ordering arbitrary total ordering showed total ordering solve simple synchronization problem future paper show approach extended solve synchronization problem total ordering defined algorithm arbitrary produce anomalous behavior disagrees ordering perceived system users prevented properly synchronized physical clocks theorem showed closely clocks synchronized distributed system important realize order events occur partial ordering idea understanding multiprocess system understand basic problems multiprocessing independently mechanisms solve appendix proof theorem define clock set equal time runs rate reset words cit dcz dtldt note cit suppose process time sends message process received time unpredictable delay pci cfftl assumptions suppose time process sends message process received time unpredictable delay repeated application inequality yields result iri deduce combining processes find sequence processes communication arcs hypothesis find times inequality form holds message timestamped tin suppose time received time pretend clock runs constant rate implies dcm rule simply sets maximum clocks reset setting equal clocks time clock largest time clocks run rate cases clock process clock message time process case simply case dcm yields pci choice txm definition communications july volume acm number letting case combine deduce process time choosing combine conclude exists process letting t-tl combining hypotheses rewrite approximate inequality -idkr holds cat holds note relation proof yields exact upper bound case assumption invalid examination proof suggests simple method rapidly initializing clocks resynchronizing synchrony reason process sends message relayed process procedure initiated process requires seconds effect synchronization assuming messages unpredictable delay programming homing languages editor shallow binding lisp henry baker massachusetts institute technology shallow binding scheme variable accessed bounded amount computation elegant model shallow binding lisp presented context-switching environment tree transformation called rerooting rerooting completely general reversible optional sense lisp interpreter operate correctly rerooting invoked context change rerooting leaves assoc invariant variables environments programmer access rerooting primitive shallow dynamic control accesses shallow deep affects speed execution program semantics addition multiple processes active environment structure long rerooting indivisible operation finally concept rerooting shown combine concept shallow binding lisp dijkstra display algol general model shallow binding key words phrases lisp environment trees funarg shallow binding deep binding multiprogramming algol display categories acknowledgment timestamps order operations concept anomalous behavior due paul johnson robert thomas received march revised october schwartz relativity lllustrations york press york taylor wheeler space-time physics freeman san francisco lamport implementation reliable distributed multiprocess systems computer networks ellingson kulpinski dissemination system-time eee trans comm commay general permission make fair teaching research part material granted individual readers nonprofit libraries acting provided acm copyright notice made date issue fact reprinting privileges granted permission association computing machinery reprint figure table substantial excerpt entire work requires specific permission republication systematic multiple reproduction research supported advanced research projects agency department defense monitored office naval research contract number -cauthor present address computer science department rochester rochester acm communications july volume acm number 
petal distributed virtual disks edward lee chandramohan thekkath systems research center digital equipment corporation lytton ave palo alto abstract ideal storage system globally accessible unlimited performance capacity large number clients requires management paper describes design implementation performance petal system attempts approximate ideal practice combination features petal consists collection networkconnected servers cooperatively manage pool physical disks petal client collection appears highly block-level storage system large abstract containers called virtual disks virtual disk globally accessible petal clients network client create virtual disk demand tap entire capacity performance underlying physical resources additional resources servers disks automatically incorporated petal initial petal prototype consisting mhz dec workstations running digital unix connected mbit atm network prototype clients virtual disks tolerate recover disk server network failures latency comparable locally attached disk throughput scales number servers prototype achieve rates requests sec bandwidth mbytes sec introduction managing large storage systems expensive complicated process single component failure halt entire system requires considerable time effort resume operation capacity performance individual components system periodically monitored balanced reduce fragmentation eliminate hot spots requires manually moving partitioning replicating files directories paper describes design implementation performance petal easy-to-manage distributed storage system clients file systems databases view petal collection virtual disks shown figure petal virtual disk published proceedings international conference architectural support programming languages operating systems copyright assocation computing machinery rights reserved republished permission scalable networka fsa fsa bsd ffsa petal dev vdisk dev vdisk dev vdisk dev vdisk dev vdisk petal client petal client petal client petal client lfs virtual diska figure client view container sparse -bit byte storage space ordinary magnetic disks data read written petal virtual disks blocks addition combination characteristics reduce complexity managing large storage systems tolerate recover single component failure disk server network geographically distributed tolerate site failures power outages natural disasters transparently reconfigures expand performance capacity servers disks added uniformly balances load capacity servers system fast efficient support backup recovery environments multiple types clients file servers databases petal virtual disks cleanly separate client view storage physical resources implement share physical resources flexibly clients offer important services snapshots incremental expandability efficient manner disk-like interface offered petal lower-level service distributed file system distributed file system efficiently implemented top petal resulting system cost scalable networka fsa fsa bsd ffsa lfsa storage servera storage servera storage servera storage servera disk storagea disk storagea disk storagea disk storagea figure physical view effective comparable distributed file system implementation accesses local disks directly separating system cleanly block-level storage system file system handling distributed systems problems block-level storage system system easier model design implement tune simplicity important design expected scale large size provide reliable data storage long period time additional benefit block-level interface supporting heterogeneous clients client applications easily support types file systems databases implemented petal servers alpha workstations running digital unix connected digital atm network petal client interface exists digital unix implemented kernel device driver allowing standard unix applications utilities file systems run unmodified petal implementation exhibits graceful scaling performance comparable local disks providing significant functionality design petal shown figure petal consists pool distributed storage servers cooperatively implement single block-level storage system clients view storage system collection virtual disks access petal services remote procedure call rpc interface basic principle design petal rpc interface maintain state needed ensuring integrity storage system servers maintain hints clients clients maintain small amount high-level mapping information route read write requests server request inappropriate server server returns error code causing client update hints retry request figure illustrates software structure petal ovals represents software module arrows module modules liveness module global state module manage distributed system aspect petal liveness module ensures servers system agree operational status running crashed service modules notably global state manager guarantee continuous consistent operation system face server communication failures operation liveness module based majority recovery moduleliveness module global statea module data access module virtual physicala translator figure petal server modules consensus periodic exchange alive alive messages servers message exchanges timely manner ensure progress arbitrarily delayed reordered affecting correctness petal maintains information describes current members storage system supported virtual disks information replicated petal servers system global state manager responsible consistently maintaining information megabyte current implementation algorithm maintaining global state based leslie lamport paxos part-time parliament algorithm implementing distributed replicated state machines algorithm assumes servers fail ceasing operate networks reorder lose messages algorithm ensures correctness face arbitrary combinations server communication failures recoveries guarantees progress long majority servers communicate ensures management operations petal creating deleting snapshotting virtual disks adding deleting servers fault tolerant modules deal servicing read write requests issued petal clients data access recovery modules control client data distributed stored petal storage system set data access recovery modules exists type redundancy scheme supported system support simple data striping redundancy replication-based redundancy scheme called chaineddeclustering desired redundancy scheme virtual disk virtual disk created subsequently redundancy scheme attributes transparently changed process called virtual disk reconfiguration virtual-to-physical address translation module common routines data access recovery modules routines translate virtual disk offsets physical disk addresses rest section examine specific aspects system greater detail virtual physical translation section describes petal translates virtual disk addresses clients physical disk addresses basic problem translate virtual addresses form virtualdisk-identifier offset physical addresses form serveridentifier disk-identifier disk-offset translation consistently efficiently distributed system events alter virtual disk address translation server gmapa server server server server offset serverid gmap gmapa pmap pmap pmap pmap vdira vdira vdira vdira vdiskid vdiskid offset serverid diskid diskoffset diskid diskoffset figure virtual physical mapping failure recovery occur unexpectedly figure illustrates basic data structures steps translation procedure important data structures virtual disk directory vdir global map gmap physical map pmap dotted lines virtual disk directory global map global data structures replicated consistently updated servers global state manager server physical map local server translating client-supplied virtual disk identifier offset disk offset occurs steps shown figure virtual disk directory translates client-supplied virtual disk identifier global map identifier global map determines server responsible translating offset physical map server translates global map identifier offset physical disk offset disk minimize communication cases server performs translation step server performs translation step client initially request server server perform steps translation locally communicating server global map virtual 
disk specifies tuple servers spanned virtual disk redundancy scheme protect client data stored virtual disk tolerate server failures secondary server assigned responsibility mapping offset primary global maps immutable change virtual disk tuple servers redundancy scheme virtual disk assigned global map section describing reconfiguration details process physical map actual data structure translate offset virtual disk physical disk offset disk similar page table virtual memory system physical map entry translates kbyte region physical disk server performs translation perform disk operations needed service original client request separation translation data structures global local physical maps bulk mapping information local minimizes amount information global data structures replicated expensive update support backup petal attempts simplify client backup procedure providing common mechanism applied clients automate backup recovery data stored system mechanism petal fast efficient snapshots virtual disks copy-on-write techniques petal quickly create exact copy virtual disk point time client treats snapshot virtual disk modified supporting snapshots requires slightly complicated virtual-to-physical translation procedure previous section virtual disk directory translate virtual disk identifier global map identifier tuple global-map-identifier epoch-number epoch-number monotonically increasing version number distinguishes data stored virtual disk offset points time tuple global-map-identifier epoch-number physical map step translation system creates snapshot virtual disk tuple epoch number created virtual disk directory accesses original virtual disk made epoch number older epoch number newly created snapshot ensures data written original virtual disk create entries epoch overwriting data previous epoch read requests find data recently written offset recent epoch creating snapshot consistent client application level requires pausing application time takes create petal snapshot alternative approach require pausing application create crash-consistent snapshot snapshot similar disk image left application crashed snapshots made consistent application level running application-dependent recovery program fsck case unix file systems implementing crash-consistent snapshots supported snapshots on-line facilitate recovery accidentally deleted files snapshot behaves read-only local disk petal client create consistent archives data utilities tar incremental reconfiguration occasionally desirable change virtual disk redundancy scheme set servers mapped change precipitated addition removal disks servers section describes petal incorporates disks servers existing virtual disks reconfigured advantage resources processes point view adding resources easily generalized removal resources process referred virtual disk reconfiguration primary focus section addition disk server handled locally server subsequent storage allocation requests automatically disk consideration load balance desirablea redistribute previously allocated storage disk redistribution easily accomplished part local background process periodically moves data disks implemented background process petal nonetheless existing data redistributed newly added disks side-effect virtual disk reconfiguration addition petal server global operation composed steps involving global state management module liveness module server added membership petal storage system server participate future global operations sets servers liveness module determining server adjusted incorporate server finally existing virtual disks reconfigured advantage server process virtual-to-physical translation procedure section absence activity system virtual disk reconfiguration trivially implemented create global map desired redundancy scheme server mapping change virtual disk directory entries refer global map refer redistribute data servers translations global map data distribution potentially require substantial amounts network disk traffic challenge perform reconfiguration incrementally concurrently processing normal client requests find acceptable procedure takes hours degrade performance system significantly virtual disk reconfigured server added performance virtual disk gradually increase reconfiguration level reconfiguration level reconfiguration describe reconfiguration algorithm steps describe basic algorithm refinement algorithm refined algorithm implemented system basic algorithm steps executed starting translations recent epoch moved data transferred collection servers global map amount data moved reconfiguration long time complete meantime clients read write data virtual disk reconfigured accommodate requests read write procedures designed function client read request serviced global map translation found global map ensures translations moved found global map client write requests access global map move data starting recent epoch ensure read requests return data older epoch requested client main limitation basic algorithm server mappings entire virtual disk changed data moved means client read request submitted based global map miss global server server server server virtual disk figure chained-declustering map forwarded require additional communication servers potential degrade performance system refined algorithm solves limitationof basic algorithm relocating small portions virtual disk time basic idea break virtual disk address range regions fenced requests regions simply global maps requests fenced region basic algorithm relocated fenced region region fence part region repeat moved data region region keeping relative size fenced region small roughly ten percent entire range minimize forwarding overhead guard fencing heavily subrange virtual disk construct fenced region collecting small non-contiguous ranges distributed virtual disk single contiguous region data access recovery section describes petal chained-declustered data access recovery modules modules give clients highly access data automatically bypassing failed components dynamic load balancing eliminates system bottlenecks ensuring uniform load distribution face component failures start describing basic idea chained-declustering move detailed descriptions read write operation figure illustrates chained-declustered data placement scheme dotted rectangle emphasizes data storage servers single virtual disk clients sequence letters represents block data stored storage system note copies block data stored neighboring servers pair neighboring servers data blocks common arrangement server fails servers automatically share server read load server experience load increase performing dynamic load balancing server copies data servers servers offload normal read load server achieve uniform load balancing chaining data placement server offload read load server immediately preceding server cascading offloading multiple serva ers uniform load maintained surviving servers contrast simple mirrored redundancy scheme replicates data stored servers failure result load increase opportunities dynamic load balancing system stripes mirrored servers load increase single server reduce system throughput current prototype implements simple dynamic load balancing scheme client track number requests pending server sends read requests server shorter queue length works requests generated clients work requests generated clients occasionally issue requests choice load balancing algorithm active area research petal project additional advantage chained-declustering placing even-numbered servers site oddnumbered servers site tolerate site failures disadvantage chained-declustering relative simple mirroring reliable simple mirroring server failed failure mirror server result data unavailable chained-declustering server fails failure neighboring servers result data unavailable implementation chained-declustering copies data block denoted primary denoted secondary read requests serviced primary secondary copy servicing write requests start primary server primary case start secondary lock copies data blocks reading writing guarantee consistency ordering guarantee avoid deadlocks read request server 
receives request attempts read requested data successful server returns requested data returns error code client server request times due network congestion server client alternately retry primary secondary servers request succeeds servers return error codes indicating satisfy request disks copies requested data destroyed write request server receives request checks primary data element primary marks data element busy stable storage simultaneously sends write requests local copy secondary copy requests complete busy bit cleared client issued request status code indicating success failure operation primary crashes performing update busy bits crash recovery ensure primary secondary copies consistent write-ahead-logging group commits makes updating busy bits efficient optimization clearing busy bits lazily maintain cache recently set busy bits write requests display locality busy bit set disk require additional server received write request secondary data element service request determine server primary copy petal clienta petal clienta petal clienta petal clienta petal virtual diska loga petal servera loga petal servera loga petal servera loga petal servera digital atm networka figure petal prototype case secondary marks data element stale stable storage writing local disk server primary copy eventually bring data elements marked stale up-to-date recovery process similar procedure primary secondary dies implementation performance petal prototype illustrated figure mhz dec running digital unix act server machines runs single petal server user-level process accesses physical disks unix raw disk interface network udp unix sockets server machine configured digital disks inch scsi device gbyte capacity machine disks write-ahead logging remaining store client data disks connected server machine mbyte fast scsi strings digital pmzaa-c host bus adapter additional machines running digital unix configured petal clients generate load servers client kernel loaded petal device driver accessing petal virtual disks clients access petal virtual disks local disks servers clients connected mbit atm links digital atm network entire petal rpc interface calls calls devoted management functions creating deleting virtual disks making snapshots reconfiguring virtual disk adding deleting servers calls typically user-level utilities perform tasks virtual disk creation monitoring physical resource pools system determine additional servers disk added petal rpc calls implement management functions infrequently executed generally complete create snapshot operations milliseconds delete reconfiguration milliseconds initiate total execution time dependent actual amount physical storage virtual disk remainder section report performance accessing petal virtual disk behavior file systems built petal primary performance goals provide latency roughly comparable locally attached disk throughclient request latency request local disk petal log nvram log byte read kbyte read kbyte read byte write kbyte write kbyte write table latency chained-declustered virtual disk put scales number servers performance gracefully degrades servers fail petal performance section examines read write performance petal chain-declustered virtual disk read request client makes rpc petal server simply returns data local disk server receives write request writes small log entry recover consistent state server crash server simultaneously writes data local disk disk mirror server disk writes complete primary server replies client read write procedures petal greater detail section table compares read write latency chaineddeclustered petal virtual disk local disk experiment single client generates requests size random disk offsets show petal performance kinds write-ahead-logging devices disk nvram device simulated ram log device service write requests affect read performance logging nvram improves write latency approximately read requests bytes kbytes petal latency slightly worse kbyte reads latency gap widens increased latency due additional delay transmitting data network includes unix socket udp atm hardware overheads accounts petal server software client interface overheads negligible overlapped reading data disks transfer data network eliminate overhead nvram log device petal write performance worse local disk addition network delay sending data primary server additional delay primary send data mirror server wait acknowledgment returning client latencies due network transmissions approximately byte kbyte kbyte write requests arms spindles primary secondary disks unsynchronized lack synchronization write requests wait slower primary secondary disk writes column table shows peak throughput chained-declustered petal virtual disk log device peak write throughput higher nvram log device small request sizes express throughput number requests larger request sizes shown megabytes measure aggregate throughput log request normal failed normal byte read req req kbyte read mbytes mbytes kbyte read mbytes mbytes byte write req req kbyte write mbytes mbytes kbyte write mbytes mbytes table normal failed throughput chained-declustered virtual disk byte read kbyte read kbyte read byte write kbyte write kbyte write number servers relative throughput figure scaling increased servers peak throughput petal clients shown figure make random requests single petal virtual disk throughput limited cpu overheads cases server cpu approximately utilized significant fraction time spent copying checksumming data network access petal servers run user-level standard unix socket interface udp protocol stacks techniques streamlining network accesses understood experiment eliminated copying checksums network layer large read requests kbyte read requests optimization reduced cpu utilization increased throughput mbytes mbytes case throughput limited disk controller column table shows performance chaindeclustered petal disk servers crashed read requests performance normal three-quarters servers three-quarters normal performance data placement dynamic load balancing schemes working effectively redistribute load write performance failure normal case servers fail virtual disk addresses managed servers longer mirrored reduces number disk writes system fraction failed servers load surviving server server failure figure shows effect scaling petal servers throughput request type normalized respect maximum throughput request type elapsed time seconds ufs advfs phase petal petal create directories copy files directory status scan files compile table modified andrew benchmark system configurations measured large determine scaling remain linear observed scaling promising file system performance petal clients large virtual disk clients network cluster file systems xfs parallel databases oracle parallel server advantage fact concurrently accessing single virtual disk multiple machines systems widely restrict attention digital unix file system ufs advanced file system advfs table compares performance modified andrew benchmark configurations ufs locally attached disk ufs petal virtual disk advfs collection locally attached disks advfs petal virtual disk petal virtual disk configured chain-declustered data placement disk logging modified andrew benchmark phases phase recursively creates subdirectories phase measures file system data transfer capabilities phase recursively examines status directories files contained fourth phase scans contents data stored file final phase indicative program development phase computationally intensive cases file system level performance petal 
virtual disk comparable locally attached disks exception phase benchmark ufs generates synchronous writes mentioned earlier writes chained-declustered petal virtual disk incur logging overheads increase synchronous write latency advfs journals meta-data updates reduce number synchronous writes suffer overheads running petal achieves higher performance ufs phase benchmark local disk measurements ufs single disk advfs disks achieve similar performance modified andrew benchmark primarily stresses latency throughput storage system case compilation phase performance primarily limited speed cpu discussion availability cost-effective scalable networks driving force work thinking network primary system-level interconnect build incrementally expandable distributed storage systems availability capacity performance current centralized storage systems distributed storage systems pose difficult management consistencyproblems petal experiment address problems petal virtual disks hide distributed nature system clients independent applications share performance capacity physical storage resources system transparently incorporate storage components provide convenient management features snapshots provide special support protecting client data clients difficult provide security virtual disk basis petal virtual disk abstraction adds additional level overhead prevent application-specific disk optimizations rely careful placement data problem reasonable tradeoff benefits petal provide view virtualization current trend sophisticated disk array controllers scsi disks obscure physical disk geometry fact petal server approximately complexity raid controller similar hardware resource requirements petal disk-like interface clients read write blocks data chose interface easily integrated existing computer system transparently support existing file systems databases alternative petal design distributed storage richer interface file system cmu nasd project potentially result system efficient simpler petal interface adequate higher level services efficiently built top petal framework sufficiently general incorporate classes redundancy schemes based parity chosen concentrate replication-based redundancy schemes chained-declustering impose higher capacity overhead readily applicable tolerating site failures present opportunities dynamic load balancing easier implement efficiently distributed systems related work section describes work related petal terms primary characteristics type abstraction block-level file-systemlevel degree distribution level fault tolerance support incremental expandability related block-level storage systems include raid-ii tickertaip logical disk loge mime autoraid swift systems support simple algorithmic mappings address space client underlying physical disks mapping completely system configured contrast autoraid logical disk loge mime petal support flexible mappings index data structures autoraid petal systems support creation multiple virtual disks block-level systems including autoraid support distribution multiple nodes geographically distributed sites exceptions tickertaip swift provide support distributing data multiple nodes assume communication interconnect reliable deal full range distributed systemsa issues addressed petal systems tolerate disk failures tickertaipis tolerate node failures contrast petal supports wider distribution tolerate node network failures closely related file systems include xfs zebra echo afs systems xfs single meta-data server partial subtree file system space ultimately limiting scalability xfs distribute management meta-data multiple nodes object-by-object basis file systems suffer problem file disk systems considered incrementally expandable sense data dumped tape restored adding extra components reconfiguring system systems step zebra autoraid disks incorporated system dynamically transparently respect clients afs nodes added volumes partial subtrees file system space moved nodes transparently afs volume span single node contrast petal virtual disk span multiple nodes goal xfs design change management node file dynamically load balancing response node additions deletions functionality implemented petal supports addition deletion nodes system face arbitrary node network failures petal virtual disk span multiple nodes transparently reconfigured advantage additional nodes reconfiguration transparent petal clients knowledge petal distributed block-level storage system supports virtual containers managing physical resources difficult storage system larger distributed found distribution virtual containers powerful combined distribution system scale large sizes virtual containers make easier allocate physical resources efficiently large-scale systems petal storage system supports transparent addition deletion nodes existing storage containers face arbitrary component network failures system-level performance single container scale gracefully additional nodes added summary conclusions petal distributed block-level storage system tolerates recovers single component failure dynamically balances load servers transparently expands performance capacity principal goal design storage system heterogeneous environments easy manage scale gracefully capacity performance significantly increasing cost managing system found combination features achieve goal actual system significant period time conclusively prove assertion designing petal decided distributed software solutions hardware solutions applicable software hardware tradeoff petal strategy fault tolerance distributed mirroring providing redundant hardware paths disk approach makes easier geographically distribute system scale larger system sizes tradeoff distributed algorithms determine servers failed generally achieve consensus reliable communication hardware specialized hardware synchronization petal block-level file-level interface petal handle heterogeneous client file systems gracefully choice block-level interface greatly simplified work adversely limiting functionality provide opens possibility encapsulating petal server software disk array controller raid software encapsulated disk array controllers today petal virtual disks proved invaluable separating client view storage physical resources system virtualization makes easier allocate physical resources heterogeneous clients enabled features snapshots transparent incremental expandability generally satisfied performance prototype read write latencies chain-declustered petal virtual disk larger locally attached disk achieve rates requests sec small read requests bandwidth mbytes sec large read requests throughput write request understand improve significantly performance performance petal degrades gracefully fraction number failed servers throughput system scales number servers measured sufficiently large system determine performance scaling linear feel confident prototype running past months working building larger production system deployment day-to-day laboratory acknowledgments authors roger needham mike schroeder bill weihl anonymous referees comments earlier drafts paper cynthia hibbard provided valuable editorial assistance thomas anderson michael dahlin jeanna neefe david patterson drew roselli randolph wang serverless network file systems acm transactions computer systems february thomas anderson susan owicki james saxe charles thacker high-speed switch scheduling localarea networks acm transactions computer systems november andrew birrell bruce jay nelson implementing remote procedure calls acm transactions computer systems february luis-felipe cabrera darrel long swift distributed disk striping provide high data rates acm computing systems fall pei cao swee boon lim shivakumar venkataraman john wilkes tickertaip parallel raid architecture acm transactions computer systems august chao english jacobson stepanov wilkes mime high performance parallel storage device strong recovery guarantees technical report hpl-csp- hewlett-packard laboratories november peter chen edward lee ann drapeau ken lutz ethan miller srinivasan seshan ken shirriff david patterson randy katz performance design evaluation raid-ii storage server journal distributed parallel databases july wiebren jonge frans kaashoek wilson hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles pages december peter druschel larry peterson 
bruce davie experiences high-speed network adaptor software perspective proceedings sigcomm symposium communications architectures protocols applications pages august english stepanov loge self-organizing disk controller proceedings winter usenix conference pages january garth gibson david nagle khalil amiri fay chang eugene feinberg howard gobioff chen lee berend ozceri erik riedel david rochberg case network-attached secure disks technical report cmu-cs- department electrical computer engineering carnegie-mellon june john hartman john ousterhout zebra striped network file system acm transactions computer systems august hui-i hsiao david dewitt chained declustering availability strategy multiprocessor database machines technical report wisconsin madison june leslie lamport part-time parliament technical report digital equipment corporation systems research center lytton ave palo alto september timothy mann andrew birrell andy hisgen chuck jerian garret swart coherent distributed file cache directory write-behind acm transactions computer systems satyanarayanan scalable secure highly distributed file access ieee computer daniel stodolsky mark holland william courtright garth gibson parity-logging disk arrays acm transactions computer systems august chandramohan thekkath henry levy limits low-latency communication high-speed networks acm transactions computer systems john wilkes richard golding carl staelin tim sullivan autoraid hierarchical storage system proceedings acm symposium operating systems principles pages december 
wisc graybox special topics gray-box systems andrea arpaci-dusseau fall department computer sciences wisconsin madison introductory lecture overview parts square motivation subject material square review reading list format class readings lecture overview parts part square building systems systems square challenge dealing inadequate interfaces existing systems square solution gray-box systems techniques square microbenchmarking square fingerprinting square reverse engineering square algorithmic mirroring simulation case studies square tcp implicit coscheduling manners visual proxies semantically smart disks differential power analysis overview parts part square building system scratch square challenge design system complete interface square solution unknown expose information control choose implementation easier express applications adapt control current information square open-ended readings part motivation software systems increasingly complex early days square build system scratch square build existing simple components square understand details components simple system motivation current state software design square impossible build system scratch square leverage complex systems components square impossible understand details components systems systems system system system system motivation current state software design square impossible build system scratch square leverage complex systems components square impossible understand details components systems systems system system component similarities modular entities square interface defines outputs function previous current inputs hides unnecessary details square implementation independent interface system component merriam-webster definition system square group units combined form operate unison andrea definition square component system interesting behavior longer precisely succinctly defined interface fails describe information square explain performance square describe internal algorithms policies square expose internal state outline systems existing systems square motivation square challenges square examples square gray-box systems square techniques case studies designing system challenges building systems choose correct components functions square implementation square requirements performance memory power usage function usage square list implementations sorted sorted fast lookup slow insert unsorted slow lookup fast insert depends common operations workload adapt behavior components square optimized square implementation cache elements reorder usage working set exceed needed information components square cost operation performance inserts lookups square parameter configuration size cache square existing policy algorithm replacement algorithm cache square internal state dynamic cache control components square change policies change replacement policy lru lfu outline systems existing systems square motivation square challenges square examples square gray-box systems square techniques case studies designing system simple examples focus systems building blocks questions square type information control component square current interfaces provide square information control obtained interface doesn exist square assumptions made components base case application workload operating system information workload information application cost parameters policies state special case database operating system policies interfaces meet database policies control inappropriate databases multiple layers database operating system application application software software hardware information multiple nodes distributed service operating system hardware operating system hardware distributed service networks application operating system hardware network outline systems existing systems square motivation square challenges square examples square gray-box systems square techniques case studies designing system main problem system provide interface desired information control assume change interface implementation solution gray-box systems assume basic behavior component interface sources base knowledge square understanding common techniques square access previous versions code square high-level documentation square skimming source code probe component observe reactions confirm learn alternatives black-box systems square defined interface assume internals square problem difficult learn making assumptions white-box systems square change details internals square problem realistic realistic assumptions component implemented change implementation change add interfaces square standards consistent source code square complex difficult understand documentation square trust correct complete outline systems existing systems square motivation square examples square gray-box systems square techniques case studies designing system goals techniques gray-box systems square circumstances square strengths weaknesses square costs overheads information control acquired techniques square change implementation square add interface systems designed techniques acquiring additional information exposed interface information cost operations key parameters square solution microbenchmarking perform operations time results time infer parameters square reading list cache tlb saavedra mhz staelin mcvoy disks papers techniques information policies algorithms solution fingerprinting square general complex microbenchmarks square add probes existing operations square observe results operations covert channel time power cache misses square combine results knowledge infer policies techniques fingerprinting reading list square networking papers identify remote passive fingerprinting square buffer-cache management burnett square scheduling hourglass reverse engineering reading list square general problem square instruction encodings techniques information internal state component solution observations square observe outputs combine general knowledge correlate internal state solution algorithmic mirroring square observe inputs component square simulate state component detailed knowledge policies reading list square self-monitoring self-adapting square istore square techniques exerting control interface solution control-theory reading list square entire square case studies case studies networking distributed systems square tcp red square implicit coscheduling application behavior square visual proxies square manners control theory semantically smart disks security square differential power analysis square differential fault analysis part previous problem square deal building blocks interfaces wrong problem square building system scratch square build building block interface square focus building block overview today square half outline systems existing systems designing system square goals square implication square reading list goal expose information explicit interfaces apps subsystems square examples cost operations parameter values algorithms policies internal state square problems goal expose information explicit interfaces apps subsystems square examples cost operations parameter values algorithms policies internal state square problems avoid drowning information identify information express simple manner information hidden security implication implementation simple square policies straight-forward express adaptation component square push adaptation end-to-end argument reading list square open implementation goal applications modify policies control extensible operating systems square single policy workload microkernels remove policies export mechanisms protection download safe code change policy problems goal applications modify policies control extensible operating systems square single policy workload microkernels remove policies export mechanisms protection download safe code change policy problems square difficult remove policies policy arbitrate processes square higher cost policies user-space square hard ensure downloaded code composes implication implement configurable default policies square prefetching configurable units square advantages arbitration flexible low cost policy in-kernel interactions composable square drawback limits extensibility reading list extensible systems synthesis spin exokernel vino scout goal combine goals applications control function current state examples square schedule process buffer cache data instruction cache loaded data square prefetch disk blocks disk head blocks 
application access blocks problems square decide fast back user implication express information terms cost benefit square charge cost process resources scheduling unloading cache data moving disk arm distance square application calculates benefit performing set operations reuse data set cache expected benefit prefetching square performs operation benefit cost implication related work square individual case studies cost-benefit affinity scheduling cohort scheduling sleds anticipatory scheduling compiler directed prefetching square cost models economic models goal general framework summary complex systems systems square interfaces don reveal information square interfaces prevent interesting control square unrealistic assume change interface existing systems ways acquire information control square gray-box systems square techniques microbenchmarking fingerprinting reverse engineering simulation system expressive interfaces implementations lead interfaces 
mapreduce simplified data processing large clusters jeffrey dean sanjay ghemawat jeff google sanjay google google abstract mapreduce programming model implementation processing generating large data sets users map function processes key valuepair generate set intermediate key pairs reduce function merges intermediate values intermediate key real world tasks expressible model shown paper programs written functional style automatically parallelized executed large cluster commodity machines run-time system takes care details partitioning input data scheduling program execution set machines handling machine failures managing required inter-machine communication programmers experience parallel distributed systems easily utilize resources large distributed system implementation mapreduce runs large cluster commodity machines highly scalable typical mapreduce computation processes terabytes data thousands machines programmers findthesystem easytouse hundredsofmapreduceprograms implemented upwards thousand mapreduce jobs executed google clusters day introduction past years authors google implemented hundreds special-purpose computations process large amounts raw data crawled documents web request logs compute kinds derived data inverted indices representations graph structure web documents summaries number pages crawled host set frequent queries day computations conceptually straightforward input data large computations distributed hundreds thousands machines order finish reasonable amount time issues parallelize computation distribute data handle failures conspire obscure original simple computation large amounts complex code deal issues reaction complexity designed abstraction express simple computations perform hides messy details parallelization fault-tolerance data distribution load balancing library abstraction inspired map reduce primitives present lisp functional languages realized computations involved applying map operation logical record input order compute set intermediate key pairs applying reduce operation values shared key order combine derived data appropriately functional model userspecified map reduce operations parallelize large computations easily re-execution primary mechanism fault tolerance major contributions work simple powerful interface enables automatic parallelization distribution large-scale computations combined implementation interface achieves high performance large clusters commodity pcs section describes basic programming model examples section describes implementation mapreduce interface tailored cluster-based computing environment section describes refinements programming model found section performance measurements implementation variety tasks section explores mapreduce google including experiences basis osdi symposium operating systems design implementationusenix association rewrite production indexing system section discusses related future work programming model computation takes set input key pairs produces set output key pairs user mapreduce library expresses computation functions map reduce map written user takes input pair produces set intermediate key pairs mapreduce library groups intermediate values intermediate key passes reduce function reduce function written user accepts intermediate key set values key merges values form possibly smaller set values typically output produced reduce invocation intermediate values supplied user reduce function iterator handle lists values large fit memory problem counting number occurrences word large collection documents user write code similar pseudo-code map string key string key document document contents word emitintermediate reduce string key iterator values key word values list counts int result values result parseint emit asstring result map function emits word count occurrences simple reduce function sums counts emitted word addition user writes code fill mapreduce specification object names input output files optional tuning parameters user invokes mapreduce function passing specification object user code linked mapreduce library implemented appendix full program text types written terms string inputs outputs conceptually map reduce functions supplied user types map list reduce list list input keys values drawn domain output keys values intermediate keys values domain output keys values implementation passes strings user-defined functions leaves user code convert strings types examples simple examples interesting programs easily expressed mapreduce computations distributed grep map function emits line matches supplied pattern reduce function identity function copies supplied intermediate data output count url access frequency map function processes logs web page requests outputs url reduce function adds values url emits url total count pair reverseweb-linkgraph map function outputs target source pairs link target url found page named source reduce function concatenates list source urls target url emits pair target list source term-vectorper host term vector summarizes important words occur document set documents list word frequency pairs map function emits hostname term vector pair input document hostname extracted url document reduce function passed per-document term vectors host adds term vectors throwing infrequent terms emits final hostname term vector pair osdi symposium operating systems design implementation usenix association user program master fork worker fork worker fork assign map assign reduce split split split split split output file write worker read worker local write map phase intermediate files local disks worker output file input files remote read reduce phase output files figure execution overview inverted index map function parses document emits sequence word document pairs reduce function accepts pairs word sorts document ids emits word list document pair set ofall output pairs forms simple invertedindex easy augment computation track word positions distributed sort map function extracts key record emits key record pair reduce function emits pairs unchanged computation depends partitioning facilities section ordering properties section implementation implementations mapreduce interface choice depends environment implementation suitable small shared-memory machine large numa multi-processor larger collection networked machines section describes implementation targeted computing environment wide google large clustersof commodity pcs connectedtogether switched ethernet environment machines typicallydual-processorx processors running linux memory machine commodity networking hardware typically megabits gigabit machine level averaging considerably bisection bandwidth cluster consists hundreds thousands machines machine failures common storage provided inexpensive ide disks attached directly individual machines distributed file system developedin-house managethe data stored disks file system replication provide availability reliability top unreliable hardware users submit jobs scheduling system job consists set tasks mapped scheduler set machines cluster execution overview map invocations distributed multiple machines automatically partitioning input data osdi symposium operating systems design implementationusenix association set splits input splits processed parallel machines reduce invocations distributed partitioning intermediate key space pieces partitioning function hash key mod number partitions partitioning function user figure shows flow mapreduce operation implementation user program calls mapreduce function sequence actions occurs numbered labels figure correspond numbers list mapreduce library user program splits input files pieces typically megabytes megabytes piece controllable user optional parameter starts manycopies program cluster machines copies program special master rest workers assigned work master arem map tasks andr reduce tasks assign master picks idle workers assigns map task reduce task worker assigned map task reads contents input split parses 
key valuepairs input data passes pair user-defined map function intermediate key pairs produced map function buffered memory periodically buffered pairs written local disk partitioned regions partitioning function locations buffered pairs local disk passed back master responsible forwarding locations reduce workers reduce worker notified master locations remote procedure calls read buffered data local disks map workers whena reduce workerhas read intermediate data sorts intermediate keys occurrences key grouped sorting needed typically keys map reduce task amount intermediate data large fit memory external sort reduce worker iterates sorted intermediate data unique intermediate key encountered passes key set intermediate values user reduce function output reduce function appended final output file reduce partition map tasks reduce tasks completed master wakes user program point mapreduce call user program returns back user code successful completion output mapreduce execution output files reduce task file names user typically users combine output files file pass files input mapreduce call distributed application deal input partitioned multiple files master data structures master data structures map task reduce task stores state idle in-progress completed identity worker machine non-idle tasks master conduit location intermediate file regions propagated map tasks reduce tasks completed map task master stores locations sizes intermediate file regions produced map task updates location size information received map tasks completed information pushed incrementally workers in-progress reduce tasks fault tolerance mapreduce library designed process large amounts data hundreds thousands machines library tolerate machine failures gracefully worker failure master pings worker periodically response received worker amount time master marks worker failed map tasks completed worker reset back initial idle state eligible scheduling workers similarly map task reduce task progress failed worker reset idle eligible rescheduling completed map tasks re-executed failure output stored local disk failed machine inaccessible completed reduce tasks re-executed output stored global file system map task executed worker executed worker failed osdi symposium operating systems design implementation usenix association workers executing reduce tasks notified reexecution reduce task read data workera read data workerb mapreduce resilient large-scale worker failures mapreduce operation network maintenance running cluster causing groups machines time unreachable minutes work unreachable worker machines continuedto makeforwardprogress eventuallycompleting mapreduce operation master failure easy make master write periodic checkpoints master data structures master task dies copy started checkpointed state single master failure current implementation aborts mapreduce computation master fails clients check condition retry mapreduce operation desire semantics presence failures terministic functions input values distributed implementation produces output produced non-faulting sequential execution entire program rely atomic commits map reduce task outputs achieve property in-progress task writes output private temporary files reduce task produces file map task produces files reduce task map task completes worker sends message master includes names temporary files message master receives completion message completed map task ignores message records names files master data structure reduce task completes reduce worker atomically renames temporary output file final output file reduce task executed multiple machines multiple rename calls executed final output file rely atomic rename operation provided underlying file system guarantee final file system state data produced execution reduce task vast majority map reduce operators deterministic fact semantics equivalent sequential execution case makes sbehavior map reduce operators nondeterministic provide weaker reasonable semantics presence non-deterministic operators output reduce task equivalent output produced sequential execution non-deterministic program output reduce task correspond output produced sequential execution non-deterministic program map task reduce tasks execution committed execution weaker semantics arise read output produced execution read output produced execution locality network bandwidth scarce resource computing environment conserve network bandwidth taking advantage fact input data managed gfs stored local disks machines make cluster gfs divides file blocks stores copies block typically copies machines mapreduce master takes location information input files account attempts schedule map task machine replica input data failing attempts schedule map task replica task input data worker machine network switch machine data running large mapreduce operations significant fraction workers cluster input data read locally consumes network bandwidth task granularity subdivide map phase pieces reduce phase intorpieces ideally larger number worker machines worker perform tasks improves dynamic load balancing speeds recovery worker fails map tasks completed spread worker machines practical bounds largem andrcan implementation master make scheduling decisions state memory constant factors memory usage small theo piece state consists approximately byte data map task reduce task pair osdi symposium operating systems design implementationusenix association constrained users output reduce task ends separate output file practice tend choose individual task roughly input data locality optimization effective make small multiple number worker machines expect perform mapreduce computations worker machines backup tasks common lengthens total time mapreduce operation straggler machine takes unusually long time complete map reduce tasks computation stragglers arise host reasons machine bad disk experience frequent correctable errors slow read performance cluster scheduling system scheduled tasks machine causing execute mapreduce code slowly due competition cpu memory local disk network bandwidth recent problem experienced bug machine initialization code caused processor caches disabled computations affected machines slowed factor hundred general mechanism alleviate problem stragglers mapreduce operation close completion master schedules backup executions remaining in-progress tasks task marked completed primary backup execution completes tuned mechanism typically increases computational resources operation percent found significantly reduces time complete large mapreduce operations sort program section takes longer complete backup task mechanism disabled refinements basic functionality provided simply writing map reduce functions sufficient found extensions section partitioning function users mapreduce number reduce tasks output files desire data partitioned tasks partitioning function intermediate key default partitioning function provided hashing hash key mod result fairly well-balanced partitions cases partition data function key output keys urls entries single host end output file support situations user mapreduce library provide special partitioning function hash hostname urlkey modr partitioning function urls host end output file ordering guarantees guarantee partition intermediate key pairs processed increasing key order ordering guarantee makes easy generate sorted output file partition output file format support efficient random access lookups key users output find convenient data sorted combiner function cases significant repetition intermediate keys produced map task userspecified reduce function commutative associative good word counting section word frequencies tend follow zipf distribution map task produce hundreds thousands records form counts network single 
reducetaskandthenaddedtogetherbythereducefunction produce number user optional combiner function partial merging data network combiner function executed machine thatperformsamaptask implement combiner reduce functions differencebetween reduce function combiner function mapreduce library handles output function output reduce function written final output file output combiner function written intermediate file reduce task partial combining significantly speeds classes mapreduce operations appendix combiner input output types mapreduce library support reading input data formats text osdi symposium operating systems design implementation usenix association mode input treats line key pair key offset file contents line common supported format stores sequence key pairs sorted key input type implementation split meaningful ranges processing separate map tasks text mode range splitting ensures range splits occur line boundaries users add support input type providing implementation simple reader interface users small number predefined input types reader necessarily provide data read file easy define reader reads records database data structures mapped memory similar fashion support set output types producing data formats easy user code add support output types side-effects cases users mapreduce found convenient produce auxiliary files additional outputs map reduce operators rely application writer make side-effects atomic idempotent typically application writes temporary file atomically renames file fully generated provide support atomic two-phase commits multiple output files produced single task tasks produce multiple output files cross-file consistency requirements deterministic restriction issue practice skipping bad records sometimesthereare reduce functions crash deterministically records bugs preventa mapreduceoperation completing usual action fix bug feasible bug third-party library source code unavailable acceptable ignore records statistical analysis large data set provide optional mode execution mapreduce library detects records deterministic crashes skips records order make forward progress worker process installs signal handler catches segmentation violations bus errors invoking user map reduce operation mapreduce library stores sequence number argument global variable user code generates signal signal handler sends gasp udp packet sequence number mapreduce master master failure record record skipped issues re-executionof map reduce task local execution debugging problems map reduce functions tricky actual computation distributed system thousand machines work assignment decisions made dynamically master facilitate debugging profiling small-scale testing developedan alternative implementation mapreduce library sequentially executes work mapreduce operation local machine controls provided user computation limited map tasks users invoke program special flag easily debugging testing tools find gdb status information master runs internal http server exports set status pages human consumption status pages show progress computation tasks completed progress bytes input bytes intermediate data bytes output processing rates pages links standard error standard output files generated task user data predict long computation resources added computation pages figure computation slower expected addition top-level status page shows workers failed map reduce tasks processing failed information attempting diagnose bugs user code counters mapreduce library counter facility count occurrences events user code count total number words processed number german documents indexed facility user code creates named counter object increments counter appropriately map reduce function osdi symposium operating systems design implementationusenix association counter uppercase uppercase getcounter uppercase map string string contents word contents iscapitalized uppercaseincrement emitintermediate counter values individual worker machines periodically propagated master piggybacked ping response master aggregatesthe counter values successful map reduce tasks returns user code mapreduce operation completed current counter values displayed master status page human watch progress live computation aggregatingcountervalues duplicate executions map reduce task avoid double counting duplicate executions arise backup tasks re-execution tasks due failures counter values automatically maintained mapreduce library number input key pairs processed number output key pairs produced users found counter facility sanity checking behavior mapreduce operations mapreduce operations user code ensure number output pairs produced equals number input pairs processed fraction german documents processed tolerable fraction total number documents processed performance section measure performance mapreduce computations running large cluster machines computation searches approximately terabyte data pattern computation sorts approximately terabyte data programs representative large subset ofthe real programswrittenby usersof mapreduce class programs shuffles data representation class extracts small amount interesting data large data set cluster configuration programs executed cluster consisted approximately machines machine ghz intel xeon processors hyperthreading enabled memory ide seconds input figure data transfer rate time disks gigabit ethernet link machines arranged two-level tree-shaped switched network approximately gbps aggregate bandwidth root machines hosting facility round-trip time pair machines millisecond memory approximately reserved tasks running cluster programs executed weekend afternoon cpus disks network idle grep grep program scans -byte records searching relativelyrare three-character pattern pattern occurs records input split approximately pieces entire output file figure shows progress computation time y-axis showsthe rate inputdata scanned rate gradually picks machines assigned mapreduce computation peaks workers assigned map tasks finish rate starts dropping hits seconds computation entire computation takes approximately seconds start finish includes minute startup overhead overhead due propagation program worker machines delays interacting gfs open set input files information needed locality optimization sort sort program sorts -byte records approximately terabyteof data programis modeledafter terasort benchmark sorting program consists lines user code three-line map function extracts -byte sorting key text line emits key osdi symposium operating systems design implementation usenix association input shuffle seconds output normal execution input shuffle seconds output backup tasks input shuffle seconds output tasks killed figure data transfer rates time executions sort program original text line intermediate key pair built-in identity function reduce operator functions passes intermediate key valuepair unchanged output key pair final sorted output written set -way replicated gfs files terabytesare writtenas theoutput theprogram input data split pieces partition sorted output files partitioning function initial bytes key segregate ofrpieces partitioning function benchmark builtin knowledge distribution keys general sorting program add pre-pass mapreduce operation collect sample keys distribution sampled keys compute splitpoints final sorting pass figure shows progress normal execution sort program top-left graph shows rate input read rate peaks dies fairly quickly map tasks finish seconds elapsed note input rate grep sort map tasks spend half time bandwidth writing intermediate output local disks intermediate output grep negligible size middle-left graph shows rate data network map tasks reduce tasks shuffling starts map task completes hump graph batch approximately reduce tasks entire mapreduce assigned machines machine executes reduce task time roughly seconds computation batch reduce tasks finish start shuffling data remaining reduce tasks shufflingisdoneabout bottom-left 
graph shows rate sorted data written tothe final output files reducetasks delay end shuffling period start writing period machines busy sorting intermediate data writes continue rate writes finish seconds computation including startup overhead entire computation takes seconds similar current reported result seconds terasort benchmark things note input rate higher shuffle rate output rate locality optimization data read local disk bypasses bandwidth constrained network shuffle rate higher output rate output phase writes copies sorted data make replicas output reliability availability reasons write replicas mechanism reliability availability provided underlying file system network bandwidth requirements writing data reduced underlying file system erasure coding replication osdi symposium operating systems design implementationusenix association effect backup tasks figure show execution sort program backup tasks disabled execution flow similar shown figure long tail write activity occurs seconds reduce tasks completed stragglers don finish seconds entire computation takes seconds increase elapsed time machine failures figure showan executionof sort program intentionally killed worker processes minutes computation underlying cluster scheduler immediately restarted worker processes machines processes killed machines functioning properly worker deaths show negative input rate previously completed map work disappears map workers killed redone re-execution map work quickly entire computation finishes seconds including startup overhead increase normal execution time experience wrote version mapreduce library february made significant enhancements august including locality optimization dynamic load balancing task execution worker machines time pleasantly surprised broadly applicable mapreduce library kinds problems work wide range domains google including large-scale machine learning problems clustering problems google news froogle products extractionof datausedtoproducereports ofpopular queries google zeitgeist extractionof propertiesof web pagesfor experiments products extraction geographical locations large corpus web pages localized search large-scale graph computations number instances source tree figure mapreduce instances time number jobs average job completion time secs machine days days input data read intermediate data produced output data written average worker machines job average worker deaths job average map tasks job average reduce tasks job unique map implementations unique reduce implementations unique map reduce combinations table mapreduce jobs run august figure shows significant growth number separate mapreduce programs checked primary source code management system time early separate instances late september mapreduce successful makes write simple program run efficiently thousand machines half hour greatly speeding development prototyping cycle programmers experience distributed parallel systems exploit large amounts resources easily end job mapreduce library logs statistics computational resources job table show statistics subset mapreduce jobs run google august large-scale indexing significant mapreduce date complete rewrite production indexosdi symposium operating systems design implementation usenix association ing system produces data structures google web search service indexing system takes input large set documents retrieved crawling system stored set gfs files raw contents documents terabytes data indexing process runs sequence ten mapreduce operations mapreduce ad-hoc distributed passes prior version indexing system provided benefits indexing code simpler smaller easier understand code deals fault tolerance distribution parallelization hidden mapreduce library size phase computation dropped approximately lines code approximately lines expressed mapreduce performance mapreduce library good conceptually unrelated computations separate mixing avoid extra passes data makes easy change indexing process change months make indexing system days implement system indexing process easier operate problems caused machine failures slow machines networking hiccups dealt automatically mapreduce library operator intervention easy improve performance indexing process adding machines indexing cluster related work systems provided restricted programming models restrictions parallelize computationautomatically forexample anassociativefunction computed prefixes element array logn time onn processors parallel prefix computations mapreduce considered simplification distillation models based experience large real-world computations significantly provide fault-tolerant implementation scales thousands processors contrast parallel processing systems implemented smaller scales leave details handling machine failures programmer bulk synchronous programming mpi primitives provide higher-level abstractions make easier programmers write parallel programs key difference systems mapreduce mapreduce exploits restricted programming model parallelize user program automatically provide transparent fault-tolerance locality optimization draws inspiration techniques active disks computation pushed processing elements close local disks reduce amount data subsystems network run commodity processors small number disks directly connected running directly disk controller processors general approach similar backup task mechanism similar eager scheduling mechanism employed charlotte system shortcomings simple eager scheduling task repeated failures entire computation fails complete fix instances problem mechanism skipping bad records mapreduce implementation relies in-house cluster management system responsible distributing running user tasks large collection shared machines focus paper cluster management system similar spirit systems condor sorting facility part mapreduce library similar operation now-sort source machines map workers partition data sorted send reduce workers reduce worker sorts data locally memory now-sort user-definable map reducefunctions make library widely applicable river programming model processes communicate sending data distributed queues mapreduce river system provide good average case performance presence non-uniformities introduced heterogeneous hardware system perturbations river achieves careful scheduling disk network transfers achieve balanced completion times mapreduce approach restricting programming model mapreduce framework partition problem large number finegrained tasks tasks dynamically scheduled workers faster workers process tasks restricted programming model schedule redundant executions tasks end job greatly reduces completion time presence non-uniformities slow stuck workers bad-fs programming model mapreduce unlike mapreduce targeted osdi symposium operating systems design implementationusenix association execution jobs wide-area network fundamental similarities systems redundant execution recover data loss caused failures locality-aware scheduling reduce amount data congested network links tacc system designed simplify construction highly-available networked services mapreduce relies re-execution mechanism implementing fault-tolerance conclusions mapreduce programming model successfully google purposes attribute success reasons model easy programmers experience parallel distributed systems hides details parallelization fault-tolerance locality optimization load balancing large variety problems easily expressible mapreduce computations mapreduce generation data google production web search service sorting data mining machine learning systems developed implementation mapreduce scales large clusters machines comprising thousands machines implementation makes efficient machine resources suitable large computational problems encountered google learned things work restricting programming model makes easy parallelize distribute computations make computations fault-tolerant network bandwidth scarce resource number optimizations system targeted reducing amount datasent acrossthenetwork lowsus read data local disks writing single copy intermediate data local disk saves network bandwidth redundant execution reduce impact slow machines handle machine failures data loss acknowledgements josh levenberg instrumental revising extending user-level mapreduce api number features based experience mapreduce people suggestions enhancements mapreduce reads input writes output 
google file system mohit aron howard gobioff markus gutschke david kramer shun-tak leung josh redstone work developing gfs percy liang olcan sercinoglu work developing cluster management system mapreduce mike burrows wilson hsieh josh levenberg sharon perl rob pike debby wallach provided helpful comments earlier drafts paper anonymous osdi reviewers shepherd eric brewer provided suggestions areas paper improved finally users mapreduce google engineering organization providing helpful feedback suggestions bug reports andrea arpaci-dusseau remzi arpaci-dusseau david culler joseph hellerstein david patterson high-performance sorting networks workstations proceedings acm sigmod international conference management data tucson arizona remzi arpaci-dusseau eric anderson noah treuhaft david culler joseph hellerstein david patterson kathy yelick cluster river makingthe fastcasecommon inproceedingsofthe sixth workshop input output parallel distributed systems iopads pages atlanta georgia arash baratloo mehmet karaul zvi kedem peter wyckoff charlotte metacomputing web proceedings international conference parallel distributed computing systems luiz barroso jeffrey dean urs olzle web searchforaplanet ieee micro april john bent douglas thain andrea arpaci-dusseau remzi arpaci-dusseau miron livny explicit control batch-aware distributed file system proceedings usenix symposium networked systems design implementation nsdi march guy blelloch scans primitive parallel operations ieee transactions computers november armando fox steven gribble yatin chawathe eric brewer paul gauthier cluster-based scalable network services proceedings acm symposium operating system principles pages saint-malo france sanjay ghemawat howard gobioff shun-tak leung google file system symposium operating systems principles pages lake george york osdi symposium operating systems design implementation usenix association gorlatch systematic efficient parallelization scan list homomorphisms bouge fraigniaud mignotte robert editors euro-par parallel processing lecture notes computer science pages springer-verlag jim gray sort benchmark home page http research microsoft barc sortbenchmark william gropp ewing lusk anthony skjellum mpi portable parallel programming message-passing interface mit press cambridge huston sukthankar wickremesinghe satyanarayanan ganger riedel ailamaki diamond storage architecture early discard interactive search proceedings usenix file storage technologies fast conference april richard ladner michael fischer parallel prefix computation journal acm michael rabin efficient dispersal information security load balancing fault tolerance journal acm erik riedel christos faloutsos garth gibson david nagle active disks large-scale data processing ieee computer pages june douglas thain todd tannenbaum miron livny distributed computing practice condor experience concurrency computation practice experience valiant bridging model parallel computation communications acm jim wyllie spsort sort terabyte quickly http alme almaden ibm spsort pdf word frequency section program counts number occurrences unique word set input files command line include mapreduce mapreduce user map function class wordcounter public mapper public virtual void map const mapinput input const string text input const int text size int skip past leading whitespace isspace text find word end int start isspace text start emit text substr start i-start register mapper wordcounter user reduce function class adder public reducer virtual void reduce reduceinput input iterate entries key add values int inputdone stringtoint inputvalue inputnextvalue emit sum inputkey emit inttostring register reducer adder int main int argc char argv parsecommandlineflags argc argv mapreducespecification spec store list input files spec int argc mapreduceinput input spec add input inputset format text inputset filepattern argv inputset mapper class wordcounter output files gfs test freq-of- gfs test freq-of- mapreduceoutput spec output outset filebase gfs test freq outset num tasks outset format text outset reducer class adder optional partial sums map tasks save network bandwidth outset combiner class adder tuning parameters machines memory task spec set machines spec set map megabytes spec set reduce megabytes run mapreduceresult result mapreduce spec result abort result structure info counters time number machines return osdi symposium operating systems design implementationusenix association 
distributed snapshots determining global states distributed systems mani chandy texas austin leslie lamport stanford research institute paper presents algorithm process distributed system determines global state system computation problems distributed systems cast terms problem detecting global states instance global state detection algorithm helps solve important class problems stable property detection stable property persists stable property true remains true examples stable properties computation terminated system deadlocked tokens token ring disappeared stable property detection problem devising algorithms detect stable property global state detection checkpointing categories subject descriptors computer-communication networks distributed systems-distributed applications distributed databases network operating systems operating systems process management-concurrency deadlocks multiprocessing multiprogramming mutual exclusion scheduling synchronization operating systems reliability-backup procedures checkpoint restart fault-tolerance verification general terms algorithms additional key words phrases global states distributed deadlock detection distributed systems message communication systems introduction paper presents algorithms process distributed system determine global state system computation processes distributed system communicate sending receiving messages process record state messages sends receives record determine global system state process enlist work supported part air force office scientific research grant afosr part national science foundation grant mcs authors addresses chandy department computer sciences texas austin austin lamport stanford research institute menlo park permission copy fee part material granted provided copies made distributed direct commercial advantage acm copyright notice title date notice copying permission association computing machinery copy republish requires fee specific permission acm acm transactions computer systems vol february pages chandy lamporl cooperation processes record local states send recorded local states processes record local states precisely instant access common clock assume processes share clocks memory problem devise algorithms processes record states states communication channels set process channel states recorded form global system state global-state-detection algorithm superimposed underlying computation run concurrently alter underlying computation state-detection algorithm plays role group photographers observing panoramic dynamic scene sky filled migrating birdsa scene vast captured single photograph photographers snapshots piece snapshots form picture scene snapshots precisely instant synchronization problems photographers disturb process photographed instance birds heavens remain motionless photographs composite picture meaningful problem define meaningful determine photographs describe important class problems solved global-state-detection algorithm predicate function defined global states distributed system true false global state predicate stable property implies global states reachable global state words stable property true point computation true points computation examples stable properties computation terminated system deadlocked tokens token ring disappeared distributed-system problems formulated general problem devising algorithm process distributed system determine stable property system holds deadlock detection termination detection special cases stable-property detection problem details algorithm presented basic idea algorithm global state system determined computed stable property holds algorithms solving deadlock termination problems determining global states distributed systems published gligor shattuck state published algorithms incorrect impractical reason incorrect impractical algorithms relationships local process states global system states points distributed computation understood contributions paper define relationships distributed algorithms structured sequence phases phase consists transient part work stable part system cycles endlessly uselessly presence stable behavior end phase phase similar series acm transactions computer systems vol february distributed snapshots iterations sequential program repeated successive iterations produce change stability attained stability detected phase terminated phase initiated termination computational phase identical termination computation computation terminates activities cease-messages process states change activity stable behavior end computational phase-messages received processes change state activity serves purpose signal end phase paper concerned detection stable system properties cessation activity stable property strictly speaking properties system deadlocked stable deadlock broken computation reinitiated exposition simple partition problem problems detecting termination phase informing processes phase ended initiating phase stable property kth computational phase terminated methods presented paper applicable detecting termination lath phase paper restrict attention problem detecting stable properties problem initiating phase computation considered solution problem varies significantly depending application database deadlock detection detecting termination diffusing computation present algorithms terms model system model chosen important couched discussion terms models describe model informally level detail make algorithms clear model distributed system distributed system consists finite set processes finite set channels labeled directed graph vertices represent processes edges represent channels figure channels assumed infinite buffers error-free deliver messages order infinite buffer assumption made ease exposition bounded buffers assumed provided exists proof process attempts add message full buffer delay experienced message channel arbitrary finite sequence messages received channel initial subsequence sequence messages channel state channel sequence messages channel excluding messages received channel process defined set states initial state set set events event process atomic action change state state channel incident state changed sending message directed receipt message directed event defined process event occurs state immediately acm transactions computer systems vol february chandy lamport fig distributed system processes channels event state immediately event channel state altered event message channel directed received directed define -tuple special symbol null occurrence change state channel global state distributed system set component process channel states initial global state state process initial state state channel empty sequence occurrence event change global state occur global state state process global state channel directed state global state sequence messages head define function global state immediately occurrence event global state defined event occur global state case global state identical state channel directed state state message deleted head channel directed state state message added tail seq sequence events component processes distributed system seq computation system event occur global state initial global state alternate model based lamport views computations partially ordered sets events illustrate definition distributed system simple system consisting processes channels shown figure system token passed process call system single-token conservation system process states state process possess token state initial state process events transition wit acm transactions computer systems vol february distributed snapshots fig simple distributed system examples channel process fig state-transition diagram process receive token transit global state token global state token -----global state token --l---------l l-------j fig global states transitions single-token conservation system sending token transition receipt token state-transition diagram process shown figure global states transitions shown figure system computation corresponds path global-state-transition diagram figure starting initial global state examples system computations empty sequence sends token receives token sends token sequence computation system sends token sends token event sends token occur state brevity global states order transition figure called in-p in-c in-q in-c denote location 
token motivate algorithm acm transactions computer systems vol february chandy lamport initial fig state-transition diagram process fig state-transition diagram process initial global state a-state rzstate sends qbc global state sends ebd global state receives add global state empty fig computation illustrates nondeterministic computations nondeterminism plays interesting role snapshot algorithm event global state system topology figure processes defined state-transition diagrams figures computation shown figure reader observe transition allowable global state instance events sends sends occur initial global state states events acm transactions computer systems vol february distributed snapshots algorithm motivation steps algorithm global-state recording algorithm works process records state processes channel incident cooperate recording channel state ensure states processes channels recorded instant global clock require recorded process channel states form meaningful global system state global-state recording algorithm superimposed underlying computation run concurrently alter underlying computation algorithm send messages require processes carry computations messages computation required record global state interfere underlying computation motivate steps algorithm assume record state channel instantaneously postpone discussion channel state recorded channel purpose gain intuitive understanding relationship instant state channel recorded instants states processes recorded single-token conservation system assume state process recorded global state in-p state recorded shows token assume global state transits in-c sends token suppose states channels process recorded global state in-c state recorded channel shows token states recorded channel process show possession token composite global state recorded fashion show tokens system global state tokens unreachable initial global state single-t conservation system inconsistency arises state recorded message state recorded message number messages beforep state recorded number messages state recorded suggests recorded global state inconsistent alternate scenario suppose state recorded global state in-p system transits global state in-c states recorded global state in-c recorded global state shows tokens system suggests recorded global state inconsistent state recorded sends message state recorded sends message learn examples general consistent global state requires acm transactions computer systems vol february chandy lamport number messages received state recorded number messages received state recorded leave reader extend show consistency requires state number messages received channel exceed number messages channel equations state channel recorded sequence messages channel sender state recorded excluding sequence messages received channel receiver state recordedthat recorded state empty sequence recorded state messages fact eqs suggest simple algorithm record state channel process sends special message called marker nth message sends sending messages marker effect underlying computation state sequence messages received records state receives marker ensure record state receiving marker receives messages suggests outline global state detection algorithm global-state-detection algorithm outline marker-sending rule process channel incident directed sends marker records state sends messages marker-receiving rule process receiving marker channel recorded state begin records state records state empty sequence end records state sequence messages received state recorded received marker termination algorithm marker receiving sending rules guarantee marker received channel process record state states acm transactions computer systems vol february distributed snapshots incoming channels ensure global-state recording algorithm terminates finite time process ensure marker remains forever incident input channel records state finite time initiation algorithm algorithm initiated processes records state spontaneously receiving markers processes postpone discussion process record state spontaneously process records state channel process record state finite time send marker channel receive marker finite time records state path graph representing system process record state finite time induction process path record state finite time termination finite time ensured process spontaneously records state path process spontaneously records state graph strongly connected process spontaneously records state processes record states finite time provided ensured algorithm process record state states incoming channels recorded process channel states collected assembled form recorded global state describe algorithms collecting recorded information algorithms simple algorithm collecting information system topology strongly connected process send information records outgoing channels process receiving information time copy propagate outgoing channels recorded information processes finite time allowing processes determine recorded global state properties recorded global state gain intuitive understanding properties global state recorded algorithm study assume state recorded global state figure state recorded recording state sends marker channel assume iystem global state marker transit marker received system global state receiving marker records state records state empty sequence recording state sends marker channel receiving marker records state sequence consisting single message recorded global state shown figure recording algorithm initiated global state terminated global state observe global state recorded algorithm identical global states occurred computation algorithm recorded global state occurred answer question acm transactions computer systems vol february chandy lamport empty state fig recorded global state seq distributed computation global state system immediately event seq algorithm initiated global state terminate global state words algorithm initiated terminates eeel observed recorded global state global states show reachable reachable specifically show exists computation seq seq permutation seq occur global states occurs earlier occurs earlier seq theorem exists computation seq foralli wherei rir subsequence permutation subsequence exists proof event seq called prerecording event process records state seq event seq called postrecording event prerecording event-that process records state seq events prerecording events events postrecording events seq postrecording event ejbefore prerecording event occur ejand processes ejand process ejis postrecording event derive computation seq permuting seq prerecording events occur postrecording events seq show global state seq prerecording events postrecording events assume postrecording event ejbefore prerecording event seq show sequence obtained interchanging ejand computation events ejand processes process ejoccurs process occurs message ejwhich received acm transactions computer systems vol february distributed snapshots message channel event ejoccurs marker ejsince ejis postrecording event message received channel occurs marker received occurs channels first-in-first-out case marker-receiving rule postrecording event state process altered occurrence event ejbecause ejis process event receives message channel message head event ejsince message ejcannot received event occur global state sjthe state process altered occurrence ejcan occur sequence events ejej ejis computation arguments paragraph global state computation global state computation ejej ejlet seq permutation seq identical seq ejare interchanged seq computation global state immediately ith event seq arguments previous paragraph repeatedly swapping postrecording events immediately follow prerecording events exists permutation seq seq prerecording events precede postrecording events seq computation foralliwherei lori foralliwhereislori show global state prerecording events postrecording events seq show state process state process computation consisting sequence prerecorded events state channel sequence 
messages prerecorded sends sequence messages prerecorded receives proof part trivial prove part channel process process state channel recorded sequence messages received records state receives marker sequence messages sends marker sequence prerecorded sends part purpose show computation seq derived computation seq sequence acm transactions computer systems vol february chandy lamport events shown computation figure sends state postrecording event sends state prerecording event receives state postrecording event postrecording event immediately precedes prerecording event interchange permuted sequence seq sends state prerecording event sends state postrecording event receives state postrecording event seq prerecording events precede postrecording events leave reader show global state recorded global state stability detection solve stability-detection problem section study stability-detection problem paradigm practical problems distributed deadlock detection stability-detection algorithm defined input stable property output boolean definite property definite definite global states system algorithm initiated terminates symbol denotes logical implication input algorithm definition function execution algorithm global state determined process system applying externally defined function global state output algorithm boolean definite specially designated process enters remains special state symbolize output definite true enters remains special state symbolize output definite false definite true implies stable property holds algorithm terminates definite false implies stable property hold algorithm initiated emphasize definite true information state system termination algorithm definite false information system state initiation algorithm deduce definite false stable property hold termination algorithm solution stability detection problem begin record global state definite end acm transactions computer systems vol february distributed snapshots correctness stability detection algorithm facts reachable reachable theorem reachable definition stable property acknowledgments contributions defining problem global state detection gratefully acknowledged grateful dijkstra scholten comments-particularly proof theorem outline current version proof suggested dijkstra note subject colorful insight problem stability detection due hoare schneider andrews helped detailed comments grateful anita jones anonymous referees suggestions chandy misra distributed computation graphs shortest path algorithms commun acm nov chandy misra haas distributed deadlock detection acm trans comput syst dijkstra distributed snapshot chandy lamport tech rep ewd univ texas austin tex dijkstra scholten termination detection diffusing computations znf proc lett aug gligor shattuck deadlock detection distributed systems ieee trans softw eng sesep lamport time clocks ordering events distributed system commun acm jul lamport chandy partially-ordered event models distributed computations submitted mahoud riordan software controlled access distributed databases infor feb menasce muntz locking deadlock detection distributed data bases ieee trans softw eng semay misra chandy termination detection diffusing computations communicating sequential processes acm trans program lang syst jan obermarck distributed deadlock detection algorithm acm trans database syst jun received january revised september accepted december acm transactions computer systems vol february 
low-bandwidth network file system athicha muthitacharoen benjie chen david mazi eres mit laboratory computer science nyu department computer science athicha benjie lcs mit nyu abstract users rarely running network file systems slow wide-area networks performance unacceptable bandwidth consumption high nonetheless efficient remote file access desirable networks high latency makes remote login sessions unresponsive run interactive programs editors remotely users run programs locally manipulate remote files file system require network file system consumes bandwidth current file systems paper presents lbfs network file system designed low-bandwidth networks lbfs exploits similarities files versions file save bandwidth avoids sending data network data found server file system client cache technique conjunction conventional compression caching lbfs consumes order magnitude bandwidth traditional network file systems common workloads introduction paper describes lbfs network file system designed low-bandwidth networks people typically run network file systems lans campus-area networks mbit sec bandwidth slower wide-area networks data transfers saturate bottleneck links unacceptable delays interactive programs freeze responding user input file batch commands research sponsored defense advanced research projects agency darpa space naval warfare systems center san diego contract times normal execution time aggressive network applications starved bandwidth users employ techniques accomplish lan file system people occasion work networks slower lans broadband internet access person working home fraction mbit sec upstream bandwidth company offices cities users collaborating single mbit sec line consultant constantly traveling sites sit access project files location absence network file system people generally resort methods accessing remote data make edit local copies files running risk update conflict remote login view edit files place machine network long latency remote login frustrating interactive applications slow responding user input worse graphical applications figure editors postscript previewers consume bandwidth run practically wide-area network network file systems potential alleviate inconveniences remote data access addition offering interface people prefer localarea networks file system provide tight consistency avoiding problem conflicts people update file file systems tolerate network latency remote login sessions running interactive programs locally accessing remote data file system avoids overhead network round trip user input event practical wide-area network file system consume significantly bandwidth current file systems maintain acceptable performance avoid monopolizing network links purposes application writers commonly assume file slower megabyte instance interactive editor stop write kbyte auto-save files worrying delaying user typing consuming significant resources traditional file system transmits entire contents files network blocking editor duration transfer contrast lbfs transmits data applications write greatly reducing time spent waiting file reduce bandwidth requirements lbfs exploits cross-file similarities files written applications number segments common files previous versions file auto-save files object files output compilers temporary files rcs revision control system postscript files word processing documents substantial similarity revision copying concatenation files building program libraries object files leads significant duplication contents exploit inter-file similarities lbfs file server divides files stores chunks indexes chunks hash lbfs client similarly indexes large persistent file cache transferring file client server lbfs identifies chunks data recipient files avoids transmitting redundant data network conjunction conventional compression technique saves order magnitude communications bandwidth common workloads lbfs traditional file system semantics consistency files reside safely server closed clients server latest version open file lbfs place network file system breaking software disturbing users file systems dealt slow intermittent network connectivity relaxing file system consistency techniques largely complement lbfs combined lbfs greater bandwidth savings altered semantics suitable purposes chose focus reducing bandwidth save changing accepted consistency guarantees section describes related work section lbfs algorithm finding commonality files explains lbfs protocol takes advantage section describes implementation lbfs section shows effective lbfs technique compressing file traffic finally section concludes related work past projects attacked problem network file systems slow networks angles lbfs complements previous work consistency place significant hardware file system structure requirements server lbfs approach unobtrusively combined techniques additional savings network bandwidth number file systems properties tolerate high network latency afs server callbacks inform clients clients modified cached files users access cached afs files requiring network traffic leases modification callbacks server obligation inform client expires period time leases reduce state stored server free server contacting clients haven touched file avoid problems client server promised callback crashed network nfs protocol reduces network round trips batching file system operations techniques applicable lbfs fact lbfs leases large persistent cache provide afs-like close-to-open consistency file systems write-behind tolerate latency echo performs write-behind metadata operations allowing completion operations traditionally require network round trip jetfile machine write file file server transmit contents directly reader coda file system supports slow networks disconnected operation file system logged client written back server background network connectivity implement functionality coda weaker-thantraditional consistency guarantees update conflicts users resolve manually coda saves bandwidth avoids transferring files server deleted overwritten quickly client lbfs contrast simply reduces bandwidth required file transfer lbfs benefit coda-style deferred operations coda benefit lbfs file transfer compression bayou investigates conflict resolution optimistic updates disconnected systems unlike coda provide file system bayou supplies api implement application-specific merging conflict resolution oceanstore applies bayou conflict resolution mechanisms file system extends work untrusted servers data encrypted format tact explores spectrum absolute consistency bayou weaker model lee extended coda support operationbased updates proxy-client strongly connected server duplicates client computation hopes duplicating output files users run modified shell bundles commands proxy-client reexecute forward error correction client proxy-client patch small glitches output files dates successful operation-based updates deliver tremendous bandwidth savings technique fairly complementary lbfs lbfs works interactive applications editors hard reexecute proxy-client operation-based updates reduce communications bandwidth command-line utilities image converters lbfs offers savings operation-based updates require dedicated proxyclient machine making bit cumbersome set reason technique widespread file system today spring wetherall proposed protocolindependent technique eliminating redundant network traffic assume cooperating caches end slow network link caches store identical copies megabytes network traffic values end send data exists cache sends token find data cache identify redundant traffic ends index cache data -byte anchors randomly chosen based hash data -byte anchor common previous traffic matching region expanded directions elide greatest amount data lbfs approach similar spirit spring wetherall technique lbfs supports multiple clients accessing file file system local users changing file system underneath server assume client server identical state rsync copies directory tree 
network directory tree similar files typically previous version tree rsync saves bandwidth exploiting commonality files problem similar synchronizing client file cache server vice versa fact tridgell suggests applying rsync file system thesis rsync inspirations lbfs file caching real time directory tree mirroring lbfs algorithm discuss rsync algorithm detail compare approach section number unix utilities operate differences files diff computes difference text files patch applies output diff transform file studies problem describing file terms minimal set edits mogul investigated transmitting deltas save bandwidth updating cached web pages cvs version management system ships patches network bring user working copy directory tree date unlike cvs file system store complete revision history files lbfs server typically exact version compute differences design lbfs designed save bandwidth providing traditional file system semantics lbfs close-to-open consistency client written closed file client opening file contents file successfully written closed data resides safely server semantics similar afs work exploring relaxed consistency semantics apply lbfs wished build file system directly substitute widely accepted network file system today save bandwidth lbfs large persistent file cache client lbfs assumes clients cache user entire working set files reasonable assumption capacities cheap ide disks today aggressive caching client server communication solely purpose maintaining consistency user modifies file client transmit server model client crash cut network similarly client reads file modified client server send latest version file lbfs reduces bandwidth requirements exploiting similarities files reconstitutes files chunks existing data file system client cache transmitting chunks network applications benefit technique worst case scenario applications encrypt files disk encryptions file commonality whatsoever nonetheless lbfs significant bandwidth reduction common workloads remainder section discuss issues involved indexing chunks file system cache data describe advantages disadvantages approaches including lbfs solution describe actual lbfs protocol chunk indexes indexing client server lbfs index set files recognize data chunks avoid sending network save chunk transfers lbfs relies collisionresistant properties shahash function probability inputs shaproducing output lower probability hardware bit errors lbfs widely-accepted practice assuming hash collisions client server data chunks producing shahash assume chunk avoid transferring contents network central challenge indexing file chunks identify commonality keeping index reasonable size dealing shifting offsets index hashes aligned kbyte data blocks files transfer file sender transmit hashes file blocks receiver request blocks figure chunks file edits horizontal stripes show -byte regions magic hash values creating chunk boundaries gray shading shows regions file changed edit database single byte inserted start large file shift block boundaries change hashes file blocks thwart potential bandwidth savings alternative index files hashes overlapping kbyte blocks offsets scheme require storage times size indexed files index entry byte file data size tolerable large disks file modification require thousands index insertions cost performing updates index secondary storage prohibitive rsync practically tackles problem files time transferring file machine machine ifb file prime rsync guesses files similar attempts exploit fact simplified version rsync algorithm proceeds recipient breaks file prime non-overlapping contiguous fixedsize blocks transmits hashes blocks turn begins computing hashes overlapping blocks hashes matches prime avoids sending sections telling find data prime couple complications arise apply rsync algorithm file system rsync choice prime based filename simple editing file foo emacs creates auto-save file named foo rcs suggestive temporary file names recipient choose prime file names select prime based fixed-size sketch broder resemblance estimation technique ignoring additional cost approach reconstructed chunks multiple files outputs software libraries object files lbfs solution order chunks multiple files recipient lbfs takes approach rsync considers non-overlapping chunks files avoids sensitivity shifting file offsets setting chunk boundaries based file contents position file insertions deletions affect surrounding chunks similar techniques successfully past segment files purpose detecting unauthorized copying divide file chunks lbfs examines overlapping -byte region file probability region contents considers end data chunk lbfs selects boundary regions called breakpoints rabin fingerprints rabin fingerprint polynomial representation data modulo predetermined irreducible polynomial chose fingerprints efficient compute sliding window file low-order bits region fingerprint equal chosen region constitutes breakpoint assuming random data expected chunk size kbytes size -byte breakpoint window discussed section experimented window sizes found bytes provided good results effect window size huge figure shows lbfs divide file chunk boundaries series edits shows original file divided variable length chunks breakpoints determined hash -byte region shows effects inserting text file text inserted chunk producing larger chunk chunks remain send transfer file recipient version modifying file change number chunks shows effects inserting data breakpoint bytes inserted splitting chunk chunks file transfered sending chunks finally shows modification breakpoints eliminated chunks file combined chunk transmitted compose file pathological cases variable-sized chunks lead pathological behavior bytes file happened breakpoint instance index large file worse hashes chunks wire consume bandwidth sending file conversely file enormous chunks rabin fingerprint property long extent zeros breakpoint discussed section lbfs transmits contents chunk body rpc message arbitrary size rpc messages inconvenient rpc libraries hold messages memory unmarshal avoid pathological cases lbfs imposes minimum maximum chunk size minimum chunk size -byte region hashing magic breakpoint constitute breakpoint maximum chunk size file contents produce breakpoint lbfs artificially insert chunk boundaries artificial suppression creation breakpoints disrupt synchronization file chunks versions file risk occurs lbfs perform ordinary file system fortunately synchronization problems result stylized files instance long run zeros repeated sequences breakpoint files conventional compression lbfs rpc traffic conventionally compressed pathological cases necessarily translate slow file access chunk database lbfs database identify locate duplicate data chunks indexes chunk bits shahash database maps -bit keys file offset count triples mapping updated file modified keeping database sync file system potentially incur significant overhead files exported lbfs modified means instance local process server lbfs prevent growing inconsistent database client side inopportune crash potentially corrupt contents disk cache avoid synchronization problems lbfs relies correctness chunk database recomputes shahash data chunk reconstruct file lbfs recomputed shavalue detect hash collisions database -bit keys low non-negligible probability collision relying database integrity frees lbfs worry crash recovery turn saves lbfs making expensive synchronous database updates worst corrupt database degrade performance protocol lbfs protocol based nfs version nfs names files server-chosen opaque handles operations handles include reading writing data specific offsets lbfs adds extensions exploit inter-file commonality reads writes 
nfs clients poll server file open check permissions validate previously cached data recently accessed files lbfs saves round trip adding leases protocol unlike nfs clients lbfs practices aggressive pipelining rpc calls tolerate network latency system asynchronous rpc library efficiently supports large numbers simultaneously outstanding rpcs finally lbfs compresses rpc traffic conventional gzip compression file consistency lbfs client performs file caching future cache portions large files user opens file file local cache cached version date client fetches version server process written file closes client writes data back server lbfs three-tiered scheme determine file date client makes rpc file lbfs back read lease file lease commitment part server notify client modifications made file term lease default minute duration serverconfigurable user opens file lease file expired version file cache date meaning server version open succeeds immediately messages server user opens file lease file expired client asks server attributes file request implicitly grants client lease file client attributes modification inode change times file stored cache client version cache communication server finally file times changed client transfer contents server gethash offset count server file cache client sha database send normal read sha database send normal read sha database put sha database put sha database break file chunks offset count return data sha return data sha data sha data sha read sha size read sha size eof true sha size sha size sha size file reconstructed return user send gethash figure reading file lbfs lbfs close-to-open consistency modified file written back server closed lbfs write leases files server demands back dirty file files written back committed atomically client crashes cut network writing file file corrupted locked clients simply continue version multiple processes client file open writing lbfs writes data back process closes file multiple clients writing file close file win overwrite semantics similar afs file reads file reads lbfs make rpc procedure nfs protocol gethash gethash retrieves hashes data chunks file identify chunks exist client cache gethash takes arguments read rpc file handle offset size practice size maximum client practices file operations returning file data gethash returns vector shahash size pairs figure shows gethash downloading file cache client calls gethash obtain hashes file chunks chunks cache client issues regular read rpcs read rpcs pipelined downloading file generally incurs network-round trip times cost downloading data cache files larger chunks client issue multiple gethash calls incur multiple round trips network latency overlapped transmission disk file writes file writes proceed differently lbfs nfs nfs updates files server incrementally write lbfs updates atomically close time reasons atomic updates importantly previous version file chunks common current version keeping version helps lbfs exploit commonality lbfs file reconstruction protocol significantly alter order writes file files written back confusing intermediary states instance ascii file temporarily blocks finally atomic updates limit potential damage simultaneous writes clients clients writing file updates simultaneously changing file bad idea occur atomic updates ensure resulting file coherent contents written clients mishmash versions lbfs temporary files implement atomic updates server creates unique temporary file writes temporary file atomically commits contents real file updated writing temporary file lbfs chunks existing files save bandwidth rpcs implement update protocol mktmpfile tmpwrite condwrite committmp mktmpfile creates temporary file atomic update mktmpfile takes arguments hashnotfound server mktmpfile fhandle tmpwrite offset count data condwrite offset count sha condwrite offset count sha condwrite offset count sha client user closes file put sha database error copy data tmp file pick server sha send data file closed return user sha database write data tmp file sha database sha database write data tmp file create tmp file map client file server sha server commit break file chunks send sha hashes server write data tmp file target file committmp target fhandle server sha figure writing file lbfs file handle file eventually atomically updated client-chosen file descriptor temporary file receiving call server creates temporary file file system handle mapping per-client file descriptor temporary file clients choose descriptors temporary files pipeline operations temporary files mktmpfile rpc returns tmpwrite similar write rpc difference client-chosen temporary file descriptor replaces nfs file handle arguments lbfs client sends tmpwrites writes update file created mktmpfile condwrite similar tmpwrite rpc arguments file descriptor offset length actual data write condwrite arguments shahash data server find data hash file system writes data temporary file offset find data request completed condwrite returns special error code hashnotfound committmp commits contents temporary file permanent file error occurred takes arguments file descriptor temporary file file handle permanent file temporary file descriptor server track errors hashnotfound occured condwrite tmpwrite rpcs error occurred file descriptor disk full committmp fails server replaces contents target file temporary file updates chunk database reflect file contents lbfs tcp rpcs delivered order client pipeline committmp operation tmpwrite rpcs figure shows file write protocol action user closes file client write back client picks file descriptor issues mktmpfile rpc handle closed file response server creates temporary file handle maps file descriptor client makes condwrite rpcs data chunks file writing back condwrites returning hashnotfound client issues tmpwrite calls finally client issues committmp pipelining writes occurs stages client pipelines series condwrite requests mktmpfile rpc condwrite replies back client turns issues tmpwrite rpcs hashnotfound responses pipelines committmp immediately tmpwrite communication overhead generally round trip latencies transmission times rpcs large files client maximum limit number outstanding condwrite tmpwrite calls spend time sending calls process replies extra network round trips generally overlap transmission time rpc calls security considerations lbfs performs wider range networks file systems protocol resist index nfs index server tcp xfs client server lbfs client lbfschunk chunk figure overview lbfs implementation wider range attacks lbfs security infrastructure sfs server public key client administrator specifies command line mounting server future intend embed public keys pathnames sfs integrate lbfs sfs auto-mounting system unprivileged users clients access server entire lbfs protocol rpc headers passed gzip compression tagged message authentication code encrypted mount time client server negotiate session key server authenticates user user authenticates client public key cryptography finally note lbfs raise non-network security issues users share file system lbfs leak information files user allowed read specifically careful condwrite user check file 
system chunk data data resides readprotected file condwrite fail chunks user read subtle timing differences user infer database contained hash chunk nonetheless lbfs provide adequate security purposes widely users accept file systems encrypt network traffic implementation figure shows architecture lbfs implementation client server run user-level client implements file system xfs device driver bundled arla file system server accesses files nfs client server communicate tcp sun rpc asynchronous rpc library sfs toolkit server nfs client lbfs client server communication rpc library support authenticating encrypting traffic client server added support compression chunk index lbfs client server maintain chunk indexes server indexing file system contents client local cache share indexing code implemented b-tree sleepycat software berkeleydb package lbfs relies chunk database correctness concern crash recoverability lbfs avoids synchronous database updates server replies clients inserting chunks database database loses hashes clients simply bandwidth database back date utility mkdb builds file system database scratch lbfs server run database server simply creates database populates users access files database operation critical path clients lookup part condwrite rpc smallest files condwrites pipelined deeply overlap database lookups transmission write data found chunk index kbyte smaller files lbfs avoids condwrites simply writes files directly server single rpc overhead multiple round trip times overshadows potential bandwidth savings small files server implementation main goal lbfs server implementation saving bandwidth providing acceptable performance build system unobtrusively installed running file system isolates lbfs benefits physical file system layout lets users advantage lbfs existing files dedicating disk partition lbfs server accesses file system pretending nfs client effectively translating lbfs requests nfs building lbfs server nfs client lets lbfs serve file system nfs server exists includes file systems unix operating systems server alternatively implemented regular system calls access file system nfs offers advantages traditional system call interface simplifies implementation lbfs protocol based nfs nfs saved lbfs server implement access control server simply maps lbfs requests user ids tags resulting nfs requests ids letting nfs server decide grant access finally nfs chunk index resilient file system file renamed nfs file handle remains chunk index updated lbfs server creates trash directory lbfs trash root directory file system exports trash directory temporary files created mktmpfile rpcs explained committmp rpc lbfs server delete committed temporary file space needed garbage-collects random file trash directory background thread purges database pointers deleted files static i-number problem major disadvantage nfs lack lowlevel control file system data structures unix file system semantics dictate file i-number change file overwritten server commits temporary file target file copy contents temporary file target file simply rename temporary file place preserve target file i-number gratuitously inefficient copy clients access target file worse server crash leave file inconsistent state client restart file transfer commit fails related problem occurs file truncation applications truncate files immediately overwrite similar versions lbfs benefit previous contents truncated file reconstructing contents obvious solution move truncated files trash directory replace zero-length files nfs interface server changing truncated file i-number avoid losing contents trucated files lbfs delays deletion temporary files committmp rpcs truncated files copies trash directory versions reconstituted copies worth noting static i-number problem solved file system operation truncates file length atomically replaces contents file previous contents afford lose original contents inopportune crash case committmp lost data resent client truncate update operation efficient easy implement unix physical file system layouts situations serve efficient alternative rename operation atomically updating files current lbfs server make operation client implementation lbfs client xfs device driver xfs lets userlevel programs implement file system passing messages kernel device node dev chose xfs suitability whole-file caching driver notifies lbfs client contents file user opened file closed written back server lbfs client responsible fetching remote files storing local cache informs xfs bindings files users opened files local cache xfs satisfies read write requests directly cache call user-level code time evaluation section evaluates lbfs experiments examine behavior lbfs content-based breakpoint chunking static file sets measure bandwidth consumption network utilization lbfs common workloads compare cifs nfs version afs finally show lbfs improve end-to-end application performance compared afs cifs nfs experiments conducted identical ghz athlon computers mbytes ram rpm seagate ide drive ide drives slower common scsi drives penalizes lbfs performing disk operations file systems noted file system clients ran openbsd servers freebsd afs client version arla bundled bsd configured mbyte cache afs server openafs running linux microsoft word experiments ran office mhz ibm thinkpad laptop mbytes ram windows openafs mbyte cache clients servers experiments connected full-duplex mbit ethernet click modular router configured measure traffic impose bandwidth limitations delay loss click ran linux athlon machine repeated data files lbfs content-based breakpoint chunking scheme reduces bandwidth files versions file share common data fortunately occurs frequently practice table summarizes amount commonality found files examined emacs commonality files software development workload emacs source tree mbytes client source emacs cache data data size data overlap emacs source emacs build tree emacs emacs printf executable emacs emacs executable emacs installation emacs emacs elisp doc page original postscript msword doc edits original msword table amount data file directory older version download mbytes reconstitute source tree savings emacs build tree consumes mbytes disk space mbytes unique chunks writing build tree lbfs save server starts empty chunk database adding debugging printf emacs recompiling changing size executable binary common emacsand executables commonality full emacs installation consumes mbytes transferred client emacs cache examined document preparation workloads adding page front emacs lisp manual postscript file common previous added pages changed page numbering chapter commonality disappeared conclude lbfs suitable postscript document previewing instance tuning document equation substantial revisions document commonality pages numbered chapter microsoft word sprinkle lbfs paper windows disk performance found version overlap original investigate behavior lbfs chunking algorithm ran mkdb server usr local directory kbyte chunk size -byte moving window usr local contained mbytes data files mkdb broke files chunks chunks appeared files generated database consumed mbytes space size directory minutes generate database figure shows distribution chunk sizes median bytes close expected bytes breakpoints suppressed minimum chunk size requirement breakpoints inserted maximum chunk size limit note database chunks shorter chunks files shorter ends larger files end file chunk number 
chunks chunk size kbytes figure distribution chunk sizes usr local database x-axis represents chunk sizes kbytes y-axis shows number chunks size exp chunk size data shared chunks window window table percentage bytes shared chunks usr local chunk window sizes minimum chunk size expected chunk size boundary table shows amount data usr local appears shared chunks expected chunk sizes breakpoint window sizes expected smaller chunks yield greater commonality smaller common segments files isolated increased cost gethash condwrite traffic smaller chunks outweighed increased bandwidth savings tests performed window size large effect commonality practical workloads workloads evaluate lbfs ability reduce bandwidth workload msword open mbyte microsoft word document make edmsword gcc normalized bandwidth cifs nfs afs leases gzip lbfs lbfs figure normalized bandwidth consumed workloads bars workload show upstream bandwidth downstream bandwidth results normalized upstream bandwidth cifs nfs table measure cost save close file workload gcc simply recompile emacs source workload involves making series perl source tree transform perl expect language scripted text editor based output diff benchmark saves files text insertion operations lines editing emacs creates auto-save files characters typed workloads reflect common activities document editing software development ran benchmark client native network file system cifs windows nfs udp unix ran benchmarks afs isolate benefits exploiting file commonality additionally measured leases gzip file system lbfs file caching leases data compression chunking scheme finally ran workloads lbfs msword benchmark lbfs runs unix ran samba server openbsd lbfs client re-exporting lbfs file system cifs saved word document windows client samba server mbit ethernet measured traffic click router samba server lbfs server msword experiments conducted warm cache original files written client file system test lbfs server output files previous runs benchmark database gcc benchmark emacs sources unpacked file system test emacs previously compiled file system intent simulate modifies header file requires entire project recompiled successive compilations emacs produce executable substantial commonalwe enable word fast saves feature reduced write bandwidth improved running time nfs tcp performs worse nfs udp extensively tested tuned ity object files created benchmark server trash directory isolate benefit measured compilation emacs lbfs server started database chunks previous compiles bug expect openbsd freebsd client instances benchmark msword benchmark ran warm client cache server previously output files ran ssh remote login connection compare distributed file system running text editor remotely simulate type-ahead benchmark sends line time waits line echo bandwidth utilization figure shows bandwidth consumed client writing reading server workloads bandwidth numbers obtained byte counters click router experiment router impose delay loss bandwidth limitations ttcp reported tcp throughput mbit sec client server ping reported round-trip time case separately report upstream traffic client server downstream traffic server client numbers normalized upstream client server bandwidth native file system cifs windows nfs unix afs leases gzip lbfs large ondisk caches systems reduce amount downstream bandwidth server client compared native file systems upstream bandwidth drops cifs nfs bandwidth afs bandwidth represent savings gained deferring writes close time eliding overwrites data drops afs bandwidth leases gzip bandwidth represent savings compression finally drops leases gzip bandwidth lbfs bandwidth represent savings gained chunking scheme msword workload savings provided msword gcc normalized execution time cifs lan cifs nfs lan nfs afs leases gzip lbfs lbfs msword gcc uplink utilization figure normalized application performance top file systems cable modem link kbit sec uplink mbit sec downlink execution times normalized cifs nfs results execution times seconds top bars uplink bandwidth utilization msword gcc benchmarks chunking scheme commonality versions document commonality large temporary files word creates saves lbfs reduce upstream bandwidth times leases gzip times afs times cifs careful analysis reveals unix samba server closes reopens temporary files requiring transferred multiple times multiple transfers largely negate benefits gzip compression leases gzip contrast lbfs exploits files common contents close consuming unnecessary traffic afs slightly bore bandwidth leases gzip extra closes artifact unix samba server windows afs implementation performs partial file caching gcc benchmark savings provided chunking scheme fact compiled object files libraries executables similar identical files server trash directory chunks written server object files differ files evicted trash directory case lbfs reduce upstream bandwidth times leases gzip times afs times nfs benefit object files database lbfs reduces upstream bandwidth utilization object files libraries executables share common data started empty chunk database lbfs upstream bandwidth leases gzip case savings provided chunking scheme writing versions files share common chunks older revisions lbfs reduce upstream bandwidth factor leases gzip afs nfs application performance figure shows normalized end-to-end application performance workloads simulated cable modem link mbit sec downstream bandwidth server client kbit sec upstream bandwidth client server round-trip latency execution times normalized cifs nfs results comparison show execution times native file system mbit sec full-duplex lan msword workload lbfs reduce execution times potentially unusable seconds cifs tolerable seconds times faster fact afs takes seconds run benchmark lan cifs takes seconds gcc workload seconds lbfs populated database times faster leases gzip times faster afs times faster nfs faster nfs lan server database lbfs reduces execution time leases gzip slower nfs lan msword gcc workloads figure shows lbfs reduces network utilization percentage bandwidth file system lbfs gcc kbit upstream link contrast gcc nfs afs msword benchmarks lbfs reduce upstream network utilization afs cifs figure examines effects network bandwidth performance gcc workload lbfs leases gzip afs experiments simulated network fixed round trip time graph shows lbfs affected reduction network bandwidth lbfs reduces read execution time bandwidth mbps afs leases gzip lbfs figure performance gcc workload bandwidths fixed round-trip time execution time round trip time afs leases gzip lbfs figure performance gcc workload range round-trip times fixed mbit sec symmetric links write bandwidth required workload point cpu network latency bandwidth limiting factors figure examines effects network latency lbfs leases gzip afs performance experiments simulated network symmetric mbit links gcc workload bandwidth leases gzip performance workload lbfs leases gzip roughly network bandwidth high hand gcc afs significantly bandwidth performs worse lbfs leases gzip graph shows execution time gcc workload degrades similarly file systems latency increases figure shows lbfs performance benchmark improvement leases gzip afs nfs execution time measure performance interactive workloads users care delays differentiate 
smaller nonetheless affect runtime scripted benchmark long delays caused tcp entering backoff state ran shortened version benchmark network execution time loss rate ssh afs leases gzip lbfs figure performance shortened benchmark loss rates network fixed mbit sec symmetric links fixed round-trip time simulated packet loss comparing performance network file systems ssh remote login program figure compares file systems ssh loss rates packet loss ssh slower file system difference affect performance rate users type loss rate increases delays imposed tcp backoff mechanism ssh packets flight lost packet puts tcp backoff imposing delay seconds user waits typed characters echo file systems outperform ssh reasons lbfs leases gzip experience fewer losses sending fewer total packets ssh file systems consume bandwidth send data packet file systems transfer large files tcp packets flight allowing recover single loss fast retransmission avoid backoff afs udp tcp reduce sending rate precipitously tcp face packet loss conclude kind editing benchmark preferable network file system run editor remotely summary lbfs network file system saves bandwidth taking advantage commonality files lbfs breaks files chunks based contents hash function small regions file determine chunk boundaries indexes file chunks hash values subsequently chunks reconstruct files data sending data network common operations editing documents compiling software lbfs consume order magnitude bandwidth traditional file systems dramatic savings bandwidth makes lbfs practical situations file systems situations lbfs makes transparent remote file access viable frustrating alternative running interactive programs remote machines acknowledgments chuck blake frans kaashoek butler lampson robert morris marc waldman anonymous reviewers feedback suggestions chuck blake frank dabek hardware nickolai zeldovich setting afs chuck blake frank dabek bryan ford assistance xfs brian berliner cvs parellizing software development proceedings winter usenix technical conference colorado springs david bindel yan chen patrick eaton dennis geels ramakrishna gummadi sean rhea hakim weatherspoon westley weimer westley weimer christopher wells ben zhao john kubiatowicz oceanstore exteremely wide-area storage system proceedings international conference architectural support programming languages operating systems pages boston november sergey brin james davis hector garcia-molina copy detection mechanisms digital documents proceedings acm sigmod international conference management data pages san jose andrei broder resemblance containment documents compression complexity sequences pages callaghan pawlowski staubach nfs version protocol specification rfc network working group june fips secure hash standard department commerce national technical information service springfield april cary gray david cheriton leases efficient fault-tolerant mechanism distributed file cache consistency proceedings acm symposium operating systems principles pages litchfield park december orn onvall assar westerlund stephen pink design multicast-based distributed file system proceedings symposium operating system design implementation pages orleans february john howard michael kazar sherri menees david nichols satyanarayanan robert sidebotham michael west scale performance distributed file system acm transactions computer systems february james kistler satyanarayanan disconnected operation coda file system acm transactions computer systems february eddie kohler robert morris benjie chen john jannotti frans kaashoek click modular router acm transactions computer systems november yui-wah lee kwong-sak leung satyanarayanan operation-based update propagation mobile file system proceedings usenix technical conference monterey june udi manber finding similar files large file system proceedings winter usenix technical conference san francisco january timothy mann andrew birrell andy hisgen chuck jerian garret swart coherent distributed file cache directory write-behind acm transactions computer systems david mazi eres toolkit user-level file systems proceedings usenix technical conference boston june david mazi eres michael kaminsky frans kaashoek emmett witchel separating key management file system security proceedings acm symposium operating systems principles pages kiawa island jeffrey mogul fred douglis anja feldmann balachander krishnamurthy potential benefits delta encoding data compression http proceedings acm sigcomm conference pages cannes france september karin petersen mike spreitzer douglas terry flexible update propagation weakly consistent replication proceedings acm symposium operating systems principles pages saint-malo france michael rabin fingerprinting random polynomials technical report tr- center research computing technology harvard shepler callaghan robinson thurlow beame eisler noveck nfs version protocol rfc network working group december neil spring david wetherall protocol independent technique eliminating redundant network traffic proceedings acm sigcomm conference pages stockholm sweden august walter tichy string-to-string correction problem block moves acm transactions computer systems november andrew tridgell efficient algorithms sorting synchronization phd thesis australian national april assar westerlund johan danielsson arla free afs client proceedings usenix freenix track orleans june usenix haifeng amin vahdat design evaluation continuous consistency model replicated services proceedings symposium operating systems design implementation san diego 
wide-area cooperative storage cfs frank dabek frans kaashoek david karger robert morris ion stoica mit laboratory computer science chord lcs mit http pdos lcs mit chord abstract cooperative file system cfs peer-to-peer readonly storage system provable guarantees efficiency robustness load-balance file storage retrieval cfs completely decentralized architecture scale large systems cfs servers provide distributed hash table dhash block storage cfs clients interpret dhash blocks file system dhash distributes caches blocks fine granularity achieve load balance replication robustness decreases latency server selection dhash finds blocks chord location protocol operates time logarithmic number servers cfs implemented sfs file system toolkit runs linux openbsd freebsd experience globally deployed prototype shows cfs delivers data clients fast ftp controlled tests show cfs scalable servers block data involves contacting servers tests demonstrate perfect robustness unimpaired performance half servers fail introduction existing peer-to-peer systems napster gnutella freenet demonstrate benefits cooperative storage serving fault tolerance load balance ability harness idle storage network resources accompanying benefits number design challenges peer-to-peer architecture symmetric decentralized operate unmanaged volunteer participants finding desired data large system fast servers join leave system frequently affecting robustness efficiency load balanced servers peer-to-peer systems common solve problems california berkeley research sponsored defense advanced research projects agency darpa space naval warfare systems center san diego contract solves cfs cooperative file system design meets challenges cfs file system exists set blocks distributed cfs servers cfs client software interprets stored blocks file system data meta-data presents ordinary read-only file-system interface applications core cfs software consists layers dhash chord dhash distributed hash layer performs block fetches client distributes blocks servers maintains cached replicated copies dhash chord distributed lookup system locate servers responsible block table summarizes cfs software layering layer responsibility interprets blocks files presents file system interface applications dhash stores unstructured data blocks reliably chord maintains routing tables find blocks chord implements hash-like operation maps block identifiers servers chord assigns server identifier drawn -bit identifier space block identifiers identifiers thought points circle mapping chord implements takes block yields block successor server closely block identifier circle implement mapping chord maintains server table information cvc servers total number servers chord lookup sends messages cvc servers consult tables cfs find data efficiently large number servers servers join leave system table updates dhash layers block management top chord dhash load balance popular large files arranging spread blocks file servers balance load imposed popular small files dhash caches block servers consulted future chord lookups block dhash supports pre-fetching decrease download latency dhash replicates block small number servers provide fault tolerance dhash enforces weak quotas amount data server inject deter abuse finally dhash control number virtual servers server provide control data server store behalf cfs implemented sfs toolkit paper reports experimental results small international deployment cfs servers large-scale controlled test-bed results confirm contributions cfs design aggressive approach load balance spreading file blocks randomly servers download performance internet-wide prototype deployment fast standard ftp provable efficiency provably fast recovery times failure simple algorithms achieve results cfs operational prompt refinements design potential area improvement ability chord lookup algorithm tolerate malicious participants verifying routing information received servers area cfs address anonymity expected anonymity needed layered top basic cfs system remainder paper organized section discusses related work section outlines design design goals cfs section describes chord location protocol section presents detailed design cfs section describes implementation details section presents experimental results section discusses open issues future work finally section summarizes findings concludes paper related work cfs inspired napster gnutella freenet cfs peer-to-peer distributed hashing similar spirit number ongoing research projects comparison existing peer-to-peer file sharing systems cfs offers simplicity implementation high performance compromising correctness cfs balances server load finds data quickly clients guarantees data availability face server failures high probability cfs complete system individual aspects common existing systems major relationships summarized naming authentication cfs authenticates data naming public keys contenthashes distributed storage systems content-hashes securely link pieces data due merkle public keys authentically data due sfs system cfs adopts naming authentication file system structure ideas sfsro implements secure distributed readonly file system file system files modified owner complete replacement file sfsro cfs significant differences architectural mechanism levels sfsro defines protocols authentication mechanisms client retrieve data server cfs adds ability dynamically find server holding desired data chord location service increases robustness availability cfs set servers transparent clients peer-to-peer search napster gnutella arguably widely peer-to-peer file systems today present keyword search interface clients retrieving uniquely identified data result search engines distributed hash tables trade scalability power gnutella broadcasts search queries machines napster performs searches central facility cfs paper doesn provide search developing scalable distributed search engine cfs mojo nation broadcast query peer-to-peer storage system divides files blocks secret sharing algorithm distribute blocks number hosts cfs divides files blocks secret sharing anonymous storage freenet probabilistic routing preserve anonymity clients publishers servers anonymity requirement limits freenet reliability performance freenet avoids associating document predictable server avoids forming globally coherent topology servers means unpopular documents simply disappear system server responsibility maintaining replicas means search visit large fraction freenet network byzantine generals problem leslie lamport robert shostak marshall pease sri international reliable computer systems handle malfunctioning components give conflicting information parts system situation expressed abstractly terms group generals byzantine army camped troops enemy city communicating messenger generals agree common battle plan traitors confuse problem find algorithm ensure loyal generals reach agreement shown oral messages problem solvable two-thirds generals loyal single traitor confound loyal generals unforgeable written messages problem solvable number generals traitors applications solutions reliable computer systems discussed categories subject descriptors computer-communication networks distributed systems--network operating hong shows figure network servers lookup path length exceed hops means hop count limited lookup fail document cfs provide anonymity guarantee tighter bounds lookup cost -node system lookups essentially exceed hops cfs caching scheme similar freenet sense leave cached copies data query path client data found cfs finds data significantly fewer hops freenet cfs structured lookup paths overlap freenet cfs make quantity cache space freenet publius focuses anonymity achieves encryption secret sharing routing publius requires static globally-known list servers stores share fixed location predictable file free haven cryptography routing re-mailers provide anonymity gnutella free haven finds data global search cfs attempt provide anonymity focusing efficiency robustness intertwining anonymity basic data lookup mechanism interferes correctness performance hand robust location storage layer anonymous client access cfs provided separate anonymizing proxies techniques similar proposed chaum reiter rubin peer-to-peer hash based systems cfs layers storage top efficient distributed hash lookup algorithm number recent peer-to-peer systems similar approaches similar scalability performance including past oceanstore ohaha detailed comparison algorithms found past storage system differs cfs approach load balance past server stores files server disk space 
store large file system sufficient free space past server solves offloading files responsible servers spare disk space past handles load serving popular files caching lookup path cfs stores blocks files spreads blocks evenly servers prevents large files causing unbalanced storage cfs solves related problem servers amounts storage space mechanism called virtual servers server managers control disk space consumption cfs block storage granularity helps handle load serving popular large files serving load spread servers blocks chord cfs client cfs server cfs server dhash chord dhash dhash chord figure cfs software structure vertical links local apis horizontal links rpc apis space-efficient large files whole-file caching cfs relies caching files small distributing blocks effective evaluating performance impact block storage granularity purposes paper oceanstore aims build global persistent storage utility data privacy client updates guarantees durable storage features price complexity oceanstore byzantine agreement protocol conflict resolution complex protocol based plaxton trees implement location service oceanstore assumes core system maintained commercial providers ohaha consistent hashing map files keyword queries servers freenet-like routing algorithm locate files result shares weaknesses freenet web caches content distribution networks cdns akamai handle high demand data distributing replicas multiple servers cdns typically managed central entity cfs built resources shared owned cooperative group users proposed scalable cooperative web caches locate data systems multicast queries require servers servers result proposed methods highly scalable robust addition load balance hard achieve content cache depends heavily query pattern cache resolver cfs consistent hashing evenly map stored data servers cache resolver assumes clients entire set servers maintaining up-to-date server list difficult large peer-to-peer system servers join depart unpredictable times design overview cfs distributed read-only file storage structured collection servers provide block-level storage publishers producers data clients consumers data layer file system semantics top block store ordinary file system layered top disk unrelated publishers store separate file systems single cfs system cfs design intended support possibility single world-wide system consisting millions servers system structure figure illustrates structure cfs software cfs client software layers file system client dhash block directory data block data block block root block public key signature inode figure simple cfs file system structure root-block identified public key signed private key blocks identified cryptographic hashes contents storage layer chord lookup layer client file system dhash layer retrieve blocks client dhash layer client chord layer locate servers hold desired blocks cfs server software layers dhash storage layer chord layer server dhash layer responsible storing keyed blocks maintaining proper levels replication servers caching popular blocks server dhash chord layers interact order integrate block identifier checking cached copies block cfs servers oblivious file system semantics simply provide distributed block store cfs clients interpret dhash blocks file system format adopted systems operating systems communications management-network communication operating systems reliability--fault tolerance general terms algorithms reliability additional key words phrases interactive consistency introduction reliable computer system cope failure components failed component exhibit type behavior overlooked--namely sending conflicting information parts system problem coping type failure expressed abstractly byzantine generals problem devote major part paper discussion abstract problem conclude indicating solutions implementing reliable computer system imagine divisions byzantine army camped enemy city division commanded general generals communicate messenger observing enemy decide common plan action generals research supported part national aeronautics space administration contract nas mod ballistic missile defense systems command contract dasg -cand army research office contract daag -cauthors address computer science laboratory sri international ravenswood avenue menlo park permission copy fee part material granted provided copies made distributed direct commercial advantage acm copyright notice title date notice copying permission association computing machinery copy republish requires fee specific permission acm acm transactions programming languages systems vol july pages byzantine generals problem traitors prevent loyal generals reaching agreement generals algorithm guarantee loyal generals decide plan action loyal generals algorithm traitors algorithm guarantee condition traitors loyal generals reach agreement agree reasonable plan insure small number traitors loyal generals adopt bad plan condition hard formalize requires precisely bad plan attempt generals reach decision general observes enemy communicates observations information communicated ith general general method combining values single plan action number generals condition achieved generals method combining information condition achieved robust method decision made attack retreat con general opinion option final decision based majority vote small number traitors affect decision loyal generals equally divided possibilities case decision called bad approach satisfy conditions assumes method generals communicate values obvious method ith general send messenger general work satisfying condition requires loyal general obtain values traitorous general send values generals condition satisfied true loyal general obtain information condition implies general necessarily obtained directly ith general traitorous ith general send values generals means careful meeting condition introduce possibility generals ith general--even ith general loyal happen condition met permit traitors loya generals base decision values retreat retreat loyal general attack requirement ith general loyal sends loyal general acm transactions programming languages systems vol july lamport shostak pease rewrite condition condition ith general loyal loyal generals conditions conditions single ith general restrict consideration problem single general sends phrase terms commanding general sending order lieutenants obtaining problem byzantine generals problem commanding general send order lieutenant generals loyal lieutenants obey order commanding general loyal loyal lieutenant obeys order sends conditions called interactive consistency conditions note commander loyal commander loyal solve original problem ith general sends solution byzantine generals problem send order generals acting lieutenants impossibility results byzantine generals problem deceptively simple difficulty surprising fact generals send oral messages solution work two-thirds generals loyal generals solution work presence single traitor oral message contents completely control sender traitorous sender transmit message message corresponds type message computers send section signed written messages true show oral messages solution generals handle single traitor simplicity case decisions attack retreat examine scenario pictured sfsro format similar figure unix file system commander loyal dhash blocks sends block attack identifiers order place disk blocks lieutenant disk traitor addresses reports shown lieutenant figure received block retreat piece order file piece file system satisfied meta-data lieutenant obey directory order maximum attack size block scenario shown order tens figure kilobytes commander parent block traitor identifiers sends children attack order publisher inserts lieutenant file system retreat order blocks lieutenant cfs lieutenant system hash traitor block content content-hash message identifier commander publisher signs lieutenant root block scenarios private pictures key inserts root block cfs public key root block identifier clients file system public key check integrity root block key integrity blocks lower tree content-hash identifiers refer blocks approach guarantees clients authentic internally consistent view file system circumstances client version recently updated file system cfs file system read-only clients concerned file system updated publisher involves updating file system root block place make point data cfs authenticates updates root blocks checking block signed key block timestamp prevents replays updates cfs file systems updated changing root block identifier external data changed data updated cfs stores data agreed-upon finite interval publishers indefinite storage periods periodically cfs extension cfs server discard data guaranteed period expired cfs explicit delete operation publisher simply stop extensions area replication caching policies cfs relies assumption large amounts spare disk space cfs properties cfs consistency integrity file systems adopting sfsro file system format cfs extends sfsro providing desirable desirable properties decentralized control cfs servers share administrative relationship publishers cfs servers ordinary internet hosts owners volunteer spare storage network resources scalability cfs lookup operations space messages logarithmic number servers availability client retrieve data long trapped small partition underlying network long data replicas reachable underlying network true servers constantly joining leaving cfs system cfs places replicas servers unrelated network locations ensure independent failure load lieutenant traitor lies consistently lieutenant distinguish situations obey attack order lieutenant receives attack order commander obey acm transactions programming languages systems vol july byzantine generals problem retreat fig lieutenant traitor retreat fig commander traitor similar argument shows lieutenant receives retreat order commander obey lieutenant tells commander attack scenario figure lieutenant obey retreat order lieutenant obeys attack order violating condition solution exists generals works presence single traitor argument convincing strongly advise reader suspicious nonrigorous reasoning result correct equally plausible proofs invalid results area computer science mathematics informal reasoning lead errors study type algorithm rigorous proof impossibility three-general solution handle single traitor refer reader result show solution fewer generals cope traitorsj proof contradiction--we assume precisely solution exists generals problem trivial generals acm transactions programming languages systems vol july lamport shostak pease solution group fewer construct three-general solution byzantine generals problem works traitor impossible avoid confusion algorithms call generals assumed solution albanian generals constructed solution byzantine generals starting algorithm fewer albanian generals cope traitors construct solution byzantine generals handle single traitor three-general solution obtained byzantine generals simulate approximately one-third albanian generals byzantine general simulating albanian generals byzantine commander simulates albanian commander albanian lieutenants byzantine lieutenants simulates albanian lieutenants byzantine general traitor simulates albanians albanian generals traitors assumed solution guarantees hold albanian generals albanian lieutenants simulated loyal byzantine lieutenant obey order order obey easy check conditions albanian generals solution imply conditions byzantine generals constructed required impossible solution difficulty solving byzantine generals problem stems requirement reaching exact agreement demonstrate case showing reaching approximate agreement hard reaching exact agreement assume agree precise battle plan generals agree approximate time attack precisely assume commander orders time attack require conditions hold loyal lieutenants attack minutes commanding general loyal loyal lieutenant attacks minutes time commander order assume orders processed day attack time order received irrelevant--only attack time order matters byzantine generals problem problem unsolvable two-thirds generals loyal prove showing solution generals coped traitor construct three-general solution byzantine generals problem worked presence traitor suppose commander wishes send attack retreat order orders attack sending attack time orders retreat sending attack time assumed algorithm lieutenant procedure obtain order receiving attack time commander lieutenant time earlier attack time retreat continue step acm transactions programming languages systems vol july byzantine generals problem lieutenant decision reached step lieutenant reached decision make decision retreat commander loyal loyal lieutenant obtain correct order step satisfied commander 
loyal prove assumption commander traitor traitor means lieutenants loyal ici lieutenant decides attack step decide retreat step decision step defer decision step case easy arrive decision satisfied constructed three-general solution byzantine generals problem handles traitor impossible three-general algorithm maintains ici presence traitor method general simulate prove solution fewer generals cope traitors proof similar original byzantine generals problem left reader solution oral messages showed solution byzantine generals problem oral messages cope traitors generals give solution works generals oral messages general supposed execute algorithm involves sending messages generals assume loyal general correctly executes algorithm definition oral message embodied assumptions make generals message system message delivered correctly receiver message absence message detected assumptions prevent traitor interfering communication generals interfere messages send confuse intercourse introducing spurious messages assumption foil traitor prevent decision simply sending messages practical implementation assumptions discussed section algorithms section require general send messages directly general section describe algorithms requirement traitorous commander decide send order lieutenants obey order default order obey case retreat default order acm transactions progranuning languages systems vol july lamport shostak pease inductively define oral message algorithms nonnegative integers commander sends order lieutenants show solves byzantine generals problem generals presence traitors find convenient describe algorithm terms lieutenants obtaining obeying order algorithm assumes function majority property majority values equal majority equals assumes sequence functions--one natural choices majority majority exists retreat median assuming ordered set algorithm requires aforementioned property majority algorithm commander sends lieutenant lieutenant receives commander retreat receives algorithm commander sends lieutenant lieutenant receives commander retreat eives lieutenant acts commander algorithm send lieutenants lieutenant received lieutenant step algorithm retreat received lieutenant majority understand algorithm works case figure illustrates messages received lieutenant commander sends lieutenant traitor step commander sends lieutenants step lieutenant sends lieutenant trivial algorithm step traitorous lieutenant sends lieutenant step lieutenant obtains correct majority commander traitor figure shows values received lieutenants traitorous commander sends arbitrary values lieutenants lieutenant obtains obtain majority step values equal recursive algorithm invokes separate executions algorithm invokes executions means lieutenant sends separate messages lieutenant distinguish messages reader verify ambiguity removed lieutenant prefixes number sends step recursion unfolds algorithm called times send prefixed sequence lieutenants numbers acm transactions programming languages systems vol july lamport shostak pease prove correctness algorithm arbitrary prove lemma lemma algorithm satisfies generals traitors proof proof induction specifies happen commander loyal easy trivial algorithm works commander loyal lemma true assume true prove step loyal commander sends lieutenants step loyal lieutenant applies generals hypothesis apply induction hypothesis conclude loyal lieutenant loyal lieutenant traitors majority lieutenants loyal loyal lieutenant majority values obtains majority step proving theorem asserts algorithm solves byzantine generals problem theorem algorithm satisfies conditions generals traitors proof proof induction traitors easy satisfies assume theorem true balance cfs ensures burden storing serving data divided servers rough proportion capacity maintains load balance data popular combination caching spreading file data servers persistence cfs commits storing data agreed-on interval quotas cfs limits amount data address insert system degree protection malicious attempts exhaust system storage efficiency clients fetch cfs data delay comparable ftp due cfs efficient lookup algorithms caching pre-fetching server selection sections present chord dhash provide properties chord layer cfs chord protocol locate blocks chord supports operation key determine node responsible key chord store keys values primitives higher-layer software build wide variety storage systems cfs chord primitive rest section summarizes chord describes algorithms robustness server selection support applications cfs consistent hashing chord node unique -bit node identifier obtained hashing node address virtual node index chord views ids occupying circular identifier space keys mapped space hashing -bit key ids chord defines node responsible key successor key successor node smallest greater equal wrap-around consistent hashing consistent hashing lets nodes enter leave network minimal movement keys maintain correct successor mappings node joins network keys previously assigned successor assigned node leaves network assigned keys reassigned successor assignment keys nodes occur consistent hashing straightforward implement constant-time lookups nodes up-to-date list nodes system scale chord scalable distributed version consistent hashing chord lookup algorithm chord node data structures perform lookups successor list finger table successor list required correctness chord careful maintain accuracy finger table accelerates lookups accurate chord aggressive maintaining discussion describes perform correct slow lookups successor list describes accelerate finger table discussion assumes malicious participants chord protocol nodes verify routing information chord participants send algorithms left future work chord node maintains list identities addresses successors chord ring fact node successor means prove case commander loyal taking equal lemma satisfies commander loyal verify case commander traitor traitors commander lieutenants traitors generals lieutenants apply induction hypothesis conclude satisfies conditions loyal lieutenants step lieutenants lieutenant loyal lieutenants vector values vnand obtain majority vnin step proving solution signed messages scenario figures traitors ability lie makes byzantine generals problem difficult problem easier solve restrict ability generals send unforgeable signed messages precisely add acm transactions programming languages systems vol july byzantine generals problem assumption loyal general signature forged alteration contents signed messages detected verify authenticity general signature note make assumptions traitorous general signature signature forged traitor permitting collusion traitors introduced signed messages previous argument generals required cope traitor longer holds fact three-general solution exist give algorithm copes traitors number generals problem vacuous fewer generals algorithm commander sends signed order lieutenants lieutenant adds signature order sends lieutenants add signatures send means lieutenant effectively receive signed message make copies sign send copies matter copies obtained single message photocopied message consist stack identical messages signed distributed required algorithm assumes function choice applied set orders obtain single requirements make function set consists single element choice choice retreat empty set note definition choice median element v--assuming ordering elements algorithm denote signed general denotes signed signed general commander algorithm lieutenant maintains set containng set properly signed orders received commander loyal set single element confuse set orders received set messages received messages order algorithm initially commander signs sends lieutenant lieutenant receives message form commander received order lets equal sends message lieutenant acm transactions programming languages systems vol july lamport shostak pease attack retreat fig algorithm commander traitor lieutenant receives message form set adds sends message lieutenant lieutenant receive messages obeys order choice note step lieutenant ignores message order set lieutenant determines step receive messages induction easily shows sequence lieutenants lieutenant receive message form step require lieutenant send message send message reporting send message easy decide messages received assumption lieutenant determine traitorous lieutenant sends messages alternatively time-out determine messages arrive time-out discussed section note step lieutenant ignores messages proper form string signatures packets identical messages avoid copy messages means throws packet consist sufficient number identical properly signed messages copies message signed lieutenants figure illustrates algorithm case generals commander traitor commander sends attack order lieutenant retreat order lieutenants receive orders step step ffi attack retreat obey order choice attack retreat observe unlike situation figure lieutenants commander traitor signature acm transactions programming languages systems vol july byzantine generals problem appears orders states generated signatures algorithm lieutenant signs acknowledge receipt order ruth lieutenant add signature order signature relayed recipient superfluous precisely assumption makes unnecessary lieutenants sign messages prove correctness algorithm theorem algorithm solves byzantine generals problem traitors proof prove commander loyal sends signed order lieutenant step loyal lieutenant receive order step traitorous lieutenant forge message form loyal lieutenant receive additional order step loyal lieutenant set obtained step consists single order obey step property choice function proves commander loyal prove case commander traitor loyal lieutenants obey order step sets orders receive step prove suffices prove puts order step thenj put order step show receives properly signed message order receives order step sends step soj receives adds order step receive message form ifj received order cases case sends message soj receive order commander traitor lieutenants traitors lieutenants loyal loyal lieutenant received receive completes proof missing communication paths assumed general send messages directly general remove assumption suppose physical barriers 
place restrictions send messages generals form nodes simple finite undirected graph arc nodes generals send messages simple graph arc joining nodes arc connects distinct nodes acm transactions programming languages systems vol july lamport shostak pease fig -regular graph fig graph -regular directly extend algorithms assumed completely connected general graphs extend oral message algorithm definition generals neighbors joined arc definition set nodes regular set neighbors node neighbor general exist paths passing paths node common graph p-regular node regular set neighbors consisting ofp distinct nodes figure shows simple -regular graph figure shows graph -regular central node regular set neighbors nodes extend algorithm solves byzantine generals problem presence traitors graph generals m-regular note m-regular graph nodes allpositive integers define algorithm graph generals isp-regular defined notp-regular definition induction algorithm choose regular set neighbors commander consisting ofp lieutenants commander sends lieutenant lieutenant receives commander retreat receives lieutenant sends lieutenant sending path existence guaranteed part definition acting commander algorithm graph generals obtained removing original commander acm transactions programming languages systems vol july byzantine generals problem lieutenant received lieutenant step retreat received lieutenant majority note removing single node p-regular graph leaves regular graph apply algorithm step prove solves byzantine generals problem traitors proof similar proof algorithm sketched begins extension lemma lemma algorithm satisfies traitors proof observe lieutenant obtains majority commander path disjoint path send values traitors half paths composed loyal lieutenants commander loyal majority values equal implies satisfied assume lemma commander loyal lieutenants correct majority loyal induction hypothesis sends correct loyal lieutenant loyal lieutenant majority correct values obtaining correct node process lookup correctly desired key node successor node key successor lookup forwarded successor moves lookup strictly closer destination node 
learns successors joins chord ring existing node perform lookup successor asks successor successor list entries list provide fault tolerance node successor respond node substitute entry successor list successors simultaneously fail order disrupt chord ring event made improbable modest values implementation fixed chosen bed foreseeable maximum number nodes main complexity involved successor lists notifying existing node node successor stabilization procedure guarantees preserve connectivity chord ring successor pointers lookups performed successor lists require average bpbe message exchanges number servers reduce number messages required cvc node maintains finger table table entries entry table node identity node succeeds cxa circle node identities nodes power-of-two intervals circle position node initializes finger table querying existing node existing nodes finger table successor list entries refer node find periodic lookups figure shows pseudo-code successor identifier cxcs main loop find predecessor sends preceding node list rpcs succession nodes rpc searches tables node nodes closer cxcs finger table entries point nodes power-oftwo intervals ring iteration set node halfway ring current cxcs preceding node list returns greater cxcs process overshoot correct successor under-shoot node recently joined cxcs case check cxcs bpbe bnd bmsuccessorcl ensures node find cxcs successor finds cxcs predecessor asks predecessor successor bmacd crcrctd cxcsb find predecessorb cxcsb return bmsuccessorb node find cxcs predecessor bmacd ctcsctcrctd cxcsb cxcs bpbe bnd bmsuccessorb clb bmpreceding node listb cxcsb bpd cpdcd bcbc bcbc alive return node list nodes finger table successor list precede cxcs bmd ctcrctcscxd csct cxd cxcsb return cud becufingers successorscv bncxcsclcv figure pseudo-code find successor node identifier cxcs remote procedure calls preceded remote node find predecessor persists finds pair nodes straddle cxcs aspects lookup algorithm make robust rpc preceding node list node returns list nodes believes desired cxcs make progress successor cxcs unresponsive lookup fail loop ensures find predecessor long find node closer cxcs long nodes careful maintain correct successor pointers find predecessor eventually succeed usual case nodes correct finger table information iteration loop eliminates half remaining distance target means hops early lookup travel long distances space hops travel small distances efficacy caching mechanism section depends observation theorems proved accompanying technical report show success performance chord lookups affected massive simultaneous failures theorems assume successor list length cvc chord ring stable node successor list correct theorem network initially stable node fails probability high probability find successor returns closest living successor query key theorem network initially stable node fails probability expected time execute find successor cvc evaluation section validates theorems experimentally server selection chord reduces lookup latency preferentially contacting nodes nearby underlying network server selection added chord part work cfs part chord originally published step find predecessor figure node lookup choose hop set nodes initially set contents routing tables subsequently set list nodes returned preceding node list rpc previous hop node tells measured latency node set collected latencies acquired finger table entries choices next-hop node query distances ring impose rpc latencies calculation seeks pick combination chord estimates total cost bvb node set potential hops bvb csa onesb cxcsb bqbq bdbibc cvc estimate number chord hops remain contacting node estimate total number chord nodes system based density nodes nearby step correctness algorithm consequence result theorem algorithm solves byzantine generals problem traitors proof lemma letting satisfies commander loyal prove assumption commander traitor prove loyal lieutenant set values step lieutenants including loyal paths pass ring cvc estimate number significant high bits cxcs successor agree bits significant bits cxcsb bqbq bdbibc cvc yields significant bits id-space distance target key cxcs function counts bits set difference approximately number chord hops cxcs multiplying average latency rpcs node issued estimates time send rpcs hops adding latency node reported node produces complete cost estimate chord node minimum bvb hop benefit server selection method extra measurements decide node closest decisions made based latencies observed building finger tables nodes rely latency measurements nodes works low latencies nodes latency low measurements internet test-bed suggest true node authentication chord nodes arbitrary ids attacker destroy chosen data choosing node data control successor attacker node effectively delete block denying block existed limit opportunity attack chord node form cwb dcb shahash function node address concatenated virtual node index virtual node index fall small maximum result node easily control choice chord node joins system existing nodes decide add finger tables part process existing node sends message claimed address nonce node address admits claimed address virtual node index hash existing node accepts defense place attacker control roughly addresses total nodes chord system order good chance targeting arbitrary blocks owners large blocks address space tend easily identifiable malicious individuals dhash layer cfs dhash layer stores retrieves uniquely identified blocks handles distribution replication caching blocks dhash chord locate blocks dhash reflects key cfs design decision split file system file blocks distribute blocks servers arrangement balances load serving popular files servers increases number messages required fetch file client block separately network bandwidth consumed lookup small compared bandwidth required deliver block addition cfs hides block lookup latency prefetching blocks systems freenet past store files results lower lookup costs cfs lookup file block requires work achieve load balance servers unlucky responsible storing large files run disk space system sufficient free space balancing load serving files typically involves adaptive caching awkward large files popular file stored entirety caching server dhash caching depends small files dhash block granularity suited serving large popular files software distributions -server system file small megabytes produce balanced serving load kbyte blocks system balances load caching files require case times total storage achieve load balance hand dhash efficient whole-file scheme large unpopular files experiments section show provide competitive download speeds dhash block granularity affect worse performance load balance small files files dhash depends caching server selection block replicas section table shows api dhash layer exposes cfs file system client layer implement application requests open files read files navigate directories publishers data special application inserts updates cfs file system put put calls replication dhash replicates block cfs servers increase availability maintains replicas automatically servers places replicas clients easily find dhash places block replicas servers immediately block successor chord ring figure dhash easily find identities servers chord -entry successor list cfs configured placement replicas means block successor server fails block immediately block successor dhash software block successor server manages replication block making successor servers copy block times successor server fails block successor assumes responsibility block replication 
scheme depends part independence failure unreachability block replica servers servers close ring physically close server based hash address desired independence failure figure placement block replicas cached copies chord identifier ring block shown tick mark block stored successor server denoted square block replicated successor commander simple induction argument applied sincep implies thatp extension algorithm requires graph m-regular strong connectivity hypothesis fact generals minimum number required m-regularity means complete connectivity algorithm reduces algorithm contrast algorithm easily extended weakest connectivity hypothesis connectivity needed byzantine generals problem solvable requires loyal lieutenant obey loyal commander impossible commander communicate lieutenant message recent algorithm dolev requires connectivity acm transactions programming languages systems vol july lamport shostak pease commander lieutenant relayed traitors guarantee lieutenant commander order similarly guaranteed lieutenants communicate traitorous intermediaries weakest connectivity hypothesis byzantine generals problem solvable subgraph formed loyal generals connected show hypothesis algorithm solution number generals--regardless number traitors modify algorithm generals send messages precisely step commander sends signed order neighboring lieutenants step lieutenant sends message neighboring lieutenant prove general result diameter graph smallest number nodes connected path arcs theorem traitors subgraph loyal generals diameter algorithm modification solves byzantine generals problem proof proof similar theorem sketched prove observe hypothesis path loyal commander lieutenant fewer loyal lieutenants lieutenants correctly relay order reaches assumption prevents traitor forging order prove assume commander traitor show order received loyal lieutenant received loyal lieutenant suppose receives order signed send neighbor received order relayed toj steps signers loyal neighbors relayed loyal generals reach steps corollary graph loyal generals connected modified solves byzantine generals problem generals proof diameter graph loyal generals diameter connected graph number nodes loyal generals fewer traitors result theorem letting theorem assumes subgraph loyal generals connected proof easily extended show case traitors algorithm properties loyal generals connected path length passing loyal generals obey order commander loyal loyal lieutenant connected path length passing loyal generals obey order acm transactions programming languages systems vol july byzantine generals problem reliable systems intrinsically reliable circuit components implement reliable computer system processors compute result perform majority vote outputs obtain single voting performed system externally users output true implementing reliable computer redundant circuitry successors circles hops typical lookup path block shown arrows block cached servers lookup path triangles cfs save space storing coded pieces blocks whole-block replicas algorithm ida cfs doesn coding storage space expected highly constrained resource placement block replicas makes easy client select replica fastest download result chord lookup block cxcs identity server immediately precedes cxcs client asks predecessor successor list include identities servers holding replicas block cxcs latency measurements predecessor servers client fetches block replica lowest reported latency chord server selection section approach works proximity underlying network transitive fetching lowest-latency replica side-effect spreading load serving block replicas caching dhash caches blocks avoid overloading servers hold popular data dhash layer sets fixed amount disk storage cache cfs client block key performs chord lookup visiting intermediate cfs servers ids successively closer key successor figure step client asks intermediate server desired block cached eventually client arrives key successor intermediate server cached copy client sends copy block servers contacted lookup path future versions dhash send copy second-to-last server path traversed lookup reduce amount network traffic required lookup significantly decreasing caching effectiveness chord lookup takes shorter shorter hops space closer target lookups clients block tend visit servers late lookup result policy caching blocks lookup path effective dhash replaces cached blocks least-recently-used order copies block servers ids block succesfunction description put block computes block key hashing contents sends key successor server storage put block pubkey stores updates signed block root blocks block signed public key block chord key hash pubkey key fetches returns block chord key table dhash client api exposed client file system software sor discarded clients stumble effect preserving cached copies close successor expands contracts degree caching block popularity caching replication conceptually similar dhash distinct mechanisms dhash stores replicas predictable places ensure replicas exist contrast number cached copies easily counted fall fault-tolerance achieved solely cached copies unpopular block simply disappear cached copy cfs avoids cache consistency problems blocks keyed content hashes root blocks public keys identifiers publisher change root block inserting signed private key means cached root blocks stale causing clients internally consistent file system client check freshness cached root block decide newer version non-root blocks longer eventually eliminated caches lru replacement load balance dhash spreads blocks evenly space content hash function uniformly distributes block ids cfs server fact ids uniformly distributed server carry roughly storage burden desirable servers storage network capacities addition uniform distribution doesn produce perfect load balance maximum storage burden cvb times average due irregular spacing server ids accommodate heterogeneous server capacities cfs notion real server acting multiple virtual servers cfs protocol operates virtual server level virtual server chord derived hashing real server address index virtual server real server cfs server administrator configures server number virtual servers rough proportion server storage network capacity number adjusted time time reflect observed load levels virtual servers potentially increase number hops chord lookup cfs avoids expense allowing virtual servers physical server examine tables fact virtual servers short-cuts routing tables compensates increased number servers cfs potentially vary number virtual servers real server adaptively based current load high load real server delete virtual servers low load server create additional virtual servers algorithm designed stability high load server overloaded cfs system overloaded automatically deleting virtual servers cascade deletions quotas damaging technical form abuse cfs encounter malicious injection large quantities data aim attack disk space cfs servers leaving legitimate data nonmalicious user kind problem accident ideally cfs impose per-publisher quotas based reliable identification publishers past system reliable identification requires form centralized administration certificate authority decentralized approximation cfs bases quotas address publisher cfs server limits address storage attacker mount attack machines successful mechanism limits storage legitimate publisher assuming publisher address limit easy subvert simple forging addresses cfs servers require publishers respond confirmation request includes random nonce section approach weaker requires publishers unforgeable identities requires centralized administrative mechanisms cfs server imposes fixed per-ip-address quota total amount storage address consume grow linearly total number cfs servers prove desirable enforce fixed quota total storage require quota imposed server decrease proportion total number servers adaptive limit form estimate total number servers chord software maintains updates deletion cfs updates publisher file system modify cfs server accept request store block conditions block marked content-hash block server accept block supplied key equal shahash block content block marked signed block block signed public key shahash block cfs key low probability finding blocks shahash prevents attacker changing block content-hash key explicit protection required file system blocks sensitive block file system root block signed safety depends publisher avoiding disclosure private key cfs support explicit delete operation publishers periodically refresh blocks cfs continue store cfs server delete blocks refreshed recently benefit cfs implicit deletion automatically recovers malicious insertions large quantities data attacker stops inserting refreshing data cfs gradually delete implementation cfs implemented lines including line chord implementation consists number separate programs run user level programs communicate udp rpc package provided sfs toolkit busy cfs server exchange short messages large number servers making overhead tcp connection setup unattractive compared udp internal structure program based asynchronous events callbacks threads software layer implemented library interface cfs runs linux openbsd freebsd chord implementation chord library maintains routing tables section exports tables dhash layer implements integrated version chord lookup algorithm implementation shacryptographic hash function produce cfs block identifiers block contents means block server identifiers bits wide chord implementation maintains running estimate total number chord servers server selection algorithm section server computes fraction ring nodes successor list cover fraction estimated total number servers system bpcu dhash implementation dhash implemented library depends chord dhash instance chord virtual server communicates virtual server function call interface dhash instances servers communicate rpc dhash implementation chord lookup algorithm relies chord layer maintain routing tables integrating block lookup dhash increases efficiency dhash called chord find successor routine awkward dhash check server lookup path cached copies desired block cost unneeded round 
trip time chord dhash end separately contacting block successor server pseudo-code dhash implementation lookup key shown figure dhash version chord code shown figure function lookup key returns data key error found function lookup operates repeatedly invoking remote procedure lookup step key returns values called server stores caches data key lookup step key returns data store data lookup step key returns closest predecessor key determined consulting local routing tables finally lookup step returns error true successor key store data lookup contact failed server rpc machinery return rpc failure function lookup backtracks previously contacted server tells failed server alert asks next-best predecessor point lookup key contacted pair servers side rpc handler server returns block key server talk bmd czd ctd czctddb czctdd ctcs crcpcrcwctcscj ctd cxcrcpd ctcsb ctd czctdd czctdd predecessorbnd ddcxcscl ctd nonexistent czctdd ddcxcsbn live successorcl ctdcd cwd live successor find highest server key finger table successor list ctdcd cwd lookup closest predb czctddb crcr cxd cud becucucxd cvctd crcrctd ctdcd cwd return cjcontinuebnd ctdcd cwd bnd crcr cxd rpc handler chord software delete server finger list successor list bmcpd ctd cxcsb return block key error runs server invokes lookup czd czctddb bmd cwb stack accumulate path cjd cpd bnd ctd clbpd bmd czd ctd czctddb repeat cpd completeb return ctd bmcscpd czctdd cpd continueb ctd bmd ctdcd cwd bmd top knew server return nonexistent czctdd bmd bnd bmd top block ctd nonexistent explore hop bmd cwb ctd bmd ctdcd cwd cjd cpd bnd ctd clbpd ctd bmd ctdcd cwd bmd czd ctd czctddb cpd rpc failureb previous hop cucpcxd ctcs bmd cpd bmd cpd bmcpd ctd cucpcxd ctcsb cjd cpd bnd ctd clbpd cpd bmd czd ctd czctddb ctd nonexistent figure procedure dhash locate block key server failures server pair key original successor server live successor hold replica key assuming replicas failed pseudo-code show virtual servers physical server routing tables block stores lookups progress faster ring increases chances encountering cached block client implementation cfs client software layers file system top dhash cfs exports ordinary unix file system interface acting local nfs server sfs user level file system toolkit cfs client runs machine cfs server client communicates local server unix domain socket proxy send queries non-local cfs servers dhash back end sufficiently flexible support number client interfaces instance implementing client acts web proxy order layer space top space world wide web experimental results order demonstrate practicality cfs design present sets tests explores cfs performance modest number servers distributed internet focuses real-world client-perceived performance involves larger numbers servers running single machine focuses scalability robustness quotas section implemented tested software cryptographic verification updates section server authentication section implemented enabled effect results presented noted tests run caching turned replication virtual server physical server server selection turned defaults effects features individually illustrated experiments involve block-level dhash operations file-system metadata client software driving experiments fetches file fetching list block identifiers server maintains successor list bed entries mentioned section maintain ring connectivity cfs automatically adjust successor list length match number servers robustness sensitive exact real life tests section cfs servers running testbed machines scattered internet machines part ron testbed sites spread united states netherlands sweden south korea servers held megabyte cfs file split blocks test download speed client software machine fetched entire file machines fetched file time rpcs average required fetch block experiments client software pre-fetch overlap lookup fetching blocks client initially issues window number parallel block fetches fetch completes client starts figure shows average download speeds range prefetch window sizes server selection block fetch server selection averages milliseconds explains download speed kbytes fetching kbyte block time increasing amount pre-fetch increases speed fetching blocks time yields average speed kbytes large amounts pre-fetch counter-productive congest client server network connection server selection increases download speeds substantially small amounts pre-fetch doubling download speed pre-fetch improvement dramatic larger amounts pre-fetch partially concentrating block fetches nearest servers overload servers links data shown obtained flawed version server selection algorithm correct algorithm yield download speeds figure shows distribution speeds downloads machines pre-fetch windows server selection distributions speeds server selection fairly narrow download require blocks server downloads similar mix protect failure individual chips ballistic missile defense system redundant computing sites protect destruction individual sites nuclear attack difference size replicated processor majority voting achieve reliability based assumption nonfaulty processors produce output true long input single input datum single physical component--for circuit reliable computer radar site missile defense system--and malfunctioning component give values processors processors per-block values times nonfaulty download input speeds unit read machine york good changing connections processors multiple read prefetch clock window advancing kbytes speed kbytes time server selection server time selection figure prevented download speeds synchronizing achieved reads fetching advancing onemegabyte file clock cfs order internet majority voting testbed yield range reliable pre-fetch system window sizes conditions point average satisfied times nonfaulty processors testbed machine input curve includes server produce selection output input unit speed nonfaulty kbytes nonfaulty processes cumulative fraction input downloads kbytes produce correct output kbytes kbytes interactive consistency kbytes conditions kbytes commander kbytes unit generating input lieutenants processors loyal means nonfaulty tempting circumvent problem hardware solution insure processors obtain input read wire faulty input unit send marginal signal wire--a signal interpreted processors guarantee processors possibly faulty input device processors communicate solve byzantine figure cumulative distribution download speeds plotted figure pre-fetch windows results server selection marked speed kbytes sec cumulative fraction transfers kbytes kbytes kbytes figure distribution download speeds achieved ordinary tcp pair hosts internet testbed file sizes backbones worst download speeds small pre-fetch windows sites united states high latency servers worst speeds large amounts pre-fetch fetches cable modem sites united states limited link capacity speed distributions server selection skewed time server selection improves download speeds modest amount improves substantially downloads initiated well-connected sites server selection makes download speeds worse downloads initiated sites united states show cfs download speeds competitive file access protocols files sizes transferred pair testbed machines ordinary tcp files transferred time file tcp connection figure shows cumulative distribution transfer speeds machine pairs kbyte kbyte mbyte files wide distributions reflect wide range propagation delays link capacities pairs machines speeds well-connected sites east coast united states worst speeds kbyte transfers occur end-points united states worst one-megabyte transfers occur endpoint united states cable modem combining high latency limited link speed cfs kbyte pre-fetch window achieves speeds competitive tcp average cfs speeds generally distribution narrower tcp means users repeatably good performance fetching files cfs fetching files ftp servers controlled experiments remainder results obtained set cfs servers running single machine local loopback network interface communicate servers act machines arrangement number cfs servers average per-query rpcs figure average number rpcs client issue find block function total number servers error bars reflect standard deviation experimental data linear log plot fits expectation logarithmic growth controlled evaluation cfs scalability tolerance failure lookup cost block data expected require cvb rpcs experiment verifies expectation range numbers servers blocks inserted system lookups single server randomly 
selected blocks number rpcs required lookup recorded averages plotted figure error bars showing standard deviation results linear log plot fit expectation logarithmic growth actual values servers lookups averaged bibmbj rpcs number rpcs required determined number bits originating server desired block differ average half bits accounts load balance main goals cfs balance load servers cfs achieves load balanced storage breaking file systems blocks distributing blocks servers balances storage placing multiple virtual servers physical server virtual server expect cvb virtual servers physical server sufficient balance load figure shows typical distributions space physical servers virtual servers physical server crosses represent actual distribution blocks physical servers virtual servers desired result server fraction virtual server server virtual servers servers store blocks store times average fraction space physical node cumulative fraction physical nodes real figure representative cumulative distributions fraction key space server responsible servers simulated generals problem faulty input device provide meaningless input values byzantine generals solution guarantee processors input input important separate input devices providing redundant values redundant radars redundant processing sites missile defense system redundant inputs achieve reliability insure nonfaulty processors redundant data produce output acm transactions programming languages systems vol july lamport shostak pease case input device nonfaulty values read changing nonfaulty processors obtain reasonable input shown functions majority choice median functions algorithms property obtained nonfaulty processors lies range values provided input unit nonfaulty processors obtain reasonable long input unit produces reasonable range values solutions stated terms byzantine generals terms computing systems examine solutions applied reliable computing systems problem implementing general algorithm processor problems lie implementing message passing system meets assumptions assumptions algorithm assumptions order assumption states message nonfaulty processor delivered correctly real systems communication lines fail oral message algorithms failure communication line joining processors indistinguishable failure processors guarantee algorithms work presence failures processor communication line failures failure communication lines attached processor equivalent single processor failure assume failed communication line result forgery signed message--an assumption reasonable signed message algorithm insensitive communication line failure precisely theorem remains valid communication line failure failed communication line effect simply removing communication line--it lowers connectivity processor graph assumption states processor determine originator message received faulty processor impersonate nonfaulty practice means interprocess communication fixed lines message switching network switching network faulty network nodes considered byzantine generals problem appears note assumption needed assumed messages signed impersonation processor imply forging messages assumption requires absence message detected absence message detected failure arrive fixed length time--in words time-out convention time-out satisfy requires assumptions fixed maximum time needed generation transmission message sender receiver clocks synchronized fixed maximum error 
assumption fairly obvious receiver acm transactions programming languages systems vol july byzantine generals problem long wait message arrive generation time long takes processor send message receiving input generate assumption obvious shown assumption equivalent solve byzantine generals problem precisely suppose algorithms generals action circumstances fixed initial time generals receipt message randomly chosen length time elapsed general set timer random act timer yields general class algorithms envision construction synchronized clocks shown algorithm solve byzantine generals problem messages transmitted arbitrarily quickly upper bound message transmission delay solution restrict traitors incorrect behavior permitted failure send message proof result scope jer note placing lower upper bound transmission delay ahows processors implement clocks sending messages back assumptions make easy detect unsent messages maximum message generation transmission delay assume nonfaulty processors clocks differ time message nonfaulty process begin generate time clock arrive destination time receiver clock receiver received message time assume arrives sender faulty correctness algorithms depend message flxing time input processor sends calculate time clock processor wait message algorithm processor wait time message signatures time clock commander starts executing algorithm clocks run precisely rate matter accurately processors clocks synchronized initially eventually drift arbitrarily periodically resynchronlzed virtual servers data marked real derived distribution blocks problem keeping processors clocks synchronized fixed amount processors faulty difficult problem byzantine generals problem solutions clock synchroniza ion problem exist closely related byzantine generals solutions future paper assumption requires processors sign messages nonfaulty processor signature forged signature piece redundant information generated process data item message signed consists pair meet parts acm transactions programming languages systems vol july lamport shostak pease function properties processor nonfaulty faulty processor generate process determine ifx equals property guaranteed data item faulty processor generate data item make probability violation small making system reliable depends type faults expect encounter cases interest random malfunction making suitably randomizing function make probability random malfunction processor generates correct signature essentially equal probability random choice procedure--that reciprocal number signatures method assume messages encoded positive integers power equal mod randomly chosen odd number letting unique number mod process check testing mod processor memory probability generating correct signature single nonzero message probability random choice note processor obtain simple procedure larger probability faulty processor forging signature substituting compute malicious intelligence faulty processor guided malicious intelligence--for perfectly good processor operated human disrupt system--then construction signature function cryptography problem refer reader discussion problem solved note easy generate signature process signature important message signed means repeatedly distribute sequence values sequence numbers appended values guarantee uniqueness conclusion presented solutions byzantine generals problem hypotheses shown implementing reliable computer systems solutions expensive amount time number messages required algorithms require message paths length words lieutenant wait messages originated commander relayed lieutenants fischer lynch shown true solution cope traitors solutions optimal respect algorithms graph completely connected require acm transactions programming languages systems vol july byzantine generals problem message paths length diameter subgraph loyal generals suspect optimal algorithms involve sending messages number separate messages required reduced combining messages reduce amount information transferred studied detail expect large number messages required achieving reliability face arbitrary malfunctioning difficult problem solution inherently expensive reduce cost make assumptions type failure occur assumed computer fail respond respond incorrectly extremely high reliability required assumptions made full expense byzantine generals solution required diffie hellman directions cryptography ieee trans inf theory itnov dolev byzantine generals strike algorithms jan pease shostak lamport reaching agreement presence faults acm apr rivest shamir adleman method obtaining digital signatures public-key cryptosystems cornrnun acm feb received april revised november accepted november acm transactions programming languages systems vol july 
servers virtual servers multiple virtual servers server sum parts space server virtual servers responsible tightly clustered average fact cfs spreads storage blocks servers means cases burden serving blocks evenly spread large files true files popular file blocks widely spread popular data consists blocks servers happen blocks successors experience high load section describes caching helps balance serving load small files caching cfs caches blocks lookup path initiating server contacts successive servers checks desired block cached initiating server found block sends copy servers contacted lookup servers add block caches scheme expected produce high cache hit rates lookup paths block sources tend intersect closer block successor server figure illustrates caching works single block inserted server system sequence randomly chosen servers fetch block graph shows number rpcs required fetch block decreases number cumulative fetches due block cached places plotted point average sequential fetches quirk implementation prevents originating server checking cache fetches rpc count expected rpc counts decrease servers block cached rpc counts decrease significantly lookups figure shows lookups caching -server system require average rpcs cumulative number lookups rpcs lookup figure impact caching successive client fetches block point average number rpcs successive fetches randomly chosen servers error bars standard deviation system servers lookups caching average hops required net effect improve client-perceived performance spread load serving small files storage space control varying number virtual servers physical server server owner control amount data cfs stores server amount server serve figure shows effective experiment involves physical servers virtual servers blocks inserted system relationship virtual servers physical server blocks store plotted physical server virtual server stores blocks total virtual servers close expected bibebj bdbcbcbcbc bdbi bebhbh relationship blocks virtual servers linear administrator easily adjust cfs server storage consumption memory overhead running virtual servers achieve fine-grained control load virtual server requires finger table successor list accounting structures block store cache total memory footprint structures unoptimized implementation kbytes effect failure cfs server fails time pass remaining servers react failure correcting finger tables successor pointers copying blocks maintain desired level replication theorems suggest cfs perform lookups correctly efficiently recovery process starts face massive failure test blocks inserted -server system number virtual nodes number blocks figure impact number virtual servers physical server total amount data physical server store failed nodes fraction failed lookups fraction figure fraction block request failures function fraction cfs servers fail data point average experiments involving block lookups error bars minimum maximum results failed nodes fraction average rpc count figure average lookup rpc count function fraction cfs servers fail servers failures data point average experiments error bars minimum maximum results block replicas including main copy stored direct successor insertions fraction servers fail warning chord starts rebuilding routing tables fetches randomly selected blocks attempted single server figure shows fraction lookups fail figure shows average rpc count lookups lookups fail fewer servers fail fail reason server finger tables successor lists provide potential paths carry query chord ring desirable finger table entry points failed server cfs entry points ring lookups start fail servers fail blocks lose copies servers fail probability losing block replicas bcbmbh bpbcbmbcbdbi close shown figure lookup failures encountered experiment due block replicas failing cfs find copy block figure shows lookups rpc longer result servers failing rpc counts include attempts contact failed servers lookups longer failures finger table entries required fast lookups point failed servers half finger table entries valid rpc makes half progress expected extra rpc fully corrects figure shows number attempts contact failed servers occur lookup averaged block lookups time server decides timeout finger table successor-list entry points failed server server stabilized massive failures effect availability data number rpcs lookup users perceive failures rpc timeouts lookups figure shows typical block lookup shortly failure expect failed nodes fraction number timed-out rpcs lookup figure number rpc timeouts incurred lookups function fraction cfs servers fail servers failures data point average experiments error bars minimum maximum results timeout retrieving desired block experiments demonstrate large fraction cfs servers fail significantly affecting data availability performance future research cfs benefit keyword search system provide adopt existing centralized search engine ambitious approach store required index files cfs idea pursuing cfs robust case fail-stop failures specifically defend malicious participants group malicious nodes form internally consistent cfs system system falsify documents cfs verify authenticity incorrectly claim document exist future versions cfs defend periodically checking consistency chord tables randomly chosen nodes cfs copy blocks servers node joins leaves system order maintain desired level replication nodes join system short time leaving copies wasted cfs lazy replica copying cfs work nat special arrangements make pool global servers acting proxies hosts nats current cfs client fixed pre-fetch window size give performance situations size depends round trip time network bandwidth pre-fetch window serves purpose similar tcp congestion window analogous adaptive algorithms conclusions cfs highly scalable secure read-only file system presents stored data applications ordinary file-system interface servers store uninterpreted blocks data unique identifiers clients retrieve blocks servers interpret file systems cfs peer-to-peer chord lookup protocol map blocks servers mapping dynamic implicit result directory information updated underlying network makes cfs robust scalable cfs replication caching achieve availability load balance cfs replicates block consecutive servers identifier space caches block lookup path starting block server finally cfs simple effective protection 
single attacker inserting large amounts data prototype implementation cfs implemented evaluated controlled internet-wide test-bed future operational deployment uncover opportunities improvement current results cfs viable large-scale peer-topeer system acknowledgments grateful hari balakrishnan john zahorjan anonymous reviewers helpful comments david andersen managing ron testbed letting akamai technologies http akamai cambridge andersen balakrishnan kaashoek morris resilient overlay networks proceedings acm symposium operating systems principles oct chankhunthod danzig neerdaels schwartz worrell hierarchical internet object cache proc usenix technical conference jan chaum untraceable electronic mail return addresses digital pseudonyms communications acm feb clarke distributed decentralised information storage retrieval system master thesis edinburgh clarke sandberg wiley hong freenet distributed anonymous information storage retrieval system proceedings workshop design issues anonymity unobservability july dingledine freedman molnar free haven project distributed anonymous storage service proceedings workshop design issues anonymity unobservability july fan cao almeida broder summary cache scalable wide-area web-cache sharing protocol tech rep computer science department wisconsin madison feb kaashoek mazi eres fast secure distributed read-only file system proceedings usenix symposium operating systems design implementation osdi october gadde chase rabinovich taste crispy squid workshop internet server performance june gnutella website http gnutella wego karger lehman leighton levine lewin panigrahy consistent hashing random trees distributed caching protocols relieving hot spots world wide web proceedings annual acm symposium theory computing kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao oceanstore architecture global-scale persistent storage proceeedings ninth international conference architectural support programming languages operating systems asplos november lewin consistent hashing random trees algorithms caching distributed networks master thesis mit malpani lorch berger making world wide web caching servers cooperate fourth international world wide web conference mazi eres toolkit user-level file systems proc usenix technical conference june mazi eres kaminsky kaashoek witchel separating key management file system security proceedings acm symposium operating systems principles sosp dec merkle digital signature based conventional encryption function advances cryptology crypto berlin pomerance vol lecture notes computer science springer-verlag mojo nation documentation http mojonation net docs napster http napster stoica zhang waypoint service approach connect heterogeneous internet address spaces proc usenix technical conference june ohaha cwd bmbbbbdbdbdbbad cwcpcwcpbacrd bbcsctd cxcvd bacwd june ohaha application longer oram peer-to-peer harnessing power disruptive computation reilly associates plaxton rajaraman richa accessing nearby copies replicated objects distributed environment proceedings acm spaa june rabin efficient dispersal information security load balancing fault tolerance journal acm ratnasamy francis handley karp shenker scalable content-addressable network proc acm sigcomm san diego reiter rubin crowds anonymity web transactions acm transactions information system security nov rowstron druschel pastry scalable distributed object location routing large-scale peer-to-peer systems proceedings ifip acm international conference distributed systems platforms middleware nov rowstron druschel storage management caching past large-scale persistent peer-to-peer storage utility proceedings acm symposium operating systems principles oct sherman karger berkheimer bogstad dhanidina iwamoto kim matkins yerushalmi web caching consistent hashing computer networks stoica morris karger kaashoek balakrishnan chord scalable peer-to-peer lookup service internet applications proc acm sigcomm san diego stoica morris karger kaashoek balakrishnan chord scalable peer-to-peer lookup service internet applications tech rep trmit cambridge march tyan case study server selection master thesis mit sept waldman rubin cranor publius robust tamper-evident censorship-resistant web publishing system proc usenix security symposium august zhao kubiatowicz joseph tapestry infrastructure fault-tolerant wide-area location routing tech rep ucb csd- computer science division berkeley apr 
run-time adaptation river remzi arpaci-dusseau wisconsin madison present design implementation evaluation run-time adaptation river dataflow programming environment goal river system provide adaptive mechanisms database query-processing applications cope performance variations common cluster platforms describe system basic mechanisms carefully evaluate mechanisms effectiveness analysis answer previously unanswered important questions core run-time adaptive mechanisms effective compared ideal keys making work applications easily primitives finally situations run-time adaptation sufficient performing study utilize three-pronged approach comparing results idealized models system behavior targeted simulations prototype implementation providing insight positives negatives run-time adaptation specifically river broader context comment interplay modeling simulation implementation system design categories subject descriptors computer-communication networks distributed systems distributed applications database management systems parallel databases general terms design experimentation measurement performance reliability additional key words phrases performance availability performance faults run-time adaptation parallel clusters robust performance introduction successful application domains mapped parallel machines realm database query-processing resulted successful research projects barclay boral dewitt lorie commercially viable products industry baru tandem performance group teradata corporation success attributed relational model describing data natural structure codd relational model affords flexibility implementation dewitt gray state ideally suited parallel execution work funded years darpa -cdarpa -cnsf cda nasa fdnagwthe california state micro program nsf ccrauthor address arpaci-dusseau computer sciences department wisconsin madison dayton madison remzi wisc permission make digital hard copy part work personal classroom granted fee provided copies made distributed profit commercial advantage copyright notice title date notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee acm acm transactions computer systems vol february pages run-time adaptation river past parallel database systems tailored run specialized parallel hardware platforms recent networking technology transformed commodity clusters workstations called networks workstations nows viable platform tightly coupled parallel applications advent switch-based lowlatency high-throughput networks enabled deployment classes applications services modern clustered systems clusters number built-in advantages specialized parallel machines including higher performance lower cost higher performance incorporate recent microprocessors track moore law lower cost due economies scale mass production anderson clusters provide excellent alternative specialized parallel machines introduce range problems system designers difficulty arises complexity modern computer systems basic building blocks clustered systems including processors networks disks software increasingly sophisticated current top-of-the-line microprocessors tens millions transistors amd operating systems typically millions lines code increasing component complexity directly affects component behavior identical complex components behave identically arpaci-dusseau arpaci-dusseau identical disks made manufacturer receiving input stream necessarily deliver performance unexpected dynamic performance heterogeneity arise number factors including fault-masking capabilities modern scsi disk drives remapping bad blocks bad sectors hidden higher levels system altering performance drive disks purveyor heterogeneity similar behavior observed cpus bressoud schneider kushman networks arpaci-dusseau memory systems raghavan hayes software systems chen bershad gribble performance heterogeneity difficult overcome mixed parallelism due performance assumptions made parallel algorithms previous systems made simplifying assumption components system operate rate times assumptions common parallel database systems traditionally static data distribution schemes rangepartitions hash-partitions move data assign work nodes system dewitt graefe strong performance assumptions global operations perform rate slowest member group decreasing performance robustness system attempting prevent performance heterogeneity occurring river parallel programming environment takes variations performance faults account inherent design consideration arpaci-dusseau river basic dataflow programming environment substrate clusters goal enabling common-case acm transactions computer systems vol february arpaci-dusseau robust performance face arbitrary unforeseen performance faults components primary focus river provide support parallel database query-processing primitives dataflow programming environment fairly general broader class applications benefit river infrastructure key delivering robust system performance river run-time adaptation river adaptive data-movement mechanisms continually gauge react performance components system avoiding performance assumptions design specifically river core adaptive mechanisms distributed queue balances data flowing consumers system graduated declustering dynamically adjusts flow data generated producers constructs designed advantage performance characteristics modern high-speed networks moving data location processed river applications utilize tandem deliver consistent high performance spite unanticipated performance variations article present design implementation evaluation second-generation river system ganges make major contributions comparison previous work describe ganges prototype detail incorporates lessons learned implementation evaluation prototype euphrates importantly provide detailed study core river run-time adaptive mechanisms initial results presented iopads workshop promising arpaci-dusseau limited important questions left unanswered core run-time adaptive mechanisms river operate compared ideal keys making work applications readily apply mechanisms fashion robust applications finally limitations mechanisms answering questions generality efficacy river approach adequacy run-time adaptation solution performancefault problem answer questions article three-pronged analysis incorporate modeling simulation study addition extending previous implementation work models quantify ideal performance comparison ideal understand core mechanisms operate simulations study river distributed algorithms depth worry interference difficulties common implementation work implementation bring issues arise accurate simulations validate simulated performance expectations approach enhance ganges prototype improving performance river mechanisms roughly factor scenarios small factors important greatly enhance understanding software adaptation technology central river feel confident core mechanisms river robust acm transactions computer systems vol february run-time adaptation river fig performance comparison implementation versus number performance faults increased x-axis percentage peak performance plotted y-axis results older version compared improved algorithm improvement gained broader evaluation technique previewed figure plot performance versions distributed queue performance faults compare ideal model line labeled original plots data directly original paper performance euphrates implementation track ideal falling sharply high number performance faults ideal model place section easy improvement line presents ganges implementation algorithm tracks ideal perfectly previous fall-off interplay algorithm message-layer flow control study carefully simulation section resulting much-improved distributed queue implementation summarize major findings core run-time adaptive mechanisms river operate compared ideal find cases refined distributed queue graduated declustering algorithms operate gracefully delivering ideal performance number perturbation scenarios quantify limitations due limited replication best-case worst-case situations previous work measured best-case scenarios keys making work find keys effective operation river order run-time adaptive primitives function desired behavior communication layer network hardware critical control flow control implemented communication acm transactions computer systems vol february arpaci-dusseau layer order perform find river mechanisms require high degree parallelism application level excess parallelism current river mechanisms avoid performance faults successfully fortunately data-intensive applications achieve high degrees parallelism readily find local data-processing decisions guided global knowledge progress main difficulty lies obtaining global information distributed scalable manner finally designing river hardware platform recommend amount slack engineered order enable 
consistent high performance spite performance fluctuations applications readily apply mechanisms fashion robust applications question difficult fully answer implementing number database query-processing operators top river find programs employ distributed queue graduated declustering create robust dataflow find applications suitable transformations fundamental reasons multiphase applications automatically utilize stages pay cost data replication many-toone dataflow easily transformed robust counterpart adq finally limitations mechanisms study describe cases localized run-time adaptation river sufficient river adaptive mechanisms tolerate local performance faults hardware software global performance fluctuations network switches difficult impossible avoid network backplane primary avenue adaptation work system behave robustly find cases run-time adaptation short-sighted memoryless speculate long-term adaptation needed note problems general run-time adaptive systems specific river rest article organized section highlights river system motivation design section describes ganges implementation section develop model expected ideal performance describe experimental environment results presented answers questions outlined sections section discuss related work section conclude river system motivation design goal river programming environment enable construction applications exhibit performance availability provide mechanisms data-intensive applications adapt run-time performance fluctuations make high performance consistently end-users stated primary focus database queryprocessing primitives broader class applications programmed river environment section briefly describe design river system detail including motivation acm transactions computer systems vol february run-time adaptation river system description core adaptive mechanisms conclude section discussion technologies river design relies order provide flexible robust programming substrate discussion limitations environment motivation previous work field distributed parallel systems addressed design large-scale systems tolerate correctness faults individual components birman cooper borg englert liskov schneider notion work distributed systems consist multiple hardware software components periodically fail system works continuously top unreliable components operate spite failures raid storage tolerate failure disk continue correct operation patterson understood notion system functions components perform expected refer unexpected low performance component performance fault clusters disk drives main source performance variation scope data-intensive applications reasons disks clustered system exhibit static dynamic performance faults single disk disks include presence multiple zones meter scsi bad-block remapping arpaci-dusseau thermal recalibrations bolosky sporadic performance absolute failure talagala patterson contention due workload imbalance structural heterogeneity due incremental growth brewer documented arpaci-dusseau arpaci-dusseau hardware software components exhibit unexpected performance variations worse faults tend occur subset components system occur tend long-lived current systems support data-intensive applications interact performance faults systems built static techniques exploiting parallelism allocating data standard striping algorithms distributing data requests set disks place amount data disk identified problem static schemes make rigid performance assumptions relative performance components perform identically result small number components deliver peak performance performance entire service reduced slow entities performance availability states performance entire system track aggregate performance components system degrading gracefully performance faults systems provide proper run-time adaptive primitives support performance availability enable delivery excellent sustained performance spite localized component performance failures acm transactions computer systems vol february arpaci-dusseau fig now-sort perturbance best-case performance now-sort versus performance slight disk cpu memory perturbations performance results relative -node run now-sort delivers data near-peak disk rate run motivate global performance problems induced local performance faults report results simple experiment nowsort arpaci-dusseau high-performance parallel external sort clusters experiment sort runs machines run perform slight perturbation sort machines results perturbation experiments shown figure graph perturbations single machine global performance effect single file single machine poor layout tracks versus outer performance drops single disk hot spot competing data stream performance drops factor cpu loads machines decrease performance proportional amount cpu steal excess memory load machine begin thrashing factor performance lost build system avoids situations balancing load system perfectly times meticulously managing resources system difficult system size complexity increase carefully managing system challenging impossible approach problem manner assuming presence performance faults providing substrate enables applications operate spite river environment river generic dataflow programming environment clusters workstations similar basic design previous parallel database environments gamma volcano dewitt graefe applications constructed piecewise fashion modules acm transactions computer systems vol february run-time adaptation river fig module api simple river module module messages upstream performs operation calling user-defined filter conditionally put messages downstream fig flow api simple reader-to-writer flow user calls place add module node dataflow graph attach connect modules finally calling program begins run ufsread module reads collection output input ufswrite module writes disk module logical thread control input output channel inside module called obtain data upstream source put called pass data downstream begin execution application control program constructs flow connects desired modules sources sinks instantiated computation begins continues data processed examples module flow figures enable applications cope performance faults deliver consistent high performance river distributed software constructs distributed queue graduated declustering dynamically adjust data transferred set producer modules processes parallel application set consumer modules set disks run-time goal tolerate performance faults transfer producers consumers constructs river solve problem cope performance-faulty consumers performance-faulty producers tandem flow build performance-robust applications acm transactions computer systems vol february arpaci-dusseau design river explicitly provide mechanisms deal absolute failure handle absolute failure applications restarted advantage checkpointing package barclay litzkow discuss desired behavior distributed queue graduated declustering detail distributed queue transferring data set producers set consumers consumers suffer performance fault proper reaction case move data consumers proportional relative bandwidths faster slower functionality provided distributed queue application writers insert program dataflow tolerate consumer faults arrive design constraint-free transfer data arbitrary number producers consumers envision scenario distributed queue producers consumers distributed queue behavior data queue producers consumers note ordering data guaranteed point-to-point producer places consumer receives receive strictly speaking bag queue terms performance ideal distributed queue deliver data consumers rates proportional rates consumption fixed time interval consumes rate rate ratio data received compared rates consumption consumers change dramatically time subject performance faults quickly adapt figure presents logical structure control dataflow producer data distribute set consumers consumer queue incoming data blocks producers process consist producers consumers data transferred parallel set producers set consumers part queue flow block behavior system determined decisions potentially requires global information producer chooses consumer receive data block show important decision performanceavailable distributed queue algorithm consumer choose order process blocks received graduated declustering corollary problem occurs data transfer producers consumers producer suffers performance fault case alternate data source producers bottleneck transfer acm transactions computer systems vol february run-time adaptation river fig data 
control basic data movement control options distributed queue producer wishes send data block faced decision consumer data decided data block consumer consumer receives blocks producers faced control option order blocks processed consumers block received consumer sends reply producer required acknowledgment-based reliable communications protocol fig data control basic data movement control options graduated declustering consumer requesting block faced decision producer request request selected producer requests consumers producer faces decision queue serviced serviced data back requesting consumer data transfer runs rate single slow producer data source replicated river applications employ graduated declustering data transfer mechanism carefully divides producer bandwidth equally consumers performance availability producer faults parallel reads mirrored on-disk data sets applied replicated in-memory data sets figure presents logical structure dataflow graduated declustering consumer sends request block replicated acm transactions computer systems vol february arpaci-dusseau sources block producer choose order handle consumer requests decisions important respect behavior algorithm formally describe desired behavior graduated declustering assume subset data set replicated copies subset refer n-way graduated declustering assume producers cases physically colocated entities producers spread cluster machines assuming extra machine resources capacity data set replication assume machines producers running machine producers icp icp replicas assume remote machines consumers consumer consumes portion data set produced replicas note bit distributed queue data producer consumer number producers consumers necessarily equivalent assume producer produces data rate producers machine share resource sum machine equals rate bottleneck resource total bandwidth machines max idp number perturbations added system bandwidth perturbation performance fault resource bandwidth assume faults present system total bandwidth consumers avail idp idf goal graduated declustering bandwidth divide equally consumers avail division accomplished consumers proceed rate finish data transfer instant performance faults system tolerated allocate producer bandwidth cope producer-side performance faults shown figure global redistribution accomplished local producer bandwidth adjustments total bandwidth resource divided producers share resource replica producers share piece flexibility apportion bandwidth producers acm transactions computer systems vol february run-time adaptation river fig dataflow alleviates problem producer-side performance faults disks producers data mirrored dataflow shown control flow consumers request data items producers depicted figure omitted sake clarity producer produces data sets consumer consumes data set performs half expected rate producers compensate bandwidth allocations consumers receive fair share aggregate bandwidth enabling technologies application writers ability construct flexible dataflows moving data computation processed current state system river design relies number recent hardware software technologies order deliver desired behavior applications river designed advantage high-throughput system-area networks myrinet boden advent networks altered clusters computers loosely coupled distributed systems tightly coupled parallel systems high-performance interconnect river degenerate system processing performed locally moving data machine costly merit consideration scenario river environment build applications dataflow model flexibility afforded river mechanisms unutilized river design integrates cleanly top active messages substrate von eicken mainwaring culler designed export raw power high-performance networks natural match river aspects river implementation discussed section direct advantage request response nature protocol focus river primarily high-throughput database queryprocessing applications relationships disk performance network performance river relies network move data rate local disk enabling flexible dataflow source sink cluster throughput streaming data disk machine process machine close equal throughput streaming data disk process machine note river rely heavily acm transactions computer systems vol february arpaci-dusseau low-latency aspects modern networks afford applications stream large data sets throughput oriented bisection bandwidth network equal sum disk bandwidth cluster requirements unreasonable hold environment sequential disk bandwidth improving rapidly grochowski network switches links buses pace arpaci-dusseau finally natural question river benefits realized standard tcp ip-based ethernet network type environment complexity software protocol stack hardware switches introducing additional overheads communication efficient global communication key river overheads penalize design freely moves data cluster long technology relationships paragraph hold benefits river approach realized limitations river programming environment strive make river mechanisms general number limitations programming model flexible data-distribution mechanisms river suited parallel applications naturally programmed dataflow style database queryprocessing primitives selects sorts joins natural candidates primary focus application study section wolniewicz graefe shown scientific parallel operators work dataflow model dataflow model ideal clustered services building internet service search engine web proxy specialized environment fox type application differs traditional dataflow model small requests processed concurrently contrast focus single parallel application time building cluster environments concept performance availability considered similar performance problems encountered birman primary mechanisms performance robustness river distributed queue graduated declustering limited applicability applications flexibility manner order process data database query-processing primitives true design advent relational model decouples manner computation performed specification desired result codd parallel applications property shown large body work flexible parallel programming environments studies existing scientific codes poole randall chakrabarti parallel applications readily programmed dataflow model acm transactions computer systems vol february run-time adaptation river case rewritten utilize performance robustness finally obvious limitation data set served fully replicated cost producing replica inhibit envision applications primarily access frequently read on-disk data collections data form redundancy reliability mirroring on-disk data expensive terms space usage disk costs decline simplicity performance advantages mirroring increasingly work favor parity-based schemes save storage space approaches amenable gd-like performance-robust technique requires alternate data source readily serve data performance fault reading block parity-based approach data block read block regenerated reading blocks parity block stripe costly note extended function partial data set replica data records replicated added complexity case performance robustness fall nonrobust mirrored system full replication ganges implementation ganges implementation second-generation prototype river dataflow system section describe ganges focusing implementation distributed queue graduated declustering ganges implemented distributed algorithms provide desired functionality global coordination centralized control algorithms distributed order scale large clusters centralized approach fundamentally provide performance availability single slowdown central master component lead global performance problems ganges implemented cluster sun ultra workstations running solaris workstation consists mhz ultrasparc processor tremblay memory seagate hawk rpm disks attached fast-narrow scsi bus s-bus disk commonly swap space system data bandwidth hawk ranges outer tracks workstations connected high-speed myrinet local-area network boden workstation single myrinet card s-bus cards capable moving data workstation approximately entire system connected collection -port myrinet switches arranged -ary fat tree communication performed active messages mainwaring culler exposes raw performance myrinet integrating features threads blocking communication events multiple independent endpoints acm transactions computer systems vol february arpaci-dusseau distributed queue important 
aspect distributed queue implementation data transfer protocol implementation possibilities producers push data consumers consumers pull data producers information exchanged relative rates execution consumers main concern construction data transfer protocol make performance assumptions producers rely priori performance characteristics consumers describe implementation key ideas combined arrive algorithm randomness picking equivalent consumers feedback flow control critical element attain desired behavior consumer perturbation producer data send consumer calls put routine internally steps producer checks total outstanding messages network outstanding determined threshold producer waits returned credits producer pick consumer send data picking random consumer checking outstanding messages consumer consumer producer picks consumer repeats check producer proceeds performs step algorithm send data chosen consumer receive side implementation straightforward consumer waits message arrive event mechanism active message layer message arrives wakes extracts message network polling message packaged returned consumer sends reply producer indicating received message producer receives reply updates flow-control counters mentioned decision made occurs messages single producer case service producers proportion current rate progress receive higher proportion service details presented section discussion key avoids assumptions performance consumer run-time adaptation producer utilizes feedback consumers gauge consumer data specifically producer sends data consumer form active message request protocol stipulates consumer reply message reason levels credit management simplicity credits desired behavior producer wait message return provide simple check condition reply message required protocols utilize acknowledgments implement reliable message transfers acm transactions computer systems vol february run-time adaptation river reply signal channel implicit information arpaci-dusseau consumer handled data request producer infers remote consumer making progress producer monitors incoming replies infer current performance level consumer algorithm property unifying seemingly diverse implementations push-based pull-based approaches found message-passing layers karamcheti chien producers bottleneck data transfer consumers respond quickly data requests choice destinations degenerates random choice entire set consumers push-based approach random choice shown good similar scenarios brewer kuszmaul consumers bottleneck producer messages outstanding consumer reply returns implicitly pulls data request replying consumer depending relative performance rates producers consumers utilize push-based pull-based approach functionality distributed queue providing simply load balancing studied extensively literature adler blumofe johnson wen effective load-balancing algorithms performanceavailable make performance assumptions form meet demands load-balancing performance availability isomorphic centralized scheme single machine rendezvous point matching producers consumers data item performance assumption algorithm makes centralized match-maker suffer performance faults performance entire system suffers advanced load-balancing schemes proposed adler johnson algorithms utilize probe-then-send model scheme consumers chosen uniformly random queried current queue length replies filter back producer picks consumer data queue sends data performance assumption family algorithms makes probes return timely manner probed consumers exhibits performance deficiencies result producer spends time waiting response query graduated declustering describe ganges implementation graduated declustering start consumer-side algorithm straightforward n-way graduated declustering consumer partition data set choices request data item jumpstart process consumer sends requests fixed number blocks distributing round-robin fashion producers sending requests producers acm transactions computer systems vol february arpaci-dusseau request producer request producer request producer request producer crucial adaptation consumer-side occurs replies back producers reply data block returns producer consumer requests data item producer producers responsive consumerside algorithm adapt responsiveness requesting data active producer aspect consumer-side algorithm important request consumer sends extra information producer progress metric small amount extra knowledge crucial producer determining consumers requests served progress metric total number bytes received consumer previous implementation average bandwidth consumer received experimentation found total number bytes robust reliable metric producer-side algorithm bit complex producer represented components set service threads scheduler thread service threads replica receive requests scheduler thread read data disk data source send data back directly consumers note n-way graduated declustering machine receive requests consumers component scheduler thread thread crucial proper operation graduated declustering examines progress metric consumer biases scheduling requests catch lagging consumer producer serving requests consumers received blocks data received blocks blocks producer caught discussion distributed algorithm implements graduated declustering based intuition producer balance progress metric consumers delivers data producers strive localized balance global balance consumers achieved key success producer-side scheduler directly charge biasing occur choice progress metric central correct operation current implementation consumer piggybacks information consumer requests order producer decide consumer receive proportion service current metric total number bytes received consumer metrics important aspect metric give notion global progress final goal average bandwidth fits definition partition data set roughly identical size future metric exposed applications internal implementation application-specific scheduling acm transactions computer systems vol february run-time adaptation river methods models study behavior river employ simulations implementation compare results simple idealized model system behavior original study arpaci-dusseau based solely results prototype implementation suffered conclude mechanisms river generally compared nonadaptive approach true upper bound performance implementation details difficult separate inherent behavior distributed algorithms remedy problem develop idealized model system behavior performance faults plotting results versus models gauge system absolute sense relative nonadaptive system models difficult satisfied system performance remedy problems utilize simulation simulations explore basic algorithmic behavior wide range system parameters feasible measure prototype measurements implementation gathered confirm simulation results bring systems issues modeled simulations combination techniques improves understanding separating intrinsic properties algorithms implementation details model system behavior develop model behavior ideal system performance faults ideal system adapt instantly performance fault move work data utilize system resources limited underlying hardware resources comparing experimental simulation results model ideal system gauge algorithms implementations functioning note model simple general applied evaluating behavior systems performance faults assume device expected deliver performance rate peak unperturbed time interval interest device suffering performance fault fault amount resources term rate fault fault fault peak resource undergoing performance fault view fault entity utilizes portion resource akin application utilizes resource delivered rate performance performancefaulty device peak fault disk deliver peak bandwidth peak suffer fault takes fault leaving applications characterize strength performance fault define fault utilization fault fault peak fault utilization ranges note absolute failure special case performance fault fault fault peak river concentrate range fault acm transactions computer systems vol february arpaci-dusseau fig performance availability spectrum performance increasing number performance faults x-axis number components experiencing performance fault increased maximum number components system system performance fraction ideal performance setting free performance faults plotted y-axis final definition required developing model behavior faults application utilization resource assume application interest runs rate app resource period interest application o-intensive 
app rate reads data disk read phase peak peak rate resource deliver fault faulty performance level application app app peak fraction resource unperturbed note app precisely resource heaviest demand application cpu computebound calculation phase disk strictly o-bound portion program understand behavior ideal system increasing number performance faults plot graph called performance availability spectrum shown figure x-axis graph increase number components experiencing performance fault simplicity notation assume performance fault occurs faulty component application utilizes fraction resources component peak rate component identical restrictions easily relaxed desired y-axis plots system performance fraction ideal performance nonperturbed perfect conditions application completes phase interest time number faults system graph plots ideal increasing x-axis ideal minimal time algorithm complete operation equal acm transactions computer systems vol february run-time adaptation river examining read phase external one-pass sort ideal time read data disks peak rate comparison point ideal spectrum differentiate poorly performing algorithms perform basic approach developing model ideal behavior simple two-step process derive point system fully utilized faults present application point sum utilization faults system application total amount resources point system duress ideal case application performance loss application resources continue unperturbed derive point components experiencing performance fault points place generate complete piecewise-linear model ideal performance point interest derive labeled loss figure point ideal system begins performance degradation due presence performance faults point perturbed resource cpu network link disk fully utilized components system point ideal system move work data suffer performance loss point performance degrades compared nonperturbed system due lack resources solve loss point resource utilized components sum application resource usage perturbation resource usage components equal app peak loss fault peak solving loss arrive loss peak app fault terms utilization loss app fault present simple make concrete imagine cpu resource interest assume parallel application runs cpus cluster process application utilizes cpu unperturbed system app peak application total cpus full cpus assume performance fault interested fault utilization fault peak fault utilizes cpu substituting values equation arrive loss half cpus perturbed expect performance drop peak make general observations loss degenerates app app peak observation matches intuition application resource unperturbed case perturbation system leads loss performance extreme app fault peak app fault loss greater plainly stated sum application resource utilization performance-fault resource utilization total amount resources components perturbation slowdown experienced ideal case point interest figure components collection experiencing performance fault call y-axis acm transactions computer systems vol february arpaci-dusseau denote performance level ideal system components incur performance faults derive point components perturbation rate node deliver perturbation peak fault calculate directly observing application slowdown resources overtaxed rate delivered divided rate needed application peak fault app terms utilization fault app application utilizes cpu performance fault utilizes nodes perturbed total ideal system performance general application increase decreases similarly fault utilization increases decreases points interest loss expect linear drop performance drop performance linear points amount resources application increases linearly resources correspond directly performance application disk bandwidth write phase slowdown factor ideal case derive slope line connects points express expected ideal system behavior closed form loss loss loss loss piecewise linear model ideal performance set performance faults judge absolute performance system simulations prototype implementation limitations model describing simulation environment briefly discuss primary limitations model address assumption linear slowdown faults model assumption hold depending scaling properties application question model application half bottleneck resource assumed application run half rate full resource holds applications interested general applications easily utilize fewer resources perform worse expected total number resources increased singh model assumes manner perturbations occur components exponentially worsen performance case parallel programs arpaci communicating processes coscheduled ousterhout performance worsen orders magnitude progress acm transactions computer systems vol february run-time adaptation river application depend progress specific process comparing ideal realistic assume program attain ideal uncorrelated faults multiple components model assumes throughput response time performance metric interest result assumption systems deliver throughput identical model response time costs queueing reflected performance analysis practice found assumptions adequate database query-processing applications focused good scaling behavior require coscheduling good performance process large volumes data throughput-oriented limitations found model invaluable tool development system providing performance target ideal model focuses performance tuning letting designer aspects system improved good model lets designer stop tuning performance approximates ideal fine-tune system simulation environment addition models measurements real implementation employ set simulations demonstrate properties distributed algorithms simulator constructed low-level primitives queues consume data user-specified rates sources generate data user-specified rates simulations distributed queue graduated declustering readily constructed components simulator core consists event-based simulator event subsystem written efficiency rest simulation system including queue source abstractions callbacks glue code written tcl ousterhout tcl affords great flexibility assembling arbitrarily complex simulations ousterhout enables visualization animation scenarios effective adaptive mechanisms river sections answer questions posed introduction basic goal understanding run-time adaptation approach robust system design stated utilize results simulations prototype implementation comparing ideal begin exploring general effectiveness river mechanisms presenting results experiments test performance increasing number performance faults experiments utilize simulation implementation results show match closely results extend previous work number axes enhancing understanding simulation showing mechanisms acm transactions computer systems vol february arpaci-dusseau fig distributed queue performance results simulation implementation presented compared ideal performance increasing number performance faults performance static transfer experiments producers generate data nonfaulty consumers consume data performance fault reduces consumer performance factor work ideally compared model displaying sensitivity layout performance faults demonstrating improved performance second-generation implementations distributed queue earlier section distributed queue presents applications high-speed backplane data sharing tolerate consumer-side performance faults application producers place data distributed queue consumers receive data consumer logically receive data block put central challenge design scalable efficient distributed algorithm moves data producers consumers faster consumers receive proportionally higher amount aggregate producer bandwidth figure illustrates performance simple perturbation scenario compares performance ideal static nonadaptive approach experiments producers send data remote consumers total machines producers generate data nonfaulty consumers sink data setup emulates set processes producers writing in-memory records parallel disk disks consumers bottlenecks transfer x-axis increase number consumers undergoing performance faults performance fault reduces rate consumer factor consumer performance reduced leftmost point x-axis shows performance system performance acm transactions computer systems vol february run-time adaptation river faults rightmost point shows performance consumers perform perturbed rate y-axis plots performance fraction 
ideal unperturbed case faults system ideal simply aggregate bandwidth consumers ideal performance line reflects model developed earlier note loss experiments test utilizes consumer bandwidth unperturbed case case ideal system drops performance single fault shown figure static transfer producer static hashing algorithm pick consumer send data performs faults system point throughput high scales machines single fault performance static approach drops immediately slowest consumer transfer complete consumer finished portion global task performance faults added system performance stays low level shown figure utilizing implicit information run-time adaptation simulated performance distributed queue performance faults excellent delivering ideal performance range induced faults figure shows simulation results closely match implementation running experiment implementation ideal datapoints graduated declustering distributed queue tolerates consumer-side performance faults moving data faster consumers suffers producer deliver data expected rate data locale producers bottleneck data transfer solution problem cases data replicated sake reliability mirrored disk system data block locations graduated declustering coordinate access replicated data set dividing aggregate producer bandwidth equally consumers lessening effects producerside performance faults figure shows performance simulation implementation simulations performance-fault layouts presented worst case faults occur adjacent producers immediately affect data sources consumer case distributed producers faults consecutive producers half producers perturbed implementation results shown layout note previous results presented best-layout performance capture range behaviors performance layout faults good ideal due limited amount replication producer contained acm transactions computer systems vol february arpaci-dusseau fig graduated declustering performance results simulation implementation compared ideal performance increasing number performance faults static transfer experiments nonfaulty producers generate data consumers consume data performance fault reduces producer performance factor due limited replication layout performance faults matters implies spread worst occur consecutively producers data set flexibility ideal performance generally level replication data item number producers system algorithm tolerate presence performance faults worst layout potentially tolerate faults layout real system randomly faults delivered performance fall bestand worst-case lines keys effective run-time adaptation discuss keys success run-time adaptation key careful management interaction communication layer communication heart adaptation river managing interaction river distributed algorithms flow control crucial robust performance key presence excess parallelism application level robust performance faults river applications move reasonable number objects fewer objects transferred performance fault unexpected deleterious effect fortunately data-intensive applications trouble meeting requirement key presence globally aware data scheduling entity participates robust data transfer monitor progress transfer compensate laggards ends goal scalable distributed implementation finally fourth key presence slack system system driven full rate acm transactions computer systems vol february run-time adaptation river fig flow control amount flow control varied set perturbation simulations line plots performance perturbation number flow-control credits total number credits producer number perturbations increased x-axis credit consumer tolerate full range perturbations experiments producers consumers rate unperturbed production consumption fault reduces rate consumption performance fault lower performance ideal system system engineered amount extra capacity adaptive mechanisms river utilize unexpected problems occur deliver performance consistently high rate keys led refinements algorithms communication layer crucial aspect implementation distributed queue graduated declustering behavior underlying message layer message layer restrict number outstanding messages mechanisms layer performance perturbation expected message layers including restrict number outstanding messages arbitrary number chosen message-layer developers worse number hidden clients communication layer render construction distributed algorithms impossible investigate number flow-control credits needed mask performance faults figure plots results simulations vary amount flow-control credits results similar shown simulations reveal importance flow control algorithm unperturbed case performance faults system face perturbation acm transactions computer systems vol february arpaci-dusseau unperturbed case outstanding credits producer performance drops ideal surprisingly producer outstanding messages fully pipeline system interestingly number faults increases number credits producer allocated increase cases number flow control credits equal number perturbed consumers performance suffers reason drop-off straightforward producer message credits outstanding slow producers algorithm remember nodes slow nodes sending messages slow nodes simulations conclude distributed queue algorithm outstanding message consumer delivers producer perspective remote performance consumers straightforward simple effective manner perspective prototype underlying message layer restrict number outstanding messages number consumers system current implementation hard limit total outstanding messages prototype tolerate performance faults consumers excess parallelism essential ingredient tolerating performance faults presence excess parallelism system producer consumers items data send perfect distribution distributed queue leads piece data consumer performance dictated rate slowest consumer realistic settings excessively perturbed nodes lead end-of-run effects first-order performance factors explore amount parallelism needed tolerate performance faults figure plots simulated performance graduated declustering single performance fault function total amount data read results identical focus solely experiment graph small amount data perform time perturbed producer deliver remaining blocks finished takes significant proportion run-time consumer send request data producer slowest producer receives messages level replication data set mirrored disk system producer processes messages end effects noticeable work system total amount work small performance faults difficult tolerate current river approach number arises hardware limitation myrinet network-interface card tandem design decisions acm transactions computer systems vol february run-time adaptation river fig excess parallelism performance single perturbation plotted function total amount work pushed simulated blocks block consumption rate perturbed producer unperturbed producer cases small number blocks pass system vulnerable performance faults delivering performance noticeably ideal simulations producers consumers note performance system deliver ideal experiment single fault present view problem weakness run-time methods river data nodes system data requested nodes operate faster problem partly remedied historical methods remember nodes performed poorly past bias actions start node system performing poorly inclusion data transfer offer gain node avoided altogether current mechanisms river memoryless runs relearn run performance characteristics system performance faults lengthy duration past performance predicts future performance historically based techniques effective solution write applications utilize parallelism amortize end-of-run effects found overly burdensome range data-intensive applications addition added complexity including historical approach warrant resultant small performance benefit globally aware local scheduling algorithm producer made aware progress consumers serves biasing scheduling data blocks lagging consumer producer make purely local decisions coerce system common global goal original algorithm acm transactions computer systems vol february arpaci-dusseau fig globally aware local scheduling average producer bandwidth plotted lifetime experiment experiment producers send data consumers consumers receive data varied lines rates producer receives roughly fair share bandwidth finish time producer sends data contention network begins leading unfair bias producers distributing progress information consumer clever local 
scheduling solves unfairness problem believed global awareness demonstrate requires globally aware local scheduling contexts figure plots performance experiments prototype system experiment producers sending data consumers experiments rate consumer sinks data controlled y-axis figure plots relative share aggregate consumer bandwidth producer numbered x-axis receives ideally producer receives equal share bandwidth reflected consumer rate producer receives fair share consumer bandwidth expected producers left graph receive notably higher portion consumer bandwidth receive correspondingly problem inherent original design firstcome first-served algorithm processing blocks consumer interaction myrinet switch fairness properties performing detailed measurements implementation found high loads participating nodes myrinet switches fairly schedule transfers producers receive fair share network bandwidth performance determined rate slowest entity unfairness performance system drop producer finish transfer acm transactions computer systems vol february run-time adaptation river overcome problem piggyback extra information messages specifically sending information rate progress producers consumers data request response consumer schedule service informed blind fcfs manner initial results modified performs consumer-side scheduling shown figure biasing service lagging producers performance level producers unfairness problem solved note behavior discovered simulation simplified simulations model switch behavior detail underscoring importance combined approach slack system finally explore slack system formally excess performance capability components slack single performance fault system reduces performance idealized model real results figure slack present run-time adaptive methods river exploit deliver consistent high performance applications figure shows results simulations unperturbed rate consumer varied graphs experiments producers generate data fixed rate peak rate consumers performs ideally graphs surprisingly consumer rate increases faults required induce performance decrease compared unperturbed system generated similar results omit resulting dilemma designer excess performance capacity engineer system decision scope current work answer question develop stochastic models modern device behavior apply models analyze behavior system enable probabilistic guarantees system behavior similar birman work bimodal multicast plan investigate work fail-stutter fault tolerance arpaci-dusseau arpaci-dusseau designer system consistent performance provided slack system engineered simply provide applications case mechanisms river graceful degradation performance failure river mechanisms applications examine system application level easily applications utilize order create robust applications study concentrate database query-processing primitives present performance results range query-processing primitives built top prototype implementation river comparing performance ideal regard previous results arpaci-dusseau present acm transactions computer systems vol february arpaci-dusseau fig slack results simulations compared ideal performance increasing number performance faults experiments producers generate data nonfaulty consumers consume data performance fault reduces consumer performance factor nonfaulty consumer performance increases faults required reduce performance peak advantage slack system predicted model results greater number applications improved implementations improve performance robustness applications discuss difficulties utilizing basic river mechanisms perfect match application semantics find suitable creating robust programs application performance general develop robust river application programmers insert distributed queues graduated declustering applications form points flexibility places dataflow performance faults tolerated application writers focus constructing flexible flows infrastructure handles rest utilizing core river mechanisms applications potentially withstand performance faults achieve ideal performance range faults study present performance o-intensive parallel database query-processing primitives transformed static parallel application robust adaptive version figure presents results primitive figure performance application increasing number disk performance faults shown acm transactions computer systems vol february run-time adaptation river fig application performance results running database primitives -machine cluster disk performance faults applications parallel scan data set read parallel generation data set write parallel filter read data disk select records based user-defined function write selected records disk top-n selection find top values data set find efficiently parallel hash-join data sets finally parallel external sort application operates roughly data disk node total machines performance fault reduces performance factor datapoint represents single run runs shown point x-axis best-fit line data plotted acm transactions computer systems vol february arpaci-dusseau fault disk utilizes half disk bandwidth applications ideal amount data touched application divided peak rate disks reasonable harsh ideal applications data intensive compute time negligible parallel scan reads total disks run ideal time scan seconds figure applications performance good faults system degrades gracefully faults imposed disks desired trend specifically performance ideal entire range disk faults applications examining leftmost point graph gain insight overheads utilizing river mechanisms provide performance robustness examine simpler applications scan scan simply parallel scan input data case processes read data disks read-only application robust version scan utilizes access data mirrored collection graph noticeable overhead roughly compared ideal application process reads data single disk overhead attributed change workload presented disk static nonrobust scan disks serves single sequential stream data single process robust scan disk serves streams processes due replication serving streams additional seeks induced lowering performance examine generate viewed converse scan parallel data generator processes generates random records writes disk robust version generate distribute load disks tolerate disk performance faults leftmost point generate application delivers ideal performance showing slight acceptable degradation applications exhibit complex combinations basic costs utilizees dqs robust versions exception found sort utilizes coarse-grained manner applications exhibits slightly worse performance characteristics expected application semantics top-n selection examine cases application semantics perfectly mesh mechanisms focus top-n selection top-n selection selects top data items collection based user-specified key small number queries form common databases internet search engines generating large set candidate results order results based quality metric present top results user acm transactions computer systems vol february run-time adaptation river fig top-n data flow static robust versions top-n selection presented top static nonrobust version application data read disk disk-read modules passed sort modules buffer amount data generating sorted run writing disk disk-write modules runs generated data transformed set sorted runs short final phase runs merged produce top data items user desires data merge continued sorted runs bottom diagram depicts disk-robust first-phase top-n selection robust mechanisms employed tolerate read side performance faults disks sort modules tolerate performance faults write side figure presents basic dataflow top-n selection phase sorted runs generated reading entire data set sorting block time read memory writing sorted run disk phase completes merging top records sorted runs final result approach user request items quickly supplied continuing merge challenge programmer make program robust disk performance faults begin phase figure shows diskrobust version phase top-n graduated declustering distributed queue utilized order make phase program robust utilizing replicated data set producer faults easily handled inserting immediately data moved 
faster sorters rates limited local disks fully disk-robust phase generated results figure confirm note ways implement top-n selection present method acm transactions computer systems vol february arpaci-dusseau find adding robustness merge phase program difficult reasons general apply applications order utilize replicate sorted runs written replication highlights general weakness run-time adaptive approach multiphase program pays high cost replicate data writing disk wishes subsequently avoid producer-faults read phase suited oft-read data sets replicated read times addition data merged single merge module prints final output module runs slowly reason application run slowly phase utilized single destination data applications dataflows similar minimize time spent portion dataflow weaknesses mind case application performance highly robust due amdahl law time spent phase large data sets disk greatly slowed phase application performance suffer unduly sorting present challenging operator external diskto-disk sorting general sorting good benchmark clustered systems stresses disk memory interconnect bandwidth section one-pass version sort two-pass sort consists multiple runs one-pass sort merge benefits development precedent set researchers measure performance sort key values uniform distributions assumption implications method distributing keys local buckets processing nodes nonuniform distribution modify implementation perform sampling phase sort blelloch dewitt sampling phase made robust topmost diagram figure presents flow data standard version sort based flow now-sort world-record breaking parallel sorting program clusters arpaci-dusseau data begin unsorted parallel collection number disks data read disk node disk read module passed range-partitioning module partitioning modules perform key-range partitioning data partitioning module reads top bits record determine sorter module record sorter module received input sorts data begins streaming disk write module proceeds write data disk stream preserving order read-sort-write phase repeats data transformed series sorted runs enhance sort disk-robustness utilize graduated declustering distributed queue shown bottommost diagram acm transactions computer systems vol february run-time adaptation river fig sort dataflow data read disk parallel set disk-read modules passed set range partitioners partitioners segment data set sort modules key sort modules present top fourth keys sorter fourth sorter sort modules received large chunk data fill memory independently sort data pass disk-write module output disk multipass sort phase repeat data sorted sorted runs facilitate robustness disk performance faults sort employ graduated declustering distributed queue graduated declustering utilized transparently transforms sort read-robust sort distributed queue complex data sorted sorters place data distributed queue standard data randomly scattered disks essentially undoing work sorting performed slightly distributed queue utilized sorter handing records distributed queue hands distributed queue entire sorted run time load balancing occurs coarser granularity preserving semantics sort figure case previous operators employ graduated declustering disk read provide performance-robust parallel data stream program addition distributed queue complex figure observe queue sort modules diskwrite modules sort modules passed sorted records distributed queue programs application perform expected distributed queue algorithm spread records randomly disks undoing work sort distributed queue sort modules key-range partitioning occurs crucial semantics sort removing acm transactions computer systems vol february arpaci-dusseau key-range partitioning change correctness sort distributed queue position placement function properly slight modification made distributed queue handing records time distributed queue sort module passes large sorted chunks data distributed queue distributed queue adapts rate disks coarser granularity sort module received data sort divide ten chunks note slightly form output one-pass sort n-node sort generates sorted runs n-node sort produces runs number sorted runs sort modules hand distributed queue performance cost extra work sort perform files opened closed standard sort figure presents results perturbation experiment figure performance perturbation stable programs attribute directly coarser granularity load balancing disks runs balance disks single slightly faster disk end noticeably larger amount work general performance degrades gracefully expected absolute performance high primitives due extra amount per-run overhead managing runs extending river application domains concentration database query-processing primitives natural question arises generality river model types applications benefit mechanisms provided river environment application written river model general-purpose programming substrate question types applications easily naturally written river framework readily utilize adaptive mechanisms provided order achieve level performance robustness application domain benefit river environment parallel scientific codes wolniewicz graefe shown common scientific operators fit dataflow environment operators reengineered performance robustness manner similar database primitives common operation types applications matrix transpose poole assuming data begin end disk transpose structurally similar external sort routing data based key record floating-point routed final destination based location final output set slight difference input read memory sort read input stream sequential order transpose slight modifications made standard distributed queue accommodate acm transactions computer systems vol february run-time adaptation river read input stream row column staggered manner balance output load properly examples work river environment out-ofcore matrix vector matrix matrix multiplications applications great deal flexibility order process data elements multiplied amenable transformation performance-robust programs finally applications poole survey o-intensive scientific applications require strict ordering records written disk hinting suitability river environment poole applications excellent candidates river implementation experimentation good match limitations river run-time adaptation finally discuss scenarios run-time adaptive techniques river work situation myrinet switch deadlock showcases reliance river network performance-reliable medium note problem arose implementation highlighting experimentation real system scenario demonstrates inherent weakness run-time adaptation decisions made run-time lack global perspective data written disk current conditions laid properly access potentially conditions global performance faults problem present result peculiar switch behavior demonstrative general problem figure plots performance graduated declustering scale increasing number producers consumers x-axis plotting total throughput percentage peak y-axis case producers throttled produce data fast perturbations system figure performance excellent low scale coming close peak drops unexpectedly producers consumers involved careful instrumentation found poorly performing experiments performance fine period time suffered dramatic systemwide two-second pauses investigation symptom led conclusion myrinet switches deadlocking halting progress detected deadlock recovered fortunately assistance implementors library altered avoid problem experience illustrative library changed fragment messages smaller chunks fragmented switches observe fragment interarrival time erroneously assume delayed fragment implied deadlock deadlock recovery mode implementors installed fix fragmenting messages switches longer reason time-out acm transactions computer systems vol february arpaci-dusseau fig switch deadlock performance graduated declustering plotted scale cases switch deadlock occur graph number producers consumers covaried x-axis percent ideal performance plotted y-axis machines involved experiment deadlock occurs scale higher rates occur performance drops noticeably general point river method run-time 
adaptation relies global characteristics network network performing expected system tolerate producerand consumer-side performance faults global performance faults network global effect avoided mechanisms contrast localized performance faults network link contention link performance-failure naturally handled adaptive mechanisms river faults indicative design diversity gray reuter called architectural heterogeneity type heterogeneity avoids problems occur collection identical components suffers identical design flaw including components makes manufacturers system gray reuter state heterogeneity akin belt suspenders belts suspenders additional network graceful performance fail-over network avoided myrinet deadlock problem solutions costly local versus global perspective finally discuss potential general weakness run-time adaptation river scenario assume application writing records disk disks faster naturally allocate proportional amount data disks application reads acm transactions computer systems vol february run-time adaptation river data back disks obtain peak performance system performance characteristics disks changed previously high-performing disks longer high-performing read performance suffer data written disk performance footprint created time future state system longer matches footprint performance mismatch occurs delivered performance longer approaches ideal method ameliorate potential problem replication replicating data access potentially tolerate unanticipated performance fluctuations left question data sets replicate type adaptation rearranging data offline account current characteristics system broader scale run-time mechanisms river designed handle complementary offline adaptive techniques similar developed neefe matthews required provide complete solution view development techniques main goals future work related work river draws related work areas parallel databases parallel storage systems parallel file systems parallel programming environments discuss related work areas turn parallel databases large-scale operations common parallel database systems number parallel databases found literature including gamma dewitt volcano graefe digital rdb prototype barclay bubba copeland systems based techniques similar dataflow model river parallel queries directed graph connects sequential data operators gamma gamma parallel database system developed wisconsin dewitt initial prototype developed shared-nothing cluster vax processors main memory connected token-ring network machines identical hard drives attached dewitt basic partitioning techniques provided distribute data processors round-robin hash range user-specified key range assuming uniform distribution communication processors performed split table takes tuples sending processor distributes receiving processors aforementioned distribution styles contrast river data distribution techniques gamma make strong performance assumptions partitioning techniques total time completion determined slowest consumer group network connects machines shared medium acm transactions computer systems vol february arpaci-dusseau case token-ring network aggregate network bandwidth scale processors data easily moved cluster remote consumption fundamental tenets river design volcano prominent parallel database system literature volcano graefe wolniewicz graefe volcano construct called exchange operator move data processors similar gamma split table case gamma contrast river volcano makes solely nonrobust distribution techniques hash-partitioning range-partitioning round-robin replication flexible distribution mechanism major difference volcano gamma models parallelism gamma demand-driven approach sinks pull data sources request messages conversely volcano data-driven approach data eagerly consumers consumers explicitly request data message-passing libraries issues arise form pull-based message layers versus push-based karamcheti chien conceptually similar gamma parallel database systems volcano intended primarily shared-memory machine early prototypes ran -processor sequent symmetry shared-memory machine excellent platform river-like system interconnect performance good digital rdb work parallel-load prototype digital rdb project barclay describe dataflow execution environment connections producers consumers data-flow rivers connect streams data stated river partitioning based split-table streams river split table suggests record inserted river river program split table pick destination stream record river extracts field values record compares values values split table pick destination stream split-table rangepartitioning hash partitioning round-robin replication input records sink operators barclay static techniques provide performance availability run rate slowest sink authors state nodes speeds amounts memory longer straight-forward distribute work evenly nodes barclay flexible method distribution found river addition parallel river takes advantage unordered record processing system provide form run-time adaptation ibm smps lindsey system shared data pools accessed multiple threads faster threads acquiring work lindsey refers access style straw model thread slurps data straw potentially rate acm transactions computer systems vol february run-time adaptation river implementing system natural smp simple lock-protected queue suffice modulo performance concerns river viewed distributed implementation concept parsets notion applying operations data set parallel explored parsets dewitt object-oriented database system application developer create function subsequently direct system apply objects collection model computation great flexibility building performance robust system data replicated robust access replicated data storage parsets implementation processes function statically data site dynamically balance load avoid ill-effects performance variations ncr teradata current commercial systems ncr teradata machine exclusively hashing partition work achieve parallelism good hash function effect dividing work equally processors providing consistent performance achieving good scaling properties jim gray teradata system performance bad worse consistency scalability goals system cost underlying hardware contrast river attempts deliver performance current configuration system stable performance river-based applications fluctuate eddies finally eddies adaptive dataflow environment built top river system avnur hellerstein eddies adaptive ideas river step reordering data operators on-thefly order achieve higher levels performance specifically monitoring selection join predicates highly selective eddies adapt dataflow place highly selective operators reduce total amount work performed system storage systems raid redundant arrays inexpensive disks raids popular organize collections multiple disks gibson katz patterson idea simple aggregate set less-expensive disks block-level interface commonly amount storage circumvent failures variety redundancy mechanisms chen excellent survey striping commonly extract full aggregate bandwidth multiple disks striping spreads blocks disks fixed round-robin pattern based logical address block simple striping breaks disks collection runs slower rate expected performance simple striping classified performance fragile entity perform expected global performance match expectations performance assumptions made acm transactions computer systems vol february arpaci-dusseau disk recent work addresses static performance heterogeneity raid systems cortes labarta relative performance rates drives change performance problems occur petal petal distributed system exports block-level interface lee thekkath assembled group workstations pcs multiple disks attached petal presents collection clients highly virtual disk place data main objective petal provide easily administrable high-performance storage scalable switch-based network petal systems provide form run-time adaptation similar spirit petal mirrors data disks set reads client directed multiple locations based load information petal simple dynamic algorithm client track number requests pending server sends read requests server shorter queue length lee thekkath performs similar balancing additional element disk server services requests biased fashion optimizing global application progress petal load-balancing writes global operations striping suffer fate traditional 
storage systems chained declustering chained declustering technique performs naive mirrored system failure present system hsiao dewitt typical mirrored systems replication naive blocks disk mirrored identically failure occurs surviving disk pair overloaded chained declustering avoids problem spreading replica blocks disks balancing load read-intensive workloads ways generalization chained declustering chained declustering works case absolute failure single disk works performance failure single disk active disks recent trend storage systems computation moved disks acharya riedel active disk systems perfect environment river problems encountered clusters encountered acharya stream-based programming model similar river dataflow model extending adaptive mechanisms straightforward additional adaptation techniques required dynamic migration computation host processor disks depending current system load network performance levels amiri parallel file systems turn attention large body work parallel file systems systems focused extracting high performance set uniform disks including ppfs huber bridge dibble acm transactions computer systems vol february run-time adaptation river panda seamons winslett galley nieuwejaar kotz vesta corbett feitelson swift cabrera long cfs nitzberg sfs loverso sio specification bershad spiffi freedman common features include scatter gather transfers asynchronous interfaces layout control prefetching caching support client server parallel file systems stripe data naively set disks subsystem undesirable performance properties shared file pointers interesting feature provided systems notion shared file pointer found cfs nitzberg spiffi freedman shared file pointer multiple processes machines access file concurrently consistent manner sharing local file pointer shared file pointers excellent performance properties group processes reading data collection faster processes read data providing coarse-grained load balancing application similar spirit shared-file pointers provide properties sequentially read files provide support load-balancing writes disk collective advanced parallel file systems higher-level interfaces data collective kotz referred disk-directed similar concept expressed two-phase choudhary original paper kotz found scientific codes show tremendous improvement aggregating requests shipping underlying system nodes schedule requests noticeably increase delivered bandwidth requests made returned specific consumers load balanced consumers dynamically solve performance problems common clustered systems panda systems discussed panda kuo seamons winslett deals explicitly performance heterogeneity solutions limited deals heterogeneity disk writes reads left unbalanced previous write perfectly balanced load disks access pattern approach priori static measurement disk performance calculate lay data disks performance write system properly react round measurement contrast river applications make decisions dynamically state drive performance handle performance run-time parallel distributed programming environments finally parallel programming environments exploited benefits run-time adaptation examples include cilk blumofe lazy threads goldstein multipol acm transactions computer systems vol february arpaci-dusseau chakrabarti systems dynamically balance load consumers order facilitate programming highly irregular finegrained parallel applications cilk cilk randall parallel programming environment designed parallel machines parallelism attained spawning extremely lightweight threads allowing users express arbitrarily complex parallel control constructs load-balancing achieved cilk work stealing processor work examines processor work queue picked uniformly random steals work conceptually stealing work work queue similar load balancing cilk implementation tuned thread-level work stealing aimed high-performance data movement contribution note cilk system authors proven cilk work-stealing scheduler achieves space time communication bounds constant factor optimal multipol multipol run-time support irregular applications distributed data structures focus hiding communication latency asynchrony chakrabarti load balancing provided distributed task queue wen similar design implementation linda linda shared globally addressable tuple-space parallel programs carriero gelernter applications perform atomic actions tuple-space inserting tuples querying space find records attributes tuple space similar general generality model high performance distributed environments shown difficult achieve bal reliable multicast finally birman encountered similar problems performance faulty nodes research reliable multicasting work alter guarantee provided multicast infrastructure absolute guarantee probabilistic avoid ill-effects stuttering node pursue similar goals exploit application flexibility obtain robust performance conclusions heart river system run-time adaptation component system statically trusts performance component node constantly gauges performance data transfers allocates data requests nodes proportion perceived performance built philosophy mind demonstrated article robust data-transfer mechanisms delivering ideal performance range perturbation scenarios acm transactions computer systems vol february run-time adaptation river keys run-time adaptation derived interaction communication layer utmost importance number flow control credits provided scale size system excess parallelism needed order overcome potential problems extremely poorly performing components performance dictated slow component system local data processing guided global knowledge progress property found fourth final slack needed run-time adaptive methods deliver peak performance presence small number performance faults slack system remains open question applications built river framework make primitives order robust disk performance faults suite database query-processing primitives run ideal broad range disk performance faults applications perfectly met system demonstrated top-n query external sort uncovered weaknesses river approach run-time adaptive methods rely strongly network backplane adaptation entire network function properly switches deadlock performance match expectations cases run-time adaptation short-sighted plan investigate complementary long-term adaptation order eventually build fully adaptive system methodological point view combination modeling simulation implementation crucial understanding system behavior simulations study isolation well-controlled setting allowing focus important properties flow control understanding algorithms behave second-generation implementation distributed algorithms proceeded ease implementation find limitations system arise simplified simulations underscoring importance building working prototype simple models gauge absolute performance faults understanding performance complex adaptive system made easier understands potential ideal acknowledgments eric anderson noah treuhaft contributed ideas implementation river system andrea arpaci-dusseau excellent aspects work dave patterson david culler joe hellerstein made substantial contributions ideas presented article jim gray excellent feedback direction advice finally anonymous referees careful feedback greatly improved substance style article acm transactions computer systems vol february arpaci-dusseau acharya uysal saltz active disks proceedings eighth conference architectural support programming languages operating systems asplos viii san jose calif adler chakrabarti mitzenmacher rasmussen parallel randomized load balancing proceedings annual acm symposium theory computing stoc acm york amd amd athlon processor architecture amd amiri petrou ganger gibson dynamic function placement dataintensive cluster computing proceedings usenix annual technical conference san diego anderson culler patterson team case networks workstations ieee micro february arpaci dusseau vahdat liu anderson patterson interaction parallel sequential workloads network workstations proceedings acm sigmetrics international conference measurement modeling computer systems ottawa arpaci-dusseau implicit coscheduling coordinated scheduling implicit information distributed system acm trans comput syst tocs august arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations proceedings acm sigmod conference management data sigmod tucson arpaci-dusseau performance availability networks workstations phd thesis california berkeley arpaci-dusseau arpaci-dusseau fail-stutter fault 
tolerance proceedings eighth workshop hot topics operating systems hotos viii schloss elmau germany arpaci-dusseau anderson treuhaft culler hellerstein patterson yelick cluster river making fast case common proceedings workshop input output parallel distributed systems iopads atlanta arpaci-dusseau arpaci-dusseau culler hellerstein patterson architectural costs streaming comparison workstations clusters smps proceedings high-performance computer architecture hpca las vegas avnur hellerstein eddies continuously adaptive query processing proceedings acm sigmod conference management data sigmod dallas bal kaashoek tanenbaum orca language parallel programming distributed systems ieee trans softw eng march barclay barnes gray sundaresan loading databases dataflow parallelism sigmod record acm sig manage data dec baru fecteau goyal hsiao jhingran padmanabhan copeland wilson parallel edition ibm syst bershad black dewitt gibson peterson snir operating system support high-performance parallel systems tech rep ccsfscalable initiative caltech concurrent supercomputing facilities caltech birman cooper isis project real experience fault-tolerant programming system oper syst rev april birman hayden ozkasap xiao bidiu minsky bimodal multicast acm trans comput syst tocs blelloch leiserson maggs comparison sorting algorithms connection machine cmin proceedings symposium parallel algorithms architectures hilton head blumofe joerg kuszmaul leiserson randall zhou cilk efficient multithreaded runtime system proceedings symposium principles practice parallel programming santa barbara calif acm transactions computer systems vol february run-time adaptation river boden cohen felderman kulawik seitz seizovic myrinet gigabit-per-second local-area network ieee micro feb bolosky iii draves fitzgerald gibson jones levi myhrvold rashid tiger video fileserver tech rep microsoft research boral alexander clay copeland danforth franklin hart smith valduriez prototyping bubba highly parallel database system ieee trans knowl data eng march borg blau graetsch herrmann oberle fault tolerance unix acm trans comput syst feb bressoud schneider hypervisor-based fault tolerance proceedings fifteenth acm symposium operating systems principles sosp copper mountain resort colo brewer inktomi web search engine invited talk proceedings acm sigmod conference brewer kuszmaul good performance cmdata network proceedings international parallel processing symposium cancun cabrera long swift distributed disk striping provide high data rates comput syst fall carriero implementation tuple space phd thesis department computer science yale chakrabarti deprit jones krishnamurthy wen yelick multipol distributed data structure library tech rep csd- california berkeley july chen bershad impact operating system structure memory system performance proceedings fourteenth acm symposium operating systems principles sosp asheville chen lee gibson katz patterson raid highperformance reliable secondary storage acm comput surv june choudhary bordawekar harry krishnaiyer ponnusamy singh thakur passion parallel scalable software input-output tech rep sccsece dept npac case center syracuse september codd relational model data large shared data banks commun acm june copeland alexander boughter keller data placement bubba proceedings acm sigmod international conference management data acm chicago corbett feitelson vesta parallel file system acm trans comput syst august cortes labarta extending heterogeneity raid level proceedings usenix annual technical conference boston dewitt gray parallel database systems future high-performance database systems commun acm june dewitt gerber graefe heytens kumar muralikrishna gamma high performance dataflow database machine tech rep trdept computer science univ wisconsin-madison march dewitt ghandeharizadeh schneider performance analysis gamma database machine sigmod record acm sig manage data september dewitt naughton shafer venkataraman parsets parallelizing oodbms traversals implementation performance proceedings international conference parallel distributed information systems ieee computer society austin texas dewitt naughton schneider parallel sorting shared-nothing architecture probabilistic splitting proceedings international conference parallel distributed information systems miami beach acm transactions computer systems vol february arpaci-dusseau dibble scott ellis bridge high-performance file system parallel processors proceedings eighth international conference distributed computer systems san jose calif englert gray kocher shah benchmark nonstop sql release demonstrating near-linear speedup scaleup large databases proceedings acm sigmetrics conference measurement modeling computer systems boulder colo fox gribble chawathe brewer gauthier cluster-based scalable network services proceedings sixteenth acm symposium operating systems principles sosp saint-malo france freedman burger dewitt spiffi scalable parallel file system intel paragon ieee trans parallel distrib syst nov gelernter carriero chandran chang parallel programming linda proceedings international conference parallel processing icpp charles ill gibson redundant disk arrays reliable parallel secondary storage acm distinguished dissertation mit press cambridge mass goldstein schauser culler lazy threads implementing fast parallel call parallel distrib comput august graefe volcano extensive parallel dataflow query processing system tech rep oregon graudate center june graefe encapsulation parallelism volcano query processing system sigmod record acm sig manage data june gray processors infinitely fast storage free invited talk proceedings iopads gray reuter transaction processing concepts techniques morgan kaufmann san francisco gribble brewer hellerstein culler scalable distributed data structures internet service construction proceedings fourth symposium operating systems design implementation osdi san diego grochowski emerging trends data storage magnetic hard disk drives datatech sept hsiao dewitt chained declustering availability strategy multiprocessor database machines proceedings sixth international data engineering conference los angeles huber elford reed chien blumenthal ppfs high performance portable parallel file system proceedings ninth acm international conference supercomputing barcelona johnson designing distributed queue proceedings seventh ieee symposium parallel distributed processing san antonio tex karamcheti chien comparison architectural support messaging tmc cmand cray proceedings annual international symposium computer architecture santa margherita ligure italy katz gibson patterson disk system architectures high performance computing proc ieee dec kotz disk-directed mimd multiprocessors proceedings symposium operating systems design implementation monterey calif kuo winslett cho lee chen efficient input output scientific simulations proceedings sixth workshop input output parallel distributed systems acm press atlanta kushman performance nonmonotonocities case study ultrasparc processor thesis massachusetts institute technology boston lee thekkath petal distributed virtual disks proceedings seventh conference iron architectural file support systems programming vijayan languages prabhakaran operating computer systems sciences asplos regional vii engineering cambridge college trichy mass india lindsey computer smp sciences intra-query parallelism udb wisconsin-madison database dissertation seminar submitted partial berkeley fulfillment acm transactions requirements computer degree systems doctor vol philosophy february computer run-time sciences adaptation river wisconsin-madison liskov committee charge distributed andrea programming arpaci-dusseau argus co-chair commun remzi acm arpaci-dusseau co-chair march david litzkow dewitt tannenbaum mary vernon basney mikko livny lipasti abstract checkpoint iron file migration systems unix vijayan processes prabhakaran disk condor drives distributed widely processing system tech primary medium rep storing information wisconsinmadison commodity computer file sciences systems april trust lorie disks daudenarde work hallmark fail completely stamos young adding intra-transaction parallelism existing dbms early experience ieee data eng newslett march loverso isman nanopoulos nesheim milne wheeler sfs parallel file system cmin proceedings summer usenix technical conference cincinnati mainwaring culler active message applications programming interface communication subsystem organization tech rep csd- california berkeley october matthews roselli costello modern wang disks exhibit complex failure modes latent anderson sector faults block improving corruptions portions performance disk fail thesis focus understanding failure policies file systems improving robustness disk failures suggest fail-partial failure model disks incorporates realistic localized faults latent sector faults block corruption develop apply semantic failure analysis technique file system block type knowledge transactional semantics inject interesting faults investigate commodity file systems react range realistic disk failures apply technique important journaling file systems linux ext reiserfs jfs xfs windows ntfs classify failure policies taxonomy measures internal robustness iron includes failure detection recovery techniques analysis results show commodity file systems store redundant information failure policies inconsistent buggy generally inadequate ability recover partial disk failures remedy reliability short comings commodity file systems addressing issues design low-level redundancy techniques file system handle disk faults begin qualitatively quantitatively evaluating redundancy information checksum parity replica order account spatially correlated faults propose probabilistic model construct redundancy sets finally describe update strategies overwrite no-overwrite approach file system update data parity blocks atomically nvram support overvi show low-level redundant information greatly enhance file system robustness incurring modest time space overheads remedy problem failure handling diffusion develop modified ext unifies failure handling centralized failure handler cfh showcase power centralized failure handling ext modified iron version ext cfh demonstrating support flexible consistent fine-grained policies carefully separating policy mechanism ext demonstrates file system provide comprehensive easily understandable failure-handling policy vii advisors andrea remzi showed research fun viii acknowledgements express heart-felt advisors andrea arpaci-dusseau remzi arpaci-dusseau distant dream parents doubtful intentions pursue doctorate andrea remzi encouraged supported decision initial sparks interest research created andrea advanced operating systems thoroughness deep thoughts interpret remzi cease amaze buried small trivial details rescues broad high-level perspectives didn realize research fun time met remzi master art living listens patiently muddled ideas sharpens thought process guides projects helps write interesting papers importantly makes feel reality guidance lot crazy ideas solid work andrea remzi source inspiration role models great times weekly meetings lasted minutes due lack capacity conversation awed clarity breadth depth thoughts end meetings end-up wondering raise level thinking fact enjoy working back time start fresh avoid mistakes learn longer meetings grateful dissertation committee members david dewitt mary vernon mikko lipasti insightful comments suggestions feedback preliminary final exams improved thesis greatly express sincere gratitude paul barford studied semester played important role motivating kind encouraging words letters indebted gave feedbacks dissertation topic interview process discussions chandhu thekkath microsoft research yasushi saito google erik riedel seagate ram rajmony ibm-austin ibm-almaden interview panel fred douglis ibm-watson mary baker labs fruitful fortunate work wonderful colleagues nitin agrawal lakshmi bairavasundaram nathan burnett timothy denehy haryadi gunawi todd jones florentina popovici muthian sivathanu late-night work conference deadlines lot fun free pizzas advisors muthian sivathanu specially eager discuss problems technical personal offer numerous occasions surprised complex clear analytical reasonings himani apte meenali rungta letting participate project answer in-depth questions learnt lot internals ext friends madison don family members united states friends felt bad home arini balakrishnan gogul balakrishnan vinod deivasigamany aditi ganesan karthik jayaraman vignesh kannappan indirajith krishnamurthy ramanathan palaniappan prabu ravindran muthian sivathanu sanjay sribalusu joseph stanley happy small successes supported difficult times miss fun screening late-night movies watching disney channel gogul prabu witty comments evening coffee conversations arini concerns eating habits reminds mother grateful family back log-structured file india systems grandpa adaptive regular methods calls proceedings spite sixteenth age acm symposium great source operating systems encouragement principles felt sosp 
low years saint-malo call france sister meter family observing minutes effects laugh multi-zone disks helped proceedings relax usenix finally conference anaheim grateful calif nieuwejaar parents faith kotz galley wishes parallel pursue file system doctorate proceedings tenth irresponsible acm son international pursuing conference desires supercomputing acm supporting press philadelphia family nitzberg performance complained showed ipsc concurrent personal file system struggles tech rep rejoiced rnd- nas smallest systems successes division showering nasa ames december love ousterhout feeling scheduling great prince techniques hope concurrent systems made proceedings proud contents abstract international conference acknowledgements distributed computing introduction systems analysis miami fort file system lauderdale ousterhout tcl robustness building robust file systems contributions outline fail-partial failure model terminology storage subsystem disks fail fail-partial failure model transience failures locality failures frequency failures trends iron taxonomy levels detection detection embedable command language proceedings usenix association winter conference washington ousterhout toolkit based tcl language proceedings usenix association winter conference dallas patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod chicago patterson gibson katz case redundant arrays inexpensive disks raid sigmod record acm sig manage data sept poole preliminary survey intensive applications tech rep ccsfscalable initiative caltech concurrent supercomputing facilities caltech raghavan hayes scalar-vector memory interference vector computers proceedings international conference parallel processing charles ill randall cilk efficient multithreaded computing phd thesis massachusetts institute technology boston riedel gibson faloutsos active storage large-scale data frequency levels recovery iron file system doesn raid make storage reliable background journaling file systems ext xii reiserfs jfs xfs ntfs file system summary failure policy analysis results semantic block-level information alternative techniques failure policy analysis methodology failure policy fingerprinting applied workload fault injection failure policy inference journaling models putting fault injection summary failure policy analysis results linux ext mining multimedia proceedings vldb york schneider implementing fault-tolerant services state machine approach tutorial acm comput surv dec seamons winslett multidimensional array panda supercomput singh weber gupta splash stanford parallel applications sharedmemory comput arch news march talagala patterson analysis error behaviour large storage system proceedings ipps workshop fault tolerance parallel distributed systems san juan puerto rico tandem performance group benchmark nonstop sql debit credit transaction proceedings sigmod international conference management data chicago teradata corporation dbc data base computer system manual release teradata corporation document number acm transactions computer systems vol february arpaci-dusseau tremblay greenley normoyle design microarchitecture ultrasparc-i proc ieee dec von eicken basu buch vogels u-net user-level network interface parallel distributed computing proceedings fifteenth acm symposium operating systems principles copper mountain resort colo wen portable library support irregular applications phd thesis california berkeley tech rep ucb csd- wolniewicz graefe algebraic optimization computations scientific databases proceedings vldb dublin received october revised june accepted july acm transactions computer systems vol february 
reiserfs ibm jfs xfs windows ntfs file system summary technique summary conclusion building low-level redundancy machinery in-disk redundancy ixt prototype iron file system implementation evaluation summary redundant parity blocks spatial locality sector failures redundant data update techniques performance analysis conclusion xiii unifying failure handling low-level machinery centralized failure handler design implementation evaluation conclusion related work robustness analysis fault injection formal methods techniques building robust file systems iron file systems disk failure modeling parity-based redundancy centralized failure handling conclusions future work summary lessons learned future work checksums relax ordering constraints understanding spatial locality disk faults impact richer interfaces reliability virtual machines improve storage stack reliability analyzing robustness data management systems xiv chapter introduction importance building dependable systems overstated fundamental requirements computer systems store retrieve information reliably disk drives widely primary storage medium decades systems including limited personal computers desktops laptops distributed file systems database systems high end storage arrays archival systems mobile devices disk failures occur traditionally systems built assumption disks operate fail stop manner classic model disks working perfectly fail absolutely easily detectable manner based assumption storage systems raid arrays built tolerate disk failures file system database system store data raid array withstand failure entire disk drive fault model presented modern disk drives complex modern drives exhibit latent sector faults block set blocks inaccessible latent sector fault sector fault occurs past fault detected sector accessed storing retrieving information blocks corrupted worse happen silently disk detect finally disks exhibit transient performance problems reasons complex disk failure modes trend common drive industry pack bits square inch bps areal 
densities disk drives growing rapid rate density increases errors bit spillovers adjacent tracks occur effects errors larger higher areal densities corrupt bits addition increased density increase complexity logic firmware manages data result increased number bugs buggy firmwares issue misdirected writes correct data disk wrong location increased low-end desktop drives ide ata drives worsens reliability problem low cost dominates design personal storage drives tested machinery handle disk errors important ata drives laptops desktops high-end storage arrays emc centera large-scale internet clusters google personal storage drives designed active hours day perform reliably operational stresses enterprise systems finally amount software storage stack increased firmware desktop drive thousand lines code storage stack consists layers low-level device driver code considered bugs rest operating system code jim gray points study tandem availability components system increasingly reliable software necessarily dominant outages developers high-end systems realized nature disk faults built mechanisms systems handle redundant storage systems incorporate background disk scrubbing process proactively detect subsequently correct latent sector faults creating copy inaccessible blocks auditing disk latent sector faults increase rate visible latent faults recent storage arrays incorporate extra levels redundancy lessen potential damage undiscovered latent faults finally highly-reliable systems tandem nonstop utilize end-to-end checksums detect block corruption occurs failure characteristics latent sector faults block corruption trends increased cheap drives knowledge reliable highend systems raise question commodity file systems handle disk failures derived answer question file system analysis led question improve robustness commodity file systems research efforts answer questions form broad parts dissertation discuss parts sections analysis file system robustness dissertation question pose modern commodity file systems react failures common modern disks question applicable data management systems databases distributed file systems raid storage arrays focus limit scope thesis local file systems run personal computers reasons commodity file systems important pervasive home environment storing valuable non-archived user data photos home movies tax returns high end storage arrays emc centera distributed file systems nfs understanding commodity file systems handle disk failures improving robustness shed light robustness complex systems commodity file systems commodity file systems specifically open source file systems designed developed inexperienced free lance programmers aware complex disk failure modes analyze file systems unearth bugs design flaws assumptions made developer finally commodity file systems cheap hardware devices hardware redundancy multiple disks expensive hardware support nvram high-end systems raises challenges addressed commodity file systems analysis centers journaling file systems modern file systems linux ext reiserfs jfs xfs windows ntfs implement journaling order maintain file system integrity step understand disk failure handling file system aggregate knowledge research literature industry field experience form model disk failure label model fail-partial failure model emphasize portions disk fail block errors data corruption model place develop apply automated failure-policy fingerprinting framework inject realistic disk faults beneath file system goal fingerprinting unearth failure policy system detects recovers disk failures injecting faults randomly file system traffic develop apply unique semantic failure analysis sfa technique fail file system blocks based types transactional states specifically write failure analysis develop transactional state specific fault-injection technique build abstract model file system update behavior orders writes disk maintain file system consistency model inject faults interesting points file system transaction monitor system reacts failures read failure analysis simply fail block types semantic failure analysis approach leverages gray-box knowledge file system data structures meticulously exercise file system access paths disk wide range workloads characterize failure policy develop internal robustness iron taxonomy catalogs broad range detection recovery techniques file system handle disk failures file system error codes lower layers detect read request completed successfully redundant information replica recover failure output fingerprinting tool broad categorization iron techniques file system constituent data structures study focuses important substantially open-source file systems ext reiserfs ibm jfs xfs closedsource file system windows ntfs analysis results find technology high-end systems checksumming disk scrubbing filtered realm commodity file systems platforms find hoc failure handling great deal illogical inconsistency failure policy due diffusion failure handling code kernel inconsistency leads substantially detection recovery strategies similar fault scenarios resulting unpredictable undesirable fault-handling strategies failure handling diffusion makes difficult examine portions code determine failure handling supposed behave diffusion implies failure handling inflexible policies spread locations code base hard change addition observe failure handling coarse-grained challenging implement nuanced policies current system discover systems implement portions failure policy incorrectly presence bugs implementations demonstrates difficulty complexity correctly handling classes disk failure observe tolerance transient failures file systems assume single temporarilyinaccessible block fatal whole-disk failure show file systems recover partial disk failures due lack in-disk redundancy finally addition fingerprinting failure policy analysis helps find bugs file systems catastrophically affect on-disk data write failures analyzed file systems commit failed transactions disk lead problems including unmountable file system building robust file systems behavior realistic disk failures led question change file systems handle modern disk failures file system view disk utterly reliable component blocks corrupt file system apply measures detect recover corruption running single disk approach instance end-to-end argument top storage stack file system data management system fundamentally responsible reliable management data metadata thesis present number techniques implemented high-end systems design implement evaluate commodity file system linux ext challenge building file system limited resources typical desktop implement techniques overly reduce performance make system attractive main challenges building robust file system important design implement low-level redundancy machinery efficiently overheads prohibitive redundant information present harder incorporate current file system framework due diffused failure handling solve problem design redundancy techniques implemented efficiently lowend system handle problem rearchitect commodity file systems centralized failure handling framework unifies low-level redundancy machinery high-level failure policies initial efforts develop family prototype iron file systems robust variants linux ext file system iron ext ixt investigate costs checksums detect data corruption replication provide redundancy metadata structures simple parity protection user data show techniques incur modest space time overheads greatly increasing robustness file system latent sector faults data corruption implementing detection recovery techniques iron taxonomy system implement well-defined failure policy subsequently provide vigorous protection broader range disk failures step building robust file systems address specific problems arise realm personal computers single disk drives spatially local sector errors lack hardware support non-volatile ram nvram context design implement evaluate techniques 
store redundant information disk focusing primarily parity redundant information due simplicity wide-spread address problem spatially local errors proposing probabilistic disk failure model characterizes disk block errors spatial correlation model layout blocks probability errors affecting related blocks low address lack nvram support low-end systems developing parity update techniques specifically design block update techniques technique updates data blocks traditional fashion overwriting contents radical approach called no-overwrite issues writes locations disk block pointers point locations evaluate validity probabilistic model comparing results fault injection experiments probability multiple failures predicted failure model matches closely results fault injection experiments evaluate performance overheads parity update techniques show overwrite no-overwrite techniques incur performance overhead compared ext cost varies percent readonly workloads percent highly synchronous write intensive workloads no-overwrite technique fragment file running de-fragmenter periodically improve performance part dissertation seek improve state art failure-handling modern file systems solving problem noticed analysis failure handling commodity file systems diffused resulting hoc inconsistent coarse-grained failure policies approach begins age-old wisdom separating policy mechanism achieve design implementation centralized failure handler cfh important failure-handling decisions routed cfh responsible enforcing specific failure-handling policy system relevant information made cfh enabling construction desired failure-handling policies demonstrate power centralized failure handling modified version linux ext file system ext show ext flexible implementing broad range vastly policies show ext mimic failure-handling behavior file systems reiserfs ntfs lines code demonstrate ext inherently consistent policies single localized portion code desired failure handling enacted presence block-type information cfh ext enables fine-grained policies developed show ext implement highly paranoid policy metadata passing user data errors application end result file system handles disk failures comprehensive easily understood fashion result ext reacts expected manner unexpected disk failure occurs contributions contributions dissertation aggregate knowledge sources define realistic failure model modern disks call fail-partial model formalize techniques file system detect recover disk errors iron taxonomy develop fingerprinting framework determine failure policy file system framework academic study file systems developers apply test systems improve robustness framework analyze popular commodity file systems discover handle disk errors explore effectiveness cost redundant information checksums parity replica building prototype version iron file system ixt apply redundancy file system data metadata combinations analyze file system robustness disk failure performance characteristics propose probabilistic disk block failure model considers spatial correlation block errors derive probability failures model separate redundant information block layouts avoid spatially local errors develop data update approaches overwrite no-overwrite differ location block writes interaction file system journaling design centralized failure handler cfh unify failure handling file system evaluate cfh show enables flexible consistent fine-grained failure policies file system outline rest dissertation organized chapter present common disk failures discussion fail-partial model iron taxonomy chapter give introduction journaling file systems commodity file systems analyze chapter explains failure policy analysis discuss semantic failure analysis methodology results chapter present details building robust file system start broad exploration redundant information discussion system evaluation explain techniques address spatially correlated errors lack hardware support chapter presents centralized failure handler file systems related work discussed chapter conclusion future directions presented chapter chapter fail-partial failure model order understand commodity file systems handle disk failures understand storage system architecture ways fail standard taxonomy detection recovery techniques file system handle disk failures order classify failure handling policies chapter explain components storage stack complex layered collection electrical mechanical firmware software components section reasons file system faults storage system discuss common disk failures section present realistic fail-partial model disks discuss aspects model section follow discussion techniques iron file system detect recover disk errors section finally conclude chapter reasoning failure handling performed file system section raid complete solution improve storage reliability section terminology proceed details clarify terminologies order make discussion unambiguous easy follow follow ieee standard software engineering terminology definitions error fault failure error difference computed observed measured condition true theoretically correct condition incorrect step process data definition incorrect result generic block device driver device controller firmware media transport host disk generic file system specific file system storage subsystem electrical mechanical cache figure storage stack present schematic entire storage stack top file system beneath layers storage subsystem gray shading implies software firmware white unshaded hardware fault defect hardware device component defective disk sector incorrect step process data definition computer program software bugs common usage terms error bug express meaning fault context thesis terms error fault interchangeably failure inability system component perform required functions performance requirements simply put result fault failure disk failure occurs storage stack unable recover faulty sector mentioned section thesis focus limit scope local file systems rest thesis stated term file system refer specific file system top storage stack end-point storage stack storage subsystem figure presents typical layered storage subsystem file system similar complex layered storage architectures data management systems databases distributed file systems storage stack consists main layers host transport layer disk error occur layers propagate file system bottom storage stack disk magnetic storage media mechanical motor arm assembly electrical components buses cache important component firmware code embedded drive control higher-level functions including caching disk scheduling error handling firmware code substantial complex modern seagate drive roughly lines code connecting drive host transport low-end systems transport medium bus scsi networks common higherend systems fibrechannel hardware raid single disk additional layer raid controller added transport layer disk drives hardware raid controller manages multiple disk drives exports standard interface scsi transport layer top stack host hardware controller communicates device software device driver controls hardware block-level software forms layer providing generic device interface implementing optimizations request reordering modern operating systems windows linux provide support stack multiple layers software components volume managers software raid layers beneath file system device specific drivers resulting complex storage stack software file system layer split pieces high-level component common file systems specific component maps generic operations data structures file system standard interface vnode vfs positioned disks fail motivate failure model describe errors layers storage stack failures discussion begin bottom stack work file system media primary errors occur magnetic media classic problem bit rot occurs magnetism single bit bits flipped type problem detected corrected low-level ecc embedded drive physical damage occur media quintessential head crash culprit drive head contacts surface momentarily media scratch occur 
particle trapped drive head media thermal asperity spike read signal caused disk asperities contaminant particles disk heads lose reading capabilities temporarily dangers well-known drive manufacturers modern disks park drive head drive reduce number head crashes scsi disks include filters remove particles media errors lead permanent failure corruption individual disk blocks mechanical wear tear eventually leads failure moving parts drive motor spin irregularly fail completely erratic arm movements head crashes media flaws inaccurate arm movement misposition drive head writes leaving blocks inaccessible corrupted subsequent reads electrical power spike surge damage in-drive circuits lead drive failure electrical problems lead entire disk failure drive firmware interesting errors arise drive controller consists thousands lines real-time concurrent firmware disks return correct data circularly shifted byte memory leaks lead intermittent failures firmware problems lead poor drive performance drive manufacturers introduce firmware versions fix existing problems production drives fixes inadvertently increase failure rates drive firmware bugs well-enough field specific names misdirected writes writes place correct data disk wrong location phantom writes writes drive reports completed reach media phantom writes caused buggy misconfigured cache write-back caching enabled summary drive firmware errors lead sticky transient block corruption lead performance problems transport transport connecting drive host problematic study large disk farm reveals systems tested interconnect problems bus timeouts parity errors occurred frequency causing requests succeed slowly fail altogether transport transient errors entire drive bus controller main bus controller problematic eide controller series motherboards incorrectly completion disk request data reached main memory host leading data corruption similar problem controllers return status bits data floppy drive time hard drive observed ide protocol version problems yield corrupt data summary controller problems lead transient block failure data corruption low-level drivers recent research shown device driver code bugs rest operating system bugs crash operating system issue disk requests bad parameters data resulting data corruption fail-partial failure model discussion root failure ready put realistic model disk failure model failures manifest ways entire disk failure entire disk longer accessible permanent classic fail-stop failure block failure blocks accessible referred latent sector faults block corruption data individual blocks altered corruption insidious silent storage subsystem simply returns bad data read term model fail-partial failure model emphasize pieces storage subsystem fail discuss key elements fail-partial model including transience locality frequency failures discuss technology market trends impact disk failures time transience failures model failures sticky permanent transient temporary behavior manifests depends root problem low-level media problem portends failure subsequent requests contrast transport higher-level software issue block failure corruption operation succeed retried locality failures multiple blocks disk fail block failures dependent root block failure suggest forms block failure exhibit spatial locality scratched surface thermal asperity render number contiguous blocks inaccessible failures exhibit locality corruption due misdirected write impact single block frequency failures block failures corruptions occur commercial storage system developer succinctly stated disks break lot guarantees fiction frequently errors occur modeling reliability deciding failures important handle talagala patterson point disk drive manufacturers loathe provide information disk failures people industry refer implicit industry-wide agreement publicize details surprisingly actual frequency drive errors errors disk fail well-known literature previous work latent sector faults errors occur commonly absolute disk failure related research work estimates errors occur times absolute disk failures recently efforts made quantify disk error rates specifically gray measure uncorrectable bit errors file system moving petabytes data terms relative frequency block failures occur reads writes due internal error handling common disk drives failed writes sector remapped distant sector allowing drive transparently handle problems remapping imply writes fail failure component media stuttering transport lead unsuccessful write attempt move network-attached storage serves increase frequency class failures remapping succeed free blocks large scratch render blocks unwritable quickly reserved space reads problematic media unreadable drive choice return error trends areas processor performance technology market trends combine improve aspects computer systems contrast technology trends market forces combine make storage system failures occur frequently time reasons level technique comment dzero detection assumes disk works derrorcode check return codes assumes lower level lower levels detect errors dsanity check data structures require extra consistency space block dredundancy redundancy detect corruption blocks end-to-end table levels iron detection taxonomy reliability greater challenge drives made increasingly dense bits packed smaller spaces drive logic complexity increases low-end drive market cost-per-byte dominates corners cut save pennies ide ata drives low-cost class drives tend tested internal machinery prevent failures occurring result field ata drives observably reliable cost pressures serve increase usage server environments finally amount software increasing storage systems noted software root errors storage system hundreds thousands lines software present lower-level drivers firmware low-level code generally type code difficult write debug source increased errors storage stack iron taxonomy section outline strategies developing iron file system file system detects recovers range modern disk failures main focus develop strategies disks common storage arrays single disk internal robustness iron needed protection file system cope failures modern disks iron file system includes machinery detect level partial faults recover level level technique comment rzero recovery assumes disk works rpropagate propagate error informs user rstop stop activity limit amount crash prevent writes damage rguess return guess wrong block contents failure hidden rretry retry read write handles failures transient rrepair repair data structs lose data rremap remaps block file assumes disk informs locale failures rredundancy block replication enables recovery forms loss corruption table levels iron recovery taxonomy tables present iron detection recovery taxonomies note taxonomy means complete techniques exist raid variations proposed years detection recovery mechanisms employed file system define failure policy difficult discuss failure policy system iron taxonomy describe failure policy file system describe cache replacement file-layout policy levels detection level techniques file system detect problem occurred block accessed corrupted simplest detection strategy file system assumes disk works check return codes section approach surprisingly common applied unintentionally errorcode pragmatic detection strategy file system implement check return codes provided lower levels storage system sanity sanity checks file system verifies data structures consistent check performed single block blocks checking single block file system verify individual fields pointers valid ranges verify type block file system superblocks include magic number older file systems pilot include header data block checking block 
correct type information file system guard forms block corruption checking blocks involve verifying blocks bitmap corresponds allocated blocks involve periodically scanning structures determine intact consistent similar fsck journaling file systems benefit periodic full-scan integrity checks buggy journaling file system unknowingly corrupt on-disk structures running fsck background detect recover problems redundancy final level detection taxonomy redundancy forms redundancy detect block corruption checksumming reliable systems years detect corruption recently applied improve security checksums number reasons assist detecting classic bit rot bits media flipped in-media ecc catches corrects errors checksums well-suited detecting corruption higher levels storage system stack buggy controller misdirects disk updates wrong location write block disk checksums carefully implemented detect problems specifically checksum stored data checksums detect misdirected phantom writes higher levels redundancy block mirroring parity error-correction codes detect corruption file system copies block reading comparing determine corrupted techniques designed correction discussed assume presence loweroverhead detection mechanism detection frequency detection techniques discussed applied lazily block access eagerly scanning disk idle time iron file systems form lazy detection additionally eager methods disk scrubbing classic eager technique raid systems scan disk discover latent sector faults disk scrubbing valuable means recovery replica exists repair now-unavailable block detect error occurred scrubbing typically leverages return codes explicitly provided disk discovers block failure corruption combined detection techniques checksums scrubbing discover block corruption levels recovery level iron taxonomy facilitates recovery block failure single disk drive techniques handle latent sector faults block corruptions simplest approach implement recovery strategy notifying clients failure occurred propagate straightforward recovery strategy propagate errors file system file system informs application error occurred assumes client program user respond appropriately problem stop recover disk failure stop current file system activity action levels granularity coarsest level crash entire machine positive feature recovery mechanism turns detected disk failures fail-stop failures preserves file system integrity crashing assumes problem transient faulty block repeatedly-accessed data script run initialization system repeatedly reboot attempt access unavailable data crash intermediate level kill process triggered disk fault subsequently mount file system read-only mode approach advantageous entire system processes continue finest level journaling file system abort current transaction approach lead system complex implement guess recently suggested rinard reaction failed block read manufacture response allowing system running spite failure failure oblivious computing received attention researchers qin similar techniques transparently recover software failures negative artificial response desirable failing retry simple response failure retry failed operation recent work shows file systems recover number disk errors simply retrying retry appropriately handle transient errors wastes time retrying failure permanent repair iron file system detect inconsistency internal data structures repair fsck block pointed marked allocated bitmap freed discussed techniques context journaling file systems bugs lead corruption file system integrity remap iron file systems perform block remapping technique fix errors occur writing block recover failed reads specifically write block fails file system choose simply write block location sophisticated strategies remap entire semantic unit time user file preserving logical contiguity redundancy finally redundancy forms recover block loss simplest form replication block copies locations disk redundancy approach employs parity facilitate error correction similar raid adding parity block block group file system tolerate unavailability corruption block group complex encodings tornado codes subject worthy future exploration redundancy disk negative consequences replicas account spatial locality failure surface scratch corrupts sequence neighboring blocks copies allocated remote parts disk lower performance in-disk redundancy techniques incur high space cost desktop settings drives sufficient free space iron file system natural question file system implement detection recovery disk modern disks internal mechanisms detecting recovering errors sufficient view primary reason detection recovery file system found end-to-end argument lower-levels system implement forms fault tolerance file system implement guard forms failure file system place detect corruption data higher levels storage stack device driver drive controller reason implementing detection recovery file system file system exact knowledge blocks file system apply detection recovery intelligently block types file system provide higher level replication metadata leaving failure detection correction user data applications specific solution explore section similarly file system provide machinery enable application-controlled replication important data enabling explicit performance reliability trade-off reason performance file systems storage systems unwritten contract file system lay blocks achieve high bandwidth unwritten contract stipulates adjacent blocks logical disk address space physically proximate disk-level recovery mechanisms remapping break unwritten contract performance problems file system assumes responsibility remap logically-related blocks file avoid problems complexities placing iron functionality file system techniques require persistent data structures track redundant copies parity blocks located mechanisms require control underlying drive mechanisms recover on-disk data modern drives attempt positioning reading strategies interface exists control low-level strategies current systems file system exact disk geometry makes harder place redundant copies tolerate spatially local errors doesn raid make storage reliable question answered simply raid techniques provide reliable robust storage raid improve storage reliability complete solution reasons systems incorporate disk sine qua redundant storage systems desktop pcs ship single disk included cost driving force marketplace adding disk solely sake redundancy palatable solution raid protect failures higher storage stack shown figure layers exist storage subsystem file system errors occur layers file system ultimately responsible detecting recovering errors ironically complex raid controller consist millions lines code source faults depending raid system employed types disk faults handled lower-end raid controller cards checksums detect data corruption recently companies included machinery cope latent sector faults iron techniques file system single-disk systems multiple drives raid-like manner focus single-disk systems paper rich space left exploration iron file systems redundant storage arrays chapter background failure policy analysis focuses journaling file systems modern file systems perform journaling understand analysis methodology understand general concepts journaling transaction checkpointing specific file system details on-disk structures updated journaling file systems order maintain file system integrity concept transactions follow write ordering constraints issue updates depending file system transactions composed file system blocks modified records analysis examine journaling file systems open source linux ext reiserfs jfs xfs windows ntfs closed source reasons selecting file systems widely default file systems operating system configurations ext default file system red hat linux reiserfs suse linux distribution ntfs windows addition personal computers high-end storage arrays chapter give general introduction journaling file systems section briefly discuss commodity journaling 
file systems sections journaling file systems file system update takes place set blocks written disk system crashes middle sequence writes file journal commit journal inode sync fixed data fixed data fixed inode fixed data journal inode sync journal commit fixed data sync fixed inode sync writeback mode data write happen time writeback ordered journal inode data fixed inode data journal commit data journal write journal commit checkpoint write figure journaling modes diagram depicts journaling modes supported modern file systems writeback ordered data diagram time flows downward boxes represent updates file system journal inode implies write inode journal destination writes labeled fixed write fixed in-place ext structures arrow labeled sync implies blocks written succession synchronously guaranteeing completes curved arrow ordering succession write happen time finally writeback mode dashed box fixed data block happen time sequence data block write inode updates propagated file system diagrams show data flow journaling modes system left inconsistent state repair inconsistency earlier systems ffs ext scan entire file system perform integrity checks fsck mounting file system scan timeconsuming process hours large file systems journaling file systems avoid expensive integrity check recording extra information disk form write-ahead log journal forcing journal updates disk updating complex file system structures write-ahead logging technique enables efficient crash recovery simple scan journal redo incomplete committed operations bring file system consistent state normal operation journal treated circular buffer information propagated fixed location structures journal space reclaimed journaling modes modern journaling file systems ext reiserfs present flavors journaling writeback mode ordered mode data journaling mode figure illustrates differences modes choice mode made mount time changed remount writeback mode file system metadata journaled data blocks written directly fixed location mode enforce ordering journal fixed-location data writes writeback mode weakest integrity consistency semantics modes guarantees integrity consistency file system metadata provide guarantees data blocks assume process appends block file internally file system allocates pointer inode data block address hold data writeback mode file system free write time careful updates specifically force journal record disk commit block commit block written file system safely recover crash time write fixed location structures makes log successfully crash disk recovery point contents correct ordered journaling mode metadata writes journaled data writes fixed location ordered journal writes metadata contrast writeback mode mode integrity semantics metadata block guaranteed point block belong file continuing forced fixed location written journal untimely crash file system recover reasonable manner ordered journaling mode consistency metadata data affected data block overwritten system crashes writing inode block journal recovery file system inode timestamps inconsistent recency data full data journaling mode file system logs metadata data journal decision implies process writes data block typically written disk journal fixed location data journaling mode strongest integrity consistency guarantees performance characteristics cases worse surprisingly cases transactions journaling file systems write updates part transaction disk file system update separate transaction modern file systems group updates single compound transaction periodically committed disk approach simple implement compound transactions performance fine-grained transactions structure frequently updated short period time free space bitmap inode file constantly extended file systems write updates journal write full block called physical journaling write modified portion block called logical journaling physical journaling incurs journal traffic single bit modified metadata block entire block written journal hand logical journaling carefully writes modified records file system structures journal efficiently fixed-location structures corrupt file system checkpointing metadata precise moment power memory sensitive power failures send garbages disk power fails data dma disk physical journaling advantage fixed-location structures corrupt recovered simply scanning replaying successfully committed transactions checkpointing process writing journaled metadata data fixedlocations checkpointing checkpointing triggered thresholds crossed file system buffer space low free space left journal timer expires crash recovery crash recovery straightforward journaling file systems basic form redo logging updates data metadata written log process restoring in-place file system structures easy recovery file system scans log committed complete transactions incomplete transactions discarded update completed transaction simply replayed fixed-place structures ext linux ext default journaling file system distributions linux red hat built extension ext file system ext jcib inode groupscylinder group journal descriptor block inode bitmap journal commit blockdb data bitmap journal superblock figure ext layout picture shows layout ext file system disk address space broken series block groups akin ffs cylinder groups bitmaps track allocations regions inodes data blocks ext journal depicted file block group file system superblock descriptor blocks describe contents commit blocks denote ends transactions data metadata eventually standard ext structures fixed-location structures organization loosely based ffs disk split number block groups block group bitmaps inode blocks data blocks ext journal log commonly stored file file system stored separate device partition figure depicts ext on-disk layout ext journal structure ext supports journaling modes additional metadata structures track list journaled blocks journal superblock tracks summary information journal block size head tail pointers journal descriptor block marks beginning transaction describes subsequent journaled blocks including final fixed on-disk location data journaling mode descriptor block data metadata blocks ordered writeback mode descriptor block metadata blocks modes ext logs full blocks physical journaling opposed differences versions single bit change bitmap results entire bitmap block logged depending size transaction multiple descriptor blocks data metadata blocks logged finally journal commit block written journal end transaction commit block written journaled data recovered loss ext transactions ext maintains single system-wide compound transaction file system updates added committed group commit improves performance potentially result combining unrelated updates lead tangled synchrony asynchronous synchronous traffic file system ext commits checkpoints transactions influence factors application initiated synchronization calls journal size commit timer settings reiserfs reiserfs default journaling file system distributions linux suse general behavior reiserfs similar ext file systems support journaling modes compound transactions perform physical journaling reiserfs differs ext primary ways file systems on-disk structures track fixedlocation data ext structures ext improved scalability reiserfs tree data stored leaves tree metadata stored internal nodes impact fixed-location data structures focus thesis difference largely irrelevant format journal slightly ext journal file partition contiguous reiserfs journal file contiguous sequence blocks beginning file system ext reiserfs journal put device reiserfs limits journal maximum ext reiserfs differ slightly journal contents reiserfs fixed locations blocks 
transaction stored descriptor block commit block unlike ext reiserfs descriptor block compound transaction limits number blocks grouped transaction jfs jfs journaling file system developed ibm designed journaling support fully integrated start adding journal located default end partition treated contiguous sequence blocks journaling mode file system mount previous work infer jfs ordered journaling mode due small amount traffic journal obvious employing data journaling differentiate writeback ordered modes observe ordering writes matched ordered mode data block written application jfs orders write data block written successfully metadata writes issued jfs logging record level logical journaling inode index tree directory tree structure structure logged entire block structure result jfs writes fewer journal blocks ext reiserfs operations jfs default group concurrent updates single compound transaction circumstances transactions grouped write commit records log page finally commit timers jfs fixed-location writes happen linux kernel timer kupdate daemon expires journal writes triggered timer journal writes indefinitely postponed trigger memory pressure unmount operation infinite write delay limit reliability crash result data loss data written minutes hours xfs xfs -bit journaling file system developed sgi xfs supports ordered journaling mode data writeback journaling modes present similar jfs xfs journals records log writing blocks records logged xfs separate journal commit block journal block types records commit record separate journal super block journal super block equivalent stores dirty state journal determine journal replayed mount xfs mount log searched find transaction replayed clean unmount xfs writes unmount record journal mark journal clean fixed-location structures xfs consist allocation groups allocation group manages inodes free space allocation group effective independent entity kernel interact multiple groups simultaneously internally allocation group trees track data metadata ntfs microsoft ntfs widely default journaling file system windows operating systems source code documentation ntfs publicly tools finding ntfs file layout exist object ntfs file metadata stored terms files journal file located center file system related work find ntfs implement data journaling find ntfs similar jfs xfs logical journaling journals metadata terms records infer ntfs performs ordered journaling introducing artificial delays data writes noting delays metadata writes file journaling mode granularity system data ordered writeback ext physical reiserfs physical jfs logical xfs logical ntfs logical table journaling file systems summary table summary journaling properties commodity journaling file systems ext reiserfs support journaling modes ordered journaling mode default file system summary table summarizes salient features journaling file systems ext reiserfs support journaling modes ordered journaling default mode file systems chapter failure policy analysis results chapter describe file system failure policy analysis broadly ways unearth failure policies examining source code understand detection recovery techniques file system failure scenarios injecting failures monitoring failures handled approach time consuming error prone due large code base consisting thousands hundreds thousands commercial file systems lines intricate code involving lots low-level calls unique semantic failure analysis sfa technique block type information transactional semantics fail blocks order understand file systems handle block failures analyze journaling file systems linux ext reiserfs jfs xfs windows ntfs analysis draw main conclusions commodity file systems store redundant information disk result recover portions disk fail find illogical inconsistency file system failure handling policies finally find bugs failure handling routines show hard handle block failures logically consistent manner begin explaining failure analysis semantic block level information section description analysis methodology section finally present results section semantic block-level information concept semantic block-level information simple file system level semantic knowledge block types low-level block perform file system performance failure analysis builds previous work semantically-smart disk systems pseudo-device driver interpose file system traffic disk apply file system semantics block discuss semantic information detail semantic block analysis file systems traditionally evaluated approaches applies synthetic real workloads measures resulting file system performance collects traces understand file systems performing isolation misses interesting opportunity correlating observed disk traffic running workload performance answer workload behaves block-level tracing disk traffic analyze number interesting properties file system workload coarsest granularity record quantity disk traffic divided reads writes information understanding file system caching write buffering affect performance detailed level track block number block read written analyzing block numbers extent traffic sequential random finally analyze timing block timing information understand file system initiates burst traffic combining block-level analysis semantic information blocks infer behavior file system call approach semantic block analysis sba simply quantifying journal traffic journal sizes measuring application bandwidth understand role played journal sizes journal commit policy main difference sba standard block-level tracing sba understands on-disk format file system test semantic failure analysis addition understanding performance behaviors block type information fault injection purposes standard fault injectors fail disk blocks type oblivious manner block failed file system repeatedly injecting faults random blocks waiting uncover aspects failure policy laborious time-consuming process yielding insight key idea test file system efficient manner semantic failure analysis sfa semantic failure analysis failing blocks obliviously fail blocks specific type inode block transactional state ordered data block semantic information crucial reverse-engineering failure policy allowing discern strategies file system applies data structures disadvantage approach fault injector tailored file system tested requires solid understanding on-disk structures benefits sfa outweigh complexities alternative techniques directly instrumenting file system obtain timing information disk traces inject faults equivalent superior sba sfa pseudo-device driver case reasons directly instrument file system source code file system re-instrument versions released contrast sba sfa require file system source driver code reused file systems versions section directly instrumenting file system accidentally miss conditions disk blocks written pseudo-device driver guaranteed disk traffic finally instrumenting existing code accidentally change behavior code efficient driver impact file system behavior section summary semantic block-level information purposes understand performance problems fingerprint file system parameters uncover design flaws correctness bugs perform failure policy analysis type-aware fault injection sections focus failure policy aspect technique failure policy analysis methodology describe methodology analyzing failure policies journaling file systems basic approach simple inject disk faults beneath file system key points operation observe resultant behavior failure policy fingerprinting main objective failure-policy fingerprinting determine detection recovery techniques file system assumptions makes underlying storage system fail comparing failure policies file systems learn file systems robust disk failures suggest improvements analysis helpful inferring iron techniques implemented effectively approach inject faults beneath file system 
observe file system reacts follow slightly methodology write read failure analysis inject write failures construct analytical model transaction update process journaling file systems inject faults points model specifically order maintain file system consistency journaling file systems update file system structures ordering constraints file system updates written journal part transaction commit write mark transaction complete transaction successfully written fixed-location structures modified point time model ordering simple analytical model inject failures specific points model addition unearth failure policy approach enables evaluate file system journaling semantics presence errors file systems issue reads ordering constraints change on-disk state read failure block corruption analysis simply fail corrupt file system block type reads fault policy consistent file system simply run workload fail blocks accessed conclude reaction block failure fully demonstrates failure policy system file systems practice complex employ techniques depending operation performed type faulty block extract failure policy system trigger interesting cases challenge coerce file system code paths observe path handles failure requires run workloads exercising relevant code paths combination induced faults file system data structures claim stress code path strive execute interesting internal cases failure policy analysis consists major steps create workload inject faults deduce failure policy describe workload purpose singlets access chdir chroot stat statfs lstat open utimes read readlink exercise getdirentries creat posix api link mkdir rename chown symlink write truncate rmdir unlink mount chmod fsync sync umount generics path traversal traverse hierarchy recovery invoke recovery log writes update journal table workloads table presents workloads applied file systems test set workloads stresses single system call group invokes general operations span calls path traversal steps detail applied workload workload suite sets programs run unix-based file systems fingerprinting ntfs requires set similar programs set programs called singlets focus single call file system api mkdir set generics stresses functionality common api path traversal table summarizes test suite workload requires existing file directory symbolic link parameter stat posix call takes file path input searches parent directories returns information file running workloads create files directories case injecting read faults clear file system buffer cache on-disk copy read workload file system test introduces special cases stressed ext inode imbalanced tree indirect doubly-indirect triply-indirect pointers support large files workloads ensure ext structures purpose inode info files directories directory list files directory data bitmap tracks data blocks group inode bitmap tracks inodes group indirect large files exist data holds user data super info file system group descriptor holds info block group journal super describes journal journal revoke tracks blocks replayed journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled table ext data structures table presents data structures interest ext file system table list names major structures purpose sufficiently large files created access structures file systems similar peculiarities make exercise -tree balancing code reiserfs block types file systems test listed tables fault injection step inject faults emulate disk adhering fail-partial failure model type aware fault injection technique inject faults disk traffic discuss error model fault injection framework sections error model error model assume latent faults block corruption originate layers storage stack errors accurately modeled software-based fault injection linux detected low-level errors reported file system uniform manner errors device-driver layer reiserfs structures purpose leaf node items kinds stat item info files directories directory item list files directory direct item holds small files tail file indirect item large files exist data bitmap tracks data blocks data holds user data super info tree file system journal header describes journal journal descriptor describes contents transaction journal commit marks end transaction journal data blocks journaled root internal node tree traversal table reiserfs data structures table presents reiserfs data structures interest purpose jfs structures purpose inode info files directories directory list files directory block alloc map tracks data blocks group inode alloc map tracks inodes group internal large files exist data holds user data super info file system journal super describes journal journal data records transactions aggregate inode info disk partition bmap descriptor describes block allocation map imap control summary info imaps table jfs data structures table presents jfs data structures interest purpose xfs structures purpose super info file system journal record header marks journal record start transaction records beginning transaction journal commit record marks end transaction unmount record marks journal clean data holds user data table xfs data structures table presents xfs data structures interest purpose ntfs structures purpose mft record info files directories directory list files directory volume bitmap tracks free logical clusters mft bitmap tracks unused mft records logfile transaction log file data holds user data boot file info ntfs volume table ntfs data structures table presents ntfs data structures interest purpose errors inject block write stream attributes similar classification faults injected linux kernel fault specification consists attributes failure type specifies read write failed read error latent sector fault block corruption additional information system crashed block failure block type attribute specifies file system block type failed request failed dynamically-typed directory block statically typed super block specific parameters passed inode number inode corrupted block number failed transience determines fault injected transient error fails requests succeeds permanent fails subsequent requests fault injection framework testing framework shown figure consists main components device driver called fault-injection driver user-level process labeled coordinator driver positioned file system disk observe traffic file system inject faults points stream coordinator monitors controls entire process informing driver specific fault insert running workloads top file system observing resultant behavior similar fault injection framework ntfs windows filter driver interpose file system traffic flow diagram benchmarking process shown figure describe entire process detail fault-injection driver fault-injection driver driver pseudodevice driver appears typical block device file system linux kernel real disk file system interest mounted top pseudo-device driver simply interposes requests real underlying disk driver passes traffic disk efficiently tracks request response storing small record fixed-sized circular buffer driver main roles system classify block written disk based type specific file-system data strucxfsext reiserfs jfs linux vfs layer idescsi log system log workload fault injection driver ioctl coordinator block match fault pass request disk inject fault block match model pass error file system receive file 
system read write requests save fault specification specification classify block types report error yesno yesno coordinator figure benchmarking framework algorithm flow figure shows benchmarking framework measure fault tolerance journaling file systems failures main components figure user level process issues fault fault-injection driver classifies blocks injects faults figure shows simplified flowchart benchmarking algorithm implemented fault-injection driver ture write represents interpret contents journal infer type journal block descriptor commit block interpret journal descriptor block data blocks journaled interpret semantics online block failures injected correctly explain complexity overhead classification sections driver model journaling file system specifically write failures model represents correct sequence states transaction committing disk read failures sufficient correctly interpret file system block types inserting failures specific block types transactional states observe file system handles types faults judge correctly handles faults injected driver inject faults system layer injects block faults reads writes block byzantine generals action implementing fail-stop processors fred schneider cornell fail-stop processor halts performing erroneous state transformation visible processors detect fail-stop processor halted due failure predefined portion storage remain unaffected failures accessible fail-stop processor fail-stop processors simplify construction fault-tolerant computing systems paper problem approximating fail-stop processors discussed fail-stop processors compared state machine approach general paradigm constructing fault-tolerant systems categories subject descriptors control structures microprogramming control structure reliability testing design memory structures reliability testing design computer-communication networks distributed systems computer systems organization performance systems--reliability availability serviceability operating systems reliability-checkpoint restart fault-tolerance general terms reliability additional key words phrases byzantine generals fail-stop fail-fast introduction designing programming fault-tolerant computing system difficult task due failure processor exhibit arbitrary behavior resulting erroneous outputs destruction critical state information multiple processors malfunctioning processor problems causing erroneous state information visible processors disastrous consequences processors actions based information processors account property avoids difficulties halt failure property processor halt performing erroneous state transformation visible processors processors halt response failures sufficient implementing systems correctness criteria involve generating outputs work supported part nsf grant mcsauthor address department computer science cornell ithaca permission copy fee part material granted provided copies made distributed direct commercial advantage acm copyright notice title date notice copying permission association computing machinery copy republish requires fee specific permission acm acm transactions computer systems vol pages fred schneider timely manner tasks run halted malfunctioning processor continued real-time constraints met means processors satisfy property failure status property processor detect processor failed halted permits processors assume tasks failed processor obvious limitations strategy--there sufficient processing capacity smaller system continue performing tasks timely manner finally order continue task running failed processor state task processor continue accomplished stable storage--storage unaffected failure accessible processor require processors satisfy property stable storage property storage processor partitioned stable storage volatile storage contents stable storage unaffected failure read processor contents volatile storage accessible processors lost result failure fail-stop processor processor satisfies halt failure property failure status property stable storage property construct fault-tolerant computing system tolerate failures application requiring processors assuming failures fail-stop processors employed fail-stop processor system halts fail-stop processors detect partition work reading stable storage fail-stop processors simplify completely solve problem building fault-tolerant computing systems problem simplified unnecessary cope arbitrary behavior corrupted state information design programs make infrequent stable storage expensive slow saving state information task continued accessing stable storage strongest argument investigating implementation failstop processors protocols implementing fault-tolerant systems assume models processors fail-stop processors equivalent models failure status property timeouts detect failures timeouts requires assumption processor clocks synchronized processors agree halted disastrous consequences processor models stable storage property assumed state information replicated processors turns approximation stable storage property work involve fail-stop stronger assumptions processor failures acm transactions computer systems vol byzantine generals action implementing fail-stop processors real processors satisfy halt failure failure status stable storage properties fact real processors good approximations fail-stop processors disappointing light number protocols written assume processors fail-stop paper develop implementation fail-stop processor approximation serves purposes feel cost complexity implementing fail-stop processors comparison protocols assume fail-stop processors protocols make weaker assumptions byzantine agreement protocols fail-stop processor approximation step practical realization fail-stop processors content approximation fail-stop processor impossible implement completely fault-tolerant computing system finite amount hardware finite amount hardware finite number failures disable error detection facilities behavior violates properties define fail-stop processor approximation k-fail-stop processor--a collection processors memories behaves fail-stop processor failures occur components approaches infinity k-fail-stop processor closer ideal approximates proceed section design correctness argument k-fail-stop processor section concerns techniques combine collection k-fail-stop processors fault-tolerant computing system section fail-stop processor approach contrasted state machine approach general technique constructing fault-tolerant computing systems finally section discussion ways approximate fail-stop processors considers open problems approximating fail-stop processors k-fail-stop processor fsp implemented collection real processors storage interconnected communications network failures result fail-stop processor reading results erroneous state transformation detected voting effects failures masked implementation consists p-processes program running processor fsp set processes s-processes storage corruption reads emulate block fault simply return error code issue operation underlying disk emulate corruption change bits block running processor fsp szk set processes question allocating processors processes discussed section program running fsp run p-processes fsp failures fsp halt detected comparing results p-process fsp writes stable storage fsp reading stable storage effects failure made visible p-processes run processors fail independently provided fewer failures occur processors running p-processes failure fsp halted occurs disagreement write requests made p-processes disagreement detected s-processes acm transactions computer systelns vol fred schneider copy contents stable storage fsp stored s-processes fsp s-processes running processor failures processors majority access correct values presupposes correctly functioning s-process updates state write performed stable storage protocol p-process access stable storage sending messages s-processes messages information time time request made local clock processor running requesting p-process rectime time request received local clock processor running s-process receiving request type depending request read write var variable stable storage written type write variable stable storage read type read val written type write make assumption communications network network reliability assumption messages delivered uncorrupted process orig originating message authenticated receiver theory satisfying assumption requires independent direct communication links p-process s-process independent channels majority message-this correct provided fewer failures occur direct channels authentication message origin practice packet-switching network made approximate network reliability assumption checksums message retransmission ensure messages delivered uncorrupted high probability digital signatures implement authentication high probability s-process k-fail-stop processor fspi system k-failstop processors fsp fsp fspn executes program figure choose stores arbitrary element clock evaluates current time processor local clock stable copy stable storage maintained s-process addition require p-process makes request stable storage fspi disseminates request satisfies ifpj nonfaulty nonfaulty s-process fspi receives request seconds measured clock s-processes k-fail-stop processor nonfaulty agree request picondition ensures s-processes receive message bounded length time request made nonfaulty p-process condition ensures s-processes agree request p-process making request faulty faulty p-process make requests s-processes copies stable storage acm transactions computer systems vol byzantine generals action implementing fail-stop processors owner failed false true --major loop fors ton clock bag requests delivered orig fsp type read type write mint minimum time minrect minimum rectime time mint minrect --m bag requests time mint d-m type read --choose send stable var orig distinct type write orig orig owner failed --choose stable var val owner failed --failed true forall fspi send 
halt skip skip rof fig program s-process fspi maintained s-processes inconsistent s-process performed update didn finally require k-fail-stop processor fsp clocks processors running p-processes fsp synchronized ensures request made nonfaulty p-process fsp time clock processes fsp running program request made nonfaulty p-process time local clock number protocols establishing --called interactive consistency byzantine agreement--have developed protocols based message delivery time maximum difference clock speeds correctly functioning processors running s-processes processors required handle faults messages authenticated implementation k-fail-stop processor acm transactions computer systems vol fred schneider tolerate failures involves processors running s-processes achieved protocol achieving clock synchronization required protocol requires processors handle faults messages authenticated single k-fail-stop processor requires processors running p-processes synchronized clocks achieved stable storage property show stable storage property holds implementation show things majority copies stable storage correct identical long fewer failures occur nonfaulty fail-stop processor write stable storage fail-stop processor read stable storage fail-stop processor fsp including fsp halted response failure proof implementation satisfies part stable storage property p-processes run program nonfaulty p-processes make requests stable storage clocks nonfaulty p-processes synchronized nonfaulty p-processes make requests time local clocks nonfaulty s-process receives request time clock receive requests time clock request made time received s-process time added s-processes request time respective bags time request made time copied s-process clock s-process program coded contents nonfaulty s-process nonfaulty s-process execution s-process program figure completely determined contents nonfaulty s-process executes identically nonfaulty s-processes update copies stable storage s-processes nonfaulty majority s-processes update copies stable storage turn part stable storage property argued nonfaulty s-processes perform stable storage majority copies stable storage correct identical program figure clear write operation attempted failstop processor fspi performed s-process p-processes fspi request write operations requested fail-stop processors owner conjunct guard p-processes request operation failed makes erroneous state information--the written--visible processes failed arbitrary behavior permitted failures failed write performed nonfaulty s-processes acm transactions computer systems vol byzantine generals action implementing fail-stop processors table interface s-process p-process p-process fspi write stable storage fspi initiate byzantine agreement write request s-processes fspi p-process fspi read stable storage fsp broadcast read request s-processes fsp received s-processes p-process fspi determine fsp halted due failure read variable failed stable storage fsp finally part suffices note read operation attempted fspi result identical responses nonfaulty s-processes p-process fspi fewer failures occur correct values total received taking majority responses p-process obtain correct variable read halt failure property detect failure major loop iteration suffices s-process check write requests received spurious writes means effects failure made visible process write request p-processes received requests identical p-processes make fsp malfunctioning case p-processes faulty concern definition k-fail-stop processor display arbitrary behavior circumstances write requests p-processes fsp received p-processes failstop processor halt message stable storage variable ailed set true correctly functioning p-processes halt receiving halt message s-processes ailed true values variables nonfaulty s-processes don change conjunct ailed guards assignment statement failure status property failure status property implemented variable failed process obtain failed time reading stable storage fsp determine fspi halted due failure reading failed fspi stable storage completes implementation k-fail-stop processor approximation interface s-processes p-processes summarized table acm transactions computer systems vol fred schneider assigning processes processors application requires fail-stop processors meet responsetime constraints failures occur implementation tolerate failures independent k-fail-stop processors required independent fail-stop processors ensures single failure fail-stop processor halt provided fewer failures occur fail-stop processors run application naive implementation computing system processors--k processors p-processes processors s-processes k-fail-stop processor resulting total processors recall programs fail-stop processors structured make minimal stable storage wasteful dedicate entire processor running s-process single k-fail-stop processor suppose single processor run s-processes delaying p-processes interact s-processes require processors run s-processes processors p-processes decrease number processors required naive implementation k-failstop processors independent--s-processes fail-stop processors share processors fortunately problem s-processes replicated -fold prepared tolerate failures processors running s-processes k-fail-stop processors failures occur processors s-processes running nonfaulty processors k-fail-stop processors majority s-processes running nonfaulty processors fail-stop processor halts nonfaulty processors running p-processes--up processors--halt processors fact faulty order recover nonfaulty processors fail-stop processor failure scheme processor recycling scheme processors partitioned groups active unavailable processors initially assigned group fail-stop processors configured processors removed group active group fail-stop processor halts processors running p-processes assigned unavailable group processors unavailable group run diagnostics processor passes diagnotics reassigned group processor recycling scheme reduces cost failure failure loss processors running p-processes fail-stop processor failure detected processor recycling scheme processors unable pass diagnostic tests remain unavailable reconfigured fail-stop processors approaches fault-tolerance implementation k-fail-stop processor application state machine approach general approach constructing distributed programs acm transactions computer systems vol byzantine generals action implementing fail-stop processors extended environments failures occur program distributed version tolerate failures returning data cases inject random noise cases block similar expected corrupted fields software layer models transient sticky faults injecting failures file system emulate faults caused layers storage subsystem unlike approaches emulate faulty disks additional hardware imitate faults introduced buggy device drivers controllers drawback approach discern lower layers handle disk faults scsi drivers retry commands failure characterizing file systems react faults correct layer fault injection coordinator coordinator monitors entire benchmarking process inserts fault-injection driver linux kernel coordinator constructs file system passes fault specification driver spawns child process run workload errors driver coordinator communicate linux ioctl interface exchange information fault specification disk traffic observed driver errors manifest numerous locales log errors coordinator collate specifically child process receive errors file system driver observe errors sequence state transitions coordinator system logs errors reported file system reflected calling child process fault injection experiment proceeds file system tested freshly created mounted journaling modes write failure analysis journaling mode information passed driver selects analytical model internally journaling mode files directories needed testing created clean file system depending type block fail coordinator constructs fault specification attributes passes fault-injection driver coordinator runs controlled workload creating file directory child process generate block write failed write failure analysis based journaling mode blocks written file system driver moves internally state journaling model expected block read written file system driver injects fault failing corrupting block driver records file system violate journaling model fault injected coordinator collects error logs child process system log driver failure policy inference running workload injecting fault final step determine file system behaved determine fault affected file system compare results running fault perform comparison observable outputs system error codes data returned file system api contents system log low-level traces recorded fault-injection layer human-intensive part process requires manual inspection visible outputs cases identify anomaly failure policy check source code verify specific conclusions complexity feasible infer failure policy code inspection collect large volumes results terms traces error logs fault injection experiment run due sheer volume experimental data difficult present results reader inspection represent file system failure policies unique representation called failure policy graphs similar shown figure figure plot workloads x-axis file system data structures y-axis applicable row column entry presents iron detection recovery technique file system applicable workload generate block type traffic gray box symbols superimposed multiple mechanisms employed file system explain entries figure interpreted walking specifically entry workload data structure symbols superimposed retry constructed running program processors connected communications rretry error propagation rpropagate finally file system stop rstop means fails workload file system retries fails stops propagates error application failure policy graphs present analysis results thesis represent failure policies versions iron file systems build workloads data structures figure failure policy graph figure presents sample failure policy graph gray box workload applicable block type multiple mechanisms observed symbols superimposed key recovery rzero rretry rpropagate rredundancy rstop data journaling model ordered journaling model writeback journaling model figure journaling models figure shows models verifying journaling modes model built based regular expression state represents state reached write failure added models represents journal writes represents data writes represents journal commit writes represents checkpoint writes represents journal super block writes represents write failure journaling models describe model journaling file systems write failures explained section journaling modes journaling modes differs type data journals order writes blocks build model journaling modes based functionality models represent journaling modes type data accept order data written model ordered journaling mode specifies ordered data written metadata committed log build models construct regular expression journaling mode regular expressions represent journaling modes concisely easy construct understand build model based regular expression figure shows models journaling mode journaling models consist states states represent state on-disk file system on-disk file system moves state based type write receives file system track state change moving correspondingly model explain briefly regular expression journaling mode represent journal writes represent data writes represent journal commit writes represent journal super block writes represent checkpoint data writes represent write failures data journaling data journaling expressed regular expression data journaling mode file system writes journaled represented ordered unordered writes writing journal blocks commit block represented written file system mark end transaction file system write transactions log transactions committed file system write checkpoint blocks represented fixed locations journal super block represented mark head tail journal convert regular expression state diagram shown figure add failure state ordered journaling ordered journaling expressed regular expression ordered mode data written metadata blocks committed journal note data blocks issued parallel journal writes writes complete commit block written commit block written transaction transactions similar data journaling file system write checkpoint blocks journal super block network message origins authenticated byzantine agreement ensure instance program sees transactions regular expression converted state diagram failure state added shown figure writeback journaling writeback journaling regular expression writeback journaling mode data written time file system written journal writes journal writes complete commit block written transaction committed file system write journal super block checkpoint blocks unordered writes writeback journaling model figure obtained taking regular expression adding state code detailed analysis preliminary analysis ext reiserfs jfs xfs ntfs sba generic sba specific sba total table code size fault injection drivers number statements counted number semicolons needed perform failure policy analysis ext reiserfs jfs preliminary analysis xfs ntfs 
xfs ntfs analysis preliminary run workloads fail data structures file systems complexity fault injection driver fault injection driver customized file system test concern amount information embedded driver file system focus understanding block types drivers embedded information interpret placement contents journal metadata data blocks analyze complexity driver linux journaling file systems ext reiserfs jfs xfs windows ntfs journaling file systems journal transactions temporarily recorded fixed-location data structures data permanently reside sba driver distinguishes traffic journal fixedlocation data structures traffic simple distinguish reiserfs jfs xfs ntfs journal set contiguous blocks separate rest file system backward compatible ext ext treat journal regular file determine blocks belong journal fault injection driver knowledge inodes indirect blocks journal change location created classification remains efficient run-time driver classify types journal blocks descriptor block journal data block commit block perform analysis journaling file systems driver understand in-core data structures file systems driver policies parameters file system table reports number statements required implement fault injection driver numbers show code driver statements linux statements windows general infrastructure approximately statements needed support journaling file systems overhead fault injection section describe overheads classifying file system traffic block types incurred driver processing memory overheads due driver minimal request operations performed driver block number comparison find file system block types block numbers identify block belongs journal fixed-location journaling file systems typically allocate journal contiguous set blocks disk order improve sequential write performance simple boundary check classify journal traffic fixed-location traffic journal created file ext driver compares block number block numbers journal file distinguish journal traffic comparison performed efficiently hash tables driver block number comparison give information statically assigned block types ext inode block locations statically assigned file system creation comparing block number static inode table disk driver find request issued inode block verifying contents block understand file system block types ext driver checks magic number journal blocks distinguish journal metadata journal data driver contents identify block types directories locations dynamically allocated run time file system fault injection driver stores small trace records details read write block number block type time issue completion fault injected block trace record takes bytes stored internal circular buffer fault injection experiments coordinator collects block-level trace analysis run fault injection experiments small file system size note small file system exercise data structures triple indirect block ext created large file sizes creating file lots holes actual blocks recreating file system testing time fault injection experiment takes seconds system crash induced analyze file system figure fault injection figure shows sequence steps fault-injection driver track transaction writes ordered journaling mode fail commit block write recovery properties fault injection minutes system rebooted system crash caused experiments add overhead analysis putting fault injection conclude methodology section fault injected journaling model figure shows sequence steps fault-injection driver track file system writes inject fault failing commit block write transaction ordered journaling mode step figure captures transition state initially transaction starts set ordered data writes figure data writes journal blocks logged figure commit block written data journal writes complete failed figure file system oblivious commit block failure continue checkpoint journaled blocks figure file system recognize failure steps prevent file system corruption moving state figure state file system abort failed transaction bad block remapping remount read-only crash system sufficient block types inject faults file system requests model fault-injection driver determine requests write failure belong failed transaction transactions file system keeping track writes journaling model fault-injection driver explain block write failure leads file system errors summary developed three-step fingerprinting methodology determine file system failure policy approach strikes good balance straightforward run exercises file system test workload suite roughly programs file system order block types block failed read write data corrupted file system amounts roughly relevant tests failure policy analysis results present results failure policy analysis commodity file systems ext reiserfs version ibm jfs xfs linux ntfs windows file system discuss general failure policy uncovered bugs illogical inconsistencies source code explain problems discover file system studied depth present failure policy graphs results showing workload block type pair detection recovery technique figures present complex graphical depiction results totally graphs figure detection recovery policies graphs graph plotted read failures write failures block corruption results columns collapse multiple workloads single entry disk traffic failure handling file system workloads provide qualitative summary results presented figure linux ext key detection key recovery dzero rzero derrorcode rretry dsanity rpropagate rredundancy rstop table keys detection recovery table presents keys represent detection recovery policies file systems detection detect read failures ext primarily error codes derrorcode write fails ext record error code dzero write errors potentially leading file system problems checkpointing transaction final location ext performs fair amount sanity checking dsanity ext explicitly performs type checks blocks superblock journal blocks type checking important blocks directories bitmap blocks indirect blocks ext performs numerous sanity checks file-size field inode overly-large open detects reports error recovery detected errors ext propagates error user rpropagate read failures ext aborts journal rstop aborting journal leads read-only remount file system preventing future updates explicit administrator interaction ext retry rretry sparingly prefetch read fails ext retries originally requested block detection recovery read failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode write failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode corruption j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure ext failure policies failure policy graphs plot detection recovery policies ext read inputs majority voting determine output computation application requires processors run meet realtime constraints state machine approach directly total processors required implement system tolerate faults additional k-fault-tolerant processor costs real processors contrast cost fail-stop processor approach s-processes share single processor total real processors required additional fail-stop processor costs approximately processors cases achieve degree fault-tolerance fail-stop processor approach requires fewer processors state machine approach direct state machine approach application advantages fail-stop processor approach state machine approach divide program state volatile stable storage develop recovery protocols reconstruct state program based contents stable storage fail-stop processor approach additional response time incurred task moved fail-stop processor delays incurred state machine approach failures masked fail-stop processor approach applications tight timing constraints fail-stop processor approach expensive byzantine agreement performed access stable storage state machine approach byzantine agreement performed input read reading input infrequent event state machine approach expend resources executing byzantine agreement protocols discussion k-fail-stop processor approximation based construction reliable kernel s-processes supports stable storage detects failures kernel reliable replicated -fold effects failures masked applications run k-fail-stop processor approximation replicated -fold cheaper sufficient detect errors mask approximate fail-stop processors paper expensive approach undoubtedly ways approximate fail-stop processors disks considered acceptable approximations stable storage triple-redundant bus authentication processors required acm transactions computer systems vol fred schneider approximate disseminating requests disks voter detect failures processors running p-processes tandem system reported employ fail-stop called fail-fast modules implemented directly hardware approximations based engineering 
data components fail approximation made assumption nature failures hand approximation expensive--perhaps expensive demanding applications suggests worthwhile pursue investigations cheaply implement fail-stop processor approximations assumptions failure modes acknowledgments lynch lundelius provided helpful comments earlier drafts paper grateful birman gries lamport schlichting skeen discussions comments material finally grateful alpern babaoglu discovered subtle bugs previous versions fail-stop implementation paper dolev byzantine generals strike algorithms fischer lynch lower bound time assure interactive consistency inf process lett halpern simons strong efficient fault-tolerant algorithm clock synchronization ibm research rep ibm san jose calif nov katzman fault-tolerant computing system proceedings hawaii international conference system sciences lamport time clocks ordering events distributed system commun acm july lamport implementation reliable distributed multiprocess systems comput networks lamport time timeout fault-tolerant distributed systems computer science laboratory sri international menlo park california june acm trans program lang syst april lamport shostak pease byzantine generals problem acm trans program lang syst july lampson atomic transactions distributed systems--architecture implementation lecture notes computer science vol springer-verlag york lynch fischer fowler simple efficient byzantine generals algorithm tech rep git-icsschool information computer science georgia institute technology atlanta georgia feb pease shostak lamport reaching agreement presence faults acm apr schlichting schneider fail-stop processors approach designing fault-tolerant computing systems acm trans comput syst aug schneider synchronization distributed programs acm trans program lang syst apr received december revised august accepted november acm transactions computer systems vol 
write corruption faults injected block type range workloads workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery log write operations gray box workload applicable block type multiple mechanisms observed symbols superimposed keys detection recovery presented table bugs inconsistencies found number bugs inconsistencies ext failure policy errors propagated user truncate rmdir fail silently ext perform sanity checking unlink check linkscountfield modifying corrupted lead system crash ext redundant copies superblock rredundancy copies updated file system creation finally important cases ext violates journaling semantics committing checkpointing invalid transactions discuss cases committing failed transactions write transaction fails ext continues write transaction log commits affect file system integrity ordered data write fails 
ordered journaling mode expect file system abort transaction commits transaction metadata blocks end pointing wrong data contents disk problem occurs ext failure ordered write data corruption checkpointing failed transactions write transaction fails file system checkpoint blocks journaled part transaction checkpointing crash occurs file system recover failed transaction result corrupted file system ext commits transaction transaction write fails committing failed transaction ext checkpoints blocks journaled transaction depending journaling mode checkpointing partial complete partial checkpointing cases ext checkpoints blocks failed transaction data journaling mode journal descriptor block journal commit block write fails cases checkpointing file system metadata blocks transaction checkpointed data blocks checkpointed data journaling mode file created data blocks transaction descriptor block fails metadata blocks file inode data bitmap inode bitmap directory data directory inode blocks written fixed locations data blocks file journaled data journaling mode written data blocks written fixed locations metadata blocks file end pointing incorrect contents disk complete checkpointing ordered writeback journaling mode file system metadata blocks journaled data blocks written log modes ext checkpoints journaled blocks failed transaction describe generic case file system corruption transactions committed block journaled blocks journaled assume transaction fails file system continues checkpoint blocks failed transaction crash occurs writing blocks fixed locations file system log recovery runs mount recovery transaction recovered failed transaction recovered contents block overwritten contents recovery file system inconsistent state block transaction block transaction explained problem occur ext write journal metadata block descriptor block revoke block commit block fails lead file system corruptions resulting loss files inaccessible directories replaying failed checkpoint writes checkpointing process writing journaled blocks log fixed locations checkpoint write fails file system attempt write mark journal checkpoint write happen log replay ext replay failed checkpoint writes data corruption data loss loss files directories replaying transactions journaling file systems maintain state variable mark log dirty clean file system mounted log dirty transactions log replayed fixed locations journaling file systems update state variable starting transaction checkpointing transaction write update state variable fails things possibly happen file system replay transaction replayed fail replay transaction recovery replaying transaction integrity problems possibility replaying journal contents lead corruption loss data ext maintains journal state journal super block ext clears field writes journal super block clean journal mark journal dirty journal super block written non-zero field journal super block write fails ext attempt write save super block locations journal super block failure ext continues commit transactions log journal super block written mark journal dirty failed journal appears clean mount transaction replay due previous crash ext fails replay result lost files directories replaying failed transactions journal data block write fails transaction aborted replayed transaction replayed journal data blocks invalid contents read written fixed location handled properly lead file system errors earlier ext abort failed transactions continues commit log recovery write invalid contents file system fixed location blocks corrupt important file system metadata result unmountable file system reiserfs detection analysis reveals reiserfs pays close attention error codes reads writes derrorcode reiserfs performs great deal internal sanity checking dsanity internal leaf nodes balanced tree block header information level block tree number items free space super block journal metadata blocks magic numbers identify valid journal descriptor commit blocks additional information finally inodes directory blocks formats reiserfs checks blocks expected values fields blocks checked carefully bitmaps data blocks type information type-checked recovery prominent aspect recovery policy reiserfs tendency panic system detection virtually write failure rstop reiserfs calls panic file system crashes leading reboot recovery sequence reiserfs attempts ensure on-disk structures corrupted reiserfs recovers read write failures differently read failures reiserfs propagates error user rpropagate performs single retry rretry data block read fails indirect block read fails unlink truncate write operations reiserfs retries write failure detection recovery read failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode write failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode corruption j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure reiserfs failure policy failure policy graphs plot detection recovery policies reiserfs read write corruption faults injected block type workloads workloads varied columns figure block types reiserfs file system varied rows workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery log write operations gray box workload applicable block type multiple mechanisms symbols superimposed keys detection recovery presented table bugs inconsistencies reiserfs exhibits inconsistencies bugs dealing indirect blocks reiserfs detects ignores read failure truncate unlink updates bitmaps super block incorrectly leaking space reiserfs calls panic failing sanity check simply returning error code sanity type checking detect corrupt journal data replaying corrupted journal block make file system unusable block written super block finally reiserfs violates journaling semantics write failures explain details committing checkpointing replaying failed transactions write failures reiserfs crash continues commit checkpoint failed transaction ordered journaling mode ordered data block write fails reiserfs journals transaction commits handling write error result corrupted data blocks failed transactions metadata blocks file system end pointing invalid data contents reiserfs uniform failure handling policy crashes write failures file system corruption prevented reiserfs crashing system ordered write failures reiserfs replays failed transaction ordered data write errors ibm jfs detection error codes derrorcode detect read failures ext write errors dzero exception journal superblock writes jfs employs minimal type checking superblock journal superblock magic version numbers checked sanity checks dsanity block types internal tree blocks directory blocks inode blocks number entries pointers block jfs checks make number maximum block type equality check field performed block allocation maps verify block corrupted recovery recovery strategies jfs vary dramatically depending block type error occurs journal superblock write jfs crashes detection recovery read failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode write failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode corruption j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure jfs failure policy tables plot detection recovery policies jfs read write corruption faults workloads varied columns block types varied rows workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery 
log write operations gray box workload applicable block type multiple mechanisms symbols superimposed keys detection recovery presented table system rstop write errors rzero block read failure primary superblock jfs accesses alternate copy rredundancy complete mount operation corrupt primary results mount failure rstop explicit crashes rstop block allocation map inode allocation map read fails error codes metadata reads handled generic file system code called jfs generic code attempts recover read errors retrying read single time rretry finally reaction failed sanity check propagate error rpropagate remount file system read-only rstop journal replay sanity-check failure replay abort rstop bugs inconsistencies found problems jfs failure policy jfs built-in redundancy expect jfs secondary copies aggregate inode tables special inodes describe file system error code returned aggregate inode read blank page returned user rguess design bug occurs read internal tree block pass sanity check bugs limit utility jfs recovery generic code detects read errors retries bug jfs implementation leads ignoring error corrupting file system finally present cases journaling semantics violations write failures jfs replaying failed checkpoint writes checkpoint write fails jfs rewrite mark transaction replay jfs simply ignores error lead corrupted file system behavior similar ext file systems record failed checkpoint writes identifying transactions replayed committing checkpointing replaying failed transactions find journaling file systems commit checkpoint transaction ordered block write failure jfs notify application ordered write failure commits transaction lead data corruption failing recover journal block write fails jfs abort failed transaction commits crash journal write failure logredo routine jfs fails unrecognized log record type lead unmountable file system xfs perform preliminary analysis failure policy write failures xfs fail writes block types xfs ordered data journal blocks metadata commit records journal blocks unmount records checkpoint blocks detection xfs detects write failures error code derrorcode including asynchronous checkpoint write errors ext jfs xfs handle write failures correctly find flaw found ext reiserfs jfs ordered data block write fails xfs continues log failed transaction journal resulting data corruption recovery xfs takes recovery techniques block failures retries checkpoint write asynchronous failures repeatedly bounds rretry block error transient repeated retry successfully writes block permanent errors failed block write repeated infinitely unmounts system rectify write error xfs crashes rstop journal write failures xfs reacts immediately stopping file system rstop windows ntfs ntfs non-unix file system study analysis requires detailed knowledge on-disk structures complete analysis figures find ntfs error codes derrorcode detect block read write failures similar ext jfs data write fails ntfs records error code dzero corrupt file system ntfs performs strong sanity checking dsanity metadata blocks file system unmountable metadata blocks journal corrupted ntfs surprisingly perform sanity checking corrupted block pointer point important system structures corrupt block pointed updated cases ntfs propagates errors rpropagate ntfs aggressively retry rretry operations fail times read failures level ext reiserfs jfs dzero derrorcode dsanity dredundancy rzero rpropagate rstop rguess rretry rrepair rremap rredundancy table iron techniques summary table depicts summary iron techniques file systems test check marks higher relative frequency usage technique ext reiserfs ibm jfs xfs committing failed transactions checkpointing failed transactions replaying failed checkpoint writes replaying transactions replaying failed transactions crashing file system table file system bugs table summary bugs identified ext reiserfs ibm jfs xfs writes number retries varies times data blocks times mft blocks file system summary present qualitative summary file systems tested table presents summary techniques file system employs excluding xfs ntfs table lists bugs found file system analysis ext simplicity ext implements simple reliable failure policy matching design philosophy found ext family file systems checks error codes modest level sanity checking recovers propagating errors aborting operations main problem ext failure handling write errors problems including file system corruption reiserfs harm reiserfs concerned disk failure concern evident write failures induce panic reiserfs takes action ensure file system corrupted reiserfs great deal sanity type checking behaviors combine form hippocratic failure policy harm jfs kitchen sink jfs consistent diverse failure detection recovery techniques detection jfs sanity checks error codes recovery jfs redundancy crashes system retries operations depending block type fails error detection api called xfs simple well-defined preliminary analysis find xfs simple well-defined failure policy handle write failures checks error codes synchronous write failures xfs stops file system propagates errors asynchronous write failures failed write retried persistently ntfs persistence virtue compared linux file systems ntfs persistent retrying failed requests times giving propagate errors user reliably testing ntfs needed order broaden conclusions part ongoing work technique summary finally present broad analysis techniques applied file systems detect recover disk failures concentrate techniques underused overused inappropriate manner detection recovery illogical inconsistency common found high degree illogical inconsistency failure policy file systems observable patterns figures reiserfs performs great deal sanity checking important case journal replay result single corrupted block journal corrupt entire file system jfs illogically inconsistent employing techniques scenarios similar note inconsistency problematic logically inconsistent good idea file system provide higher level redundancy data structures deems important root directory criticizing inconsistencies undesirable unintentional jfs attempt read alternate superblock read failure occurs reading primary superblock attempt read alternate deems primary corrupted estimation root illogical inconsistency failure policy diffusion code implements failure policy spread kernel diffusion encouraged architectural features modern file systems split generic specific file systems observed cases developers implement portions code implement failure policies cases reiserfs panic write failure arises due inconsistency indicative lack attention paid failure policy pay attention problem design centralized failure handler file systems discuss chapter detection recovery bugs common found numerous bugs file systems tested found sophisticated techniques generally indicative difficulty implementing correct failure policy hints effort put testing debugging code suggestion literature helpful periodically inject faults normal operation part fire drill method reveals testing broad cover code paths testing indirect-block handling reiserfs observe classes fault mishandling detection error codes amazingly error codes file system common jfs found occasionally file systems testing framework part file system developer toolkit tools class error easily discovered detection sanity checking limited utility file systems sanity checking ensure metadata meets expectations code modern disk failure modes misdirected phantom writes lead cases file system receive properly formatted incorrect block bad block passes sanity checks corrupt file 
system file systems tested exhibit behavior stronger tests checksums recovery stop correctly file systems employed form rstop order limit damage file system types errors arose reiserfs calls panic virtually write error prevent corruption structures careful techniques write failure ext abort transaction correctly squelch writes file system leading corruption fine-grained rebooting difficult apply practice recovery stop overused downside halting file system activity reaction failure inconvenience recovery takes time requires administrative involvement fix file systems form rstop innocuous read failure occurred simply returning error requesting process entire system stops draconian reactions possibly temporary failures avoided recovery retry underutilized file systems assume failures transient lower layers system handle failures retry requests time systems employ retry generally assume read retry write retry transient faults due device drivers transport issues equally occur reads writes retry applied uniformly ntfs lone file system embraces retry issue higher number requests block failure observed recovery automatic repair rare automatic repair rarely file systems rstop technique file systems require manual intervention attempt fix observed problem running fsck detection recovery redundancy finally importantly virtually file systems include machinery detect disk failures apply redundancy enable recovery failures lone exception minimal amount superblock redundancy found jfs redundancy inconsistently jfs places copies close proximity making vulnerable spatially-local errors explored potentially handling failures common drives today investigate inclusion forms redundancy failure policy file system conclusion commodity file systems variety detection recovery techniques handle disk sector errors chapter explain process semantic failure analysis apply knowledge file system block types transactional semantics unearth failure policies analysis find commodity file systems built assumption disk fails failstop manner store redundant information disk find failure policies enacted file system hoc inconsistent buggy conclude time rearchitect file systems redundant information handle partial disk errors framework support well-defined failure policies chapter building low-level redundancy machinery mention introduction section challenges building robust file system design low-level redundancy machinery detect recover partial disk errors failure handling framework unify low-level machineries partial disk errors failure policies chapter focus problem design implement redundancy techniques single disk drive problem addressed chapter note challenge building cost effective low-level machinery important context commodity file systems store redundant information disk well-known technique building reliable systems redundant components specialized hardware mechanisms improve reliability performance degrading commodity systems desktops laptops afford redundant components extra disk drives special hardware support nvram high-end systems result increased system cost system size heat dissipation specifically focus storing redundant information single disk drive additional hardware support redundant information stored ways technique merits demerits explore types redundant information thesis checksums parity replicas due wide spread high-end robust file storage systems describe qualitatively evaluate robustness performance space overheads redundancy options file systems section describe implementation evaluation robust file system iron ext broadly explore applications replication parity checksums file system data metadata section iron ext evaluate redundancy mechanisms separately combinations find incur minimal time space overhead tolerable desktop settings evaluation shows parity cost-effective terms space performance overhead study parity detail sections order evaluate impact locality failures parity-based redundancy technique develop probabilistic model characterizes spatial correlation disk errors section finally design parity update techniques update redundant information nvram support section evaluation shows data parity layout parity update techniques greatly improve robustness file system incurring modest performance overhead in-disk redundancy file storage system store redundant copies data disk detection recovery purposes data block replicated copies read detect block corruption addition replicas redundant information include processed copies data checksums parity involved error correction codes thesis study detail types redundant information checksums parity replicas philosophy building robust systems end-point detect recover failures irrespective intermediate layers handle errors approach instance end-to-end argument principle design implement redundant information file system layer block device layer software raid focus journaling file systems integrate techniques transactional semantics order provide consistency primary secondary copies important issues incorporating redundancy file systems effectiveness redundancy technique handling disk sector errors performance space overheads discuss issues detail effectiveness effectiveness redundancy techniques vary parity block computed set blocks recover failure blocks set replicas recover errors similarly cryptographic checksum shaon actual data fewer collisions corrupted data simple checksums crc factor affects effectiveness location redundant information respect primary copy sectors disk subject spatially local errors specifically block layout schemes treat block errors independent events place redundant information block case checksums block represent scheme discussion subscripts represents distance data redundant information scheme takes account spatial locality disk errors separates tracks denote achieve greater reliability redundant copy stored farther primary copy subject correlated failures redundant copy detection recovery updated carefully respect primary copy primary secondary copies block updated atomically file system crash multiple data copies inconsistent redundant information recover latent sector faults occur journal recovery hardware support nvram updating redundant information consistently atomically lack low-end systems means techniques updating redundant data writing primary secondary copies journal part transaction updating fixed location copies achieve atomicity high performance overheads discussion denote techniques update data redundant information atomically subscript atomic techniques property represented non-atomic performance overheads adding redundant information decrease performance system reasons happen file systems pay cost updating redundant copies primary copy updated journaling file system metadata blocks including journal replicated metadata write incurs write costs writes primary secondary journal writes primary secondary fixed-location copies systems relax synchronous update redundant copies order mitigate performance overheads endanger reliability system performance drop due additional disk reads issued redundant information checksums order verify validity data file systems follow write ordering constraints updating primary secondary copies achieve atomicity turn additional rotational latencies disk seeks fourth portion host main memory hold redundant information checksums affect performance reduces amount memory system finally computationally expensive produce types redundant information reed-solomon tornado error correction codes processing significant amount cpu power system space overheads surprisingly redundant information incur space overheads varies redundancy technique stronger detecting corruptions cryptographic checksum shaoccupies bytes simple crc-type checksum similarly parity blocks replicas error correction codes occupy amount space depending copies maintained table presents qualitative evaluation effectiveness performance space overheads redundancy techniques redundancy techniques listed row effectiveness single block error spatially local errors spread disk tracks spatially local errors spread disk tracks listed columns performance space cost study technique recover failure normal file system operation journal 
recovery mark means technique guaranteed detect recover failure mark represents guarantee failure detection recovery explain table entries detail broadly checksums stored classes locations sector neighboring block block separated tracks checksum stored sector update guaranteed atomic due basic write guarantees offered disk drive separated data checksum update atomic checksumwithin atomic guaranteed detect phantom write misdirected write errors general space time costs tolerable checksums atomic techniques require extra writes journal increase performance overheads case parity calculated set contiguous blocks tolerate spatially correlated failures calculated blocks separated tracks parity update atomic logging parity blocks journal decrease performance significantly finally replicas incur high space performance overheads journal ensure atomic updates severe effectiveness overheads single block spatial tracks spatial tracks time space techniques normal recovery normal recovery normal recovery checksumwithin low low checksumd atomic low low checksumd atomic high low checksumd atomic low low checksumd atomic high low parityd atomic low low parityd atomic high low parityd atomic low high parityd atomic high high replicad atomic high high replicad atomic high high replicad atomic high high replicad atomic high high table qualitative evaluation redundancy techniques table presents qualitative evaluation effectiveness performance space overheads redundancy techniques detect recover sector errors corruption mark means technique guaranteed detect recover failure mark represents guarantee failure detection recovery performance penalties conclusion observe table effective redundancy technique handling errors higher performance space overhead sections implement redundancy techniques table evaluate robustness cost quantitatively ixt prototype iron file system describe implementation evaluation iron ext ixt design goal detect recover errors affordable time space cost ixt implement family recovery techniques commodity file systems provide give importance file system metadata protect replicas simple parity-based redundancy scheme data blocks recover single block error file single parity block file strive minimize performance space cost significant data blocks replicated increase robustness ixt applies checksums metadata data blocks design care handle spatially local failures separating replicas checksums disk blocks ixt backward compatible ext modify internal structures inode stores information file block pointers section describe implementation demonstrate robust broad class partial disk failures investigate time space costs ixt showing time costs small modest space costs reasonable performance measurements activate deactivate iron features independently understand cost approach implementation describe ixt implementation explain add checksumming metadata replication user parity performance enhancement transactional checksums existing ext file system framework checksumming implement checksumming ixt borrow techniques recent research checksumming file systems specifically compute checksums file system block treat checksums metadata placing journal checkpoint final location end file system checksums separated blocks distance blocks approximately tracks ensure atomicity metadata checksums checksumd atomic logging metadata checksums journal updating final fixed-location copies data checksum updates guaranteed atomic checksumd atomic incorporating checksumming existing transactional machinery ixt cleanly integrates ext framework checksums small cached read verification current implementation shato compute checksums implementing strong cryptographic checksum shawhich involves computational overhead simple checksum crc explore extent checksums impact performance file system metadata replication replicas protect file system metadata including ext journal treated metadata apply similar approach adding metadata replication ixt metadata blocks written separate replica log checkpointed fixed location block group distant original metadata replica log beginning file system blocks primary replica logs separated tracks transactions ensure copies reach disk consistently summary implement replicad atomic protect metadata blocks parity implement simple parity-based redundancy scheme data blocks parity block allocated file simple design enables recover data block failure file explore involved design sections modify inode structure ext associate file parity block data blocks specifically ext inode block pointers direct single indirect double indirect triple indirect block pointers direct pointer pointing parity block file parity blocks allocated files created file modified parity block read updated respect contents improve performance file creates preallocate parity blocks create file system assign files created current simple design preallocate blocks block group file system created follow default block allocation scheme ext ordered journaling mode system crashes data block write transaction commit journal recovery file system recompute parity make consistent modified data block contents ext ordered journaling mode additional information stored journal locations data blocks modified address problem denehy introduce journaling mode called declared journaling mode store block numbers data blocks modified part transaction commit journal support parity data blocks add declared mode ixt summary implement parityd non-atomic design parity blocks ixt transactional checksums explore idea leveraging checksums journaling file system specifically checksums relax ordering constraints improve performance updating journal standard ext ensures previous journal data reaches disk commit block enforce ordering standard ext induces extra wait writing commit block incurs extra rotational delay avoid wait ixt implements call transactional checksum checksum contents transaction placing checksum journal commit block ixt safely issue blocks transaction concurrently crash occurs commit recovery procedure reliably detect crash replay transaction checksum journal data match checksum commit block shafor transactional checksums note transactional checksum crash semantics original ext iron extensions cleaning overheads note cleaning overhead large problem pure log-structured file systems major performance issue journaling file systems ixt -style checksumming replication journaling file systems incorporate cleaning on-line maintenance costs ext writes metadata journal cleans journal checkpointing data final fixed location additional cleaning performed ixt increases total traffic small amount evaluation evaluate prototype implementation ixt focus major axes assessment robustness ixt checksums parity replicas modern disk failures time space overhead additional redundancy mechanisms employed ixt problem exists journaling file systems reiserfs jfs xfs ntfs detection recovery read failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode write failure j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode corruption j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure ixt failure policy tables plot detection recovery policies ixt read write corruption faults injected block type range workloads workloads varied columns figure block types ixt file system varied rows workloads grouped manner figure keys detection recovery presented table robustness test robustness ixt harness fault injection framework running partial-failure experiments ixt results shown figure ixt detects read failures ext error codes lower level derrorcode metadata block read fails ixt reads replica copy rredundancy replica read fails behaves ext propagating error rpropagate stopping file system activity rstop data block read fails parity block data blocks file read compute failed data block contents rredundancy ixt detects 
write failures error codes derrorcode aborts journal mounts file system read-only stop writes disk rstop data metadata block read checksum contents computed compared checksum block dredundancy checksums match read error generated rpropagate read errors contents failed block read replica computed parity block rredundancy process building ixt fixed numerous bugs ext avoided cases ext commit failed transactions disk potentially corrupt file system employing checksumming detect corruption replication parity recover lost blocks ixt robust file service spite partial disk failures quantitatively ixt detects recovers partial-error scenarios induced result logical welldefined failure policy time overhead assess performance overhead ixt isolate overhead iron mechanism enabling checksumming metadata data metadata replication parity user data transactional checksumming separately combinations standard file system benchmarks ssh-build unpacks compiles ssh source distribution web server benchmark responds set static http requests postmark emulates file system traffic server tpc-b runs series debit-credit transactions simple database run experiment times present average results ssh-build time measures time unpack configure build ssh source tree tar source size web server benchmark transfers data http requests postmark run transactions ssh web post tpcb baseline ext table overheads ixt variants results running variants ixt ssh-build ssh web server web postmark post tpc-b tpcb benchmarks presented file sizes ranging subdirectories files tpc-b run randomly generated debit-credit transactions benchmarks exhibit broad set behaviors specifically ssh-build good albeit simple model typical action developer administrator web server read intensive concurrency postmark metadata intensive file creations deletions tpc-b induces great deal synchronous update traffic file system table reports relative performance variants ixt workloads compared stock linux ext rows vary redundancy technique implemented combinations implies metadata checksumming enabled data checksumming enabled replication metadata turned parity data blocks enabled transactional checksums results normalized performance standard linux ext interested reader running times standard ext ssh-build web postmark tpc-b seconds slowdowns greater marked bold speedups relative base ext marked brackets testing linux kernel ghz intel memory western digital wdc bbdaa disk explain reasons performance overheads checksumming data metadata add high overhead workloads cases data checksums postmark increase performance cost due fact postmark benchmark writes data checksums calculated data block checksums considered metadata data checksums coupled metadata replication overhead postmark raises applied individually metadata replication reduces performance metadata intensive workloads postmark tpc-b causing overheads ssh-build web server benchmarks parity data blocks add cost individually combined metadata replication raised overhead tpc-b reason behavior due interaction declared mode metadata replication declared mode extra synchronous writes issued journal addition causing rotational latencies incur disk seeks primary secondary journal coupled transactional checksums overhead drops numbers draw main conclusions sshbuild web server workload time overhead iron techniques enabled ssh-build indicative typical activity checksumming replication parity incurs cost similarly web server benchmark conclude read-intensive workloads suffer addition iron techniques metadata intensive workloads postmark tpc-b overhead noticeable postmark tpc-b row workloads metadata intensive results represent worst-case performance expect observe implementation metadata replication row incurs substantial cost data checksumming row user parity metadata checksums contrast incur cost rows untuned implementation ixt results demonstrate worst case costs robustness prohibitive finally performance synchronous tpc-b workload demonstrates benefits transactional checksum base case technique improves standard ext performance row combination checksumming replication parity reduces overhead roughly row row additional robustness checksums applied improve performance journaling file systems files dir indir size checksum parity replicas count table space overhead table lists space overhead due redundant information home directories inode blocks allocated statically ext adds replica overhead file system checksums block constant space overhead file system space overhead evaluate space overhead measured local file systems home directories graduate students computer sciences department computed increase space required metadata replicated room checksums included extra block parity allocated table presents detailed list space overhead numbers row lists space cost file system file parity block allocated directory indirect blocks dynamic metadata blocks file systems inode blocks bitmap blocks statically allocated file system created checksums constant small space overhead found space overhead checksumming metadata replication small range found parity-block overhead user files bit substantial range depending volume analyzed summary investigate family redundancy techniques find ixt greatly increases robustness file system partial failures incurring modest time space overheads parity blocks strike balance terms robustness performance space overheads user data occupies significant part local file system ixt represents single point large space iron file systems sections explore designs focusing specifically parity blocks redundant parity blocks previous section studied in-breadth cost effectiveness redundant information cases design decisions simple single parity block file section explore in-depth issues building in-disk redundancy commodity hardware focusing primarily parity redundant information due simplicity minimal space overhead ease computation widespread file storage systems order explore data layout parity blocks robust spatially correlated failures failure model regard propose markov random fields express spatial dependencies disk blocks develop simple probabilistic model data placement assuming spatially local failures techniques update parity blocks data blocks atomically writing journal significantly increase performance cost discuss parity update techniques traditional overwrite radical no-overwrite technique differ interact file system journaling ordering block reads writes overwrite technique guarantee atomic update data parity paritynon-atomic no-overwrite technique atomicity parityatomic finally evaluate performance overhead overwrite no-overwrite techniques spatial locality sector failures handle block failures file system model block failures occur important characteristic failures exhibit spatial locality section specifically media errors due particle scratches thermal asperity bad disk head render set contiguous blocks unusable file system developers aware problem considered faultisolated data placements important blocks original design berkeley fast file system redundant super blocks carefully single failure disk track cylinder surface endangers copies generic block layout algorithms modern file systems focused improving performance oblivious spatially local disk errors ibm jfs stores primary secondary copies super block close separated blocks design decisions make primary secondary blocks unusable spatially local errors section propose probabilistic disk failure model characterizes disk block errors spatial locality model layout blocks probability errors affecting related blocks low techniques developed specifically journaling file systems modern file systems windows ntfs linux ext reiserfs ibm jfs xfs journaling file systems file systems provide parity-based redundancy evaluate validity probabilistic model comparing results fault injection experiments probability multiple failures predicted failure model matches closely results fault injection experiments probabilistic failure 
model define redundancy set set disk blocks tolerate maximum failures blocks set blocks fail redundancy set recovered disk blocks file system data metadata discussion redundancy set tolerate failure recover multiple failures define events block redundancy set fails block redundancy set fails event file system recover failed blocks redundancy set probability failure failure occurred conditional probability objective data layout algorithm select blocks probability block failure occurring redundancy set threshold calculate probability block failure cases block errors independent events derive probability failures redundancy set incorporate spatial locality model solve generic case denote denotes event failures happening redundancy set blocks case treat block failures independent events assume uniformly spread disk blocks disk block fails probability block fails block failures independent failure failures kcj number combinations blocks blocks probability block fail case reality latent sector faults independent spatial locality realistic fault model takes account block note interchangeably term multiple failures refer failures failures correlated probability failure differ location block respect faulty blocks disk propose spatial dependencies set sites expressed markov random fields mrf mrf probability distribution site conditionally sites neighborhood neighborhood site defined function sites radius site defined neighbors give introduction mrf discuss express dependencies disk block failures mrf detailed treatment mrf found finite collection random variables random variable corresponds site takes set set neighbors mrf random variable depends neighbors formally negationslash condition brings local characteristics states site depends neighbors note mrf sufficiently large condition holds probability disk block fails depends neighbors spatial locality disk block failures expressed markov random field block affected failures blocks track blocks track defined neighbors general definition blocks disk neighbors fact definition adopt evaluation depending definition neighborhood block layout algorithm construct redundancy set errors happening redundancy set minimal discussion treat state block random variable values valid failed treat set disk block states mrf derive probability failures blocks redundancy set assume failed neighbor affected failure neighbor probability fails depends distance due spatial locality distance block failed block represent probability failure distance distribution chosen reflect extent block errors spatial choose exponential decay distribution probability failure drops exponentially distance increases blocks redundancy set block blocks neighbors sake simplicity blocks exact distance failed block probability failures redundancy set block failed block fails failed block failed blocks fail failed block failed probability blocks fail practice blocks distance equation upper bound probability multiple failures objective find probability multiple failures redundancy set ksummationdisplay equations give probability failures redundancy set blocks disk blocks redundancy sets find probability multiple failures happening entire disk redundancy sets calculating probability equation derive probability multiple failures redundancy set represented entire disk multiple failures redundancy sets qproductdisplay whereproducttextqi probability multiple failures happening redundancy sets failure model section discuss failure model employed file system construct redundancy sets lay blocks equation compute probability multiple failures describe probability file systems construct redundancy sets practice type distribution choose derived analyzing sector errors large set failed disks distribup failure distance exponential distribution distance linear distribution distance step-like distribution figure multiple failures probability distributions tion today due proprietary nature disk industry made future disk manufacturers users large disk farms numbers choose distribution functions compute probability multiple errors happening redundancy set entire disk figure plot probability multiple failures distributions exponential decay curve probability failure drops exponentially distance block increases failed block left-most graph figure distribution linear drop probability middle graph figure finally step-like distribution probability set contiguous tracks drops right-most graph figure graph figure curves curve denoted plots failure probability block distance failed block curves plot probability multiple failures happening entire disk values size redundancy set discuss main points figure plotting values equation probability multiple failures redundancy set plot probability multiple failures happening redundancy set entire disk size equation reason capture large disk thousands redundancy sets higher probability multiple failures plot probability multiple failures values size redundancy set increases probability failures increases finally graphs plot failure distribution simple dimensional surface disks multiple surfaces failure distribution chosen express spatial locality multiple surfaces multiple tracks single surface file system compute probability multiple failures failure model layout related blocks assuming distance represents track separation exponential decay curve similar left graph figure file systems construct redundancy sets blocks set separated distance tracks order probability multiple failures low important note logical file system entities files directories allocated contiguously irrespective distance set setting means redundancy set blocks separated tracks figure depicts case layout file allocated contiguously single track blocks track belong redundancy set file system equation derive size redundancy set based fault tolerance requirements failure model limitations model limitations account temporal locality block access patterns blocks accessed affected bad disk head block layout algorithm based file redundancy set redundancy set track track track track figure block layout failure model figure shows redundancy sets constructed disk tracks note physical contiguity logically contiguous blocks affected blocks file allocated continuously assumption file systems observe disk geometry control block placements logical block physical disk address mappings obtained scsi disks low level scsi commands low end ata drives features low end drives disk features extracted running micro benchmarks similar developed talagala finally lower layers storage stack reassign blocks locations disk breaking layout pattern suggested file system disk drives bad block remapping affect file system assumption disk blocks located modern systems remapping rare locations remapped blocks obtained low-level disk commands model evaluation section describe evaluation probabilistic failure model evaluate probability model comparing results equation results set fault injection experiments simulated disk fault injection experiment works construct parity sets disk blocks block disk blocks selected failure distance exponential distribution model simulate model simulate failure distance linear distribution model simulate model simulate failure distance step distribution model simulate model simulate figure comparing probability multiple failures failure model simulation random failed failure blocks parity set considered failure depending distance block failed block compute chances failure inject fault probabilistically run experiment iterations simulated disk probability higher number failures computed number times higher number faults injected runs repeat experiment parity set sizes block distances figure plot probability failures equation results fault injection tests figure curves model experiments match closely redundant data update techniques pointed section commodity file systems run cheap disk drives additional hardware support nvram nvram 
support redundant information updated atomically primary secondary copies updated modified section describe techniques update data parity explain traditional overwrite technique updates data blocks inplace employ technique implementing parity ixt simpler implement show method guarantee atomic update data parity paritynon-atomic develop radical no-overwrite approach writes data unallocated blocks switches block pointers point blocks no-overwrite technique atomically update data parity parityatomic principle similar write file layout approach maintain file system integrity overwrite no-overwrite techniques integrated transactional semantics journaling file systems implement overwrite no-overwrite techniques block layout proposed probabilistic model previous section performance evaluation shows separating related blocks widely avoid spatially local errors performance impact varies workload run microbenchmarks show overwrite update incurs performance overhead compared no-overwrite technique batching parity updates nooverwrite technique reduces costly disk seeks reduces performance overhead no-overwrite technique fragment file requires periodic cleaning re-layout file blocks contiguously briefly recapitulate journaling functionality explanation parity update schema interact journaling modes describe overwrite no-overwrite techniques finally evaluate techniques journaling file systems explain section journaling file systems offer journaling modes data journaling ordered journaling writeback journaling mode differs type blocks write journal order blocks written focus ordered journaling mode default mode modern file systems linux ext reiserfs ibm jfs xfs windows ntfs due low performance overhead compared data journaling mode consistency semantics compared writeback mode fact jfs xfs ntfs support journaling modes ordered journaling mode metadata blocks written journal data blocks written directly fixed locations data writes ordered complete metadata blocks committed journal application writes block data block disk updated inode block metadata blocks committed journal transaction committed point time inode metadata blocks written final fixed locations disk process called checkpointing parity scheme journaling file systems file system implement parity-based redundancy scheme parity calculated set blocks stored separate disk location parity calculated file system data metadata location parity block set disk blocks stored mapping mapping protected redundant copies discussion refer redundancy set parity redundant information parity set journaling file system parity-based redundancy works journaling layer block written parity block protects block updated tandem ordered writes important data parity updated atomically contents parity block inconsistent respect blocks parity set phases data parity update procedure parity update phase copies data parity blocks read disk contents parity block computed xor operations transaction commit phase data parity blocks updated ordered writes metadata blocks committed journal finally checkpoint phase metadata blocks logged journal written final fixed locations disk overwrite no-overwrite techniques differ order phases executed data written disk specifically overwrite no-overwrite techniques implement paritynon-atomic parityatomic table inode inode journal disk memory write block xor overwrite phase parity update inode inode inode commit journal memory disk overwrite phase transaction commit inode inode inode commit journal memory disk iii overwrite phase iii transaction checkpoint figure overwrite technique parity update figure shows sequence steps file system update parity block overwrite technique overwrite figures iii depict phases overwrite update phase parity update phase write issued copies data block parity block read disk steps copies write parity contents calculated step phase transaction commit phase data parity blocks written fixed locations disk steps ordered writes metadata blocks including inode block committed journal step phase checkpointing phase journaled blocks written fixed locations disk step end phase parity set consistent blocks parity set fails recovered rest blocks overwrite update algorithm provide atomic update data parity recover block errors journal recovery explained case file system crashes step completion step figure journal recovery disk blocks parity set read recompute parity due lack atomicity results inconsistent parity block crash reading parity set blocks file system encounters latent sector fault block corruption parity block recover failed block parity set inconsistent state root problem overwrite update modifying contents data block parity set made inconsistent parity block updated contents note parity block consistent state crash crash happened step step file system provide guarantees consistency parity block read blocks compute parity general problem occur file system crashed point time completion journal commit step transaction commit phase figure no-overwrite section describe no-overwrite technique update parity block schema data block overwritten written location disk phases technique shown figures iii phase transaction commit contrast parity update overwrite technique write block issued overwriting contents block allocated data written location steps constitutes ordered write ordered journaling mode note block allocated parity set block figure blocks parity group ordered writes metadata blocks including inode committed inode inode write block journal inode memory disk commit overwrite phase transaction commit inode inode journal inode memory disk xor commit overwrite phase parity update inode inode journal inode disk memory commit iii overwrite phase iii transaction checkpoint figure no-overwrite technique parity update figure shows sequence steps file system update parity block no-overwrite technique journal step phase parity update phase checkpointing phase versions data parity blocks read disk parity contents calculated steps final phase checkpoint phase parity block written original location disk step checkpoint metadata blocks fixed locations step note step block pointer inode changed refer location data block data blocks overwritten schema atomicity recover block failures journal recovery file system crashes step figure parity set remains consistent state block error parity set recovered reading blocks parity set file system crash happening step figures iii single block error journal recovery successfully recovered point parity block consistent contents block contents block parity block consistent state block errors recovered successfully disadvantage approach fragments file blocks due block allocation overwrite periodic cleaning fragmented blocks reallocate contiguously performed improve file contiguity disk performance analysis evaluate performance overwrite no-overwrite approaches compare default ext run set microbenchmarks macrobenchmarks similar stein evaluate checksum implementation present implementation system configuration details discuss microbenchmark results finally explain macrobenchmark results implementation implement overwrite no-overwrite techniques linux ext techniques reads directly issued disk blocks reads incur additional processing overhead file system writes incur extra disk reads writes overwrite mode ordered checkpoint data writes copies blocks read present local cache parity computed written disk no-overwrite mode block allocated ordered write parity computation occurs checkpointing evaluation purposes run experiments partition separate blocks parity set tracks design tolerate spatially local failures longer order reduce seek overhead group writes queue issue experiments cache parity blocks consumed memory test system files 
number files created file creates no-over default bandwidth amount data written file writes no-over default bandwidth amount data read file reads no-over default figure performance microbenchmarks graphs plot performance overwrite no-overwrite techniques respect ext performance file creates file writes file reads plotted top middle bottom graphs consists linux kernel ghz intel memory western digital wdc bbdaa disk microbenchmarks microbenchmarks consist phases files created synchronously data create files files directory order limit pathname lookup time data written file synchronously totally write files finally files read phases issue plenty random small files spread multiple directories sequential file phase file system unmounted clear cache contents artificial helps separate performance impact phase figure presents results microbenchmarks make observations file creates overwrite no-overwrite perform worse default ext due additional disk traffic incurred reads copies blocks writes parity blocks file writes no-overwrite performs overwrite approach examining disk traffic find due higher disk seek incurred overwrite approach writes issued multiple transactions overwrite technique updates parity transaction reading copies data parity blocks figure no-overwrite scheme batches parity updates checkpointing figure parity update batching reduces disk seeks improves write performance finally file reads contrast writes no-overwrite scheme performs worse overwrite approach performance degradation respect default ext no-overwrite allocates block ordered write time data scattered disk reading back data written pay additional cost seeking scattered data locations periodic de-fragmentation overhead reduced techniques ssh web postmark tpc-b overwrite no-overwrite table performance overheads overwrite no-overwrite table lists relative running overheads benchmarks overwrite no-overwrite techniques compared linux ext overheads greater marked bold macrobenchmarks run set macrobenchmarks similar evaluate performance ixt section benchmark representing desktop type workloads ssh-build read intensive workload web server metadata intensive benchmark postmark finally workload lots synchronous writes tpc-b relative overheads benchmarks overwrite no-overwrite techniques respect default ext presented table table derive conclusions overwrite no-overwrite incur additional overhead compared default ext magnitude overhead depends workload write intensive workloads idle time postmark tpc-b overhead varies note run benchmarks cold cache copies disk blocks present memory results extra overhead due reads disk block copies actual system blocks present cache expect reduce overhead ssh desktop-like workload overhead modest overwrite no-overwrite additional overheads web server workload overwrite technique no-overwrite incurs overhead due scattered file blocks similar behavior file reads microbenchmark finally no-overwrite technique performs compared overwrite technique simple write intensive microbenchmarks performance degrades benchmark involves reads writes summary no-overwrite technique incurs overhead compared overwrite technique primarily due batching parity updates no-overwrite technique disadvantage reallocating file blocks locations disk fragmenting file periodic cleaning file re-layouts maintain physical contiguity logically contiguous file blocks conclusion redundant information years building robust file storage systems single disk studied low-end systems offer challenges including lack redundant disk drives nvram support chapter explore effectiveness cost redundancy techniques checksum parity replica detect recover errors experiences building ixt conclude performance cost redundant information severe high intensive workloads overheads modest typical desktop applications find parity approach strikes balance terms effectiveness performance space overheads disk sectors subject spatially correlated failures parity blocks laid carefully derive simple probabilistic equation file system construct parity sets chances multiple errors affecting set low finally develop parity update techniques traditional overwrite approach modifies data blocks place radical no-overwrite approach writes data locations block pointers no-overwrite offers atomicity updating redundant information fragment file requires periodic cleaning chapter unifying failure handling low-level machinery chapter address challenge building robust file systems incorporate low-level redundancy machinery myriads high-level failure handling policies restructure commodity file systems centralized failure handler unifies failure handling single point describe design implementation details explain evaluation system centralized failure handler failure policy analysis reveals file system failure handling broken failure policies inconsistent inflexible coarse-grained erroneous error propagation file systems complex interleaving calls initiated locations file system code disk reads writes issued system calls background daemons journaling layer buffer cache manager addition functions notify completion special routines functions issue results diffusion specific routines file system code diffusion calls leads diffusion failure handling code detects failures performs recovery retry stopping file system spread places leads problems inconsistent policies file systems illogically inconsistent failure handling policies failure handling techniques similar failure scenarios unintentionally figure presented section observe similar indirect block read errors recovery actions ext rzero propagate rpropagate stop rstop retry rretry confirm verifying source code inconsistent failure handling due hoc application recovery techniques tangled policies mechanisms due diffusion calls failure handling harder separate failure policies detect block corruption implementation specific sanity checking result tangled policy mechanism modified affecting resulting inflexible failure handling system coarse grained policies modern file systems support fine-grained perblock-type failure policies block type failures file systems recovery technique stopping entire file system current file system framework failures handled failure policies detection recovery code change places issued increase diffusion failure handling turn worsen mentioned problems propose centralized failure handler cfh file systems mitigate aforementioned problems support well-defined failure policies cfh architecture decouples failure handling mechanisms policies offers configurable framework file system per-block-type failure policies challenges building cfh cfh semantic information calls block type represents broadly ways obtain information apply reverse engineering techniques file system level gray-box knowledge stream find block types similar techniques developed past extent complex guarantee correct block type detection due file system asynchrony approach modify file system source code explicitly pass block type information call adopt approach access source code correct block type due generic file system components issued file system generic buffer cache layer block type information lost modify common journaling layer buffer cache manager overcome limitation finally failure policies harder correct framework due hundreds combinations block types detection recovery actions developed policy table abstracts complex details mechanisms offers simple interface intricate failure policies design implement evaluate prototype cfh linux ext show cfh detect recover failures consistent manner file system evaluation brings flexibility cfh lines change configuration modifies failure policy ext behave reiserfs ntfs design implementation main goal cfh provide single locale system file system policy appropriately handled requests routed cfh failures reported cfh key vision policy table specifies failure-handling policy system enabling cfh cleanly separate policy mechanism policy table easy read ensuring policy enacted policy desired easy modify ensuring policies easily put place needed critical design 
cfh availability correct semantic information specific request passes cfh information block type relevant information request accessible cfh information crucial enabling fine-grained flexible policies implemented describe approach centralized failure handling start basic framework discuss issues design policy table acquiring semantic information finally explain implementation details basic framework figure shows framework cfh calls issued components file system core file system journaling layer buffer cache manager reads writes passed cfh issues request lower level device drivers disk call standard information block number request size semantic information semantic information includes block type inode numext journal buffer cache ext stop sanity checking type specific queue failed retry replication parity checksum sha remap ext block semantics generic layer ext specific layer failure handlers inode dir data sanity check retry stop sanity check retry stop retry stop ext policy table read write block size status read write block size status context type recovery detection ext block semantics figure cfh framework figure shows components centralized failure handler main subsystems file system specific layer generic layer threads failure handling file system specific layer maintains failure policy table generic layer mechanisms replication parity checksums commonly file systems ber pointers specific detection recovery routines information crucial enabling fine-grained flexible policies cfh cfh basic framework consists main components file system specific layer generic layer additional threads handle failures describe turn file system specific layer file system specific layer receives requests issued file system layers behalf file system understands semantics requests ext specific component understands block types ext file system generic detection recovery routines checksum retry module file system specific block type sanity checking modules special recovery routines current prototype implementation policies file system developer extending interface users system administrators simple file system mounts registers specific detection recovery routines ext register function pointer ext inode sanity check cfh mount called time inode block read written ext note framework file system perform sanity checking writes issued disk find software-induced memory-related corruptions data reaches disk generic layer cfh consists generic layer implements mechanisms retry replication parity remap types checksums policy implementor functionality provided generic layer commonly multiple file systems generic mechanisms easily configured suit file system number retries easily set ext reads writes block types design aspect cfh clean separation mechanisms policy failure handlers cfh additional threads handle failures completion notified interrupt context failure recovery actions invoked interrupt context time critical performing complex recovery actions crash entire system error detected block context information added queue failure handler thread woken failure handler thread removes failed enforces recovery policy file system file system retry failed reissued implement apply techniques iron taxonomy tables failure handlers failure handling threads monitoring asynchronous calls typically asynchronous failures detected file system recovery action cfh failure handlers capture failures including asynchronous handle appropriately policy table cfh maintains policy table initialized file system mount modified running policy table specifies kind detection recovery actions block type failure single easily understandable location failure-handling policy table shows fields record policy table record block type enabling cfh implement fine-grained policy block type file system explain policy table fields detail block type information type blocks confirming type fields represent type block type information validate verify block contents retry max maximum number retries remap block remapped isolate number blocks isolate failed block inode file directory block belongs sanity pointer block type specific sanity checking module recover pointer recovery module arg file system specific argument in-core super block passed recovery function table cfh policy table table presents fields cfh policy table record fields configured separately offer great flexibility supporting fine-grained failure policies policy record applied validate flag specifies block content validated checksums sanity checking validation values field include dont validate sanity check checksum implement generic recovery routines retrying failed requests remapping writes location file system recovery techniques theretry maxto maximum number times failed retried remapto point block write failure remapped blocks failed block higher chances failing file system isolate number blocks remapped failed block addition offering perblock-type failure policy fine tune policy file directory apply stronger failure handling approaches root directory rest directories achieved inode number file directory inode field sanity recover function pointers specific file system inode failure policy record file system set sanity inode sanity checking routine recover file system stop function file system recovery actions stopping entire file system require access in-core super block file system policy table record field arg store noted design policy table fields approach complex designs file system remap routine order remap entire logically contiguous unit general file system implement specialized routines iron detection recovery techniques techniques cfh generic layer support highly flexible designs policy table fields changed significant implemented extension prototype design acquiring semantic information design issues building cfh understanding semantics file system specific component cfh understand context block type invoke specific failure policies failure understanding semantics straightforward reads writes issued directly file system modified ext pass context information call calls issued common file system layers generic buffer cache manager issues modifying functions buffer cache layer cfh buffer cache commonly parts system file systems cfh affected solve problem altering buffer cache layer file system cfh calls redirected issued generic layer file system specific semantics lost calls handle problem adding calls ext pass block type information cfh issued generic layer file system recovering crash file system blocks written journal part successfully committed transaction read written fixed locations important type recovered blocks failure policies applied recovery configure file system pass block type information journal recovery file system active recovery completes solve problem storing block type information blocks journal read recovery apply failure policy function ext block type readdir directory rmdir directory data bitmap inode bitmap unlink data bitmap inode bitmap indirect block truncate data bitmap indirect block write data bitmap indirect block table fixing error propagation table presents list functions fail propagate error application block type failures ext fix functions send error code application failure implementation details implemented centralized failure handler linux kernel modified linux ext modify generic buffer cache journaling layer issued disk reads fail-stop writes processors cfh approach requests designing issued computing behalf systems richard ext schlichting call resulting arizona system fred ext schneider verified cornell fault-tolerant calls methodology redirected facilitates cfh design flagging fault-tolerant requests computing systems cfh presented ensuring based requests notion fail-stop passed processor 
disk flag comprehensive simple technique gave confidence cfh interposed relevant order build robust system error codes propagated reliably layers system error handled appropriately cfh designed failure recovered propagates failure file system logs error message system log file systems ignore failures fail propagate error application analysis find ext fails propagate error application posix calls directory block failure rmdir notified application table lists set functions ignore error propagation ext fix functions ext evaluation evaluate ext show flexibility consistency ext making mimic reiserfs-like ntfs-like failure policies changing lines code policy table ext acts differently response disk failure evaluate ext implement finegrained policies block types specifically develop evaluate policy acts paranoid fashion metadata simply propagating failures encountered accessing user data finally evaluate performance overhead routing requests cfh running macrobenchmarks flexibility consistency ext linux ext highly flexible easily configured mimic failure policies modifying lines configuration file configuration command ext simple ext dir block type ext dir block validate sanity check retry max sanity ext dir sanity check recover ext stop code defines failure policy directory blocks ext informs cfh perform sanity checking ext dir sanity check routine failure asks cfh stop system calling ext stop configuration mimics reiserfs-like failure policy previous work shown reiserfs paranoid disk failures stops file system block errors show flexibility cfh changing failure policy reiserfs ntfs altering lines ntfs assumes failures transient performs persistent retries make ext behave ntfs directory failures setting retry max non-zero recover error propagating function testing framework similar previous analysis section perform failure policy analysis ext run workloads fail specific file system block types tables figure present ext failure policy read failures column represents set workloads row represents data structure ext data structure read workload entry failure policy enforced cfh changing lines data structures ext made mimic failure policy block type failures results shown ensure ext enforces similar policy write failures reiserfs-like failure policy j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode ntfs-like failure policy j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure consistent failure policies ext table recovery policies ext read faults injected block type range workloads workloads path traversal access chdir chroot stat statfs lstat open chmod chown utimes read readlink getdirentries creat link mkdir rename symlink write truncate rmdir unlink mount fysnc sync umount recovery log write operations gray box workload applicable block type multiple mechanisms observed symbols superimposed key recovery represent retry error propagation file system stop addition flexible ext highly consistent applying policies noticed figure design aspects cfh enables consistent application policies separate failure policies mechanisms failure policies enforced central place result file system well-defined failure handling system fine-grained failure policies fine-grained failure policy j-dataj-commit j-descj-revoke j-superg-desc superdata indirecti-bitmap bitmapdir inode figure fine-grained failure policies ext table fine-grained policies cxt write failures workloads data structures figure key recovery failure policy represent retry remap file system stop error propagated failures error code logging error messages file systems configure cfh implement policies block types depending importance system file system paranoid metadata block failures handle differently data block failures show cfh implement fine-grained policies configuring ext policy file system metadata write errors retried remapped journal write failures retried cases ext stops system recover write error data write fails ext retries remap stop propagates error enabling application action evaluate failure policy write failures block types figure presents failure policy table table retry detect failure retry detect failure remap detect failure isolate remap stop remapretry detect failure remap area file system stops section csection bsection section dir isolate supercommit time block figure fine-grained failure handling figure shows block type failures handled differently cfh x-axis completion time plotted y-axis block numbers shown failed shown legends filled successful shaded black writes marked reads show block type failures commit block write failure transient error directory block super block write errors permanent cfh retries commit write failure successfully recovers section cfh configured retry times directory write failure retries failed remaps directory block location disk section cfh marks blocks failed dir block isolation reads remaps section finally super block write fails retried times remapped remap fails cfh stops file system section workloads metadata write failures handled data block failures figure shows specific policy commit processor automatically halts response internal failure effects failure visible problem implementing processors high probability behave fail-stop processors addressed axiomatic program verification techniques developing provably correct programs fail-stop processors design process control system illustrates methodology categories subject descriptors computer-communications networks distributed systems--network operating systems special-purpose application-based systems --real-time systems operating systems logics meaning programs verifying reasoning programs general terms reliability verification additional key words phrases fail-stop introduction programming computer system subject failures difficult task malfunctioning processor perform arbitrary spontaneous state transformations transformations programs executes correct program counted implement desired inputoutput relation executed malfunctioning processor hand impossible build computer system operates correctly spite work supported part nsf grant mcs- authors addresses schlichting department computer science arizona tucson arizona schneider department computer science cornell ithaca york paper originally submitted transactions programming languages systems responsible editor susan graham authors editor kindly agreed transfer paper acm transactions computer systems permission copy fee part material granted provided copies made distributed direct commercial advantage acm copyright notice title date notice copying permission association computing machinery copy republish requires fee specific permission acm acm transactions computing systems vol august pages fail-stop processors failures components finite amount hardware goal implementing completely fault-tolerant computing systems unattainable fortunately applications require complete fault-tolerance sufficient system work correctly provided predefined number failures occur time interval types failures occur modest goal attainable paper present approach designing fault-tolerant computing systems based notion fail-stopprocessor processor well-defined failure-mode operating characteristics briefly approach software designed assuming existence computing system composed fail-stop processors number processors required dictated response-time constraints satisfied system computing system designed implements requisite fail-stop processors proceed section describes characteristics fail-stop processor considers processors approximated presentday hardware section discusses extensions axiomatic verification techniques facilitating development provably correct programs fail-stop processors satisfying response time constraints presence failures subject section section discusses application approach nontrivial problem design fault-tolerant process-control system section contrasts work approaches designing fault-tolerant systems section presents conclusions fail-stop processors definition processor characterized instruction set instruction welldefined transformation internal state processor connected storage peripheral devices effects executing instruction precise semantic definition temporalaxiomatization instruction set principles operation manual failure occurs behavior processor consistent semantic definition fail-stop processor distinguished extremely simple failure-mode operating characteristics internal state fail-stop processor predefined portion connected storage assumed volatile contents volatile storage irretrievably lost failure occurs remaining storage defined stable unaffected kind failure contrast real processor fail-stop processor performs erroneous state transformation due failure processor simply halts visible effects failure fail-stop processor fsi stops executing internal state contents volatile storage connected lost sed quis custodiet ipsos custodes guard guards juvenal satires acm transactions computer systems vol august schlichting schneider implementation notion fail-stop processor abstraction impossible implement finite amount hardware finite amount hardware sufficient number failures disable error detection mechanisms arbitrary behavior construct computing systems high probability approximate behavior fail-stop processor approach construct system behaves failures occur time interval assumptions behavior made k-fail-stop processor computing system behaves fail-stop processor failures occur components implementation k-fail-stop processor appears k-fail-stop processor implemented exploiting solution byzantine generals problem interactive consistency problem protocol collection processors agree potentially faulty transmitter commanding general nonfaulty processor agrees transmitter transmitter nonfaulty nonfaulty processor receives transmitter number real processors volatile memory units interconnected communications network form single k-fail-stop processor attendant stable storage memory unit read processors written failures detected processor run program comparing results copy variable stored memory unit execution variable stable storage required variable read memory unit solution byzantine generals problem employed distribute vector values read processor values identical failure occured signaled nonfaulty processors halt failure signaled total processors required order failures tolerated processors execute asynchronously execution replicated programs synchronized compare results accomplished assuming processor clocks run roughly rate associating logical clock program logical clock incremented variable supposed stored stable storage k-fail-stop processor read written synchronize processor constructs vector values clocks solution byzantine generals problem marks time components vector timeout period elapsed case failure occurred signaled collection k-fail-stop processors interconnected implement system real-time response constraints met failures case k-fail-stop processor detect stopped read contents k-fail-stop processor stable storage computation progress time failure continued acm transactions computer systems vol august fail-stop processors accomplished k-fail-stop processor connected communications network read contents memory units make k-fail-stop processors special location failedi memory unit reserved record processor thinks fail-stop processor part halted due failure k-fail-stop processor fsp determines fsp halted computing vector values failedi memory unit fsp solution byzantine generals problem components true fsp deemed halted fsp require values variables stable storage fsp reconstructed processor fsp reads memory units make fsp solution byzantine generals problem values exchanged majority variable values read fsp wrong memory units required implement stable storage feasibility implementing fail-stop processors established argument practicality recent work implementation highly reliable processors reason practical implement fail-stop processor approximations ftmp sift configured behave collection fail-stop processor approximations employ replicated processor memory units redundancy introduced lower levels variety ways level redundancy applied important issue treated programming fail-stop processor recovery protocols program executing fail-stop processor halted failure occurs execution restarted correctly functioning fail-stop processor original processor failure repaired fail-stop processor program restarted internal processor state contents volatile storage unavailable routine needed complete state transformation progress time failure restore storage well-defined state routine called recovery protocol recovery protocol execute correctly started intermediate state visible failure information stable storage addition code recovery protocol failure stable storage associate recovery protocol sequence statements called action statement form fault-tolerant action fta fta action recovery end execution fta consists establishing recovery protocol effect executed executing execution fta interrupted acm transactions computer systems vol august schlichting schneider failure restart execution continues recovery protocol effect subsequent failures execution fta halted execution recovery protocol effect begin anew program restarted execution fta terminates execution performed entirety interruption time recovery protocol effect fta started reestablished fault-tolerant action fta fta recovery protocol fta established syntactic abbreviation denote action statement serves recovery protocol fta action recovery end fault-tolerant action called restartable action program running fail-stop processor times recovery protocol effect case program single faulttolerant action alternatively program structured sequence faulttolerant actions 
assuming establishment recovery protocol times recovery protocol effect assumption reasonable axioms fault-tolerant actions floyd-hoare axiomatic approach assertion booleanvalued expression involving program logical variables syntactic object assertions programming language statement called triple triple theorem exists proof formal deductive system called programming logic programming logic consists set axioms rules inference relate assertions programming language statements triples interest logics sound respect execution programming language statements program state--that deductive systems consistent operation real machine notation execution begins state true terminates true resulting state numerous programming languages defined logics pascal-like language extended guarded commands paper convenient write proof outline formal proof proof outline sequence programming language statements interleaved assertions statement proof outline preceded directly assertion called precondition denoted pre directly assertion called postcondition denoted post proof outline fault-tolerant action converted restartable action simply omitting action statement acm transactions computer systems vol august fail-stop processors abbreviation proof poi statement triple pre post theorem programming logic adjacent proof outline provable fta fault-tolerant action formed action statement recovery protocol develop inference rule derivation fta theorem preserving soundness programming logic respect execution fail-stop processor assume proved execution establish similarly recovery protocol establish hold recall invoked failure definition contents volatile storage undefined time program variables needed execution stable storage require program variables named stable storage ensure recovery protocol receives control stable storage state satisfies facilitated constructing replete proof outline proof outline assertions describing states visible failure require precondition recovery protocol satisfied states replete proof outline proof outline assertions deleted rpoi assertion appears adjacent fault-tolerant actions rpo triple replete proof outline satisfies sequence fault-tolerant actions invariant execution rpo rpo follow program state exists execution fault-tolerant actions fta fta visible recovery protocol enclosing fault-tolerant action--either recovery stronger variables stored stable storage interest minimizing amount stable storage proofs terms weakest assertions acm transactions computer systems vol august schlichting schneider protocol fta recovery protocol fta receive control rpo remains true executed true state visible recovery protocol failure occur assertions replete proof outline fta fta fta proof outline fta fta fta replete proof outline assignment integer variable performed executing single indivisible store instruction--as machines--then replete proof outline precondition postcondition true state occurs execution assignment assignment implemented execution single instruction val-x val kval replete proof outline assertion val destroyed assignment true execution val addition correct operation recovery protocol requires fault-tolerant action action statement recovery protocol satisfying assertions replete proof outline assertions replete proof outline lastly guaranteed failures processors executing fta interfere invalidate assertions proof outline fta suppose assertion fta names variables stored volatile storage processor processor fail assertion longer true contents volatile storage lost require variables stored volatile storage named assertions appearing programs executing processors fault-tolerant action restartable action implements state transformation constructed recovery protocol proof theory action statement actions concurrently executing processes synchronized assert collection processes executing phase time include assertions state reasoning acm transactions computer systems vol august fail-stop processors unnecessary practice additional flexibility results action statement recovery protocol helpful failures infrequent recovery protocol considerable amount extra work order minimize amount expensive stable storage algorithms normal processing unacceptable fault-tolerant programs--a simple addition allowing axiomatic verification programs written terms fault-tolerant actions permit programmer develop fault-tolerant program proof hand-in-hand proof leading advocated variables stored stable storage identified mechanical proof construction replete proof outline mechanical determine intermediate states visible failure illustrate rules aid developing recovery protocol artificial problem substantial treated section periodically variables updated based previous values function routine called update desired runs fail-stop processor satisfies specification update logical variables represent initial values possibility failure program suffice sla pla slb plb note replete proof outline provided assignment implemented atomic operation pla invariant execution sla pla plb invariant execution slb things complicated possibility failure considered action statement restartable action violated assuming identity function pla plb false order construct restartable action find make progress--compute --without destroying initial values values updated modify values computed stored temporary variables giving restartable action action recovery xay-y ula xnew xnew ulb ynew xnew ynew end xnew ynew acm transactions computer systems vol august schlichting schneider note order satisfy stored stable storage variables computing established simple matter establish xnew ynew xnew xnew ynew ynew xnew ynew replete proof outline provided xnew ynew stored stable storage satisfied action recovery xnew ynew xnew xnew ynew ynew xnew ynew end restartable action desired program termination response time statements programming notation guaranteed terminate started loops fault-tolerant actions techniques based variant functions well-founded sets proving loop terminate knowledge frequency failures statement execution times termination program written terms fault-tolerant actions proved failures occur sufficiently high frequency guarantee component faulttolerant actions terminate action statement recovery protocol fault-tolerant action guaranteed run interruption recovery protocol continually restart liveness properties expressed hoarestyle programming logic resort informal means argue program terminate timely manner point future formalize arguments harter bernstein describe extensions temporal logic construction proof program meet specific response-time goals work extended deal stochastically defined events context execution program fault-free processor maximum length time elapses execution statement begun execution fault-tolerant action started define tmax max ses execution terminate sufficient intervals length vmax failures fault-tolerant action forever restarted result high frequency failures acm transactions computer systems vol august fail-stop processors bound time elapse completes argued guaranteed terminate elapsed time successive failures long surprising provide insight structure program terms fault-tolerant actions frequent failures expected endeavor minimize tmax achieved making entry faulttolerant action frequent event nesting fault-tolerant actions composing sequence collection fail-stop processors configure system implements relation input output performs state transformation timely manner occurrence failures failure fail-stop processor fsp reconfiguration rule assign programs running fsp working fail-stop processors recovery protocol effect time failure facilitates restart program processor failures transparent possibly increased execution times result failure execution delays sources incurred time tdetect elapse fail-stop processor halts fact detected reconfiguration begun reconfiguration execution delays block write fails transient manner recovered retry directory block write fails continuously cfh remaps location implement isolation policy cfh reads blocks failed block writes location marks region unusable prevent future errors finally super block write fails cfh retries times recover remapping finally stops file system remap fails show simple policy file system implement finegrained policies treating root directory failures differently directory failures performance overhead benchmark ext ext ssh web postmark tpc-b table performance overheads cfh table lists running time seconds benchmarks ext ext finally evaluate ext performance understand additional processing caused passing cfh adds performance overhead run file system benchmarks ssh-build unpacks compiles ssh source code web server benchmark reads file ext sends network standard postmark benchmark emulates mail server workload tpc-b-like benchmark runs debit-credit transactions results presented table run iterations benchmarks ext ext find noticeable difference performance conclusion design implement centralized failure handler file systems forwards requests responses file system disk cfh implements generic specific error handling routines file system handle disk failures unifying failure handling single point separating policies mechanisms cfh enables fine-grained consistent failure handling file systems cfh offers greater flexibility failure handling implementation efforts non-trivial modified lines existing code ext buffer cache journaling layer added lines code build cfh including generic detection recovery mechanisms checksums remap extending cfh file systems expensive basic cfh framework place fact file systems reiserfs generic buffer cache easier embed block-type semantic information calls find easier maintain cfh individual file systems design cfh clear separation file system specific layer generic layer file system data structure modified routines handle data structure change affecting rest failure handler parts chapter related work effort builds related work bodies literature file system analysis section related efforts inject faults test robustness systems failure work building robust file systems including prototype ixt section probabilistic model data layouts section parity update techniques section centralized failure handler chapter draw efforts building software robust hardware failure discuss turn robustness analysis approaches 
analyze measure robustness system discuss related approaches faults injected resulting system behavior monitored formal techniques model checking find bugs system code finally failure characteristics system measured stress testing collecting failure data production systems fault injection fault injection decades mechanism measure robustness systems software simulate effects hardware failures inject faults dynamically determining file system block types previous work similar software-implemented fault injection swifi techniques evaluate dependability computer systems generic fault injection fiat fault injection-based automated testing early systems fault injection techniques simulate occurrences hardware errors changing contents memory registers fiat injects memory bit errors task image physical location fault gathered compiler loader information automatically fault injection experiments driver automatically determines fault location block fail inferring block type information ferrari fault error automatic real-time injector tool emulates permanent transient hardware faults software software traps inject faults modules roughly correspond functionalities coordinator fault injection driver system initializer activator module ferrari prepares system fault injection user information module obtains parameters fault error types user fault injector injects faults program execution finally data collection analysis module collects logs results coordinator driver perform activities system systems swifi approach include ftape fault tolerance performance evaluator tool performs dynamic workload measurements injects faults automatically determining time location maximize fault propagation ftape stress-bases injection techniques inject faults high workload activities ensure higher fault propagation ftape framework similar inject disk system faults driver emulate errors important difference system fault initiated workload stress framework fault initiated block type written disk fault injection experiments study fault propagation system fine fault injection monitoring environment tool developed kao inject hardware induced software faults unix kernel trace execution flow kernel explicitly trace fault propagation system disk errors inject propagate origin generic device driver layer reaches file system behaviors including system crashes work targeted kernel reliability analysis recent work fault injection techniques test linux kernel behavior presence errors kernel instruction code test kernel subsystems including architecture dependent code virtual file system memory management core kernel testing focuses file systems inject trecon required determine assignment programs remaining fail-stop processors tmove required move program code contents stable storage worst case effects seconds execution action statement failure lost seconds worth execution repeated attempts perform recovery protocol result subsequent failure lost defined specific execution interrupted suggests strategy constructing fault-tolerant systems continue behave correctly spite failures program developed implements desired state transformations run fail-stop processors satisfies real-time response constraints provided failures 
occur process respond event seconds tdetect tmove supppose fail-stop processors required ensure hold computing system fail-stop processors tolerate fail-stop processor failures meet response-time goals obvious reconfiguration nile note stable storage shared fail-stop processors tmove made precomputing configurations made negligible requires sufficient amount stable storage store configurations lastly made restartable actions uniformly degrades execution speed failures occur acm transactions computer systems vol august schlichting schneider fault-tolerant process-control software turn substantial illustration application methodology development fault-tolerant process control program correct program fault-free computing system developed program extended run correctly system fail-stop processors fair amount detail presented details derive establish correctness program sensors determine state environment actuators exert control environment correct operation process-control system requires values written actuators related values read sensors application-specific function correct operation involves liveness property sensors read actuators updated make attempt argue faults instruction streams inject disk failures file storage system testing fault injection specifically study reliability storage systems brown developed method measure system robustness applied measure availability software raid systems linux solaris windows emulate disk disk emulator inject faults test software raid systems work targets file systems file system knowledge carefully select fail specific block types require semantic program satisfies real-time response constraints informal arguments developed section timing data information fault injection studies evaluated raid storage systems reliability availability studies developed detailed simulation models raid storage arrays network clusters obtain dependability measures contrast simulationbased fault injection perform prototype-based fault injection failure analysis file system testing tools test file system api types invalid arguments siewiorek develop benchmark measure system robustness test dependability file system libraries benchmark consists parts exercising part file management system part focuses data structures file management corrupting file pointer file system flag part measures effectiveness input error detection mechanisms functions standard input output library stdio similarly koopman ballista testing suite find robustness problems safe fast sfio library test functions sfio api show sfio robustness greater stdio fair number robustness failures functions read write contrast work test robustness file system api measure robustness file system disk write failures major difference related work fault injection approach focuses file systems handle broad class modern disk failure modes previous work approach assumes implicit knowledge file-system block types ensure test paths file system code previous work inserts faults blind fashion uncover problems found formal methods static source code analysis popular approach test robustness software systems model checking techniques applied file system code find bugs design flaws model checking comprehensive fault injection error detection requires work terms modeling complex file system operations recent work yang model checking comprehensively find bugs file systems ext reiserfs jfs formal verification techniques systematically enumerate set file system states verify valid file system states work identify problems deadlock null pointers work focuses file systems handle latent sector faults recently yang develop method automatically find bugs file system code sanity checks on-disk data structure values symbolic execution system called exe instruments program runs symbolic input initially free based conditional expressions data values exe generates constraints input values generates cases true path false path conditional expressions approach formal methods requires access source code source code analysis work techniques systems stress tested monitored understand failure characteristics gray measure disk error rates sata drives moving data run programs office-like data-center-like setups write read data large files compare checksum data uncorrectable read errors measure uncorrectable bit errors file system errors application level conclude uncorrectable read errors dominant system-fault source suggest time data loss mttdl metric maintainers large disk farms log collect failure data production systems results published anonymizing drive manufacturer information effect firmware failure mechanisms studied family drives concluded fixes applied improve performance increase failure rates building robust file systems system researchers aware disk failures decades considered reliable file systems important pieces operating system design quote needham integrity file system important requirement requires special treatment dealing errors sections explain related work realm designing building robust file systems iron file systems work iron file systems partially inspired work google acharya suggests cheap hardware paranoid assume fail unpredictable ways google good reason treats application-level problem builds checksumming top file system disk-level redundancy drives machines drive extend approach incorporating techniques file system applications benefit note techniques complimentary applicationlevel approaches file system metadata block inaccessible user-level checksums replicas enable recovery now-corrupted volume related approach driver hardening effort linux stated hardened driver extends realm well-written include professional paranoia features detect hardware software problems page drivers generally improve system reliability faults handled file system end-to-end argument fail-partial failure model disks understood high-end storage high-availability systems communities network appliance introduced row-diagonal parity tolerate disk faults continue operate order ensure recovery presence latent sector faults virtually network appliance products checksumming detect block corruption similarly systems tandem nonstop kernel include end-to-end checksums handle problems misdirected writes interestingly redundancy single disk instances ffs internal replication limited fashion specifically making copies superblock platters drive noted earlier commodity file systems similar provisions suggest making replicas disk raid array reduce rotational latency primary intention copies recovery storage array difficult apply techniques selective manner metadata work 
replication improving performance faulttolerance future investigation iron strategies checksumming commonplace improve system security patil stein suggest implement evaluate methods incorporating checksums file systems systems aim make corruption file system data attacker difficult finally dynamic file system sun good file system iron techniques dfs checksums detect block corruption employs redundancy multiple drives ensure recoverability contrast emphasize utility replication drive suggest evaluate techniques implementing redundancy show embellish existing commodity file system dfs written scratch limiting impact disk failure modeling failure models disks specifically context raid storage arrays studied gibson develops analytical model reliability redundant disk arrays discusses models ranging simple considers independent disk failures complex models include spare disks dependency disk unit failures models validated software simulation similar work kari develops reliability models includes sector faults disk unit faults treats sector failures independent events account spatial locality sector errors comparison work failure model developed single disk markov random field express local characteristics disk errors markov random field extensively field computer vision interpreting spatially correlated features image pixels express locality disk sector failures reliability long term digital archives photos emails web site archives received attention research community baker model reliability long-term replicated storage systems correlated failures occur due spatial locality assuming correlated failures exponentially distributed model similar probabilistic model model system tolerate failure spatial locality parity-based redundancy parity widely mechanism store redundant information disk topic research work seminal work raid storage arrays includes discussion raidcan constructed parity blocks tolerate single disk failure recent work focused parity blocks stand double disk failures evenodd rdp techniques spread parity multiple disks data recovered double disk failure recent work denehy develop techniques handle parity update consistency issues software raid systems introduce mode operation journaling file system called declare mode information outstanding disk writes crash augment software raid layer improve speed resynchronization parity blocks file system previous work focused adding parity layer beneath file system work primarily built implementing redundancy file system centralized failure handling system designers considered separation policies mechanisms basic design principle decades classic work hydra operating system built mechanisms kernel perform operations scheduling paging protection user-level control policies congestion manager architecture similar work advocates centralized mechanism implementing variety policies motivated problematic behavior exhibited applications flows compete resources share network information addresses problems inserting module maintains network statistics flows orchestrates data transmissions hybrid congestion control algorithm chapter conclusions future work problem deal bizarre occurrences disc channel writes wrong place disc error indication worse written fact theory practice operating system design needham hartley sosp commodity operating systems grown assume presence reliable hardware result case file systems commodity file systems include requisite machinery handle types partial faults expect modern disk drives thesis develop employ technique semantic failure analysis sfa block type information transaction semantics test understand failure handling journaling file systems built robust versions linux ext redundant information centralized failure handler provide fine-grained well-defined failure policies chapter summarize dissertation recapitulating failure policy analysis experiences building iron file systems section list set lessons learned dissertation section finally present future directions thesis possibly extended section summary storage stacks modern computers present complex failure modes latent sector faults data corruption transient errors part thesis focuses understanding failure policies file system confronted faults choose focus local file systems due ubiquitous presence challenges present commodity file systems large complex pieces software intricate interactions parts system buffer cache manager schedulers journaling layer low level drivers complexity techniques approaches system developers users file systems test understand commodity file systems handle disk failures regard develop technique called semantic failure analysis applies file system specific knowledge block types transaction semantics low-level block stream fails specific file system contrast traditional fault injection fails blocks blind fashion semantic failure analysis fast identify failure policies bugs inconsistencies easily analyze commodity journaling file systems linux ext reiserfs jfs xfs windows ntfs make observations commodity file systems built assumption disk fails fail-stop manner store redundant information disk result portions disk fail file systems recover file systems exhibit illogical inconsistency failure handling policies similar disk failure scenarios detection recovery actions employed suspect result assumptions made developers writing sections code finally failure handling hard current file system architecture frameworks supporting well-defined failure policies part thesis focus improving robustness commodity file systems disk failures explore effectiveness cost redundant information checksum replica parity applying redundancy file system metadata data individually combinations study breadth cost terms time space results show typical desktop-like workloads feasible redundant information small overheads focus depth issues presented single disk drives lowend systems spatially correlated faults lack nvram support develop probabilistic model file system construct redundancy sets chances multiple faults affecting blocks set low develop parity update techniques integrate journaling framework update data parity blocks consistently techniques no-overwrite approach provide atomicity updating redundant information result handle latent sector faults journal recovery evaluation techniques point performance overhead parity update techniques varies typical desktop-type workloads increases high synchronous write-intensive benchmarks finally rearchitect file system centralized failure handler cfh handle failures consistent manner cfh receives calls file system semantics block type information separating policies mechanisms handling failures single point cfh enables uniform fine-grained flexible failure policies show cfh incurs additional overhead forwarding requests file system disk conclude time reexamine file systems handle failures excellent model operating system kernel networking subsystem network hardware long considered unreliable hardware medium software stacks designed well-defined policies cope common failure modes lessons learned present list lessons learned working dissertation relevant realm thesis importance semantic information semantic information crucial building high performance dependable systems addition semantic knowledge perform informed analysis realize failure policy analysis transactional semantics block type information unearth file system failure policies harder correct semantic information cases access source code building centralized failure handler file system semantics lost issued generic buffer cache manager journaling layer modified ext generic buffer cache layer journaling layer pass semantics file system practically feasible modifying common layer buffer cache subsystems generally discouraged circumstances techniques developed context semantically-smart disk systems reverse engineer file system information immensely failure class citizen traditionally systems built performance main focus result policies developed applications policy aimed maximizing performance specific workload behavior systems grow complexity reliability primary design goal failures treated class 
citizens systems failure policies presented application choose policy suits cfh step achieving goal apply finegrained policies file system data metadata benchmarks represent desktop workloads thesis focus local file systems running low-end systems estimate cost redundancy techniques systems ssh-build section representative workload benchmarks built systematically depict applications running desktop laptop setting current file system benchmarks stress file system path targeted emulating high end systems postmark represents mail server workload tpc-b represents transactional workload database server workload suit built emulate desktop applications commodity operating system developers ways describe policies policies harder framework failure policies hundreds combinations block types detection recovery techniques thesis show cfh interface intricate policies simple commands future work present set research directions pursue future work discuss stem efforts analyzing building robust file systems checksums assuming failures process-control system structured collection cyclic processes execute concurrently process responsible controlling set actuators acti reads sensors maintains statei--a vector state variables reflects sensor valuespi read actions interprocess communication accomplished disciplined shared variables process read write state variables read state variables maintained processes moment ignore problems arise concurrent access state variables process consist single loop execution loop body processpi reads sensors computes values actuators controls state variables maintains writes relevant values act updates statei application-dependent routines compute values written actuators values stored state variables loss generality assume state variable sensor read execution routines statej denote statej read tth execution loop body sensors denote values read sensors tth execution loop body act denote values written acti bypi tth execution loop body behavior satisfying characterized process values statei correctly encode past actions performed encoding denoted function beginning code satisfies restriction written local variables store state variables sensor values state variable sensor stored local variable read subsequent made local variable acm transactions computer systems vol august fail-stop processors execution loop body atpi istate cor statei sensors statel staten values written actuators computed application-specific function called based sensor values read past actions processes updates act tth time iact cor acti sensors statel staten true auxiliary variable defined time executions loop body completed initialized implicitly automatically incremented immediately loop body executed correctness criterion satisfied istate iact true beginning execution loop body process order construct loop variable newstate introduced values update state actuators consistent vnewstate newstate ffi sensors state staten loop process loop invariant process true calc newstate sensors statei state vnewstate istate iact act acti newstate vnewstate istate iact state newstate vnewstate istate iact end processes execute asynchronously access state variables synchronized process read state variables midst updated process perform wrong actions avoid problem state variables maintained process assumed characterized cci called consistency constraint statei true state updating variables performing assume code compute application dependent functions works correctly long values satisfy consistency constraints read ensure values satisfying consistency constraints read read write locks implement reader-writer exclusion state variables maintained process process read variables statei acquire read lock state lock granted write lock held state variables process delayed statei updated notation cor ifa true acm transactions computer systems vol august schlichting schneider process update statei delayed processes reading values lock operations explicitly included programs simplify exposition part routine compute calc routine update state variables similarly assume code compute requires sensor values consistent natural laws govern physical world ensure time values sensors consistent process reads sensors simultaneously consistent values obtained simultaneous read operation implementable assume sensors change values slowly processes execute quickly consistent set values obtained reading sensors sequence normal execution speed allowing failures deal failures attempting mask effects endeavor preserve time state variables actuators values failure occurred recall characterizes values state variables actuators satisfy modify loop body true state visible failure satisfied task modify loop body constitutes restartable relax ordering constraints order maintain file system integrity modern file systems implement ordering constraints update on-disk structures write ordering journaling modes journaling file systems file systems soft updates write file layout wafl log structured file systems ordering file system interactions disk introduces problems performance overhead introducing wait calls journaling file systems wait transaction writes finish writing commit block mark transaction complete spite sequential write journal incurs additional rotational latency disk drive implementing correct ordering types disk blocks multiple concurrent updates nightmare propose checksums file systems relax ordering constraints providing consistency integrity guarantees explore application idea thesis call transactional checksums section show transactional checksums significantly improve file system performance workloads small synchronous writes future plan examine checksums file systems soft updates wafl limiting case study plan apply idea ffs file system ext make crash recovery faster benefits free checksum computational cost add overhead increasing processing power limited improvement disk access speeds profitable trade-off make understanding spatial locality disk faults attributes disk failures conditions particle scratch thermal asperity faults spatially correlated field data faults spread distribution bad blocks disk platters deriving data drives discarded unusable shed light block faults higher layers improve data layout patterns scsi drives provide low-level commands obtain mapping details logical block physical location disk including details cylinder track sector step reconstruct layout logical blocks disk show bad blocks disk drive brings bad blocks identified disk drive unearth internal policies disk firmware bad block remapping algorithm possibly higher layer improve layout impact richer interfaces reliability pointed section storage stacks consist layers failure modes detection recovery techniques current architecture details failure exposed end point file system information file system receives succeeded exposing information layer caused error detection recovery actions intermediate layers improve failure handling end point possibly redundant components controller recover failure information failure handling mechanisms disk drives interfaces control improve robustness storage stack interfaces exists control additional information stored bytes bytes sector low-level information disk drives implement remapping internally information number location blocks reserved remapping list remapped disk blocks algorithm remapping interface control block remapping file system determine data layout policies disks expose set detection recovery actions reading writing data layers storage stack avoid re-executing similar actions disk notifies retried times declaring block inaccessible layer disk drive avoid retrying request higher level semantic information end storage stack disk drives improve reliability drive understands physical disk blocks constitute logical file system entity physical blocks alive improve internal reliability mechanisms bad block remapping semantically-smart disk systems explored file system level knowledge disk drives reverse engineered higher-level semantics advent objectbased storage systems information explicit interfaces virtual machines improve storage stack reliability thesis show failure handling commodity file systems broken modify file systems improve robustness altering file systems feasible practice future propose explore idea virtual machines improve reliability storage stack virtual machine detects recovers partial disk faults presents simple fail-stop virtual storage system commodity file system achieved pseudo-device driver powerful virtual machine potential virtual machine alter file system behavior file systems reiserfs crash entire system simple read errors calling kernel-level panic virtual machines possibly capture calls turn expensive stops read-only mount analyzing robustness data management systems limit focus thesis local file systems issues discuss latent sector faults block corruption spatial locality sector faults impact redundancy techniques applicable data management systems distributed file systems database management systems systems complex local file system layers storage stack presence network distributed systems step study partial failure scenarios introduced context systems support redundant components multiple disks single site multiple sites raises issue finding trade-offs redundancy single disk multiple disks site finally distributed disks optimizations transactional checksums directly applicable databases fact explore applicability relaxing ordering constraints checksums distributed protocols bibliography anurag acharya reliability cheap learned stop worrying love cheap pcs easy workshop october anton altaparmakov linux-ntfs project http linuxntfs sourceforge net ntfs august guillermo alvarez walter burkhard flaviu cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca pages denver colorado dave anderson don jack disks acm queue june dave anderson drive manufacturers typically don talk disk failures personal communication dave anderson seagate dave anderson osd drives snia events past developer dba snia osd pdf dave anderson jim dykes erik riedel interface scsi ata proceedings usenix symposium file storage technologies fast san francisco california april akshat aranya charles wright erez zadok tracefs file system trace proceedings usenix symposium file storage technologies fast san francisco california april andrea arpaci-dusseau remzi arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp pages banff canada october andrea arpaci-dusseau remzi arpaci-dusseau lakshmi bairavasundaram timothy denehy florentina popovici vijayan prabhakaran muthian sivathanu semantically-smart disk systems past present future sigmetrics performance evaluation review march remzi arpaci-dusseau andrea arpaci-dusseau fail-stutter fault tolerance eighth workshop hot topics operating systems hotos viii pages schloss elmau germany lakshmi bairavasundaram muthian sivathanu andrea arpacidusseau remzi arpaci-dusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca pages munich germany june mary baker john hartman martin kupfer ken shirriff john ousterhout measurements distributed file system proceedings acm symposium operating systems principles sosp pages pacific grove california october mary baker mehul shah david rosenthal mema roussopoulos petros maniatis giuli prashanth bungale fresh reliability long-term digital storage proceedings eurosys conference leuven belgium april hari balakrishnan hariharan rahul srinivasan seshan integrated congestion management architecture internet hosts proceedings 
sigcomm pages cambridge massachusetts august wendy bartlett lisa spainhower commercial fault tolerance tale systems ieee transactions dependable secure computing january barton czeck segall siewiorek fault injection experiments fiat ieee transactions computers april steve jfs log journaled file system performs logging proceedings annual linux showcase conference pages atlanta steve jfs overview ibm developerworks library jfs html dina bitton jim gray disk shadowing proceedings international conference large data bases vldb pages los angeles california august blaum brady bruck menon evenodd optimal scheme tolerating double disk failures raid architectures proceedings annual international symposium computer architecture ieee standards board ieee standard glossary software engineering terminology september tim bray bonnie file system benchmark http textuality bonnie aaron brown david patterson maintainability availability growth benchmarks case study software raid systems proceedings usenix annual technical conference usenix pages san diego california june george candea shinichi kawamoto yuichi fujiki greg friedman armando fox microreboot technique cheap recovery proceedings symposium operating systems design implementation osdi pages san francisco california december remy card theodore stephen tweedie design implementation extended filesystem proceedings dutch international symposium linux amsterdam holland andy chou jun-feng yang benjamin chelf seth hallem dawson engler empirical study operating system errors proceedings acm symposium operating systems principles sosp pages banff canada october peter corbett bob english atul goel tomislav grcanac steven kleiman james leong sunitha sankar row-diagonal parity double disk failure correction proceedings usenix symposium file storage technologies fast pages san francisco california april timothy denehy andrea arpaci-dusseau remzi arpacidusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix pages monterey california june timothy denehy andrea arpaci-dusseau remzi arpacidusseau journal-guided resynchronization software raid proceedings usenix symposium file storage technologies fast pages san francisco california december john devale philip koopman performance evaluation exception handling libraries proceedings international conference dependable systems networks dsngoteborg sweden june john douceur willian bolosky large-scale study filesystem contents proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics pages atlanta georgia james dykes modern disk roughly lines code personal communication james dykes seagate august jon elerath reliability disk drive industry mtbf proceedings annual reliability maintainability symposium rams pages january jon elerath sandeep shah server class disk drives reliable proceedings annual reliability maintainability symposium rams pages emc emc centera content addressed storage system http emc ralph waldo emerson essays english traits self-reliance harvard classics edited charles eliot york collier son volume foolish consistency hobgoblin minds adored statesmen philosophers divines dawson engler david chen seth hallem andy chou benjamin chelf bugs deviant behavior general approach inferring errors systems code proceedings acm symposium operating systems principles sosp pages banff canada october gregory ganger marshall kirk mckusick craig soules yale patt soft updates solution metadata update problem file systems acm transactions computer systems sanjay ghemawat howard action true execution statement act begins statement completes mask intermediate states execution act devise execute act atomic action option precluded hardware implement construct single fault-tolerant action updates actuators state variables basis newstate vnewstate upall vnewstate istate iact long newstate saved stable storage replete proof outline satisfies accomplishes desired transformation upall action recovery vnewstate act acti newstate vnewstate iact statei newstate vnewstate istate iact end replete proof outline code executed process action recovery true calc newstate sensors statei state vnewstate istate iact acm transactions computer systems august fail-stop processors upall action recovery act acti newstate statei newstate end end notice failure process attempt acquire read write lock granted failure occurred executed recovery protocol attempt acquire write lock statei owned repeated requests process lock intervening release operations delay invoker implementation read write locks property binary semaphores suffice discussion related work general techniques developed aid design programs cope operational failures hardware support software paradigm based state machines pioneered lamport program viewed state machine receives input generates actions output internal state reliable system constructed replicating state machines running parallel solution byzantine generals problem machine guaranteed receive input failures comparison state machine approach fail-stop processors fault-tolerant actions appears general paradigm appears promising based nested atomic transactions variety protocols specialized problems developed included protocols recovery database systems implementation highly reliable file systems checkpoint restart facilities operating systems apparent similarity recovery block construct developed newcastle-upon-tyne fault-tolerant actions constructs intended purposes recovery block consists primary block acceptance test alternate blocks entry recovery block primary block executed completion acceptance test executed determine primary block performed acceptably test passed recovery block terminates alternate block--generally implementation algorithm--is attempted acceptance test repeated execution alternate block attempted sequence produces state acceptance test succeeds execution alternate block begun recovery block initial state recovery blocks mask design errors fault-tolerant actions constructing programs cope operational failures underlying hardware software recovery blocks cope operational acm transactions computer systems vol august schlichting schneider failures circumstances lead difficulties recovery block finite number alternate blocks large number failures underlying system alternatives exhausted recovery block model admit possibility stable storage program variables fail-stop processors definition fail-stop processor underlying computational model partial correctness programming logic fail-stop processor failures detected incorrect state transformations result failures execution statement terminates definition transformation statement occurred--the effect execution consistent programming logic hand failure definition prevents statements terminating partial correctness opposed 
total correctness nature programming logic subsumes consequences failures application methodology successfully employed methodology paper verify existing fault-tolerant protocols devise twophase commit protocol verified process-control section paper developed part project apply methodology design distributed computing system navigation gobioff shun-tak leung google file system proceedings acm symposium airplane details work discussed natural components proof rule faulttolerant actions restrictive case exist fault-tolerant actions behave correctly proof proved relative completeness rule success application derived suggest restrictive proof correct fault-tolerant action conclusions methodology constructing fault-tolerant systems based notion fail-stop processor--a processor simple welldefined failure-mode operating characteristics fail-stop processors appealing abstract machines program approximated real hardware shown axiomatic program verification techniques extended proving correctness programs written fail-stop processors programmer argue convincingly correctness program post facto important programmer develop fault-tolerant program proof hand-in-hand leading advocated computing weakest precondition recovery protocol simple mechanical determining program variables stored stable storage constructing replete proof outline similarly defines intermediate states visible failure states recovery protocol acm transactions computer systems vol august fail-stop processors acknowledgments work benefited discussions andrews conway constable dijkstra lamport levin reitman wall worona process-control application suggested kemp comfort kushner ibm fsd owego gries made helpful comments earlier draft paper referees careful reading paper avizienis fault-tolerant systems ieee trans comput dec barlow proschan mathematical theory reliability wiley york denning fault-tolerant operating systems comput surv dec dijkstra discipline programming prentice-hall engiewood cliffs gray notes data base operating systems operating systems advanced lecture notes computer science vol springer-verlag york gries science programming springer-verlag york harter bernstein proving real time properties programs temporal logic proc sospasilomar california dec hoare axiomatic basis computer programming commun acm oct hoare wirth axiomatic definition programming language pascal acta inf hopkins smith lala ftmp--a highly reliable fault-tolerant multiprocessor aircraft proc ieee oct lamport time clocks ordering events distributed system commun acm july lamport time timeout fault-tolerant distributed systems tech rep sri int june lamport shostak pease byzantine generals problem acm trans program lang syst july lampson atomic transactions distributed systems--architecture implementation lecture notes computer science operating systems principles sosp pages bolton landing lake george york october garth gibson redundant disk arrays reliable parallel secondary storage phd thesis california berkeley garth gibson david rochberg jim zelenka david nagle khalil amiri fay chang eugene feinberg howard gobioff chen lee berend ozceri erik riedel file server scaling networkattached secure disks proceedings joint international conference measurement modeling computer systems sigmetrics performance pages seattle washington june jason goldberg spatially local failures caused thermal asperity personal communication jason goldberg seagate research february jim gray census tandem system availability technical report tandem computers jim gray catharine van ingen empirical measurements disk failure rates error rates microsoft technical report december jim gray andreas reuter transaction processing concepts techniques morgan kaufmann roedy green eide controller flaws version http mindprod jgloss eideflaw html february edward grochowski emerging trends data storage magnetic hard disk drives datatech september weining kalbarczyk ravishankar iyer zhenyu yang characterization linux kernel behavior error proceedings international conference dependable systems networks dsnpages san francisco california june haryadi gunawi nitin agrawal andrea arpaci-dusseau remzi arpaci-dusseau jiri schindler deconstructing commodity storage clusters proceedings annual international symposium computer architecture isca pages madison wisconsin june robert hagmann reimplementing cedar file system logging group commit proceedings acm symposium operating systems principles sosp austin texas november val henson history unix file systems http infohost nmt val slides pdf dave hitz james lau michael malcolm file system design nfs file server appliance proceedings usenix winter technical conference usenix winter san francisco california january mei-chen hsueh timothy tsai ravishankar iyer fault injection techniques tools ieee computer yiqing huang zbigniew kalbarczyk ravishankar iyer dependability analysis cache-based raid system fast distributed simulation ieee symposium reliable distributed systems pages gordon hughes joseph murray reliability security raid storage systems archives sata disk drives acm transactions storage february intel corp ibm corp device driver hardening http hardeneddrivers sourceforge net kanawati kanawati abraham ferrari tool validation system dependability properties international symposium fault tolerant computing ftcspages boston massachusetts hannu kari latent sector faults reliability disk arrays phd thesis helsinki technology september hannu kari saikkonen lombardi detection defective media disks ieee international workshop defect fault tolerance vlsi systems pages venice italy october jeffrey katcher postmark file system benchmark technical report trnetwork appliance october mohamed kaniche romano zbigniew kalbarczyk ravishankar iyer karcich hierarchical approach dependability analysis commercial cache-based raid storage architecture twenty-eighth annual international symposium fault-tolerant computing pages june steve kleiman vnodes architecture multiple file system types sun unix proceedings usenix summer technical conference usenix summer pages atlanta georgia june john kubiatowicz david bindel patrick eaton yan chen dennis geels ramakrishna gummadi sean rhea westley weimer chris wells hakim weatherspoon ben zhao oceanstore architecture globalscale persistent storage proceedings international conference architectural support programming languages operating systems asplos pages cambridge massachusetts november james larus singularity operating system seminar wisconsin madison levin cohen corwin pollack wulf policy mechanism separation hydra proceedings acm symposium operating systems principles sosp pages texas austin november blake lewis smart filers dumb disks nsic osd working group meeting april stan markov random fields gibbs distributions markov random field modeling image analysis chapter springer-verlag michael luby michael mitzenmacher amin shokrollahi daniel spielman volker stemann practical loss-resilient codes proceedings twenty-ninth annual acm symposium theory computing stoc pages paso texas wei lun kao ravishankar iyer dong tang fine fault injection monitoring environment tracing unix 
system behavior faults ieee transactions software engineering pages marshall mckusick william joy sam leffler robert fabry fast file system unix acm transactions computer systems august marshall kirk mckusick willian joy samuel leffler robert fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april larry mcvoy carl staelin lmbench portable tools performance analysis proceedings usenix annual technical conference usenix san diego california january needham hartley theory practice operating system design proceedings acm symposium operating system principles pages princeton jersey william norcutt iozone filesystem benchmark http iozone john ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference anaheim june john ousterhout herve costa david harrison john kunze mike kupfer james thompson trace-driven analysis unix bsd file system proceedings acm symposium operating system principles sosp pages orcas island washington december arvin park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- department computer science princeton november swapnil patil anand kashyap gopalan sivathanu erez zadok in-kernel integrity checker intrusion detection file system proceedings annual large installation system administration conference lisa atlanta georgia november david patterson aaron brown pete broadwell george candea mike chen james cutler patricia enriquez armando fox emre kiciman matthew merzbacher david oppenheimer naveen sastry william tetzlaff jonathan traupman noah treuhaft recovery oriented computing roc motivation definition techniques case studies technical report csd- berkeley march david patterson garth gibson randy katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod pages chicago illinois june jon postel rfc transmission control protocol september ftp ftp rfc-editor in-notes rfc txt august vijayan prabhakaran file system fingerprinting semantically-smart disk systems master thesis wisconsin madison vijayan prabhakaran andrea arpaci-dusseau remzi arpacidusseau analysis evolution journaling file systems proceedings usenix annual technical conference usenix pages anaheim california april vijayan prabhakaran andrea arpaci-dusseau remzi arpacidusseau model-based failure analysis journaling file systems proceedings international conference dependable systems networks dsnpages yokohama japan june vijayan prabhakaran lakshmi bairavasundaram nitin agrawal haryadi gunawi andrea arpaci-dusseau remzi arpacidusseau iron file systems proceedings acm symposium operating systems principles sosp pages brighton united kingdom october feng qin joseph tucek jagadeesan sundaresan yuanyuan zhou treating bugs allergies safe method survive software failures proceedings acm symposium operating systems principles sosp brighton united kingdom october david redell yogen dalal thomas horsley hugh lauer william lynch paul mcjones hal murray stephen purcell pilot operating system personal computer communications acm february hans reiser reiserfs namesys peter ridge gary field book scsi starch june martin rinard christian cadar daniel dumitran daniel roy tudor leu william beebe enhancing server availability security failure-oblivious computing proceedings symposium operating systems design implementation osdi san francisco california december drew roselli jacob lorch thomas anderson comparison file system workloads proceedings usenix annual technical conference usenix pages san diego california june mendel rosenblum john ousterhout design implementation log-structured file system acm transactions computer systems february jerome saltzer david reed david clark end-to-end arguments system design acm transactions computer systems november russel sandberg design implementation sun network file system proceedings usenix summer technical conference pages berkeley june stefan savage john wilkes afraid frequently redundant array independent disks proceedings usenix annual technical conference usenix pages san diego california january jiri schindler experienced severe performance degradation identified problem disk firmware disk drives reprogrammed fix problem personal communication schindler emc july steven schlosser gregory ganger mems-based storage devices standard disk interfaces square peg round hole proceedings usenix symposium file storage technologies fast pages san francisco california april fred schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december thomas schwarz qin xin ethan miller darrell long andy hospodor spencer disk scrubbing large archival storage systems proceedings annual meeting ieee international symposium modeling analysis simulation computer telecommunication systems mascots volendam netherlands october margo seltzer keith bostic marshall kirk mckusick carl staelin implementation log-structured file system unix proceedings usenix winter technical conference usenix winter pages san diego california january sandeep shah jon elerath disk drive vintage effect reliability proceedings annual reliability maintainability symposium rams pages sandeep shah jon elerath reliability analysis disk drive failure mechanisms proceedings annual reliability maintainability symposium rams pages january siewiorek hudak suh segal development benchmark measure system robustness proceedings international symposium fault-tolerant computing ftcstoulouse france june muthian sivathanu lakshmi bairavasundaram andrea arpacidusseau remzi arpaci-dusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december muthian sivathanu vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau improving storage system availability graid proceedings usenix symposium file storage technologies fast pages san francisco california april muthian sivathanu vijayan prabhakaran andrea arpaci-dusseau remzi arpaci-dusseau improving storage system availability graid acm transactions storage tos muthian sivathanu vijayan prabhakaran florentina popovici timothy denehy andrea arpaci-dusseau remzi arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast pages san francisco california april david solomon inside windows microsoft programming series microsoft press edition christopher stein john howard margo seltzer unifying file system protection proceedings usenix annual technical conference usenix boston massachusetts june lex stein stupid file systems tenth workshop hot topics operating systems hotos sante mexico june adan sweeney doug doucette wei curtis anderson mike nishimoto geoff peck scalability xfs file system proceedings usenix annual technical conference usenix san diego california january michael swift brian bershad henry levy improving reliability commodity operating systems proceedings acm symposium operating systems principles sosp bolton landing lake george york october nisha talagala remzi arpaci-dusseau dave patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley nisha talagala david patterson analysis error behaviour large storage system ieee workshop fault tolerance parallel distributed systems san juan puerto rico april data clinic hard disk failure http dataclinic hard-diskfailures htm transaction processing council tpc benchmark standard specification revision technical report tsai iyer measuring fault tolerance ftape fault injection tool international conference modeling techniques tools computer performance evaluation pages september theodore discussion file system robustness cheap hardware failure subject reiser plugins http ussg hypermail linux kernel html theodore discussion soft updates subject soft update journaling http lkml lkml stephen tweedie journaling linux ext file system fourth annual linux expo durham north carolina stephen tweedie ext journaling file 
system olstrans sourceforge net release ols -ext ols -ext html july werner vogels file system usage windows proceedings acm symposium operating systems principles sosp pages kiawah island resort south carolina december larry wang mike sullivan jim chao thermal asperities sensitivity particles methodology test results journal tribology april john wehman peter den haan enhanced ide fast-ata faq http thef-nym sci kun cgi-pieterh atazip atafq html glenn weinberg solaris dynamic file system http members visi net thedave sun dynfs pdf john wilkes richard golding carl staelin tim sullivan autoraid hierarchical storage system acm transactions computer systems february junfeng yang sar paul twohey cristian cadar dawson engler automatically generating malicious disks symbolic execution ieee security privacy berkeley california junfeng yang paul twohey dawson engler madanlal musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october zhou costa smith file system tracing package berkeley unix proceedings usenix summer technical conference usenix summer pages salt lake city utah june 
vol springer-verlag york lampson sturgis crash recovery distributed data storage system published owicki lamport proving liveness properties concurrent programs acm trans program lang syst july pease shostak lamport reaching agreements presence faults acm april pnueli temporal semantics concurrent programs semantics concurrent computation lecture notes computer science vol springer-verlag york randell lee treleaven reliability issues computing system design comput surv june schlichting axiomatic verification enhance software reliability thesis dept comput sci cornell univ jan schlichting schneider understanding asynchronous message passing proc acm sigact-sigops symp principles distributed computing ottawa canada aug acm york schneider synchronization distributed programs acm trans program lang syst apr schneider fail-stop processors digest papers spring compcon san francisco calif mar ieee computer society york schneider schlichting fault-tolerant process control software acm transactions computer systems vol august schlichting schneider proc eleventh ann int syrup fault-tolerant computing portland maine june ieee computer society york siewiorek swarz theory practice reliable system design digital press bedford mass wensley wensky lamport goldberg green levitt melliarsmith shostak weinstock sift design analysis faulttolerant computer aircraft control proc ieee oct received november revised july accepted february acm transactions computer systems vol august 
secure untrusted data repository sundr jinyuan maxwell krohn david mazi eres dennis shasha nyu department computer science abstract sundr network file system designed store data securely untrusted servers sundr lets clients detect attempts unauthorized file modification malicious server operators users sundr protocol achieves property called fork consistency guarantees clients detect integrity consistency failures long file modifications implementation performs comparably nfs worse offering significantly stronger security introduction sundr network file system addresses longstanding tension data integrity accessibility protecting data viewed problem building fence storage servers limiting number people access disabling unnecessary software remotely exploitable staying current security patches approach drawbacks experience shows people frequently build high fences entrust fences administrators completely trustworthy important high fences inconvenient restrict ways people access update manage data tension evident free software source code repositories free software projects involve geographically dispersed developers committing source internet making impractical fend attackers firewalls hosting code repositories requires palette tools cvs ssh remotely exploitable bugs worse projects rely third-party hosting services centralize responsibility large numbers independent code repositories sourceforge net hosts cvs repositories mit lab software packages packages bundled operating system distributions meaningful audit compromising sourceforge attacker introduce subtle vulnerabilities software eventually run thousands millions machines concerns mere academic exercise debian gnu linux development cluster compromised unauthorized attacker sniffed password kernel vulnerability gain superuser access debian primary cvs web servers detecting break-in administrators forced freeze development days employed manual ad-hoc sanity checks assess extent damage similar attacks succeeded apache gnome popular projects hope invulnerable servers developed sundr network file system reduces trust storage servers place sundr cryptographically protects file system contents clients detect unauthorized attempts change files contrast previous byzantine-fault-tolerant file systems distribute trust assume threshold fraction honest servers sundr vests authority write files users public keys malicious user gains complete administrative control sundr server convince clients accept altered contents files lacks permission write security properties sundr creates options managing data sundr organizations outsource storage management fear server operators tampering data sundr enables options data backup recovery disaster sundr server recover file system data untrusted clients file caches clients cryptographically verify file system state indifferent data recovered untrusted clients resided untrusted server paper details sundr file system design implementation describe sundr security protocol present prototype implementation performance generally comparable popular osdi symposium operating systems design implementationusenix association nfs file system software development workload microbenchmarks results show applications cvs benefit sundr strong security guarantees paying digestible performance penalty setting sundr file system interface remote storage nfs network file systems secure source code repository instance members project mount remote sundr file system directory sundr sundr cvsroot cvs repository checkouts commits place sundr ensuring users detect attempts hosting site tamper repository contents figure shows sundr basic architecture applications access file system client software internally translates system calls series fetch modify operations fetch means retrieving file contents validating cached local copy modify means making file system state visible users fetch modify turn implemented terms sundr protocol rpcs server section explains protocol section describes server design set sundr server runs server software networked machine dedicated sundr disks partitions server host file systems create file system generates public private superuser signature key pair public key server keeping private key secret private key exclusive write access root directory file system directly indirectly access file root privileges confined file system sundr server hosts multiple file systems superusers single person write access files user sundr file system signature key establishing account users exchange public keys superuser superuser manages accounts superuser-owned file root directory file system sundr users lists users public keys numeric ids sundr group designates groups membership mount file system superuser public key command-line argument client give client access private key sundr equally manage keys groups flexible certificate schemes system requires users validate keys group membership paper term user desigrpc block store fetch cache layer consistency server server application client syscall security layer modify figure basic sundr architecture nate entity possessing private half signature key mapped user sundr users file depending context person owns private key client key act behalf user sundr assumes user aware operation performed implementation client remembers operation performed behalf user move clients user private key operation performed behalf concisely version number alternatively person employ multiple user ids possibly public key clients assigning file permissions personal group sundr architecture draws important distinction administration servers administration file systems administer server private superuser keys fact security key pairs generated separate trusted machines private keys reside server memory important keys superuser key stored line floppy disk encrypted passphrase sundr protocol sundr protocol lets clients detect unauthorized attempts modify files attackers control server server behaves correctly fetch reflects authorized modifications happened call property fetch-modify consistency server dishonest clients enforce slightly server public key prevent network attackers framing honest servers server key irrelevant sundr security compromised servers formally irreflexive partial order preserves temporal order non-concurrent operations linearizability orders operations client orders modification respect operation file osdi symposium operating systems design implementation usenix association weaker property called forkconsistency intuitively fork consistency dishonest server fetch user miss modify user detect attack subsequent operation perpetuate deception server fork user views file system put equivalently client accepts modification performed modification users identical fetch-modify-consistent views file system formally fork consistency assuming digital signatures collision-resistant hash function proven sundr protocol achieves violation fork consistency means underlying cryptography broken implementation deviated protocol flaw mapping high-level unix system calls low-level fetch modify operations order discuss implications fork consistency describe sundr start simple straw-man file system achieves fork consistency cost great inefficiency section propose improved system reasonable bandwidth requirements called serialized sundr section finally relax serialization requirements arrive concurrent sundr system built section straw-man file system roughest approximation sundr straw-man file system avoid concurrent operations system consume unreasonable amounts bandwidth computation server maintains single untrusted global lock file system fetch modify file user acquires lock performs desired operation releases lock long server honest operations totally ordered operation completes begins straw-man file server stores complete ordered list fetch modify operation performed operation digital signature user performed signature covers operation complete history operations precede operations history sig mod user fetch fetch 
user sig sig user mod sig fetch sig user buser fetch modify file client acquires global lock downloads entire history file system validates user recent signature client checks user previous operation downloaded history user operation file system client traverses operation history construct local copy file system modify encountered client additionally checks operation permitted user group files validate signing user file owner group checks succeed client appends operation list signs history sends server releases lock operation modification appended record contents files directories informally malicious server convince client file modification server send signed history assuming server users keys forge signatures modifications clients accept signed authorized user server trick users signing inappropriate histories concealing users previous operations instance happen operation history server failed show user recent modification file users sign histories user user sig sig sig sig fetch sig sig sig sig user user user user buser auser buser fetch mod fetch mod mod fetch user fetch history prefix clients check user previous operation history point sign extensions history sign extensions attack users enjoyed fetch-modify consistency attack users forked suppose server acts collusion malicious users possess signature keys compromised users restrict analysis histories signed honest uncompromised users similar forking property holds honest users sign incompatible histories subsequent operations detecting problem server extend sign compromised users histories change files compromised users write reosdi symposium operating systems design implementationusenix association maining files modified honest users histories continue fork consistent implications fork consistency fork consistency strongest notion integrity on-line trusted parties suppose user line modifies file line line reads file doesn accessed file system detect attack server simply discards fork consistency implies type undetectable attack server file integrity consistency communicate future file system operations detect attack fork consistency leverage trusted parties line gain stronger consistency fetch-modify consistency instance section sundr server consists programs block store handling data consistency server small amount state moving consistency server trusted machine trivially guarantees fetch-modify consistency problem trusted machines worse connectivity availability untrusted bound window inconsistency placing trusted machine critical path time stamp box permission write single file box simply update file sundr seconds users box updates partitioned past seconds boxes replicated byzantine fault tolerance replica updating single file alternatively direct client-client communication leveraged increase consistency users write login logout records current network addresses files find continuously exchange information latest operations malicious server disrupt network communication clients unable fork file system state on-line clients deem malicious network partitions warrant service delays face client failures conservatively pause file access communication outages serialized sundr straw-man file system impractical reasons record ship complete file system operation histories requiring enormous amounts bandwidth storage serialization operations global lock impractical multi-user network file system subsection explains sundr solution problem describe simplified file system serializes operations global lock respects similar sundr subsection explains sundr lets clients execute non-conflicting operations concurrently signing operation histories strawman file system sundr effectively takes approach signing file system snapshots roughly speaking users sign messages tie complete state files mechanisms files writable user group efficiently aggregated single hash called i-handle hashtrees i-handle tied latest version i-handle version vectors data structures delving protocol details begin describing sundr storage interface data structures recent file systems sundr names on-disk data structures cryptographic handles block store indexes persistent data structures -byte shahashes making server kind large high-performance hash table believed computationally infeasible find data blocks shahash client requests block hash check integrity response hashing incidental benefit hash-based storage blocks common multiple files stored sundr stores messages signed users indexed hash public key index number distinguish multiple messages signed key figure shows persistent data structures sundr stores indexes hash algorithm computing i-handles file identified principal i-number pair principal user group allowed write file i-number perprincipal inode number directory entries map file names principal i-number pairs per-principal data structure called i-table maps i-number inode user i-tables map number hash inode call file i-hash group i-tables add level indirection mapping group i-number user i-number indirection user perform multiple successive writes group-owned file updating group i-handle inodes shahashes file data blocks indirect blocks i-table stored -tree internal nodes osdi symposium operating systems design implementation usenix association i-handle group maps offset data i-handle group i-table maps user user metadata inode maps i-hash user i-table data block locore main inode maps directory block figure user group i-handles i-handle root hash tree user group i-table denotes shawhile denotes recursive application shato compute root hash tree group i-table maps group inode numbers user inode numbers user i-table maps user inode numbers i-hashes i-hash hash inode turn hashes file data blocks shahashes children forming hash tree hash -tree root i-handle block store blocks requested shahash user i-handle client fetch verify block file user i-table recursively requesting intermediary blocks question obtain verify user latest i-handle protocol i-handles stored digitally-signed messages version structures shown figure version structure signed user structure user i-handle addition optionally i-handles groups user belongs finally version structure version vector consisting version number user group system user performs file system operation client acquires global lock downloads latest version structure user group call set version structures version structure list vsl vsl transfer elided users groups changed version structures user operation client computes version structure potentially updating i-handles setting version numbers reflect current state file system specifically set i-handles fetch structure i-table i-table version vector signature version figure version structure group i-handle client simply copies previous i-handle changed modify client computes includes i-handles groups i-tables modifying client sets version vector reflect version number vsl entry version structure principal user group letz denote version number version vector entry principal ify entry vsl version structure latest i-handle set finally client bumps version numbers reflect i-handles sets osdi symposium operating systems design implementationusenix association siga sig sig sig sig prime figure signed version structures forking attack i-handle group i-handle sets client checks vsl consistency version structures define iff check consistency client verifies vsl previous version structure set vsl entries combined totally ordered user signs version structure sends server commit rpc server adds structure vsl retires entries updated i-handles point client releases file system lock figure revisits forking attack end section showing 
version vectors evolve sundr version structure signed user reflects highest version number user increments version number reflect recent i-handle violation consistency users sign incompatible version structures structures negationslash negationslash server performs forking attack step user updates i-handle prime aware change result version structures signed incompatible straw-man file system users signed incompatible version structures sign compatible operations detecting attack proven earlier work optimization worth mentioning sundr amortizes cost recomputing hash trees operations shown figure i-handle hash tree root small log made i-table change log avoids users fetch i-table blocks group i-table maps user group i-handle prime change log figure i-table group showing change log prime recent i-table applying log prime yields re-validating cached file changed hash tree root computed concurrent sundr version structures sundr detect inconsistency serialized sundr conservative prohibits client wait previous client version vector computing signing reflect version numbers operations proceed concurrently time client wait reads file process writing update certificates sundr solution concurrent updates users pre-declare fetch modify operation receiving vsl server signed messages called update certificates ify current vsl entry update certificate operation version number parenleftbig pipelining multiple updates parenrightbig hash vsl entry possibly empty list modifications perform modification delta types set file user i-hash set group file group user set delete entry directory user group avoid waiting clients event read-after-write conflict turns impossible untrusted servers single signed message atomically switch file states server conceal change initially apply long forking file system users longer updates osdi symposium operating systems design implementation usenix association pre-allocate range group i-numbers pointing unallocated user i-numbers client sends update certificate server update rpc server replies vsl list pending operations reflected vsl call pending version list pvl note fetch modify operations require update rpcs fetches deltas rpc refers updating vsl file contents executing complex system calls rename single update rpc deltas affecting multiple files directories possibly i-tables honest server totally orders operations arrival order update rpcs operation reflected vsl pvl returned update rpc happened conversely reflected vsl pvl happened happened server mounted forking attack signing update certificate client predict version vector version structure vector depend concurrent operations clients server precisely operations forthcoming version structure reflect update certificate server calculates forthcoming version structure i-handle unsigned version structure paired update certificate pvl pvl list update certificate unsigned version structure pairs algorithm computing version structure begins serialized sundr principal set entry vsl version vector incremented reflect pending updates pvl including user version numbers simple update certificate signed user set groups situation complicated fact operations commit order slow fast clients update i-table pvl entry updating group i-table increment pvl entry happened initialized determine increment version number comparing pvl entry unsigned version vector call lscript iflscript negationslash set result version vector obtain serialized sundr waiting previous version structures receiving vsl pvl client ensures vsl unsigned version structures pvl version structure totally ordered checks conflicts operations pvl change files client fetching group i-tables modifying client simply signs version structure sends server inclusion vsl update conflicts client fetching file pvl modification file signifies read-after-write conflict case client commits version structure waits fetched files committed vsl returning application fetchpending rpc lets clients request version structure server arrives trickier situation occurs pvl modification group i-handle client wishes modify signifying write-after-write conflict client modifying group i-table recompute i-handle operations pvl affect operation pvl happened version structure call handle reflect operations pvl hand server behaved incorrectly forthcoming version structures pvl entries incompatible case critical launder operations alerted people server misbehavior recall clients check pvl read-afterwrite conflicts client sees conflicting modification pvl wait vsl entry incorporated change problem remains malicious server prematurely drop entries pvl case client incorrectly fetch modifications reflected properly committed solution incorporate modifications pvl reflected record current contents pvl field version structure clients detect missing pvl entries notice entries referenced version structure include full pvl large simply records pvl entry user performing operation user version number operation hash expected version structure i-handles omitted applies pvl simply appending change log i-handle efficient rehashing i-table saves fetching uncached portions i-table osdi symposium operating systems design implementationusenix association set directory add entry set set directory i-table user metadata sundr tmp inode i-table sundr tmp inode server signature version version update certificateu update certificate user signature metadata set add entry figure concurrent updates sundr tmp users figure shows users group modifying directory creates file creates sundr tmp directory group-writable files assume pending updates assume sundr tmp mapped group number user calculates i-hash file call allocates i-number call allocates i-number hold contents modified directory finally sends server update certificate declaring deltas mapping file i-hash addition entry directory re-mapping i-number similarly sends server update certificate creation file sundr tmp server orders update respond vsl pvl update send pvl reflecting updates apply modification directory computing i-handle incorporating directory entry ordinarily incorporate re-mapping directory remapping directory supersedes important subtlety protocol shown figure version structure hash forthcoming version structure i-handles ensures server surreptitiously drops update certificate pvl commits sees incorrect pvl forked version signature unsigned version lscript structure version structurecertificate update signature lscript figure pending update user reflected user version structure discussion sundr detects attacks resolve server compromise users find caching divergent copies directory tree resolving differences studied context optimistic file system replication invariably conflicts require applicationspecific reconciliation cvs users employ cvs merging facilities resolve forks sundr protocol leaves considerable opportunities compression optimization version structure signatures cover version vector users groups transmit entire vectors rpcs ordering entries mostto osdi symposium operating systems design implementation usenix association least-recently updated tail idle principals omitted client update rpc signing hash version vector hashing oldest newest clients pre-hash idle principals version numbers speed version vector signatures finally contents unsigned version structures pvl implicit based order pvl omitted server computes unsigned version structures deterministically based order receives update rpcs optimizations implemented sundr semantics differ traditional unix clients supply file modification inode change times modifying files allowing values prohibited unix time access directories sticky bit group-writable file sundr owned user unix owned group file owner field user wrote contrast unix disk quotas charge owner group-writable file writes users sundr block store enforced quotas charge user precisely blocks 
written user change owner file sundr sundr copy arbitrarily large files cost pointer manipulations due hash-based storage mechanism sundr implements chown creating copy file owned user group updating directory entry point copy requires write permission directory semantics hard links chown affects single link difference unix owner directory delete entries directory including non-empty subdirectories write permission unix users rename directories additionally allowing delete permission appreciably affect security similar vein users create multiple hard links directories confuse unix software situations types malformed directory structure interpreted equivalent legal duplicate directory entries counts sundr offer read protection confidentiality confidentiality achieved encrypted storage widely studied problem terms network latency sundr comparable polling network file systems sundr waits update rpc complete returning application file system call system call caused modifies fetched data hit cache synchronous round trip required commit background fsync behavior similar systems nfs makes access rpc open writes data back server close note callbackor lease-based file systems achieve round trips server committed notifying clients cache invalidations file system implementation sundr client implemented user level modified version xfs device driver arla file system top slightly modified freebsd kernel server functionality divided programs consistency server handles update certificates version structures block store stores data update certificates version structures disk experiments paper block server consistency server ran machine communicating unix-domain sockets configured run machines communicate authenticated tcp connection file system client xfs device driver sundr designed whole-file caching file opened xfs makes upcall sundr client file data client returns identity local file cached copy data reads writes performed cached copy involvement sundr file closed flushed fsync modified xfs makes upcall client write data back server types upcalls xfs names directories request file attributes create delete files change metadata distributed xfs interface posed problems sundr xfs caches information local file bindings satisfy requests upcalls sundr requests require interaction consistency server security properties hold modified xfs invalidate cache tokens immediately writing back cached data ensure user-level client control protocol requires update rpc similarly changed xfs defeat kernel cache system calls require single interaction sundr consistency server result multiple kernel vnode operations xfs upcalls system call stat osdi symposium operating systems design implementationusenix association results xfs getnode upcalls directory lookups getattr system call require update rpc user-level client upcalls behalf system call check freshness handles separate times update rpcs eliminate unnecessary rpcs modified freebsd kernel count number system call invocations require interaction consistency server increment counter start system call takes pathname argument stat open readlink chdir sundr client memory-maps counter records xfs makes upcall change state file system counter changed client cached copies i-handles signature optimization cost digital signatures critical path sundr significant implementation esign signature scheme order magnitude faster popular schemes rsa experiments reported paper -bit public keys techniques require larger work factor break -bit rsa move verification critical path consistency server processes replies update rpc verifying signature update certificate verifies signature replying accepting rpcs users signature fails verify server removes update certificate pvl drops tcp connection forging client behavior acceptable faulty client send invalid signatures optimization consistency server verification signature overlap client computation clients similarly overlap computation network latency roughly half cost esign signature attributable computations depend message contents waiting reply update rpc client precomputes signature consistency server consistency server orders operations sundr clients maintains vsl pvl section addition polices client operations rejects invalid rpcs malicious user specifically version esign shown secure random oracle model parameter honest server fail crash recovery consistency server store vsl pvl persistent storage responding client rpcs current consistency server stores block server vsls pvls small relative size file system feasible non-volatile ram nvram block store implementation block storage daemon called bstor handles disk storage sundr clients interact directly bstor store blocks retrieve shahash consistency server bstor store signed update version structures sundr server signature keys lacks permission repair file system crash reason bstor synchronously store data disk returning clients posing performance challenge bstor heavily optimizes synchronous write performance bstor basic idea write incoming data blocks temporary log move blocks venti-like storage batches venti archival block store appends variable-sized blocks large append-only ide log disk indexing blocks shahash fast scsi disks bstor temporary log relaxes archival semantics venti allowing shortlived blocks deleted small window creation bstor maintains archival flavor supporting periodic file system snapshots temporary log bstor achieve low latency synchronous writes venti require index lookup ensure block duplicate bstor sector-aligns blocks temporary log temporarily wasting average half sector block avoid multiple writes sector cost disk rotation temporary log improves write throughput sustained load transferring blocks permanent log large batches bstor order index disk accesses bstor large in-memory cache recently blocks caches blocks temporary log avoid reading temporary log disk bstor special hardware section describe sundr performance improve bstor small amount nvram store update certificates interface bstor exposes rpcs sundr clients osdi symposium operating systems design implementation usenix association store header block retrieve hash vstore header pubkey block vretrieve pubkey time decref hash snapshot store rpc writes block header stable storage bstor copy block header information encapsulating block owner creation time fields concert encoding compression retrieve rpc retrieves block store shahash returns header storedwith block vstore vretrieve rpcs store retrieve signed blocks signed blocks indexed public key small index number vretrieve default fetches recent version signed block supplied timestamp optional argument vretrieve returns newest block written time decref short decrement count informs store block shahash discarded sundr clients decref discard temporary files short-lived metadata bstor deletion semantics conservative block stored bstor establishes short window minute default deleted client stores decrefs block window bstor marks block garbage permanently store clients store block dereference window block marked permanent administrator issue snapshot rpc periodically create coherent file system image clients revert case accidental data disruption receiving rpc bstor simply immunizes newly-stored blocks future decref flags stored permanent log snapshot vretrieve time argument designed browsing previous file system state functionality implemented client index bstor index system locates blocks permanent log keyed shahashes ideal index simple in-memory hash table mapping -byte shablock hashes -byte log disk offsets assume average block stored system index roughly capacity log disk present ratio 
disk memory commodity components convinced memory hard disks future venti strategy striping diskresident hash table multiple high-speed scsi disks bstor hashes -byte shahashes index-disk-id index-disk-offset pairs disk offsets point sector-sized on-disk data structures called buckets index-entries sorted shahash index-entries turn map shahashes offsets permanent data log index-entry written read disk bstor stores inmemory lru cache bstor accesses index system venti answering retrieve rpcs miss block cache bstor moves data temporary permanent log access index system block check block duplicate write index entry block committed permanent log cases bstor sorts disk accesses index disks service batch requests disk arm sweep optimizations bstor writes blocks permanent log order arrived randomly reordering blocks hinder sequential read performance large files data management recover crash unclean shutdown system recreates index consistent permanent log starting checkpoint index recovery server updates index lazily storing blocks permanent log bstor processes temporary log storing fresh blocks permanent log updating index appropriately venti authors argue archival storage practical ide disk capacity growing faster users generate data users fit paradigm bstor alternatively modified support mark-and-sweep garbage collection general idea copy reachable blocks log disk recycle disk disks bstor respond rpcs garbage collection performance primary goal testing sundr ensure security benefits high price relative existing file systems section compare sundr performance nfs perform microbenchmarks explain application-level reosdi symposium operating systems design implementationusenix association sults support claims block server outperforms venti-like architecture setting experimental setup carried experiments cluster ghz pentium machines running freebsd machines connected fast ethernet ping times block server microbenchmarks additionally connected block server client gigabit ethernet machine running bstor ram array disks seagate cheetah scsi drives spin rpm index western digital caviar rpm eide drives permanent temporary logs microbenchmarks bstor goals evaluating bstor quantify raw performance justify design improvements relative venti experiments configured bstor scsi disks space indexing hopes maintain good index performance overflow buckets index remain half full configuration usable index -byte index entries bstor accommodate permanent data flow control fairness bstor allowed clients make outstanding rpcs purposes microbenchmarks disabled bstor block cache enabled index cache entries circular temporary log filled experiments measured bstor performance storing fetching batch unique blocks figure shows averaged results runs block experiment cases standard deviations average results results show bstor absorb bursts blocks fast ethernet rates sustained throughput limited bstor ability shuffle blocks temporary permanent logs bottleneck storeing blocks temporary log cpu future versions bstor eliminate unnecessary memcpys achieve throughput hand bstor process temporary log fast read index disks room improvement disks faster index disks compare venti-like system implemented venti-like store mechanism venti store bstor operation store burst store sustained venti store retrieve random cold index cache retrieve sequential cold index cache retrieve sequential warm index cache figure bstor throughput measurements block cache disabled checks block existence index stores block permanent log found venti store entails access index disks results show venti store achieve store burst throughput sustained throughput figure presents read measurements bstor client reads blocks order written sequential reads bstor seek permanent log disk throughput case limited per-block cost locating hashes index disks increases warm index cache randomly-issued reads fare poorly warm index cache bstor seek permanent log context sundr slow random retrieves affect system performance client aggressively caches blocks reads large files sequentially finally latency bstor rpcs largely function seek times store rpcs require seeks return venti store returns seek index disk cost sequential retrieves hit miss index cache return seek log disk takes random retrieves hit miss index cache return cryptographic overhead sundr clients sign verify version structures update certificates -bit esign keys implementation based gnu multiprecision library version complete signatures approximately verify precomputing signature requires roughly finalizing precomputed signature observed measurements vary pentium factor well-controlled micro-benchmarks comparison optimized version rabin signature scheme -bit keys running osdi symposium operating systems design implementation usenix association hardware compute signatures verify end-to-end evaluation end-to-end experiments compare sundr nfs nfs servers running hardware show nfs light nfs experiments run fast scsi disks sundr indexes slower larger eide log disks include nfs results nfs write-through semantics sundr nfs sundr write modified file data disk returning close system call nfs offer guarantee finally section sundr clients wait consistency server write small pieces data vsls pvls stable storage consistency server storing pvls client critical path present result sets consistency servers running flushes secondary storage intend mode flushes disabled simulate consistency server nvram application results shown average runs relative standard deviations noted lfs small file benchmark lfs small file benchmark tests sundr performance simple file system operations benchmark creates files reads back deletes modified benchmark slightly write random data files writing file times give sundr hash-based block store unfair advantage figure details results client accessing file system create phase benchmark single file creation entails system calls open read close sundr nvram open call involves serialized rounds consistency protocol costs write call noop file buffered close close call involves round protocol synchronous write file data block server client overlap entire sequence takes nvram round protocol takes approximately longer consistency server wait bstor flush unlike sundr nfs server wait disk seek creating file synchronously writes metadata seek costs fast scsi drives nfs create read unlink run time nfs nfs sundr sundr nvram figure single client lfs small file benchmark operations files random content file creation practice nfs requires service system calls create stage read phase benchmark sundr performs round consistency protocol open system call nfs client accesses server access rpc server data buffer cache point seeking required nfs contact server phase unlink stage benchmark clients issue single unlink system call file unlink sundr triggers round consistency protocol asynchronous write block server store updated i-table directory blocks sundr sundr nvram outperform nfs stage experiment nfs servers require synchronous disk seek file unlinked performed experiments multiple clients performing lfs small file benchmark concurrently directories results create phase reported figure phases benchmark show similar trends surprising result sundr scales nfs client concurrency increases limited tests nfs seek-bound single client case number seeks nfs servers require scale linearly number concurrent clients sundr latencies induced consistency protocol limit individual client performance latencies overlap clients act 
concurrently sundr disk accesses scalable sequential sector-aligned writes bstor temporary log group contention group protocol incurs additional overhead folding users group i-table directory characterized cost mechanism measurosdi symposium operating systems design implementationusenix association concurrent clients average run time nfs nfs sundr sundr nvram figure concurrent lfs small file benchmark create phase creations files relative standard deviation sundr concurrent clients case ing workload high degree contention group-owned directory ran micro-benchmark simultaneously created files groupwritable directory clients concurrent create required client re-map group i-number group i-table apply user copy directory clients average sundr nvram nfs comparison ran benchmark concurrently separate directories required average sundr nvram nfs results suggests contention incurs noticeable cost sundr performance case line nfs real workloads figure shows sundr performance untaring configuring compiling installing cleaning emacs distribution experiment sundr client total blocks block server totaled size duplicate blocks bstor discards account data client successfully decrefed blocks total space savings end blocks totaled permanent storage sundr faster nfs competitive nfs stages emacs build process sundr sluggish performance install phase artifact implementation serializes concurrent xfs upcalls simplicity correctuntar config make install clean run time nfs nfs sundr sundr nvram figure installation procedure emacs concurrent clients average run time nfs sundr nfs sundr nvram figure concurrent untar emacs tar ness concurrent xfs upcalls prevalent phase experiment due install command manipulation file attributes figure details performance untar phase emacs build client concurrency increases noted similar trends phases build process experiments suggest scalability sundr exhibited lfs small file benchmarks extends real file system workloads cvs sundr tested cvs sundr evaluate sundr performance source code repository experiment typical progression client imports arbitrary source tree test groffwhich files totaling clients check copy local disks commits groffwhich affects files lastly updates local copy figure shows results sundr fares badly commit phase cvs repeatedly opens memory maps unmaps closes osdi symposium operating systems design implementation usenix association phase sundr sundr nfs ssh nvram import checkout commit update figure run times cvs experiments seconds repository file times rapid succession open requires iteration consistency protocol sundr freebsd nfs apparently elides asynchronously performs access rpcs closely-spaced open calls cvs feasibly cache memory-mapped files point experiment single cvs client holds lock directory small change significantly improve sundr performance benchmark related work number non-networked file systems cryptographic storage data secret check integrity network file systems provide varying degrees integrity checks reduce integrity read sharing vulnerable consistency attacks sundr system provide well-defined consistency semantics untrusted server unimplemented previously published version sundr protocol groups address write-after-write conflicts byzantine fault-tolerant file system bfs replication ensure integrity network file system long server replicas uncompromised data read file system written legitimate user sundr contrast require replication place trust machines user client sundr weaker freshness guarantees bfs possibility malicious sundr server fork file system state users evidence on-line activity projects investigated storing file systems peer-to-peer storage systems comprised potentially untrusted nodes farsite spreads file system people unreliable desktop machines cfs secure read-only file system ivy read-write version cfs convinced re-order operations clients pond relies trusted core machines security distributing trust bfs-like sundr hash trees introduced verify file block integrity touching entire file system duchamp bfs sfsro tdb made hash trees comparing data checking integrity part larger collection data sundr version vectors detect consistency violations version vectors ficus detect update conflicts file system replicas secure partial orderings straw-man file system resembles timeline entanglement reasons temporal ordering system states hash chains conclusions sundr general-purpose multi-user network file system presents applications incorrect file system state server compromised sundr protocol provably guarantees fork consistency essentially ensures server behaves correctly failure detected communication users event consequences undetected server compromise limited concealing users operations forking point server tamper inject re-order suppress file writes measurements implementation show performance close popular nfs file system reducing amount trust server sundr increases people options managing data significantly improves security files acknowledgments michael freedman kevin daniel giffin frans kaashoek jinyang robert morris anonymous reviewers shepherd jason flinn material based work supported national science foundation nsf grant ccrmaxwell krohn partially supported nsf graduate fellowship david mazi eres alfred sloan research fellowship dennis shasha nsf grants iismcb- mcbreferences apache compromise report http apache info -hack html debian investigation report server compromises http debian news december osdi symposium operating systems design implementationusenix association atul adya william bolosky miguel castro gerald cermak ronnie chaiken john douceur jon howell jacob lorch marvin theimer roger wattenhofer farsite federated reliable storage incompletely trusted environment proceedings symposium operating systems design implementation pages december brian berliner cvs parellizing software development proceedings winter usenix colorado springs usenix matt blaze cryptographic file system unix acm conference communications computing security pages november miguel castro barbara liskov practical byzantine fault tolerance proceedings symposium operating systems design implementation pages orleans february frank dabek frans kaashoek david karger robert morris ion stoica wide-area cooperative storage cfs proceedings acm symposium operating systems principles pages chateau lake louise banff canada october acm dan duchamp toolkit approach partially disconnected operation proceedings usenix pages usenix january kevin frans kaashoek david mazi eres fast secure distributed read-only file system acm transactions computer systems february eu-jin goh hovav shacham nagendra modadugu dan boneh sirius securing remote untrusted storage proceedings tenth network distributed system security ndss symposium pages internet society isoc february maurice herlihy jeannette wing linearizability correctness condition concurrent objects acm transactions programming languages systems kallahalla riedel swaminathan wang plutus scalable secure file sharing untrusted storage usenix conference file storage technologies fast san francisco april james kistler satyanarayanan disconnected operation coda file system acm transactions computer systems umesh maheshwari radek vingralek build trusted database system untrusted storage proceedings symposium operating systems design implementation san diego october petros maniatis mary baker secure history preservation timeline entanglement proceedings usenix security symposium san francisco august david mazi eres dennis shasha building secure file systems byzantine storage proceedings annual acm sigact-sigops symposium principles distributed computing pages july david mazi eres dennis shasha building secure file systems byzantine storage technical report nyu department computer science ralph merkle digital signature based conventional encryption function carl pomerance editor advances cryptology crypto volume lecture notes computer science pages berlin 
springer-verlag ethan miller darrell long william freeman benjamin reed strong security distributed file systems proceedings ieee international performance computing communications conference pages phoenix april athicha muthitacharoen robert morris thomer gil benjie chen ivy read write peer-to-peer file system proceedings symposium operating systems design implementation pages december tatsuaki okamoto jacques stern uniform density power residues provable security esign advances cryptology asiacrypt pages page guy heidemann ratner reiher goel kuenning popek perspectives optimistically replicated peer-to-peer filing software practice experience february stott parker gerald popek gerard rudisin allen stoughton bruce walker evelyn walton johanna chow david edwards stephen kiser charles kline detection mutual inconsistency distributed systems ieee transactions software engineering semay sean quinlan sean dorward venti approach archival storage usenix conference file storage technologies fast monterey january david reed liba svobodova swallow distributed data storage system local network west janson editors local networks computer communications pages north-holland publ amsterdam michael reiter gong securing causal relationships distributed systems computer journal sean rhea patrick eaton dennis geels pond oceanstore prototype usenix conference file storage technologies fast san francisco april rosenblum ousterhout design implementation log-structured file system proceedings acm symposium operating systems principles pages pacific grove october acm russel sandberg david goldberg steve kleiman dan walsh bob lyon design implementation sun network filesystem proceedings summer usenix pages portland usenix sean smith tygar security privacy partial order time proceedings isca international conference parallel distributed computing systems pages las vegas october christopher stein john howard margo seltzer unifying file system protection proceedings usenix usenix june owen taylor intrusion gnome http mail gnome archives gnome-announce-list -march msg html march assar westerlund johan danielsson arla free afs client proceedings usenix freenix track orleans june usenix charles wright michael martino erez zadok ncryptfs secure convenient cryptographic file system proceedings pages june tatu onen ssh secure login connections internet proceedings usenix security symposium pages san jose july osdi symposium operating systems design implementation usenix association 
storage management caching past large-scale persistent peer-to-peer storage utility antony rowstron microsoft research george house guildhall street cambridge united kingdom antr microsoft peter druschel rice main street houston usa druschel rice abstract cccwcxd cpd ctd ctd ctd cpd ctdacpd cpd ctd cwct cpcvct cpd cpcvctd ctd cpd crcpcrcwcxd cxd btcbccb cpd cvctb crcpd ctctd ctctd ctd cxd ctd cpcvct cxd cxd ddbac btcbcc cxd cqcpd ctcs ctd cub cvcpd cxdecxd cvb ctd ctd cqcpd ctcs dactd cpdd ctd dbd cpcvct csctd cwcpd crd ctd cpd cxdactd acd ctd cxctd cxd ctd cxcrcpd acd ctd cpd crcpcrcwct cpcscscxd cxd cpd crd cxctd cpd acd ctd cwct btcbcc ddd ctd cpcvct csctd cpd acd ctd cpd ctcpcrcw cpd cxcvd ctcs cxcud cscxd cxcqd ctcs cxcsctd cxacctd cpd ctd cxcrcpd acd cpd ctcs cpd csctd dbcwd cxcsctd cxacctd cpd crcwctd crd ctd cwct acd ctb cxcsctd cxacctd cccwcxd cpd cxd cxcrcpd cpd cxcvd ctd acd ctd cpcvct csctd cpd dccxd cpd ctd cqcpd cpd crctd cwct cqctd acd ctd ctcs ctcpcrcw csctba dbctdactd cxcud cpcvct csct crcpd cpcrcxd cxctd cpd acd cxdectd ctd cxd ctdcd cxcrcxd cpcvct cpcs cqcpd cpd crcxd ctd cxd cvd cpcrctcud cqctcwcpdacxd csctd cwcxcvcw cvd cqcpd cpcvct cxb cxdecpd cxd cxczctdbcxd ctb cxcud cpd cxd acd ctd ctd cxd ctd crcpcrcwcxd cxd cxd cxdect cuctd crcw cscxd cpd crct cpd cqcpd cpd crct cwct ctd cpcsba cfct ctd ctd cpd ctdacpd cpd btcbccb dbcxd cpd ctd cwcpd cxd cxd cpcvct cpd cpcvctd ctd cpd crcpcrcwcxd ddd ctd bxdcd ctd cxdact cpcrctb csd cxdactd ctdcd ctd cxd ctd cwd cwcpd cwct ddd ctd cxd cxd cxdectd cuctd crcw cscxd cpd crctb cwcpd cxd cqcpd cpd crctd cwct ctd cpcs cud cpd acd ctd cpd cwcpd cxd cscxd cpddd cvd cpcrctcud csctcvd cpcscpd cxd ctd cud cpd crct cpd cwct cvd cqcpd cpcvct cxd cxdecpd cxd cxd crd ctcpd ctd cqctddd blbhb introduction ctctd ctctd ctd ctd cpd cxcrcpd cxd cwcpdact ctcrctd cqctctd cpd cxdectcs cwd cvcw acd cwcpd cxd cpd cxcrcpd cxd crcwcpd cpd ctd bzd ctd cpd byd ctctc ctd cjbdb beb bdbfclba cfcwcxd crcw cwct cpd ctd cxd cwcpd cqctctd cud crd ctcs cwct crd ddd cxcvcwd cxd ctd cpcxd ctcs cqdd cwctd cpd cxcrd cpd cpd cxcrcpd cxd ctctd ctctd ddd ctd cwcpdactd cpd ddcxd ctd ctd cxd ctcrcwd cxcrcpd cpd ctcrd cxczct csctcrctd cpd cxdectcs crd ctd cub cvcpd cxdecpd cxd cpcscpd cpd cxd cpd crcpd cpcqcxd cxd ddba ctctd ctctd ddd ctd crcpd cqct crcwcpd cpcrd ctd cxdectcs cpd cscxd cxcqd ctcs ddd ctd cxd dbcwcxcrcw cpd csctd cwcpdactcxcsctd cxcrcpd crcpd cpcqcxd cxd cxctd cpd ctd cxcqcxd cxd cxctd cpd cpd crd cxcrcpd cxd cxd ddd ctd cxcrba cccwctd cpd crd ctd cpd cyctcrd cpcxd ctcs cpd crd crd cxd ctctd ctctd cpd cxcrcpd cxd cpd csctd cpd cscxd cwct cxd ctd cpd ctd cxd ctd ctd crcw cpd cxcrcpd cxd cpd ddd ctd cjbdb beb bkb bdbfb bdbhb bebcclba cfct cpd csctdactd cxd btcbccb cpd ctd ctd cqcpd ctcsb ctctd ctctd cvd cqcpd cpcvct cxd cxd ddb dbcwcxcrcw cpcxd dacxcsct ctd cxd ctd crctb cwcxcvcw cpdacpcxd cpcqcxd cxd ddb crcpd cpcqcxd cxd cpd ctcrd cxd ddba cccwct btcbcc ddd ctd cxd crd ctcs csctd crd ctcrd ctcs cwct ctd ctd dbcwctd ctcpcrcw csct cxd crcpd cpcqd cxd cxd cxcpd cxd cpd cxd crd cxctd ctd ctd cxd ctd ctd cxctdact acd ctd cxd cpd ddb csctd cpdd cpd crd cxcqd cpcvct cwct ddd ctd cccwct btcbcc csctd cud ctd cub cvcpd cxdecxd dactd cpddd ctd dbd czba ctd ctcs acd ctd cpd ctd cxcrcpd ctcs cpcrd cxd csctd cud cpdacpcxd cpcqcxd cxd ddba cfcxd cwcxcvcw cqcpcqcxd cxd ddb cwct ctd csctd dactd dbcwcxcrcw acd cxd ctd cxb crcpd ctcscxd cscxdactd cxd ctd cvctd cvd cpd cwcxcr crcpd cxd dbd ctd cwcxd cpcsd cxd cxd cpd cxd ctd dbd crd ctcrd cxdacxd ddb cpdbb ctd crba cpcvct cxd cxd ddd cxczctc btcbcc cxd cpd cpcrd cxdact cud ctdactd cpd ctcpd bycxd cxd ctdcd cxd cwct cxd csct cpd cscxdactd cxd cxd cvctd cvd cpd cwddb dbd ctd cwcxd cpcsd cxd cxd cpd cxd cyd cxd cscxcrd cxd ctd crbab csctd cxd cwct ctd ctd cpcrcwcxctdact ctd cxd ctd crct cpd cwcxcvcw cpdacpcxd cpcqcxd cxd ddba cccwcxd cqdacxcpd ctd cwct ctctcs cud cwddd cxcrcpd cpd cpcvct ctb cscxcp ctcrd cqcpcrczd cpd cpd crcwcxdacpd cscpd cpbn cxczctdbcxd ctb cxd cqdacxcpd ctd cwct ctctcs cud ctdcd cxcrcxd cxd cxd ctd cwcxcvcw cpdacpcxd cpcqcxd cxd cpd cwd cvcwd cud cwcpd ctcs cscpd cpba cvd cqcpd cpcvct cxd cxd cpd cucpcrcxd cxd cpd ctd cwct cwcpd cxd cpcvct cpd cqcpd csdbcxcsd cwb cwd ctd cxd cxd cvd csctd cyd cxd cqd cxd crd ctd cwcpd dbd ctdccrctctcs cwct crcpd cpcrcxd cqcpd csdbcxcsd cpd cxd cscxdacxcsd cpd csctba cfcwcxd btcbcc abctd ctd cxd ctd cpcvct ctd dacxcrctd cxd ctd cpd cxcrd cscxabctd cud cwcpd crd dactd cxd cpd acd ctd ddd ctd bycxd ctd ctcs cxd btcbcc cpd cpd crcxcpd ctcs dbcxd cpd cxb cxd acd ctc cwcpd cxd cvctd ctd cpd ctcs cpd cwct cxd cwct acd ctb cxd ctd cxd cxd btcbccba cccwctd ctcud ctb acd ctd ctcs cxd btcbcc cpd cxd cpcqd cxd crct acd crcpd cqct cxd ctd ctcs cxd cxd ctd dbcxd cwct cpd acd ctc csba bycxd ctd crcpd cqct cwcpd ctcs cpd cwct dbd ctd cscxd crd ctd cxd cqdd cscxd cxcqd cxd cwct acd ctc ctd cxcpd cpd ddd ddb cpd csb cxcu ctcrctd cpd ddb csctcrd ddd cxd czctddba btd crcxctd cxd crcwctd crcpd ctcs cpd cjbebjcl ctd ctd cwcpd crd cxctd ctd ctd cpd ctd cxcpcqd ctcs cwct cpd cxcpd csctd bvd cxctd ctd ctd ctd cxctdact acd cpd ctcsb dbcxd cwcxcvcw cqb cpcqcxd cxd ddb csct cwcpd cxd ckcrd cxd cwct ctd dbd czay cwct crd cxctd cwcpd cxd ctcs cwct ctd ctd cpd cwct cxdact csctd cwcpd cwct ctd ctd ctcs acd ctba cccwct cqctd btcbcc csctd cpdactd ctcsb cpd dbctd cpd cwct cqctd ctd cpcvctd ctdccrcwcpd cvctcs dbcwcxd cxd crd cxctd ctd ctd cxd cvcpd cxd cwd cxcr 
cxd cwct cpd cqctd btcbcc csctd cxd cwct ddd ctd csctd cpd ctd cpd cxd ccd ctd cxctdact acd cxd btcbccb crd cxctd czd cxd acd ctc cpd csb cxcu ctcrctd cpd ddb cxd csctcrd ddd cxd czctddba btcbcc csd ctd dacxcsct cucpcrcxd cxd cxctd cud ctcpd crcwcxd cvb cscxd ctcrd czd czctdd cscxd cxb cqd cxd cpddctd cxd crcw cucpcrcxd cxd cxctd cpd ddb cwct cpd ctctd ctctd cqd cpd cwcpd btcbcc cxd cqcpd ctcs cxd cwct cqb cyctcrd crd ctd ctd ctcpd crcwba bycxd cpd ddb btcbcc cxd cxd ctd csctcs cpd cpd cpd crcwcxdacpd cpcvct cpd crd ctd cscxd cxcqd cxd cxd cxd cpd cpd cvctd ctd cpd acd ctd ddd ctd cxd cpd ctcs cwcpd ctd cxd ctd cpcrd cxd cpd cxd dbcxd crd dactd cxd cpd acd ctd ddd ctd dbcwcxcrcwcpcrd cpd crcpd crcpcrcwct cud acd ctd ctcs cxd btcbccba cwcxd cpd ctd dbct cud crd cwct cpcvct cpd cpcvctd ctd cpd crcpcrcwcxd cxd btcbccba cbctcrd cxd beb cpd dactd dacxctdb cwct btcbcc cpd crcwcxd ctcrd cxd cvcxdactd cpd dbct cqd cxctaddd csctd crd cxcqct cpd ddb btcbccb crd ctd crcpd cxd cpd cxd crcwctd ctba cbctcrd cxd csctd crd cxcqctd cwct cpcvct cpd cpcvctd ctd cpd cbctcrd cxd cwct ctcrcwcpd cxd cpd cxcrcxctd cud crcpcrcwcxd cxd btcbccba cactd cpd ctdcd ctd cxd ctd cpd ctdacpd cpd cxd btcbcc cpd ctd ctd ctcs cxd cbctcrd cxd bhba cactd cpd ctcs dbd cxd cscxd crd ctcs cxd cbctcrd cxd cpd dbct crd crd csct cxd cbctcrd cxd bjba past overview btd cwd crd ctcrd ctcs cwct ctd ctd crcpd cpcrd cpd btcbcc csct cqdd cxd cpd cxd cwct cpd cxcpd cud dbcpd ctba cccwct crd ctcrd cxd btcbcc csctd cud cpd dactd cpddd ctd dbd czcxd cwctc ctd ctd cxd cxd cpd ddb cpc btcbcc csct cpcrd cpd cpd cpcrcrctd cxd cud ctd cxd cpd ddb cpc btcbcc csct cpdd cpd crd cxcqd cpcvct btcbcc cpd cpd cxcrcxd cpd cxd cwct cxd ctd ctd dbcxd cwcxd cwct btcbcc ctd dbd czba cccwct btcbcc ddd ctd ctdcd cwct cud dbcxd ctd ctd cpd cxd cxd crd cxctd cucxd ctc ctd cpd ctb dbd ctd crd ctcsctd cxcpd czb cucxd ctb ctd acd cpd ctd ctcrcxacctcs cqctd cscxdactd csctd dbcxd cwcxd cwct btcbcc ctd dbd czba cccwct ctd cpd cxd csd crctd bdbibcb cqcxd cxcsctd cxacctd acd ctc csb cwcpd crcpd cqct ctcs cqd ctd ctd cxcsctd cxcudd cwct acd ctba cccwct acd ctc cxd crd ctcs cpd cwct ctcrd cwcpd cbc btb bdb cwct acd ctb cpd ctb cwct dbd ctd cqd cxcr czctddb cpd cpd csd crcwd ctd cpd cccwcxd crcwd cxcrct ctd ctd dbcxd dactd cwcxcvcw cqcpcqcxd cxd ddb cwcpd acd ctc csd cpd cxd ctba cacpd acd ctc cscrd cxd cxd cpd csctd ctcrd ctcs cpd ctcpcs cwct ctcyctcrd cxd cwct cpd ctd cxd ctd ctcs acd ctba cucxd czd cucxd ctc csb ctd cxcpcqd ctd cxctdactd crd ddd cud cwct acd cxcsctd cxacctcs cqdd acd ctc cxcu cxd ctdccxd cxd btcbcc cpd cxcu cwct csctd cwcpd cwct acd cxd ctcpcrcwcpcqd dacxcp cwct ctd ctd cccwct acd cxd cpd ctd cxctdactcs cud cxdact csct ckd ctcpd cwct btcbcc csct cxd cxd cwct czd cxd ctd cwct dccxd cxd ctd cxcrb cpd cwct csctd cwcpd cwct acd ctba cactcrd cpcxd cucxd ctc csb dbd ctd crd ctcsctd cxcpd ctcrd cpcxd cwctd cpcvct crcrd cxctcs cqddd cwctcz crd cxctd cwct acd cxcsctd cxacctcs cqdd acd ctc csba crct cwct ctd cpd cxd crd ctd ctd btcbcc cvctd cvd cpd cpd ctctd cwcpd czd ctd cpd cxd dbcxd csd crct cwct acd ctba cdd cxczct csctd ctd ctd cpd cxd ctcrd cpcxd csd ctd cvd cpd cpd ctct cwcpd cwct acd cxd ctd dbd dccxd cxd cxd cqcpd ctcs crcpd cpd ctd cxcr crcwcpd cwct cqctd cxd cwd cqcpd csdbcxcsd cwb cvctd cvd cpd cwcxcr cscxd cpd crctb ctd crba cvctd cpdacpcxd cpcqd cpcud ctd cxd dbcpd ctcrd cpcxd ctcsba cccwctd dbctcpczctd ctd cpd cxcrd cpdad cxcs crd ctdc cpcvd ctctd ctd crd cpd cwct csctd cxd cwct acd ctba bxcpcrcwc btcbcc csct cxd cpd cxcvd ctcs bdbebkb cqcxd csct cxcsctd cxacctd crcpd ctcs cpd csctc csbacccwct csctc cxd cscxcrcpd ctd csctb cxd cxd cxd crcxd crd cpd cpd ctd cpcrctb dbcwcxcrcw cpd cvctd cud bdbebk bdba cccwct csctc cpd cxcvd ctd cxd cpd cxb cpd csd ctbacvbab cbc btb cwcpd cwct csctb cqd cxcr czctddb cpd crcpd cqct cqcxcpd ctcs cqdd cpd cxcrcxd csct ctd cpd cccwcxd crctd ctd ctd cwcpd cwctd cxd crd ctd cpd cxd cqctd dbctctd cwct dacpd cwct csctc cpd cwct csctb cvctd cvd cpd cwcxcr crcpd cxd ctd dbd crd ctcrd cxdacxd ddb dbd ctd cwcxd cyd cxd cscxcrd cxd cud dbd cwctd cwcpd ctd csctd dbcxd cpcscycpcrctd csctc csd cpd cwcxcvcwd cxczctd cqct cscxdactd cxd cpd cwctd cpd ctcrd cbd crcw ctd cxd cwctd ctcud cpd ctdccrctd ctd crcpd cscxcscpd cud cxd cwct ctd cxcrcpd acd ctb cpd cwct csctd cxd cwct ctd cpd cxczctd crd cxd cqct cqcyctcrd crd ctd cpd ctcs cucpcxd ctd cwd ctcpd bwd cxd cpd cxd ctd ctd cpd cxd btcbcc ctd cwct acd cwct btcbcc csctd dbcwd csctc csd cpd ctd cxcrcpd crd ctd cwct bdbebk cxcvd cxaccrcpd cqcxd cqb cwct acd ctb acd ctc csba cccwcxd cxd dacpd cxcpd cxd cpcxd cpcxd ctcs dactd cwct cxcuctd cxd acd ctb csctd cxd cwct cpd cxdacpd cucpcxd cpd ctcrd dactd btcbcc csctd byd cwct ctcpb cxd ctcs cpcqd dactb dbcxd cwcxcvcw cqcpcqcxd cxd ddb cwct ctd cxcrcpd cpd ctcs cscxdactd ctd btcbcc csctd btd cwctd cxd dacpd cxcpd cxd cwcpd cqd cwct ctd ctdccxd cxd csctc dacpd ctd cpd dbctd cpd cwct ctd ctdccxd cxd acd ctc dacpd ctd cpd cxb cud cscxd cxcqd ctcs cxd cwctcxd ctd ctcrd cxdact csd cpcxd cccwcxd ctd ddcud dbd cud cwct cpd cxb cpd csd cpd cxcvd ctd cud csctc csd cpd acd ctc csd cxd ctd ctd cwcpd cwct cqctd acd ctd ctcs cqdd ctcpcrcw btcbcc csct cxd cvcwd cqcpd cpd crctcsba cccwcxd cucpcrd dacxcsctd cpd cxd 
cxd cxcpd cpd dccxd cpd cxd cqcpd cpd crcxd cwct cpcvct cxb cxdecpd cxd cpd cwct btcbcc csctd cbcxd crct acd ctd cscxabctd cxd cxdect cpd btcbcc csctd cscxabctd cxd cwct cpd cpcvct cwctdd dacxcsctb cpcscscxd cxd cpd ctdcd cxcrcxd ctcpd cpcs cqcpd cpd crcxd cpd ctd cxd ctcsbn cwctdd cpd csctd crd cxcqctcs cxd cbctcrd cxd bfba cccwct cqctd cxd crcwd ctd ctctd cwct cpdacpcxd cpcqcxd cxd ctctcsd acd ctb ctd cpd cxdact cwct ctdcd ctcrd ctcs cucpcxd cpd ctd cxd cscxdacxcsd cpd csctd dbctdactd cpd acd ctd cpdd ctctcs cqct cpcxd cpcxd ctcs cpd cpd csctd cxd csctd ctctd cpd cqcpd cpd crct cwct ctd cpcs cud cwct acd cpd cxd cxd cxdect cpd ctd crdd cpd ctd dbd crba btcbcc cpcscpd ctd cpcs cqdd crcpcrcwcxd cpcscscxd cxd cpd crd cxctd acd ctd cxd cwct ctcs cxd btcbcc csctb crcpd cscxd czd cdd cxczct cwct cxd cpd ctd cxcrcpd acd ctb crcw crcpcrcwctcs crd cxctd cpdd cqct cscxd crcpd csctcs cqdd csct cpd cpd cxd ctba bvcpcrcwcxd cxd btcbcc cxd cscxd crd ctcs cxd cbctcrd cxd bgba btcbcc cxd cpddctd ctcs cpd ddb ctctd ctctd ctd ctd cxd cpd crd ctd crcpd cxd crcwctd ctba cpd cxd cud csctb crd cxcqctcs cpd ctdacpd cpd ctcs cxd cjbebjclba ccd cpczct cwcxd cpd ctd ctd cub crd cpcxd ctcsb dbct cvcxdactcpcqd cxctcud dactd dacxctdb cpd ddba pastry cpd cxd ctctd ctctd cxd cqd cpd cwcpd cxd crcxctd crcpd cpcqd ctb cucpd ctd cxd cxctd cpd ctd cub cvcpd cxdecxd cvba bzcxdactd acd ctc csb cpd ctd cpd cpd crcxcpd ctcs ctd cpcvct dbcpd csd cwct csct dbcwd csctc cxd ctd cxcrcpd crd ctd cwct bdbebk cqd cwct acd ctc csb cpd cpd cxdact csctd bzcxdactd cwct btcbcc cxd dacpd cxcpd cwcpd acd cxd ctcs cwct csctd dbcwd csctc csd cpd ctd cxcrcpd crd ctd cwct bdbebk cqd cwct acd ctc csb cxd cud dbd cwcpd acd crcpd cqct crcpd ctcs ctd cpd csctd cwcpdact cucpcxd ctcs cxd cpd ctd cxbactbab dbcxd cwcxd ctcrd dactd ctd cxd csb btd cxd btcbcc ctd dbd crd cxd cxd csctd cpd crcpd cwct ctd cxcrcpd crd ctd csct cud cvcxdactd acd ctc cxd ctd cwcpd csd cqc ctd csctd cpd ctd cpd cxd cxd crd accvd cpd cxd cpd cpd ctd ctd dbcxd ddd cxcrcpd dacpd bgb bwctd cxd crd crd ctd csct cucpcxd ctd ctdactd cpd csctd cxdactd cxd cvd cpd cpd ctctcs ctd cqd bpbecr csctd dbcxd cpcscycpcrctd csctc csd cucpcxd cxd cpd ctd cxd crd accvd cpd cxd cpd cpd ctd ctd dbcxd ddd cxcrcpd dacpd bfbeb cccwct cpcqd ctd ctd cxd ctcs cxd ctcpcrcwc btcbcc csct cwcpdact bdb csd cqc ctb bed ctd cxctd dbcwctd ctcpcrcwctd cpd csctc cwct cpd crcxcpd ctcs csctb cpcscsd ctd ctd dactd cpcud ctd csct cucpcxd cwctcpd cxdacpd ctdb csctb cwctcxd dacpd cxcpd crcpd cqctd ctd ctcs cqdd ctdccrcwcpd cvcxd cqc ctd cpcvctd cpd cwct cpabctcrd ctcs csctd cwct cud dbcxd cvb dbct cqd cxctaddd czctd crcwd cwctc cpd cxd crcwctd ctba byd cwct cxd cvb csctc csd cpd acd ctc csd cpd cwd cvcwd cpd ctd ctd crct cscxcvcxd dbcxd cqcpd csctb cxd cpcqd cxd cvcpd cxdectcs cxd csd cqc ctdactd dbcxd ctd cxctd ctcpcrcwba cccwct ctd cxctd cpd ctdactd cwct cxd cpcqd ctcpcrcw ctcuctd csct dbcwd csctc cwcpd ctd cwct ctd ctd csctb csctc cxd cwct acd cscxcvcxd cqd dbcwd bdd cwcscxcvcxd cwcpd cwct cxcqd dacpd ctd cwctd cwcpd cwct bdd cscxcvcxd cxd cwct ctd ctd csctb cxcsba bxcpcrcw ctd cxd cwct cxd cpcqd cxd ctd cxcpd cpd csctd dbcwd csctc cwcpdact cwct cpd cxcpd ctacdcbn cxd cpcrd cxcrctb csct cxd crcwd ctd cwcpd cxd crd cwct ctd ctd csctb cpcrcrd cscxd cwct dccxd cxd ctd cxcrba csct cxd czd dbd dbcxd cxd cpcqd csctc csb cwctd cwct cxd cpcqd ctd cxd ctcud ctd ddba cccwct cxcud cscxd cxcqd cxd csctc csd ctd ctd cpd ctdactd cpd cxd cwct csctc cpcrctbn cwd csd cqc ctdactd cpd cpd ctcs cxd cwct cxd cpcqd ctba cpcscscxd cxd cwct cxd cpcqd ctb ctcpcrcw csct cpcxd cpcxd cpcsb csd ctd ctd cud cwct csctd cxd cxd ctcpcu ctd cpd cxd ctcxcvcwcqd cwd csd ctd cccwct ctcpcu ctd cxd cwct ctd csctd dbcxd cwct bpbed ctd cxcrcpd crd ctd cpd cvctd csctc csd cpd cwct bpbe csctd dbcxd ctd cxcrcpd crd ctd cpd ctd csctc csd ctd cpd cxdact cwct ctd ctd csctb csctc csba cccwct ctcxcvcwcqd cwd ctd cxd ctd csctd cwcpd cpd ctcpd cwct ctd ctd csctb cpcrcrd cscxd cwct dccxd cxd ctd cxcrba cxd ctcs cxd cxd cvb cqd cxd ctcud csd cxd csct cpcscscxd cxd bbd ctcrd dactd ddba bycxcvd csctd cxcrd cwct cpd btcbcc csct dbcxd cwct csctc bdbcbebfbfbdbcbe cqcpd bgb cxd cwddd cwctd cxcrcpd ddd ctd cwcpd ctd bdbi cqcxd csctc csd cpd dacpd ctd cpd bpbkba ctcpcrcw cxd ctd csct cpd cud dbcpd csd cwct ctd cpcvct csct dbcwd csctc cwcpd ctd dbcxd cwct acd ctc ctacdc cwcpd cxd cpd ctcpd cscxcvcxd cqcxd cvctd cwcpd cwct ctacdc cwcpd cwct acd ctc cwcpd ctd dbcxd cwct ctd ctd csctb cxcsba crcwd csct cxd czd dbd cwct ctd cpcvct cxd cud dbcpd csctcs csct dbcwd csctc cwcpd ctd cpd ctacdcdbcxd cwctacd ctc cscpd cpd cwctcrd ctd csctb cqd cxd ctd cxcrcpd crd ctd cwct acd ctc cwcpd cwct ctd ctd csctb cxcsba cbd crcw csct cqct cxd cwct ctcpcu ctd ctd cwct ctd cpcvct cwcpd cpd ctcpcsdd cpd cxdactcs cpd cwct csct dbcxd ctd cxcrcpd crd ctd csctc csba btd csb ctd cqd bpbecr cpcscycpcrctd csctd cxd cwct ctcpcu ctd cwcpdactcucpcxd ctcs cxd cpd ctd ddb cpd ctcpd cwd csctd cqct cxdactba crcpd cxd ctdcd dbct cqd cxctaddd cscxd crd cpd ddb ctd cxctd dbcxd ctd ctcrd cwct ctd dbd dccxd cxd ctd cxcrba cactcrcpd cwcpd cwct ctd cxctd cxd cwct csct cxd cpcqd ctd cpd crcwd ctd ctcuctd ctcpd cqdd csctb cxd ctd cwct dccxd cxd ctd cxcrb dbcxd cwct cpd cxcpd csctc ctacdcba btd ctd cxd ctcpcrcw ctd ctd cpcvct cxd ctcs ckd ctcpd cqdday csct dbcxd cvctd ctacdc cpd crcw 
nodeid routing table neighborhood set leaf set largersmaller bycxcvd bdbm cbd cpd cpcwddd cwctd cxcrcpd cpd csct dbcxd csctc bdbcbebfbfbdbcbeb beb cpd bkba btd cqctd cpd cxd cqcpd bgba cccwct cwct cxd cpcqd ctd ctd ctd ctdactd dectd cccwct cwcpcsctcs crctd cpd ctcpcrcw ctdactd cwct cxd cpcqd cwd dbd cwct crd ctd cscxd cscxcvcxd cwct ctd ctd csctb csctc csba cccwct csctc csd cxd ctcpcrcw ctd cwcpdact cqctctd cxd cwd dbd cwctcrd ctacdc dbcxd bdbcbebfbfbdbcbe ctdcd cscxcvcxd ctd csctc csba cccwct cpd crcxcpd ctcs cpcscsd ctd ctd cpd cwd dbd cqdd cscxcvcxd cccwcxd crcpd cwctd cxd cxcr cqdacxd crcpd cpcrcwcxctdact cvd cqcpd cwd ctd ctd cqd cxd cpd cxd cwcpdact cwd dbd cwcpd cwct cpdactd cpcvct cscxd cpd crct cpdactd ctcs cqdd ctd cpcvctb cxd ctd cwct dccxd cxd ctd cxcrb cxd bhbcb cwcxcvcwctd cwcpd cwct crd ctd cscxd ckcscxd cpd crctay cwct crct cpd csctd cxd cpd cxd cxd cwct csctd ddcxd ctd dbd cjbebjclba ctd dactd cxd crct cpd ctd ctcpd ctcsd cpczctd crcpd ckd cwd cxd ctd dbcpd csd csct cwcpd cwcpd ctd cvctd ctacdc dbcxd cwct acd ctc csb ctd cpcvctd cwcpdact ctd csctd crdd acd ctcpcrcwcpd csctb cpd cwct csctd cwcpd cwct ctd ctd ctcs acd ctb cwcpd cxd ctcpd cwct crd cxctd cpcrcrd cscxd cwct dccxd cxd ctd cxcrba ctdcd ctd cxd ctd cwd dbd cwcpd cpd ctd cxcrcpd ctcs crd cxctd acd ctb cpd cxd cpcqd acd cwct ckd ctcpd ctd crd cxd bjbib cpd czd cpd cxd acd csd cwct dbd ckd ctcpd ctd crd cxctd cxd blbeb cpd czd cjbebjclba csct cpcscscxd cxd cpd cucpcxd btczctdd csctd cxcvd cxd cxd cpd cxd cwd crcxctd cpd csddd cpd cxcrcpd cpcxd cpcxd cwct csct cpd ctb cxbactbab cwct cxd cpcqd ctb ctcpcu ctd cpd ctcxcvcwcqd cwd ctd cxd cwct ctd ctd crct csct cucpcxd ctd csct ctcrd dactd cxctd cpd ctdb csct cpd cxdacpd cccwct crd cxd csctd crd cxcqctcs cpd ctdacpd cpd ctcs cxd cud csctd cpcxd cxd cjbebjclba bud cxctadddb cpd cpd cxdacxd csct dbcxd cwct ctdbd crcwd ctd csctc crcpd cxd cxd cxcpd cxdect cxd cpd cqddcrd cpcrd cxd ckd ctcpd cqdday csct cpcrb crd cscxd cwct dccxd cxd ctd cxcrb cpd cpd czcxd ctcrcxcpd ctd cpcvct dbcxd cwct csctd cxd cpd cxd ctd cgba cccwcxd ctd cpcvct cxd ctcs cwct ctdccxd cxd csct dbcxd csctc ctd cxcrcpd crd ctd cwctd cqd cpcxd cwct ctcpcu ctd cud cib cwct ctcxcvcwcqd cwd ctd cud btb cpd cwct cxd cwct cxd cwct ctdccrctctcscxd cvd cxczctd ctdactd cwcpd cpd cpd ctd cpd cwctd ctdbd csctd cqd cpcxd ctdb csctc csba cpcqd cud cwct cxd csct ctd crd ctd ctcs cpd cwct cud ciba crcpd cwd cwcpd cxd cwcxd cxd cud cpd cxd crcpd crd ctcrd cxd cxd cxcpd cxdect cxd cpd cpd cxcudd cpd csctd cwcpd ctctcs czd cxd cpd cxdacpd cwctd ctcqdd ctd cxd cpd cpd ddb cxd dacpd cxb cpd ccd cwcpd csd csct cucpcxd ctd ctcxcvcwcqd cxd csctd cxd cwct csctc cpcrct dbcwcxcrcwcpd ctcpdbcpd ctcpcrcwd cwctd cqdd dacxd cqctcxd cxd ctcpcrcw cwctd ctcpcu ctd ctd cxd cscxcrcpd ctdccrcwcpd cvct czctctd cpd cxdactd ctd cpcvctd cucp csctcxd ctd cxdact cud ctd cxd ccb cxd cxd ctd ctcs cucpcxd ctcsba btd ctd cqctd cwct cucpcxd ctcs csctb ctcpcu ctd cpd cwctd cxacctcs cpd cwctdd cscpd cwctcxd ctcpcu ctd ctd cwct cxd dacpd cxcpd cbcxd crct cwct ctcpcu ctd csctd dbcxd cpcscycpcrctd csctc csd dactd cpd cwcxd cscpd cxd cxdacxcpd btd ctcrd dactd cxd csct crd cpcrd cwct csctd cxd cxd cpd czd dbd ctcpcu ctd cqd cpcxd cwctcxd crd ctd ctcpcud ctd cscpd ctd cxd dbd ctcpcu ctd cpd cwctd cxacctd cwct ctd cqctd cxd ctdb ctcpcu ctd cxd ctd ctd crctba cad cxd cpcqd ctd cxctd cwcpd ctcuctd cucpcxd ctcs csctd cpd ctd cpcxd ctcs cpdecxd ddbn cwct csctd cpcxd cpd ctd ctdacpd cwct cqcyctcrd cwcxd cpd ctd cjbebjclba cpd ddb cpd csctd crd cxcqctcs cucpd cxd csctd ctd cxd cxd cxcr cpd cwd dad ctd cpcqd cpd cxcrcxd cucpcxd ctcs csctd cpd cwct cwcpd cpcrb crctd ctd cpcvctd cqd csd crd ctcrd cud dbcpd cwctd cactd ctcpd ctcs ctd cxctd crd cwd cucpcxd ctcpcrcw cxd ctb cxd crct cwctdd cpd cxczctd cpczct cwct cpd ctba ccd dactd crd cwcxd cqd ctd cwct cxd cxd cpcrb cpd cpd csd cxdectcsba ccd cpdad cxcs cxd ctd cpcvct cpd dbcpddd cqct cud dbcpd csctcs csct cwcpd cwcpd ctd cpd ctcpd cpd ctacdc dbcxd cwb cqd cxd ctd cxcrcpd crd ctd cwct csctd cxd cpd cxd csct cxd cwct cpd ctd cpcrct cwcpd cwct crd ctd csctbacccwct crcwd cxcrct cpd cxd crcwd csctd cxd cpd csd cpcrd cxcrctb cwct cqcpcqcxd cxd cscxd cxcqd cxd cxd cwctcpdacxd cqcxcpd ctcs dbcpd csd cwct cqctd crcwd cxcrct ctd dbcpdactd cpcvct csctd cpddba cwct ctdactd cpd cxcrcxd cucpcxd ctcs csct cpd cwct cpd cwb cwct crd cxctd cpddcwcpdact cxd ctdactd cpd ctd ctd cxd cxd crcwd ctd cwcpd cpdad cxcsd cwct cqcpcs csctba cud csctd crd cxd cxd cpd ctdacpd cpd cxd cpd crcpd cqct cud cxd cjbebjclba cxd crcxd ctb cxd cwd cqct cxcqd cpddctd btcbcc cwct cwctd ctctd ctctd cxd crcwctd ctd csctb crd cxcqctcs cxd cwct cxd ctd cpd ctb crcwcpd cccpd ctd cjbfbdclb bvcwd cjbfbccl bvbtc cjbebhclba dbctdactd btcbccb ctd cxctd dbcxd ctd ctcrd ctd dbd crcpd cxd cpd cucpd ctd cxd cxctd crct cpddcrcwcpd cvct cxd cwcxd crcpd ctb csctd ctd cscxd cwct ctd cxctd cwct csctd ddcxd cxd crcwctd ctba past operations ctdcd dbct cqd cxctaddd csctd crd cxcqct cwd dbc btcbcc cxd ctd ctd cwct cxd ctd czd cpd ctcrd cpcxd ctd cpd cxd ctd cpd cxd ctd ctd ctd acd ctc cxd crd ctcs cpd cwct cbc btb bdcwcpd cwcrd csct cwct acd ctb ctdcd cpd cpd ctb cwct crd cxctd cqd cxcr czctddb cpd cpd csd cpd cccwct ctd cxd ctcs cpcvct acd cxdect cxd ctd czb cxd csctcqcxd ctcs cpcvcpcxd cwct crd cxctd cpcvct cpb cpd acd crctd cxaccrcpd cxd cxd ctcs cpd cxcvd ctcs dbcxd cwct dbd ctd cxdacpd czctddba cccwct crctd cxaccrcpd crd cpcxd cwct acd 
ctc csb cbc btb cwcpd cwct acd ctb crd ctd cwct ctd cxcrcpd cxd cucpcrd czb cwct cpd crd ctcpd cxd cscpd cpd cwctd cxd cpd acd ctd cpcscpd cpba cccwct acd crctd cxaccrcpd cpd cwct cpd crcxcpd ctcs acd cpd cwctd ctcs dacxcp cpd ddb cxd cwct acd ctc cpd cwct csctd cxd cpd cxd cfcwctd cwct ctd cpcvct ctcpcrcwctd cwct acd cpd cwct csctd crd ctd cwct acd ctc csb cwcpd csct dactd cxacctd cwct acd crctd cxaccrcpd ctb ctcrd ctd cwct crd ctd cwcpd cwcrd csct cpd crd cpd ctd cxd dbcxd cwct crd ctd cwcpd cwcrd csct cxd cwct acd crctd cxaccrcpd ctba ctdactd ddd cwcxd crcwctcrczd cwctd cwct csct cpcrcrctd ctd cxcqcxd cxd cud ctd cxcrcp cwct acd cpd cud dbcpd csd cwct cxd ctd ctd ctd cwct cwctd csctd dbcxd csctc csd ctd cxcrcpd crd ctd cwct acd ctc csba crct cpd csctd cwcpdact cpcrcrctd ctcs ctd cxcrcpb cpd cpcrczd dbd ctcscvb ctd cxd cpd ctcs cqcpcrcz cwct crd cxctd dbcwcxcrcw ctcpcrcw cwct ctd cxcrcp cxd csctd cpd cpcrcwcpd ctd ctcrctcxd cccwct crd cxctd dactd cxb acctd cwct ctcrctcxd crd acd cwcpd cwct ctd ctd ctcs cqctd crd cxctd cwcpdact cqctctd crd ctcpd ctcsba ctd cwcxd cvd ctd dbd cpd cpd cxd csd cxd cwct cxd ctd cxd crctd crcwcpd cpd cxd ctcvcxd cxd cpd acd crctd cxaccrcpd ctb crd ctcs crd ctd cucpcxd crcpd cub accrcxctd cpcvct cwct crd cxctd cpd cpd cxcpd ctd cxd cscxcrcpd cxd cxd ctd ctcs cwct crd cxctd ctd czd ctd ctd cwct crd cxctd csct ctd csd cpd cpd cxcpd ctd ctd ctd cpcvctb cxd cwct ctd ctd ctcs acd ctc cpd cwct csctd cxd cpd cxd btd cpd cwct ctd ctd ctd cpcvct ctcpcrcwctd csctd cwcpd ctd cwctacd ctb cwcpd csctd ctd csd dbcxd cwctcrd ctd cpd cwct ctcs acd crctd cxaccrcpd ctbn cwct ctd ctd ctd cpcvct cxd ctcs cud cwctd bwd cwct crcpd cxd ctd cxctd cwct cpd cxd crcwctd cpd cwct cucpcrd cwcpd acd ctd cxcrcpd cpd ctcs csctd dbcxd cpcscycpcrctd csctc csd czd cxd cxczctd acd ctd cxcrcp cwcpd cxd ctcpd cwct crd cxctd cpcrcrd cscxd cwct dccxd cxd ctd cxcrba ctcrd cpcxd ctd ctd crctctcsd cpd cpd cvd cpd cxd ctd ctd ctd cccwct crd cxctd csct cxd ctd ctcrd cpcxd crctd cxaccrcpd ctb dbcwcxcrcw cpd dbd cwct ctd cxcrcp cxd csctd dactd cxcudd cwcpd cwct acd ctb ctcvcxd cxd cpd dbd ctd cxd ctd ctd cxd cwct ctd cpd cxd cccwct cxd csctd ctcpcrcw cxd cpd ctd ctcrd cpcxd ctcrctcxd dbcwcxcrcw cwct crd cxctd csct dactd cxacctd cud crd ctcscxd cpcvcpcxd cwct ctd cpcvct cpba csctd cpcxd cpcqd cpd cpcvctd ctd crcpd cqct cud cxd cjbdbiclba security cfcwcxd cwct csctd cpcxd ctcrd cxd ddcxd btcbcc cpd cqctddd cwct crd cwcxd cpd ctd dbctcvcxdact cwctd cqd cxctcu dactd dacxctdbba csctd cpcxd crcpd cqct cud cxd cjbdbicl cpd cxd cud cwcrd cxd cpd ctd bxcpcrcwc btcbcc csct cpd ctcpcrcw ctd cwct ddd ctd cwd cpd crcpd ctcpcsb crd cxctd csd ctctcs crcpd csb cxdacpd ctbbd cqd cxcr czctdd cpcxd cxd cpd crcxcpd ctcs dbcxd ctcpcrcw crcpd csba bxcpcrcw cpd crcpd csb cqb cxcr czctdd cxd cxcvd ctcs dbcxd cwct cpd crcpd cxd ctd cxdacpd czctdd cud crctd cxaccrcpd cxd ctd cccwct cpd crcpd csd cvctd ctd cpd cpd dactd cxcudd cwct dacpd cxd crctd cxaccrcpd ctd cpd cwctdd cpcxd cpcxd cpcvct cpd cxd cxcqd ctd cpd btcbcc dbcxd cwd cpd crcpd csd cwd dbctdactd dacxcscxd crd cpd cpcqd ctcrd cxd cpd ddd ctd dbcxd cwd cpd crcpd csd crd cxcrcpd ctd cwct ddd ctd cjbdbiclba cccwct cud dbcxd cpd cxd csctd btcbccb ctcrd cxd csctd bdb cxd crd cpd cxd cpd cxd cuctcpd cxcqd cqd ctcpcz cwct cqd cxcrb czctdd crd ddd ddd ctd cpd cwct crd ddd cvd cpd cwcxcr cwcpd cud crd cxd ctcs cxd btcbccbn beb dbcwcxd crd cxctd csct ctd cpd cpd csct cud dbcpd cpd ctcs cpd cpd cpcrczctd cpdd crd cwct cqctcwcpdacxd cxd cscxdacxcsd cpd btcbcc csctd cxd cxd cpd ctcs cwcpd csctd cxd cwct dactd cpddd ctd dbd cpd dbctd cqctcwcpdactcsbn cpd csb bfb cpd cpd cpcrczctd crcpd crd cwct cqctcwcpdacxd cwct cpd crcpd csd cccwct cpd crcpd csd ctd cwct cxd ctcvd cxd csctc cpd acd ctc cpd cxcvd ctd cwd ctdactd cxd cpd cpd cpcrczctd cud crd cxd cpcscycpcrctd csctd cxd cwct csctc cpcrctb cscxd ctcrd cxd acd cxd ctd cxd ctcrcxaccr cxd cwct acd ctc cpcrctba cbd ctcrctcxd ctdactd cpd cxcrcxd csct cud crcpd cxd cwct ddd ctd crd ctcpd cuctdbctd cwcpd cscxdactd ctd cxcrcpd acd dbcxd cwd cwctcrd cxctd cxcrb cxd cxd cccwct acd crctd cxaccrcpd ctd cpd cpcvct csctd cpd crd cxctd dactd cxcudd cwct cxd ctcvd cxd cpd cpd cwctd cxcrcxd ctcs crd ctd bycxd cpd ctcrd cpcxd crctd cxaccrcpd ctd cwctd ctd cud crct crd cxctd cpcvct cpd csctd cxd ctcsb crd cxctd crcpd ctd acd cxdacpcrdd cqdd ctd crd ddd cxd cwct crd ctd cqctcud cxd ctd cxd cwct acd cxd btcbccba cccwct cpd cxd crcwctd crcpd cqct cpd csd cxdectcsb cwd ctb dactd cxd cpd cxcrcxd csct cpd cwct cpd cud ctd ctcpd ctcsd cxd ctd crctd cxd ctd ctd ctd cpcvct cpd crcpd cxd csctd cxcpd ctd dacxcrctba btd cxd cpcqd ctd cxctd cxbactba csctc cpcscsd ctd cpd cxd cvd cpd cxcvd ctcs cqdd cwct cpd crcxcpd ctcs csct cpd crcpd cqct dactd cxacctcs cqdd cwctd csctd cccwctd ctcud ctb cpd cxcrcxd csct cpdd cpd dbd ctd dacpd cxcs ctd cxctd cqd cxd crcpd cud cvct ctd cxctd btd cxd cxd cud cpd cxd cxd cpd cxd cxd cwctd ctd ctcsd cscpd cpd cvd cqcpd cscxd ctd cxd cpd ctcsba dbcxd cpd cwctd cucpcrd cpd cxd cwcxcvcwd ctd cxd cxctd cpd cxcrcxd csctd cpd cxd cxd cwctcxd cxd cpcrd csctcvd cpcscpd cxd cxd cxd ctd cud cpd crct cpd cpd csctd cpd dbctd cqctcwcpdactcsba cwct dbd crcpd ctb dbcxcsctd ctcpcs crd cxd csctd crd crcpd cxd cucpcxd ctd cpd cwd csctd cxcpd ctd dacxcrctba cwct cud dbcxd ctcrd cxd dbct csctd crd cxcqct cwct cpcvct 
cpd cpcvctb ctd cpd cwct crcpcrcwcxd cxd btcbccba cccwct cxd cpd cvd cpd cpcvct cpd cpcvctd ctd cxd ctd cwct cpdacpcxd cpcqcxd cxd acd ctd dbcwcxd cqcpd cpd crcxd cwct cpcvct cpcs cpd cwct ddd ctd cpd cpcrcwctd cxd cpdccxb cpd cpcvct cxd cxdecpd cxd cccwct cvd cpd crcpcrcwcxd cxd cxd cxd cxdect crd cxctd cpcrcrctd cpd ctd crcxctd cpdccxd cxdect cwct ctd cwd cvcwd cpd cqcpd cpd crct cwct ctd cpcs cxd cwct ddd ctd storage management btcbccb cpcvct cpd cpcvctd ctd cpcxd cpd cpd dbcxd cwcxcvcw cvd cqcpd cpcvct cxd cxdecpd cxd cpd cvd cpcrctcud csctcvd cpcscpd cxd cpd cwct ddd ctd cpd cpcrcwctd cxd cpdccxd cpd cxd cxdecpd cxd cccwct cpcvcvd ctcvcpd cxdect acd ctd cxcrcpd ctcs cxd btcbcc cwd cqct cpcqd cvd cpd cvct cud cpcrd cxd cwct cpcvcvd ctcvcpd cpcvct crcpd cpcrcxd cpd btcbcc csctd cqctcud cpd cvct cud cpcrd cxd cxd ctd ctd ctd cpd ctcyctcrd ctcs abctd cud csctcrd ctcpd ctcs ctd cud cpd crctba cfcwcxd cxd cxd cscx crd ctcscxcrd cxcu ddd ctd crcwcpd btcbcc dbcxd cqct ddd cxcrcpd ctd cpd ctcs cpd cwcxcvcw ctdactd cpcvct cxd cxdecpd cxd cxd cxd crd ctd cxd cwcpd cpd cwcxcvcwd crcxctd cpd ctd cxcpcqd ddd ctd ctd cpcxd cqd cxd cwct ctdactd ctdcd ctd ctd cpd cxd crd cscxd cxd cxd dbcxd cwct dactd cpd csctcrctd cpd cxdectcs cpd crcwcxd ctcrd btcbccb cpd cxd cpd csctd cxcvd cvd cpd cud cwct cpcvct cpd cpcvctd ctd cxd ctd crcpd crd cscxd cpd cxd cpd csctd dbcxd ctcpd cqdd csctc csd cud cxd ctcvd cpd cpcvct cpd cpcvctd ctd dbcxd acd cxd ctd cxd cpd cxd crd csctd ctd cud cpd crct dactd cwctcpcsd ctd cpd ctcs cpcvct cpd cpcvctd ctd cccwct ctd cxcqcxd cxd cxctd cwct cpcvct cpd cpcvctd ctd cpd bdb cqcpd cpd crct cwct ctd cpcxd cxd cud ctct cpcvct cpcrct cpd csctd cxd cwct btcbcc ctd dbd cpd cwct ddd ctd dbcxcsct cpcvct cxd cxdecpd cxd cxd cpd cpcrcwcxd bdbcbcb cpd csb beb cpcxd cpcxd cwct cxd dacpd cxcpd cwcpd crd cxctd ctcpcrcw acd cpd cpcxd cpcxd ctcs cqdd cwct csctd dbcxd csctc csd crd ctd cwct acd ctc csba bzd cpd bdb cpd beb cpd ctcpd cqct crd adcxcrd cxd cvb cxd crct ctd cxd cxd cwcpd acd cxd ctcs csctd crd ctd cxd acd ctc ctcpdactd cud cpd ctdcd cxcrcxd cpcs cqcpd cpd crcxd cvba btcbcc ctd dactd cwcxd crd adcxcrd cxd dbd dbcpddd bycxd btcbcc cpd dbd csct cwcpd cxd cwct ctd cxb crcpd crd ctd csctd cwct acd ctc cpd ctd cpd cxdactd cwct acd ctb cxcu cxd cxd cxd cwct ctcpcu ctd cwd csctd cccwcxd crctd cxd crcpd ctcs ctd cxcrcp cscxdactd cxd cpd cxd cxd cpcrcrd cscpd cscxabctd ctd crctd cxd cwct cpcvct crcpd cpcrcxd cpd cxd cxdecpd cxd csctd dbcxd cwcxd ctcpcu ctd cactd cxcrcp cscxdactd cxd cqct csd dbcxd crcpd ctb ctd cwcpd cwct acd cpdacpcxd cpcqcxd cxd cxd csctcvd cpcsctcsba cbctcrd csb acd cscxdactd cxd cxd ctd cud ctcs dbcwctd csctb ctd cxd ctcpcu ctd cxd ctcpcrcwcxd crcpd cpcrcxd ddba cxd cpcrcwcxctdact cvd cqcpd cpcs cqcpd cpd crcxd cpcrd cpd cvct cxd cwct csctc cpcrctba acd cxd cscxdactd ctcs cscxabctd ctd cpd cwct csctc cpcrct cqdd crcwd cxd cscxabctd ctd cpd cxd cwct cvctd ctd cpd cxd cxd acd ctc csba cwct ctd cwcxd ctcrd cxd dbct cscxd crd crcpd ctd cpcvct cxd cqcpd cpd crctb cpd cpd cxd cpcqd ctd csct cpcvct cpd cwctd ctd ctd cwct cpd cvd cxd cwd cud ctd cxcrcp cpd acd cscxdactd cxd bycxd cpd ddb dbct csctd crd cxcqct cwd cwct cpcvct cxd dacpd cxcpd cxd cpcxd cpcxd ctcs cxd cwct ctd ctd crct ctdb csct cpcscscxd cxd csct cucpcxd cpd ctcrd dactd ddba btd ctdcd ctd cxd ctd cpd ctdacpd cpd cxd btcbccb cpcvct cpd cpcvctd ctd cud dbd cxd cbctcrd cxd bhba storage load imbalance cactcrcpd cwcpd ctcpcrcw btcbcc csct cpcxd cpcxd ctcpcu ctd dbcwcxcrcw crd cpcxd cwct csctd dbcxd csctc csd ctd cxcrcpd crd ctd cwct cvcxdactd csct bpbe dbcxd cpd cvctd cpd bpbe dbcxd cpd ctd csctc csd cpd ddb cwct ctd cxcrcpd acd cpd ctcs cwct csctd cwcpd cpd ctd cxcrcpd crd ctd cwct acd ctc crcpd cqct cpd cvctd cwcpd bpbeb bdb bvd cxcsctd cwct crcpd dbcwctd cpd cwct crd ctd csctd crcpd cpcrb crd cscpd ctd cxcrcp csd cxd crcxctd cpcvctb cqd csctd ctdccxd dbcxd cwcxd cwct ctcpcu ctd cwct csctd cwcpd crcpd cpcrcrd cscpd cwct acd ctba cbd crcw cpd cxd cqcpd cpd crct cxd cwct cpdacpcxd cpcqd cpcvct cpd cwct csctd cxd cwct cxd ctd ctcrd cxd cwct ctcpcu ctd crcpd cpd cxd cud ctdactd cpd ctcpd bwd cpd cxd cxcrcpd dacpd cxcpd cxd cxd cwct cpd cxcvd ctd csctc csd cpd acd ctc csd cwct cqctd acd ctd cpd cxcvd ctcs ctcpcrcwd csctd cpdd cscxabctd cccwct cxdect cscxd cxcqd cxd cxd ctd ctcs acd ctd cxb cpd crct cpd cpddcqctcwctcpdadd cpcxd ctcsba cccwct cpcvct crcpd cpcrcxd cxd cscxdacxcsd cpd btcbcc csctd cscxabctd cactd cxcrcp cscxdactd cxd cpcxd cpd cqcpd cpd crcxd cwct ctd cpcxd cxd cud ctct cpcvct cpcrct cpd cwct csctd cxd ctcpcrcw ctcpcu ctd cpcscscxd cxd cpd cwctcvd cqcpd cpcvct cxd cxdecpd cxd btcbccd ddd ctd cxd crd ctcpd ctd acd cscxdactd cxd cpdd cpd cqctcrd ctcrctd cpd cqcpd cpd crct cwct cpcvct cpcs cpd cscxabctd ctd cxd cwct csctc cpcrctba per-node storage cfct cpd cwcpd cwct cpcvct crcpd cpcrcxd cxctd cxd cscxdacxcsd cpd btcbcc csctd cscxabctd cqdd cwcpd dbd csctd cpcvd cxd csct cpd cvcxdactd cxd ctba cccwct cud dbcxd cscxd crd cxd dacxcsctd cyd cxacb crcpd cxd cud cwcxd cpd cxd btcbcc csctd cpd cxczctd cwct crd ctabctcrd cxdact cwcpd csb dbcpd cpdacpcxd cpcqd cpd cwct cxd cwctcxd cxd cpd cpd cxd btd cwct cxd cwcxd dbd cxd cxd cvb cwcxd cxcvcwd cqct dbcxd cpd cqctd bibcbzbu cscxd csd cxdactd bzcxdactd cwcpd cwct cxdect cwct crd ctabctcrd cxdact cscxd cxdect crcpd cqct ctdcd ctcrd ctcs csd cqd cxd ctd cwcpd ddctcpd csct dbcxd ddd cxcrcpd dbb crd crd accvd cpd cxd cpd cwct 
cxd cxd cxd cpd cpd cxd cwd ctd cpcxd dacxcpcqd cud cpd ddctcpd cxbactbab cxd crcpd cpcrcxd cwd csd cqctd dbd dbd csctd cpcvd cxd csct cwct cpd cvctd ctdbd cxd cpd ctcs csctba cpd cxd csd ctd ctdactd cwct crd crd cxd cxd ctd cwcpd dacxcsct cpd cvctb crcpd cpcvctba cbd crcw cxd dbd cqct crd accvd ctcs cpd crd ctd cvcxcrcpd ctd cpd cpd btcbcc csctd dbcxd ctd cpd cpd csctc csd cfcwctd cwctd cwct cpd crcxcpd ctcs cwcpd csdbcpd cxd crctd cpd cxdectcs cpd cvct cxd crctd csct dbcxd cabtc cpcvct cqb ddd ctd crd ctd bvd ctcpcrcw dbcxd cpd cqctd cpd cpcrcwctcs cscxd czd cxd cxd ctd ctdacpd cpd cwd cvcw crd cxcsctd cpd cxd crd cpd cucpd ctd cxd cxctd crct cpd cucpdad crd ctd bvd bxdactd cwd cvcw cwct cxd csctd cxd cpd cpcsd cxd cxd cpd cxdactd cxd csctd ctd csctd cpd cpdd cwcpdact crd ctd cpd ctcs cucpcxd ctd cwct crcw cxd ctd csd ctd cxcvd cxaccrcpd cpabctcrd cwct cpdactd cpcvct cscxdactd cxd cwct csctd ctd ctcrd ctcs ctd cxcrcpd cvcxdactd acd ctb cpd cpd cwct cqctd csctd cxd cxd cxd dactd cpd crd cpd ctcs cwct cpd cqctd cud csctd cxd cpc btcbcc ddd ctd btcbcc crd cwct cscxd cxcqd cxd ctd csct cpcvct crcpd cpcrcxb cxctd cqdd crd cpd cxd cwct cpcsdactd cxd ctcs cpcvct crcpd cpcrcxd ctdbd cyd cxd cxd csct dbcxd cwct cpdactd cpcvct cpcvct crcpd cpcrcxd csctd cxd cxd ctcpcu ctd cwct csct cxd cpd cvctb cxd cxd cpd czctcs cxd cpd cyd cxd csctd cxd csctc csd csct cxd cpd cxd cxd ctcyctcrd ctcsba csct cxd cud ctct cpcsdactd cxd cud cpcrd cxd cxd cpcrd cpd cscxd cpcrct cud cqddc btcbccba cccwct cpcsdactd cxd ctcs crcpd cpcrcxd cxd ctcs cpd cwct cqcpd cxd cud cwct cpcsd cxd cxd csctcrcxd cxd replica diversion cccwct ctd cxcrcp cscxdactd cxd cxd cqcpd cpd crct cwct ctd cpcxd cxd cud ctct cpcvct cpcrct cpd cwct csctd cxd ctcpcu ctd cactd cxcrcp cscxdactd cxd cxd cpcrcrd cxd cwctcs cpd cud dbd cfcwctd cpd cxd ctd ctd ctd ctd cpcvct acd ctcpcrcwctd csct dbcxd csctc cpd cwct ctd cxcrcpd crd ctd cwct acd ctc csb cwct csct crcwctcrczd ctct cxcu cxd crcpd cpcrcrd cscpd crd cwct acd cxd cxd crcpd cscxd czba cxd ctd cwct acd ctb cxd ctd ctcrctcxd cpd cud dbcpd csd cwct ctd cpcvct cwct cwctd csctd dbcxd csctc csd crd ctd cwct acd ctc csba cbcxd crct cwctd csctd ctdccxd cxd cwct csctb ctcpcu ctd cwct ctd cpcvct crcpd cqct cud dbcpd csctcs cscxd ctcrd ddb bxcpcrcwd cud cwctd csctd cxd cpd ctd ctd cxcrcp cwct acd cpd ctd ctcrctcxd csct crcpd cpcrcrd cscpd crd crcpd ddb cxd crd cxcsctd ctd cxcrcp cscxdactd cxd byd cwcxd ctb crcwd ctd csct cxd cxd ctcpcu ctd cwcpd cxd cpd cwct crd ctd cpd csd ctd cpd ctcpcsdd cwd cscxdactd ctcs ctd cxcrcp cwct acd ctba cpd czd crd cxd cqctcwcpd cub cwctd ctd ctd cpd ctd cud cwct acd cxd cxd cpcqd dbcxd cxd ctd bub cpd cxd ctd ctcrctcxd cpd cpd cfctd cpdd cwcpd cwcpd cscxdactd ctcs crd cwct acd csct buba bvcpd cqct cpczctd ctd cwcpd cscxdactd ctcs ctd cxcrcp crd cxcqd ctd cpd crcwd dbcpd csd cwct dactd cpd cpdacpcxd cpcqcxd cxd cwct acd cpd crcpd ctcs ctd cxcrcpba cpd cxcrd cpd dbct ctd cwcpd bdb cucpcxd csct crcpd ctd cwct crd ctcpd cxd ctd cpcrctb ctd ctd cxcrcpb cpd cwcpd beb cwct cucpcxd csct csd ctd ctd csctd cwct ctd cxcrcp ctcs cxd cpcrcrctd cxcqd ctba cxd cscxcsb cwctd ctdactd cscxdactd ctcs ctd cxcrcp dbd csd cqd cwct cqcpcqcxd cxd cwcpd cpd ctd cxcrcpd cxcvcwd cqct cxd cpcrcrctd cxcqd ctba cccwct csct cucpcxd ctcrd dactd crctcsd csctd crd cxcqctcs cxd cbctcrd cxd bfbabh ctd ctd crd cscxd cxd bdb bvd cscxd cxd beb crcpd cqct cpcrcwcxctdactcs cqddctd ctd cxd cxd ctd cwct ctd cxcrcp ctcs cxd cwct acd cpcqd cwct csct dbcxd cwct bdd crd ctd csctc cwct acd ctc csba csct cucpcxd cwctd csct cxd cpcxd cpcxd cxd ctd cwct ctd cxcrcp ctcs bub cpcxd cpcxd cxd cwct cxd dacpd cxcpd cwcpd cwct crd ctd csctd cpcxd cpcxd ctcxd cwctd ctd cxcrcp ctcuctd ctd crct cscxd cxd crd cscxdactd ctcs ctd cxcrcpba csct cucpcxd cwctd csct cxd cpd ctcuctd ctd crct cwct bdd crd ctd csctba cactd ctd ctd ctcs cxd cbctcrd cxd cwd cwcpd ctd cxcrcp cscxdactd cxd cpcrcwcxctdactd crcpd cpcvct cpcrct cqcpd cpd crcxd cpd cxd ctcrctd cpd cpcrcwcxctdact cwcxcvcw dactd cpd cpcvct cxd cxdecpd cxd cpd cvd cpcrctcud csctcvd cpb cscpd cxd cpd cwct btcbcc ddd ctd ctcpcrcwctd cxd cpcvct crcpd cpcrcxd ddbacccwct dactd cwctcpcs cscxdactd cxd ctd cxcrcp cxd cpd cpcscscxd cxd cpd ctd cxd cwct acd cpcqd ctd dbd csctd cpd bvb cqd ctd cxctd cxd cxd bub dbd cpcscscxd cxd cpd cac bvd csd cxd cxd ctd cpd cpcscscxd cxd cpd cac csd cxd czd cwcpd ctcpcrcwctd cwct cscxdactd ctcs crd ddba ccd cxd cxd cxdect cwct cxd cpcrd ctd cxcrcp cscxdactd cxd btcbccb ctd cud cpd crctb cpd cxcpd cxcrcxctd cqct ctcs cpdad cxcs ctcrctd cpd ctd cxcrcp cscxdactd cxd policies cfctd ctdcd csctd crd cxcqct cwct cxcrcxctd ctcs cxd btcbccd crd ctd cxcrcp cscxdactd cxd cccwctd cpd cwd ctct ctd ctdacpd cxcrcxctd cpd ctd bdb cpcrb crctd cpd crct ctd cxcrcpd cxd csctb crcpd ctb beb ctd ctcrd cxd csct cscxdactd ctcs ctd cxcrcpb cpd bfb csctcrcxcscxd dbcwctd cscxdactd acd cscxabctd ctd cpd cwct csctc cpcrctba crcwd cxd cpd cxcpd cxcrcxctd cud ctd cxcrcp cscxdactd cxd cwct cud dbcxd crd cxcsctd cpd cxd cpd ctd ctdacpd bycxd cxd cxd ctcrctd cpd cqcpd cpd crct cwct ctd cpcxd cxd cud ctct cpcvct cpcrct cpd csctd cpd cpd cwct cxd cxdecpd cxd cpd csctd cxd dbba bwd cxd dbd cwcpdact cpcsdacpd cpcvct cqd cxd crd cwct crd ctd cxcrcp cscxdactd cxd cbctcrd csb cxd cxd ctcuctd cpcqd cscxdactd cpd cvct acd cpd cwctd cwcpd cxd cpd ctd bwcxdactd cxd cpd cvct acd ctd ctcsd crctd cwct cxd ctd cxd 
dactd cwctcpcs ctd cxcrcp cscxdactd cxd cud cvcxdactd cpd cud ctct cpcrct cwcpd ctctcsd cqct cqcpd cpd crctcsbn cpczcxd cxd cpcrcrd cwcpd dbd czd cpcsd cpd cud ctd cqcxcpd ctcs dbcpd csd czd cpd acd ctd cxd crcpd cpd cxd cxd cxdect cwct cxd cpcrd cwct czd dactd cwctcpcs ctd cxcrcp cscxdactd cxd cccwcxd csb ctd cxcrcp cwd cscpd dbcpddd cqctcscxdactd ctcscud csctdbcwd ctd cpcxd cxd cud ctct cpcrct cxd cxcvd cxaccrcpd cqctd dbcpdactd cpcvct csct dbcwd cud ctct cpcrct cxd cxcvd cxaccrcpd cpcqd dact cpdactd cpcvctbn dbcwctd cwct cud ctct cpcrct cvctd cxcud cxd ctcpcu ctd cxd cxd cqctd ctd cscxdactd cwct acd cxd cpd cwctd cpd cwct csctc cpcrct cwcpd cpd ctd cscxdactd ctd cxcrcpd cpd cwct cxd ctcpcscxd crcpd cwcxcvcw cxd cxdecpd cxd ctcxcvcwcqd cxd cpd cwct csctc cpcrctba cccwct cxcrdd cud cpcrcrctd cxd ctd cxcrcp cqdd csct cxd cqcpd ctcs cwct ctd cxcr bpby dbcwctd ctcb cxd cwct cxdect acd cpd cxd cwct ctd cpcxd cxd cud ctct cpcvct cpcrct cpd csctc cpd cxcrd cpd csct ctcyctcrd acd cxcu bpby bqd cxbactbab dbd crd cwcpd cvcxdactd cud cpcrd cxd ctd cpcxd cxd cud ctct cpcrctba csctd cwcpd cpd cpd cwct ctd cxcrcpd crd ctd acd ctc cxd cpd ctd cxcrcp ctd cpd dbctd cpd csctd cpd cwct crd ctd cscxdactd ctcs ctd cxcrcp ctd cwct cpd crd cxd ctd cxd cwd dbctdactd cwct cud ctd cwd ctd cwd dbcwcxd cwct cpd ctd cscxda dbcwctd bqd cscxda cccwctd cpd ctdactd cpd cwcxd cvd cpcqd cwcxd cxcrddba bycxd cpd cxd cwcpd cwct cpdactd cpcvct acd cxdect cxd crcw cpd ctd cwcpd csctb cpdactd cpcvct cpcvct cxdectb btcbcc csct cpcrcrctd cpd cqd dactd cxdectcs acd ctd cpd cpd cxd cxd cxdecpd cxd cxd dbba cccwcxd ctd cpdad cxcsd ctcrctd cpd cscxdactd cxd dbcwcxd cwct csct cxd cwcpd ctd cpcrctba cbctcrd csb cwct cxcrdd cscxd crd cxd cxd cpd ctd cpcvcpcxd cpd cvct acd ctd cpd csctcrd ctcpd ctd cwct cxdect cwd ctd cwd cpcqd dact dbcwcxcrcwacd ctd cvctd ctcyctcrd ctcs cpd cwct csctb cxd cxdecpd cxd cxd crd ctcpd ctd cccwcxd cqcxcpd cxd cxd cxdectd cwct cqctd cscxdactd ctcs ctd cxcrcpd cpd ctd csd cscxb dactd cpd cvct acd ctd acd dbcwcxd ctcpdacxd cud cpd acd ctd cccwcxd csb cwct crd cxd ctd cxd cud cpcrcrctd cxd cscxdactd ctcs ctd cxcrcpd cxd ctd cxcrb cxdact cwcpd cwcpd cud cpcrcrctd cxd cxd cpd ctd cxcrcpd cwcxd ctd ctd cwcpd csct ctcpdactd cxd cpcrct cud cxd cpd ctd cxcrcpd cpd cwcpd ctd cxcrcpd cpd cscxdactd ctcs csct dbcxd cxcvd cxaccrcpd cud ctct cpcrctba cxd cpd csct cwcpd ctcyctcrd ctd cxcrcp ctctcsd ctd ctcrd cpd cwctd csct cwd cwct cscxdactd ctcs ctd cxcrcpba cccwct cxcrdd cxd crcwd cwct csct dbcxd cpdccxd cpd ctd cpcxd cxd cud ctct cpcrctb cpd cwct csctd cwcpd cpd cpb cxd cwct ctcpcu ctd cqb cwcpdactcpd csctc cwcpd cxd cpd cwct csctd crd ctd cwct acd ctc csb cpd crb csd cpd ctcpcsdd cwd cscxdactd ctcs ctd cxcrcp cwct cpd acd ctba cccwcxd cxcrdd ctd ctd cwcpd ctd cxcrcpd cpd cscxdactd ctcs cwct csct dbcxd cwct cud ctct cpcrctb cpd cwct ctd cxcvcxcqd csctd cwcpd ctd ctcrd ctcs csct cpdd ctcyctcrd cwct cscxdactd ctcs ctd cxcrcp cqcpd ctcs cwct cpcqd dactd ctd cxd ctcs cxcrdd cud cpcrcrctd cxd ctd cxcrcpd bycxd cpd ddb cwct cxcrdd cud cscxdactd cxd cpd ctd cxd acd cxd cpd cwctd cpd cwct csctc cpcrct cxd cpd cud dbd cfcwctd cwct csctd dbcxd csctc csd crd ctd cwct acd ctc csctcrd cxd ctd cxd ctd cxcrcpb cpd cwct csct cxd cwctd crcwd ctd cwd cwct cscxdactd ctcs ctd cxcrcp cpd csctcrd cxd ctd cwctd cwct ctd cxd acd cxd cscxdactd ctcsba cwcxd crcpd ctb cwct csctd cwcpd cwcpdact cpd ctcpcsdd ctcs ctd cxcrcp cscxd crcpd cwct ctd cxcrcpb cpd ctcvcpd cxdactcpcrczd dbd ctcscvd ctd ctd cpcvct cxd ctd ctcs cwct crd cxctd csctb crcpd cxd acd cscxdactd cxd file diversion cccwct acd cscxdactd cxd cxd cqcpd cpd crct cwct ctd cpcxd cxd cud ctct cpcvct cpcrct cpd cscxabctd ctd cxd cwct csctc cpcrct cxd btcbccba cfcwctd acd cxd ctd ctd cpd cxd cucpcxd cqctcrcpd cwct csctd crd ctd cwct crcwd ctd acd ctc crd cpcrcrd cscpd cwct acd cscxdactd cwct ctd cxcrcpd crcpd dbcxd cwcxd cwctcxd ctcpcu ctd ctcvcpd cxdactcpcrczd dbd ctcscvd ctd cxd ctd ctcs cwctcrd cxctd csctba cccwct crd cxctd csct cxd cvctd ctd cpd ctd ctdb acd ctc cxd cscxabctd ctd cpd dacpd cpd ctd cxctd cwct cxd ctd ctd cpd cxd btcrd cxctd csct cwctd ctd ctcpd cwcxd crctd cud cwd ctct cxd ctd cub cpcud ctd cud cpd ctd cwct cxd ctd ctd cpd cxd cxd cucpcxd cwct ctd cpd cxd cxd cpcqd ctcs cpd cpd cxd ctd cucpcxd cxd ctd ctcs cwct cpd cxcrcpd cxd cbd crcw cucpcxd cxd cscxcrcpd ctd cwcpd cwct ddd ctd dbcpd cpcqd crcpd cwct ctcrctd cpd cpcrct crd cxctd cwct acd ctba crcw crcpd ctd cpd cpd cxcrcpd cxd cpddcrcwd ctd cwct ctd cpd cxd dbcxd cpd ctd acd cxdect ctbacvba cqdd cud cpcvd ctd cxd cwct acd ctb cpd csbbd cpd ctd cqctd ctd cxcrcpd maintaining replicas btcbcc cpcxd cpcxd cwct cxd dacpd cxcpd cwcpd crd cxctd ctcpcrcw cxd ctd ctcs acd cpd cpcxd cpcxd ctcs cscxabctd ctd csctd dbcxd cwcxd ctcpcu ctd cccwcxd cxd cpcrcrd cxd cwctcs cpd cud dbd bycxd ctcrcpd cwcpd cpd cpd cwct cpd crd ctcxcvcwcqd cxd csctd cxd cwct csctc cpcrct ctd cxd cscxcrcpd ctdccrcwcpd cvct czctctd cpd cxdact ctd cpcvctd csct cxd ctd cxdact cud ctd cxd ccb cxd cxd ctd ctcs cucpcxd ctcs cpd cpd cxcvcvctd cpd cpcscyd ctd cwct ctcpcu ctd cxd cpd cpabctcrd ctcs csctd cbd ctcrcxaccrcpd ddb ctcpcrcw cwct csctd cxd cwct ctcpcu ctd cwct cucpcxd ctcs csct ctd dactd cwct cucpcxd ctcs csct cud cxd ctcpcu ctd cpd cxd crd csctd cxd ctcpcs cwct cxdact csct dbcxd cwct ctdcd crd ctd csctc csba cbctcrd csb dbcwctd ctdb csct cyd cxd cwct ddd ctd ctdacxd cucpcxd ctcs csct cvctd cqcpcrcz cxd ctb cxd cxd cpd cpcscyd ctd cwct ctcpcu ctd crcrd cxd 
cwct csctd cwcpd crd cxd cwct ctcpcu ctd cwct cyd cxd cxd csctba ctd ctb cwct cyd cxd cxd csct cxd cxd crd csctcs cpd cpd cwctd csct cxd csd ctcs cud ctcpcrcw cwct ctdacxd ctcpcu ctd btd cpd cwctd cpcscyd ctd csct cpdd cqctcrd cwct crd ctd csctd cud crctd cpcxd acd ctd cwct cpcvct cxd dacpd cxcpd ctd cxd ctd crcw csct cpcrd cxd ctd cxcrcp ctcpcrcwd crcwacd ctb cwd ctb crd ctcpd cxd ctd cxcrcpd cwcpd dbctd ctdacxd cwctd cqdd cwct cucpcxd ctcs csctba cbcxd cxd cpd ddb csct cpddcrctcpd ctd cqctd ctd cud cwctcz csctd cud crctd cpcxd acd ctd cwct cxd dacpd cxcpd cpd dbd csct cscxd crcpd crcw crd cxctd bzcxdactd cwct crd ctd cpd cxd cscxd cpcvct dactd dbcxcsctb cpd ctcp ctd ctd cqcpd csdbcxcsd cwb cxd cxd cxd ctb crd cxd cpd cxd crcxctd cud csct ctd ctd ctd cxcrcpd cpd acd ctd cud dbcwcxcrcw cxd cwcpd cyd cqctcrd cwct ctd cxcrcpd crd ctd csctd cccwcxd cxd cpd cxcrd cpd cqdacxd cxd cwct crcpd ctdb csct ctcrd dactd cxd csct dbcwd cscxd crd ctd dbctd cpd cpd cwct cucpcxd ctba ccd dact cwcxd cqd ctd cwct cyd cxd cxd csct cpdd cxd ctcpcs cxd cpd cxd ctd cxd cxd acd cpcqd ctb ctcuctd cxd cwct csct cwcpd cwcpd cyd crctcpd ctcs cqct cwct ctd cxcrcpd crd ctd cwct acd ctc csb cpd ctd cxd cxd cwcpd csct czctctd cwct ctd cxcrcpba cccwcxd crctd cxd ctd cpd cxcrcpd cxcsctd cxcrcpd ctd cxcrcp cscxdactd cxd cpd cwct ctdccxd cxd ctcrcwcpd cxd ctd cpdacpcxd cpcqcxd cxd cpd ctd ctcs ctct cbctcrb cxd bfbabfb cccwct cpabctcrd ctcs acd ctd crcpd cwctd cqct cvd cpcsd cpd cxcvd cpd ctcs cwct cyd cxd cxd csct cpd cpd cqcpcrczcvd ctd cpd cxd cfcwctd btcbcc ctd dbd cxd cvd dbcxd cvb csct cpcscscxd cxd cpdd crd ctb cpd cwct cxd cpd cxd dbcwctd csct cwcpd cwd csd cscxdactd ctcs ctd cxcrcp cpd cwct csct cwcpd ctcuctd cwcpd ctd cxcrcp cpd cvctd cpd cwct cpd ctcpcu ctd buctcrcpd crcw csctd cpd cpd cpd cxcrcpd cxacctcs ctcpcrcw cwctd cucpcxd ctb cwctdd ctdcd cxcrcxd ctdccrcwcpd cvct czctctd cpd cxdact ctd cpcvctd cpcxd cpcxd cwct cxd dacpd cxcpd ccd cxd cxd cxdect cwct cpd crcxcpd ctcs dactd cwctcpcsb cpabctcrd ctcs ctd cxcrcpd cpd cvd cpcsd cpd cxb cvd cpd ctcs csct dbcxd cwcxd cwctd ctcuctd cxd csctb ctcpcu ctd dbcwctd ctdactd cxcqd ctba bvd cxcsctd cwct crcpd dbcwctd csct cucpcxd cpd cwct cpcvct cxb cxdecpd cxd cxd cwcxcvcw cwcpd cwct ctd cpcxd cxd csctd cxd cwct ctcpcu ctd cpd cpcqd cpcscscxd cxd cpd ctd cxcrcpd ccd cpd btcbcc cpcxd cpcxd cxd cpcvct cxd dacpd cxcpd csctd cwctd crcxd crd cpd crctd csct cpd czd cwct dbd cscxd cpd ctd cqctd cxd ctcpcu ctd cxd cwct csctc cpcrctb crcpd csct cxd cwctcxd ctd ctcrd cxdact ctcpcu ctd cwcpd crcpd cwct acd ctba cbcxd crct ctdccpcrd cwcpd cwct csctb ctcpcu ctd dactd cpd dbcxd ctcpcrcwd cud cwctd dbd csctd ctcpcu ctd cpd bed csctd crcpd cqct ctcpcrcwctcs cxd cwcxd dbcpddba cbcwd cwctd csctd cqct cpcqd cpcrcrd cscpd cwct acd ctb cwctd cxd cxd cxczctd cwcpd cpcrct crcpd cqct cud cpd dddbcwctd cxd cwct ddd ctd cpd cwct cqctd ctd cxcrcpd cpdd ctd cpd cxd csd cqctd cxd csctd cscxd cpcrct cqctcrd cpdacpcxd cpcqd ctba cccwct cqd ctd dacpd ctcpcsctd cpddcwcpdact cxcrctcs cpd cwcxd cxd cwcpd cpcxd cpcxd cxd crd cxctd acd cxd btcbcc ddd ctd dbcxd cwcxcvcw cxd cxdecpd cxd cxd cxcqd cxcu cwct cpd cpd cscxd cpcvct cxd cwct ddd ctd csd ctd csctcrd ctcpd ctba cpd cscxd cpcvct dbctd csctcrd ctcpd csd csct cpd cscxd cucpcxd ctd cwcpd dbctd ctdactd cpd cqcpd cpd crctcs cqdd csct cpd cscxd cpcscscxd cxd cwctd cwct ddd ctd dbd ctdactd cpd ctdccwcpd cpd cxd cpcvctba buctddd crctd cpcxd cxd cwct ddd ctd dbd cqct cpcqd ctb ctd cxcrcpd acd ctd cpczct cud ctd cxcrcpd csd csct cucpcxd ctd cpcxd cpcxd cxd cpcsctd cpd ctd crctd cpd cxd cxdecpd cxd cxd cqb ctd cxd ddd ctd cxczctc btcbcc cwcpd cpd crctd cpd cpd cpcvctcsba btd cxd dacxcsct cxd crctd cxdactd cud ctd cqcpd cpd crct cwctcxd ctd crct crd cxd dbcxd cwct ctd crctd cwctdd crd cxcqd cwct ddd ctd btcbcc cpcscsd ctd ctd cwcxd cqd ctd cqdd cpcxd cpcxd cxd cpcvct cpd cwd ctd cxd cwcpd csctd cpd cud cpcvct crcpd ctdccrctctcs cwct ddba cud cscxd crd cxd cwctd cpd cpcvctd ctd cpd ctcrd cxd cpd ctcrd btcbcc cxd cqctddd cwct crd cwcxd cpd ctd file encoding cbd cxd crd ctd crd cxctd acd cxd cwct cpcvctb crcxctd ctd cwd cpcrcwcxctdact cwcxcvcw cpdacpcxd cpcqcxd cxd ddba cfcxd cactctcsb cbd ctd crd cscxd cvb cud cxd cpd crctb cpcscscxd cpcscscxd cxd cpd crcwctcrczb cqd crczd cxcvcxd cpd cscpd cqd crczd cpd ctd cpd cxdectb cpd dbd ctcrd dactd cud ctd cscpd crcwctcrczd cqd crczd cjbebfclba cccwcxd ctcsd crctd cwct cpcvct dactd cwctcpcs ctd cxd ctcs ctd cpd cucpcxd ctd cud bpd cxd ctd cwct acd cxdectba budd cud cpcvb ctd cxd acd cxd cpd cvct cqctd cscpd cqd crczd cwct cpcvct dactd cwctcpcs cud cpdacpcxd cpcqcxd cxd crcpd cqct cpcsct dactd cpd csctd ctd csctd cwct ctd crd cscxd cvb cxd cud cpcvd ctd acd cpd ctd cpd cpd csctd cpd cwctd ctcqdd cxd cxd cwct acd dactd ctdactd cpd cscxd czd crcpd cpd cxd dact cqcpd csdbcxcsd cwba dbctdactd cwctd ctd cxcpd cqctd ctacd cqct dbctcxcvcwctcs cpcvcpcxd cwct crd cxd ctd cpd ctd crddb cpcvcvd ctcvcpd ctd cpd ctd dbd cpcsb cpd cpdacpcxd cpcqcxd cxd ddb crd cpcrd cxd ctdactd cpd csctd ctd cxctdact acd ctba cccwcxd crd cpdd dbctcxcvcw cwct cqctd ctacd cud cpd cqd cpd cvct acd ctd ctdcd cxd cwcxd cxd cxd cud dbd czba cccwct cpcvct cpd cpcvctd ctd cxd ctd cscxd crd ctcs cxd cwcxd cpd ctd cwd dbctdactd cpd cpd cvctd cwd cvd cpd cwct crcwd cxcrct acd ctd crd cscxd cpd cxd cxd cvba caching cwcxd ctcrd cxd dbct csctd crd cxcqct crcpcrcwct cpd cpcvctd ctd cxd btcbccba cccwct cvd cpd 
crcpcrcwct cpd cpcvctd ctd cxd cxd cxd cxdect crd cxctd cpcrcrctd cpd ctd crcxctd cuctd crcw cscxd cpd crctb cpdccxd cxdect cwct ctd cwd cvcwd cpd cqcpd cpd crct cwct ctd cpcs cxd cwct ddd ctd cwcpd cqctcrcpd btcbcc cxd cxd cpd dactd cpdd ctd dbd czb cuctd crcw cscxd cpd crct cxd ctcpd ctcs cxd ctd cpd cxd cwd cccwct ctd cxcrcpd acd cpd cpcxd cpcxd ctcs cqddc btcbcc cxd cpd cxd cud ctcpd cpdacpcxd cpcqcxd cxd ddb cpd cwd cvcw csctcvd ctct ctd cpcs cqcpd cpd crcxd cpd cpd ctd crdd ctcsd crd cxd ctd ccd ctct cwcxd ctcrcpd cwcpd cwct csctd dbcxd cpcscycpcrctd csctc csd cwcpd crd cxctd acd cpd cxczctd cqct dbcxcsctd cscxd ctd ctcs cpd cwcpd cpd cxd cxczctd crd cxctd czd ctd ctd cwct ctd cxcrcp crd ctd cwct crd cxctd dbctdactd cwcxcvcwd cpd acd cpdd csctd cpd cpd cwcpd ctd cxcrcpd cxd csctd cpcxd cxd czd cpcs dbcwcxd cxd cxd cxdeb cxd crd cxctd cpd ctd crdd cpd ctd dbd crba byd cwctd ctb cxcu acd cxd cpd cpd crcpd crd ctd crd cxctd cxd cxd cpcsdacpd cpcvctd crd ctcpd ctcpcrcw crd ctd bvd ctcpd cxd cpd cpcxd cpcxd cxd crcw cpcscscxd cxd cpd crd cxctd cxd cwct cpd crcpcrcwct cpd cpcvctd ctd cxd btcbccba btcbcc csctd cwct ckd ctcsay cxd cwctcxd cpcsdactd cxd ctcs cscxd cpcrct crcpcrcwct acd ctd bvcpcrcwctcs crd cxctd crcpd cqct ctdacxcrd ctcs cpd cscxd crcpd csctcs cpd cpd cxd ctba cpd cxcrd cpd dbcwctd csct ctd ctdb cxd cpd ctcscxd ctcrd ctcs ctd cxcrcp acd ctb cxd ddd cxcrcpd ctdacxcrd crcpcrcwctcs acd ctd cpczct cud cwct ctd cxcrcpba cccwcxd cpd cpcrcw cwcpd cwct cpcsdacpd cpcvct cwcpd ctcs cscxd cpcrct cxd btcbcc cxd ctcs cxd dact ctd cud cpd crctbn cwct cwctd cwcpd csb cpd cwct cpcvct cxd cxdecpd cxd cwct ddd ctd cxd crd ctcpd ctd crcpcrcwct ctd cud cpd crct csctcvd cpcsctd cvd cpcrctcud ddba cccwct crcpcrcwct cxd ctd cxd cxcrdd cxd btcbcc cxd cpd cud dbd acd cwcpd cxd ctcs cwd cvcw csct cpd cpd czd cxd ctd ctd cpd cxd cxd cxd ctd ctcs cxd cwct crcpd cscxd crcpcrcwct cxcu cxd cxdect cxd ctd cwcpd cud cpcrd cxd cwct csctb crd ctd crcpcrcwct cxdectb cxbactbab cwct cxd cwct csctb cpcvct crd ctd ctcs cxd cpd cscxdactd ctcs ctd cxcrcpd cccwct crcpcrcwct ctd cpcrctd ctd cxcrdd ctcs cxd btcbcc cxd cqcpd ctcs cwct bzd ctctcsddbwd cpd cbcxdect bzbwb cbb cxcrddb dbcwcxcrcw dbcpd cxcvcxd cpd csctdactd ctcs cud crcpcrcwcxd cfctcq dccxctd cjbdbdclba bzbwb cpcxd cpcxd cpdbctcxcvcwd cud ctcpcrcwcrcpcrcwctcs acd ctba cdd cxd ctd cxd crcpcrcwct cwcxd cwctdbctcxcvcwd cpd crcxcpd ctcs dbcxd acd cxd ctd crb csb bpd csb dbcwctd crb csb ctd ctd ctd crd cpd crcxcpd ctcs dbcxd csb cpd csb cxd cwct cxdect cwct acd csba cfcwctd acd ctctcsd cqct ctd cpcrctcsb cwct acd cxd ctdacxcrd ctcs dbcwd cxd cxd cxd cpd cpd cpd crcpcrcwctcs acd ctd cccwctd cxd cqd cpcrd ctcs cud cwct dacpd ctd cpd ctd cpcxd cxd crcpcrcwctcs acd ctd cwct dacpd crb csb cxd ctd ctb cwct cxcrdd cpdccxd cxdectd cwct crcpcrcwct cwcxd cpd ctba cactd ctd ctd ctcs cxd cbctcrd cxd csctd cpd cwct ctabctcrd cxdactd ctd crcpcrcwcxd cvcxd btcbccba experimental results cwcxd ctcrd cxd dbct ctd ctd ctdcd ctd cxd ctd cpd ctd cqd cpcxd ctcs dbcxd ddd cxd ctd ctd cpd cxd btcbccba cccwct btcbcc csct cud dbcpd dbcpd cxd ctd ctd ctcs cxd cpdacpba ccd cqctcpcqd ctd ctd cud ctdcd ctd cxd ctd dbcxd cpd cvct ctd dbd czd cpd csctd dbct cpd cxd ctd ctd ctcs ctd dbd ctd cpd cxd ctd dacxd ctd cwd cvcw dbcwcxcrcw cwct cxd cpd crctd cwct csct cud dbcpd crd cxcrcpd ctba cpd ctdcd ctd cxd ctd ctd ctcs cxd cwcxd cpd ctd cwct cpd csctd dbctd crd accvd ctcs cxd cxd cvd cpdacp cec cccwcxd cxd cpd cvctd cpd cpd ctd cwct cpd cxd ctd ctd cpd cxd dgd cwct cpdacp cxd ddd ctd cpd cpd cxcrcpd ctcsd crctd crd cxcrcpd cxd cpd cwct cpd csctd crcpd cqcyctcrd cxd dad crcpd cxd btd ctdcd ctd cxd ctd dbctd ctd cud ctcs cpcsb crctd bvd cpd btd cwcpcbctd dactd bxcbbgbc bhbcbcc bebdbebibg btd cwcp bvc cdd dbcxd bzbuddd ctd cpcxd ctd ddb cxd ccd ctbibg cdc cgb dactd cxd bgbabcbyba cccwct cpd csct cud dbcpd dbcpd cxd ctd ctd ctcs cxd cpdacp cpd ctdcctcrd ctcs cxd bvd cpd cpdacp cbbwc dactd cxd bdbabebabeb cpd cwct bvd cpd bycpd cec dactd cxd bdbabebabeb bgba dbcpd dactd cxacctcs cwcpd cwct cpcvct cxd dacpd cxcpd cpd cpcxd cpcxd ctcs ctd csctd cxd cpd csd csct cucpcxd ctd cpd ctcrd dactd cxctd btcbcc cxd cpcqd cpcxd cpcxd cwctd cxd dacpd cxcpd cpd cpd cpd cxd cpcqd cpcxd cpcxd ctd ctcpcu ctd dbcwcxcrcw cxd cwct crcpd ctd cqd bpbecr csctd dbcxd cpcscycpcrctd csctc csd cucpcxd dbcxd cwcxd ctcrd dactd ctd cxd csba cccwct ctd cpcxd cxd ctdcd ctd cxd ctd cpd ctd cpd cscxdacxcsctcs cxd cwd cpd cpd dddecxd cwct ctabctcrd cxdactd ctd cwct btcbcc cpcvct cpd cpcvctb ctd cpd cwd ctdccpd cxd cxd cwct ctabctcrd cxdactd ctd cwct crcpcrcwcxd ctcs cxd btcbccba storage byd cwct ctdcd ctd cxd ctd ctdcd cxd cwct cpcvct cpd cpcvctd ctd dbd cscxabctd ctd dbd czd cpcsd dbctd ctcsba cccwct acd crd cxd ctd dbctcq dcdd cvd cud btc cud bhd cpd crcw bebcbcbdb dbcwcxcrcwdbctd crcpd ctcs crd cpcxd bgb bcbcbcb bcbcbc ctd cxctd ctcuctd ctd crb cpd cxd cpd cpcqd cpd cud btd cxctcs ctd dbd cactd ctcpd crcwb cud bmbbbbcxd crcpcrcwctbad cpd bad ctd bbccd cpcrctd cpd cxd cpd cbcrcxctd crct byd cscpb cxd cvd cpd bvcab blbibdbibibcbe cpd bvcab blbhbebdbjbgbhb cxd bdb bkbibfb bcbhbh cxd cdcac cpd cxd bdbkbabj bzbuddd ctd crd ctd dbcxd ctcpd acd cxdect bdbcb bhbdbj cqddd ctd ctcscxcpd acd cxdect bdb bfbdbe cqddd ctd cpd cpd cvctd bbd cpd ctd acd cxdect bdbfbk buddd ctd cpd cqddd ctd ctd ctcrd cxdactd ddba cccwct ctcrd cwct dbd czb cpcsd dbcpd cvctd ctd cpd ctcs cqdd crd cqcxd cxd acd cpd cpd acd cxdect 
cxd cud cpd cxd cud ctdactd cpd acd ddd ctd cpd cwct cpd cwd cwd cxd cxd cxd cccwct acd ctd dbctd ctcs cpd cwcpcqctd cxcrcpd cqddacd ctd cpd dacxcsct cpd csctd cxd cvba cccwct cpcrct crd cpcxd ctcs beb bcbebjb blbcbk acd ctd dbcxd crd cqcxd ctcs acd cxdect bdbibibabi bzbuddd ctd dbcxd ctcpd acd cxdect bkbkb bebfbf cqddd ctd ctcscxcpd acd cxdect bgb bhbjbk cqddd ctd cpd cpd cvctd bbd cpd ctd acd cxdect bebabj bzbuddd ctd cpd cqddd ctd ctd ctcrb cxdactd ddba cbctd ctcrd cxd cpd cpd cxcpd dbd czd cpcs ctdacpd cpd ddd ctd cxczct btcbcc cxd cscx crd cxd crct dbd czd cpcs cpcrctd cud ctdccxd cxd ctctd ctctd ddd ctd cpd cpdacpcxd cpcqd cpd ctd cpd cxdactd cxd cxd czd dbd cpcqd cwctcxd crcwcpd cpcrd ctd cxd cxcrd cjbebkclba cfctcrcwd dbctcq dcdd cpd acd ctd ddd ctd dbd czd cpcsd ctdacpd cpd cpcvct cpd cpcvctd ctd cpd crcpcrcwcxd cxd btcbccba cccwct acd ctb cxdect cscxd cxcqd cxd cxd cwct dbd dbd czd cpcsd cpd dactd cscxabctd ctd cpd cwd cqd cpcrczctd cwct cpd cvct cxdect cscxd cxcqd cxd cxczctd cqct ctd crd ctd ctcs cqddc btcbccba byd cwct ctd ctdacpd cpd cxd crcpcrcwcxd cvb cwct crcpd cxd ctd cxctd cxd cwct dbctcq dcdd cwd cvcxdact cvcw cxcsctcp cwct crcpd cxd dbd ctdcd ctcrd btcbcc dbd czd cpcsba cpd ctdcd ctd cxd ctd cwct cqctd ctd cxcrcpd cud ctcpcrcw acd dbcpd acdcctcs cpd bhb dbcpd acdcctcs cpd bgb cpd cwct cqctd btcbcc csctd dbcpd acdcctcs cpd bebebhbcba cccwct cqctd ctd cxcrcpd dbcpd crcwd ctd cqcpd ctcs cwct ctcpd ctd ctd cpd cscpd cpd ddd cxd cxd cjbkclb dbcwcxcrcw crd cxcsb ctd cpdacpcxd cpcqcxd cxd csctd czd crd ctd cxd crd cpd ctd dbd ctd dacxd ctd cccwct cpcvct cpcrct crd cxcqd ctcs cqddctcpcrcwc btcbcc csct dbcpd crcwd ctd cud crcpd ctcs cpd cscxd cxcqd cxd dbcxd ctcpd cpd cscpd csctdacxcpd cxd arb cpd dbcxd ctd cpd dbctd cxd cxd cpd dcar cpd dcarb ctd ctcrd cxdactd ddba cccpcqd cwd dbd cwct dacpd ctd cpd cud cud cscxd cxcqd cxd ctcs cxd cwct acd ctd ctdcd ctd cxd ctd cccwct dbctd cpd ctd cqd csd cxd cscxcrcpd dbcwctd cwct cpcxd cwct cpd cscxd cxcqd cxd dbctd crd cwct crcpd cpd cwct dbctd cpd ctd cqd dbcpd csctacd ctcs cpd bebmbfar cpd bebmbfarb ctd ctcrd cxdactd ddba byd cpd cwct dbctd cpd ctd cqd dbcpd acdcctcs cpd cqcxb cpd cxd ddb cpd cscpd cpd cvctar dbcpd ctcsba cfctcwcpdact cpd ctdcd ctd cxd ctd ctcs dbcxd cxcud cscxd cxcqd cxd cpd cud cwcpd cwct ctd dbctd cxcvd cxaccrcpd cpabctcrd ctcsba cccwct ctcpd cpcvct crcpd cpcrcxd cxctd cwctd cscxd cxcqd cxd cpd cpd dccxd cpd ctd cucpcrd bdbcbcbc cqctd dbcwcpd cxcvcwd ctdcd ctcrd cxd cpcrd cxcrctba cccwcxd crcpd cxd dbcpd ctcrctd cpd ctdcd ctd cxd ctd dbcxd cwcxcvcw cpcvct cxd cxdecpd cxd cpd cqd cpd cxcpd cqctd csctd cvcxdactd cwcpd cwct dbd czd cpcs cpcrctd cpdacpcxd cpcqd cwcpdact cxd cxd ctcs cpcvct ctd cxd ctd ctd cxcrct cwcpd ctcsd crcxd cwct csct cpcvct crcpd cpcrcxd ddcxd cwcxd dbcpdd cpczctd cpcvct cpd cpcvctd ctd cscx crd ctd cpd crd ctd dacpd cxdactba cccwct acd ctd ctdcd ctd cxd ctd cwct btc cpcrctd cccwct ctcxcvcwd ctd cpd cpd dbctcq cpcrctd dbctd crd cqcxd ctcsb ctd ctd dacxd cwct ctd cpd csctd cxd cwct ctd cxctd cxd ctcpcrcw crd ctcpd cxd cvd cvba cccwct acd bgb bcbcbcb bcbcbc ctd cxctd cwcpd dbctd ctcs cxd ctd ctd crctb dbcxd cwct acd cpd ctcpd cpd crct cdcac cqctcxd ctcs cxd ctd cwct acd cxd btcbccb cpd dbcxd cqd ctd ctd ctcuctd ctd crctd cwct cpd cdcac cxcvd ctcsba cdd ctd cwctd dbcxd cpd ctcsb cwct csct cpcvct cxdectd dbctd crcwd ctd cud cscxd cxcqd cxd bwcxd dbctd cdd ctd ccd cpd cpd cqd cqd crcpd cpcrcxd bebj bdbcbabk bhbd bibdb bcbcbl bebj blbabi bgbl bibdb bdbhbg bebj bhbgbabc bgbk bibdb bgblbf bebj bhbgbabc bhbf bhblb bhblbh cccpcqd bdbm cccwct cpd cpd ctd ctd cud cpd cscxd cxcqd cxd csct cpcvct cxdectd ctcs cxd cwct ctdcd ctd cxd ctd btd accvd ctd cxd buddd ctd cwct acd ctdcd ctd cxd ctd cqd ctd cxcrcp cscxdactd cxd cpd acd cscxb dactd cxd dbctd cscxd cpcqd ctcs cqdd ctd cxd cwct cwd ctd cwd cud cxd cpd ctd cxcrcp ctd bpbdb ctd cxd cwct cwd ctd cwd cud cwct cscxb dactd ctcs ctd cxcrcp ctd cscxda cpd cqdd csctcrd cpd cxd acd cxd ctd cxd ctcyctcrd ctcs cwct acd cxd ctd cucpcxd cxbactbab ctb cpd cxd cvb cccwct cwcxd ctdcd ctd cxd ctd cxd csctd cpd cwct ctctcs cud ctdcd cxcrcxd cpcvct cpcs cqcpd cpd crcxd cxd btcbccba cccwct ctd cxd dbctcq cpcrct dbcpd cpddctcs cpcvcpcxd cwct btcbcc ddd ctd cfcxd ctd cxcrcp cpd acd cscxdactd cxd bhbdbabdb cwct acd cxd ctd cxd cucpcxd ctcs cpd cwct cvd cqcpd cpcvct cxd cxdecpd cxd cwct btcbcc ddd ctd cpd cwct ctd cwct cpcrct dbcpd bibcbabkb cccwcxd crd ctcpd csctd cpd ctd cwct ctctcs cud cpcvct cpd cpcvctd ctd cxd cpd ddd ctd cxczctc btcbccba cccpcqd bed cwd dbd cwct ctd cwct cpd ctdcd ctd cxd ctd dbcxd acd cpd ctd cxcrcp cscxdactd cxd ctd cpcqd ctcsb bcbmbdb cscxda bcbmbcbhb cud cwct dacpd cxd cscxd cxcqd cxd cpcvct csct cxdectd cpd cud dbd ctd cxd cvd cwct ctcpcu ctd cxdect bdbi cpd bfbeba cccwct cpcqd cwd dbd cwct ctd crctd cpcvct crcrctd cud cpd crcrctd cud cxd ctd ckcbd crcrctd cpd ckbycpcxd ctd ctcrd cxdactd ddba cccwct ckbycxd cscxdactd cxd crd cwd dbd cwct ctd crctd cpcvct crcrctd cud cxd ctd cwcpd cxd dad dactcs acd cscxdactd cxd cxcqd cxd cxd ctd cpd ckcactd cxcrcp cscxdactd cxd cwd dbd cwctcud cpcrd cxd ctcs ctd cxcrcpd cwcpd dbctd cscxdactd ctcsba ckcdd cxd cwd dbd cwct cvd cqcpd cpcvct cxd cxdecpd cxd cwct btcbcc ddd ctd cpd cwct ctd cwct cpcrctba bwcxd cbd crcrctctcs bycpcxd bycxd cactd cxcrcp cdd cxd cpd cscxdactd cxd cscxdactd cxd bpbdbi blbjbabib bebabgb bkbabgb bdbgbabkb blbgbablb blbjbabkb bebabeb bkbabcb bdbfbabjb blbgbabkb blbibablb bfbabdb bkbabeb bdbjbabjb 
blbgbabcb blbgbabhb bhbabhb bdbcbabeb bebebabeb blbgbabdb bpbfbe blblbabfb bcbabjb bfbabhb bdbibabdb blbkbabeb blblbabgb bcbabib bfbabfb bdbhbabcb blbkbabdb blblbabgb bcbabib bfbabdb bdbkbabhb blbkbabdb blbjbablb bebabdb bgbabdb bebfbabfb blblbabfb cccpcqd bebm bxabctcrd dacpd ddcxd cwct cpcvct cscxd cxcqd cxd cpd ctcpcu ctd cxdectb dbcwctd bpbcbmbd cpd cscxda bpbcbmbcbhba cccwct ctd cxd cccpcqd cwd cwcpd cwct cpcvct cpd cpcvctd ctd cxd btcbcc cxd cwcxcvcwd ctabctcrd cxdactba bvd cpd ctcs cwct ctd dbcxd ctd cxcrcp acd cscxdactd cxd cwct cxd cxdecpd cxd cwcpd cxd ctd cud bibcbmbkb blbgb cpd blbkb dbcxd bpbdbicpd csd bfbeb ctd ctcrd cxdactd ddba byd cwctd ctb cwct cscxd cxcqd cxd csct cpcvct cxdectd cwcpd cactcrcpd cwcpd acd cxdectbbcud ctct cpcvct cpcrct cwd ctd cwd cud acd cqct ctcs csctba cxd cxd cpcrd cwct ctd cud cpd crct btcbccb cud cwct ctd cscxd cxcqd cxd ctcs cxd cwcxd ctdcd ctd cxd ctd btd cwct cqctd cpd csctd cxd crd ctcpd ctd cxd cpd cwctd cqctd ctd cxcrcp cscxb dactd cxd cpd csb ctd ctd csctcvd ctctb cwct cqctd acd cscxdactd cxd cxd crd ctcpd ctd dbcwcxcrcw cxd ctdcd ctcrd ctcsba cccwctd cxd cxcrctcpcqd cxd crd ctcpd cxd cwct ctd cud cpd crct dbcwctd cwct ctcpcu ctd cxdect cxd cxd crd ctcpd ctcs cud bdbi bfbeba cccwcxd cxd cqctcrcpd cpd cvctd ctcpcu ctd cxd crd ctcpd ctd cwct crd cud crcpd cpcs cqcpd cpd crcxd cvba cfcxd cwct cpcvct cxdect cscxd cxcqd cxd ctcs cxd ctdcd ctd cxd ctd cxd crd ctcpd cxd cwct ctcpcu ctd cxdect cqctddd bfbe ddcxctd csd cud cwctd cxd crd ctcpd cxd ctd cud cpd crctb cqd csd ctd cxd crd ctcpd cwct crd btcbcc csct cpd cxdacpd cpd csctd cpd ctba cccwctd ctcud ctb cud cwct ctd cpcxd csctd cwct ctdcd ctd cxd ctd ctcpcu ctd cxdect bfbe cxd ctcsba cccwct ctdcd ctd ctdcd ctd cxd ctd ctdccpd cxd ctd cwct ctd cxd cxdacxd ctd cwct ctd cxd cwct cpd cpd ctd ctd cpd cscxda dbcwcxcrcw crd ctd cxcrcp cpd acd cscxdactd cxd cwct acd cwctd ctdcb ctd cxd ctd cwct dacpd dbcpd dacpd cxctcs cqctd dbctctd bcbabcbh cpd bcbabh dbcwcxd czctctd cxd cscxda crd cpd cpd bcbabcbh cpd cxd cpd cwct csct cpcvct cxdect cscxd cxcqd cxd cccpcqd cwd dbd cwct ctd cbd crcrctctcs bycpcxd bycxd cactd cxcrcp cdd cxd cscxdactd cscxdactd bcbabh bkbkbabcbeb bdbdbablbkb bgbabgbfb bdbkbabkbcb blblbabjb bcbabe blbibabhbjb bfbabgbeb bgbabgbdb bdbkbabdbfb blblbabgb bcbabd blblbabfbfb bcbabibib bfbabgbjb bdbibabdbcb blbkbabeb bcbabcbh blblbabfbcb bcbabebjb bebabdbjb bdbebabkbib blbjbabgb cccpcqd bfbm ctd cxd cpd cxd cxcrd cpd cxd cxdecpd cxd btcbcc cpd cxd dacpd cxctcs cpd cscxda bpbcbmbcbhba bycxcvd cwd dbd cwct crd cpd cxdactcucpcxd ctd cpd cxd dactd cpcvct cxd cxdecpd cxd cud cwct cpd ctdcd ctd cxd ctd cccwct crd cpd cxdact cucpcxd cpd cxd cxd csctacd ctcs cpd cwct cpd cxd cpd cucpcxd ctcs acd cxd ctd cxd dactd cpd acd cxd ctd cxd cwcpd crcrd ctcs cwct cxd dbcwctd ctd cwct cvcxdactd cpcvct cxd cxdecpd cxd dbcpd ctcpcrcwctcsba cccwcxd cscpd cpb cxd crd cyd crd cxd dbcxd cccpcqd bfb cwd dbd cwcpd cpd cxd cxd crd ctcpd ctcsb cuctdbctd acd ctd cpd crcrctd cud cxd ctd ctcsb cqd cwcxcvcwctd cpcvct cxd cxdecpd cxd cxd cpcrcwcxctdactcsba cccwcxd crcpd cqct ctdcd cpcxd ctcs cqdd crd cxcsctd cxd cwcpd cxd cvctd ctd cpd cwct dbctd cwct dacpd cwct ctd cxczctd cxd cxd cwcpd cpd cvct acd crcpd cqct ctcs cpd cxcrd cpd btcbcc csctba cpd cpd acd ctd crcpd cqct ctcs cxd cpcrct cpd cvct acd ctbn cwctd ctcud ctb cwct cqctd acd ctd ctcs cxd crd ctcpd ctd cpd csctcrd ctcpd ctd cqd cwct cxd cxdecpd cxd csd cqctcrcpd cpd cvct acd ctd cpd cqctcxd ctcyctcrd ctcs cpd cxd cxdecpd cxd ctdactd cccwctd ctcud ctb dbcwctd cwct cpcvct cxd cxdecpd cxd cxd dbb cwcxcvcwctd cpd cxd ctd cxd cucpcxd cxd cqd ctd dactcs cud cpd ctd dacpd ctd cccpcqd cwd dbd cwct ctabctcrd dacpd ddcxd cwct cscxda cpd cpd ctd ctd cqctb dbctctd bcbabd cpd bcbabcbcbhb dbcwctd bpbcbmbd cpd cpcvct cxdect cscxd cxcqd cxd cxd ctcsba bycxcvd cwd dbd cwct crd cpd cxdact cucpcxd cpd cxd dactd cpcvct cxd cxdecpd cxd cud cwct cpd ctdcd ctd cxd ctd btd cxd cwct ctdcd ctd cxd ctd dacpd ddcxd cpd cwct dacpd cscxda cxd cxd crd ctcpd ctcs cwct cpcvct cxd cxdecpd cxd cxd dactd cqd cuctdbctd cxd ctd cxd crd ctd crcrctd cud ddb cud cwct cpd ctcpd cfct ctd ctcpd ctcs cwct ctd cxd cxdacxd ctdcd ctd cxd ctd cxd cwct acd ctd ddd ctd cpcrct cpd csb csctd cxd cwct cscxabctd ctd acd cxdect cscxd cxcqd cxd cxd cwcpd cpcrctb cwct ctd dbctd cxd cxd cpd bucpd ctcs cwctd ctdcd ctd cxd ctd dbct crd crd csct cwcpd bpbcbmbdcpd csd cscxda bpbcbmbcbh dacxcsct cvd cqcpd cpd crct cqctd dbctctd cpdccxd cpd cpcvct cxd cxdecpd cxd cpd dbacd cxd ctd cxd cucpcxd cpd cpd cpcvct cxd cxdecpd cxd utilization cumalative failure ratio bycxcvd bebm bvd cpd cxdact cucpcxd cpd cxd dactd cpcvct cxd cxdecpd cxd cpcrcwcxctdactcs cqdd dacpd ddcxd cwct cpd cpd ctd ctd cpd cscxda bpbcbmbcbhba cscxda cbd crcrctctcs bycpcxd bycxd cactd cxcrcp cdd cxd cscxdactd cscxdactd bcbabd blbfbabjbeb bibabebkb bhbabcbjb bdbfbabkbdb blblbabkb bcbabcbh blblbabfbfb bcbabibib bfbabgbjb bdbibabdbcb blbkbabeb bcbabcbd blblbabjbib bcbabebgb bcbabhbfb bdbhbabebcb blbfbabdb bcbabcbcbh blblbabhbjb bcbabgbfb bcbabhbfb bdbgbabjbeb blbcbabhb cccpcqd bgbm ctd cxd cpd cxd cxcrd cpd cxd cxdecpd cxd btcbcc cpd cscxda cxd dacpd cxctcs cpd bpbcbmbdba cccwct ctdcd ctd ctd ctdcd cxd csctd cpcxd cpd dbcwcpd cxb cxdecpd cxd ctdactd cwct acd cscxdactd cxd cpd ctd cxcrcp cscxdactd cxd cqctcvcxd cxd cpcrd btcbccb ctd cud cpd crctba bycxcvd cwd dbd cwct ctd crctd cpcvct cxd ctd ctcs acd ctd cwcpd cpd cscxdactd ctcsd crctb dbcxcrct cwd ctct cxd ctd cpd cwctcrd cpd cxdactcucpcxd cpd cxd dactd cpcvct cxd cxdecpb cxd cccwct ctd cwd cwcpd acd cscxdactd cxd cpd ctcvd cxcvcxcqd cpd cpd cpcvct cxd cxdecpd cxd cxd cqctd bkbfb cpdccxd cwd ctct acd cscxdactd cxd cpd ctd cpd cpcsct cqctcud cpd cxd ctd cxd cxd crd cxcsctd ctcs cucpcxd ctcsba bycxcvd cwd dbd cwct cpd cxd ctd cxcrcpd cwcpd cpd 
cscxdactd ctcs cwct cpd ctd cxcrcpd ctcs cxd btcbccb dactd cpcvct cxd cxdecpd cxd btd crcpd cqct ctctd cwct cqctd cscxdactd ctcs ctd cxcrcpd ctd cpcxd cpd ctdactd cpd cwcxcvcw cxd cxdecpd cxd cpd bkbcb cxd cxdecpd cxd ctd cwcpd bdbcb cwct ctd cxcrcpd ctcs cxd btcbcc cpd cscxdactd ctcs ctd cxcrcpd cccwctd cpd dbd ctd ctd cwd dbd cwcpd cwctd dactd cwctcpcs cxd ctcscqdd ctd cxcrcp cpd acd cscxdactd cxd cxd csctd cpd cpd cpd cwct cxd cxdecpd cxd cxd ctd cwcpd cpcqd blbhb bxdactd cpd cwcxcvcwctd cxd cxdecpd cxd cwct dactd cwctcpcs ctd cpcxd cpcrcrctd cpcqd ctba cccwct ctdcd ctd cwd dbd cwct cxdect cscxd cxcqd cxd cwct acd ctd cwcpd crd cqct cxd ctd ctcs cxd btcbccb cpd cud crd cxd cxd cxdecpb cxd bycxcvd cwd dbd crcpd ctd cxd ctd cxd cucpcxd ctd cqdd acd cxdect ctcud dactd cxcrcpd cpdccxd dactd cxd cxdecpd cxd ctdactd cpd dbcwcxcrcw cwct cucpcxd crcrd ctcsba btd cwd dbd cxd cwct cud cpcrd cxd cucpcxd ctcs cxd ctd cxd dactd cxd cxdecpd cxd cxcvcwd dactd cxcrcpd cpdccxd cwct cvd cpd cwb acd ctd cpd cvctd cwcpd cwct dbctd cqd cwct cpcvctb crcpd cpcrcxd ddcscxd cxcqd cxd cpd cwd dbd cxd cwct btc cpcrctb acd ctd cpd cpd cvctd cwcpd cwct ctd cpcvct crcpd cpcrcxd cqd csb bebc cpd cpd cvctd cwcpd cwct ctcpd cpcvct crcpd cpcrcxd ddb cpd blbibg cpd cpd cvctd cwcpd cwct dbctd cpcvct crcpd cpcrcxd cqd csba cccwct cqctd acd ctd cpd cvctd cwcpd cwct dbctd cpcvct crcpd cpcrcxd cqd cwcpd dbctd crcrctd cud cxd ctd ctcs dbcpd blba cwct acd ctd cpd cvctd cwcpd cwct ctcpd utilization cumulative failure ratio bycxcvd bfbm bvd cpd cxdact cucpcxd cpd cxd dactd cpcvct cxd cxdecpd cxd cpcrcwcxctdactcs cqdd dacpd ddcxd cwct cscxda cpd cpd ctd ctd cpd bpbcbmbdba utilization cumulative ratio redirects redirects redirect insertion failure bycxcvd bgbm cacpd cxd acd cscxdactd cxd cpd crd cpd cxdact cxd ctd cxd cucpcxd ctd dactd cpcvct cxd cxdecpd cxd bpbcbmbd cpd cscxda bpbcbmbcbhba cpcvct crcpd cpcrcxd dddbctd crcrctd cud cxd ctd ctcsba bycxcvd cwd dbd cwcpd cpd cwct cpcvct cxd cxdecpd cxd cxd crd ctcpd ctd cpd ctd acd ctd cucpcxd cqct cxd ctd ctcsba dbctdactd cwct cxd cxdecpd cxd ctcpcrcwctd blbcbabhb cqctcud acd cpdactd cpcvct cxdect bdbcb bhbdbj cqddd ctd cxd ctcyctcrd ctcs cud cwct acd cxd ctba cdd cxd cyd dactd bkbcb cxd cxdecpd cxd acd ctd cpd ctd cwcpd bcbabh buddd ctd ctbacvbab bebhb cwct cxd cxd cpd csct cpcvct crcpd cpcrcxd ddb cxd ctcyctcrd ctcsba ctd dactd cwct cpd cpd cucpcxd ctcs cxd ctd cxd cxd ctdcd ctd ctd cpd cpd cxd cxdecpd cxd cqctd blbcb cpd ctdactd cpd blbhb cxd cxdecpd cxd cwct cpd cpd cucpcxd ctd cxd cqctd bcbabcbhb ctcpcrcwcxd bcbabebh cpd blbkb cpdacxd cwd dbd cwct ctd cxctd btcbcc cxd cwct btc cpcrctd dbct crd cxcsctd ctd cxd cwct acd ctd ddd ctd dbd czb cpcsba cccwct cpd cxdect cpd cwct acd ctd cxd cwcpd dbd czd cpcs cxd cxcvb cxaccrcpd cpd cvctd cwcpd cxd cwct btc dbctcq dcdd cpcrctba cccwct cpd cqctd btcbcc csctd bebebhbcb cxd ctcs cxd cwct ctdcd ctd cxd ctd cwctd ctcud cwct cpcvct crcpd cpcrcxd ddcrd cxcqd ctcs cqddctcpcrcw csct cwcpd cqct cxd crd ctcpd ctcsba byd cwcxd ctdcd ctd cxd ctd dbct ctcs cvctd ctd cpd cwct cpcvct crcpd cpcrcxd cxctd cqd cxd crd ctcpd ctcs cwct cpcvct crcpd cpcrcxd ddd cuctcpcrcwd csctcqdd cucpcrd bdbcba cccwct ctd cxd dbctd bbd ctd cqd cpcvct crcpd cpcrcxd cxd bebc cqddd ctd cpd bhbdbc cqddd ctd ctd ctcrd cxdactd ddb dbcwcxd cwct ctcpd cxd bebjbc cqddd ctd utilization mulative ratio eplica iversions bycxcvd bhbm bvd cpd cxdact cpd cxd ctd cxcrcp cscxdactd cxd dactd cpcvct cxd cxdecpd cxd dbcwctd bpbcbmbd cpd cscxda bpbcbmbcbhba utilization file size bytes failure ratio failed insertion failure ratio bycxcvd bibm bycxd cxd ctd cxd cucpcxd ctd dactd cpcvct cxb cxdecpd cxd cud cwct btc cpcrctb dbcwctd bcbmbdb cscxda bpbcbmbcbhba cccwct cpd cpcvct crcpd cpcrcxd cwct bebebhbc csctd cxd bhblbi bzbuddd ctd bycxcvd cwd dbd ctd cwct cpd ctdcd ctd cxd ctd cpd bycxcvd bib cqd cxd cwct acd ctd ddd ctd dbd czd cpcsba btd cqctcud ctb acd ctd cpd cvctd cwcpd cwct cpd ctd cpcvct crcpd cpcrcxd cpd cwd dbd cxd cwct acd ctd ddd ctd cpcsb acd ctd cpd cpd cvctd cwcpd cwct ctd cpcvct crcpd cpcrcxd ddb bdbd cpd cpd cvctd cwcpd cwct ctcpd cpcvct crcpd cpcrcxd ddb cpd bibjbl cpd cpd cvctd cwcpd cwct dbctd cpcvct crcpd cpcrcxd cqd csba cccwct cqctd acd ctd cpd cvctd cwcpd cwct cpd ctd cpcvct crcpd cpcrcxd ddd cwcpd dbctd crcrctd cud cxd ctd ctcs dbcpd bebfb cpd cwct acd ctd cpd cvctd cwcpd cwct ctcpd cpcvct crcpd cpcrcxd dddbctd cxd ctd ctcs crcrctd cud ddba caching cccwct ctd ctd ctd ctcs cxd cwcxd ctcrd cxd csctd cpd cwct cxd cpcrd crcpcrcwcxd cxd btcbccba ctdcd ctd cxd ctd ctd cwct btc cpcrctba cccwct cpcrct crd cpcxd bjbjbh cxd crd cxctd dbcwcxcrcw cpd cpd ctcs btcbcc csctd crcw cwcpd ctd ctd cud crd cxctd cxd cwct cpcrct cxd cxd ctcs cud cwct crd ctd cscxd btcbcc csctba cccwct cpd cxd cxd cpcrcwcxctdactcs cpd cud dbd cccwctd cpd ctcxcvcwd cxd cscxdacxcsd cpd dbctcq dcdd cpcrctd dbcwcxcrcwcpd ctcrd cqcxd ctcsb ctd ctd dacxd ctd cpd csctd cxd crd ctcpd cwct cxd cvd cpcrct ctcs cxd cwct ctdcd ctd cxd ctd cccwctd ctcxcvcwd cpcrctd crd cud ctdactd dcdd ctd dactd cscxd cxcqd ctcs cvctd cvd cpd cwcxcrcpd cpcrd cwct cdcbbtba cfcwctd ctdb crd cxctd cxcsctd cxacctd cxd cud cxd cpcrctb ctdb csct cxd cpd cxcvd ctcs cxd cxd crcwcpdbcpdd ctd cwcpd ctd ctd cud cwct cpd cpcrct cpd utilization file size bytes failure ratio failed insertion failure ratio bycxcvd bjbm bycxd cxd ctd cxd cucpcxd ctd dactd cpcvct cxb cxdecpd cxd cud cwct acd ctd ddd ctd dbd czd cpcsb dbcwctd bpbcbmbdb cscxda bpbcbmbcbhba cxd ctcs cud btcbcc csctd cwcpd cpd crd ctcpcrcw cwctd cxd ctd cpd ctcs ctd dbd czba cccwct acd cxd cdcac cxd ctctd cxd cwct cpcrctb cwct 
ctcuctd ctd crctcs acd cxd cxd ctd ctcs cxd btcbccbn cqd ctd ctd crcrd ctd crctd cwct cdcac crcpd czd cqct ctd cud ctcsba bud cwct cxd ctd cxd cpd czd cpd ctd cud ctcs cud cwct btcbcc csct cwcpd cpd crcwctd cwct crd cxctd cxcsctd cxacctd cud cwct ctd cpd cxd cxd cwct cpcrctba bycxd ctd cpd crcpcrcwctcs cpd btcbcc csctd csd cxd crcrctd cud cxd ctd cxd cpd csd cxd crcrctd cud czd cpd cwct csctd cwd cvcw dbcwcxcrcw cwct ctd ctd cxd ctcsba cccwct cpd cpd ctd ctd cxd ctd bdba btd cqctcud ctb cwct ctdcd ctd cxd ctd ctd bebebhbc btcbcc csctd dbcxd cwct cpcvct crcpd cpcrcxd cscxd cxcqd cxd bpbcbmbdcpd csd cscxda bpbcbmbcbhba utilization global cache hit rate average number routing hops gd-s hit rate lru hit rate gd-s hops lru hops hops hops lru hit rate gd-s hit rate lru hops gd-s hops bycxcvd bkbm bzd cqcpd crcpcrcwct cwcxd cpd cxd cpd cpdactd cpcvct cqctd ctd cpcvct cwd dactd cxd cxdecpd cxd cxd ctcpd cactcrctd ddb cdd ctcs cacdb bzd ctctcsddbwd cpd cbcxdect bzbwb cbb cpd crcpcrcwcxd cvb dbcxd bpbcbmbd cpd cscxda bpbcbmbcbhba bycxcvd cwd dbd cqd cwct cqctd cxd cwd ctd cxd ctcs ctd cud crcrctd cud czd cpd cwct cvd cqcpd crcpcrcwct cwcxd cpd cxd dactd cxd cxdecpd cxd cccwct bzd ctctcsddbwd cpd cbcxdect bzbwb cbb cxcrdd csctb crd cxcqctcs cxd cbctcrd cxd cxd ctcsba byd crd cpd cxd dbct cpd cxd crd csct ctd dbcxd cwct ctcpd cactcrctd ddb cdd ctcs cacdb cxcrddba cfcwctd cwct crcpcrcwcxd cxd cscxd cpcqd ctcsb cwct cqctd cxd cwd cpdactd cpcvct ctd cxd ctcs cxd crd cpd cpcqd bjbcb cxd cxdecpd cxd cpd cwctd cqctcvcxd cxd cxcvcwd ddba cccwcxd cxd csd ctd cxcrcp cscxdactd cxd crcrd cxd cvbn cwctd ctcud ctb cpd ctd crctd cpcvct cwct czd cscxdactd ctcs ctd cxcrcp cxd ctd cxctdactcsb cpcscscxd cpd ctdcd cxd cwd cwd cqct ctcs cwcpd csd bdbi bebebhbcct bpbfbacccwct cvd cqcpd crcpcrcwct cwcxd cpd cud cqd cwct cacd cpd cwct bzbwb cpd cvd cxd cwd csctcrd ctcpd ctd cpd cpcvct cxd cxdecpd cxd cxd crd ctcpd ctd buctcrcpd cwct cicxd cub cxczct cscxd cxb cqd cxd dbctcq ctd ctd cjbdbcclb cxd cxd cxczctd cwcpd cpd cqctd acd ctd cpd cqctcxd ctd ctd ctcs dactd cud ctd cccwctd ctcud ctb dbcwctd cwct ddd ctd cwcpd cxd cxdecpd cxd cwctd acd ctd cpd cxczctd cqct dbcxcsctd crcpcrcwctcsba btd cwct cpcvct cxd cxdecpd cxd cxd crd ctcpd ctd cpd cwct cqctd acd ctd cxd crd ctcpd ctd cwct crcpcrcwctd cqctcvcxd ctd cpcrct acd ctd cccwcxd ctcpcsd cwct cvd cqcpd crcpcrcwct cwcxd cpd csd cxd cvba cccwct cpdactd cpcvct cqctd cxd cwd cud cqd cacd cpd bzbwb cxd cscxcrcpd ctd cwct ctd cud cpd crct cqctd ctacd crcpcrcwcxd cvb cxd ctd crd cxctd cpd ctd crdd cpd ctd dbd crba btd cpcvct cxd cxdecpb cxd crd ctcpd cwct acd ctd cpd cqctcxd crcpcrcwctcs cxd cwct ctd dbd crd dbcwctd cwctdd cpd ctd ctd ctcsba btd cwct cvd cqcpd crcpcrcwct cwcxd cpd cxd dbctd dbcxd cxd crd ctcpd cxd cpcvct cxd cxdecpd cxd cwct cpdactd cpcvct cqctd cxd cwd cxd crd ctcpd ctd dbctdactd ctdactd cpd cpcvct cxd cxdecpd cxd blblb cwct cpdactd cpcvct cqctd cwd cxd cqctd cwct ctd dbcxd crcpcrcwcxd cvba cccwcxd cxd cxczctd cqctcrcpd cwct acd cxdectd cxd cwct dcdd cpcrct cwcpdact ctcscxcpd dacpd bdb bfbdbe cqddd ctd cwctd crctb ctdactd cpd cwcxcvcw cpcvct cxd cxdecpd cxd cwctd cxd crcpd cpcrcxd ddd crcpcrcwct cwctd cpd acd ctd ctd cvd cqcpd crcpcrcwct cwcxd cpd cxd cpd cpdactd cpcvct cqctd cxd cwd bzbwb ctd cud cqctd ctd cwcpd cacdba cfctcwcpdact csctd cxcqctd cpd ctd ctd ctcs czd ctd cud cpd crct cxd ctd cwct cqctd cpd cxd cwd cqctcrcpd cpcrd cpd czd csctd cpddd cvd csctd ctd ctd cwd ctd dbd csctd cpddd ccd cvcxdact cpd cxd cscxcrcpd cxd cpcrd cpd csctd cpddd crcpd cqddc btcbcc cxd ctd cub ctd cxctdab cxd bdc acd cud csct cpd cwd cpdbcpdd btc cpczctd cpd dccxd cpd ctd bebhd cccwcxd ctd crcpd cxczctd cqct cxd dactcs cqd cpd cxcpd dbcxd cpd cxcpd ctd cud cpd crct cxd cxd ddd cxd ctd ctd cpd cxd related work cccwctd cpd crd ctd ctdactd cpd ctctd ctctd ddd ctd cxd ctb cpd cpd cpd csctd csctdactd ctd btd cwct cxb ctd cpd acd cwcpd cxd cucpcrcxd cxd cxctd crcwcpd bzd ctd cjbecl cpd byd ctctb ctd cjbdbfclba cccwct cpd ctd cjbdcl cxcr ctdccrcwcpd cvct ctd dacxcrct dacxcsctcs crcw cwct cxcvcxd cpd cxdacpd cxd cud ctctd ctctd ddd ctd cqd cxd cxd ctctd ctctd ddd ctd cqctcrcpd cxd cscpd cpcqcpd cxd crctd cpd cxdectcsba btd cwd ctct ddd ctd cpd cxd cpd cxd cxd ctd csctcs cud cwct cpd cvctb crcpd cwcpd cxd cscpd acd ctd ctd cxd ctd crct cpd ctd cxcpcqd crd ctd crcpd cxd cpd cvd cpd cpd ctctcs ctcrctd cpd cxd cwcxd ctd dacxd ctd crd cpd cxd btcbcc cpcxd cpd crd cqcxd cxd cwct crcpd cpcqcxd cxd ddcpd ctd cub cvcpd cxdecpd cxd ddd ctd cxczctbyd ctctc ctd dbcxd cwctd ctd cxd ctd crct cpd ctd cxcpcqcxd cxd ctdcd ctcrd ctcs cpd cpd crcwcxdacpd cpcvct ddd ctd cwcxd ctcvcpd csb cxd cxd crd ctd ctd cpd ctcs dbcxd cyctcrd cxczct crctcpd cbd cjbebcclb bycpd cbcxd cjbkclb byd ctctc cpdactd cjbdbhclb cpd bxd ctd cxd cjbhclba byd ctctc ctd byd ctctc cpdactd cpd bxd ctd cxd cpd cud crd ctcs dacxcscxd cpd ddd cxd cpd cpd cxb crctd cwcxd crctcpd cbd dacxcsctd cvd cqcpd cpd cpcrd cxd cpd ctd cxd ctd cpcvct ctd dacxcrct cwcpd ctd cxcpd cxdecpcqd cscpd ctd dbcxcsctd ctd cxb crcpd ctcs cpd cpcscxcr cscpd cpba crd cpd btcbcc dacxcsctd cxd ctb ctcpd cpcvct cpcqd cpcrd cxd cud ctd cxd ctd cxd cpcqd acd ctd dbcxd cwct cxd ctd cxd cwcpd cwcxd cxcrcpd ctcs cpcvct ctd cpd cxcrd ctbacvbab cpcqd acd ctd cqct cqd cxd btcbcc cxcu ctctcsctcsba cdd cxczctc btcbccb bycpd cbcxd cwcpd cpcscxd cxd cpd acd ctd ddd ctd ctd cpd cxcrd cscxd cxcqd ctcs cscxd ctcrd ctd dacxcrct cxd 
ctcs cxd bycpd cbcxd crcpd crd ctd cwcxd cxd cscxabctd ctd cud btcbccb cpd crcwctd ctb dbcwcxcrcw cxd ctcvd cpd ctd crd ctd crcpd cxd cpd cxd cvba bvd ctd ddb cwctd cxd cqd cxd cwctcs crcpd cpcqcxd cxd cpd cpd ddd cxd bycpd cbcxd ctba bxdactd csct cpcxd cpcxd cpd cxcpd cxd cwct cxdact csctd cud dbcwcxcrcw cxd crcwd ctd csctd cwcpd cwd ctd cxcrcpd crcwd cubycpd cbcxd ctb csctd cxcvd cxd cxdacpd ctcs cqddcpcuctcpd cxcqcxd cxd csdd cwcpd ctcpd ctd crd cpd btc cjbkclbn cxd cpd cxd cpdd cwd cxd dbcxcsctb cpd ctcp ctd dacxd ctd cpd ddb cpd dbcxd cccpd ctd cjbfbdclb bvcwd cjbfbccl cpd bvbtc cjbebhclb ctd ctd ctd ctcrd cvctd ctd cpd cxd ctctd ctctd cxd cpd crcpd cxd crcwctd ctd cwcpd dbctd cxd cxd ctcs cqdd cwct cxd ctctd cxd dbd ddd ctd cxczctbyd ctctc ctd cpd bzd ctd cpba cdd cxczct cwcpd ctcpd cxctd dbd czb cwctdd cvd cpd cpd ctct csctacd cxd cpd dbctd ctd cxd cqd csctcs cqctd ctd dbd cwd dbcwcxd ctd cpcxd cxd cwct crcpd cpcqcxd cxd byd ctctc ctd cpd cwct ctd cub cvcpd cxdecxd ctd cxctd cqd byd ctctc ctd cpd bzd ctd cpba cpd cpd cccpd ctd cqctcpd cxd cxd cpd cxd cwct dbd cqdd cpdcd ctd cpd cjbebgclba cccwct cpd cpcrcw cxd cqcpd ctcs cpcscsd ctd ctacdcctd dbcwcxcrcw crcpd cqct dacxctdbctcs cpd cvctd ctd cpd cxdecpd cxd cwddd ctd crd cqct cxd cvb cxd crd cpd cwd ctct crcwctd ctd dbctdactd cxd cwct cpdcd crcwctd cwctd cxd ctcrcxcpd csct cpd crcxcpd ctcs dbcxd ctcpcrcw acd ctb dbcwcxcrcw cud cxd cvd cxd cucpcxd ctba btd cpdcb csd ctd cwcpd csd cpd cpd cxcr csct cxd ctcvd cpd cxd cpd cucpcxd ctcrd dactd ddb cxbactbab cxd cxd ctd cub cvcpd cxdecxd cvba cpd cpd cccpd ctd cscxabctd cxd cwctcxd cpd cpcrcwd cpcrcwcxctdacxd ctd dbd crcpd cxd ddcpd csd ctd cxcrcpd cxd cqcyctcrd cpd cpd cpd ctcpd cqct ctd crd ctdcba cccwct bvcwd crd cxd crd ctd ctd cpd ctcs cqd cpd cpd cccpd ctd ddb cqd cxd ctcpcs cxd dbcpd csd csctd cwcpd cwcpd crcrctd cxdactd cvctd cpcscsd ctd ctacdcctd dbcxd cwct csctd cxd cpd cxd bvcwd cud dbcpd csd ctd cpcvctd cqcpd ctcs ctd cxcrcpd cscxabctd ctd crct dbcxd cwct csctd cxd cpd cxd cpcscsd ctd cdd cxczctc cpd cpd cccpd ctd ddb bvcwd cpczctd ctdcd cxcrcxd ctabd cpcrcwcxctdact cvd ctd dbd crcpd cxd ddba bvbtc ctd ctd cpcvctd cxd csb cscxd ctd cxd cpd cpcrctb dbcwctd ctcpcrcw csct cpcxd cpcxd cxd cpcqd dbcxd csb ctd cxctd cpd cpd csct crcpd cqct ctcpcrcwctcs cxd csc bdbpcs cxd cwd cdd cxczctc cpd ddb cwct cxd cpcqd csd ctd cvd dbcxd cwct ctd dbd cxdectb cqd cwct cqctd cxd cwd cvd dbd cucpd ctd cwcpd cvc bvbycb cjbdbgcl cxd csctcrctd cpd cxdectcsb crd ctd cpd cxdact ctcpcsb cpcvct ddd ctd cxczctc btcbccb cxd cxd cqd cxd ctctd ctctd cxd cpd czd cqd cpd ctb cxd cwcxd crcpd bvcwd csba cdd cxczctc btcbccb cxd cxd cxd ctd csctcs ctd cpd acd cwcpd cxd ctcscxd cpd cwd dacxcsctd dbctcpcz ctd cxd ctd crctba bvbycb cpcvct cxd cqd crczb cxctd ctcs cpd crd dactd cxd cpd cdc cgb cxczct acd ctd ddd ctd cxd cpddctd ctcs cxd bxcpcrcwcqd crcz cxd ctcs cxd csctd dbcxd cpcscycpcrctd bvcwd csct cxcsd cpd cpd cqd crczd crcpd cqct crcpcrcwctcs cpd cpcsb cscxd cxd cpd csctd cxd cxd cpd cwct dbcpddctd cxd acd ctd cpd ctcs cxd btcbccba bvd cpd ctcs btcbccb cwcxd cxd crd ctcpd ctd acd ctd cxctdacpd dactd cwctcpcsb cpd ctcpcrcw acd cscpd cpd ctd cpcscpd cqd crczd cqct crcpd ctcs cxd ctd cpd cpd bvcwd czd cwct cwctd cwcpd csb bvbycb ctd cxd cpd cpd ctd cqd crcz ctd cxctdacpd dbcwcxcrcw cqctd ctacd cpd cvct acd ctd bvbycbb csctd cxcvd cpd ctd cpd cpcqd cscpd crctd cud ctct cscxd czd cpcrctba bvd cqcxd ctcs dbcxd cxd cqd crczd cxctd cpd cxd cpd dbctcpcz ctd cxd ctd crctb cwcxd cxd cxacctd cxd cpcvct cpd cpcvctd ctd dbcwctd crd cpd ctcs ddd ctd cxczctc btcbccba ccd cpcrcrd cscpd csctd dbcxd cwcpd cwct cxd cxd cpd cpcvct cxdectb bvbycb ctd cxctd cwd cxd cxd cvcxb crcpd csctd ctd cwddd cxcrcpd csctd ctcpcrcw dbcxd ctd cpd cpd bvcwd cxcsba btcbcc ctd cwcxd ctcrcwd cxd cud csctd dbcwd cpcvct cxdect ctdccrctctcsd cwct cxd cxd cpd cxdect cqdd cwcpd dbd csctd cpcvd cxd csctba byd cqd bvcwd cpd cpd ddb cwct dactd cwctcpcs cpcxd cpcxd cxd cpd cud cxd cvcxcrcpd csctd cxd crd ctcpd ctd cxd cpd ddba dcbycb cjbicl cxd ctd dactd ctd acd ctd ddd ctd cfcwcxd cxd cwcpd ctd cxd csctcrctd cpd cxdectcs cpd crcwcxd ctcrd dbcxd ctctd ctctd ddd ctd cxczctc btcbccb cxd cxd cxd ctd csctcs cpd cvctd ctd cpd acd ctd ddd ctd ctd dacxd cxd cvd cvcpd cxdecpd cxd dbcxd cwcxd btc btd crcwb cxd csctd cxcvd cvd cpd cpd cpd cxd dbcxd ctd ctcrd ctd cud cpd crctb ctd dbd crcwcpd cpcrd ctd cxd cxcrd ctcrd cxd ddb cpd cpcsd cxd cxd cpd cxd cpd dactd cscxabctd ctd cud btcbccb ctd ctd cpd ctcs cxd dbd dactd cpddd ctd dbd czd cjbdbjclb cpcs cwd ctd dbd cxd cjbjb bebeclb cpd cxd cjbfb blb bdbeb bebdb bebib beblcl cpd cfctcq crd ctd ctd cxcrcpd cxd cjbgb bdbkb bdblclba conclusion cfct ctd ctd ctcs cwct csctd cxcvd cpd ctdacpd cpd cxd btcbccb cpd ctd ctd cqcpd ctcs cvd cqcpd ctctd ctctd cpcvct cxd cxd ddb dbcxd cwcpcud crd btcbccb cpcvct cpd cpcvctd ctd cpd crcpcrcwcxd cvba cbd cpcvct csctd cpd acd ctd cxd btcbcc cpd ctcpcrcw cpd cxcvd ctcs cxcud cscxd cxcqd ctcs cxcsctd cxacctd cpd ctd cxcrcpd acd ctd cpd ctcs cpd cwct csctd dbcwd csctc csd cpd ctd cxcrcpd ddcrd ctd cwctacd ctb acd ctc csba ctd cwd cwcpd cwct cpcvct cpcs cqcpd cpd crct dacxcsctcs cqdd cwcxd cpd cxd cxcrcpd cpd cxcvd ctd cxd cxd crcxctd cpcrcwcxctdact cwcxcvcw cvd cqcpd cpcvct cxd cxdecpd cxd cvcxdactd ddd cxcrcpd acd cxdect cscxd cxcqd cxd cpd cxcud cpcvct csct crcpd cpcrcxd cxctd 
cfct ctd ctd cpd cpcvct cpd cpcvctd ctd crcwctd cwcpd cpd dbd cwct btcbcc ddd ctd cpcrcwcxctdact cwcxcvcw cxd cxdecpd cxd dbcwcxd ctcyctcrd cxd cuctdb acd cxd ctd ctd ctd cccwct crcwctd ctd cxctd crcpd crd cscxb cpd cxd cpd cwct csctd cxd ctcpcu ctd cpd cxd ctd cxd dactd cwctcpcsba bwctd cpcxd ctcs ctdcd ctd cxd ctd cpd ctd cwd cwcpd cwct crcwctd cpd btcbcc cpcrcwcxctdact cvd cqcpd cpcvct cxd cxdecpd cxd cxd ctdccrctd blbkb ctd dactd cwct cpd cucpcxd ctcs acd cxd ctd cxd ctd cpcxd cqctd bhb cpd blbhb cpcvct cxd cxdecpd cxd cpd cucpcxd ctcs cxd ctd cxd cpd cwctcpdacxd cqcxcpd ctcs dbcpd csd cpd cvct acd ctd byd cwctd ctb dbct csctd crd cxcqct cpd ctdacpd cpd cwct crcpcrcwcxd cxd btcbccb dbcwcxcrcw cpd dbd cpd ddd csct ctd cpcxd cpd cpcscscxd cxd cpd crd ddd cucpacd ctbacfct cwd cwcpd crcpcrcwcxd cxd ctabctcrd cxdactcxd cpcrcwcxctdacxd cpcs cqcpd cpd crcxd cvb cpd cwcpd cxd ctcsd crctd cuctd crcw cscxd cpd crct cpd ctd dbd crba acknowledgments cfct dbd cxczct cwcpd cxcvd ctd bvcpd cpd dacxd cccwctcxd ctd bwcpd cfcpd cpcrcwb cwct cpd ddd ctdacxctdbctd cpd cwctd cwctd btd csd ctdb ddctd cud cwctcxd ctcud crd ctd cpd cuctctcscqcpcrczba ctb ctd bwd crcwctd cwcpd czd cxcrd cud cactd ctcpd crcwb bvcpd cqd cxcscvctb cdc cpd cwct cpd cpcrcwd ctd cxd ccctcrcwd cvdd cud cwctcxd csd cxd cwcxd dacxd cxd cxd bycpd bebcbcbc cpd cbd cxd bebcbcbdb ctb ctcrd cxdactd ddb cpd bvd cpd cud csd cpd cxd ctd cxd ctd ctcs cxd cwcxd dbd czba cjbdcl cpd ctd cwd bmbbbbdbdbdbbad cpd ctd bacrd bbba cjbecl cccwct bzd ctd crd ctcrcxaccrcpd cxd bebcbcbcba cwd bmbbbbcsd bacrd cxd bebacrd bbbzd ctd cpc crd bcbgbad cscuba cjbfcl cfba btcscycxctb cfcxd bxba cbcrcwdbcpd deb bucpd cpczd cxd cwd cpd cpd cxd ctddba cccwct csctd cxcvd cpd cxd ctd ctd cpd cxd cpd cxd ctd cxd cpd cpd cxd ddd ctd crba cbc cbc blblb cxcpdbcpcw cpd csb cbbvb bwctcrba bdblblblba cjbgcl chba btd cxd btba ctd ctd cpd bwba cbcwcpdbba cbctcpd ctd ctd ctcrd cxd cwct cqctd crd cud ctd ctd dbcxcsct ctd cxcrcpd ctcs dbctcq ctd dactd crba bdbed cbddd cxd bwcxd cxcqd ctcs bvd cxd cvb btd csd bzd ctctcrctb cbctd bdblblbkba cjbhcl caba btd csctd cccwct bxd ctd cxd ctd dacxcrctba crba cabtbzc bvcachc ccb blbib cpcvctd bebgbedfbebhbeba bvcccd cqd cxd cwcxd ctb bdblblbiba cpcvd ctb bvdectcrcw cactd cqd cxcrba cjbicl ccba btd csctd bwcpcwd cxd ctctcuctb bwba cpd ctd bwba cad ctd cxb cpd caba cfcpd cvba cbctd dactd ctd ctd dbd acd ddd ctd crba bdbhd btbvc cbc cbc bvd ctd cpcxd bvc bwctcrba bdblblbhba cjbjcl byba buctd ctd bwba bvd cpd czctb buba bxdacpd btba ctd btba ctd cpd bwba ctcpd czba cxcrd ctd ctd cqctcscsctcs cqcxd ctd dbd czcxd cvba bxbxbx ctd cpd bvd cxcrcpd cxd bgb bhb bmbkdfbdbhb crd cqctd bdblblbjba cjbkcl cfba bud czddb caba bwd crctd bwba bxd ddb cpd cccwctcxd ctd byctcpd cxcqcxd cxd ctd dactd ctd cscxd cxcqd ctcs acd ddd ctd csctd ddctcs cpd ctdccxd cxd ctd csctd czd bvd crba cbc bzc bxcccac bvcbb bebcbcbcb cbcpd bvd cpd cpb bvbtb bebcbcbcba cjblcl bud dbd cpd ctd ctd cpd btba chctcpd cdd cxdactd btd cpd cxcqd ctb cqcpd ctcs cpd ctd dactd cbd cud dbcpd ctdgc cpcrd cxcrct cpd bxdcd ctd cxctd crctb bebcb bgb bmbgbcbfdfbgbebgb btd bdblblbcba cjbdbccl bud ctd cpd babvcpd babycpd bzba cwcxd cxd cpd cbba cbcwctd czctd cfctcq crcpcrcwcxd cpd cicxd cub cxczct cscxd cxcqd cxd bxdacxcsctd crct cpd cxd cxcrcpd cxd crba bxbxbx cud crd blblb ctdb chd czb chb cpd bdblblblba cjbdbdcl bvcpd cpd cbba cpd cxba bvd cpdbcpd cfcfcf dcdd crcpcrcwcxd cpd cvd cxd cwd crba cdcbbxc cbddd cxd ctd ctd ccctcrcwd cvcxctd cpd cbddd ctd cdcbc cccbb ctd ctddb bvbtb bwctcrba bdblblbjba cjbdbecl bwba caba bvcwctd cxd cpd ccba cpd bwctcrctd cpd cxdecxd cvd cqcpd cpd cxd ctd dacxcrct cud cxd dactcs ctd cud cpd crct cpd cucpd ctd cpd crctba btbvc ccd cpd bvd cbddd bab bjb beb bmbdbgbjdfbdbkbfb cpdd bdblbkblba cjbdbfcl bvd cpd czctb cbcpd cscqctd cvb buba cfcxd ctddb cpd ccba cfba cvba byd ctctd ctd cscxd cxcqd ctcs cpd ddd cxd cud cpd cxd cpcvct cpd ctd cxctdacpd ddd ctd cfd czd cwd bwctd cxcvd ctd cxd btd ddd cxd cpd cdd cqd ctd dacpcqcxd cxd ddb cpcvctd bfbdbddfbfbebcb bebcbcbcba bvcbc buctd czctd ctddb bvbtb cdcbbtba cjbdbgcl byba bwcpcqctczb byba cpcpd cwd ctczb bwba cpd cvctd caba cxd cpd cbd cxcrcpba cfcxcsctb cpd ctcp crd ctd cpd cxdact cpcvct dbcxd bvbycbba crba btbvc cbc cbc bcbdb bucpd abb bvcpd cpcscpb crd bebcbcbdba cjbdbhcl caba bwcxd cvd ctcscxd ctb byd ctctcsd cpd cpd bwba cpd cccwct byd ctct cpdactd cyctcrd bwcxd cxcqd ctcs cpd ddd cpcvct ctd dacxcrctba crba cfd czd cwd bwctd cxcvd ctd cxd btd ddd cxd cpd cdd cqd ctd dacpcqcxd cxd ddb buctd czctd ctddb bvbtb bebcbcbcba cjbdbicl babwd crcwctd cpd btba cad dbd btcbccbm cpd cvctb crcpd ctb ctd cxd ctd ctctd ctctd cpcvct cxd cxd ddbac crba cec cbcrcwd bxd cpd bzctd cpd ddb cpdd bebcbcbdba cjbdbjcl cpd cxb bwba bzcxabd csb cwd byba cpcpd cwd ctczb cpd cfba ccd ctba dactd crcpd cactd cxcpcqd cxcrcpd cxd dbcxd cpd dactd cpddd ctd dbd czba crba cbbwc bebcbcbcb cbcpd bwcxctcvd bvbtb crd cqctd bebcbcbcba cjbdbkcl cpd cvcpd cwcpd cyd cfba cad cqctd cpd cfba cad ctd cud cpd crct ctdacpd cpd cxd ctcscxd ctcrd cxd crcwctd ctd cxd crd ctd cscxd cxcqd cxd ctd dbd czd crba bgd cfctcq bvcpcrcwcxd cfd czd cwd cbcpd bwcxctcvd bvbtb cpd bdblblblba cjbdblcl cpd cvcpd cwcpd cyd cpd cfba cad ctd cxcrcpd ctcs cpd crcwcxd ctcrd cud cwct csd cpcxd cpd ddd ctd crba bxbxbx cud crd bebcbcbcb ccctd btdacxdab cpctd cpd bebcbcbcba cjbebccl cqcxcpd dbcxcrdeb bwba bucxd csctd chba bvcwctd cbba bvdectd dbcxd czcxb bxcpd bwba bzctctd caba bzd cpcscxb cbba cacwctcpb cfctcpd cwctd cfba cfctcxd ctd bvba cfctd cpd buba cicwcpd crctcpd ctbm btd cpd crcwcxd ctcrd cud cvd cqcpd crcpd ctd cxd ctd ctba 
crba btcbc cbb bebcbcbcb bvcpd cqd cxcscvctb btb dactd cqctd bebcbcbcba cjbebdcl buba cpd bwctd cxcvd cxd cvd cqcpd cpd ctd dacxcrctba crba bycxcud cbddd cxd cwct cxd crcxd ctd bwcxd cxcqd ctcs bvd cxd cvb cpcvctd bddfbdbcb cxd cpczcxb bvcpd cpcscpb btd cvba bdblbkbiba cjbebecl cxb cpd cxb bwba cbba bwba bvd bwba caba cpd cvctd cpd caba cxd crcpd cpcqd crcpd cxd ctd dacxcrct cud cvctd cvd cpd cwcxcrcpd cpcs cwd cxd cvba crba btbvc buc bvc bebcbcbcb bud btb btd cvd bebcbcbcba cjbebfcl cbba cpd czba cxcpd cactctcsb cbd crd cscxd cud cucpd ctd cpd crct cxd cabtc bwb cxczct ddd ctd cbd cud dbcpd ctdg cpcrd cxcrctcpd csbxdcd ctd cxctd crctb bebjb blb bmblblbhdfbdbcbdbeb cbctd bdblblbjba cjbebgcl bvba bzba cpdcd caba cacpcycpd cpd cpd cpd btba cfba cacxcrcwcpba btcrcrctd cxd ctcpd cqdd crd cxctd ctd cxcrcpd ctcs cqcyctcrd cxd cscxd cxcqd ctcs ctd dacxd ctd cccwctd bvd cxd cbddd ctd bfbebmbebgbddfbebkbcb bdblblblba cjbebhcl cbba cacpd cpd cpd ddb babyd cpd crcxd cpd csd ctddb caba cpd cpd cbba cbcwctd czctd crcpd cpcqd crd ctd cpcscsd ctd cpcqd ctd dbd czba crba btbvc cbc bzbvc bcbdb cbcpd bwcxctcvd bvbtb btd cvba bebcbcbdba cjbebicl cactddd csd cabybv bdbfbcblbm ccctcrcwd cxcrcpd dactd dacxctdb cscxd ctcrd ctd dacxcrctd cxd cwct cgbabhbcbc crd cpd bdblblbeba cjbebjcl btba cad dbd cpd bwd crcwctd cpd ddbm cbcrcpd cpcqd ctb cscxd cxcqd ctcs cqcyctcrd crcpd cxd cpd cxd cud cpd cvctb crcpd ctctd ctctd ddd ctd crba byc bbbtbvc cxcscsd ctdbcpd bebcbcbdb ctcxcsctd cqctd cvb bzctd cpd ddb daba bebcbcbdba cjbebkcl cbba cbcpd cxd bzd cpcscxb cpd cbba bwba bzd cxcqcqd ctba ctcpd ctd ctd csdd ctctd ctctd acd cwcpd cxd ddd ctd ccctcrcwd cxcrcpd cactd cdcfb bvcbbxb bcbdb bcbib bcbeb cdd cxdactd cxd ddd cucfcpd cwcxd cvd bebcbcbdba cjbeblcl btba cbcwctd csd btba bwd cscpb caba cfctcxd cpd bwba bzcxabd csba bwcxd crd dactd ctd crct cscxd crd dactd ddd ctd cqcpd ctcs crd ctd cxd cvba crba bfd ctd cpd cxd cpd cfd cfcxcsct cfctcq bvd cuctd ctd crctb bwcpd cpcsd bzctd cpd ddb bdblblbhba cjbfbccl cbd cxcrcpb caba cxd bwba cpd cvctd byba cpcpd cwd ctczb cpd bucpd cpczd cxd cwd cpd bvcwd csbm crcpd cpcqd ctctd ctctd czd ctd dacxcrct cud ctd ctd cpd cxcrcpd cxd crba btbvc cbc bzbvc bcbdb cbcpd bwcxctcvd bvbtb btd cvba bebcbcbdba cjbfbdcl buba chba cicwcpd bwba cqcxcpd dbcxcrdeb cpd btba bwba ctd cwba cccpd ctd ddbm btd cxd cud cpd crd cud cucpd ctd cxd cxctd dbcxcsctb cpd ctcp crcpd cxd cpd cxd cvba ccctcrcwd cxcrcpd cactd cdbvbubbbbbvcbbwb bcbdb bdbdbgbdb cdba bvba buctd czctd ctddb btd cxd bebcbcbdba 
performance debugging distributed systems black boxes marcos aguilera jeffrey mogul janet wiener labs patrick reynolds duke athicha muthitacharoen mit page october project sosp multi-tier system client web server client web serverweb server database server database server application server application server authentication server page october project sosp motivation complex distributed systems built black box components systems performance problems high erratic latency locating problems hard examine modify system components tools infer bottlenecks choose black boxes open page october project sosp contributions work tools highlight black boxes open require passive information packet traces infer time spent traces person invasive tools instrument boxes reduce time cost debug complex systems improve quality delivered systems page october project sosp causal path client web server client web serverweb server database server database server application server application server authentication server page october project sosp goals tools find high-impact causal paths distributed system causal path series nodes received messages message caused receipt previous message causal paths occur times high impact occurs frequently contributes significantly latency modifications semantic knowledge report per-node latencies causal paths page october project sosp overview approach obtain traces messages components ethernet packets middleware messages collect traces non-invasively analyze traces algorithms nesting faster accurate limited rpc-style systems convolution works message-based systems visualize results highlight high-impact paths require information timestamp source destination call return call-id page october project sosp outline problem statement goals overview approach algorithms nesting algorithm convolution algorithm experimental results visualization gui related work conclusions page october project sosp nesting algorithm traces call-return semantics infers causality nesting relationships suppose calls calls returning call nested call time node node node call call return return page october project sosp nesting causal path system nodes time node node node call return call call return node return internal delays node page october project sosp steps nesting algorithm pair call return messages find score nesting relationships nested nested pick parents unambiguous reconstruct call paths run time number messages time node node node call return call call return node return page october project sosp inferring nesting local info aggregate info histograms track latencies histogram node triplet passes trace build histograms assign nesting heuristics paper time node node node parallel calls tricky page october project sosp outline problem statement goals overview approach algorithms nesting algorithm convolution algorithm experimental results visualization gui related work conclusions page october project sosp convolution algorithm time signal messages source node destination node message times messages time page october project sosp convolution algorithm time-shifted similarities compute convolution fast fourier transforms peaks suggest causality time shift peak delay page october project sosp convolution details time complexity evlogv messages output edges number time steps trace choose time step size shorter delays interest coarse poor accuracy fine long running time robust noise trace page october project sosp algorithm comparison nesting individual paths aggregates finds rare paths requires call return style communication fast real-time analysis convolution applicable broader class systems slower work information time steps good results reasonable off-line analysis details paper page october project sosp outline problem statement goals overview approach algorithms experimental results maketrace trace generator maketrace web server simulation pet store ejb traces execution costs visualization gui related work conclusions page october project sosp maketrace synthetic trace generator needed testing validate output input check corner cases set causal path templates call return messages latencies delays seconds gaussian normal distribution recipe combine paths parallelism start stop times path duration trace page october project sosp desired results trace causal paths time spent nodes host component time spent node nodes calls edges time parent waits calling child page october project sosp experimental results trace correct false positives frequent path total paths page october project sosp results petstore sample ejb application middleware java instrumentation stanford pinpoint project delay added mylist jsp page october project sosp results running time nesting convolution time step petstore petstore multi-tier long multi-tier multi-tier short cpu time sec memory duration sec length messages trace details results paper page october project sosp accuracy parallelism increased parallelism degrades accuracy slightly parallelism number paths active time parallelism node false positives page october project sosp results nesting algorithm clock skew effect accuracy skew delays interest drop rate effect accuracy drop rates delay variance robust variance noise trace matters nodes send noise effect accuracy noise page october project sosp visualization gui goal highlight dominant paths paths sorted frequency total time red highlights high-cost nodes timeline nested calls dominant subcalls time plots node time call delay page october project sosp related work systems trace end-to-end causality modified middleware modified jvm layers magpie microsoft research aimed performance debugging pinpoint stanford berkeley aimed locating faults products appassure performasure optibench systems make inferences traces intrusion detection zhang paxson lbl traces statistics find compromised systems page october project sosp future work automate trace gathering conversion sliding-window versions algorithms find phased behavior reduce memory usage nesting algorithm improve speed convolution algorithm validate usefulness complicated systems limits approach page october project sosp conclusions bottlenecks black box systems finding causal paths find bottlenecks algorithms find paths traces work find correct latency distributions algorithms similar results passively collected traces sufficient information information http hpl research project contact multi-hop message traces 
appearing annual symposium operating systems design implementation osdi december tag tiny aggregation service ad-hoc sensor networks samuel madden michael franklin joseph hellerstein wei hong madden franklin jmh berkeley wei hong intel-research net berkeley intel research berkeley abstract present tiny aggregation tag service aggregation low-power distributed wireless environments tag users express simple declarative queries distributed executed efficiently networks low-power wireless sensors discuss generic properties aggregates show properties affect performance network approach include performance study demonstrating advantages approach traditional centralized out-of-network methods discuss variety optimizations improving performance faulttolerance basic solution introduction recent advances computing technology led production class computing device wireless battery powered smart sensor sensors active full fledged computers capable measuring real world phenomena filtering sharing combining measurements small sensor devices motes development berkeley current generation motes roughly equipped radio processor memory small battery pack suite sensors mote operating system tinyos set primitives designed facilitate deployment motes ad-hoc networks networks devices identify route data prior knowledge assumptions network topology allowing network topology change devices move run power experience shifting waves interference due relative ease deployment mote-based sensor networks practitioners variety fields begun range monitoring data collection tasks civil engineers motes monitor building integrity earthquakes work partially supported nsf grants iisand research funds ibm intel microsoft micro program biologists planning mote deployments habitat monitoring administrators large computer clusters interested motes monitor temperature power usage data centers sensor applications depend ability extract data network data consists summaries aggregations raw sensor readings researchers noted importance data aggregation sensor networks previous work tended view aggregation applicationspecific mechanism programmed devices as-needed basis typically error-prone low-level languages contrast position aggregation central emerging sensor network applications provided core service system software set extensible apis service consist generic easily invoked high-level programming abstraction approach enables users sensor networks networking experts computer scientists focus applications free idiosyncrasies underlying embedded hardware tag approach developed tiny aggregation tag generic aggregation service hoc networks tinyos motes essential attributes service simple declarative interface data collection aggregation inspired selection aggregation facilities database query languages intelligently distributes executes aggregation queries sensor network time power-efficient manner sensitive resource constraints lossy communication properties wireless sensor networks tag processes aggregates network computing data flows sensors discarding irrelevant data combining relevant readings compact records tag operates users pose aggregation queries powered storage-rich basestation operators implement query distributed network piggybacking existing hoc networking protocol appearing annual symposium operating systems design implementation osdi december sensors route data back user routing tree rooted basestation data flows tree aggregated aggregation function value-based partitioning query query counts number nodes network indeterminate size request count injected network leaf node tree reports count parent interior nodes sum count children add report parent counts propagate tree manner flow root overview paper contributions paper four-fold propose simple sql-like declarative language expressing aggregation queries streaming sensor data identify key properties aggregation functions affect extent efficiently processed inside network demonstrate network execution yield order magnitude reduction communication compared centralized approaches show adopting well-defined declarative query language level abstraction user specific networking routing protocols number optimizations transparently applied reduce data demands system finally show focus high-level language leads end-to-end techniques reducing effects network loss aggregate results remainder paper structured section briefly review tinyos hardware software environment discuss syntax semantics queries tag classify types aggregates supported system focusing characteristics aggregates impact performance fault tolerance present core tag algorithm show solution satisfies query requirements providing performance tolerance network faults discuss optimizations improving performance basic approach additionally include experimental results demonstrating effectiveness robustness algorithms simulation environment study real-world deployment tinyos motes finally discuss related work conclude motes ad-hoc networks section provide overview mote hardware architecture tinyos system hoc routing algorithm mote-based sensor networks motes current generation tinyos motes equipped mhz atmel microprocessor ram code space mhz rfm radio running eeprom expansion slot accommodates variety sensor boards exposing number analog input lines popular chip-to-chip serial busses current sensor options include light temperature magnetic field acceleration sound power single-channel radio half duplex meaning motes send receive time default tinyos implementation csma-like media access protocol random backoff scheme message delivery unreliable default applications build acknowledgment layer message acknowledgment obtained free section power supplied battery pack coin-cell attached expansion slot effective lifetime device determined power supply turn power consumption sensor node dominated cost transmitting receiving messages terms power consumption transmitting single bit data equivalent instructions energy tradeoff communication computation implies applications benefit processing data inside network simply transmitting sensor readings battery pack mote send million messages computation powers radio transmit equivalent message day months long goal deploy long lived zero-maintenance ad-hoc sensor networks powerconserving algorithms important discuss section design amenable low power modes radio powered long periods time understand data routed ad-hoc aggregation network properties radio communication emphasized radio broadcast medium mote hearing distance hears message irrespective mote intended recipient make symmetric links mote hear mote hear common ad-hoc protocols asymmetric links detected blacklisted technique similar proposed aodv messages current generation tinyos fixed note sensor devices integrated expected ratio communication computation costs important time silicon efficiency increases physical costs pushing radio waves air remain constant appearing annual symposium operating systems design implementation osdi december size default bytes device unique sensor distinguishes messages recipient broadcast meaning recipients allowing motes ignore messages intended non-broadcast messages received motes range unintended recipients simply drop messages addressed ad-hoc routing algorithm overview mote environment discuss sensor devices route data common technique sketch build routing tree omit details approach due space constraints number routing protocols suitable purpose proposed reader referred information general tag agnostic choice routing algorithm requiring provide capabilities deliver query requests nodes network provide routes node root network aggregation data collected routes guarantee copy message arrive duplicates generated tree-based routing scheme mote appointed root point user interfaces network root broadcasts message motes organize routing tree message specifies level distance root case mote assigned level hears message assigns level level message chooses sender message parent route messages root motes rebroadcasts routing message inserting ids levels routing message floods tree fashion node rebroadcasting message nodes assigned level parent routing messages periodically broadcast root process topology discovery continuously constant topology maintenance makes easy adapt network caused mobility 
nodes addition deletion motes describe specific topology maintenance protocol experiments loss section maintain stability network parents retained child hear long period time point note optimization routing layer limit extent queries propagated based properties query short-lived query constrained geographic area motes area reserve optimizations future work selects parent process detail robustness approach respect loss effect aggregate values section mote wishes send message root broadcasts message addressed parent turn forwards message parent eventually reaching root section show data routed root combined data motes efficiently combine routing aggregation turn syntax semantics aggregate queries tag query model environment goal allowing users pose declarative queries sensor networks needed language expressing queries inventing chose adopt sql-style query syntax support sql-style queries joins single table called sensors schema base station case cougar table thought append-only relational table attribute input motes temperature light tag focus problem aggregate sensor readings facilities collecting individual sensor readings exist describing semantics queries general begin query user wishes monitor occupancy conference rooms floor building chooses microphone sensors attached motes rooms average volume threshold assuming rooms multiple sensors query expressed select avg volume room sensors floor group room avg volume threshold epoch duration query partitions motes floor room located hard-coded constant device determined localization component devices query reports rooms average volume threshold updates delivered seconds user deregister query time general queries tag form select attrsa sensors selpredsa group attrsa havingpredsa epoch duration exception epoch durationclause semantics statement similar sql aggregate appearing annual symposium operating systems design implementation osdi december queries select clause specifies arbitrary arithmetic expression aggregation attributes expect common case simply single attribute attrs optionally selects attributes sensor readings partitioned subset ofattrsthat group clause syntax clause discussed note multiple aggreggates computed single query clause filters individual sensor readings aggregated predicates typically executed locally mote readings communicated group clause specifies attribute based partitioning sensor readings logically reading belongs group evaluation query table group identifiers aggregate values clause filters table suppressing groups satisfy havingpreds predicates primary semantic difference tag queries sql queries output tag query stream values single aggregate batched result monitoring applications continuous results single isolated aggregate users understand network behaving time observe transient effects message losses make individual results isolation hard interpret stream semantics record consists group aggregate valuea pair group group time-stamped readings compute aggregate record belong time interval epoch duration epoch argument epoch duration clause specifies amount time seconds devices wait acquiring transmitting successive sample large user desires long time takes mote process transmit single radio message local processing including average mac backoff low-contention environment current generation motes yielding maximum sample rate samples section discuss situations require longer lower bounds epoch duration structure aggregates problem computing aggregate queries large clusters nodes addressed context sharednothing parallel query processing environments sensor networks environments require coordination large number nodes process aggregations severe bandwidth limitations lossy communications variable topology sensor networks specific implementation techniques environments differ leverage techniques aggregate decomposition database systems approach systems tag implement functions merging functiona initializer evaluator general structure multi-valued partial state records computed sensor values representing intermediate state values required compute aggregate partialstate record resulting application function merging function average partial state record consist pair values sumandcount anda state records initializer needed instantiate state record single sensor average sensor initializer returns tuple finally evaluator takes partial state record computes actual aggregate average evaluator simply returns functions easily derived basic sql aggregates general operation expressed commutative applications binary function expressible taxonomy aggregates basic syntax structure aggregates obvious question remains aggregate functions expressed tag original sql specification offers options count min max sum average basic functions suitable wide range database applications constrain tag choices reason present general classification aggregate functions show dimensions classification affect performance tag paper assume aggregation functions registered tag classified dimensions omit detailed discussion aggregate functions registered motes aggregates pre-compiled motes virtual-machine languages recently proposed tinyos-style motes purpose appearing annual symposium operating systems design implementation osdi december max min count sum average median count distinct histogram section duplicate sensitive section exemplary summary section monotonic section partial state distributive distributive algebraic holistic unique content-sensitive section table classes aggregates classify aggregates properties important sensor networks table shows specific aggregation functions classified properties sections paper dimensions classification emphasized dimension duplicate sensitivity duplicate insensitive aggregates unaffected duplicate readings single device duplicate sensitive aggregates change duplicate reading reported duplicate sensitivity implies restrictions network properties optimizations section exemplary aggregates return representative values set values summary aggregates compute property values distinction important exemplary aggregates behave unpredictably face loss reason amenable sampling conversely summary aggregates aggregate applied subset treated robust approximation true aggregate assuming subset chosen randomly correlations subset accounted approximation logic monotonic aggregates property partial state records combined resulting state record property important determining predicates applied network final aggregate early predicate evaluation saves messages reducing distance partial state records flow aggregation tree fourth dimension relates amount state required partial state record partial average record consists pair values partial countrecord constitutes single tag correctly computes aggregate conforms specification section performance inversely related amount intermediate state required aggregate categories dimension distributive algebraic holistic initially presented work data-cubes distributive aggregates partial state simply aggregate partition data computed size partial state records size final aggregate algebraic aggregates partial state records aggregates partitions constant size holistic aggregates partial state records proportional size set data partition essence holistic aggregates partial aggregation data brought aggregated evaluator unique aggregates similar holistic aggregates amount state propagated proportional number distinct values partition content-sensitive aggregates partial state records proportional size statistical property data values partition approximate aggregates proposed recently database literature content-sensitive examples aggregates include fixed-width histograms wavelets overview functions summary classified aggregates state requirements tolerance loss duplicate sensitivity monotonicity refer back classification text properties determine applicability communication optimizations present understanding aggregates fit categories cross-cutting issue critical aspects sensor data collection note formulation aggregate functions combined taxonomy flexible encompass wide range sophisticated operations implemented simulator section isobar finding aggregate duplicateinsensitive summary monotonic content-sensitive aggregate builds topological map representing discrete bands attribute light plotted attributes position local coordinate space histogram aggregate sorts sensor readings fixed-width buckets returns size bucket content-sensitive number buckets varies depending widely spaced sensor readings 
count distinct returns number distinct values reported motes appearing annual symposium operating systems design implementation osdi december attribute catalog queries tag named attributes mechanism needed users determine set attributes query motes advertise attributes provide tag include mote small catalog attributes catalog searched attributes specific iterated limit burden reporting catalog information motes assume central query processor caches stores attributes motes access tag sensor receives query converts named fields local catalog identifiers nodes lacking attributes query simply tag missing attributes null result records alternatively query lacking node opt query technique increases scalability large sensor network deployments require nodes global knowledge attributes relational databases partial state records resulting evaluation query layout nodes tuples tag selfdescribing attribute names carried results leading significant reduction amount data propagated tuple time nodes identical catalogs heterogeneous sensing capabilities incremental deployment motes attributes tag direct representations sensor values light temperature introspective remaining energy network neighborhood information generally represent time-varying statistics local sensor values exponentially decaying average light readings complicated attributes room number gps coordinate relative distance neighbor localization component individual software components tinyos choose attributes make provide accessor function acquiring attribute reading network aggregates simple routing protocol section query model discuss implementation core tag algorithm network aggregation naive implementation sensor network aggregation centralized server-based approach sensor readings base station computes aggregates tag compute aggregates network properly implemented approach lower number message transmissions latency power consumption server-based approach measure advantage network aggregation section present basic algorithm detail operation basic approach absence grouping show extend grouping section tiny aggregation tag consists phases distribution phase aggregate queries pushed network collection phase aggregate values continually routed children parents recall query semantics partition time epochs duration produce single aggregate grouping combines readings devices network epoch goal messages collection phase ensure parents routing tree wait heard children propagating aggregate current epoch accomplish parents subdivide epoch children required deliver partial state records parent-specified time interval interval selected time parent combine partial state records propagate record parent mote receives request aggregate mote user awakens synchronizes clock timing information message prepares participate aggregation tree based routing scheme chooses sender message parent addition information query includes interval sender expecting hear partial state records forwards query requesta network setting delivery interval children slightly time parent expects seea partial state record tree-based approach forwarding consists broadcast include nodes hear previous round include children nodes continue forward request manner query propagated network epoch query propagation mote listens messages children interval forwarding query computes partial state record consisting combination child values heard local sensor readings finally transmission interval requested parent mote transmits partial state record network figure illustrates process notice parents listen longer transmission interval appearing annual symposium operating systems design implementation osdi december level level level level level time tree depth sensing processing radio idle delivery interval transmitting listening receiving radio processor idle end epoch start epoch root figure partial state records flowing tree epoch overcome limitations quality clock synchronization algorithms parents children aggregates flow back tree interval-byinterval eventually complete aggregate arrives root subsequent epoch aggregate produced notice significant portion epoch motes idle enter low power state scheme begs question parents choose duration interval receive values long node children report long epoch ends nodes deep tree schedule communication longer intervals require radios powered time consumes precious energy general proper choice duration intervals environment specific depends density radio cells bushiness network topology purposes simulations experiments paper assume network maximum depth set duration interval epoch duration nodes levela transmitting interval rely tinyos mac layer avoid collisions nodes transmitting interval note lower-bound epoch durationand constrains maximum sample rate network epoch long partial state records bottom tree propagate root increase sample rate pipelining communications schedule shown figure pipelining output network delayed epochs nodes wait epoch report aggregates collected current epoch exchange delays effective sample rate system increased reason pipelining long processor stage increases clock rate cpu schemes detail discussed fully-pipelined approach aggregation workshop submission section show tag provide order magnitude decrease communications costs centralized approach discussing performance extend approach support grouping grouping grouping tag functionally equivalent group clause sql sensor reading group groups partitioned expression attributes basic grouping technique push expression query nodes choose group belong answers flow back update aggregate values groups partial state records aggregated approach records tagged group node leaf applies grouping expression compute group tags partial state record group forwards parent node receives aggregate child checks group child group node combines values combining functiona group stores child group forwarding epoch child message arrives group node updates aggregate epoch node sends groups collected information previous epoch combining information multiple groups single message long message size permits figure shows computing query grouped temperature selects average light readings recall queries clause constrains set groups final query result predicate passed network group avg group avg group avg temp light temp light temp light temp light temp light temp light aggregate avg light groups temp temp temp group avg select avg light temp sensors group temp figure sensor network left network grouped aggregate applied parenthesized numbers represent nodes contribute average appearing annual symposium operating systems design implementation osdi december grouping expression predicate potentially reduce number messages predicate form max attr information groups max attr transmitted tree predicate network node detects group satisfy clause notify nodes network information suppress transmission storage values group note clauses pushed monotonic aggregates nonmonotonic aggregates amenable technique allhavingpredicates monotonic aggregates pushed max attr applied network node local max entire group grouping introduces additional problem number groups exceed storage nonleaf device proposed solution evict groups local storage eviction victim selected forwarded node parent choose hold group continue forward tree notice single node evict groups single epoch group multiple times bad victim selected group storage full group evicted time eviction decision made time representing unknown previously evicted group arrives groups evicted base station top network called combine partial groups form accurate aggregate evicting partially computed groups partial preaggregation shown partition sensor readings number groups properly compute aggregates groups amount group information exceeds storage device briefly mention experiments grouping group eviction policies section summarize additional benefits tag additional advantages tag principal advantage tag ability dramatically decrease 
communication required compute aggregate versus centralized aggregation approach tag number additional benefits ability tolerate disconnections loss sensor environments aggregation requests partial state records garbled devices move run power losses invariably result nodes lost parent incorporated aggregation network initial flooding phase include information queries partial state records lost nodes reconnect listening node state records necessarily intended flow tree revisit issue loss section advantage tag approach cases mote required transmit single message epoch depth routing tree centralized tag case data converges root nodes top tree required transmit significantly data nodes leaves batteries drained faster lifetime network limited top routing tree forward messages node network maximum sample rate system inversely proportional total number nodes radio channel capacity messages motes participating centralized aggregate obtain sample rate samples messages flow root epoch larger sample ratea messages mote epoch network density tag maximum transmission rate limited occupancy largest radio-cell general expect cell fewer thana motes advantage tag explicitly dividing time epochs convenient mechanism idling processor obtained long idle times figure show intervals radio processor put deep sleep modes power bootstrapping phase needed motes learn queries system acquire parent synchronize clocks simple strategy involves requiring node wake infrequently periodically advertise information devices received advertisements neighbors listen times period sleep intervals research energy aware mac protocols presents similar scheme detail work discusses issues time synchronization resolution maximum sleep duration avoid adverse effects clock skew individual devices tag features provide users stream aggregate values sensor readings underlying network change readings provided energy bandwidth efficient manner appearing annual symposium operating systems design implementation osdi december simulation-based evaluation section present simulation environment tag evaluate behavior simulator initial real-world deployment discuss performance end paper section study algorithms presented paper simulated tag java simulator models mote behavior coarse level time divided units epochs messages encapsulated java objects passed directly nodes model time send decode nodes allowed compute transmit arbitrarily single epoch node executes serially messages nodes epoch delivered random order epoch model parallel execution note simulator account low-level properties network fine-grained model time model radio contention byte level simulation includes interchangeable communication model defines connectivity based geographic distance figure shows screenshots visualization component simulation square represents single device shading images represents number radio hops device root center darker closer measure size networks terms diameter width sensor grid nodes diameter network devices run experiments communications models simple model nodes perfect lossless communication neighbors regularly figure random placement model figure realistic model attempts capture actual behavior radio link layer tinyos motes figure model notice number hops node root longer directly proportional geographic distance node root values related model results real world experiments approximate actual loss characteristics tinyos radio loss rates high realistic model pair adjacent nodes loses traffic devices separated larger distances lose traffic simulator models costs topology maintenance node transmit reading epochs case optimizations node periodically send heartbeat advertise alive parents children routing data interval heartbeats chosen arbitrarily choosing longer interval means fewer messages simple random realistic figure tag simulator communications models diameter requires nodes wait longer deciding parent child disconnected making network adaptable rapid change simulation measure number bytes messages partial state records radio mote simulate mote cpu give accurate measurement number instructions executed mote obtain approximate measure state required algorithms based size data structures allocated mote experiments simple radio topology loss assume sensor values change single simulation run performance tag set experiments compare performance tag network approach centralized approaches queries classes aggregates discussed section centralized aggregates communications cost irrespective aggregate function data routed root experiment compared cost number bytes required distributive aggregates max count algebraic aggregate average holistic aggregate median content-sensitive aggregate histogram unique aggregate count distinct results shown figure values experiment represent steady-state cost extract additional aggregate network query propagated cost flood request tree considered node network max count cost processed network bytes epoch total nodes send single integer partial state record similarly average requires integers double cost distributive aggregates median costs centralized aggregate bytes epoch significantly expensive appearing annual symposium operating systems design implementation osdi december count min histogram average count distinct median centralized tag aggregation function bytes transmitted epoch sensors tag network centralized aggregate in-network centralized aggregation network diameter loss figure network centralized aggregates aggregates larger networks parents forward children values root count distinct slightly expensive bytes duplicate sensor values uniform sensor-value distribution reduce cost aggregate histogram aggregate set size fixed-width buckets sensor values ranged interval messages epoch histogram efficient means extracting density summary readings network note benefit tag pronounced depending topology flat single-hop environment motes directly connected root tag centralized approach topology motes arranged line centralized aggregates require partial state records transmitted tag require records shown simulation topology network aggregation reduce communication costs order magnitude centralized approaches worst case median performance equal centralized approach grouping experiments ran experiments measure performance grouping tag focusing behavior eviction techniques number simple eviction policies found choice policy made difference sensor-value distributions tested extreme case difference worst case eviction policy accounted total messages due relative insignificance results space limitations omit detailed discussion merits eviction policies optimizations section present techniques improve performance accuracy basic approach techniques function dependent classes aggregates note general techniques applied user-transparent fashion explicitly part query syntax affect semantics results taking advantage shared channel discussion aggregation algorithms point largely fact motes communicate shared radio channel fact message effectively broadcast nodes range enables number optimizations significantly reduce number messages transmitted increase accuracy aggregates face transmission failures section shared channel increase message efficiency node misses initial request begin aggregation initiate aggregation missing start request snooping network traffic nearby nodes hears device reporting aggregate assume aggregating allowing nodes examine messages directly addressed motes automatically integrated aggregation note snooping require nodes listen time listening predefined intervals short mote time-synchronized neighbors duty cycles low snooping reduce number messages classes aggregates computing max group motes node hears peer reporting maximum greater local maximum elect send affecting final aggregate hypothesis testing snooping showed hear node node affect end aggregate aggregates fact exploited significantly reduce number nodes report technique generalized approach call hypothesis testing classes aggregates node presented guess proper aggregate decide appearing annual symposium operating systems design implementation 
osdi december locally contributing reading readings children affect aggregate max min monotonic exemplary aggregates technique directly applicable number ways applied snooping approach nodes suppress local aggregates hear aggregates invalidate alternatively root network subtree network seeking exemplary sensor min compute minimum sensor highest levels subtree abort aggregate issue request values tree approach leaf nodes send message greater minimum observed topa levels intermediate nodes forward partial state records suppressed transmit assuming moment sensor values independent uniformly distributed leaf node transmit probability branching factor number nodes top levels low small values bushy routing trees technique offers significant reduction message transmissions completely balanced routing tree cut number messages required performance benefit substantial non-uniform sensor distributions instance distribution sensor readings clustered minimum messages saved hypothesis testing similarly balanced topologies line nodes benefit approach summary aggregates average variance hypothesis testing guess root applied message savings dramatic monotonic aggregates note snooping approach applies monotonic exemplary aggregates values suppressed locally information central coordinator obtain benefit summary aggregates hypothesis testing user define fixed-size error bound tolerate aggregate error network hypothesis case average device sensor error bound hypothesis answer parent assume approximate answer count apply technique average parents children shown total computed average messages epoch sensor network diameter steady state messages epoch max query hypothesis testing hypothesis hypothesis hypothesis hypothesis snooping figure benefit hypothesis testing max actual average error bound leaf nodes values close average required report scheme depends distribution sensor values uniform distribution fraction leaves report approximates size error bound divided size sensor distribution interval values distributed larger fraction leaves report conducted simple experiment measure benefit hypothesis testing snooping max aggregate results shown figure experiment sensor values uniformly distributed range hypothesis made root notice performance savings two-fold hypothesis compared hypothesis testing approach snooping approach effective non-uniform distribution surprisingly snooping beat approaches offering three-fold performance increase no-hypothesis case densely packed simple node distribution devices neighbors snoop suggesting devices transmit topology maintenance forwarding child values parents savings snooping reduced factor improving tolerance loss point experiments reliable environment messages dropped nodes disconnected offline section address problem loss effect algorithms presented unlike traditional database systems communication loss fact life sensor domain techniques section seek mitigate loss appearing annual symposium operating systems design implementation osdi december topology maintenance recovery tag designed sit top shifting network topology adapts appearance disappearance nodes study mechanisms adapting topology central paper completeness describe basic topology maintenance recovery algorithm simulation implementation approach similar techniques practice existing tinyos sensor networks derived general techniques proposed ad-hoc networking literature networking faults monitored adapted levels node maintains small fixed sized list neighbors monitors quality link neighbors tracking proportion packets received neighbor locally unique sequence number message sender node observes link quality parenta significantly worse node chooses parent close closer root parent conditions prevent routing cycles node observes heard parent fixed period time relative epoch duration query running assumes parent failed moved resets local level picks parent neighbor table quality metric linkquality note parent select node routing subtree underneath parent child nodes reselect parent failure occurred observe parent level note switching parents introduce possibility multiple records arriving single node node transmits epoch switches parents epoch parent switching temporary disconnections additional lost records topology due children selecting parent parent level effects single loss study effect single device offline aggregate important measurement intuition magnitude error single loss generate note hierarchical aggregation single mote offline entire subtree rooted node temporarily disconnected experiment simple topology sensor readings chosen uniform distribution afpercent error network diameter maximum error aggregation function average count minimum median maximum error network diameter average error aggregation function average count minimum median average error figure effect single loss aggregate functions computed total runs point error bars standard error confidence intervals ter running simulation epochs selected uniformly random node disable environment children disabled node temporarily disconnected eventually values reintegrated aggregate rediscovered parents note amount time lost nodes reintegrate directly proportional depth lost node measure experimentally measured maximum temporary deviation true aggregate loss caused perceived aggregate root epoch maximum computed performing runs data point selecting largest error reported run report average maximum error runs figure shows results experiment note maximum loss figure highly variable aggregates considerably sensitive loss count instance large error worst case node connects root large portion network lost temporary error high variability maximum error connected subtree selected victim assuming uniformity placement devices arranged line network size increases chances selecting node larger proportion nodes leaves tree average case figure error count high losses result large number disconnections note min insensitive loss uniform distribution nodes true minimum error median average count min sensitive variations number nodes dramatically count effect realistic communication experiment examine tag performs realistic simulation environment discussed appearing annual symposium operating systems design implementation osdi december section environment technique counteract loss large number partial state records invariably dropped reach root tree ran experiment measure effect loss realistic environment simulation ran aggregate arrived root average number motes involved aggregate epochs measured cache line figure shows performance approach diameter partial state records reflected aggregate root diameter percentage fallen performance falls number hops average node root increases probability loss compounded additional hop basic tag approach presented running current prototype hardware high loss rates highly tolerant loss large networks note centralized approach suffer loss problems child cache improve quality aggregates propose simple caching scheme parents remember partial state records children reported number rounds previous values values unavailable due lost child messages long duration memory shorter interval children select parents technique increase number nodes included aggregate over-counting nodes caching temporally smear aggregate values computed reducing temporal resolution individual readings possibly making caching undesirable workloads note caching simple form interpolation interpolated previous sophisticated interpolation schemes curve fitting statistical observations based past behavior conducted experiments show improvement caching offers basic approach allocate fixed size buffer node measure average number devices involved aggregation section results shown top lines figure notice epochs cached state offer significant increase number nodes counted aggregate rounds increases number nodes involved diameter network versus cache temporal smearing additional drawbacks caching memory group storage sets minimum bound time devices wait depercentage network involved network diameter percentage network involved child caching cache 
epochs cache epochs cache epochs cache figure percentage network participating aggregate varying amounts child cache termining parent offline benefit terms accuracy disadvantages substantial benefit technique suggests allocating ram application level caching beneficial allocating lower-level schemes reliable message delivery schemes advantage semantics data transmitted redundancy situations ram latency costs child cache desirable worthwhile alternative approaches improving loss tolerance section show network topology leveraged increase quality aggregates mote choices parent sending aggregate parent send parents node easily discover multiple parents building list nodes heard step closer root duplicate-sensitive aggregates section sending results multiple parents undesirable effect causing node counted multiple times solution send part aggregate parent rest count mote children parents send count parents count single parent generally aggregate linearly decomposed fashion broadcast single message received processed parents scheme incurs message overheads long parents level request data delivery sub-interval epoch simple statistical analysis reveals advantage assume message transmitted probabilitya losses independent messagea node lost transition parent lost transit independent failures valid assumption appearing annual symposium operating systems design implementation osdi december case sends single parent expected transmitted count probability probabilitya variance standard bernoulli trials probability successa multiplied constant case sends parents linearity expectation tells expected sum expected parent similarly sum variances parent var variance multiple parent count single parent expected assuming independence message parents lost single loss dramatically affect computed ran experiment measure benefit approach realistic topology forcountwith network diameter measured number devices involved aggregation epoch period sending multiple parents count sending parent count surprisingly sending multiple parents substantially increases aggregate due fact losses independent assumed technique applies equally distributive algebraic aggregate holistic aggregates median technique applied partial state records easily decomposed prototype implementation based encouraging simulation results presented built implementation tag tinyos mica motes implementation include optimizations discussed paper core tag aggregation algorithm catalog support querying arbitrary attributes simple predicates section briefly summarize results experiments prototype demonstrate simulation numbers consistent actual behavior show substantial message reductions centralized approach real implementation experiments involved sixteen motes arranged depth tree computing count aggregate -second epochs minute run child caching snooping techniques figure shows occur local interference loss hidden node garble communication parent process radio message arrives count epoch count epoch nodes epoch duration seconds tag centralized figure comparison centralized tag based aggregation approaches lossy prototype environment computing count node network count observed root centralized approach messages forwarded root versus network tag approach notice quality aggregate substantially tag due reduced radio contention measure extent contention compare message costs schemes instrumented motes report number messages received centralized approach required messages tag required representing communications reduction order-of-magnitude shown figure count prototype network topology higher average fanout simulated environment messages centralized case retransmitted fewer times reach root hop loss rates network approach centralized approach increased network contention drove loss rates poor performance centralized case due multiplicative accumulation loss messages nodes bottom routing tree arrived root completes discussion algorithms tag turn extensive related work networking database communities related work database community proposed number distributed push-down based approaches aggregates database systems universally assume well-connected low-loss topology unavailable sensor networks systems present techniques loss tolerance power sensitivity notion aggregates tied taxonomy techniques transparently applying aggregation routing optimizations lacking partial preaggregation techniques enable group eviction proposed technique deal large numbers groups improve efficiency hash joins bucket-based database operators appearing annual symposium operating systems design implementation osdi december components partial-state dimension taxonomy presented section algebraic distributive holistic originally developed part research data-cubes duplicate sensitivity exemplary summary monotonicity dimensions unique content-sensitive state components partial-state addition discusses online aggregation context nestedqueries proposes optimizations reduce tuple-flow outer queries bear similarities technique pushing clauses network respect query language epoch based approach related languages models temporal database literature survey relevant work cougar project cornell discusses queries sensor networks work fjords considers moving selections aggregates network presents specific algorithms sensor networks literature active networks identified idea network simultaneously route transform data simply serving end-to-end data conduit recent sigcomm paper esp constrained framework network aggregation-like operations traditional network sensor network community work networks perform data analysis largely due usc isi ucla communities work directed diffusion discusses techniques moving specific pieces information place network proposes aggregation-like operations nodes perform data flows work low-levelnaming proposes scheme imposing names related groups devices network scheme partitions sensor networks groups work greedy aggregation discusses networking protocols routing data improve extent data combined flows sensor network low level techniques building routing trees computing tag style aggregates papers recognize aggregation dramatically reduces amount data routed network present application-specific solutions unlike declarative query approach approach tag offer simple interface flexible naming system generic aggregation operators aggregation viewed application-specific operation diffusion coded low-level language tag aggregates application-specific users provide functional guarantees composability aggregates classification semantics quantity partial state monotonicity enable transparent application optimizations create possibility library common aggregates tag users freely apply queries directed diffusion puts aggregation apis routing layer expressing aggregates requires thinking data collected data collected similar old-fashioned query processing code thought navigating records database contrast goal separate expression aggregation logic details routing users focus application issues enables system dynamically adjust routing decisions general taxonomic information aggregation function networking protocols routing data wireless networks popular literature address higher level issues data processing techniques data routing treebased routing approach inferior approaches peer peer routing works aggregation scenarios focusing work acks suppression thereof scalable reliable multicast trees bears similarity problem propagating aggregate routing tree tag systems fixed limited types aggregates acks naks regions recovery groups finally presented early version work workshop conclusions summary shown declarative aggregate queries distributed efficiently executed sensor networks network approach provide order magnitude reduction bandwidth consumption approaches data aggregated processed centrally declarative query interface end-users advantage benefit wide range aggregate operations modify low-level code confront difficulties topology construction data routing loss tolerance distributed computing interface tightly integrated network enabling transparent optimizations decrease message costs improve tolerance failure loss plan extend work data collection wireless sensor community evolve moving event-driven model queries initiated results collected response external events interior network results internal sub-queries aggregated nodes shipped points network edge sensor networks widely deployed remote difficult administer locations appearing annual symposium operating systems design implementation osdi december 
bandwidthand power-sensitive methods extract data networks increasingly important scenarios users scientists lack fluency embedded software development interested sensor networks research users high-level programming interfaces necessity interfaces balance simplicity expressiveness efficiency order meet data collection battery lifetime requirements balance tag promising service data collection simplicity declarative queries combined ability tag efficiently optimize execute makes good choice wide range sensor network data processing situations acknowledgments shepherd deborah estrin anonymous reviewers thoughtful reviews advice robert szewczyk david culler alec woo ramesh govindan contributed design networking protocols discussed paper ake larson suggested partial preaggregation group eviction kyle stanek implemented isobar aggregates simulator adjue-winoto schwartz balakrishnan lilley design implementation intentional naming system acm sosp december bancilhon briggs khoshafian valduriez fad powerful simple database language vldb barbar dumouchel faloutsos haas hellerstein ioannidis jagadish johnson poosala ross sevcik jersey data reduction report data engineering bulletin calvert griffioen wen lightweight network support scalable end-to-end services acm sigcomm cerpa elson estrin girod hamilton zhao habitat monitoring application driver wireless communications technology acm sigcomm workshop data communications latin america caribbean floyd jacobson liu mccanne zhang reliable multicase framework light-weight sessions application level framing ieee transactions networking ganesan network dynamics rene motes powerpoint presentation january goff abu-ghazaleh phatak kahvecioglu preemptive routing hoc networks acm mobicom july gray bosworth layman pirahesh data cube relational aggregation operator generalizing group-by cross-tab sub-total icde february heidemann silva intanagonwiwat govindan estrin ganesan building efficient wireless sensor networks low-level naming sosp october hellerstein hass wang online aggregation proceedings acm sigmod pages tucson intanagonwiwat estrin govindan heidemann impact network density data aggregation wireless sensor networks submitted icdcsnovember intanagonwiwat govindan estrin directed diffusion scalable robust communication paradigm sensor networks mobicom boston august kulik rabiner balakrishnan adaptive protocols information dissemination wireless sensor networks mobicom larson data reduction partial preaggregation icde levis culler mat tiny virtual machine sensor networks submitted lin paul rmtp reliable multicast transport protocol infocom pages madden franklin fjording stream architechture queries streaming sensor data icde madden hong hellerstein franklin tinydb web page http telegraph berkeley tinydb madden szewczyk franklin culler supporting aggregate queries ad-hoc wireless sensor networks workshop mobile computing systems applications mainwaring polastre szewczyk culler wireless sensor networks habitat monitoring acm workshop sensor networks applications park corson highly adaptive distributed routing algorithm mobile wireless networks infocom bonnet gehrke seshadri sensor database systems conference mobile data management january perkins royer ad-hoc on-demand distance vector routing workshop mobile computing systems applications pottie kaiser wireless integrated network sensors communications acm shatdal naughton adaptive parallel aggregation algorithms acm sigmod snodgrass editor tsql temporal query language kluwer academic publisher subramanian katz architecture building self-configurable systems mobihoc boston august tan goh ooi online feedback nested aggregate queries multi-threading vldb tennenhouse smith sincoskie wetherall minden survery active network research ieee communications berkeley smart buildings admit faults web page november lab notes research college engineering berkeley http coe berkeley labnotes smartbuildings html woo culler transmission control scheme media access sensor networks acm mobicom july yan larson eager aggregation lazy aggregation vldb heidemann estrin energy-efficient mac protocol wireless sensor networks ieee infocom chen postgres user manual berkeley 
osdi symposium operating systems design implementationusenix association magpie request extraction workload modelling paul barham austin donnelly rebecca isaacs richard mortier pbar austind risaacs mort microsoft microsoft research cambridge abstract tools understand complex system behaviour essential performance analysis debugging tasks open research problems development magpie toolchain automatically extracting system workload realistic operating conditions low-overhead instrumentation monitor system record fine-grained events generated kernel middleware application components magpie request extraction tool application-specific event schema correlate events precisely capture control flow resource consumption request removing scheduling artefacts whilst preserving causal dependencies obtain canonical request descriptions construct concise workload models suitable performance prediction change detection paper describe evaluate capability magpie accurately extract requests construct representative models system behaviour introduction tools understand complex system behaviour essential performance analysis debugging tasks exist open research problems development magpie ability capture control path resource demands application requests serviced components machines distributed system extracting per-request behaviour ways firstly detailed picture request serviced throwing light questions modules touched time spent request disk access data served cache network traffic request generate per-request data analyzed construct concise workload models suitable capacity planning htbellfvtbelffetxff crenqsosisodleackenqdc htbellfvtbelffetxff beleotenqetxdlesodc synetbffetxbelcan emenqeotffbel esc esc esc esc esc esc esc esc esc esc esc esc esc esc esc crenqsosisodleackenqdc dlefsbel crdc vtffetxbeleot sohgsbelsoetx rseotenqackbelff htbellfvtbelffetx crdc vtffetxbeleotff synsiusetx enqeotbelcrsicandc sisobelsoetxff sieot sienqbs sohgsbelsoetx synack belcanenq figure magpie toolchain requests move system event traces generated machine processed extract control flow resource usage individual request scheduling variations removed finally canonical requests clustered construct models workload performance debugging anomaly detection modelsrequire ability measurea request sresource demands discarding scheduling artefacts due multitasking timesharing effect obtain picture request serviced apply information modelling workload addition data serviced detailed analysis individual request behaviour contributions work summarized unobtrusive application-agnostic method osdi symposium operating systems design implementation usenix association extracting resource consumption control path individual requests unlike approaches request tracking magpie require unique request identifier propagated system accurately attributes actual usage cpu disk network request achieved correlating events generated requests live schema event relationships carrying temporal join event stream mechanism constructing concise model workload request expressed canonical form abstracting scheduling artefacts present original event trace representative set request types identified clustering canonical request forms set requests relative frequencies compact model workload performanceanalysis purposes validation accuracy extracted workload models synthetic data evaluation performance realistic workloads magpie request tracking technique unique event logs collected realistic operating environment handles interleaving request types impervious unrelated activity taking place time attribute resource usage individual requests executing concurrently request-oriented approach understanding characterizing system behaviour complements existing methods performance modelling analysis faults performance problems revealed simply inspecting magpie trace individual request comparing expected behaviour contrast traditional approach monitoring system health log aggregateperformance counters raise alarms thresholds exceeded effective identifying throughput problems catch poor response time incorrect behaviour item added shopping cart straightforward programming errors hardware failures root mostproblems obscured interactions multiple machines heterogeneous software components performance modelling key importance commercial enterprises data centers current methods constructing workload models atool magpie workload models capacity planning performance prediction tasks derived carefully controlled measurement environment system input contrived stress request type isolation requires manual configuration expert knowledge system behaviour compromises accuracy variables caching behaviour workload models automatically derived magpie quicker easier produce accurately capture resource demands constituent requests magpie preliminary step systems robust performance-aware self-configuring autonomic computing articulation grand vision previously discussed applications utility magpie workload models scenarios ranging capacity planning on-line latency tuning emphasis paper bottom-up evaluation practical situations demonstrate magpie accurately extracts individual requests realistic operating conditions aggregation data leads representative workload models sections describe design implementation magpie prototype toolchain section evaluate magpie approach simple synthetic workloads straightforward assess results obtained progressing complex workloads section design implementation workload system comprised categories request paths system exercising set components consuming differing amounts system resources request system-wide activity takes place response external stimulus application traced stimulus http request trigger opening file locally execution multiple database queries remote machines accounted http request application scenarios database queries considered requests magpie functional progress request resource consumption stage recorded request path system components involve parallel branches usage cpu disk accesses network bandwidth magpie prototype consists set tools event logs eventually produce workload models figure illustrates process intention designing tools deploy online versionofmagpiethat monitorsrequestbehaviour osdi symposium operating systems design implementationusenix association live system constantly updating model current workload magpie operates offline online goal dictated design choices places earlier versions magpie generated unique identifier request arrived system propagated component technique employed request tracking technologies pinpoint events logged component annotated identifier developed invasive request extraction techniques describe detail eschewing requirement global identifiers avoids problems guaranteeing unique identifier allocation avoids complicated adhocstatemanagementorapi modificationtomanagethe identifiers propagated finally ensures instrumentation independent definition request uncommon applications share component desirable set instrumentation support tracing applications instrumentation instrumentation framework support accurate accounting resource usage instrumentation points enable multiple requests sharing single resource distinguished threads sharing cpu rpcs sharing socket consequence requirement high precision timestamps events generated components user-space kernel mode attribution events requests relies properly ordered windows based operating systems large proportion kernel device driver activity occurs inside deferred procedure calls dpcs aformof softwareinterrupt withahigherpriority thanall normal threads important eventoccurred inside dpc standard interrupt occurred context switch order required precision processor cycle counter strictly monotonic event timestamp event tracing windows etw low overhead event logging infrastructure built recent versions windows operating system technology underpinning magpie instrumentation make extensive pre-existing etw event providers added custom event tracing components instrumented components e-commerce site prototyping depicted figure main parts instrumentation kernel etw tracing supports accounting thread bsetxetxbelhtlfvtlf ffeotenqstxeotcr sosistxdledc ackdc naksynsoh enqsosistxgsdc rslf dleetxsubetxsiack stxetxeotstxetx sohsohdle stx escsiack nakdlesynht etbem ffeotenqstxeotcr sosistxdledc ackdc naksynsoh enqsosistxgsdc rslf dle dleeotenqcaneotenq synsubackdc eotetx acksubbeletx enqeot synsubackdc eotetx acksubbeletx enqeot subetxsub enq bel bel eot enq dleetxdc enqeotgs belenqdc ackeotgs enqeotlf etb etxeotstxgseotgs dlesynlf nak etbem embsenqeotsubgs beldc embsenqeotsubgs beldc embsenqeotsubgs beldc figure instrumentation points web server database server test e-commerce site components http sys kernel module iis process generate events request arrival parsing additional instrumentation inserted magpie shown gray generates events events logged event tracing windows subsystem cpu consumption disk requests winpcap packet capture library modified post 
etw events captures transmitted received packets machine application middleware instrumentation covers points resources multiplexed demultiplexed flow control transfer components prototype platform middleware components winsock specific application-level components asp net isapi filter generate active content instrumented order track request end end etw event consists timestamp event identifier values typed attributes typical system multiple event providers event identifiers hierarchical form provider eventname typical event log form time provider eventname attr etw event provider produces ordered stream timestamped events time large number requests present system generating events variety components subsystems serviced result stream events invariably comprise nondeterministic interleaving events active requests stage workload extraction demultiplex event stream accounting resource consumption individual requests events identifier set attributes osdi symposium operating systems design implementation usenix association workload extraction pipeline toolchain consumes events generated system instrumentation section sections present workload extraction pipeline detail request parser identifies events belonging individual requests applying form temporal join event stream rules event schema process preserves causal ordering events allowing canonical form request inferred captures resource demands opposed service request received discussed section canonical form request deterministically serialized leading representation suitable behavioural clustering section describe behavioural clustering builds workload models comparing requests control path resource demands request parser request parser responsible extracting individual requests interleaved event logs determining events pertain specific request parser builds description request captures flow control resource usage stage writtento operate eitheronlineoroffline thepublic etw consumer api parser considers event stream timestamp order speculatively builds sets related events relies event schema describe event relationships application interest case events occurring sequence thread belong request thiswill expressedin schema events related thread thread post identical event sequence number requests idea temporal joins ensures events occur thread working request joined resulting event sets eventually identified describing complete request discarded events related defined outof-band application-specific schema request parser builtin assumptions application system behaviour event schema event type schema specifies attributes connect events event processed parser type looked schema event added set reisapiid threadid threadid cpuid time win unblock server start aspnetreq start aspnetreq end server endwin block figure illustrationof howtheparserjoinsa sequenceofiisevents event shown black vertical line binds attribute-value pairs represented circles horizontal attributevalue lines joining events depicted transparent gray lines valid-intervals displayed vertical line left bind start bind stop join attributes portion request show valid-interval opened opening closing valid-interval threadid event server start attribute tid bind basic attribute bind basic event server end attribute tid bind basic attribute bind stop event aspnetreq start attribute tid bind basic attribute isapiid bind basic event aspnetreq end attribute tid bind basic attribute isapiid bind basic event win unblock attribute tid bind start attribute cpuid bind start event win block attribute cpuid bind stop attribute tid bind stop figure portion iis schema perform event parsing illustrated figure binding types bind start bind stop instruct parser open close valid-interval lated events words event joined events iis web server schema specifies join attributes server start server end events means events occur joined figure graphical representation process schema states threadid join attribute events attributes posted event types transitively joined request shown diagram aspnetreq start isapiid posted threadid server start events joined osdi symposium operating systems design implementationusenix association shared threadid attribute turn isapiid join attribute events isapiid added set related events set events belonging request incrementally built parser processes event stream additionto identifyingwhich attributescauseevents joined schema nature joins description stop server start events posted thread values joined mechanism needed prevent requests merged captured notion temporal joins temporal joins request progresses relationships attribute values broken created worker thread pool re-tasked request http connection reused request threadid worker thread posts server start event behalf request handing request processing offto thread period threadid working exclusively request defines validinterval attribute-value pair threadid terminology borrowed temporal database community denote time range row table present database databases arbitrary sql queries executed database time theoretically implement magpie parser queries temporal database table holds events type finding events relating request relational join number event types involved request valid-interval events joined usual valid-interval closed attribute-value pair events added event set pair iis schema specifies event win block closes valid-interval threadid attribute event likewise win unblock event opens valid-interval valid-interval threadid opened request preventing merging disjoint requests opening closing valid-intervals controlled schema binding types join specifications bind start begins validinterval attribute-value pair bind stop terminates current valid-interval attribute joins event affecting valid-interval connhash socket threadid cpuid time win unblock winsock connect winsock send perfinfo dpcstart winpcap pkttxwin block figure transitive joins enable packet bandwidth cpu consumption network stack correctly attributed thread issues send request swapped time packet transmitted diagonal pair lines crossing horizontal lines passing arbitrary amount time bind basic binding type theoretical implementation temporal database validinterval closed attribute-value pair events deleted relevant table fragment iis schema matching discussed shown figure prototype parser implementation schema written macros event macro takes provider event join attribute listed attribute macro binding type flags resource accounting event types consumption physical resources specifically context switch events give number cpu cycles thread timeslice disk read write events posted number bytes read written likewise packet events packet size event types added request relevant resource consumed behalf request figure shows cpu consumption request threadid running network stack dpc thread nominally swapped cpu winpcap pkttx event associates network bandwidth packet request user mode winsock connect kernel mode winpcap pkttx events joined shared source destination address attributes represented diagram connhash figure shows annotated screenshot visualization tool developed debug parser highlighted sub-graph events http request forthe urlshortspin aspx generates hencethe n-way relational join findall events request span multiple valid-intervals multiple tables osdi symposium operating systems design implementation usenix association incoming http request packet iis worker network stack runs dpc http response packet ack http sys asp net worker figure annotated screenshot parservisualization single request event attribute-value pairs active thedisplayed time period depicted horizontal timeline events shown binding timelines binding type stop start small vertical barrier portions timeline belong request emphasized showing parser effect flood-fill graph formed join attributes events make easy threads running time highlighted pale rectangle small amount dynamic content returned single http response packet spins cpu approximately executing tight loop 
request type request experimental evaluation presented section implementation design parser severely constrained necessity minimal cpu overhead memory footprint asitisintendedto run online onproductionservers additionally process trace events order delivered seek ahead trace log creates complexity online mode kernel logger batches events subsystems efficiency delivers buffer time context switch events cpu delivered separate buffers whilst individual buffers events timestamp order events buffers interleaved processing reorderings performed priority queue add approximately pipeline delay parser reorder queue eventsthat posted end operation disk start time event soh stx etx eotenqackbelbs htlfvtffcrsoacklfsi dleackvtlfffacklf crackcrack enqacknakacksobsvtlf enqack nakacksobsvtlfsi htetblfbsdc etbesc dleackfscracksibssi gsvtbelrsescdc enqack siacksobsvtlfsi emdc ffusstx vtbelbelusetx emdc ffus crussoh etx ffus htvtlfbsusstx emdc ffus figure parser data structures hash table entries represent current valid-intervals live sectors event shown black circle added live sector lists values binding attributes non-live sector lists representing closed valid-intervals reachable live sector hash table parameter parser creates synthetic start event correct timestamp inserts event stream correct place figure depicts data structures parser temporal joins implemented appending events time-ordered lists represents current valid-interval attribute recent live valid-interval attribute maintained hash table requests gradually built eventattribute bindings connect lists previous section parser enter server start event current list events current list events threadid osdi symposium operating systems design implementationusenix association presence event lists joined forming larger sub-graph schema identifies seed event occurs request web server http request start event events processed timestamp order sub-graphs unreachable valid-intervals closed sub-graph request seed event connected events output complete request garbage collected note thattherewill bemanylists orgraphs turnout represent request events generated applications background system activities additional timeout bound resources consumed activities ideally request schema written in-depth knowledge synchronization mechanisms resource usage idioms application question preferably time application instrumented harder retrofit schema instrumentation existing application knowledge impossible fact applications mentioned paper area future work explore extensionsto expressivenessof schema event affect timelines attributes enhancement ability transitively start stop valid-intervals timelines performance evaluation assessed performance impact magpie event tracing parsing running web stress-test benchmark number http requests served twominute interval recorded cpu-bound workload generated active html content saturated cpu measurements repeatable requests instrumentation server complete requests requests logging turned real-time mode event consumer discernible difference throughput dummy event consumer immediately discarded event reduced throughput requests running magpie parser extract requests online resulted throughput requests imately half attributed etw infrastructure experiments average cpu consumption parser peak memoryfootprint mbandsome millioneventswere parsed web server cpu-bound experiments directly accounts sync tcpip recv connhash httprequest start sync httprequest deliver win unblock sync aspnetreq start isapiid thread unblock sync thread exit tid thread join wait win block figure statements iis schema add explicit thread synchronization points parsed http requests sync statement specifies source destination events transitively joined shared attribute event pattern matching sync specification result signalevent beinginserted source thread aresumeevent destination thread wait event type generates additional synthetic wait event observed drop http throughput etw logging enabled write eventsto file operating real-time mode server throughput requests indicating impact etw infrastructure offline mode negligible workload minutes duration total binary log file produced parser extracted correct number requests peak working set size approximately average number events attributed request synchronization causal ordering lowest level events totally ordered timestamp leading trace execution resource consumption varydepending howthe threads acting request happen scheduled extract meaningful workload model recover threading structure request determining thread block unblock inter-thread causality tells leeway thescheduler hasto re-order processing stages request infer portions request parallelize interest multi-processor deployments web server figure kernel tcpip recv event unblocks iis thread parses http requests unblocks isapi filter thread eventually unblocks thread run asp net active content generator thread blocks sending http response back client threads typically thread pool occasionally observe thread processing multiple logically distinct segments request soitis importantto awareofthesesynchronization points apparent requests synchronization points implicit primitives invoked send receive places thread synchronization performed mechanisms instrumentation osdi symposium operating systems design implementation usenix association figure canonical version http request produced eliding scheduling behaviour retaining thread synchronization points top window shows request scheduled experiment concurrent clients lower window canonical version user-level scheduler reason provide parsed event graphs annotate request withadditional semantics application domain define synthetic events inserted parsed requests signal wait resume explicit causality related signal resume events connected shared attribute-value expressed schema -tuple source event executed thread time destination event thread time join attribute shared events attribute necessarily parameter event types transitively shared throughother joined eventson thread events thread timestamps happen thread events timestamps greater scheduling discipline figure shows synchronization statements iis schema figure synchronization events inserted parser original request events canonicalization system heavily loaded requests tend scheduled highly interleaved fashion shown figure request url identical figure request serviced differs due multiple clients competing system resources figure thread completes work single quantum cpu time top window figure frequently preempted activity interspersed threads servicing connections detailed per-request view system activity undoubtedly determining path request consumed resources constructing workload models performancepredictionordebuggingpurposeswewouldrather represent requests canonical sequence absolute resource demands ignore information request serviced causal ordering annotations discussed previous section produce canonical version request effect concatenating resource demands betweensynchronization points thenscheduling machine unlimited number cpus lower window figure shows result processing stage applied request upper window canonical version modelling purposes cross-machine activity requests activity multiple machines stitch events collected computer machine system separate clock domain timestamps request parser run machine online offline straightforward extend parser deal multiple clock domains current priority request fragments machine canonicalized previously run offline tool combines canonical request fragments connecting synchronization events transmitted packets received packets request comparison thread synchronization overlay binary tree structure linear timestamp-ordered event stream threads synchronize sending message create logical fork event tree original source thread continues whilst enabling destination thread execute parallel thread blocks receive message treated leaf node results applying procedure contrived rpc-style interaction shown figure sohstxetxeotenqack beleotbshteotbs beleotenqlf vteotffcr sobeletxsienqdlestxdc vteotdc vteotdc stxdc sovteotnaksynetbeotdc beleotenqlf vteotdc stxdc sobeletxsienqdlestxdc candleetxack vteotdc stxdc vteotdc vteotffcr sovteotnaksynetbeotdc candleetxack vteotffcr esc fsgs candleetxack vteotffcr etxack beleoteotlf belackdlebsack figure binary tree structure overlaid rpc-style thread interactions tree deterministically serialized order shown osdi symposium operating systems design implementationusenix association deterministically flattening tree representation standard depth-first traversal cluster requests simple string-edit-distance metric 
requiring elaborate traditionally expensive graph-edit-distance metrics produced reasonable results prototype losing tree structure comparing requests limit usefulness approach larger distributed systems requests complex structure recent work developed suitably efficient algorithms treeand graph-edit-distance investigated graph-clustering applying techniquestoimproveourworkloadextractionprocessiscur- rently investigation behavioural clustering clustering stage toolchain groups requests similar behaviour perspective event ordering resource consumption require processing pipeline functions online simple incremental clustering algorithm resulting clusters basis workload model expresses requests occur typified cluster representative occur proportion cluster size clusterer maintains set active workload clusters cluster record representative request referred centroid cluster cluster diameter set requests considered members cluster additionally algorithm track average cluster diameter average inter-cluster distance request presented clusterer computes string-edit-distance serialized representation cluster centroid distance metric function character edit cost traditional form string-edit-distance resource usage deltas events comparison characters represent disk read give edit cost proportional difference bytes read request added cluster smallest edit distance edit distance exceeds trigger threshold case cluster created validation support claim magpie extract individual requests construct representative workload models attempt examine techniques isolation section present results include type url resource longspin aspx thread spins cpu shortspin aspx thread spins cpu small gif retrieve static image tiny gif retrieve static image parspin aspx threads spin cpu concurrently rpcspin aspx thread spins cpu signal thread wait spin cpu thread spins cpu table request types consumptions primary resources assessment quality clusters obtained checks resource accounting accurate experiments events generated instrumentation section event streams parsed discussed section flattened representations requests clustered presented section machines dualprocessor ghz intel running windows server iis sql server experimentswe application center test stress client evaluate accuracy magpie workload models traces synthetic workload contrast themore realistic tions web site behaviour calibrated check extract workload injected experiments intended investigate effectiveness request canonicalization mechanism behavioural clustering algorithm separate requests resource consumption experiments performed minimal asp net website written url consumes resources depicted table cpu consumed tight loop fixed number iterations network bandwidth retrieving fixed size images resource consumption string-edit-distance metric clustering algorithm treats events annotated resource usage points multi-dimensional space intuitive explanation resulting clusters present results resource consumption single dimension concurrency complicating factor examine request extraction modelling tools perform concurrent request invocations concurrent operations place single request processor time differ consumption cpu cycles produce single-varietal workloads mixtures osdi symposium operating systems design implementation usenix association cpu resource consumption single conc mixed exp workload clients accntd cpu clusters dia min model found sep error table clusters produced single mixed request type workloads consuming cpu concurrent serialized request invocations accntd cpu fraction cpu consumed relevant process accounted individual requests clusters found column number requests found cluster total requests dia min sep average cluster diameter distance centroid nearest cluster model error refers difference resource consumption derived workload model parsed requests serialized requests single client concurrent requests clients total workloads listed left-hand columns table note trivial requests exhibit fairly complex interactions kernel multiple worker threads apparent figure depicts type request table records clusters found workloads shows average distance cluster centroid request cluster dia distance centroid nearest cluster min sep experiments requests type longspin aspx requests type shortspin aspx produced large cluster repeating experiments concurrent stress clients produced similar results clusters produced concurrent workload generally larger internal diameter serial workload examining distributions total cpu consumed individual requests shows predicted clusters concurrent workloads exhibit slightly larger spreads due cache effects scheduling overheads validation check total amount cpu time accounted requests parser summed compared aggregatecpu times consumed processes kernel experiment table column entitled accntd cpu indicatesthefractionof relevant cputhatwasaccounted request activity relevant cpu excludes background processes idle thread includes kernel mode activity user mode processing web server threads information directly network resource consumption single conc mixed exp workload clients accntd cpu clusters dia min model found sep error table clusters produced single mixed request type workloads differing primarily consumption network bandwidth concurrent serialized request invocations table caption explanation column headings thereportedfigures due non-request related web server activity health monitoring garbage collection difficulty attributing kernel mode cpu usage correctly current kernel instrumentation final column table labelled model error records resource consumption constructed model differs measured figure computed percentage difference resource consumption requests parsed synthetic workload generatedby mixing clusterrepresentatives inallcases thecluster sizes representatives concisely represent experimental workload cross-check experiment centroid represents resource usage control flow requests type measured isolation experiment similar centroid requests type table shows experiment requests mix type requests compare cpu time consumed requests experiment cpu time predicted type centroids added type centroids prediction falls short observed due worse cache locality repeating computation experiment deficit network activity table request types differ consumption network bandwidth accntd cpu figures noticeably lower experiments surprising common load increases formultiple deferred procedure call network packets correctly ascribed requests osdi symposium operating systems design implementationusenix association internal concurrency single mixed exp workload clients accntd cpu clusters dia min model found sep error table clusters produced single mixed request type workloads consuming cpu request internal concurrency concurrent serialized request invocations table caption explanation column headings places insufficient instrumentation present safely account computation individual request cases parser errs conservative side concurrency internal structure figure shows canonical versions compute bound requests table internal structure requests perform sequential computations lengths performs amount work parallel threads fourth request consumes amount resource time synchronous rpc-style interaction worker thread whilst requests consume amount cpu resource significantly behaviour multiprocessor machine response time latency perspective extracting workload models important capture differences concurrency structure tables show clustering results suite experiments constructed combinations requests tables clear distance metric clustering algorithm capable separating requests differ internal concurrencystructure note experiment table request types fall separated clusters outliers internal concurrency single mixed exp workload clients accntd cpu clusters dia min model found sep error table clusters produced single mixed request type workloads consuming cpu request internal concurrency blocking concurrent serialized request invocations table caption explanation column headings anomaly detection experiments noticed occasional unexpected outlier requests cluster marked result tables examination individual requests revealed case chunk cpu consumed inside dpc sampling profiler events logged etw offending intervals problem tracked miniport ethernet driver calling kedelayexecution transmit callback function concrete proof magpie highlight apparently anomalous behaviour extracted requests 
common outlier requests include jit compilation loading shared libraries cache misses genuine performance anomalies identify isolate outliers advantage accurate workload modelling functionisimplemented abusywait documentation states delays osdi symposium operating systems design implementation usenix association dpcs cpu cpuid threadid threadid threadid sequential compute type dpcs cpu cpuid threadid threadid threadid sequential compute type dpcs cpu cpuid threadid threadid threadid threadid parallel compute type dpcs cpu cpuid threadid threadid threadid threadid compute nested synchronous rpc type figure canonical versions compute-bound http requests internal concurrency structure evaluation turn evaluation toolchain realistic scenarios small two-tier web site enterprise-class database server duwamish section extract requests distributed system accuracy derived workload model experimental setup machine system running duwamish bookstore sample commerce application distributed visual studio augmented duwamish database images stored web server auto-generated data accordance volume size distributions mandated tpc-w previous experiments obtained results requests havethe sameurl lookedat clusters produced mixed workload dynamic html urls characteristics book aspx pageinvokesa single database query retrieve book item data author publisher image book cover retrieved web server file system categories aspx page retrieves details category making rpcs database logon aspx url runs web server section stitcher matches packet events request fragments individual machines produce single canonical form request multiple machines table shows results clusteringonthe web andsqlfragments entire request clarity reported maximum cluster diameter minimum inter-cluster separation set clusters similar previous experiments report fraction relevant cpu included extracted requests accntd cpu column two-tier e-commerce site single mixed exp workload book logon categories mixed accntd cpu clusters max min found dia sep web sql web web sql web sql table clusters found duwamish requests single mixed url workloads results shown clustering workloads individual machines web sql endto-end requests closer inspection resulting clusters reveals book requests primarily differentiated disk read performed web server fetch image book cover categories page amount network traffic varies categories major minor clusters formed stored proceduresinvoked categories aspx getcategories getbooksbycategoryid identified table database request fragments bar url put cluster sql serverquery analyzer tool stored procedures similar cost surprising clusters mixed workload show book logon pages form tighter clusters categories requests spread clusters results workload model based per-url measurements representativethan constructed groupingsimilar requests accordingto observedbehaviour osdi symposium operating systems design implementationusenix association tpc-c benchmark size contents dia min sep key neworder payment stocklevel orderstatus delivery version table clusters formed tpc-c workload workload constrained ratio mix transaction types shown key additional column shows distance cluster centroid null request indication amount resource consumption largely disk case tpc-c tpc-c benchmark results presented section generated microsoft tpc-c benchmark kit configured simultaneous clients single server resulting database fit memory modern machine ran sql server restricted memory accurately reflect cache behaviour realistically dimensioned tpc-c environment clustering algorithm created clusters requests clusters tightly formed dia andwellseparated high inter-cluster distances sep clusters higher intra-cluster distances separated cluster unimportant examining make-up clusters reveals amount performed dominantfactor deciding cluster request cluster version transactions payment transactions cluster delivery transactionsand neworder transactions transactions small amount cluster holds remaining neworder transactions moderate amount orderstatus transactions split clusters based finally clusters stocklevel transactions predicted orders magnitude expensivethan expensive transaction sql server query analyzer shared buffer cache resources clusters represent reasonable summary benchmark workload experimental configuration expose area requiring attention applications database servers shared buffer cache dominant factor affecting performance instrumentation record cache memory observing disk cache misses log writes giventhe explicit sql buffer cache api simple matter record locality sequence pages tables referenced query transaction types predict miss rates cache sizes remains area future work related work closely related work magpie pinpoint pinpoint collects end-to-end traces client requests environment tagging call request simpler event correlation extract requests requires propagation global request heterogeneous software components aim pinpoint diagnose faults applying statistical methods identify components highly correlated failed requests isin contrasttothe path request resource consumption understand model system performance aguilera proposed statistical methods derive causal paths distributed system traces communications approach minimally invasive requiring tracing support rpc layer treating machine black box sacrifice ability separate interleaved requests single machine attribute cpu disk usage accurately approach aimed examining statistically common causal paths find sources high latency magpie request parsing hand captures causal paths workload including rare possibly anomalous distributed event-based monitors debuggers track event sequences machines monitor resource usage essential performance analysis conversely systems track request latency single system address distributed case tipme tracked latency interactive operations initiated input window system path profiling traces control flow patterns basic blocks running program similar approaches operating systems include linux trace toolkit tracks request latencyonasinglemachine themagpietoolchaincould easily built consume ltt events etw events sophisticatd instrumentation framework solaris dtrace arbitrary predicates actions instrumentation points dtrace providesan option speculative tracing potentially lightweight mechanism enabling request sampling chen perkowitz measure web application response times embedding javascript web pages fetched modifying content served instrumenting client server code aggregated data view client-side latency complement detailed server-side workload characterisation obtained magpie conclusion paper magpie toolchain takes stand-alone events generated operating system middleware application components correlates related events extract individual requests expresses requests canonicalized form finally clustersthemto produceaworkload model validated approach traces synthetic workloads showed approach promising complicated applications shown magpie isolate resource demands path requests construct stochastic models give good representation workload behaviour great advantage magpie request structures learnt observing live system realistic workload consequence parsed event trace individual request recorded giving detailed picture requests serviced system acknowledgements gratefully acknowledge encouragement insightful comments shepherd eric brewer proofreaders steve hand tim harris andrew herbert dushyanth narayanan james bulpin past contributions magpie project aguilera mogul wiener reynolds muthitacharoen performance debugging distributed systemsofblackboxes inproc systems principles sosp pages oct al-shaer abdel-wahab maly hifi monitoring architecture distributed systems management proc ieee international conference distributed computing systems icdcs pages barham isaacs mortier narayanan magpie online modelling performance-aware systems workshop hot topics operating systems hotos pages bates debugging heterogeneous distributed systems event-based models behavior acmtransactions computer systems tocs bunke recentdevelopments graphmatching proc international conference pattern recognition pages cantrill shapiro leventhal dynamic instrumentation production systems proc usenix annual technical conference pages june chen perkowitz end-user latency manage internet infrastructure proc workshop industrial experiences systems software wiess dec chen accardi man lloyd patterson 
fox brewer path-based failure evolution management proc symposium networked systems design implementation nsdi pages mar endoand seltzer improving interactive performance tipme proc acm sigmetrics june gao jensen snodgrass soo join operations temporal databases technical report trtime- center oct isaacs barham bulpin mortier narayanan request extraction magpie events schemas temporal joins acm sigops european workshop sept kephart chess vision autonomic computing ieee computer jan joyce lomow slind unger monitoring distributed systems acm transactions computer systems tocs larus program paths proc acm conference programming language design implementation sigplan pages june mattson gecsei slutz traiger evaluation techniques storage hierarchies ibm systems journal microsoft application center test visual studio net edition http msdn microsoft library default asp url library en-us act htm actml main asp microsoft corp event tracing windows etw http msdn microsoft library en-us perfmon base event tracing asp park raghuraman server diagnosis request tracking workshop design self-managing systems held conjunction dsn june risso degioanni architecture high performance network analysis proc ieee symposium computers communications pages july transaction processing performancecouncil tpc benchmark on-line transaction processing specification http tpc tpcc transaction processing performance council tpc benchmark web commerce specification http tpc tpcw yaghmour dagenais measuring characterizing system behavior kernel-level event logging proc usenix annual technical conference june usenix associationosdi symposium operating systems design implementation potentially lightweight mechanism enabling request sampling chen perkowitz measure web application response times embedding javascript web pages fetched modifying content served instrumenting client server code aggregated data view client-side latency complement detailed server-side workload characterisation obtained magpie conclusion paper magpie toolchain takes stand-alone events generated operating system middleware application components correlates related events extract individual requests expresses requests canonicalized form finally clustersthemto produceaworkload model validated approach traces synthetic workloads showed approach promising complicated applications shown magpie isolate resource demands path requests construct stochastic models give good representation workload behaviour great advantage magpie request structures learnt observing live system realistic workload consequence parsed event trace individual request recorded giving detailed picture requests serviced system acknowledgements gratefully acknowledge encouragement insightful comments shepherd eric brewer proofreaders steve hand tim harris andrew herbert dushyanth narayanan james bulpin past contributions magpie project aguilera mogul wiener reynolds muthitacharoen performance debugging distributed systemsofblackboxes inproc systems principles sosp pages oct al-shaer abdel-wahab maly hifi monitoring architecture distributed systems management proc ieee international conference distributed computing systems icdcs pages barham isaacs mortier narayanan magpie online modelling performance-aware systems workshop hot topics operating systems hotos pages bates debugging heterogeneous distributed systems event-based models behavior acmtransactions computer systems tocs bunke recentdevelopments graphmatching proc international conference pattern recognition pages cantrill shapiro leventhal dynamic instrumentation production systems proc usenix annual technical conference pages june chen perkowitz end-user latency manage internet infrastructure proc workshop industrial experiences systems software wiess dec chen accardi man lloyd patterson fox brewer path-based failure evolution management proc symposium networked systems design implementation nsdi pages mar endoand seltzer improving interactive performance tipme proc acm sigmetrics june gao jensen snodgrass soo join operations temporal databases technical report trtime- center oct isaacs barham bulpin mortier narayanan request extraction magpie events schemas temporal joins acm sigops european workshop sept kephart chess vision autonomic computing ieee computer jan joyce lomow slind unger monitoring distributed systems acm transactions computer systems tocs larus program paths proc acm conference programming language design implementation sigplan pages june mattson gecsei slutz traiger evaluation techniques storage hierarchies ibm systems journal microsoft application center test visual studio net edition http msdn microsoft library default asp url library en-us act htm actml main asp microsoft corp event tracing windows etw http msdn microsoft library en-us perfmon base event tracing asp park raghuraman server diagnosis request tracking workshop design self-managing systems held conjunction dsn june risso degioanni architecture high performance network analysis proc ieee symposium computers communications pages july transaction processing performancecouncil tpc benchmark on-line transaction processing specification http tpc tpcc transaction processing performance council tpc benchmark web commerce specification http tpc tpcw yaghmour dagenais measuring characterizing system behavior kernel-level event logging proc usenix annual technical conference june 
boxwood abstractions foundation storage infrastructure john maccormick nick murphy marc najork chandramohan thekkath lidong zhou microsoft research silicon valley abstract writers complex storage applications distributed file systems databases faced challenges building complex abstractions simple storage devices disks challenges exacerbated due additional requirements faulttolerance scaling paper explores premise high-level fault-tolerant abstractions supported directly storage infrastructure ameliorate problems built system called boxwood explore feasibility utility providing high-level thefundamentalstorage infrastructure machines boxwood abstractions perform close limits imposed processor disk native networking subsystem abstractions directly implemented nfsv file service demonstrates promise approach introduction implementing distributed reliable storage-intensive softwaresuchas file systemsordatabasesystems hard systems deal challenges including matching user abstractions files directories tables indices provided underlying storage designing suitable data placement prefetching caching policies providing adequate fault-tolerance incremental scalability ease management generallybelievedthat properties unrealistic ideal hypothesis paper perceived difficulty considerably lessened suitable abstractionssuchas trees linkedlists andhash-tables provided directly storage subsystem compromising performance scalability manageability top built system called boxwood explorethe feasibility utility providingsuch highlevel abstractions data structures fundamental storage infrastructure abstractions directly implemented highly scalable nfsv coherently exporting underlying file system boxwood approach storage significant departure traditional block-oriented interfaces provided disks physical logical virtual key advantages advantage evidenced experience multi-machine nfs server directly integrating data structures persistent storage architecture higher-levelapplications simpler build benefits fault-tolerance distribution scalability cost abstractions inherently deal sparse non-contiguousstoragefree space free-space management contrast sophisticated virtual disk systems provide scalability system deal free space management data placement maintaining user-visible abstractions advantage structural information inherent data abstraction system perform load-balancing data prefetching informedcaching mechanisms implemented infrastructure duplicated subsystem application earliest experience boxwood convinced single universal abstraction storage serve clients current prototype b-tree abstraction typicaloperationslike lookups insertions deletions enumerations simpler chunk store variable sized dataitems canbe allocated freed written read memory today specific choices motivated obserosdi symposium operating systems design implementationusenix association vations b-trees abstraction storage found file systems databases building high-performance scalable distributed b-tree considerable challenge building centralized version good concurrent performance difficult experience data structure complement existing literature building distributed data structures hash tables simple chunk store abstraction match applications notneedthestrict ture b-tree simpler abstraction offers good performance flexibility client applications offloading details free space virtual disk environment address space management prototype implemented collection server nodes cpu ram disks network interface packaged rackmounted server imagine alternative implementationsofservernodesrangingfromindividualdiskunits disk controllers control sets disks addition focus distributed storage abstractions paper offers insights structure fault-tolerant distributed systems classic approachtobuildingsuchsystemsistouselamport sreplicated state machines paxos approach highly reliant paxos consensus fault-tolerant distributed lock service simple notion shared memory precisely shared store programming deal inherent complexity distributed system independentfailures fault-tolerant distributed systems boxwood system structure goal boxwood project experiment data abstractions underlying basis storageinfrastructure generally termstorageinfrastructure connotes requirements redundancy backup schemes tolerate failures expansion mechanisms load capacity balancing consistency maintenance presence failures ideally boxwood providing distributed data structures current status satisfy ideal made progress deal transient failures provide services logging transaction recovery deal presenceof failures automatic reconfiguration expansion providemechanisms apaxosconsensusmodule alock service failure detector insure correct inventory components system provide consistent view system section describe parts design relates data abstractions storage infrastructure mechanisms envisage system deployed machine room enterprise cluster principal storage infrastructure file systems database systems services environment justifies assumptions impact design choices enumerate assumptions design principles describing system greater detail preliminaries boxwood system targeted environment multiple processing nodes locally attached storage interconnected high-speed network implement abstractions services software running processor communicates locally attached disks usinga theunix raw device interface remote procedure call rpc access resources services executing remote processors boxwood system organized interdependent services layering managing complexity boxwood system b-tree chunk store services mentioned earlier section constructed layering top chunk store service turn layered top simple replicated logical device abstraction section layeringhas potential context switching overheads design avoids problems running layers single address space interconnection network gigabit ethernet feel justified providing fault-tolerance synchronous replication data disks attached separate machines scheme fault-free operations primary replica wait secondary finish writing copy return client wait large slow network tolerable high-speed lan feel justified assuming cost making rpcs small network scaled adding switches implementation results bear assumptions security model target environment specifically assume machines single administrative domain physically secure send messages machines clear make special provisions encryption authentication security assume cpus disks networks fail failures transient permanent examples transient failures tolerate faulty osdi symposium operating systems design implementation usenix association power supply takes machine attached disks back contents ram power restored operator mistakenly unplugs network cable examples permanent failures disk suffers catastrophic media failure server log destroyedbeyondrepair assume realistically failure disk affects disk failure machine affects disks attached althoughunlikely assume networks partition failures assumed fail-stop fundamental mechanism protecting data catastrophic media failure chained-declustered replication permanent failure single disk data loss data unavailability fact chained-declustering prevents data loss presence combinations multiple disk failures combinationsof multiple disk failures deal failure mode machines suffer transient power outage back lost contents ram design service consists software modules executing multiple machines service independentlyarrangesfor failoverand highavailability presenceof multiplefailures forinstance ourpaxos consensus service section works long majority paxos servers running double failures tolerated running paxos servers triple failures similarly lock service section single master slaves standby depending number slaves choose run tolerate multiple permanent failures b-tree nfs services sections impose additional availability constraints long instance eachmoduleis runningandthe underlyingservices locking consensus replicated data scale deployment increases probability multiple failures increases design vulnerable increased disk failures regard probability double disk failures problem data protection scheme triplexing erasure coding lowest layers changingthe design services boxwood b-tree abstraction simple chunk store abstraction provided chunk manager figure shows organization abstractions relative introduce briefly defer fuller description sections b-tree abstraction b-trees variants widely viewed b-tree module allocate deallocate read write chunk manager read write rldev rldev rldev rldev figure design boxwood abstractions tree layered atop chunk manager layered top replicated logical device numbers parentheses refer section describing design module nary supportinginsertion lookup deletion andenumeration key-value pairs secondary storage b-trees complex exercise fully features foibles distributed storage architecture theyseemed excellentcandidatefor data abstraction implemented boxwood boxwood b-tree module distributed implementationof sagiv b-link tree algorithm variant lehman-yao b-link tree algorithm sagiv algorithm desirable property locking considerably simplified traditional 
b-tree algorithms compromising concurrency atomicity sagiv original b-link tree algorithm classicbtree -tree -tree counterparts runs single machine environment locks synchronizing accesses multiple threads stores data memory persistently disk sagiv algorithm suited distributed implementation observation independentlymade johnson colbrook algorithm implement b-link tree alreadydeal thread concurrencywithin single machine design extends design multiple machines ensuring simple constraints met threads executing multiple machines global locks synchronizingaccess shared data data storedbyone threadrunningona machinecan accessed thread constraint readily providedby distributed tomeetthesecond osdi symposium operating systems design implementationusenix association requirement existing virtual logical disk logical volume manager decided reasons existing logical virtual disk systems require management physical virtual space systems support faulttolerance incremental expansion scalability systems petal fab implement logging recovery schemes duplicate logging recovery required b-link tree level increasing bookkeeping overheads making difficult implement optimizations design chunk data store data store b-tree abstraction provided bya chunkmanager theprincipalfunctionof thechunk manageris hide details physical storage media provide level address mapping b-tree algorithm deal opaque pointers stored data chunk manager acts memory allocator hands variable length chunks data store subsequentlybe written read deallocated chunk manager carves chunks storage services lower layer called replicated logical device rldev layer rldev access fixed amount chain-declustered storage rldev implemented machines separate physical disk drives replication infrastructure services addition software modules implement abstractions boxwood important modules provide essential distributed system services heavily boxwood system implement abstractions directly external clients boxwood system services paxos service implementation paxos part-time parliament algorithm usedto store globalsystem state suchas thenumber machines number rldevs system thedistributedlock serviceto track client information identity lock master recovery lock service transient failure lock service distributed lock service handles multiple-reader single-writer locks service rldev module chunkmanager multi-nodenfs server transaction service service redoundo logging facility transaction support recovery b-tree module nfs server boxwood design section describes design components boxwood system detail interdependencies components describe order minimizes forward paxos service paxos service general-purpose implementation state machine approach paxos parttime parliament algorithm refer service general-purpose clients service define arbitrary client-specific state pass client-specific decrees modifyand query state making rpc calls paxos service state maintained machines paxos state occur order replica state consistent long majority replicas non-faulty boxwood depends types client states maintained paxos client state typically refers essential state required internal boxwood service lock service rldev layer chunk manager state consulted eachserviceas figuration layer typically state includes machines disks resources network port identifiers client paxos service implemented small collectionofmachines typicallythree withtwomachinesconstitutingaquorumormajority donotdedicateanentire machine implementing service paxos service instance machine restricted single server process paxos state maintained disks locally attached machines hosting server butmerelythat persistent failure disk storage machine considered failure paxos server choice machines arbitrary tolerate failure machine small cluster tolerate failure machines running service machines draw clear distinction characteristics paxos service characteristics chain replication supporting high throughput availability robbert van renesse rvr cornell fred schneider fbs cornell fast search transfer asa troms norway department computer science cornell ithaca york abstract chain replication approach coordinating clusters fail-stop storage servers approach intended supporting large-scale storage services exhibit high throughput availability sacrificing strong consistency guarantees outlining chain replication protocols simulation experiments explore performance characteristics prototype implementation throughput availability objectplacement strategies including schemes based distributed hash table routing discussed introduction storage system typically implements operations clients store retrieve change data file systems database systems examples file system operations read write access single file idempotent database system operations transactions access multiple objects serializable paper concerned storage systems sit file systems database systems concerned storage systems henceforth called storage services store objects unspecified nature support query operations return derived single object support update operations atomically change state single object pre-programmed possibly non-deterministic computation involving prior state object file system write special case storage service update turn special case database transaction increasingly on-line vendors amazon search engines google fast host information-intensive services provide connecting large-scale storage systems networks storage service compromise applications database system expensive file system lacks rich semantics challenge building large-scale storage service maintaining high availability high throughput failures concomitant storage service configuration faulty components detected replaced consistency guarantees crucial construction application fronts storage service simplified strong consistency guarantees assert operations query update individual objects executed sequential order effects update operations necessarily reflected results returned subsequent query operations strong consistency guarantees thought tension achieving high throughput high availability system designers reluctant sacrifice system throughput availability regularly decline support strong consistency guarantees google file system gfs illustrates thinking fact strong consistency guarantees osdi symposium operating systems design implementationusenix association large-scale storage service incompatible high throughput availability chain replication approach coordinating fail-stop servers subject paper simultaneously supports high throughput availability strong consistency proceed interface generic storage service explain query update operations implemented chain replication chain replication viewed instance primary backup approach compares summarizes experiments analyze throughput availability prototype implementation chain replication simulated network simulations compare chain replication storagesystems cfs past based distributed hash table dht routing simulations reveal surprising behaviors system employing chain replication recoversfrom server failures chain replication compared work scalable storage systems trading consistency availability replica placement concluding remarks endnotes storage service interface clients storage service issue requests query update operations ensure request reaching storage service guaranteed performed end-to-end argument suggests point clients storage service simply generates reply request receives completes lost requests lost replies handled client re-issues request time elapsed receiving reply reply query objid opts derived object objid options opts characterizes parts objid returned objid remains unchanged reply update objid newval opts depends options opts general case produced nondeterministic current objid newval objid query operations idempotent update operations client re-issues nonidempotent update request precautions ensure update state hist objid update request sequence pending objid request set transitions client request arrives pending objid pending objid client request pending objid pending objid pending objid client request pending objid processed pending objid pending objid query objid opts reply options opts based hist objid update objid newval opts hist objid hist objid reply options opts based hist objid figure client view object performed client issue query determine current object reflects update client request lost reaching storage service indistinguishable client storage service means clients exposed failure mode storageserver exhibits transient outages client requests acceptable client performance depend rest storageand abstraction-related services isolated scaling system osdi symposium operating systems design implementation usenix association scaling paxos client state stored paxos dynamically changed addition rldevs chunk managers machines disks requireus dynamicallychangethe numberof paxos servers store state conceivablethat extremely large scales increase fly number paxos servers system guard increased machine failures implement protocols increase decrease dynamically number paxos servers ensure paxos involved failures system reconfigurationsof system avoid overloading servers forthecommoncase complete quickly implement slightly restricted form paxos decreasing degree concurrency allowed standard paxos multiple decrees concurrently executed system restrict decrees passed sequentially makes implementation slightly easier sacrificing effectiveness protocol purposes ensureliveness propertiesin consensusalgorithm paxos shown failure detector fairly weak properties principle weak failure detector failure detectors stronger guarantees forourrpc andlockservermodules implement multiple failure detection modules single restrictions required paxos failure detector failure detection module implemented machines exchange periodic keepalive beacons machine monitored collection observer machines exchanges keepalive messages machine check status machine querying observers machine considered failed majority observer machines heard keepalive messages threshold period invariants maintain machine dies failure detector eventually detect failure detector service confusedwithanindividualobserver tellsaclientthatamachine dead machine dead figure sketches message protocol assuming single observer majority messages udp fail arriveor arriveout order albeit low probability lan assume synchronized clock machine assume clocks forwards clock drift machines bounded udp message delays non-zero client client figure periodically intervals sends beacon messages observer messages delivered reliably observer echoes beacon message receives backto client anypointin time observerconsiders client dead received beacon client preceding graceperiod client considers receipt echo acknowledgement observer client track time beacon acknowledged observer knowledgement considers dead kills time client observer message delay query reply alive query reply dead query earliest time declare dead client beacon beacon ack acked beacon latest time considers alive thinks dead thinks alive grace period grace period lost figure message protocol failure detector assuming single observer clients time advances thinks dead dead client client figure wishes monitor client client sends query message observer observerthensends view receives reply observer claiming dead considers dead considers alive assumptions clock drift non-zero message delay protocol conservative maintains invariants reality don single observer collection observers reasons fault-tolerance case client figure pronounces dead replies majority observers observer pronounce dead receivea majorityof responses pronouncea dead assumes alive hand considers deadas soonasgraceperiodtime elapses acknowledgement majority protocol lead state considers dead thinks alive point thinks dead majority pronounces dead receivedan acknowledgementfrom majority dead maintains invariant considers dead failure detector dead osdi symposium operating systems design implementationusenix association protocol sketched works clients ensure practice client monotonically increasing incarnation number time transient failure values graceperiod number observers tunable parameters found suitable environment observer machines clients system convenience feasibletousedifferentsubsetsofmachinesasobserversfor specific client distributed lock service thelock serviceprovidesa ingmechanismthatallows clients toacquiremultiplereader single-writer locks design borrows techniques earlier work describe details scheme underscore rationale choices locks uniquely identified byte arrays lock service impose semantics locks explicit timeouts failure detector time unresponsive clients essence locks act degenerate leases clients lock service clerk module linked address spaces leases cached clerk revoked service conflicting request clerk clerk blocks incoming revocation request outstanding local lease completed optimization don support clerk release lease time clients optionally arrange lock service call recovery function instance limiting frequency duration transient outages chain replication duration transient outage shorter time required remove faulty host add host client request processing proceeds minimal disruption face failure recovery reconfiguration replica-management protocols block operations sacrifice consistency guarantees failures reconfigurations functionality storage service giving client view object state object state transitions response query update requests figure pseudo-code give specification object objid figure defines state objid terms variables sequence hist objid updates performed objid set pending objid unprocessed requests osdi symposium operating systems design implementation usenix association replies tail queries head updates figure chain figure lists state transitions transition asserts arriving client request added pending objid pending requests transition transition frequently transition high-level view request processing request removed pending objid query suitable reply produced update appends denoted tohist objid chain replication protocol servers assumed fail-stop server halts response failure making erroneous state transitions server halted state detected environment object replicated servers servers fail compromising object availability object availability increased probability servers hosting object failed simulations explore probability typical storage systems henceforth assume servers replicating object fail concurrently chain replication servers replicating object objid linearly ordered form chain figure server chain called head server called tail request processing implemented servers roughly reply generation reply request generated tail query processing query request directed tail chain processed atomically replica objid stored tail update processing update request directed head chain client request processed instance atomically fail replica lock service objid guarantees head leases state acquired client forwarded subsequently reliable failed fifo link released element recoveryis chain successfully completed handled modest forwarded extension gray cheriton request standard handled lease mechanism tail strong primarily consistency focused write-through client query caching requests deal update residual requests state exists processed serially client single lease server timed tail processing query request usage involves found single server b-tree 
module section failureofa lockclient determinedbythe failure detector notice scheme work correctly client lock service failure detector consistent fashion lock service revoke lease client believed lease ensure correct behavior requiring conditions failure detector client dies failure detector eventually notice dead failure detector claims client dead client died time prior alive transient failure ensure client alive finishes recovery assumes holds leases registers lock service fault-tolerance lock service consists single master server slave servers running separate machines cluster typically single slave server multiple machine failuresarecommon onlythe master server identityis part globalstate paxos hands leases lock service list clerks part paxos state failure detector pronounces current master dead slave takes passing paxos decree identity current master list clerks queries clerks lease state clerks dead point case lock service calls recovery means query cheap operation update request processed computation ofthet servers contribute producing reply arguably redundant redundant servers increase fault-tolerance note redundant computation servers avoided chain replication computed head forwarded chain replica perform write forwarding state means update non-deterministic operation non-deterministic choice made head protocol details clients directly read write variables hist objid pending objid figure free implement convenient chain replication implement specification figure hist objid defined hist objid hist objid stored tail chain pending objid defined set client requests received server chain processed tail chain replication protocols query processing update processing shown satisfy specification figure demonstrating state transition made server chain equivalent no-op allowed transitions descriptions hist objid pending objid implemented chain assuming moment failures occur observe server transitions affecting hist objid pending objid server chain receiving request client affects pending objid tail processing client osdi symposium operating systems design implementationusenix association request affects hist objid server transitions equivalent no-ops suffices show transitions consistent client request arrives chain clients send requests head update tail query receipt request adds set requests received server processed tail receipt adds pending objid defined chain consistent request processed tail execution request 
removed set requests received replica processed tail deletes request pending objid defined chain step processing request tail replica hist objid defined implements hist objid remaining steps coping server failures response detecting failure server part chain fail-stop assumption failures detected chain reconfigured eliminate failed server purpose employ service called master detects failures servers informs server chain predecessor successor chain obtained deleting failed server informs clients server head tail chain assume master single process fails simplifies exposition realistic assumption prototype implementation chain replication replicates master process multiple hosts paxos coordinate replicas behave aggregate single process fail master distinguishes cases failure head failure tail iii failure server chain handling depends insight updates propagated chain server head chain labeled server labeled tail label define hist objid precedesequal hist objid hold sequence requests hist objid server label prefix sequence hist objid server label updates elements chain reliable fifo links sequence updates received server prefix received successor update propagation invariant servers labeled holds predecessor chain hist objid precedesequal hist objid failure head case handled master removing chain making successor head chain successor exist assumption holds servers faulty changing chain deleting transition shown noop consistent figure easily altering set servers chain change contents pending objid recall pending objid defined set requests received server chain processed tail deleting server chain effect removing pending objid requests received forwarded successor removing request pending objid consistent transition deleting chain consistent specification figure failure tail case handled removing tail chain making predecessor tail chain predecessor exist assumption server replicas faulty change chain alters values pending objid hist objid manner consistent repeated transitions pending objid decreases size hist objid precedesequal hist objid due update propagationinvariant tholds changing tail potentially increases set requests completed tail definition decreases set requests pending objid required update requests completed osdi symposium operating systems design implementation usenix association completed hist objid tail hist objid defined hist objid failure servers failure server internal chain handled deleting chain master informs successor chain configuration informs predecessor update propagation invariant invalidated means employed ensure update requests received failing forwarded chain update requests hist objid predecessor obvious candidate perform forwarding bookkeeping coordination required set requests total ordering requests set define request sequence consistent requests requestsare arranged ascending order finally request sequences prime consistent define prime sequence requests appearing prime prime consistent requests sequence prime ordered update propagationinvariant preservedby requiring thing replica connecting successor send fifo link connects requests hist objid reached process forward requests receives subsequent assuming chain position end server maintains list update requests forwarded successor processed tail rules adding deleting elements list straightforward server forwards update request successor server appends tail sends acknowledgement ack predecessor completes processing update request receipt ack server deletes forwards ack predecessor request received tail received predecessors chain conclude inprocess requests invariant hist objid hist objid master figure space-time diagramfor deletion internal replica update propagation invariant maintained receiving notification master successor forwards sequence requests forward prefix appears hist objid protocol execution depicted figure embodies approach including optimization sending prefix message informs role message acknowledges informs master sequence number update request received message informs role compute suffix send message carries suffix extending chain failed servers removed chains shorter chains tolerate fewer failures object availability ultimately compromised server failures solution add servers chains short provided rate servers fail high adding server long chain length close desired servers failures needed compromise object availability server theory added chain practice adding server end chain simplist tail empty list initializing trivial remains initialize local object replica hist objid satisfies update propagation invariant initialization hist objid accomosdi symposium operating systems design implementationusenix association plished chain current tail forward object replica hist objid stores forwarding time object large concurrent processing query requests clients processing updates predecessor provided update appended hist objid precedesequal hist objid holds forwarding update propagation invariant holds hist objid hist objid holds inprocess requests invariant established begin serving chain tail notified longer tail free discard query requests receives clients policy forward requests tail requests sequence master notified tail clients notified query requests directed primary backup protocols chain replication form primary backup approach instance state machine approach replica management primary backup approach server designated primary imposes sequencing client requests ensures strong consistency holds distributes sequence servers backups client requests resulting updates awaits acknowledgements non-faulty backups receiving acknowledgements sends reply client primary fails back-ups promoted role chain replication primary role sequencing requests shared replicas head sequences update requests tail extends sequence interleaving query requests sharing responsibility partitions sequencing task enables lower-latency lower-overhead processing query requests single server tail involved processing query processing delayed activity chain compare primary backup approach primary responding query await acknowledgements backups prior updates chain replication primary backup approach update requests disseminated servers replicating object replicas diverge chain replication dissemination serially resulting higher latency primary backup approach requests distributed backups parallel parallel dissemination time needed generate reply proportional maximum latency non-faulty backup serial dissemination proportional sum latencies simulations reported quantify performance differences including variants chain replication primary backup approach query requests server expectations trading increased performance strong consistency guarantee simulations understanding differences server failures handled approaches central concern duration transient outage experienced clients service reconfigures response server failure concern added latency server failures introduce delay detect server failure dominant cost cost identical chain replication primary backup approach analysis recovery costs approach assuming server failure detected message delays presumed dominant source protocol latency chain replication cases failure head failure middle server failure tail head failure query processing continues uninterrupted update processing unavailable message delivery delays master broadcasts message head successor notifies clients head broadcast middle server failure query processing continues uninterrupted update processing delayed update requests lost transient outage experienced provided server prefix chain received request remains operating osdi symposium operating systems design implementation usenix association failure middle server lead delay processing update request protocol figure involves message delivery delays tail failure query update processing unavailable message delivery delays master sends message tail notifies clients tail broadcast primary backupapproach cases failure primary failure backup query update requests affected primary failure transient outage message delays experienced master detects failure broadcastsa message backups requesting number 
updates processed telling suspend processing requests backup replies master master broadcasts identity primary backups primary processed largest number updates forward backups updates missing finally master broadcasts message notifying clients primary backup failure query processing continues uninterrupted provided update requests progress update request progress transient outage message delay experienced master sends message primary indicating acknowledgements forthcoming faulty backup requests subsequently worst case outage chain replication tail failure long worst case outage primary backup primary failure case chain replication middle server failure shorter case outage primary backup backup failure duration transient outage dominant consideration designing storage service choosing chain replication primary backup approach requires information mix request types chances servers failing simulation experiments understand throughput availability chain replication performed series experiments simulated network involve prototype implementations chain replication alternatives interested delaysintrinsic processing communications chain replication entails simulated network infinite bandwidth latencies message single chain failures simple case chain failures replication factor compare throughput replication management alternatives chain chain replication primary backup onbehalfof clients onthe clients arealive considers leases held dead clients free recovery procedure weak-chain chain replication modified query requests random server weak-p primary backup modified query requests established dead client lock service considers leases held client free tity master server informationis cached rpc master returns exception point refreshed rpcs machine return exceptionif failure detector claims target dead lock service fails servers fail clients make forward progress re-established simple master-slave design lock service elaborate scalable schemes unnecessary storage-centric environments ourrationaleis active lock servers specific lock implemented single lock server time highly contended lock lock contention due data sharing performance problem clients long implementation lock server bottleneck experience deploying petal frangipani scalable lock service bear rldevs replicated logical devices boxwood implements storage replication simple abstraction call replicated logical device rldev rldev logically block device expects block-aligned accesses multiples block units osdi symposium operating systems design implementation usenix association choseto implementreplicationat fairlylow level abstraction hierarchy principal reasons providing replication low level higher layers typically complex nature depend fault-tolerant storage makes logic higher layers simpler reason instance implementation chunk manager section considerably easier rldev layer replicating low level abstraction relevant replication mapping failover logic internal data structures made simple rldevs implement chained declustering single rldev fixed size consists segments equal size located disks machines singlediskwill list rldevs segments belonging identity machines host primary secondary segments disks part global state maintained paxos rldev added locations segments belongingto rldev changed paxos decree passed thereplicationprotocolis fairlystandard onereplica designated primary secondary initialization replica reads state paxos service monitors peer failure detector replicas writes performed reads client sends write requests primary waits completion clients rldev hints determinewhere replicas located hints wrong updated reading paxos state rldev ing hints refreshing replicas dies surviving replica continues accept writes reads call degraded mode writes system accepting data replicating dead replica subsequent failure surviving replica accepted degradedmode data replica finished recovering lead data loss accepts degraded mode writes survivor passes decree effect dead replica back transient failure reconcile stale data importantly accept reads writes reconcile stale data happenbecausethe replica working degraded mode dead blocks degraded mode writes put log called degraded mode log reconciliation fast involves affected blocks notice worst case entire segment written degraded mode previously dead replica prepared copy entire segment leverage mechanism implement automatic reconfiguration rldev reconfigurationoperation redistribution data rldev disk machine enable load balancing making rldev small makes easy copy data quickly high-bandwidth lan link cutting reconfigurationtime mentioned previously replicas primary waits secondary commit write returning client order cope transient failure writes flight rldev implements dirty region log serves purpose degraded mode log mentioned previously dirty region log primary track writes flight secondary secondaryreplies log entrycan removedfrom primary principle recover random server note weak-chain weak-p implement strong consistency guarantees chain fix query latency server fix update latency numbers based actual values querying updating web search index assume update entails initial processing involving disk read cheaper forward object-differences storage repeat update processing anew replica expect latency replica process object-difference message couple disk accesses modest computation chain comprises servers total latency perform update message client head update latency head process object difference message servers additional forwarding latencies query latency figure graph total throughput function percentage requests updates andt clients mix requests split queries updates consistent percentage client submits request time delaying requests long receive response previous request understanding clients dealing operator mistakes concurrent internet requests services kiran outstanding nagaraja throughput abio weak-chain oliveira ricardo osdi bianchini symposium richard operating martin systems thu design nguyen department implementationusenix computer science rutgers piscataway knagaraj fabiool association ricardob total rmartin throughput tdnguyen percentage updates rutgers weak chain abstract operator mistakes significant source unavailability modern internet services total paper characterize transient mistakes failure performing primary throughput percentage updates weak chain total throughput percentage updates weak chain figure request throughput function percentage updates replication management alternatives chain andweak denoting weak-chain andweak-p replication factors weak-p found virtually identical figure single curve labeled weak separate curves weak-chain weak-p observe chain replication chain hasequal superior performance primary-backup percentages updates replication factor investigated consistent expectations head tail chain replication share load primary backup approach handled solely primary curves weak variant chain replication surprising weak variants perform worse chain replication strong consistency update requests factors involved weak variants chain replication primary backup outperform pure chain replication query-heavy loads distributing query load servers advantage increases replication factor percentage update requests increases ordinary chain replication outperforms weak variant updates head pure chain replication queries delayed head awaiting completion update requests time consuming capacity head update request processing query requests handled weak-chain andweak-p implement strong consistency guarantees surprisingly settings replication management schemes preferred finally note throughput chain replication primary backup affected replication factor provided sufficient concurrent requests multiple requests pipelined multiple chains failures object managed separate chain objects large adding replica involve considerable delay time required transferring object state replica hand objects small large storage service involve objects processor system host servers multiple chains costs multiplexing processors communications channels prohibitive failure single processor affects multiple chains set objects grouped single volume considered object purposes chain replication designer considerable latitude deciding object size set experiments assume constant number volumes hash function maps object volume unique chain chain comprises servers hosted processors selected implementing storage service osdi symposium operating systems design implementation usenix association average throughput servers queries updates figure average request throughput client function number servers percentages updates clients assumed send requests dispatcher computes hash determine volume chain storing object concern forwards request chain master sends configuration information volume dispatcher avoiding master communicate directly clients interposing dispatcher adds delay updates queries doesn affect throughput reply produced chain directly client dispatcher clients experiments submitting queries updates random uniformly distributed chains clients send requests fast subject restriction client request outstanding time facilitate comparisons gfs experiments assume volumes replicated times vary number servers found difference chain weak chain andweak alternatives figure shows average request throughput client chain replication function number servers varying percentages update requests effects failures throughput chain replication server failure three-stage process start time conservatively assume seconds experiments elapses master detects server failure offending server deleted chain master ultimately adds server chain initiates data recovery process takes time proportional data stored faulty server network bandwidth delays detecting failure deleting faulty server chain increase request processing latency increase transient outage duration experiments section explore assume storage service characterized parameters table values inspired reported gfs assumption network bandwidth based reserving data recovery half bandwidth mbit network time copy gigabytes stored server hours minutes order measure effects failures storage service apply load exact details load matter greatly experiments eleven clients client repeatedly chooses random object performs operation awaits reply watchdog timer client start loop iteration seconds elapse reply received ten clients exclusively submit query operations eleventh client exclusively submits update operations parameter number servers number volumes chain length data stored server gigabytes maximum network bandwidth devoted data recovery server megabytes sec server reboot time failure minutes table simulated storage service characteristics osdi symposium operating systems design implementationusenix association query thruput time query thruput time upd thruput time upd thruput time failure failures figure query update throughput failures time experiment executes simulated hours thirty minutes experiment failure servers simulated gfs experiments master detects failure deletes failed server chains involving server chain shortened failure master selects server add data recovery servers started figure shows aggregate query update throughputs function time case single server fails note sudden drop throughput simulated failure occurs minutes experiment resolution x-axis coarse throughput seconds failure master requires bit seconds detect server failure delete failed server chains failed server deleted chains processing proceed albeit lower rate fewer servers operational request processing load shared data recovery consuming resources servers lower curves graph reflect minutes failed server operational target data recovery time data recovery volume successfully completes query throughput improves graph tail chain handling growing proportion query load expect data recovery concludes query throughput start experiment reality subtle volumes longer uniformly distributed servers server participate fewer chains servers tail chain participate load 
longer balancedoverthe servers andaggregatequery throughput lower update throughput decreases time server failure master deletes failed server chains throughput initially throughput improvement occurs server failure chains length reducing amount work involved performing update gfs experiments case servers fail figure depicts chain replication protocol recovery smooth takes additional time large scale replication critical data number servers increases aggregate rate server failures servers fail volume unavailable osdi symposium operating systems design implementation usenix association mtbu days servers mtbu days servers servers ring rndseq rndpar figure mtbu confidence intervals function number servers replication factor placement strategies dht-based placement maximum parallel recovery random placement parallel recovery limited degree dhts random placement maximum parallel recovery probability depends volumes servers extent parallelism data recovery investigated volume placement strategies ring replicas volume consecutive servers ring determined consistent hash volume identifier strategy cfs past number parallel data recoveries limited chain length rndpar replicas volume randomly servers essentially strategy gfs notice servers limit number parallel data recoveries rndseq replicas volume randomly servers rndpar maximum number parallel data recoveries limited ring strategy system benchmark quantifying impacts placement parallel recovery understand advantages parallel data recovery server fails participating chains chain data recovery requires source volume data fetched host element chain processors constraints placement volumes easy ensure elements disjoint random placement volumes sources extensive set experiments human operators realistic three-tier auction service mistakes observed range software misconfiguration fault misdiagnosis incorrect software restarts propose validate operator actions made visible rest system demonstrate accomplish task creation validation environment extension online system componentscan validatedusingreal workloads migrated running service show prototype validation system detect operator mistakes observed introduction online services search engines e-mail workgroup calendars music juke-boxes rapidly numeroususers work leisure increasingly services comprised consults complexconglomerates dirty region distributed log hardware determine software components including front-end rent devices contents numerous kinds disks application packages writes authenticators loggers databases differing storage servers crash ensuring rldevs high availability client turn services dirty challenging region task log clients frequent explicitly hardware read software write upgrades replica reconcile systems constantly differences evolving crash boxwood evolution complexity clients maintain services imply log large number reasons unforeseen interactions added cost component maintaining failure equivalent common occurrence dirty region services information typically modest based violation commodity layering components felt fast deployment tradeoff low justified cost performance factors gain saving research additional disk partially write supported recovering nsf rldev grants fairly straightforward eiaeia- failure cases ccris permanent surprising failure service disk failures occur transient failure frequently processor paper attached disks characterize alleviate permanent significant disk source failure service rldevs failures hosted operator mistakes disk context reconstituted cluster-based internet orperhapsmorethanone services disk studies recoveryof shown rldev proceeds percentage independently service rldev failures recovering froma attributable permanentdisk operator failure mistakes contacts rldev increasing machine hosts decade surviving recent segment study copies commercial contents services showed entire segment operator contrast mistakes responsible failure transient failures data retrievedfrom peer services limited dominant degraded mode source writes failures peer largest contributor recovering time segment degraded repair mode older writes study tandem applied systems recovering found segment operator dirty mistakes region log dominant surviving reason segment read outages work begins recovering set server live apply operator in-flight experiments writes explore recovering nature segment operator mistakes date recovering impact server passes availability paxos decree three-tier indicating auction state service rldevis experiment operator writes osdi perform scheduled symposium maintenance task operating systems design diagnoseand-repair task implementationusenix category association encompasses tasks chunk manager upgrading fundamental software storage upgrading unit hardware boxwood adding chunk removingsystem chunk components sector-aligned category sequence encompasses consecutive experiments bytes rldev inject allocated fault response service single request operator chunk discover identified fix opaque problem handle operator globally experiments unique seek system cover chunks managed operator tasks chunk manager achieve module complete supports statistical characterization operations allocate operator deallocate behavior read goal write deallocated characterize handles guaranteed mistakes reused occur reading common writing operator unallocated tasks handle raises gather exception detailed traces rldev operator actions accessed machinewith evaluate suitable clerk linked effectiveness techniques designed principle prevent chunk mitigate manager impact allocate chunks operator mistakes rldev fact continuing long experiments single cover chunk manager wider range alive operator tasks collect manage larger sampling rldevs behaviors non-faulty performed simplicity experiments ease load volunteer balancing operators performance wide variety designate skill pair levels chunk managers results show running total mistakes machines ranging manage software space configuration set fault misdiagnosis rldevs typically softosdi rldevs symposium operating primary systems secondary design segments located implementationusenix disks association ware local restart mistakes chunk configuration managers mistakes reduces number types network hops required common perform chunk occurrences operations incorrect software restarts primaryinitiatingall common occurrences perform large reads number writes mistakes mapping led opaque degradation handle service throughput rldev offset large replicated number persistently mistakes rldevs operators made mapping information propose accessed services allocate validate call requires operator actions mapping exposing created effects deallocate clients call deletes key idea mapping reads check writes correctness require operator mappingto actions consulted validation thereforecache environment mappingon extension primary secondary map lock lock service protects mappings primary makes mappings lock cached primary exclusive mode primary mappings writes mappingto rldev sends rpc secondary directly updates cached copy mappings acquiring lock startup secondary read latest mappingstatefromtherldevsothatsubsequentrpcs inordertogetthelateststatefromtherldev lock sharedmode reads themappingfromtherldev releases lock acquires primary hand acquires map lock exclusive mode making mapping ordinarylocking mechanism ensure consistency data primary dies secondarywill notice failure detector acquires map lock exclusive mode acts primary secondary dies primary detect failure detector continues update mappings rldev make rpcs revocation map lock indicating secondary alive read state design chunk manager simple largely decisions implement replication level chunk manager locking service failover figure shows relationship chunk manager rldev layer primary chunk manager cached mapping state rpc update mapping rldev secondary chunk manager cached mapping state read write mapping read mapping figure chunk manager pair relies shared rldev rpcs mapping information consistent mapping opaque handles disk offsets opaque handle consists -bit chunk manager identifier -bit handle identifier chunk manager identifier corresponds pair chunk managers locations primary secondary chunk manager change time maintained 
paxos andcachedbythe chunkmanagerclerk clerk mapping direct chunk requests appropriatemanager ifthecacheis outofdate thereisnocorrectness issue misdirecting rpc incorrectchunkmanagerwill returnan error whichcauses clerk refresh mappings translation handle identifier rldev performed chunk manager responsible handle consults cached mapping table translate rldev offset invokes rldev clerk accesses physical disks cases segment rldev local disk accesses incur network overhead anopaque handle reused enforces condition reusing handle identifier deallocated online system components validation called disjoint disjoint sources elements data recovery chains occur parallel shorter interval data recovery implies shorter window vulnerability small number concurrent failures render volume unavailable seek quantify time unavailability mtbu object function number servers placement strategy server assumed exhibit exponentially distributed failures mtbf time failures hours number servers storage system increases number volumes add servers experiments number volumes defined times initial number servers server storing volumes time postulate time takes copy data server hours corresponds copying gigabytes mbit sec network restricted half bandwidth data recovery gfs experiments maximum number parallel data recoveries network limited servers minimum transfer time set seconds time takes copy individual gfs object kbytes figure shows mtbu ring strategy appears approximately zipfian distribution function number servers osdi symposium operating systems design implementationusenix association order maintain mtbu grow chain length increasing number servers graph chain length increased logarithm number servers figure shows mtbu rndseq fort rndseq lower mtbu ring compared ring random placement inferior random placement sets servers store copy chain higher probability chain lost due failures random placement makes additional opportunities parallel recovery servers figure shows mtbu rndpar servers rndpar performs rndseq increasing opportunity parallel recovery number servers improves mtbu eventually rndpar outperforms rndseq importantly outperforms ring related work scalability chain replication jimen ez-peris pati no-mart nez call rowaa read write approach report rowaa approaches provide superior scaling availability quorum techniques claiming availability rowaa approaches improves exponentially number replicas argue non-rowaa approaches replication necessarily inferior rowaa approaches exhibit quorum systems write-only applications rowaa choice replication real settings file services trade consistency performance scalability examples include bayou ficus coda sprite typically systems continued operation network partitions offering tools fix inconsistencies semi-automatically chain replication offer graceful handling partitioned operation trading supporting high performance scalability strong consistency large-scale peer-to-peer reliable file systems recent avenue inquiry oceanstore farsite past examples oceanstore strong fact transactional consistency guarantees google file system gfs large-scale cluster-based reliable file system intended applications similar motivating invention chain replication gfs concurrent overwrites serialized read operations synchronized write operations replicas left states content returned read operations vanish spontaneously gfs weak semantics imposes burden programmers applications gfs availability versus consistency vahdat explore trade-off consistency availability argue relaxed consistency models important stay close strong consistency availability maintained long run hand gray argue systems strong consistency unstable behavior scaled-up proposethe tentative update transaction circumventing scalability problems amza present one-copy serializable transaction protocol optimized replication chain replication updates replicas queries processed replicas store completed updates chain replication tail replica store completed updates protocol performs aswell replicationprotocolsthat provide weak consistency scales number replicas analysis behavior face failures replica placement previous work replica placement focussed achieving high throughput low latency supporting high availability acharya zdonik advocate locating replicas predictions future accesses basing predictions past accesses mariposa project set rules users create replicas move data query query data cache data consistency transactional consideration availability wolfson strategies optimize database replica placement order optimize performance oceanstore project considers replica placement cdn content distribution network akamai perspective creating replicas supporting quality service guarantees significant body work concerned placement web page replicas perspective reducing latency network load osdi symposium operating systems design implementation usenix association douceur wattenhofer investigatehow maximize worst-case availability files farsite spreading storage load evenly servers servers assumed varying availabilities algorithms repeatedly swap files machines improves file availability results theoretical nature simple scenarios unclear algorithms work realistic storage system concluding remarks chain replication supports high throughput query update requests high availability data objects strong consistency guarantees part storage services built chain replication exhibit transient outages clients distinguish outages lost messages transient outages chain replication introduces expose clients failure modes chain replication represents interesting balance failures hides clients failures doesn chain replication employed high availability data objects carefully selecting strategy placement volume replicas servers experiments adequateforthis purpose cision gain experience system osdi symposium operating systems design implementation usenix association mapping handles disk offsets stored stable state rldev state rldev consists checkpoint separate list incremental applied checkpoint list updated synchronously themappingchanges infrequently periodically apply list checkpoint truncate list simpleas longas rldev holding stable state primary chunk manager applies list incremental current checkpoint updated state masked components subjected realistic live workloads critically configurations modified transitioning validation live operation demonstrate approach evaluate efficacy implemented prototype validation framework modified applications cooperative web server three-tier auction service work framework prototype includes validation techniques trace-based replica-based validation trace-based validation involves periodically collecting traces live requests replaying trace validation replica-based validation involves designating masked component mirror live component requests live components duplicated mirrored masked component results fromthe maskedcomponentsare compared produced live component evaluate effectiveness validation approach running wide range experiments prototype microbenchmarks isolate performance overhead experiments human operators experiments mistake traces mistakeinjection experiments microbenchmarks find overhead validation acceptable cases experiments find prototype easy practice combination trace replica-based validation effectivein catching majority mistakes detailed traces operator mistakes show prototype detected mistakes observed operator experiments summary make main contributions present detailed data operator behavior large set live experiments realistic service traces experiments http vivo rutgers contribution important actual data operator mistakes internet services publicly due commercial privacy considerations analyze categorize reasons mistakes detail design implement prototype validation framework includes realistic validation environment dealing operator mistakes demonstrate benefits prototype extensive set experiments including experiments actual operators conclude operators make mistakes fairly simple tasks plenty detailed information service task conjecture mistakes result complex nature modern internet services fact large fraction mistakes problems interaction service components actual processing client requests suggesting realism derived hosting validation environment online system critical experiencewith prototype conclude validation real commercial services capable detecting types operator mistakes remainder paper organized section describes related work section describes operator experiments results section describes details validation approach prototype implementation compares approach offline testing undo context mistakes observed section presents results validation experiments finally section concludes paper related work papers addressed operator mistakes internet services work oppenheimer considered universe failures observed commercial services respect operators broadly categorized mistakes mistakes suggested avenues dealing extend work describing mistakes observed detail designing implementing prototype infrastructure detect majority mistakes brown patterson proposed undo rollback state recovering operator mistakes brown performed experiments exposed human operators implementation undo service hosted single node extend results complex service hosted cluster validation approach orthogonal undo hide operator actions live service validated realistic validation environment discuss undo section osdi symposium operating systems design implementation usenix association closely related technique offline testing validation approach takes offline testing step operating components validation environment extension live service catch larger number mistakes discuss section microvisorwork lowell isolates online maintenance parts single node keeping environments identical work validate interaction components hosted multiple nodes trace-based validation similar flavor fault diagnosis approaches maintain statistical models normal component behavior dynamically inspect service execution deviations behavior approaches typically focus data flow behavior systems components trace-based validation inspects actual responses coming components semantic levels replica-based validation compares responses directly correct live components replication-based validation tolerate byzantine failures malicious attacks context replicas permanent part distributedsystem andvalidationis constantly performedthrough voting contrast ourapproach focuses solely dealing operator mistakes require replicas validation normal service execution simpler intrusive orthogonal approaches dealing operator mistakes reducing operator intervention caches memory stores stably demonstrated disk dht-based truncates placement list strategies availability ready service scale requests increases chunk numbers manager depends servers rldev demonstrated recovery random placement rldevs volumes permit recoveredfirst availability transaction scale logging service number boxwood servers simple placement transaction strategy logging support concert clients parallel data provide logging recovery perform introduced redo gfs undo current operations prototype transactions intended logs primarily duplexed storing homogeneous lan rldev clusters prototype resilient single deployed failures heterogeneous universally wide-area accessible setting uniform machine random placement recovery volume service replicas longer make machine sense logging system replica supports placement group commits depend access patterns clients network option proximity selectively observed flushing host volatile reliability in-core protocols log disk re-order elements transaction chain commit transaction system crucial order provide control isolation load guarantees imbalances clients prototype explicitly chain replication locks implementation consists lock lines server java clients code deadlock avoidance lines lock java ordering code paxos library currentset chain replication protocols structured library makes studied ajmani eliminate operator intervention software clients system deadlock avoidance attractive alternative providing deadlock detection traditional transaction system b-tree module assume reader familiar b-tree variant -tree structure b-tree genuinekeys reside leaf nodes non-leaf nodes shadow keys acting purely index finding desired key leaf b-link tree -tree extra pointer node extra link points node tree level current node extra links make trivial enumerate keys b-link tree extra links leaf node importantly extra links make efficient concurrent operations give details main intuition operations recover unexpected situations extra links concurrent operations require locks proceed fewer locks b-link tree algorithms efficient concurrent operations introduced lehman yao significantly improved sagiv sagiv algorithms require locks lookups insertions require single lock held time insertions node splits levels tree lock protects single tree node updated deletions handled insertions hold single lock updated node operations provably deadlock free deletions leave node empty partially empty nodes fixed separate activity background compression threads compression thread lock nodes simultaneously b-link tree operation acid properties single tree involved acid properties multiple trees maintained clients transaction locking services earlier implement distributed version sagiv algorithm conceptually simple fashion global lock server simple locks chunk manager storage ordinary disks instance b-tree module maintains writeahead log rldev recoveringthe b-tree service replayingthe write-ahead log records log refer handles implemented chunk manager b-tree service recovered rldev service chunk manager b-tree server crash active log records records operations haven made disk protectedby leases leases expire lockservice initiate recoveryusing b-tree server write-ahead log recall section clients lock service designate recoveryfunction invokedon lease expiration lock service client failed recoveryis complete expiredleases madeavailable servers acquire b-tree servers dead case lock server call recovery machine leases expire defers recovery tree server registers point server logs replayed leases handed performance boxwood prototype experimental setup boxwood deployed lab cluster machines connected gigabit ethernet switch machine dell poweredge server single osdi symposium operating systems design implementationusenix association ghzxeonprocessor gbofram withanadaptec aicdual scsi adapter scsi drives rpm maxtor atlas drive system disk remaining rpmdrives seagatecheetah store data machine executes boxwood software runs user-level process windows server kernel failure detector observers paxos service lock service run separate processes rest layers libraries linked applications boxwood software written garbage-collected strongly typed language low-level routines written networking subsystem provided kernel capable transmitting data sec tcp request response protocol layered tcp rpc system deliver sec rldev performance machine run rldev server manages rldevs locally attached disks rldevs server primary secondary measurethe performanceusing simple benchmark program performs read write accesses size write read bytes xput util xput util cpu disk cpu disk table aggregate random write read throughput machine configuration diskqueuelength constrainedto write request involves local disk write remote disk write read request involves rpc call disk read remote disk table shows performance replicated random writes reads smallest machine configuration write accesses made benchmark rldevs primary server local write result local disk write rpc remote server update mirror read accesses made remote rldev server cpu utilization represents average cpu usage server constrain kernel disk access routines single outstanding request disk represents pessimistic performance throughput mbytes sec servers figure scaling writes rldev throughput mbytes sec servers figure scaling reads rldev figures show scaling number servers increased read slightly faster writes general disk inherently slightly read performance write manufacturer specification sheet small packet sizes throughputis limited disk latency large packet sizes performance close rpc system imposed limit cases observe good scaling chunk manager performance discuss performance chunk manager read write performance chunk manager repeated describe performanceimpact allocations deallocations tion defers work allocation time log allocation locally onthe client butavoid anrpc server wholeset allocation requests subsequently batch server table shows performance effect amount batching varied part allocating handle osdi symposium operating systems design implementation usenix association batch size amortized latency table effect batching allocations allocation single region zeroed disk latency amortized batch size storage allocated zeroed disk allocation costs vary chunk size show typical size single allocations batching latency cost high requirethree rldev writes stably recordthe mapping batching effective small numbers typically dozen allocations batched tests deallocations easier allocations cost single deallocation client milliseconds independentof number servers addition handle allocation request server deallocated avoid making operations server brings time b-tree performance reporton performanceof sets experiments b-tree experiment number machines inserting keys separate b-tree subsequent phase machine keys beforeeachofthethree phases entries b-tree caches evicted b-trees private lock contention evenly distributed machines throughput ops sec servers insert del inserts deletes lookups figure performance b-link tree operations machine performs operations separate tree starting cold cache figure reportson theaggregatesteadystate throughput machines phases observe good scaling experimental error surprising contentionfor locks rldev performanceis scale figures throughput ops sec servers insert del inserts deletes lookups figure performance b-link tree operations machines perform operations single shared tree starting cold cache study performance b-tree operations contention experiment redo previous experiment single b-tree machines contend avoid extreme case insertion contends global lock partition keys client acts disjoint region key space arrangement leads lock contention upperlevels 
tree againwe performthree phases cold caches figure shows aggregate throughput machines contention performanceof b-link operations flatten relative figure relative experiment size shared tree proportionately larger experiment leads lowered hit rates cache slows performance lookups case general performanceofb-trees dependenton parameters include branching factor distributionof keys andthe size ofthe keys anddata exhaustive characterization b-tree module parameter values trees measure -byte keys data tree node branching factor boxfs multi-node nfs server previous sections showed functional characteristics boxwood abstractions test abstractions built boxfs file system boxwood trees exportedusingthenfsv protocol itrunsatuserlevelanddirectlyorindirectlydependsonalltheservices osdi symposium operating systems design implementationusenix association earlier ing directories files symbolic links stored boxwood b-tree tree single well-known distinguished key hold attributes access times protection owner information similar unix inode addition directories files b-tree storing directory keyed thefile directory orsymboliclink thedatacorresponding key refers byte array on-wire representation nfs file handle nfs file handle interpreted client boxfs stores bookkeeping information including b-tree handle represents file directory symbolic link b-tree storing file keyed block number data key opaque chunk handle actual data file stored chunk uninterpreted bytes usage b-trees directories chunks file data consistent idea strict atomicity guarantees needed file system metadata file user data alternatively stored user data part file b-tree file write translate expensive b-tree insert operation attendant overheads redo undo logging b-tree storing symbolic link degenerate tree single special key byte array referring target link b-trees maintained boxwood b-tree operations single operation acid properties upgrades b-tree kiciman simultaneously automate accessed construction machines correctness file system constraints operations require checking multiple software b-tree operations configurations operator performed actions atomically mistakes rename operation requires section analyze combinationof maintenance delete diagnose-and-repair b-tree experiments insertion performed tree human atomic operators atomicity three-tier achieved auction service transaction service start describing section experimental setup explained earlier actual clients experiments provide isolation guarantees operators boxfs volunteered ensures isolation participate acquiring study locks file system detail objects experiment boxwood highlighting lock service mistakes made predefined lock order operators experimental locks setup acquired experimental performsthe testbed consists requiredoperation part online auction transaction service commits modeled releases ebay locks service b-tree organized operations intothreetiersofservers benefit web application b-tree cache anddatabase built tiers boxwood machines b-tree module tier contrast running file data apache web stored server b-trees version accessed machines chunk manager running interface tomcat requires servlet server separate cache version boxfs cache tier coherent acquiring tier machine shared running exclusive mysql lock relational database boxwood lock version service web data read write ghz intel celeron data access processor result ram b-tree operations database machine relies encapsulatedwithinasingletransaction ghz theseb-treeoperations pentium generate log entries ram committed machines run log linux kernel transactioncommits service requests transaction received system web servers group commit flow event single transaction tiers commit replies flow single disk write path usual reverse case direction eachweb log serverkeepstrack full ofthe actual requestsit b-tree sends nodes application modified servers transaction application server remain maintains soft b-tree state in-core cache client sessions make disk serving boxfs state perform consists metadata intensive auctions file interest operations efficiently clients boxfs makes dynamic requests simplifying assumptions belonging session 
processed application server restricting load balancing heartbeat-based membership protocol reconfigure service nodes unavailable added cluster client emulator exercise service workload consists number concurrent clients repeatedly open sessions service client issues request receives parses reply thinks andfollowsa link containedin reply userdefined markov model determines link follow experiments load imposed system requests approximately service maximum achievable throughput code service client emulator publicly dynaserver project rice important component experimental setup monitoring infrastructure includes shell records timestamps single command result executed operator infrastructure measures system throughput onthe-fly presenting operator visually assess impact actions system performance experiments operators experiments categorized scheduled maintenance tasks diagnose-and-repair tasks table summarizes classes experiments operator interact system provide conceptual information system architecture design interface convey information verbally graphical representation system consulted timeduringanexperiment wealsogivetheoperatortwo osdi symposium operating systems design implementationusenix association task category subcategory scheduled maintenance node addition data migration software upgrade diagnose-and-repair software misconfiguration application crash hang hardware fault table categories experiments sets written instructions general directions system interface specific instructions task operator operator allowed refer sets experiment total people varying skill levels volunteered act operators experiments graduate students operations staff members professional programmers students staff department staff members system administrator large clusters database administrator department programmers work system administrators work jeeves commercial search engine investigate distribution operator mistakes skill levels divide operators categories novice intermediate expert deem staff members professional programmers experts based experience system administration remaining programmers classified intermediate finally asked graduate students complete questionnaire designed assess operation experience based responses ended experts intermediates eleven novices section discuss breakdown mistakes operator categories gavethe noviceoperatorsa warmup task involving addition web serverto system give opportunity understand system configuration tier-to-tier communication issues crystallize information conveyed orally task provided detailed instructions study mistakes made warm task consideration sets instructions questionnaire operator behavior data collected http vivo rutgers maintenance task add application server experiment operator add tomcat server tier nutshell operator supposed copy tomcat binary distribution machine tier machine configure properly exchange information database tier addition correctly reconfigure restart web servers newly added tomcat server actuallyreceiveand process requests theexperiment set system resources handle load imposed client emulator tomcat server imply increase throughput experiment conducted novice operators intermediates experts average time run hour operators completely successful make configuration mistakes affect system ability service client requests upcalls storage service application experiments paper assumed null service simulated network library runs java socket library support variety storage servicelike applications donotbelievethese acknowledgements assumptions materially colleagues affect akon usability brug ard system kjetil jacobsen validity knut omang hypothesis fast easy brought build file problem systems attention higher-level discussion abstractions mark default linderman data cache sarah chung flushed onceevery helpful seconds revising earlier strictly version accordance paper nfs semantics grateful comments b-tree log osdi reviewers metadata shepherd operations margo seltzer flushedto stable grant storagewith research council periodicity norway file fast system asa metadata noted consistent acknowledged van lose renesse seconds work schneider machine supported crashes part finally afosr grant access times darpa files afrlifga grant directories date boxfs system views runs conclusions multiple contained machines simultaneously theauthors exports file interpreted system file necessarily representing system official consistent policies virtue endorsements distributed expressed lock service implied organizations exporting file government system notes nfs case protocol clients newval yields fully exploit semantics benefits update coherent simply caching locking file 
system write operation case newval objid amounts support atomic read-modify-write operations objects powerful semantics falls short supporting transactions request query update multiple objects indivisibly actual implementation store current object storing sequence updates produces current employ sequence updates representation simplifies task arguing strong consistency guarantees hold hist objid stores current objid entire history hist objid interpreted denote applying update object hist objid current state sequence updates precedesequal defined prior relation prefix relation placement strategy discussed gfs load balancing results approximately load servers simulations expect random placement good approximationof strategy osdi symposium operating systems design implementationusenix association unrealistically short mtbf selected facilitate running long-duration simulations acharya zdonik efficient scheme dynamic data replication technical report cs- brown september adya bolosky castro cermak chaiken douceur howell lorch theimer wattenhofer farsite federated reliable storage incompletely trusted environment proc ofthe symp operating systems design implementation boston december usenix alsberg day principle resilient sharing distributed resources proc int conf software engineering pages october amza cox zwaenepoel distributed versioning consistent replication scaling back-end databases dynamic content web sites proc middleware pages rio janeiro brazil june baker ousterhout availability sprite distributed file system operating systems review april appeared acm sigops european workshop fault tolerance support distributed systems chen katz kubiatowicz dynamic replica placement scalable content delivery proc int workshop peer-to-peer systems cambridge march dabek kaashoek karger morris stoica wide-area cooperative storage cfs proc acm symp operating systems principles banff canada october douceur wattenhofer competitive hill-climbing strategies replica placement distributed file system proc international symposium distributed computing lisbon portugal october douceur wattenhofer optimizing file availability secure serverless distributed file system proc symp reliable distributed systems ieee geels kubiatowicz replica management game proc european sigops workshop saint-emilion france september acm ghermawat gobioff leung google file system proc acm symp operating systems principles bolton landing october gray helland neil shasha dangers replication solution proc international conference management data sigmod pages acm june heidemann popek file system development stackable layers acm transactions computer systems february jimen ez-peris pati no-mart nez quorums alternative data replication acm transactions database systems september kistler satyanarayanann disconnected operation coda file system preliminary version acm transactions computer systems february lamport part-time parliament acm transactions computer systems petersen spreitzer terry theimer demers flexible update propagation weakly consistent replication proc acm symp operating systems principles pages saint-malo france october qiu padmanabhan voelker placement web server replicas proc infocom anchorage march ieee rowstron druschel storage management caching past large scale persistent peer-to-peer storage utility proc strictly hand operators made mistakes varying degrees severity paragraphs discuss mistakes apache misconfigured common mistake recognized flavors affecting system differently severe misconfiguration novice operators make needed modifications apache configuration file added information machine file forgot add machine line file specifies tomcat server names result tomcat correctly started machine client requests forwarded operators made mistake spend time apache log files make tomcat server processing requests analyzed wrong log files misconfiguration affect system performance immediately introduced latent error flavor apache misconfiguration subtle severe terms performance impact novice operator introduced syntax error editing apache configuration file caused module responsible forwarding requests tier mod crash outcome system inability forward requests tier operatornoticedthe problembylooking desperatelyatour performance monitoring tool find minutes point gave told problem mistake occurred expert novice modified configuration file left identical application server names made mod crash led severe throughput drop decrease mistake affected web servers web servers compromised operators made mistake correct problem minutes osdi symposium operating systems design implementation usenix association average period decided continue showed mistakes finally apache configuration file altogether reflect addition application server mistake resulted inability apache forward requests application server operator detect mistake fix problem minutes inthiscase oneintermediate operator reconfigured apache distribution thereweretwo apache distributions installed first-tier machines mistake made affected web server unable process client requests bringing web servers experiments reconfiguring apache account thenewapplicationserver twonoviceand threeintermediateoperatorsunnecessarilyshutdownbothweb servers time consequence made service unavailable tomcat incorrectly started novice expert unable start tomcat correctly forgot obtain root privileges starting tomcat expert operator started tomcat multiple times killing processes remaining previous launches scenario led tomcat silently dying make matters worse heartbeat service separate process running web servers continued forwarding requests machine unable process result substantially degraded throughput operators corrected mistake minutes average maintenance task upgrade database machine purpose experiment migrate mysql database slow machine powerful equipped memory faster disk faster cpu note fast machine experiments database machine bottleneck testbed system saturated slow machine expected outcome experiment higher service throughput experiment involves steps compile install mysql machine bring service dump database tables mysql installation copy machine configure mysql properly modifying cnf file initialize mysql create empty database import dumped files empty database modify relevant configuration files inall application serversso thattomcat forward requests database machine start mysql application servers web servers novices intermediates experts performed task average time run hours minutes detail mistakes observed password set mysql root user acm symp operating systems principles banff canada october saltzer reed clark end-to-end arguments boxfs fine-grained file system metadata stored key-data pairs b-tree metadata modifications made machines automatically locked consistency b-tree module global lock server addition boxfs protects individual file blocks taking locks block machines writing blocks file contend metadata lock update file attributes individual block reduces lock contention cache coherence traffic due false sharing boxwood abstractions enabled file system code base small actual file system code lines code verbose language boxfs code implementing simple lru buffercache additional lines largely reuse code b-tree cache module size code compares favorably mark shand classic user-level nfs daemon lines read-only access unix file hierarchy addition wrote code support nfs rpc udp tcp xdr accounted additional lines lines interface boxfs native windows networking libraries osdi symposium operating systems design implementation usenix association boxfs performance table shows connectathon performance benchmarks http connectathon nfstests html boxfs comparison show performance stock nfs server running hardware run boxfs single machine cluster rldevs locally attached disks replication enabled stock nfs server windows services forunix sfu nfs serverthat runs in-kernel ntfs local file system local disks client linux nfs client cases mount file system nfs rpcs made udp send receive sizes maximum supported kernel description time secs boxfs nfs create files dirs remove files dirs getwd stat calls chmods stats write file times read file times reads dir entries files unlinks renames links files symlinks readlinks files statfs calls table connectathon benchmarks boxfs writes file data asynchronously test reads file repeatedlyis anomalybecause reads hit cache ontheclient mance in-core metadata log flushed disk transaction commit periodically show performance boxfs metadata log lazily flushed disk general performance boxfs comparable native nfs server top local ntfs file system slight edge meta-data intensive operations attribute usage b-trees depending synchronously update log test shows marked difference versus seconds file data updated asynchronously cases file write requires metadata update involves log write report results running postmark metadata intensive benchmark benchmark models expected file system work load imposed electronic mail web based commerce netnews creates initial set files varying sizes parameter num initial files num transactions file sizes create delete ratio read append ratio read write block size table postmark parameters unix buffered default random number seed metric boxfs async sync file system design acm transactions computer systems november schneider byzantine generals action implementing fail-stop processors acm transactions computer systems schneider implementing fault-tolerant services state machine approach tutorial acm computing surveys december stonebraker aoki devine litwin olson mariposa architecture distributed data proc int conf data engineering houston wolfson jajodia huang adaptive data replication algorithm acm transactions computer systems june vahdat cost limits availability replicated services proc acm symp operating systems principles banff canada october osdi symposium operating systems design implementation usenix association 
creations sec file read sec file append sec file deletions sec data read sec data write sec table postmark results gain substantial performance flushing meta data log disk periodically significant change semantics file system measures performance file system creating small files measures performance predetermined number transactions transaction creationor deletionof file followedby read append file ran benchmark thesettings shownin table ourresults areshown table benchmark emphasizes metadata intensive operations boxfs performs multi-node boxfs scaling boxfs servers readfile writefile mkdirent figure performance boxfs sharing experiments mkdirent latency elapsed time insert files benchmarks multi-node file systems boxfs difficult obtain show scaling characteristics boxfs simple experiments figure experiments run boxwood abstractions machines cluster export single sharednfs volumefromoneor moreofthese machines osdi symposium operating systems design implementationusenix association nature nfs mounting single volume appears separate file systems client experiment readfile figure single nfs file read multiple mount points cold client cache cold server cache wouldexpect linearly number servers increased contention secondexperiment mkdirent figure create files unique names root directory nfs volume results nfs create rpc requests boxfs machine turn modifies b-tree single shared directory potential sources conflict traffic inserting shared b-tree result locking data transfer b-tree module boxfs acquirea 
lockto ensurethat metadata update b-tree insertion consistently experiments figure show performin excess insertions b-tree performance bottleneck traffic metadata date comparison performancewhen sharing single machinecase final experiment writefile figure write data shared file non-overlappingoffsets write results nfs write rpc boxfs machine boxfs implements fine-grained locking file block level coherence traffic lock contention determines scaling limited required file metadata date comparison performance single machine contention case sec related work early seeds work present scalable distributed data structures sddss litwin litwin sddss offer algorithms interacting scalable fashion family data structures focus boxwood significantly concerned systems issues reliability fault-tolerancein general abstractions issues largely litwin work scalable distributed data structure approach gribble previous work similar boxwood view work complementary implementation data structures hash tables versus b-trees offers tradeoffs failure models lead verydifferentsystem design techniques gribble designed system assuming sotransient failures entire cluster extremely rare rely data place ram disk multiple machines design recoveryprotocols production systems berkeleydb datablade related approach sources thesesystems offermuch morefunctionalitythan moment distributed petal related approach scalable storage petal similar durability consistency guarantees boxwood presents applications single sparse address space blocks applications coherently manage space implement logging recovery mechanisms data structures employ basic services petal similar spirit similarly structure b-link tree service reminiscent frangipani distributed databases approach scalably storing information view system lower-level infrastructure hope database designers plan deliberately avoided mechanisms felt unnecessary lower levels storage architecture query parsing deadlock detection full-fledged transactions distributed file systems enable storage scale respects view layered facilities boxwood boxwood layering file system top b-trees related olson inversionfile system layered top full-fledged postgres database inversion file system capable answering queries file system implementation file system layered simpler abstraction things equal expect performanceincrease expense reduced querying capability recent work semantically smart disks related boxwood boxwood semantically smart disksstriveforbetterfile ing usage patterns semantics lower layers storage system unlike boxwood semantically smart disks present conventional disk interface scsi higherlevels buttrytoencapsulate orinfer performance projects including farsite oceanstore rapidly-growing literature distributed hash tables attempted provide scalable storage wide area network address set trade-offs boxwood wide-area solutions deal osdi symposium operating systems design implementation usenix association untrusted participants frequent reconfiguration high network latency variable bandwidth solutions advantageof benign conditions boxwood designed projects local area clusters provide scalable services persistent distributed objects typically rely file system database managetheirstorage orbuildanapplicationspecific storage novice operator failed assign password mysql root user mysql configuration mistake led severe security vulnerability allowing virtually execute operation database mysql user privileges part database migration operators ensure application servers connect database issue requests involves reconfiguring tomcat forward requests database machine granting proper privileges mysql user tomcat connect database novice expert grant privileges preventing application servers establishing connections database result tomcat threads eventually blocked system unavailable expert managed detect correct problem minutes novice identify problem apache incorrectly restarted intermediate operator launched apache wrong distribution restarting service mistakecaused service completely unavailable operator minutes detect fix mistake database installed wrong disk powerful machine disks rpm scsi disk rpm ide disk database machine bottleneck system database migration needed service load imposed emulated clients operators havehesitated install mysql faster scsi disk novice operator installed database slow disk limiting throughput achieved service operator realized mistake maintenance task upgrade web server experiment operators required upgrade apache version version machine nutshell involves downloading apache source code web compiling configuring properly integrating service intermediate expert operators participated maintenance experiment average time run hours describe observed mistakes osdi symposium operating systems design implementationusenix association apache misconfigured spawning web server processes apache version service offer easily-utilized atomically-updatingdata structures network attached secure disks nasd related boxwood provide storage abstraction higher level raw blocks choiceof b-link tree motivatedby observation suited distributed implementation similar observation made independently johnson colbrook proposed b-link tree variants called deand db-trees shared memory multiprocessors scheme explicitly replicates internal nodes specific processors stores contiguous sets keys processor implementation dynamic algorithmis simpler cost distributed lock service conclusions initial experience scalable data abstractionsas fundamental low-levelstorage primitives attractive appears difficult settle single universal abstraction fit combination abstractions services offer sound substrate top multiple abstractions readily built ouruse thechunkmanageras generalizedstorage allocator obviating address space management system widely applicable case enabled distribute complicated data structure modest programmingeffort strategy isolating paxos module small part system worked practice continue scale rest system dynamically hindrance design boxfs support claim buildingscalable applicationsusing boxwoodinfrastructure easy subjective question feeling positive boxfs distributed file system good resilience scalability require complicated locking logging recovery schemes weneedfurther performance analysis make objective comparisons acknowledgements current colleagues andrew birrell mike burrows ulfar erlingsson jim gray lee roy levin mike schroeder interesting discussions related ideas paper ahmed talat roopesh battepati ahmed mohamed helped understand windows networking sfu nfs qin feng zhou interns successive summers helped understandseveral subtle issues design anonymous reviewers shepherd jason flinn comments paper adya bolosky castro cermak chaiken douceur howell lorch theimer wattenhofer farsite federated reliable automatically storage invokes syntax incompletely checker trusted environment analyzes main proc symp configuration operating file systems make design server implementation osdi started pages december thisfeatureis extremely anderson dahlin avoid exposing neefe operator mistakes patterson live roselli system wang serverless experiments network syntax file checker systems caught proc symp configuration operating mistakes systems involving principles sosp directives longer pages valid december newer apache bayer version mccreight organization checker solely maintenance concerned large syntax ordered indexes acta catch configuration mistakes inf cardoza expert launched apache glover snaman main design configuration file trucluster apache multicomputer system map digital url unix request environment 
tomcat servlet misconfiguration led inabilityof upgraded apache forward requests tier causing degraded throughput operator fixed problem minutes investigation addition latent error resulted misconfigurations ured apache server html files apache directory tree similar mistake made respect location heartbeat service program latent error activated removed files belonging distribution mistake occurred expert correctly realized heartbeat program executed apache directory tree incorrectlyspecifiedthepathfortheprogram inoursetup heartbeat program launched apache wrong path mod crashed apache started made server unable process requests dynamic content resulting throughput degradation operator fix problem minutes diagnose-and-repair task webserver misconfigurationand crash observe operator behavior resulting latent errors activated performed experiments apache server misconfiguration crash injected system sequence events mimics accidental misconfiguration corruption configuration file restart server detail system starts operating wemodify configuration file pertaining mod apache servers restart server segmentation fault crash server assoonastheserverisabnormally terminated throughput decreases half prior operators task diagnose fix problem normal throughput regained experiment presented novices intermediates experts average time run hour minutes operators understand system malfunctioning fix inabout hourand minutes alloperators eventhe successful made mistakes aggravated problem mistakes caused misdiagnosing source service malfunction misdiagnosis due misdiagnosis operators categories unnecessarily modified configuration files system case caused throughput drop apparent reason behavior fact operators tempted literally interpret error messages appearing log files reasoning real problem words reading apache busy increase maxclients parameter log file operators performed suggested action reasoning noticed mistake starting wrong apache distribution previously discussed made novice intermediate severely degrading throughput making drop couple operators suggested replacement hardwarecomponents result incorrectly diagnosing problem disk memory fault diagnose-and-repair task application server hang experiment inject kind fault force tomcat hang second-tier machines system working perfectly perturb conducted experiment novices intermediate andoneexpertoperator alloperatorswere detect fault fix problem hour minutes average noticed mistakes discussed tomcat incorrectly restarted novice operator restarted working servlet servers root privileges causing crash caused service lose servlet server remaining overloaded operator detect crashed tomcat server minutes database unnecessarily restarted diagnose problem novice operator unnecessarily restarted database server database machine replicated bringing results system inability process requests mysql denied write privileges intermediate operator diagnose problem decided verify mysql files operator inosdi symposium operating systems design implementation usenix association figure operator mistakes impact figure operator mistakes operator category advertently write-protected mysql directory tree leading mysql inability write tables mistake effect alreadybeenopenedby database server mistake led latent error arise reason database restarted diagnose-and-repair task disk fault database machine experiment mendosus fault-injection network-emulation tool force disk timeout occur periodically database machine timeouts injected exponential inter-arrival distribution average rate occurrences database bottleneck disk timeouts substantially decrease service throughput experts participated experiment fouroperators wereunableto discovertheproblem interacting system hours average correctlydiagnosed fault minutes interaction service unsuccessful operators made mistakes caused misdiagnosing real root problem misdiagnosis operators ended diagnosing fault intermittent network problem tiers operators reached conclusion observed incorrect diagnoses part dos attack tomcat misconfiguration lack communication tiers operator suspicious mysql misconfiguration adjust parameters database subsequently restarted influence error messages reported log files operator changed application server machines portwhich tomcat toreceiverequests apache result affected application servers unreachable operators looked main kernel log database machine messages logged scsi driver reporting disk malfunction messages suspect disk misbehaving summary figures summarize findings x-axis figure effects operator mistakes stacked bars show number occurrencesof legend shows number mistakes mistake category figure incomplete component integration refers scenarios added component components wrong choice component refers installing database slow disk unnecessary replacements refers misdiagnosing service malfunction hardware problem digital technical journal chandra hadzilacos toueg weakest failure detector solving consensus journal acm jacm july devarakonda kish mohindra recovery calypso file system acm transactions computer systems august ferreira shapiro blondel fambon garcia kloosterman richer robert sandalky coulouris dollimore guedes hagimont krakowiak perdis design implementation persistent distributed store recent advances distributed systems volume lecture notes computer science chapter pages springerverlag february fox gribble chawathe brewer gauthier cluster-based scalable network services proc symp operating systems principles sosp pages october lund merchant saito spence veitch fab building reliable enterprise storage systems cheap proc int conf architectural support programming languages operating systems asplos october osdi symposium operating systems design implementationusenix association ghemawat gobioff leung google file system proc symp operating systems principles sosp pages december gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective high-bandwidth storage architecture proc int conf architectural support programming languages operating systems asplos pages october gray cheriton leases efficient faulttolerant mechanism distributed file cache consistency proc symp operating systems principles sosp pages december gribble brewer hellerstein culler scalable distributed data structures internet service construction proc symp operating systems design implementation osdi pages october hsaio dewitt chained declustering availability strategy multiprocessor database machines proc international conference data engineering pages february sleepycat software berkeley riders publishing johnson colbrook distributed data-balanced dictionary based b-link tree technical report mit lcs trmit jonge kaashoek hsieh logical disk approach improving file systems proc symp operating systems principles sosp pages december katcher postmark file system benchmark technical report netapp kubiatowicz bindel chen czerwinski eaton geels gummadi rhea weatherspoon weimer wells zhao oceanstore architecture global-scale persistent storage proc int conf architectural support programming languages operating systems asplos unnecessary restart component refers restarts database server observed mistakes cases single operator made mistake experiment asweindicatedbefore frequent mistake experiments figure distinguish local global misconfiguration mistakes global misconfiguration refers inconsistencies configuration files compromising communication system components local misconfiguration refers misconfigurations affect component system local misconfiguration configuration mistake caused tomcat crash led security vulnerability potentially prevent apache servicing requests global misconfigurations involve mistakes prevented web servers forwarding requests application server machine sending requests newly added application server prevented application servers establishing osdi symposium operating systems design implementationusenix association connections database server prevented web servers processing requests led web servers forward requests nonexisting application server figure show distribution mistakes operator categories operator part types experiments normalized number mistakes dividing total number experiments operator category participated figure illustrates experts made mistakes misconfigurations significant fraction experiments reason counter-intuitive result hardest experiments performed experts experiments susceptible local global misconfiguration discuss detail validation approach catch majority mistakes mistakes observed remaining mistakes including unnecessary software restarts unnecessary hardware replacements made expert mistakes intermediate mistakes novice mistakes operators validation expert operators make mistakes propose operator actions validated effects exposed end users specifically build validation infrastructure components validated slice online system tested aseparate offline environment start section overview proposed validation approach describe prototype implementation experience modifying three-tier auction service include validation implemented validation framework press clustered web server press interesting counter-point multithreaded auction service single-tier event-based server earlier technical report discusses implementation finally close section discussion operators make mistakes validation comparison validation offline testing undo overview validation environment closely tied online system reasons avoid latent errors escape detection validation activated online system differences validation online environments load components validation realistic workload enable operators bring validated components online change components configurations minimizing chance operator mistakes hand components validation call masked components simplicity isolated live service incorrect behaviors service failures meet goals divide cluster hosting service logical slices online slice hosts live service validation slice components validated integrated live service figure shows validation architecture context three-tier auction service protect integrity live service completely separating slices reduce validation slice offline testing system erect isolation barrier slices introduce set connecting shunts shunts one-way portals duplicate requests replies inputs outputs passing interfaces components live service shunts log requests replies forward validation slice shunts easily implemented open proprietary software long components interfaces well-defined build validation harness consisting proxy components form virtual service masked components shown dashed box figure virtual service duplication requests replies shunts operators validate masked components realistic workloads inparticular previously recorded logs accepts forwarded duplicates live requests responses shunts feeds requests masked components verifies outputs masked components meet validation criteria proxies implemented modifying open source components wrapping code proprietary software well-defined interfaces finally validation harness set comparator functions test correctness masked 
components functions compute set observations validation service match set criteria figure comparator function determine streams requests replies pair connections labeled labeled similar declare masked web server working correctly comparison fails error signaled validation pages november lamport thepart-time parliament acmtransactions computer systems lee thekkath petal distributed virtual disks proc int conf architectural support programming languages operating systems asplos pages october lehman yao efficient locking concurrent operations b-trees acm trans database systems lindsay retrospective proc ieee litwin linear hashing tool file table addressing proc int conf large data bases vldb pages october litwin neimat schneider familyoforder preserving proc int conf large data bases vldb pages september litwin neimat schneider scalable distributed data structure acmtrans database systems olson design implementation 
inversion file system proc usenix winter technical conference pages january sagiv concurrent operations onb -treeswithovertaking journal computer system sciences saito bershad levy manageability availability performance porcupine highly scalable cluster-based mail service proc symp operating systems principles sosp pages december shand unfsd user-level nfs server comp sources unix sivathanu prabhakaran popovici denehy arpaci-dusseau arpacidusseau semantically-smart disk systems usenix conference file storage technologies fast pages march thekkath mann lee frangipani scalable distributed file system proc symp operating systems principles sosp pages october wedekind selection access paths data base system klimbie koffeman editors data base management pages northholland amsterdam zhao huang stribling rhea joseph kubiatowicz tapestry resilient global-scale overlay service deployment ieee journal selected areas communications jsac osdi symposium operating systems design implementation usenix association 
fails threshold period time comparisons match component considered validated infrastructure approach conceptually simple operator places components worked validation environment effectively masking live service operosdi symposium operating systems design implementation usenix association replies requests http odbc replies requests validation slice proxy components load validate web server requests replies web server validated web server web server server application server application requests replies logged database production web server online slice web server client application server proxy proxy shunt shunt figure three-tier auction service validation case single component web server validated inside validation slice validation harness client proxies load web server application server proxies field requests dynamic content web server ator acts masked components live service operator instructs validation harness surround masked components virtual service load components check correctness masked components pass validation operator calls migration function fully integrates components live service slice isolation critical challenge building validation infrastructure isolate slices validated components migrated live service requiring internal state configuration parameters current implementation achieves isolation transparent migration granularity entire node running nodes virtual network created mendosus virtual network fairly easy impose needed isolation mendosus designed inject network faults partition network simply instruct mendosus partition system parts isolate slices mendosus runs node enforcing network partition drops packets including multicast packets partition enforced partition means nodes validation slice online slice vice-versa tunnel barrier shunts forward information special nodes privilege bypass network partition virtual network node migrated slices requiring node network configurations long software components comprising service dynamically discover automatically adjust service configuration include components running slice characteristic production-levelservices transparent network-levelmigration critical detecting global misconfiguration mistakes section migrating live components modifying internal state difficult detail accomplish migration validation strategies inherently difficult problem validation drive masked components realistic workloads validate correctness validation single component approach create demanding workload stress-test component approach lends itselfto trace-based techniques requests replies passing shunts equivalent live component logged replayed replay logged replies compared replies produced masked component approach replica-based current offered load live service requests passing shunts equivalentlivecomponent duplicated forwarded real-time validation harness drive masked component shunts capture replies generated live component forward harness compares replies coming masked component core differences approaches assumptions request stream connections components logged osdi symposium operating systems design implementationusenix association synthetic request streams exercise difficult cases preferable light live load hand replica-based validation service data sets changed sufficiently previously collected traces longer applicable reality strategies mutually exclusive validation apply approaches concluding masked component working properly supporting infrastructure implement approaches data collected shunts saved disk forwarded directly validation slice building approaches system desirable long sufficient storage bandwidth trace collection network bandwidth live forwarding explore resource requirements approaches section state management important issue related validation strategy component state management work component state application-defined externalizable data hard soft accrued service execution affect subsequent responses specifically validation system faces state management issues initialize masked component internal state handle validation workload correctly migrate validated component online slice migrating state accumulated validation valid live service initialize masked component depends validation strategy trace-based validation masked component initialized state recorded trace record state trace capture halt component traced temporarily buffering incoming requests shunts allowing current requests complete replica-based validation masked component initialized copy state live component make copy trace-based validation state forwarded masked component saved disk general strategy applicable component supports checkpointing state including proprietary systems care avoid capturing state overload conditions sufficient buffering exist enable momentary halting component impossible capture large amount state database online case component removed active service state captured validation operator move component holding soft state online slice restarting instructing mendosus migrate online slice join service fresh instance component type components hard state migration online slice forms application integrate component type service involve data redistribution component migrated state similar components soft state operator restart component state migrating multi-component validation discussion assumes validation single component simplicity general validate interaction multiple components address global configuration mistakes section adding application server auction service web servers list application servers expand include server list updated properly requests forwarded server migration introducing latent error ensure connection configured correctly validate existing web servers application server concurrently rently handle suppose operator introduce component requires configurations existing live components adding application server operator introduce component validation slice validate correctness discussed component live service configuration modified interact properly component brought validation slice one-by-one component pair validated ensure correct interoperability validation existing component migrated back online slice component migrated online slice checking interactions existing component configurations changed note components validation point time multicomponent validation approach general multicomponent validation extended include additional components found implementing validation setting validation infrastructure involves modifying online slice shunt requests responses setting harness composed proxies validation slice defining comparators checking correctness implementing mechanisms correct migration components osdi symposium operating systems design implementation usenix association slices section discuss issues comment implementation effort respect auction service shunts implemented shunts component types three-tier auction service implementation straightforward defined middleware libraries figure shows client requests responses intercepted module apache side current implementation intercept requests static content represent small percentage requests requests responses database open database connectivity odbc protocol intercepted tomcat mysql driver sql queries responses addition duplicating requests replies tag request reply pair unique identifier validation harness identify matching requests responses generated masked component logged forwarded requests responses live system comparator functions applied validation harness validation harness implement service masked components order exercise check functionalities validate application server auction service validation harness provide web server database server masked application server connect approach building service surrounding masked component complete set real opposed proxy components reminiscent offline testing complete model live system built approach straightforward disadvantages devote sufficient resources host entire service validation slice checkpoint service state content database session state beginning validation process finally fit comparators real components address limitations built lighter weight component proxies interact masked component requiring full service proxies send requests masked component check replies coming services communicating components connected common connection mechanism event queues straightforward realize entire 
virtual service collectionofsuchqueues ina singleproxy forheterogeneous systems auction service tiers connect variety communication mechanisms built proxies representing connection protocols common core auction service required proxies client web server application server database proxies proxy typically implements modules amembershipprotocol aserviceinterface messaging core themembership protocol guarantee dynamic discovery nodes validation slice service interface correct communication interacting components common messaging core takes shunted logged requests load masked components responds requests made masked components state management focus solely soft state stored main memory application servers theauctions ofinterest users handle state extend application servers implement explicit state checkpointing initialization api api invoked proxies initialize state masked application server equivalent live component experience indicatesthat theeffort involvedin implementing proxies small core components easily adaptable services messaging core iscommon acrossall proxies proxies modifying non-comment source lines ncsl rice client emulator apache web server tomcat application server ncsl application server includes code implement state management api mysql database proxy written scratch required ncsl comparator functions current set comparator functions includes throughput-based function flow-based function set component-level data matching functions throughput-based function validates average throughput masked component threshold expected flow-based function ensures requests replies flow inter-component connections expected active finally data matching functions match actual contents requests replies due space limitations type comparator function paper describe handle cases exact matches non-determinism non-determinism non-determinism forms timing responses routing requests actual content responses found timing content non-determinism significant problems applications studied osdi symposium operating systems design implementationusenix association hand data components replicated found significant non-determinism routing requests auction service web server forward request session application servers press routing nondeterminism lead local memory access sending request remote node local disk proxies detect behaviors equivalent possibly generate replies fortunately implementing detection generation simple services validation scenario pieces fit operator upgrade operating system web server node auction service figure operator instruct mendosus remove node online slice effectively taking offline upgrade upgrade node tested offline boots starts web server correctly operator instruct mendosus migrate node validation slice validation harness automatically start application server proxy web server connect components started discover multicast-based discoveryprotocol interconnect form virtual service harness start client proxy load virtual service trace-based validation harness replay trace client proxy generating logged requests accepting replies shown figure application server proxy accepting requests dynamic content web server generating replies logged data shown harness compare messages web server application server client proxies logged data trace completes encountering comparison mismatch harness declare web servernode validated finally operator placethenodeback intotheliveserviceby restarting totheonline slice changing configurations discussion describedvalidationin detail wenowdiscuss generality limitations prototype remaining open issues compare approach offline testing undo generality implementations context systems auction service press server infrastructure general reusable auctionservice isimplemented widely usedservers apache tomcat mysql proxies shunts implemented auction service directly reusable service built servers services built components shunts reusable implemented based standard communication libraries mentioned implementing proxies requires effort core logging forwarding replay infrastructure finally experience press suggests event-based servers amenable validation approach interactions components pass common queuing framework application-specific parts validation approach comparator functions state management api generic comparator functions check characteristics throughput reusable comparator functions depend semantics applications unavoidably application-specific tailored specific application state management api provided off-the-shelf stateful servers servers provide api implemented scratch tomcat limitations behavior component validated correct instance trace sbehavior content served web server correctly leads responses client requests validation applicable validation harness check problems missing content unexpected structure addition instance validated manner point validating additional instances component type approach introduces scope mistakes view unavoidable bootstrapping issue action lead mistakes restart components live service validation step typically ensure validated components join live service proper state potential mistakes minimized scripting restart open issues leave questions shouldbevalidated thedegreeof validation andfor long open issues terms trace-based validation issues traces gathered gathered long issues boil policy decisions involve trade-offs probabilosdi symposium operating systems design implementation usenix association ity catching mistakes cost resources validation online live operator experiments validation leave decisions discretion operator future plan address issues detail interesting direction study strategies dynamically determining long validation based intensity offered load periods heavy load validation retain resources productively live service plan explore richer space comparator functions weaker forms comparison statistical sampling greatly improve performance retaining benefit validation comparison offline testing sites offline testing years offline testing environment typically resembles live service closely allowing operators act components exposing mistakes intermediate states users components working correctly moved live service critical difference validation approach offline testing fact validation environment extension replica live service misconfiguration mistakes occur offline testing software hardware configurations changed moving components live service adding application server requires modifying configuration file web servers misconfigured web server offline environment detected offline testing failing correctly modify live configuration file result error mistakes made actions exposed end users order gauge ability offline testing catch mistakes observed section assume trivial mistakes involve intercomponent configuration repeated live system assumption offline testing haveallowedtheoperator tocatch instances start wrong software version category instance local misconfiguration caused database security vulnerability assuming operators explicitly test case instances global misconfiguration incorrect port tomcat configuration file find offline testing deal mistakes observed mistakes validation approach caught comparison undo undo essentially orthogonal validation approach undo focuses enabling operators fix mistakes bringing service back correct state focus validation hide operator actions live service validated validation environment technique benefit undo benefit validation avoid exposing operator mistakes live service clients validation benefit undo correct operator mistakes validation environment assuming undo mistakes observed immediately exposed clients explicit error reply degraded server performance caused latent errors vulnerabilities undo restoring system consistent state afteramaliciousattackresultingfromthedatabasesecurity vulnerability problem combined offline testing undo havehelpedfix themistakes detected offline experimental validation results section describe performance impact validation infrastructure live service micro-benchmarks concretely evaluate validation approach methods shunting overheads measured shunting overhead terms cpu usage disk network bandwidth interception logging forwarding inter-component interactions auction service note comparator functions run online slice amount information log forward depends nature comparator functions validation investigate performance impact levels data collection including collecting complete requests replies collecting summaries sampling figure shows percentage cpu utilization live web server auction service function offered load utilization load curve unmodified server labeled base curves complete logging trace-based validation forwarding replica-based validation labeled 
trace-val replica-val curves show performance shunting requests replies web application servers observe complete logging additional cpu utilization throughput range forwarding adds straightforward approach reducing overheads summary responses trace-summary-val replica-summary-val curves give osdi symposium operating systems design implementationusenix association cpu requests sec cpu utilization base trace-val replica-val trace-summary-val replica-summary-val trace-sample-val replica-sample-val trace-data-flow-val replica-data-flow-val figure processor overhead incurred performing validation operations web server utilization logging forwarding bytes http responses additional cpu utilizations case logging forwarding approach reducing overheads sample collecting requests responses measure impact approach programmed shunts log forward client sessions leading trace-sample-val replicasample-val curves optimization carefully implemented avoid skewing states compared components replica-based validation optimization reduces overheads logging forwarding sampling approach shunt compare final input outputs ignoring internal messages trace-dataflow-val replica-data-flow-val versions sample http requests responses ignore messages logging forwarding examined impact shunting disk network bandwidth find worst case bandwidth load generated methodreduced bandwidth load final-resultonly sampling method reduced bandwidth results encouraging show validation impact throughput gigabit networks storage systems find cpu overheads significant base prototype cases increasing utilization additional network disk traffic significant optimizations reduce cpu overheads results positive approach loads live components simultaneously validation services run fairly low cpu utilization deal load spikes overhead affect throughputs practice buffering overheads state checkpointing initialization performed shunts proxies involved validation stateful server make operations atomic draining requests processed components involved validation requests complete start required state operations draining processing state operations buffer requests arriving affected components long buffer requests determines delay imposed fraction requests buffer space overheads delays space overheads vary depending size state maximum duration outstanding request find tolerable validation application server auction service find replica-based validation longest buffering duration duration shorter translating required buffer capacity requests heavily loaded replica server average state size small bytes auction service synthetically increased size session object kbytes study impact large states resulted response time seconds insignificant manageable validation slice operator mistake experiments experimentation techniques test efficacy validation techniques experiments span range realism repeatability live-operator experiments realistic repeatable repeatable experiments operator emulation mistake injection experiments set identical section added nodes implement validation infrastructure live-operator experiments experiments operator instructed perform task steps component touched operator identified offline required actions performed operator validation slice validate component operator allowed choosethe duration validation run finally operator migrate component online slice optionally operator osdi symposium operating systems design implementation usenix association place component online validation confident component working correctly threeapplication server addition tasks section server upgrade tasks section web server misconfiguration section application server hang section graduate students department acted operators run experiment validation observeda totalof operatormistakes infiveof experiments validation successful catching mistakes caught validation latent errors mistake caught implementation test thestaticfilesservedby thewebservers alreadymentioned section addressing mistake requires extension prototype process requests static files responses interestingly run web server upgrade task operator started apache modifying main configuration file default validation caught mistake prevented unconfigured apache exposure operator configure upgraded apache minutes number unsuccessful validations gaveup shows important area future research extending validation infrastructure operator easily find unsuccessful validation operator-emulation experiments experiments command trace previous run operator task replayed shell scripts emulate operator actions motivation approach collection reuse operator actions repeatable testbed techniques deal operator mistakes approach limitation operator mistake caught subsequent recovery actions scripts undefined find ability repeat experiments extremely traces derived manually logs collected operator experiments section emulation scripts emulation step consists combination summary steps actual run goal preserving operator actions impact system operator performed set read-only diagnostic steps subsequently modified file trace script perform file modification derived total scripts operator mistakes observed mistakes reproducible due infrastructure limitations table summarizes findings terms coverage mistakes caught respect mistakes validation catch coverage impact technique latent total total total trace-based replica-based multi-component table coverage results emulation experiments reproducible mistakes mistakes impact caused latent errors trace replica-based validation caught mistakes causing impact singlecomponentvalidationfailedtocatchthelatenterrorsdur- ing addition application server mistakes resulted web servers updated correctly include application server mistakes caught multi-component validation approach section validation caught non-reproducible mistakes mistakes impact similar number reproducible mistakes caught single-component validation assuming mistakes caught validation ofthe mistakes observed mistake-injection experiments hand-picked additional mistakes injected test effectiveness validation system goal covermistakesthatwerenot observed live-operator experiments emulate mistakes content management extended mendosus inject permission errors missing files file-corruption errors press injection permission missing files errors readily detected validation infrastructure file corruption errors caught thresholds thecomparatorfunctions web page allowed slack comparator preventexcessivefalse positives case illustrates comparator functions carefully designed balance false positive rate exposing mistakes mendosus perform manipulations configuration parameters impacted performance component specifically altered inmemory cache size press maximum number clients apache auction service mistakes resulted component performance dropping threshold throughput comparator caught validation experiments highlight importance designing suitable comparators workloads osdi symposium operating systems design implementationusenix association conclusions paper collected analyzed extensive data operator actions mistakes total experiments human operators three-tier auction service found operator mistakes common software misconfiguration mistakes incorrect software restarts mistakes large number mistakes immediately degraded service throughput based results proposed services validate operator actions virtual environment made visible rest system users designed implemented prototype validation system evaluation showed prototype imposes acceptable performance overhead validation results showed prototype detect operator mistakes observed acknowledgments volunteer operators jeeves programmers dcs lcsr staff members donated considerable time effort christine hung neeraj krishnan brian russell building part infrastructure operator experiments shepherd margo seltzer extensive comments support work ajmani liskov shrira scheduling simulation upgrade distributed systems proceedings workshop hot topics operating systems hotos barham isaacs mortier narayanan magpie real-time modelling performance-aware systems proceedingsof workshop hot topics operating systems hotos barrett maglio kandogan bailey usableautonomiccomputingsystems theadministrator perspective proceedings international conference autonomic computing icac boyapati liskov shrira moh richman lazy modular upgrades persistent object stores proceedings acm sigplan conference object-oriented programming languages systems applications oopsla oct brown recovery-oriented approach dependable services repairing past errors system-wide undo phdthesis california berkeley brown patterson undo operators inproceedingsof usenix annual technical conference june carrera bianchini efficiency portability cluster-based network servers proceedings symposium principles practice parallel programming ppopp june castro liskov proactive recovery byzantine-fault-tolerant system proceedings usenix symposium operating systems design implementation osdi oct castro liskov base abstraction improve faulttolerance inproceedingsofthe thacm symposium operating systems principles sosp oct chen accardi 
kiciman lloyd patterson fox brewer path-based failure evolution management proceedings international symposium onnetworked systemsdesign implementation nsdi mar gray computers stop aboutit inproceedingsof symposiumon reliability distributedsoftware database systems jan gray dependability internet era keynote presentation hdcc workshop kalbarczyk iyer bagchi whisnant chameleon software infrastructure adaptive fault tolerance ieee transactions parallel distributed systems kiciman wang discovering correctness constraints self-management system configuration proceedings international conference autonomic computing icac martin nagaraja nguyen zhang mendosus san-based fault-injection testbed construction highly network services proceedings workshop system area networks sanjan lowell saito samberg devirtualizable virtual machines enabling general single-node online maintenance proceedings international conference architectural support programming languages operating systems asplos oct murphy levidow windows dependability technical report msr-tr- microsoft research june nagaraja oliveira bianchini martin nguyen understanding dealing operator mistakes internet services technical report dcstr- department computer science rutgers oppenheimer ganapathi patterson internet services fail proceedings usenix symposium internet technologies systems usits mar patterson brown broadwell candea chen cutler enriquez fox kiciman merzbacher oppenheimer sastry tetzlaff traupman treuhaft recovery-oriented computing roc motivation definition techniques case studies technical report ucb csd- california berkeley mar rice dynaserver project http rice systems dynaserver welsh culler brewer seda architecture well-conditioned scalable internet services proceedings acm symposium operating systems principles sosp oct osdi symposium operating systems design implementation usenix association 
microreboot technique cheap recovery george candea shinichi kawamoto yuichi fujiki greg friedman armando fox computer systems lab stanford candea skawamo fjk gregjf fox stanford abstract significant fraction software failures large-scale internet systems cured rebooting exact failure unknown rebooting expensive causing nontrivial service disruption downtime clusters failover employed work separation process recovery data fine-graintechnique surgically recovering faulty application components disturbing rest application evaluate microrebooting internet auction system running application server microreboots recovermostof samefailuresas full reboots butdoso order magnitude faster result order magnitude savings lost work cheap form recoveryengenders approach high availability microreboots employed slightest hint failure prior node failover multi-node clusters mistakes failure detection failure recovery masked end users transparent call-level retries systems rejuvenated parts shut introduction spite ever-improving development processes tools production-quality software bugs bugs escape testing difficult track resolve form heisenbugs race conditions resource leaks environment-dependent bugs bugs manifest production systems fix time failure fortunately application-level failures bring enterprise-scale software underlying platform hardware operating system reliable comparison contrast smaller-scale systems desktop computers hardware operating system-level problems significant downtime failure strikes large-scale software systems found internet services operators afford run real-time diagnosis focus bringing system back means diagnosis challenge find simple practical effective approach managing failure large complex systems approach accepting fact thatbugs inapplicationsoftware willnot eradicatedany time results studies experience field suggest failures successfully recovered rebooting failure root unknown surprisingly today state art achieving high availability internet clusters involves circumventing failed node failover rebooting failed node subsequently reintegratingthe recoverednode cluster reboots provide high-confidence reclaim stale leaked resources rely correct functioning rebooted system easy implement automate return software start state understood tested state systems unexpected reboots result data loss unpredictable recovery times occurs frequentlywhen software lacks clean separation data recovery process recovery performance optimizations writeback buffer caches open window vulnerability allegedly-persistent data stored volatile memory unexpected crash reboot restart system processes buffered data lost paper presents practical recovery technique call microreboot individual rebooting fine-grain application components achieve benefits whole-process restarts order magnitude faster order magnitude lost work describe general conditions microreboots safe well-isolated stateless components important application state specialized state stores data recovery completely separated reboot-based application recovery describe prototype microrebootable system evaluating microreboot-based recovery low cost microrebooting engenders approach high availability microrebooting attempted front-line recovery failure detection prone false positives failure microreboot-curable microreboot recover system subsequent recovery action recovery time added initial microreboot attempt negligible multinode clusters microreboot preferable node failover avoids overloading non-failed nodes preserves in-memory state minimallydisruptive microreboots rejuvenate system parts shutting transparent call-level retries mask microreboot end users osdi symposium operating systems design implementationusenix association rest paper describes section design microrebootable software section prototype implementation sections evaluate prototype recovery properties fault injection realistic workload section describes simpler approach failure management brought cheap recovery section discusses limitations microrebooting section presents roadmap generalizing approach implemented prototype section presents related work section concludes designing microrebootable software workloads faced internet services consist short tasks long-running affords opportunity recovery reboot work-in-progress lost due rebooting represents small fraction requests served day set optimize large-scale internet services frequent fine-grain rebooting led design goals fast correct component recovery strongly-localized recovery minimal impact parts system fast correct reintegration recovered components earlier work introduced motivated crash-only software programs safely crashed parts recover quickly time high-level recipe building systems structure collection small well-isolated components separate important state application logic place dedicated state stores provide framework transparently retrying requests issued components temporarily unavailable microrebooting summarize main points crash-only design approach fine-grain components component-level reboot time determined long takes underlying platform restart target component component reinitialize microrebootable application aims components small terms program logic startup time benefits design favored largescale internet software partitioning system components inherently system-specific task developers benefit existing component-oriented programming frameworks prototype state segregation ensure recovery correctness prevent microreboots inducing corruption inconsistency application state persists microrebooting inventors transactional databases recognized segregating recovery persistent data application logic improve recoverability application data persist failures thisidea furtherand requirethat microrebootable applications important state dedicated state stores located application safeguarded strongly-enforced high-level apis examples state stores include transactional databases session state managers enabling safe microreboots complete separation data recoveryfrom application recoverygenerally improves system robustness shifts burden data management often-inexperienced application writers specialists develop state stores number applications vast code quality varies wildly database systems session state stores code consistently robust face demands ever-increasing feature sets application recovery code bug-free data process separation improve dependability making process recovery simpler benefits separation outweigh potential performance overhead decoupling components loosely coupled application gracefully tolerate microreboot components crash-only system well-defined well-enforced boundaries direct pointers span boundaries cross-component needed stored components application platform marshalled form inside state store retryable requests smooth reintegration microrebooted components inter-component interactions crash-only system ideally timeouts response received call allotted time frame caller gracefully recover timeouts provide orthogonal mechanism turning non-byzantine failures fail-stop events easier accommodate component invokes microrebooting component itreceivesaretryafter exception call estimated recovery time idempotent non-idempotent calls rollback compensating operations components transparently recover in-flight requests intra-system component failures microreboots hidden end users leases resources system leased improvethe reliability cleaning upafter rbs inaddition memory file descriptors types persistent state carry long-term leases expiration state deleted archived system cpu execution time leased computation hangs renew execution lease terminated requests carry time-to-live stuck requests automatically purged system ttl runs crash-only design approach embodies well-known principles robust programming distributed systems push principles finer levels granularity applications giving non-distributed applications robustness distributed brethren section describe application design principles implementation platform microrebootable applications osdi symposium operating systems design implementation usenix association microrebootable prototype enterprise edition java framework building large-scale internet services motivated frequent critical internet-connected applications current enterprise application market chose add microreboot capabilities open-sourcej eeapplicationserver jboss andconverted application rubis crash-only model made jboss platform universally benefit applications running section describe details prototype component framework common design pattern internet applications three-tiered architecture presentation tier consists stateless web servers application tier runs application persistence tier stores long-term data databases framework designed simplify developing applications model applications consist portable components ejbs platform-specific xml deployment descriptor files application server akin operating system internet services deployment information instantiate application ejbs inside management containers container ejb object manages instances object server-managed containers 
provide application components rich set services thread pooling lifecyclemanagement client session management database connection pooling transaction management security access control theory application run application server modifications needed deployment descriptors end users interact application web interface application presentation tier encapsulated war web archive war component consists servlets java server pages jsps hosted web server invoke methods ejbs format returned results presentation end user invoked ejbs call ejbs interact back-end databases invoke web services ejb similar event handler constitute separate locus control single java thread shepherds user request multiple ejbs point enters application tier returns web tier ejbs provide level componentization suitable building crash-only applications microreboot machinery added microreboot method jboss invokedprogramaticallyfrom withintheserver remotely http modified jboss server microreboots performed application safe application crash-only microreboot method applied ejb war components destroys extant instances objects kills shepherding threads instances releases resources discards server metadata maintained behalf component reinstantiates initializes component resource discard component classloader jboss separate class loader ejb provideappropriate sandboxingbetween components caller invokes ejb method caller thread switches ejb classloader java class identity determined classloader responsible loading discarding ejb classloaderupon update internal microrebooted component preserving classloader violate sandboxing properties keeping classloader active reinitialize ejb static variables acceptable strongly discourages mutable static variables prevent transparent replication ejbs clusters ejbs microrebooted individually ejbs maintain ejbs metadata relationships span containers wemicroreboot transitive closure inter-ejb dependents group determine recovery groups examine ejb deployment descriptors information typically application servers determine order ejbs deployed crash-only application companies jboss run production applications found unwilling share applications converted rice rubis web-based auction system mimics ebay functionality ebid crash-only version rubis additional functionality ebid maintains user accounts bidding selling buying items item search facilities customized information summary screens user feedback pages state segregation e-commerce applications typically handle types important state long-term data persist years customer account activity session data persist duration user session shopping carts workflow state enterprise applications static presentation data gifs html jsps ebid types state database dedicated session state storage ext filesystem optionally mounted read-only entityejbsandstateless session ejbs entity ejb implements persistent application object traditional oop sense instance state mapped row database table stateless session ejbs perform higher level operations entity ejbs end user operation implemented stateless session ejb interacting osdi symposium operating systems design implementationusenix association entity ejbs place bid item ejb interacts entity ejbs user item bid mixed procedural design consistent practices building scalable applications persistent state ebid consists user account information item information bid buy sell activity maintained mysql database entity ejbs idmanager user item bid buy category olditem region userfeedback mysql crash-safe recovers fast datasets items bids users entity bean container-managed persistence mechanism delegates management entity data ejb container jboss provide transparent data persistence relieving programmer burden managing data directly writing sql code interact database ejb involved transactions time microreboot automatically aborted container rolled back database session state ebid takes form items user selects buying selling biddding userid synthesize user session independent stateless http requests discarded user logs session times users identified http cookies commercial application servers store session state middle tier memory case server crash ejb microreboot user sessions lost prototype ensure session state survives rbs application dedicated session state repository options session state storage built fasts in-memory repository inside jboss embedded web server api consists methods reading writing httpsession objects atomically fasts illustrates session state segregated application java virtual machine jvm isolated compiler-enforced barriers fasts fast access session objects survives rbs modified ssm clustered session state store similar api fasts ssm maintains state separate machines isolated physical barriers slower access session state survives rbs jvm restarts node reboots session storage model based leases orphaned session state garbage-collected automatically isolation decoupling compiler-enforced interfaces type safety provide operational isolation ejbs ejbs internal variables allowed mutable static variables ejbs obtain order make inter-ejb method calls naming service jndi provided jboss cached obtained inter-ejb calls mediated application server containers suite interceptors order abstract details remote invocation replication cases ejbs replicated performance load balancing reasons preservation state microreboots segregation session state ebid offers recovery decoupling data shared components means state store frees components recovered segregation helps quickly reintegrate recovered components perform data recovery evaluation framework evaluate prototype developeda client emulator fault injector system automated failure detection diagnosis recovery injected faults ebid measured recovery properties microrebooting wrote client emulator logic load generator shipped rubis human clients modeled markov chain states end user operations ebid login buynow oraboutme transitioning state client issue http request inbetween successive url clicks emulated clients independent times based exponential random distribution seconds maximum seconds tpc-w benchmark chose transition probabilities representative online auction users resulting workload shown table mimics real workload major internet auction site user operation results requests read-only access browse category initialization deletion session state login exclusively static html content home page search search items session state updates select item bid database updates leave seller feedback table client workload evaluating microreboot-based recovery enable automatic recovery implemented failure detection client emulator primitive diagnosis facilities external recovery manager real end users web browsers report failures internetservices theyuse client-sidedetection mimics wan services deploy client-like endto-end monitors internet detect service user-visible failures setup measurements focus recovery aspects prototype orthogonal problem detection diagnosis implemented fault detectors simple fast client encounters network-level error connect server http error flags response faulty errors occur received html searched keywords indicative failure exception failed error finally detection application-specific problem osdi symposium operating systems design implementation usenix association mark response faulty problems include prompted log logged encountering negative item ids reply html fault detector submits parallel request application instance injecting faults separate known-good instance machine compares result truth provided flagging differences failures detector identify complex failures surreptitious corruption dollar amount bid tweaks required account timing-related nondeterminism built recovery manager performs simple failure diagnosis recovers microrebooting ejbs war ebid restarting jvm runs jboss ebid rebooting operating system listens udp port failure reports monitors failed url type failure observed static analysis derived mapping ebid url prefix path sequence calls servlets ejbs recoverymanager incremented time component path originating failed url decides micro reboot based hand-tuned thresholds accurate sophisticated failure detection topic work simplistic approach diagnosis yields false 
positives part goal show mistakes resulting simple sloppy diagnosis tolerable low cost rbs recovery manager simple recursive recovery policy basedon principle tryingthe cheapest recovery reboots progressively larger subsets components microreboots ejbs ebid war entire ebid application jvm running jboss application server finally reboots actions cure failure symptoms notifies human administrator order avoid endless cycles rebooting notifies human notices recurring failure patterns recovery action performed remotely invoking jboss microreboot method ejb war ebid executing commands kill ssh jboss node-level reboot evaluated availability prototype metric action-weighted throughput view user session beginningwith loginoperation andending explicit logout abandonment site session consists sequence user actions user action sequence operations http requests culminates commit point operation succeedfor useraction consideredsuccessful operation action placing bid results committing bid database action succeeds fails atomically operations action succeed count actionweighted goodput good operation fails operations action marked failed counting action-weighted badput bad unlike simple throughput accounts fact long-running short-running operations succeed user happy service captures fact action operations succeeds generally means user work short action figure uset compare recovery recovery jvm restart evaluation results prototype answer questions microrebooting rbs effective recovering failures rbs jvm restarts rbs clusters rb-friendly architectures incur performance overhead section build results show microrebooting change manage failures internet services ghz pentium machines ram web middle tier nodes databases hosted athlon machines ram rpm disks emulated clients ran variety multiprocessor machines machines interconnected mbps ethernet switch ran linux kernel sun java sun microrebooting effective popularity unable find published systematic studies faults occurring production systems deciding faults inject prototype relied advice colleagues industry routinely work enterprise applications application servers found production systems frequently plagued deadlocked threads leak-induced resource exhaustion bug-induced corruption volatile metadata java exceptions handled incorrectly added hooks jboss injecting artificial deadlocks infinite loops memory leaks jvm memory exhaustion application transient java exceptions stress ebid exceptionhandling code corruption data structures addition hooks fig faumachine inject low-level faults underneath jvm layer ebid crash-only application relativelylittle volatile state subject loss corruption application state fasts ssm inject faults data handling code code generates application-specific primary keys identifying rows entity bean instances corrupt class attributes stateless session beans addition application data corrupt metadata maintained application server accessible ebid code jndi repository maps ejb names containers transaction method map stored entity ejb container finally corrupt osdi symposium operating systems design implementationusenix association data inside session state stores bit flips database manually altering table contents perform types data corruption set null generally elicit nullpointerexception access set invalid non-null type-checks invalid application point view userid larger maximum userid set wrong valid application point view incorrect swapping ids users injecting fault recursive policy earlier recover system relied comparison-based failure detector determine recovery action successful failures encountered recovery escalated level policy table show worst-case scenario encountered type injected fault reporting results differentiate resuscitation restoring system point resume servingof requestsfor users necessarily fixed resulting database corruption recovery bringingthe systemtoa statewhereit functions correct database financial institutions aim resuscitation applying compensating transactions end business day repair database inconsistencies sign rightmost column additional manual database repair actions required achieve correct recovery resuscitation based results conclude ejb-level war-level microrebooting prototype effective recovering majority failure modes today production systems rows table microrebooting ineffective types failures rows coarser grained reboots manual repair required fortunately failures constitute significant fraction failures real eesystems faults jndicorruption cured non-reboot approaches reboot-based approach simpler quicker reliable cases manual actions required restore service correctness jvm restart presented benefits component rebooting common recover middleware real world rest paper compare ejb-level microrebooting jvm process restart restarts jboss implicitly ebid microreboot full reboot respect availability internet service operators care user requests system turns downtime evaluate microrebooting respect end-user-awaremetric captured inject faults prototype recovery manager recover system automatically ways restarting jvm process running jboss microrebooting ejbs recovery deemed successful injected fault type reboot level deadlock ejb infinite loop ejb application memory leak ejb transient exception ejb corrupt primary keys set null ejb invalid ejb wrong ejb corrupt jndi entries set null ejb invalid ejb wrong ejb corrupt transaction method map set null ejb invalid ejb wrong ejb corrupt stateless session ejb attributes set null unnecessary invalid unnecessary wrong ejb war corrupt data inside fasts set null war invalid war wrong war corrupt data inside ssm corruption detected checksum bad object automatically discarded corrupt data inside mysql database table repair needed memory leak application intra-jvm jvm jboss extra-jvm kernel bit flips process memory jvm jboss bit flips process registers jvm jboss bad system call return values jvm jboss table recovery injected faults worst case scenarios ejb jboss operating system reboots faults require microrebooting ebid web component war cases resuscitation needed injected fault naturally expunged system call fails case recovering persistent data automatically transaction rollback case injecting wrong data manual reconstruction data required column comparisonbased fault detector experiments table end users experience failures recovery figure shows results experiment injected faults minutes session state stored fasts ran load concurrent clients connected application server node specific setup lead cpu load average similar deployed internet systems noted concurrent clients node subsequent experiment rbs jvm restarts reduced thenumber failedrequestsby visually theimpact failure recovery event estimated area correspondingdip goodt larger dips indicating higher service disruption area dip determined width time recover depth throughput requests turned recovery factor isolation wider dip int requests arrive recovery requests fail user actions fail retroactively marking actions requests failed measured recovery time granularities summarize results table columns break recovery time long osdi symposium operating systems design implementation usenix association resp sec process restart action-weighted throughput correctly satisfied requests failed requests resp sec microreboot timeline minutes correctly satisfied requests failed requests figure usingt tocomparejvmprocess restarttoejbmicroreboot sample point represents number successful failed requests observed min corrupt transaction method map entitygroup ejb recovery group takes longest recover min corrupt jndi entry registernewuser next-slowest recovery min inject transient exception browsecategories entry point browsing most-frequently called ejb workload requests actions failed recovering process restart shown top graph requests actions failed recovering microrebooting ejbs average failed requests actions process restart failed requests actions microreboot ejbs target takes crash forcefully shut long takes reinitialize ejbs recover order magnitude faster jvm restart explains width 
good dip case negligible section ejbs interdependencies captured deploymentdescriptors require microrebooted ebid recovery group entitygroup entity ejbs category region user item bid time ejbs requires microreboot entire entitygroup restarting entire ebid application optimized avoid restarting individual ejb ebid takes sum components crash start jvm crash operating system-level kill reboot-based recovery times dominated initialization case jvm-level restart time spent initializing jboss services transaction service takes sec initialize embedded web server sec jboss control management service takes sec remaining startuptime spent deployingandinitializing ebid ejbs war ejb deployer service verifies ejb object conforms ejb specification required interfaces allocates initializes container sets object instance pool sets security context inserts name-to-ejb mapping jndi initialization completes individual ejbs start methods invoked removing ejb system reverse path microrebootsreduce functional disruption duringrecovery figure shows good drops component time crash reinit msec msec msec aboutme authenticate browsecategories browseregions buynow commitbid commitbuynow commituserfeedback dobuynow entitygroup identitymanager leaveuserfeedback makebid olditem registernewitem registernewuser searchitemsbycategory searchitemsbyregion userfeedback viewbidhistory viewuserinfo viewitem war web component entire ebid application jvm jboss process restart table average recovery times load msec individual components entire application jvm jboss process ejbs superscript entity ejbs rest stateless session ejbs averages computed trials component single-node system sustained load concurrent clients recovery individual ejbs ranges msec jvm restart system serves requests time case microrebooting system continues serving requests faulty component recovered illustrate effect figure graphing availability ebid functionality perceived emulated clients group ebid end user operations functional groups bid buy sell browse view search user account operations zoom recovery events figure user account search browse view bid buy sell process restart client-perceived availability user account search browse view bid buy sell microreboot timeline seconds figure functional disruptionas perceived end users eachpoint horizontal axis solid vertical line bar time service perceived unavailable end user gap aninterval thatsomerequest whoseprocessingspanned time eventually failed suggesting site osdi symposium operating systems design implementationusenix association faulty component recovered microrebooting operations functional groups succeed user account group operations served successfully recovery registernewuser requests fail show entire group unavailable fractional service degradation compounds benefits swift recovery increasing end user-perceived availability service microrebootsreduce lost work figure number requests fail jvm-level recovery completed happen microreboot case failures due session state lost recovery fasts survive jvm restarts ssm fasts jvm restart case exhibited failed requests recovery fraction retroactively failed requests successful overallgood havebeen slightly lower section rbs fasts case allowed system preserve session state recovery avoid cross-jvm access penalties microrebooting clusters typical internet cluster unit recovery full node small relative cluster learn rbs yield benefit systems webuilta clusterof nodes clusters servers typical enterprise settings high-end financial telecom applications running nodes gigantic services ebay online auction service run pools clusters totaling application servers distribute incoming load nodes client-side load balancer failure-free operation distributes incoming login requests evenly nodes established sessions implements session affinity non-login requests directed node session originally established inject rb-recoverablefault table server instances bad failure detectors notice failures report recovery manager decides perform recovery notifies redirects requests bound bad uniformly good nodes oncen bad recovered notifies requests distributed failure failover normal load explored configuration found today systems session state stored locally node fasts failover requests require session state searching browsing successfully served good nodes requests require session state fail injected fault mostfrequently called component browsecategories ran experiment clusters sizes load clients node left graph figure shows results recovering bad jvm restart number user requeststhat failis dominatedby thenumber sessionsthat number requests number nodes cluster node failover recovery microreboot process restart failed requests sessions failed total requests number nodes cluster relative number failures process restart ejb microreboot figure failover normal load left show number requests failed-over sessions case jvm restart show fraction total user requests failed test -minute interval function cluster size established time recovery bad case ejb-level microrebooting number failed requests roughly proportional number requests flight time recovery submitted recovery cluster grows number failed user requests stays fairly constant recovering jvm restart average requests failed case microrebooting requests failed relative benefit microrebooting decreases number cluster nodes increases graph figure recovering microreboot result fewer failed requests jvm restart regardlessof clustersize howmanyclients cluster node serves improves availability cluster aimed level availability offered today telephone switches offer nines availability roughly means satisfy ofrequestsitreceives failatmost -node cluster served requests minutes extrapolated -node cluster application servers implies requests served year six-nines cluster fail jvm restarts number single-node failovers year microreboots failures permissible repeatedsomeof aboveexperimentsusing ssm availability session state recovery longer problem per-node load increased recovery good nodes temporarily handle bad -bound requests addition increased load session state caches populated ssm session state bad -bound sessions factors resulted increased response time exceeded sec jvm restarts microrebooting sufficiently fast make effect unobservable overload situations mitigated overprovisioning cluster investigate microrebooting reduce additional hardware microreboots preserve cluster load dynamics repeated experiments fasts doubled concurrent user population clients node load spike model modest compared occur production systems cnn faced -fold surge load osdi symposium operating systems design implementation usenix association causedtheir cluster tocollapse undercongestion allowthe system tostabilize higher load prior injecting faults reason experiment time interval increased minutes jvm restarts disruptive microreboots mild two-fold change load stability initial conditions favor full process restarts morethan rbs sothe results shownhere conservative respect microrebooting figure shows response time preserved recovering rbs unlike jvm restarts response time msec nodes process restart microreboot nodes process restart microreboot response time msec timeline seconds nodes process restart microreboot timeline seconds nodes process restart microreboot figure failover doubled load show average response time request computed -second intervals cluster configurations nodes ebid fasts storing session state jvm restart microreboot case vertical scales graphs differ enhance visibility details stability response time results improved service end users response times exceeding seconds computer users distracted task pursuing engage making common threshold web site abandonment surprisingly service level agreements financial institutions stipulate seconds maximum acceptable response time measured requests exceeded threshold failover table shows results nodes process restart ejb microreboot table requests exceeding sec failover doubled load asked colleagues industry commercial application servers admission control overloaded surprised learn reason cluster operators significantly overprovision clusters complex load balancers tuned experts order avert overload oscillation problems microreboots reduce overprovisioningor 
sophisticated load balancing rbs successfulat keepingresponsetimes seconds prototype expect user experience improved clustered system microreboot-based recovery process restarts performance impact section measure performance impact modifications steady-state fault-free throughput latency measure impact microrebootenabling modifications application server comparing original jboss microreboot-enabled variant measure cost externalizing session state remote state store comparing ebid fasts ebid ssm table summarizes results configuration throughput average latency req sec msec jboss ebid fasts jboss ebid fasts jboss ebid ssm jboss ebid ssm table performance comparison original jboss microrebootenabled jboss intra-jvm session state storage ebid fasts extra-jvm session state storage ebid ssm throughput varies configurations margin error latency increases ssm moving state jboss remote session state store requires session object marshalled network unmarshalled jvm minimum human-perceptible delay msec increase latency consequence interactive internet service latency-critical applications fasts ssm performance results range measurements major internet auction service latencies average msec depending operation average throughput req sec node itis meaningfultocomparethe performanceof ebid original rubis semantics applications rubis requires users provide username password time perform operation requiring authentication ebid users log beginning session subsequently identified based http cookies supply server access refer reader detailed comparison performance scalability architectures applications approach failure management previous section showed microreboots significant quantitative benefits terms recovery time functionality disruption amount lost work preservation load dynamics clusters quantitative improvements beget qualitative change manage failures large-scale componentized systems present possibilities osdi symposium operating systems design implementationusenix association alternative failover schemes microrebootable cluster rb-based recovery attempted prior failover earlier node failover destabilizing set experiments section failing requests good nodes bad recovering resulted failed requests figure average number failures requests continued recovering node shows failover improves user-perceived availability failover benefit pre-failover due mismatch node-level failover component-level recovery coarse-grained failoverpreventsn bad serving large fraction requests serve recovering figure redirecting requests nodes requests fail ssm unnecessarily overload good nodes ssm pre-failover prove ineffective load balancer failover haven bad rebooted cost microrebooting nonrb-curable case negligible compared impact recovery average failed requests microreboot update computation six-nines availability section microreboots failover -node cluster fail times year offer nines availability writing microrebootable software allowed fail day times year weeks times year jvm restart recovery mitigate coarseness node-level failover component-level failover reduced cost reboot making finer-grain microfailover natural solution load balancers augmented ability fail requests touch component recovering failing requests microfailover accompanied microreboot reduce recovery-induced failures microfailover requires load balancer understanding application dependencies make impractical real internet services user-transparent recovery recovery sufficiently non-intrusive low-level retry mechanisms hide failure recovery callers won notice fortunately http specification offers return code indicating web server temporarily unable handle request typically due overload maintenance code accompanied aretry-after headercontaining time web client retry implemented call retry prototype previously step microrebooting component removal binding jndi bind component sentinel processing idempotent request servlet encounters sentinel ejb lookup servlet container automatically replies retry-after seconds client idempotency information url prefixesbased understanding ebid inferred static call analysis measured effect http retry calls components found transparent retry masked roughly half failures table corresponds two-fold increase perceived availability operation retry delay component retry retry viewitem browsecategories searchitemsbycategory authenticate table masking microreboots http retry-after data averaged trials component shown failed requests visible end users requests entered system microreboot started reduce failures experimented introducing -msec delay sentinel rebind beginning microreboot allowed requests processed about-to-bemicrorebooted component complete component encountered failure process requests prior recovery instances ejb faulty instances microreboot recycles instances component column table shows significant reduction failed requests analyze tradeoff number saved requests -msec increase recovery time tolerating lax failure detection general downtime incident sum time detect failure det time diagnose faulty component time recover failure monitor quality generally characterized quick det detections mistaken false positive rate det real failures misses false negative rate det monitors make tradeoffs betweentheseparameters alongert det generallyyields lower det det sample points gathered analysis cheap recovery relaxes task failure detection ways longer det additional requests failing detection compensated reduction failed requests recovery false positives result useless recovery leading unnecessarily failing requests cheaper recovery reduces cost false positive enabling systems accommodate higher det trading det det result lower false negative rate improve availability osdi symposium operating systems design implementation usenix association illustrate det relaxation left graph figure inject fault frequently called ejb delay recovery det seconds shown horizontal axis perform recovery jvm restart microreboot dotted line rb-based recovery monitor seconds detect failure providing higher user-perceived availability jvm restarts detection det curves graph asymptotically close large values det number requests fail detection due delay recovery eventually dominate fail recovery real-time diagnosis recovery opportunity cost experiment requests failed waiting contrast microreboot averages failed requests takes msec table suggests microrebooting diagnosis result approximately number failures offers possibility curing failure diagnosis completes failed requests detection time sec process restart microreboot failed requests false positives false positive rate process restart microreboot figure relaxing failure detection cheap recovery graph figure shows effect false positives end-user-perceived availability averages figure failed requests jvm restart requests false positive detections occur inbetween correct positive detections false result pointless recovery-induced downtime correct lead recovery simplicity assume det graph plots number failed requests caused sequence useless recoveries triggered false positives recovery response correct positive number false positives inbetween successive correct detections corresponds det dotted line availability achieved jvm restarts andfp det improvedwith rb-based recoveryevenwhenfalse positiveratesareas highas engineering failure detection fast accurate difficult microreboots give failure detectors headroom terms detection speed false positives allowing reduce false negative rates reduce number real failures miss lower false negative rates lead higher availability expect extra headroom improvingthe precision monitors pinpoint faulty components microrebooting requires component-levelprecision unlike jvm restarts averting failure microrejuvenation automatic garbage collection resource leaks major problem large-scale java applications recent study ibm customers e-business softwarerevealedthatproductionsystemsfrequentlycrashbe- memory leaks avoid unpredictable leakinduced crashes operators resort preventive rebooting software rejuvenation largest financial companies reboot servers daily recover memory network sockets file descriptors section show rb-based rejuvenation microrejuvenation effective jvm restart preventing leak-induced failures cheaper wrote server-side rejuvenationservice periodically checks amount memory jvm drops alarm bytes recovery service microreboots components rolling fashion memory exceeds threshold sufficient ejbs microrebooted sufficient reached jvm restarted production systems monitor number additional system parameters number file descriptors cpu utilization lock graphs 
identifying deadlocks rejuvenation service knowledge components microrebooted order reclaim memory builds list components components microrebooted service remembers memory released list sorted descending order released memory time memory runs low rejuvenation service microrejuvenates components expected release memory re-sorting list needed induce memory leaks components viewitem stateless session ejb called frequently workload item entity ejb part longrecovering entitygroup choose leak rates experiment minutes mem timeline minutes figure memory microrejuvenation inject invocation leak item invocation leak viewitem alarm set -gbyte heap sufficient figure show free memory varies worst-case scenario microrejuvenation initial list components components leaking memory end round microrejuvenation interval timeline ebid ends rebooted pieces time viewitem found leaked memory osdi symposium operating systems design implementationusenix association item second-most list candidate components reordered improving efficiency subsequent rejuvenations time alarm reached att memory threshold rejuvenation viewitem item require rejuvenation fourth viewitem sufficient repeating experiment rejuvenation jvm restarts resulted total requests failed -minute interval microrejuvenating rbs requests failed order magnitude improvement good dropped commonly argument motivate software rejuvenation turns unplanned total downtime planned total downtime microrejuvenation turn planned total downtime planned partial downtime limitations recovery microreboot rbs introduce classes problems interruption component state update improper reclamationof microrebooted component sexternal resources delay needed full reboot impact shared state state updates atomic theyare databases fasts ssm distinction rbs process restarts state perspective case non-atomic updates state microrebooting component leave state inconsistent unbeknownst components share jvm restart hand reboots components simultaneously give opportunity inconsistent state best-practices documents discourage sharing state passing components static variables requirement enforced suitably modified jit compiler alternatively runtime detects unsafe state sharing practices disable rbs application question jvm restart refresh components discards volatile shared state inconsistent rbs state persist crash-only system state survives recovery components resides state store assumes responsibility data consistency order accomplish dedicated state repositories apis sufficiently high-level repository repair objects manages detect corruption faults inconsistencies perpetuate application-generic checkpoint-based recovery unix found work logical limit applications stateless recovery involves microrebooting processing components repairing data state stores interaction external resources component circumvents jboss acquires external resource application server aware microrebooting leak resource jvm jboss restart experimentally verified ejb directly open connection database jboss transaction service acquire database lock share connection ejb ifx microrebooted prior releasing lock database connection open session stays alive database release lock session times case jvm restart resulting termination underlying tcp connection operating system termination session release lock jboss knew acquired session properly free session case contrived violates programming practices illustrates application components obtain resources exclusively facilities provided platform delaying full reboot state segregated application effective reboot scrubbing data implementation scrub data maintained application server behalf application database connection pool caches microreboots generally recover problems occurring layers application application serveror jvm requirea fulljvm restart full process restart required poor failure diagnosis result ineffectual componentlevel rbs discussed section failure localization precise microreboots jvm restarts recursive policy microrebooting progressivelylarger groups components eventually restart jvm diagnosis case rbs add small additional cost total recovery cost generalizing prototype applications microreboot-friendly require minimal advantage rb-enabled application server based experience applications learned biggest challenges making microrebootable extricating session state handling application logic ensuring persistent state updated withtransactions server leveraged applications feel makes easier write microrebootable application model amenable state externalization component isolation hope microreboot support types systems section describe design aspects deserve consideration extensions isolation property microrebootable systems critical partitioning system fine-grain isolated osdi symposium operating systems design implementation usenix association components partitioning system-specific task frameworkslike eeand net component isolation enforced lower-level hardware mechanisms case separate processaddressspaces bugs thejava virtual machine application server result state corruption crossing component boundaries depending system stronger levels isolation warranted achieved processes virtual machines dependencies components minimized dense dependency graph increases size recovery groups making rbs longer disruptive workload microreboots thrive workloads consistingoffine-grain independentrequests ifasystemisfaced long running operations individual components periodically microcheckpointed cost rbs low keeping mind risk persistent faults vein requests sufficiently self-contained fresh instance microrebooted component pick request continue processing previous instance left resources java offer explicit memory release lease-based allocation call system garbage collector form resource reclamation complete amount time independent size memory unlike traditional operating systems efficient support microreboots requires nearlyconstant-time resource reclamation mechanism microreboots synchronously clean resources related work work major themes reboot-based recovery minimizing recovery time reducing disruption recovery section discuss small sample work related themes separation control data key reboot-based recovery ways isolate subsystems processes virtual machines microkernels protection domains isolated processing components appeared pre-j transaction processing monitors piece system functionality doingi owithclients separateprocess communicating theothers ipc rpc session state managed memory dedicated component architecture scale verywell onecomponent oneprocess approachprovided isolation monolithic architectures amenable microrebooting baker observed emphasizing fast recovery crash prevention potential improve availability ways build distributed file systems recoverquickly crashes design recovery box safeguards metadata memory recovery warm reboot work provide components general framework reduces impact crash speeds recovery work internet services focused reducing functional disruption recovering transient failure failover clusters canonical brewer proposed principle understand partial failure multi-node service mapped decrease queries served decrease data returned query research systems embraced approach reducing downtime recovering sub-system levels nooks isolates drivers lightweight protection domains inside operating system kernel driver fails restarted affecting rest kernel farsite peerto-peer file system recently restructured collection crash-only components recovered rebooting systems provide examples microrebootable systems lend credibility belief non-j systems structured effective microrebootability conclusions employing reboot-based recovery root failures identified fixed rebooting simply separation concerns betweendiagnosis recovery consistentwith theobservation prerequisite attempting recover reboot-curable failure reboot entails risk taking longer disruptive reboot place hurting availability completely separating process recovery data recovery delegating specialized state stores enabled microreboots achieve process recovery experiments microreboots cured majority failures empirically observed downtime deployed internet services compared process restart-based recovery microrebooting order magnitude faster disruptive multinode clusters fault microrebootable systems attempt microreboot-based recovery long costs skipping node failover clusters microrebooting faulty node improve availability commonly-used fail reboot node approach microreboot-based recovery achieve higher levels availability false positive rates fault detection high microreboots reclaim memory leaks prototype application shutting improving availability order magnitude significant limitation developing bug-free software size accepting bugs fact argue structuring systems cheap reboot-based recovery promising path dependable large-scale software osdi symposium 
operating systems design implementationusenix association acknowledgments david cheriton andour colleagues recovery-oriented computing project early feedback work indebted shepherd jason nieh anonymousosdi reviewers katerina argyraki kim keeton adam messinger martin rinard westley weimer patiently helping improve paper adya bolosky castro cermak chaiken douceur howell lorch theimer wattenhofer farsite federated reliable storage incompletely trusted environment proc symposium operating systems design implementation boston baker sullivan recovery box fast recovery provide high availability unix environment proc summer usenix technical conference san antonio barnes application servers market overview meta group march bhatti bouch kuchinsky integrating userperceived quality web server design proc international wwwconference amsterdam holland brewer lessons giant-scale services ieee internet computing july broadwell sastry traupman fig prototype tool online verification recovery mechanisms workshop self-healing adaptive self-managed systems york buchacker sieh framework testing faulttolerance systems including network aspects proc ieee high-assurance system engineering symposium boca raton candea fox recursive restartability turning reboot sledgehammer scalpel proc workshop hot topics operating systems elmau germany candea fox crash-only software proc workshop hot topics operating systems lihue hawaii cecchet marguerite zwaenepoel performance scalability ejb applications proc conference object-oriented programming systems languages applications seattle chen zheng lloyd jordan brewer failure diagnosis decision trees proc intl conference autonomic computing york chou fault tolerance ieee computer chou personal communication oracle corp cohen jacobs personal comm oracle duvur personal comm sun microsystems information obtained agreement prohibits disclosure company garfinkel pfaff chow rosenblum boneh terra virtual machine-based platform trusted computing proc acm symposium operating systems principles bolton landing gettys mogul frystyk masinter leach berners-lee hypertext transfer protocol http internet rfc june gray computers stop proc symp reliability distributed software database systems los angeles huang kintala kolettis fulton software rejuvenation analysis module applications proc international symposium fault-tolerant computing pasadena jboss web page http jboss keynote systems http keynote lefebvre cnn facing world crisis talk usenix systems administration conference levine personal communication ebates liedtke real microkernels communications acm ling kiciman fox session state soft state proc symposium networked systems design implementation san francisco lowell chandra chen exploring failure transparency limits generic recovery proc symposium operating systems design implementation san diego messer personal communication bancorp messinger personal comm bea systems microsoft microsoft net framework microsoft press redmond miller response time man-computer conversational transactions proc afips fall joint computer conference volume mitchell ibm research personal comm mitchell sevitsky leakbot automated lightweight tool diagnosing memory leaks large java applications proc european conf objectoriented programming darmstadt germany murphyandt gent reliability automated data collection process quality reliability engineering intl pal personal communication yahoo reimer ibm research personal comm rubis project web page http rubis objectweb smith tpc-w benchmarking e-commerce solution transaction processing council sullivan chillarege software defects impact system availability study field failures operating systems proc international symposium fault-tolerant computing montr eal canada sun microsystems http java sun swift bershad andh levy improvingthe reliability commodity operating systems proc acm symposium operating systems principles bolton landing whisnant kalbarczyk iyer microcheckpointing checkpointing multithreaded applications proc ieee intl on-line testing workshop wood software reliability customer view ieee computer aug zona research bulletin speed apr osdi symposium operating systems design implementation usenix association 
fuse lightweight guaranteed distributed failure notification john dunagan nicholas harvey michael jones dejan kosti marvin theimer alec wolman discouraged failure step forward thomas edison abstract fuse lightweight failure notification service building distributed systems distributed systems built fuse guaranteed failure notifications fail failure notification triggered live members fuse group hear notification bounded period time irrespective node communication failures contrast previous work failure detection responsibility deciding failure occurred shared fuse service distributed application applications implement definitions failure experience building scalable distributed event delivery system overlay network convinced usefulness service results demonstrate network costs fuse group small overlay network implementation requires additional liveness-verifying ping traffic needed maintain overlay making steady state network load independent number active fuse groups introduction paper describes fuse lightweight failure notification service building distributed systems managing failures important complex task architectures abstractions services proposed address fuse programming model failure management simplifies task agreeing failures occurred distributed system reducing complexity faced application developers closely related prior work coping failures centered failure detection services fuse takes approach microsoft research microsoft corporation redmond jdunagan mbj theimer alecw microsoft department computer science duke durham dkostic duke computer science artificial intelligence laboratory massachusetts institute technology cambridge nickh mit application applications create fuse group immutable list participants fuse monitors group fuse application decides terminatethegroup guaranteed learn group failure bounded period time focus delivering failure notifications leads refer fuse failure notification service approach prior systems adopted argue good approach wide-area internet applications applications make fuse abstraction application asks fuse create group participating nodes fuse finishes constructing group returns unique identifier group creator application passes fuse applications nodes group registers callback fuse fuse guarantees group member reliably notified callback failure condition affects group failure notification triggered explicitly application implicitly fuse detects communication group members impaired applications create multiple fuse groups purposes fuse groups span set nodes event fuse detects low-level communication failure failure signalled fuse groups path individual fuse group application signal failure affecting groups fuse guarantee notifications delivered bounded period time face node crashes arbitrary network failures refer semantics distributed one-way agreement one-way refers fact transition group member live failed failure notification group detecting future failures requires creating group providing semantics fuse ensures failure notifications fail greatly simplifies failure handling nodes state handle coordinated fashion fuse efficiently handles corner cases guaranteeing members notified failure condition affecting group applications built top fuse worry osdi symposium operating systems design implementationusenix association failure message orphaned state remains system primary target fuse wide-area internet applications peer-to-peer applications web services grid computing fuse targeted applications require strong consistency replicated data stock exchanges missile control systems techniques virtual synchrony paxos bft proven thesetechniques incur significant overhead limited scalability fuse provide resilience malicious participants preclude solving problem higher layer previous work failure detection services membership fundamental abstraction services typically provide list component system membership services widespread success building block implementing higher level distributed services consensus deployed commercially important systems york stock exchange disadvantage membership abstraction application components failed respect action respect suppose node engaged operation peer point fails receive timely response failure management service support declaring operation failed requiring node process declared failed fuse abstraction flexibility fuse tracks individual application communication paths working manner acceptable application scenario occur wide-area internet applications face demanding operating environment network congestion leads variations network loss rate delay intransitive connectivity failures called partial connectivity failures occur due router firewall misconfiguration individuals network components links routers fail conditions difficult applications make good decisions based solely information provided membership service scenario illustrating applications participate deciding failure occurred delivery streaming content internet failure achieve threshold bandwidth unacceptable application applications perfectly happy connectivity provided network path fuse implementation scalable scaling respect number groups multiple failure notification groups share liveness checking messages implementations fuse abstraction sacrifice scalability favor increased security fuse implementation designed support large numbers small moderate size groups attempt efficiently support large groups large groups tend suffer too-frequent failure notification making fuse implementation well-suited applications scalable overlay networks scalable overlay networks liveness checking maintain routing tables fuse re-use liveness checking traffic deployment network traffic required implement fuse independent number groups failure occurs creation teardown group introduce per-group overhead overlay network implementation construct overlay alternative liveness checking topology implemented fuse top skipnet scalable overlay network built scalable event delivery application fuse evaluated implementation main techniques discrete event simulator evaluate scalability live system overlay participants running process running cluster workstations evaluate correctness performance simulator live system identical code base base messaging layer live system evaluation shows fuse implementation lightweight latency fuse group creation latency rpc call furthest group member latencyofexplicitfailurenotificationissimilarlydominated network latency show implementation robust false positives caused network packet loss summary key contributions paper present abstraction fuse failure notification group semantics distributed one-way agreement desirable semantics chosen setting wide-area internet applications vice describe significantly reduced complexity task work allowed support fuse adding additional liveness checking experimentally evaluated performance implementation live system virtual nodes related work failure detection subject decades research work broadly classified unreliable failure detectors weakly consistent membership services strongly consistent membership services unreliable failure detectors provide weakest semantics directly standard building block constructing membership services stronger semantics weakly-consistent strongly-consistent osdi symposium operating systems design implementation usenix association membership services based abstraction list unavailable components typically processes machines contrast fuse group bound process machine contexts correspond set processes related data stored machines abstraction fuse provide semantics distributed agreement subject elaborate discussion weakly consistent membership services chandra formalized concept unreliable failure detectors showed detector weakest failure detector solving consensus detectors typically provide periodic heartbeating callbacks component responding responding callbacks aggregate judgments based sets pings unreliable failure detectors provide semantic guarantee fail-stop crashes identified bounded amount time extensive work detectors focusing aspects interface design scalability rapidity failure detection minimizing network load fuse similar lightweight mechanism periodic heartbeats unreliable failure detectors stronger distributed agreement semantics unreliable failure detectors typically component membership service membership service responsible implementing distributed agreement semantics subject extensive body work work broadly classified differing speed failure detection accuracy low rate false positives message load completeness failed nodes agreed failed system epidemic gossip-style algorithms build highly scalable implementations service previous application areas 
proposed membership services distributed collaborative applications online games large-scale publish-subscribe systems multiple varieties web services application domains targeted fuse fuse similar overhead weakly consistentmembershipserviceinthesesettings typicalcharacteristics applications operations idempotent straightforwardly undone operations re-attempted set participants decision retry deferred user instant messaging service aspect fuse abstraction ability handle arbitrary network failures contrast weakly consistent membership services provide semantic guarantees assuming fail-stop model kind network failure fuse abstraction intransitive connectivity failure reach reach reach class network failuresishardforaweaklyconsistentmembershipserviceto handle abstraction membership list limits service choices drawbacks declare nodes experiencing intransitive connectivity failure failed prevents node node reach declare nodes experiencing intransitive failureto alive nodes inthe system reach application block duration connectivity failure persistent inconsistency nodes views membership list forces application deal inconsistency explicitly membership service longer reducing complexity burden application developer fuse appropriately handles intransitive connectivity failures allowing application node experiencing failure declare fuse group failed fuse groups involving node utilizing failed communication path continue operate application participation required achieve fuse detected failure weakly consistent membership services fuse typically monitors subset application-level communication paths fuse requireapplication involvement infailure detection monitored communication paths multi-tier service composed front-end middle-tier back-end suppose middle-tier component misconfigured fuse front-end declare failure perform failure recovery finding middle-tier pool machines difference usage membership services fuse reflects difference philosophy membership services proactively decide system level nodes processes fuse mechanism applications declare failures application-level constraints configuration violated contrast approaches fuse abstraction enables fate-sharing distributed data items associating items single fuse group application developers enforce invalidating item willcause remaining data items invalidated weakly consistent membership services explicitly provide tying distributed data strongly consistent membership services share abstractionofamembershiplist buttheyalsoguarantee atomic updates membership services important component building highly reliable systems virtual synchrony notable examplesofsystemsbuiltusingthisapproacharethenewyork osdi symposium operating systems design implementationusenix association swiss stock exchanges french air traffic control system aegis warship limitation virtual synchrony shown perform small scales node systems network routing protocols is-is ospf autonet mechanisms similar fuse similar aspect autonet teardown recreate manage failures linkstate change autonet switches discard link-state databases rebuild global routing table ospf is-is local link observations propagate network link-state announcements tolerate arbitrary network failures timers explicit refreshes maintain linkstate databases fuse timers keep-alives tolerate arbitrary network failures fuse tie sets links provide end-to-end connectivity group members provide decision connectivity satisfactory distantly related area prior work blackbox techniques diagnosing failures techniques failed requests contrast fuse assumes application developer participation semantic guarantees developer significant distinction black-box techniques typically require data aggregation central location analysis fuse requirement distributed transactions well-known abstraction simplifying distributed systems fuse weaker semantics distributed transactions fuse maintain semantic guarantees network failures distributed transactions block theoretical results consensus show possibility blocking fundamental protocol distributed transactions design choices made building fuse recommended recent works dealing architectural design network protocols surveyed hard-state soft-state signaling mechanisms broad class network protocols recommended soft state approach combining timers explicit revocation fuse mogul argued state maintained network protocol implementations exposed clients protocols section modified overlay routing layer expose mechanism fuse piggyback content overlay maintenance traffic fuse semantics api begin describing simple approach implementing fuse abstraction suppose group member periodically pings group member message group member reason node failure network disconnect network partition transient overload fail respond ping member initiated missed ping ensure failure notification propagates rest group ceasing respond pings fuse group individual observation failure converted group failure notification mechanism failure notifications delivered pattern disconnections partitions node failures specific fuse implementation guarantees failure notifications propagated party periodic pinging interval implementation liveness checking topology discussed section ping retry policy retry policy guarantees failure notification latency discussed section fuse derived analogy laying fuse group members group member wishes signal failure light fuse failure notification propagate group members fuse burns connectivity failure node crash intermediate location fuse fuse lit fuse start burning direction failure ensures communication failures stop progress failure notification fuse burnt relit analogous fuse facility notifies application fuse group fuse implementations implementations fuse abstraction provide distributed one-way agreement failure notifications delivered live group members node crashes arbitrary network failures fuse implementations strategies group creation liveness checking topology retry programming interface persistence consequent variations performance section describe choices made fuse implementation resulting semantics application developers understand alternative strategies fuse implementations programming interface fuse groups created calling creategroup desired set member nodes generates fuse unique group communicates fuse layers members returns caller applications subsequently expected explicitly communicate fuse creator group members applications learning fuse register handler fuse notifications registerfailurehandler function design fuse layer responsible communicating fuse applications nodes creator osdi symposium operating systems design implementation usenix association creates fuse notification group nodes set fuseid creategroup nodeid set registers callback function invoked notification occurs fuse group void registerfailurehandler callback handler fuseid application explicitly fuse failure notification void signalfailure fuseid figure fuse api fuse fate-sharing distributed application state applications learn fuse ids sufficient context application state associate fuse failure handlers simply perform garbage collection application state handler free attempt re-establish application state usinganewfusegroup ortoexecutearbitrarycode brevity refer permissible applicationlevel actions short-hand garbage collection layer believes failure occurred node communication failure application explicitly signalled failure event group members registerfailurehandler called fuse parameter exist signalled handler callback invoked immediately applications explicitly signal failure calling signalfailure function group creation group creation implemented ways return immediately block nodes group contacted returning immediately reduces latency fuse checked group members alive application perform expensive operations fuse signal failure short time contrast blocking members contacted increases creation latency decreases likelihood fuse group immediately fail chose implement blocking create application developers semantic guarantee group creation returns successfully group members alive reachable high rate churn group members repeatedly prevent fuse group creation succeeding based low latency fuse group creation section high churn rate system fail ways fuse bottleneck creategroup called fuse generates globally unique fuse group node contacted asked join fuse group group member unreachable nodes learned fuse group notified failure group members learned group subsequently unreachable similarly detect group failure inability communicate group members fuse layer successfully contacted members fuse returned creategroup caller fuse state orphaned failures failures occur group creation application receive fuse group creator attempt associate failure handler fuse group find group longer exists failure signalled failure handler 
invoked notification arrived failure handler registered liveness monitoring failure notification setup complete fuse implementation monitors liveness group members spanning tree individual branches follow overlay routes group creator group members ifeither side decides link failed ceases acknowledge pings fuse group links occurs immediately signal group failure implementation attempts repair explained detail section mechanism fuse guarantee member group failure notification received live group member fuse invokes failure notification handler node tearing state fuse group node hearing notification due crash network disconnect partition notification explicitly triggered group member fuse guarantee notice persistent communication problems group members automatically guarantees communication failure noticed group member detected group members fuse guarantees delivery failure notifications nodes contacted group creation note fuse clients mechanism implement general-purpose reliable communication well-known impossibility results consensus apply fuse concrete illustrating limitation network partition fuse members sides partition receive failure notifications communicate additional information failure partition fuse generate notification entire group nodes alive attempted communication succeed refer notification false positive easy false positives occur transient communication failurescantriggergroupnotification thepossibilityoffalse osdi symposium operating systems design implementationusenix association positives inherent building distributed systems top unreliable infrastructure tune timeout retrypolicy liveness checking mechanism ure detection probability timeouts generate false positives provide api mechanism applications modify fuse timeout retry policy providing mechanism add complexity implementation providing benefit applications implement timeouts dictated choice transport layer fuse requires participation applications fuse necessarily monitor link implement liveness checking overlay routes maximum notification latency proportional diameter overlay successive failures adversarially chosen times link fail failure timeout previous link expect notification failure rarely require single failure timeout interval fuse implementation attempts aggressively propagate failure notifications application developers maximum latency order timeouts mentioned sends monitored timeout transport layer application sender times signal fuse layer explicitly node waiting receive message timeout difficultbecause onlythesenderknowswhen thetransmission initiated case sender crashed developers rely fuse layer timeout guarantee failure handler called fuse failure notifications necessarily eliminate race conditions application developer handle group member signal failure notification initiate failure recovery sending message group member failure recovery message arrive group member receives fuse failure notification experience version-stamping data races fail-on-send fuse guarantee communication failuresbetweengroupmemberswillbeproactivelydetected wireless networks link conditions small messages liveness ping messages larger messages case application detect communication path working explicitly signal fuse call reason explicitly signalling fuse fail-on-send send communication path successfully transmits fuse liveness checking messages meet application failed communication path application fuse monitoring category intransitive connectivity failure applications communicate directly responding fuse messages party experience failure attempting exchange message fuse guarantees party triggers notification point live group members hear notification applications generate mixed acknowledged un-acknowledged traffic application send streaming video udp alongside control stream tcp case application failon-send failure case handled manner previous cases failure model security fuse designed handle node crashes arbitrary network failures malicious behavior application built fuse handles malicious behavior redundancy fuse layer multiple content distribution trees section fuse assumes network failure model consisting anypatternofpacketloss duplicationorre-ordering includes simultaneous network partitions adversary dropping packets based content network failure fuse guarantees parties agree failure occurred fuse implementation routes fuse overlay messages tcp connections implementation handles arbitrary packet loss re-ordering handles duplication extent tcp straightforward extend implementation handle arbitrary duplication incorporating digital signatures timestamps extension prevent tampering message contents fuse ability handle packet loss dependent reliable transport layer tcp alternative fuse implementations unreliable transport layers udp transport present performance characteristics application developers aware model node failures fail-stop software failures recognized application misconfiguration detected handled explicitly signalling fuse group fuse handles software failures result process exit unhandled exceptions fuse handle nodes behave maliciously due explicit compromise due software faults appropriately contained malicious nodes attack fuse ways sarily generating failure notifications dropping failure notification continuing generate ping messagesforthefailedgroup thisviolatesthefuse osdi symposium operating systems design implementation usenix association notification semantics generating unnecessary failure notifications prevent functional fuse groups leading denial-of-service dos attack application response failure notification re-attempt failed operation set nodes sustaining dos attack difficult crash recovery implementation fuse stable storage crash recovery trivial implementation initialization recovering node failure notification propagated group members fuse handles case corner cases nodes actively compare lists live fuse groups part liveness checking discuss details implementation section effect disagreements current set live fuse groups detected failure timeout interval disagreements resolved triggering notification groups considered failed group member alternative fuse implementation stable storage attempt mask node crashes node recovering crash assume fuse groups participates alive active comparison fuse ids suffice reliably reconcile node rest world compatibility issue nodes employing stable storage co-exist nodes employing stable storage change fuse semantics case persistent communication failure node recovering crash applications make volatile-state fuse groups guard state stored stable storage requires additional application-level complexity applications part herald project build scalable event notification service exploring construction scalable reliable application-level multicast groups scalable peer-to-peer overlay network grappling complexities implementing failure handling automatic re-configuration led invent abstraction failure notification deciding factor inventing fuse design multicast groups well-known technique implementing application-level multicast overlay construct multicast trees reverse path forwarding scribe major drawback approach nodes overlay routing path subscriber root node forward potentially evenifthey interest remove potential obstacle deployment designed subscriber volunteer trees trees route content non-interested parties establishing separate content-forwarding links thisleadstotwointerrelated data structures content forwarding tree overlaid reverse path forwarding rpf tree content-forwarding tree straightforward construct absence failure repairing tree introducing distributed routing cycles proved difficult face arbitrary possibly simultaneous node failures link failures message loss routing overlay manage complexity adopted simple design pattern garbage collect out-ofdate state fuse retry establishing fuse group installing application-level state fuse allowed tie distributed state needed garbage collected design pattern drastically reduced state space instrumental achieving working tree implementation single fuse group ties endpoints content-forwarding link rpf tree nodes bypassed link failure notification group garbage collects relevant state failure notification subscriber requested creation nowfailed content-forwarding link responsible creating replacement fuse group forwarding link subscriberisdead natural choice fuse group creator fuse obviating voting mechanism manage group creation re-creation mentioned section fuse eliminate race conditions trees remaining trivial handle subscribers add version stamps subscription request prevent latearriving fuse notifications acting contentforwarding links fuse reduced amount code required implement trees fuse include large amount additional context message recipient garbage-collect now-invalid 
state found difficult reason correctness non-fuse alternative design whenamulticast group participant voluntarily leaves tree explicitly signal fuse group signaled node failed repairs occur removing node content-forwarding tree desire support large multicast trees requirethatwesupportindividualfusegroupswithalarge number members designed trees large number small medium size fuse groups forexample simulatinga subscribertreeona node overlay required average members fuse osdi symposium operating systems design implementationusenix association group maximum size verified maximum fuse group sizes depend size multicast tree increase slowly size overlay applications experience implementing event delivery service fuse applications built top scalable overlay networks benefit fuse applications construct large number trees monitor parent-child links trees application-level heartbeats fuse application-level heartbeats liveness checking traffic shared trees type application fuse content delivery network cdn replicates large number documents pushes updates basis entail large number replication multicast trees common strategy reliability trees mirrors approach discussed peer-topeer applications heartbeats ensure replica site object track receiving updates correctly disconnected tree fuse replace per-tree heartbeat messages efficient scalable means detecting trees reconfigured due node network failures peer-to-peer storage systems totalrecall andom ing fuse implement liveness checking totalrecall relies overlay liveness-checking eager replicas separately implement livenesschecking lazy replicas substitution fuse groups straightforward implements failure detection timeout scheme leases leases replaced fuse groups fuse good fit replica regenerate entire replica set monitor liveness replicas symmetric responsibility corresponds semantics fuse group notifications lastly potential false positives fuse compromise consistency guarantees failure-induced reconfiguration protocol designed robust failuredetection false positives addition fuse application areas targeted weakly consistent membership services vogels argue weakly consistent membership services benefit web services ranging scientific computing federated business activities fuse suitable choice emerging applications group group figure fuse groups monitored overlay pings black lines denote end-to-end checking dashed gray lines denote active overlay pings liveness checking topologies liveness checking topologies implement fuse section describe detail topology chose per-group spanning trees overlay network discuss topologies security scalability tradeoffs overlay design requires content-addressable overlay dhts chord pastry skipnet tapestry figure depicts overlay topology live fuse groups spanning tree fuse group group members additional nodes refer delegates delegates arise fuse liveness checks routed overlay paths betweenmembers members overlay routes change delegates fail delegates added deleted liveness checking tree repair process explained detail section building liveness checking trees top overlay lets reuse liveness checks overlay maintain routing tables liveness checking tree fuse group union overlay routes betweenthegroupcreator theroot andthegroupmembers multiple fuse groups overlapping trees overlay ping message monitors fuse groups liveness checking trees include overlay link figure illustrates sharing fuse groups absence failures fuse implementation requires additional messages overlay pings monitor fuse groups group setup teardown repair incur per-group costs build systems require large numbers fuse groups alternative topologies section present alternative topologies fuse liveness monitoring provide security guarantees cost worse scalability implementations topologies simpler provide stronger guarantees worst-case failure notification latency mentioned section malicious nodes mount kinds attacks fuse dropped osdi symposium operating systems design implementation usenix association notification attack unnecessary notification attack overlay topology fuse implementation attacks mounted malicious group members delegates tree application handles dropped notification attack fuse layer redundant content-distribution trees handles unnecessary notification attack re-creating fuse groups sets members alternative topology per-group spanning trees overlay routing liveness checking traffic directly members topology eliminates threat delegates launching attacks fuse scalability tradeoff additional security overhead liveness checking traffic additive number fuse groups opportunities share liveness checking traffic depend degree overlap fuse group membership alternative topology pergroup all-to-all pinging overlay improves security all-to-all pinging robust dropped notification attacks members member relies node forward failure notifications topology requires messages group size significantly per-group spanning tree topology added benefit all-to-all topology worst-case failure notification latency reduced pinging interval final topology central server toping allnodes thismay beanappropriate topology fuse data center environment security standpoint server represents single point trust easier secure larger collection machines server compromised attacks launched fuse group system security guarantees allto-all pinging topology settings span multiple administrative domains single trusted server scalability topology limited fuse traffic passes server bottleneck large number fuse participants load group member minimal group member pings central server ping interval implementation key architectural choice faced implementing fuse route fuse messages overlay paths route messages directly group members topology spanning trees overlay routes path failures involving delegates dealt ways option signal failure fuse groups path advantage implementation simplicity significant source false positives chose option attempt repair liveness monitoring topology group repair succeed members group communicate directly repair routes failures involving delegates chose implement repair routing repair messages directly root group members overriding factor choice rapidity failure detection relying solely overlay routes require waiting overlay attempt repair signaling failure direct root member communication failures involving group members detected rapidly direct communication results latencies group creation application-signalled failure notifications overlay routing paths working scalability benefits shared spanning trees overlay remainder section describe functionality exposed skipnet overlay discuss details implementing fuse operation overlay functionality implementation fuse top skipnet overlay required features skipnet client applications messages routed overlay result client upcall intermediate overlay hop overlay routing table visible client functionality standard overlays fuse implementation re-uses overlay routing table maintenance traffic piggybacking sha hash bytes ping requests hash encodes fuse groups overlay link fuse butthepiggybacking approach amortizes messaging costs skipnet pings client upcall destination destination fuse layer examine piggybacked contents skipnet links monitored sides add additional pinging fuse layer ensure implementing fuse overlay properties require fuse perform additional pinging fuse overlay messages system delivered tcp inherit tcp retry congestion control behaviors tcp connection breaks timeout interpret node end unavailable group creation implement group creation group creation finish member node timer installed signal failure event future communication failures timers reset receipt liveness checking messages future communication failures converted failure notifications achieve low creation latencies creating node directly contacts member node parallel osdi symposium operating systems design implementationusenix association installchecking creategroup groupcreaterequest groupcreatereply figure group creation root node sends groupcreaterequest messages directly group members nodes reply directly groupcreatereply messages route installchecking messages overlay declaring creation succeeded replied creategroup called node node referred root generates unique group root creates entry list groups created associates timeout group creation attempt entry fuse list group members members root received reply root 
sends groupcreaterequest messages member nodes message sequence result group creation shown figure receiving groupcreaterequest member node installs fuse member state group unique sequence number initially incrementedbygrouprepair andtheidentityoftheroot concurrently sending groupcreatereply directly root member node routes installchecking message root overlay routing installchecking message set timer node encounters ensure liveness checks heard root receives groupcreatereply member group creation attempt timeout installs fuse root state group unique sequence number identities group members timer checking installchecking messages arrived member root removes group list groups created returns unique fuse client application groupcreatereply received node group creation attempt timeout group creation fails root returns failure fuse client application root attempts send failure notification fuse group group member notification hardnotification elaborate types notifications section finally root removes group list groups created prevents groupcreatereply messages receivedlaterfromcausinginstallationofstateforthefailed group creation installchecking message arrives delegate node installs fuse delegate state group fuse sequence number current time previous hop hop installchecking message timers hops node forwards message root timer receiving installchecking messages fireson root root attempts repair steady-state operation overlay node initiates ping routing table neighbor piggybacks hash list fuse ids node believes jointly monitoring neighbor neighbor receives message hash matches neighbor resets timers fuse neighbor pairs represented hash timer fuse node tree timers fires node sends softnotification message neighbor liveness checking tree fuse group cleans fuse delegate state group additionally timer firing member repair initiated node receives non-matching hash fuse ids neighbor nodes attempt reconcile difference exchanging listsof live fuse ids ifthey communicate remove liveness checking trees disagree timers reset communicate relevant checking state removed softnotification messages group creation race condition exists hash mismatches node received hop installchecking message resolve race anodeonlyremoves liveness checking tree neighbor exists tree existed longer grace period implementation period seconds notifications achieve simultaneous goals low notification latency resilience delegate failures fuse implementation distinguishes classes failures failures steady-state liveness checking trigger softnotification message distributed liveness checking tree alerts root repair needed prevents storm softnotifications root rest tree members receiving softnotification initiate repair directly root section failures group creation group repair trigger hardnotification create repair direct root-to-member communication delegate failures incur false positives note softnotifications failure notifications application layer inosdi symposium operating systems design implementation usenix association softnotifications figure explicitly signalled notification signalfailure called node node sends hardnotification root forwards hardnotifcation remaining group member generates softnotification clean liveness checking tree stead trigger repair actions failure repair actions lead hardnotification reflected application layer achieve low latency explicitly signalled notifications hardnotifications convey member generating hardnotification sends root turn forwards group members node receiving hardnotification immediately invokes application-installed failure handler root node additionally sends softnotifications proactively clean liveness checking tree message sequence shown figure node receiving softnotification message checks sequence number greater equal recorded sequence number specifiedgroup repair process message discarded sequence number current node forwards message neighbors liveness checking tree message originator removes delegate state group node member root initiates repair group repair member initiates repair sends root needrepair message fromtheroot ifthetimerfires tion fuse client application sends hardnotificationmessagetotheroot group root signalled repair needed paths needrepair directly member softnotification spreading liveness checking tree needrepair message needed remove potential source variability latency repair member receives softnotification good estimate root similarly receive softnotification notifications routed overlay breaks overlay routing require timeout side continue progress softnotification sea soft notification failed ping figure messages triggering group repair delegate sends ping delegate ping acknowledged sends softnotification member sends needrepair root alerted coordinates repair coordinated creation quence messages leading needrepair shown figure root attempts repair sends grouprepairrequest messages member members answer grouprepairreply messages send installchecking messages group creation state management root repair similar creation involving repair attempt table open repairs recorded state management member nodes repair message encounters member longer knowledge group fails signals hardnotification guarantees repairs suppress hardnotification reached members notifications garbage collect group state node nodes receiving grouprepairrequest increment group sequence number late-arriving softnotification messages trigger redundant repair usingthesamecriterion create failing root sends hardnotifications members signals application discussed section overlay paths achieve low steady-state message overheads repair scheme localizes repair traffic case worth optimizing transient overlay routing failures repairs frequent node consulting routing table learn hop installchecking message reduce message volume circumstances implement per-group exponential backoffs capped seconds frequency repairs repair generates additional network traffic shortly failure detected tcp serves regulate additional network load experimental evaluation evaluate fuse running top skipnet overlay network main techniques scalable virtual nodes running cluster workstations skipnet fuse implementations running osdi symposium operating systems design implementationusenix association live system simulator identical code base base messaging layer methodology configured skipnet overlay employ ping period base size leaf set size node overlay yielded average distinct neighbors node cluster evaluation router modelnet emulate wide-area internet-like network characteristics weran processesoneachofthe physical nodes total virtual nodes order emulate nodes running physically separate machines explicit state sharing processes communication processes forced pass modelnet motivating scenario choice router topology assignment link latencies bandwidths small large corporations multiple continents direct internet connections live simulator experiments run mercator topology nodes links node assigned autonomous systems ass links ass topology assigned links link assigned link latency uniformly milliseconds assigned bandwidth mbps link assigned linklatency uniformlybetween milliseconds assigned bandwidth mbps led round-trip latencies median milliseconds significant heavy-tail figure curve labeled simulator shows cdf end-to-end latencies paths crossing links heavy-tail topology simulator ran experiments nodes model system scale larger deployment simulator latency values model bandwidth constraints event notification service workload section creates large number groups average group size scaling node overlay simulator maximum group size observed results informed fuse design determined choice evaluation parameters evaluated fuse workload groups ranging members calibration simulator modelnet experiment performed rpc message exchanges randomly chosen nodes node overlay network calibrate wide-area network topology model experiments make results obtained simulation comparable obtained running live cluster modelnet rpc time milliseconds log scale cluster rpc cluster rpc simulator figure rpc latencies towardsthatend wemeasured rpcsonbothour cluster simulator figure shows cumulative distribution function cdf rpc times measured sets rpcs obtained simulator kinds rpc times obtained cluster cluster code caches tcp connections pairs nodes communication pair nodes takes longer subsequent communications due additional time required connection establishment experiment performs back-to-back rpcs pairs nodes 
cluster reports durations rpc incur connection setup overhead figure values rpc cluster closely track simulator confidence simulator modelnet faithfully modeling chosen mercator topology fuse group creation latencies measured time cluster required create fuse group function group size group group sizes created groups size figure shows results themoremembers greater chance including nodes significant network distance root node group note instance groups size percentile time significantly larger median groups size percentile median percentile close members chance encountering slow communication paths high simulator evaluated node node system simulated creation times pattern cluster tended half long reasons simulated actual rpc times figure connections differed factor group creation times simulated node node osdi symposium operating systems design implementation usenix association group size percentile median percentile figure latency group creation simulated system expected creation messages routed directly root members affected length overlay routes failure notification latencies latency failure notification actual failure comprised parts time node system decide failure occurred time fuse propagate information group members time detect failure depends type failure node link failures monitored perform experiments characterize costs experiment investigates latency explicitly signaled failures investigates latency failure notification nodes crash measure notification latency explicitly signaled failures chose group member random set groups group creation experiments explicitly signal failure figure shows notification times create notify cycles expected notification latencies significantly lower group creation latencies improvement result factors messaging layer maintains cache recently tcp connections opening tcp connection time message experiment failure notifications travel cached tcp connections connections recently perform group creation failure notifications require oneway message round-trip creation blocks root members contacted single node group network incontrast notificationtakeseffectateachmemberassoonasthenotification arrived maximum notification time observed fuse group simulation results nodes matched results obtained cluster alsoinvestigated scaling behavior inthesimulator andwe found expected result notification times increase node overlay failure notifications effect member arrival figure mediannotificationlatencyshowsadependenceonthegroup size rise curve group sizes due group size percentile median percentile figure latency signaled notification extra forwarding hop needed notifications generated non-root node size notifications travel member root sizes notifications travel member root members additional increase notification latency groups size reflects latency added messaging layer root implementation xml-based messaging system high message serialization overhead reducing overhead straightforward focus work ran micro-benchmarks determined running virtual nodes physical machine adds approximately overhead message base overhead message send including xml serialization measure latency failure notification nodes crash performed experiment created fuse groups size disconnected network physical machines disconnecting virtual nodes fuse groups contained disconnected virtual nodes members remaining members groups received failure notifications total notifications figure shows distribution notification times times components time ping failed node attempted timeout ping time member learn failed ping time subsequent repair attempt fail actual notification time repair failed ping interval minute ping timeout seconds total ping timeout latency uniformly distributed seconds member failed root times minutes repair response root failed members time minute repair response leading shorter failure notification times figure shows notification times deduce ping repair timeouts dominate failure notification times event node crash steady state load churn set experiments performed measured amount background network traffic present due overlay network due fuse groups osdi symposium operating systems design implementationusenix association minutes figure combined latency ping timeout repair timeout failure notification ping repair timeouts dominate factors churn churn churn fuse figure costs overlay churn overlay network liveness checking cluster observed background network traffic loads messages minute interval fuse groups present messages subsequent minute interval fuse groups members present experiments verified absence node failures fuse groups imposed additional messages imposed overlay additional cost byte hash piggybacked ping nodes entering leaving overlay network referred toas churn overlay paths liveness checking nodes fuse group maychange causing thelivenesschecking statetohaveto reconstructed overlay churn false positives fuse fuse generate higher network load experimentally quantified network load imposed high churn rate fuse groups designed churn experiment aggressive rate arrivals departures stablenodesthatremainedaliveforthedurationoftheexper- iment nodes killed restarted average churning nodes alive time rate churn resulted systemwide average half-life minutes factor higher rate churn observed study overnet peer-to-peer system created total fuse groups size stable nodes average stable node member fuse groups measured cpu loads network message traffic cpu loads show noticeable increases durroute loss rate median loss median loss median loss figure cdfs per-route loss rates per-link loss rates group size median loss median loss median loss median loss figure group failures due packet loss failures occurred loss rates ing overlay churn basis comparison stable node overlay fuse groups generates load messages -node overlay network churn results average live nodes time generates load messages increase due costs repairing overlay adding -member fuse groups churning overlay results total messages increase cost churning overlay fuse groups results displayed figure additional load caused group repair traffic section proportional number groups times averagegroupsize whiletheoverlay routesareinflux liveness checking paths installed repair mechanism triggered repeatedly reduce fuse overhead churn employing proactive repair strategy overlay level high rate churn overlay routing fail case fuse liveness checking traffic proportional number groups times average group size reaching steady state root node periodically ping group member directly grouprepairrequest message false positives studied robustness implementation false positives stemming sources delegate failures unreliable communication links previously churn node crash experiments groups experienced delegate failures osdi symposium operating systems design implementation usenix association perform repairs experiments notifications delivered groups member crashed delegate failures led false positive understand impact potentially unreliable links fuse ran set experiments modelnet configured probabilistically drop packets perlink basis routes topology ranged hops median figure shows cdfs per-route loss rates experiments varied per-link loss rates cdfs labeled median end-to-end loss rates created fuse groups sizes enabled losses allowed system run additional minutes figure shows number fuse group failures observed loss rate false positives occur per-route median loss rates tcp masks drops lower loss rates retransmissions higher loss rates groups fail tcp desired fuse implementation continued monitor links conditions alternative messaging layer employed conclusions paper presented fuse lightweight distributed failure notification facility fuse abstraction fuse group targeted thefusegroupabstraction distributed application developers simple programming paradigm handling failure failure notifications fail failures respect groups individual members significant advantage fuse abstraction detecting failures shared responsibility fuse application applications implement definitions offailure extending applicability failure 
management services implemented fuse peer-to-peer overlay network evaluated behavior cluster workstations variety conditions including node failures packet loss overlay churn evaluation showed fuse implementation lightweight implementation scales reusing overlay maintenance traffic perform liveness checking fuse groups imposing additional traffic absence node failures fuse abstraction implemented efficiently find application domains consensus-style protocols proven heavyweight fuse simplifies complex task handling failures distributed applications experiences building scalable reliable application-level multicast groups fuse fuse made task easier fuse likewise simplify construction distributed systems acknowledgements ken birman jon howell patricia jones keith marzullo stefan saroiu amin vahdat geoff voelker comments earlier drafts paper ashwin bharambe insight integrating modelnet wealsothankourshepherd gregganger anonymous reviewers insightful feedback aguilera chen toueg heartbeat timeout-free reliablecommunication workshop distributed algorithms pages anker breitgand dolev levy congress connection-oriented group-address resolution service technical report hebrew jerusalem bhagwan savage voelker understanding availability intl workshop peer-to-peer systems iptps bhagwan tati cheng savage voelker totalrecall system support automated availability management nsdi birman van renesse vogels adding high availability autonomic behavior web services intl conference software engineering icse birman reliable distributed systems technologies web services applications springer-verlag scheduled cabrera jones theimer herald achieving global event notification service hotos callon rfc osi is-is routing tcp dual environments dec candea delgado chen sun fox automatic failure-path inference generic introspection technique software systems ieee workshop internet applications wiapp castro liskov practical byzantine fault tolerance osdi chandra hadzilacos toueg weakest failure detector solving consensus podc chandra toueg unreliable failure detectors reliable distributed systems journal acm chen accardi kiciman lloyd patterson fox brewer path-based failure evolution management nsdi chen kiciman accardi fox brewer runtime paths macroanalysis hotos chen kiciman fratkin fox brewer pinpoint problem determination large dynamic systems dsn chen zheng lloyd jordan brewer statistical learning approach failure diagnosis intl conference autonomic computing icac osdi symposium operating systems design implementationusenix association chen toueg aguilera quality service failure detectors dsn dabek zhao druschel stoica common api structured peer-to-peer overlays intl workshop peer-to-peer systems iptps das gupta motivala swim scalable weakly consistent infection-style process group membership protocol dsn dunagan harvey jones theimer wolman subscriber volunteer trees polite efficient overlay multicast trees http research microsoft herald papers svtree pdf submitted felber defago guerraoui oser failure detectors class objects intl symposium distributed objects applications fischer lynch paterson impossibility distributed consensus faulty process journal acm gupta chandra goldszmidt scalable efficient distributed failure detectors podc halpern moses knowledge common knowledge distributed environment journal acm harvey jones saroiu theimer wolman skipnet scalable overlay network practical locality properties fourth usenix symposium internet technologies systems usits kurose towsley comparison hard-state soft-state signaling protocols sigcomm labovitz ahuja experimental study internet stability wide-area backbone failures faulttolerant computing symposium ftcs lamport time timeout faulttolerant distributed systems acm toplas lamport part-time parliament acm tocs liskov distributed programming argus cacm mahajan wetherall anderson understanding bgp misconfiguration sigcomm mogul brakmo lowell subhraveti moore unveiling transport hotnets moy ospf anatomy internet routing protocol addison-wesley ratnasamy francis handley karp shenker scalable content-addressable network sigcomm rodeheffer schroeder automatic reconfiguration autonet sosp rowstron druschel pastry scalable distributed object location routing large-scale peer-to-peer systems middleware rowstron kermarrec castro druschel scribe design large-scale event notification infrastructure intl workshop networked group communications stelling lee foster von laszewski kesselman fault detection service wide area distributed computations high performance distributed computing stoica morris karger kaashoek balakrishnan chord scalable peer-to-peer lookup service internet applications sigcomm tangmunarunkit govindan shenker estrin impact routing policy internet paths infocom vahdat yocum walsh mahadevan kosti chase becker scalability accuracy large-scale network emulator osdi van renesse minsky hayden gossipbased failure detection service middleware vogels world wide failures acm sigops european workshop vogels ws-membership failure management web-services world intl world wide web conference vahdat consistent automatic replica regeneration nsdi zhao kubiatowicz joseph tapestry infrastructure fault-resilient wide-area location routing technical report ucb csd- berkeley osdi symposium operating systems design implementation usenix association 
demos development distributed operating system barton miller david presotto computer sciences department bell laboratories wisconsin west dayton street mountain avenue madison wisconsin murray hill jersey michael powell dec western research laboratory hamilton avenue palo alto california demos development distributed operating system summary demos operating system moved super computer simple addressing structure network microcomputers transformation significant semantics original demos existing demos programs run demos demos simplified structure primitive objects functions objects structure demos links processes major contributors simplicity made produce demos involved internal structure link modification parts kernel limited system processes keywords distributed operating system message-base demos links network introduction demos operating system began cray computer transitioned computing environments current home collection processors connected network distributed version demos demos demos successfully moved substantially architectures providing consistent programming environment user basic demos system paper concentrates details demos relevant demos operation distributed environment semantics user interface demos changed significantly original demos system demos runs collection loosely coupled processors processor speed memory architecture architecture differ substantially cray spite differences program runs demos environment run demos goals demos project goal provide software base osmosis distributed systems project demos clean message interface structured system modified easily system base experiments operating systems distributed systems process migration reliable computing distributed program measurement goal experiment design distributed operating system goal mechanisms adapt easily distributed environment maintaining high degree network transparency areas important describing development demos area collection semantic structures demos survive transitions environments unscathed semantic structures visible users system demos simple uniform communication model communications model based links resource naming abstraction management demos processes links means interaction area made system mechanisms reflect hardware environments mechanisms lowest level kernel responsible hardware resource management major demos network virtual memory addressing structure network interface added demos natural extension link-based communications goal make presence network visible research supported national science foundation grant mcsthe state california micro program defense advance research projects agency dod arpa order monitored naval electronic system command contract -cosmosis stands operating system making operating system implementation studies system users original demos implemented memory space base bounds registers swapping demos implemented collection machines paged segmented address space demand paging area system services server processes function distributed environment form replication service performance reliability reasons demos project involved progression activities leading running distributed environment moved dec vax running simulation unix operating system involved writing code generator vax instruction set model compiler model morris manual initial work intermachine communications started vax version demos code generator written testing began network dual processor machines years network subsystem virtual memory process migration added basic functioning system experiments started fault tolerant computing distributed program measurement tools links processes basic units program constructed computational elements processes communications paths join elements links link-based communication forms lowest level operating system semantics links distributed parts demos operate distributed environment change links link one-way message channel user process system process kernel process table links table maintained kernel link table identifies complete set communications 
paths process single interface remainder system links protected objects naming mechanism resources controlled centralized resource manager role links provide function capabilities features demos links facilitate implementation distributed environment features one-way communication paths process address structure delivertokernel attribute link data areas variables specification communication mechanism synchronous asynchronous operations buffered unbuffered messages unidirectional bidirectional communication paths decision unidirectional bidirectional paths significant issue distributed environment one-way communication path requires state information sender end link results needing potentially smaller amount needed link table space reduction link table space result asymmetry client server relationship client possess link services wishes request servers hold link client period time request received reply process link server size server link table considerably smaller total number processes interprocess communication routines including link table data structures kernel require information sender end link connection network protocol routines information ends network connection machine-to-machine basis opposed process-to-process basis one-way communication paths require creation reply links request server extra performance cost avoided call kernel call composite operations createlink send receive call kernel call reduces number context switches kernel performance cost kernel calls performance section call kernel call commonly client server processes short messages part synchronous exchange call longer data transfers handled special function description movedata kernel call one-way communication paths simplify moving migration process process moves links holds require updating links held processes point moving process updated time referenced move link considered global address process sense capabilities global address implemented providing process unique identifier figure unique identifier solves part addressing problem identifies process locate machine field included address link machine address initially set process current location process moved updated message link discusses effectiveness machine field process location links mechanism needed locate process communications performed links control functions process performed links control functions links delivertokernel attribute messages links normal message delivery mechanism arrival destination machine passed kernel machine insures control messages delivered correct kernel follow process movements machine machine demos incorporates idea intermachine address intermachine communications lowest level kernel operating system communications level ignore machine boundaries desired ignoring machine boundaries simplifies implementation functions remote paging virtual memory section demos design contrast accent system local naming kernel remote messages interpreted agent process kernel process creates link pointing area memory read written holder link data transferred link data area movedata kernel call link data areas provide potential performance optimization local remote large data transfers local data transfers quickly mapping part process address space process paged virtual memory architecture alignment link data area page boundary read transfer mapping memory page reading process address space process modify page copy made copy-on-write idea tenex operating system page mapping implementation optimization effect demos semantics remote transfers large data blocks efficiently link data areas special protocol intermediate acknowledgments transfers data large blocks restrictions order delivery sections data area transferred kernel data segments fixed length messages achieve effect message send receive locally remotely control functions initiating file read operation movedata large data transfers copying data read file processes demos process encapsulated link table process abilities determined links holds distinctions system privileged processes standard user processes links process possesses process privileges determined links holds section data area labeled offset data area sections arrive put proper place 
demos supports form lightweight process kernels called kernel processes processes lie kernel address space minimum state kernel processes long term asynchronous activities performed demos kernels large data transfers device interfacing process migration kernel processes remain dormant message received kernel kernel process kernel message channels kernel processes state static glocal variables maintained kernel kernel processes private stack local variables kernel process determines functions contents message state selected variables kernel processes thought transactions performed kernel response messages flexible structure real processes reside kernel address space kernel processes maintain local state dynamically created destroyed network messages links original demos obey properties messages process arrive order messages lost messages duplicated extended demos work network environment changing communication semantics maintain properties adding links address processes machines section describes original demos network protocols support extension extensions demos process single processor demos unique identifier demos identifier made unique network wide appending unique machine process created process retains identifier migrate machines process visible processes exist links processes processes links manipulated message kernel processes changed support space network-wide process names sufficient identify target message processes migrate machines names describe location target process machine field added process contained link field hint routing messages target process machine sender hint field destination machine time machine field process migrates processes migrate frequently thrashing easily result process migrated message remote process update back sender informing migrated process location updates machine fields sender machine result slight delay messages complete description method updating field network protocol implemented special-purpose light-weight protocol based demos model interprocess communication similar approach locus distributed operating system protocol derives simplicity type abstractions model programming language abstractions place thought layers network protocols interface abstractions procedure call data passes abstractions standard model parameter passing method avoids wrapping unwrapping data layers protocol header abstractions interface network media demos physical device abstractions network fsm abstractions remote processor object abstractions figure illustrates abstractions abstraction instances paper call instances objects noted object passive actions initiated calls made executing process functions abstraction exports device abstraction hides idiosyncrasies devices rest system providing uniform interface devices device object exists device attached demos processor functions exported device abstraction write block read block return device status return list pending interrupts device abstractions exist -bit parallel interfaces serial lines baud vax host machines simulated acknowledging ethernet network finite-state machine nfsm abstraction adds network semantics physical device nfsm exists device network communications nfsm circular list remote processor objects list represents processors accessible interface nfsm finite-state machines input output input fsm reads blocks devices assembles blocks network messages calls remote processor object act uncorrupted network messages output fsm calls remote processor object list message send exists writes device nfsm transitions effected calls processes nfsm object instance nfsm abstraction kernel process continuously scans devices clock interrupts calls nfsm objects notify events remote processor objects call nfsm objects notify network messages waiting remote processor abstraction guarantees invariance demos communication semantics network remote processor object exists processor processors network message kernel encounters message destined remote processor calls object pass message end-to-end acknowledged window protocol ensure ordered delivery messages destination machine reconfiguration failure binding network nfsm objects objects consists list objects held nfsm event failure network reconfiguration lists updated kernel network process rebinding completely transparent conversations objects protocol objects end-to-end acknowledged redundancy connectivity network long path message source destination demos communications survive network failure human intervention node failures performance enhancement performance enhancement identified types messages guaranteed unguaranteed unguaranteed messages timely messages pointless resend messages information inherently redundant message traffic statistics routing information messages form moderately large percentage normal message traffic messages bypass checks guarantee properties demos messages corruption detection handed directly target processes general messages correspond messages broadcast processors ethernet-based system routing information time day virtual memory implementation demos virtual memory base bounds registers swapping simple design driven largely cray architecture demos modified support complex memory management architectures making segmentation paging network communications machine architecture demos runs dual processor system zilog processor running megahertz system supports virtual address space k-byte segments processor runs demos kernel access memory -bit unmapped addresses processor runs demos processes dual processor system cpu recover page fault user processor fault activity suspended kernel processor services fault user processor continue execution kernel processor accesses memory directly assumed address fault incorrect operation system design complete description handshake processors address translation maps figure user processor contained special hardware registers registers segment maps processes page maps segments maps set kernel processor memory maps considered scarce resource processes executing maps support maps allocated lru policy translation hardware dirty bits restricting page replacement algorithms easily implemented dual processor configuration performance advantages disadvantages advantages ability kernel user processors compute independently user processor running kernel processor performing operations network communications machines store-and-forward communications receipt packet disturb user computation disadvantage demos dual architecture page fault activity user processor suspended means runable processes run simulation studies demos system shown current cpu memory disk speeds typical utilization user processor heavily loaded system faster cpu slower paging devices remote paging substantially reduce fortunately microprocessor industry producing processors recover page faults page fault handling page fault handler demos kernel links link data areas transfer pages disk driver service page fault kernel creates link data area points page read written disk data area address kernel address space physical memory address size data area page size actual transfer requesting service disk driver performs movedata operation page transfers link data areas movedata function works machine boundaries remote demand paging simple addition permits demos multiple machines sharing paging device location paging area machine established time machine booted remote paging implemented machines disks system tasks demos system server processes process management memory management file services clock services naming connection services initial version demos required system processes operational significant change lowlevel process manager communicate multiple kernels resource naming switchboard demos level version flat space provided original demos demos single server switchboard aids establishing connections processes telephone operator sets call people process link switchboard announce world request link process announced announce process sends message switchboard link assign link process sends request message switchboard switchboard returns link announced process switchboard processes started simple provide link switchboard process 
types switchboard processes global local global switchboard location broadcast network part network routing protocol processors added network told switchboard location local switchboards exist processor network locations broadcast process sends message switchboard link message kernel sends message local switchboard exists global switchboard local switchboard requests local switchboards global switchboard effectively implements level naming hierarchy hierarchical design solves problems scaling space locating switchboard scaling problem caused potentially larger number names resources multiple machine environment locating problem caused switchboard process machine hierarchy implementation publishing experiment create separate instance system programs recording node recording node act independently rest system giving local switchboard exact copy standard global switchboard process memory management demos memory manager performs low-level memory policy process management decisions memory manager link process memory kernel process demos kernel level existence multiple machines visible time machine booted posts link switchboard memory manager outstanding request switchboard links aware machines entering re-entering network multiple memory managers exist partitioning machines memory manager partition partitions correspond allocation switchboards high-level process management functions performed process manager process process manager talk memory manager processes number memory managers process manager talks function number machines control replication level fault tolerance administrative divisions file system demos file system ran change demos version file system processes distributed machines performance reasons buffer manager disk interface processes typically machine device control multiple disk interface buffer manager pairs multiple disks connected machines disks controlled single pair performance performance major design issue demos important report measure system performance results evaluating hardware software operation cost null kernel call create destroy link disk block read write bytes operation local cost remote cost send receive -byte message send receive -byte message table basic performance measures table summarizes basic performance results demos implementation result time null kernel call elapsed time user request kernel trap instruction kernel returns control user process interval includes time enter exit kernel synchronization user kernel processors part synchronization time due machine interrupts kernel call trap instruction user-to-kernel processor interrupt kernel-to-user processor interrupt result table cost pair simple kernel calls create destroy link result reflects cost calls kernel time create destroy link cost dominated user-kernel processor synchronization time result table cost reading writing -byte disk block time needed kernel processor access transfer data disk disk read write time controlled speed parallel interface disk network connections interface maximum speed bytes bits results table describe communication performance demos results show cost send receive pairs kernel calls local times measured process repeatedly sending message receiving process run machine elapsed time measured iterations remote times measured process machine sending message process machine message returned processes run respective machines elapsed time measured iterations local cost sending null message reflects general non-network overhead transmitting message cost remote message dominated data transfer time parallel interface points determine demos performance processors run megahertz clock compared processors today running megahertz kernel-user processor synchronization needed separate processor required handle page faults interprocessor handshaking increases time enter exit kernel increases time process kernel calls page faults parallel interfaces operate bit compared current megabit ethernets speed parallel interfaces affects communication performance special effort made tune demos kernel performance informal code inspections identified areas optimizations in-line procedure expansion substantially improve performance areas publishing distributed recovery demos test idea process recovery publishing publishing model crash recovery distributed computing environment recovery mechanism completely transparent failed process processes interacting publishing intended message-based systems preferably centralized communications medium bus ring ethernet centralized recorder reliably stores messages transmitted checkpoint recovery information detects failure recorder restart affected processes checkpoints recorder subsequently resends process messages time checkpoint ignoring duplicate messages demos provided excellent environment test publishing state processes demos defined checkpointing restarting processes simple operation unlike systems support message passing communication processes demos message based requests kernel sending receiving messages messages inter-process communications captured recorder prototype version implemented demos demonstrates error recovery transparent user processes centralized network distributed program performance tools demos operating system test bed set distributed program performance tools message semantics demos natural environment construction multiple process programs spread machines performance tools designed measure things parallelism program structure communications levels conclusion success demos implementation due simplicity organization system demos message passing basic structuring tool operating system users message passing implemented lowest level system system users features demos design prove noteworthy feature link data area glance data areas escape mechanism processes share memory viewed separating common forms communications short control messages bulk data transfers messages short control functions link data areas large data transfers data areas contribute ease implementing process migration remote paging feature delivertokernel links links provide control processes machine boundaries requiring minimum additional mechanism cases control messages process directed correct machine standard message routing feature structure intermachine protocols type abstractions strongly typed language layers protocol constructed small performance penalty interactions abstractions layers protocol difficult design operating system run distributed environment main point avoid uncontrolled sharing data components operating system demos component kernel system processes operating system maintains state local component links means state maintained component process opens file receives link represents resource open file state state open file maintained file server process user process moves requirement link valid process location needed local system state difficult distribute operating system make reliable distributed system difficult reliable network communication protocol system handle machine crashes network failures resulting loss system function difficult place parts file service machines placement increases probability losing part file service design service survive failures difficult solution provide system-wide recovery mechanism publishing process solves recovery problem system continue failure baskett howard montague task communcations demos proc sixth symp operating sys principles november baskett pascal virtual memory based design station proc spring compcon bobrow burchfiel murphy tomlinson tenex paged time sharing system pdpcommunications acm march carter osmosis project control system dual processor architecture report california berkeley december cheriton zwaenepoel distributed kernel performance diskless workstations proc ninth symp operating sys principles october fabry capability-based addressing communications acm july miller performance characterization distributed programs dissertation technical report ucb computer science dpt california berkeley miller sechrest macrander distributed program monitor berkeley unix software practice experience february morris manual model programming language los alamos scientific laboratory los alamos mexico february powell demos 
file system proc sixth symp operating sys principles november powell miller process migration demos proc ninth symp operating sys principles october powell presotto publishing reliable broadcast communication mechanism proc ninth symp operating sys principles october rashid robertson accent communication oriented network operating system kernel proc eighth symp operating sys principles december walker popek english kline thiel locus distributed operating system proceedings symp operating system prin operating systems review november process location change set process creation local unique creating machine unique process machine figure link process address fsm abstraction device abstraction network kernel abstraction processor process process physical remote abstraction remote processor figure demos message paths network process entries maps entries maps page frame offset physical address virtual address page map seg page offsetword status segment map figure address translation maps 
part-time parliament leslie lamport digital equipment corporation recent archaeological discoveries island paxos reveal parliament functioned peripatetic propensity part-time legislators legislators maintained consistent copies parliamentary record frequent forays chamber forgetfulness messengers paxon parliament protocol implementing state machine approach design distributed systems categories subject descriptors computer-communication networks distributed systems network operating systems operating systems reliability faulttolerance computer applications administrative data processing government general terms design reliability additional key words phrases state machines three-phase commit voting problem island paxos early millennium aegean island paxos thriving mercantile center wealth led political sophistication paxons replaced ancient theocracy parliamentary form government trade civic duty paxos devote life parliament paxon parliament function legislators continually wandered parliamentary chamber problem governing part-time parliament bears remarkable correspondence problem faced today fault-tolerant distributed systems legislators correspond processes leaving chamber corresponds failing paxons solution interest computer scientists present short history paxos parliament protocol shorter discussion relevance distributed systems author address systems research digital equipment corporation lytton avenue palo alto permission make digital hard copy part work personal classroom granted fee provided copies made distributed profit commercial advantage copyright notice title date notice copying permission acm copy republish post servers redistribute lists requires prior specific permission fee acm confused ionian island paxoi corrupted paxos acm transactions computer systems vol pages paxon civilization destroyed foreign invasion archeologists recently begun unearth history knowledge paxon parliament fragmentary basic protocols ignorant details details interest liberty speculating paxons requirements parliament primary task determine law land defined sequence decrees passed modern parliament employ secretary record actions paxos remain chamber session act secretary paxon legislator maintained ledger recorded numbered sequence decrees passed legislator entry olive tax drachmas ton believed decree passed parliament set tax olives drachmas ton ledgers written indelible ink entries changed requirement parliamentary protocol consistency ledgers meaning ledgers contradictory information legislator fis entry lamps olive oil lamport acm transactions computer systems vol ledger legislator ledger entry decree legislator entry ledger decree learned decree passed consistency ledgers sufficient trivially fulfilled leaving ledgers blank requirement needed guarantee decrees eventually passed recorded ledgers modern parliaments passing decrees hindered disagreement legislators case paxos atmosphere mutual trust prevailed paxon legislators pass decree proposed peripatetic propensity posed problem consistency lost group legislators passed decree painting temple walls forbidden left banquet group legislators entered chamber knowing happened passed conflicting decree freedom artistic expression guaranteed progress guaranteed legislators stayed chamber long time paxon legislators unwilling curtail activities impossible ensure decree passed legislators guarantee chamber aides act promptly parliamentary matters guarantee allowed paxons devise parliamentary protocol satisfying progress condition majority legislators chamber entered left chamber sufficiently long period time decree proposed legislator chamber passed decree passed ledger legislator chamber assumptions requirements parliamentary protocol achieved providing legislators resources legislator received sturdy ledger record decrees pen supply indelible ink legislators forget left chamber write notes back ledgers remind important parliamentary tasks entry list decrees changed notes crossed achieving translating progress condition rendered paxon word mad vriti majority legislators alternative translations word proposed discussed section tragic incident legislator tvy developed irreversible amnesia hit head falling statue chamber part-time parliament acm transactions computer systems vol progress condition required legislators measure passage time simple hourglass timers legislators carried ledgers times read list decrees note crossed ledgers made finest parchment important notes legislator write notes slip paper lose left chamber acoustics chamber poor making oratory impossible legislators communicate messenger provided funds hire messengers needed messenger counted garble messages forget delivered message deliver legislators served messengers devoted part time parliamentary duties messenger leave chamber conduct business taking six-month voyage delivering message leave forever case message delivered legislators messengers enter leave time inside chamber devoted business parliament remained chamber messengers delivered messages timely fashion legislators reacted promptly messages received official records paxos claim legislators messengers scrupulously honest strictly obeyed parliamentary protocol scholars discount propaganda intended portray paxos morally superior eastern neighbors dishonesty rare undoubtedly occur mentioned official documents knowledge parliament coped dishonest legislators messengers evidence uncovered discussed section single-decree synod paxon parliament evolved earlier ceremonial synod priests convened years choose single symbolic decree centuries synod chosen decree conventional procedure required priests present commerce flourished priests began wandering chamber synod progress finally protocol failed synod ended decree chosen prevent repetition theological disaster paxon religious leaders asked mathematicians formulate protocol choosing synod decree protocol requirements assumptions essentially parliament sequence decrees ledger decree resulting synod protocol parliamentary protocol section mathematicians derived synod protocol series steps proved results showing protocol satisfying constraints lamport acm transactions computer systems vol guarantee consistency progress preliminary protocol derived directly constraints restricted version preliminary protocol provided basic protocol guaranteed consistency progress complete synod protocol satisfying consistency progress requirements obtained restricting basic protocol mathematical results section protocols informally sections formal description correctness proof basic protocol appears appendix mathematical results synod decree chosen series numbered ballots ballot referendum single decree ballot priest choice voting decree voting ballot set priests called quorum ballot succeeded priest quorum voted decree formally ballot consisted components qualified set finite set dec decree voted qrm nonempty set priests ballot quorum vot set priests cast votes decree bal ballot number ballot successful iff qrm vot soa successful ballot quorum member voted ballot numbers chosen unbounded ordered set numbers bal bal ballot ballot order ballots conducted ballot place earlier paxon mathematicians defined conditions set ballots showed consistency guaranteed progress set ballots place satisfied conditions complete history synod protocol discovery modern computer scientists paxon mathematicians describe elegant logical derivations bore resemblance algorithms derived mathematical results theorems section precede protocol discovered mathematicians response request protocol attempting prove satisfactory protocol impossible modern nations paxos fully grasped nature athenian democracy paxon mathematicians remarkably advanced time knowledge set theory liberty translating paxons primitive notation language modern set theory priests quorum voted paxon mathematicians found easier convince people protocol correct proof allowed priest vote ballot part-time parliament acm transactions computer systems vol conditions simple stated informally ballot unique ballot number quorums ballots priest common condition complicated paxon manuscript contained confusing statement ballot priest quorum voted earlier ballot decree equals decree latest earlier ballots interpretation cryptic text aided manuscript pictured figure illustrates condition set ballots synod consisting priests set ballots ballot 
set voters subset priests quorum names enclosed boxes ballot number decree quorum priests set voters condition form condition ballot conditions ballots figure ballot number earliest ballot condition ballot trivially true ballot quorum members voted earlier ballot condition ballot trivially true fig paxon manuscript showing set consisting ballots satisfies conditions explanatory column headings added lamport acm transactions computer systems vol member ballot quorum vote earlier ballot voted ballot number condition requires ballot decree equal ballot decree successful ballot members ballot quorum priest vote earlier ballot earlier ballot voted ballot earlier ballot voted ballot latest earlier ballots ballot condition requires ballot decree equal ballot decree members ballot quorum earlier ballot voted number priest voted ballots voted ballots latest earlier ballots number condition requires ballot decree equal ballot decree state formally requires notation vote defined quantity consisting components priest pst ballot number bal decree dec represents vote cast priest pst decree dec ballot number bal paxons defined null votes votes bal dec blank ballot number blank decree priest defined null unique null vote pst paxon mathematicians defined total ordering set votes part manuscript definition lost remaining fragment votes ifv bal bal relative order defined bal bal set ballots set votes votes defined consist votes pst vot bal bal dec dec ifp priest ballot number maxvote defined largest vote votes cast bal ortobenull vote null smaller real vote cast means maxvote largest vote set votes pst bal null nonempty set priests maxvote defined equal maximum votes maxvote conditions stated formally bal bal paxon mathematical symbol meant equals definition part-time parliament acm transactions computer systems vol qrm qrm maxvote bal qrm bal dec maxvote bal qrm dec definition maxvote depends ordering votes implies maxvote dec independent votes equal ballot numbers ordered show conditions imply consistency paxons showed imply ballot successful ballot decree lemma hold qrm vot bal bal dec dec proof lemma ballot set ballots decree bal bal dec dec prove lemma suffices show qrm vot empty paxons gave proof contradiction assumed existence qrm vot obtained contradiction choose bal min bal proof exists nonempty finite bal bal proof definition vot qrm proof semantically-smart disk systems muthian sivathanu computer science anna india computer science wisconsin-madison dissertation submitted partial fulfillment requirements degree doctor philosophy computer sciences wisconsin madison committee charge andrea arpaci-dusseau co-chair remzi arpaci-dusseau co-chair david dewitt mark hill mikko lipasti iii copyright muthian sivathanu rights reserved abstract semantically-smart disk systems muthian sivathanu robust efficient storage data prerequisite current future computer systems pace rapid technology increasing demands applications users alike storage systems evolve interesting ways storage systems today problem range functionality provide fundamentally limited presence significant processing power main reason limitation storage systems communicate world narrow block-based interface today lack higher-level semantic understanding thesis proposes solution fundamental problem presents class storage systems called semantically-smart disk systems sds disk systems capable providing classes functionality exploiting information system file system database management system sds carefully monitoring low-level stream block reads block writes storage system sees inferring higher-level behaviors system importantly sds existing block-level storage interface taking pragmatic approach enables ready deployment existing computing environments thesis present variety techniques sds track semantic information underneath modern file systems demonstrating transform request stream source high-level information underlying disk system demonstrate utility semantic information disk system presenting improvements availability security performance storage built storage system exhibits availability multiple failures keeping semanticallyvi meaningful data case study show semantic knowledge storage system enable reliable secure deletion data innovations impossible implement current storage infrastructure acquisition careful semantic information finally present logic framework reasoning file systems interaction storage systems logic prove properties inference semantically-smart disk system parents viii acknowledgements indebted advisors andrea arpaci-dusseau remzi arpaci-dusseau making graduate study experience amazingly enjoyable fruitful intention pursuing joined work andrea remzi year sufficient convince great experience retrospect glad stayed invaluable guidance constant feedback helped mature significantly researcher years weekly meetings fun part unflinching enthusiasm sense humor professor remzi andrea ideal model advisors strive emulate mark hill david dewitt mikko lipasti serving thesis committee providing valuable feedback suggestions mark hill david dewitt great support job search provided wonderful feedback practice job talk plenty insights discussing job options ben liblit suman banerjee sharing insights job search process summer internships graduate school career enjoyable terms providing varied perspectives industrial research mentors colleagues internships mahesh kallahalla ram swaminathan john wilkes labs honesty young ibm almaden anurag acharya google madhu talluri yousef khalidi microsoft wonderful people work learned lot internships anurag yousef support insights job search fortunate wonderful colleagues work nitin agrawal lakshmi bairavasundaram john bent nathan burnett tim denehy brian forney haryadi gunawi todd jones james nugent florentina popovici vijayan prabhakaran group meetings hallway discussions coffee ice-cream breaks enjoyable stay madison made pleasant fun-filled great set friends gogul lakshmi koushik madhu nitin prabu pranay ram ravi sekar veeve venkat venkatanand vijayan vinod great time words suffice express indebtedness gratitude parents single largest contributors accomplishments means boundless love constant support guidance encouragement actions deeply thankful brothers gopalan sankaran love support view profoundly lucky wonderful parents brothers dedicate dissertation contents abstract acknowledgements introduction motivation acquiring semantic knowledge exploiting semantic knowledge semantic disks underneath dbms reasoning semantic disks evaluation methodology contributions outline background modern storage systems raid layout nvram buffering block migration summary file system background common properties linux ext linux ext vfat windows hypothesis qrm vot maxvote bal qrm bal ntfs xii semantic disks bal proof definition maxvote bal qrm maxvote bal qrm votes proof implies maxvote bal qrm null vote definition maxvote bal qrm paxon mathematicians provided careful structured proofs important theorems sophisticated modern mathematicians omit details write paragraph-style proofs making mistake lamport acm transactions computer systems vol maxvote bal qrm dec dec proof maxvote bal qrm dec dec proof definition maxvote bal qrm bal bal proof imply maxvote bal qrm bal bal maxvote bal qrm votes proof definition maxvote bal qrm bal bal proof definition maxvote bal qrm contradiction proof lemma easy show hold successful ballots decree theorem hold qrm vot qrm vot dec dec proof theorem ifb bal bal implies bal bal theorem immediately lemma paxons proved theorem asserting priests chamber conduct successful ballot preserving guarantee progress shows balloting protocol based deadlock theorem ballot number set priests bal qrm ifb hold ballot bal qrm vot hold proof theorem condition choice bal assumption condition choice qrm assumption maxvote bal dec decree equal maxvote dec condition part-time parliament acm transactions computer systems vol preliminary protocol paxons derived preliminary protocol requirement conditions remain true set ballots conducted definition protocol set changed set explicitly calculated paxons referred quantity observed gods mortal ballot initiated priest chose number decree quorum priest quorum decided vote ballot rules determining initiator chose ballot number decree quorum priest decided vote ballot derived directly maintain maintain ballot receive unique number remembering notes ledger ballots previously initiated priest easily avoid initiating ballots number priests initiating ballots number set ballot numbers partitioned priests obvious method ballot number pair consisting integer priest lexicographical ordering grai linsei grai paxon alphabet case priest unbounded set ballot numbers reserved maintain ballot quorum chosen madz vriti set priests initially madz vriti set meant simple majority observed fat priests mobile spent time chamber thin madz vriti set set priests total weight half total weight priests simple majority priests group thin priests complained unfair actual weights replaced symbolic weights based priest attendance record primary requirement madz vriti set sets madz vriti set priests priest common maintain priest initiating ballot chose qrm majority set condition requires maxvote dec equal blank ballot number quorum decree maxvote dec ifmaxvote dec equals blank ballot decree maintain initiating ballot ballot number quorum priest find maxvote dec find maxvote priest recall maxvote vote largest ballot number votes cast ornull vote lamport acm transactions computer systems vol ballot numbered priest obtains maxvote exchange messages steps protocol conducting single ballot initiated priest chooses ballot number sends nextballot message set priests priest responds receipt nextballot message sending lastvote message vote largest ballot number cast null vote null vote ballot numbered priest notes back ledger remember votes previously cast sends lastvote message equals maxvote set ballots ballots initiated votes cast priest maxvote choosing decree true maxvote change lastvote message maxvote changing cast votes ballot numbers bal sending lastvote message promising cast vote promise record information ledger steps balloting protocol begun step priest receiving lastvote message priest majority set priest initiates ballot number quorum decree chosen satisfy records ballot back ledger sends beginballot message priest receipt beginballot message priest decides cast vote ballot number overview basic approach benefits concerns alternative approaches explicit implicit evaluation methodology acquiring semantic knowledge static information dynamic information classification association operation inferencing accuracy inference dealing asynchrony indirect classification association operation inferencing impact uncertainty evaluation time overheads space overheads summary exploiting semantic knowledge file system model semantic caching tolerance inaccuracy journaling design implementation evaluation complexity analysis summary improving availability d-graid introduction problem reduced availability due semantic ignorance solution d-graid xiii key techniques extended motivation design d-graid expectations graceful degradation design considerations fast recovery exploring graceful degradation space overheads static availability dynamic availability file system model arbitrary ordering delayed update hidden operation implementation making d-graid graceful degradation live-block recovery aspects alexander alexander fat evaluating alexander alexander work correctly time overheads introduced effective access-driven diffusion fast live-block recovery benefits expect d-graid complex implementation d-graid levels d-graidno redundancy d-graidmirroring d-graidparity summary discussion impact wrong summary exploiting liveness cast knowledge vote faded violate promise implied lastvote message introduction extended ballot decides vote ballot motivation number liveness storage sends voted message records taxonomy vote granularity back ledger execution liveness step xiv considered accuracy add liveness ballot bal information qrm timeliness vot voted information file system ballot dec step priest decides vote model techniques ballot liveness executing priests detection content simplicity protocol sending messages liveness block case reality priest talk liveness generation messengers part-time parliament acm transactions liveness case computer study systems secure vol step considered change delete goals set ballots adding set vot voters faded basic ballot priest option vote operation coverage step casting vote violate previous deletes promise faded fact file steps protocol optional systems priest ignore nextballot message executing evaluation step implicit failure detection action prevent progress ntfs explicit inconsistency liveness make false notification effect granularity receiving free message prevent action notification timeliness happening message free loss inconsistency notification protocol orphan guarantees consistency priests leave chamber messages allocations explicit lost notification receiving multiple copies message action ext explicit repeated notification step performing action time ext explicit effect secure sending voted messages delete step effect sending repetition discussion step prevented entry made back ledger summary semantic executed disks database consistency condition systems maintained 
messenger delivers message times steps describe complete protocol initiating ballot voting remains determine results balloting announce decree selected recall ballot successful iff priest quorum voted decree successful ballot chosen synod rest protocol received voted message priest quorum ballot number writes decree ballot ledger sends success message priest receiving success message priest enters decree ledger steps describe individual ballot conducted preliminary protocol priest initiate ballot time step maintains entire protocol maintains conditions priest enters decree ledger decree successful ballot theorem implies priests ledgers consistent protocol address question progress step decree determined condition decree written ledger priest priest quorum left chamber consistency guaranteed step allowed greater freedom choosing basic protocol preliminary protocol priest record number ballot initiated vote cast lastvote lamport acm transactions computer systems vol message keeping track information difficult busy priests paxons restricted preliminary protocol obtain practical basic protocol priest maintain information back ledger lasttried number ballot initiate prevvote vote cast highest-numbered ballot voted voted nextbal largest lastvote message message steps preliminary protocol describe single ballot conducted initiator priest preliminary protocol conduct number ballots concurrently basic protocol conducts ballot time ballot number lasttried initiates ballot ignores messages pertain ballot previously initiated priest information progress ballot number lasttried slip paper loses slip paper stops conducting ballot preliminary protocol lastvote message priest represents promise vote ballot numbered bal basic protocol represents stronger promise cast vote ballot numbered stronger promise prevent casting vote step basic protocol allowed cast preliminary protocol preliminary protocol option casting vote basic protocol require allowed preliminary protocol steps preliminary protocol steps conducting ballot basic protocol information conduct ballot lasttried prevvote nextbal slip paper priest chooses ballot number greater lasttried sets lasttried sends nextballot message set priests receipt nextballot message nextbal priest sets nextbal sends lastvote message equals prevvote anextballot message nextbal part-time parliament acm transactions computer systems vol receiving lastvote message introduction extracting semantic information static information dynamic information partial availability d-graid design transactions recovery evaluation discussion secure delete faded table-level deletes record-level deletes performance discussion semantic disk-friendly dbms information required dbmses semantic disks summary logic file systems semantic disks introduction extended motivation reasoning existing file systems building file system functionality designing semantically-smart disks background file system metadata file system consistency file system asynchrony formalism basic entities beliefs actions ordering beliefs actions proof system attributes containers logical postulates file system properties container exclusivity reuse ordering pointer ordering modeling existing systems data consistency modeling file system journaling redundant synchrony ext support consistent undelete undelete existing systems undelete generation pointers implementation undelete ext application semantic disks block typing utility generation pointers summary xvi related work smarter storage fixed interfaces expressive interfaces programming environments smarter file systems implicit systems partial availability distributed file systems traditional raid systems logical modeling systems conclusions future work lessons learned future work implicit inference domains integrating logic implementation checkers semantic disk functionality making semantic disks semantic summary chapter introduction storage systems form backbone modern computing innovation storage crucial improving present future computing environments improving storage systems dimensions availability security performance paramount importance pace ever-increasing modes usage requirements storage systems range innovation storage today limited due narrow interface exists storage system software layer file system dbms storage system storage systems today export simple block-based interface scsi abstracts storage system linear array blocks file systems perform block reads block writes linear address space interface designed time storage systems simple passive disks fit simple abstraction linear address space storage systems evolved massively complex 
powerful systems incorporating wide range optimizations today storage systems composed multiple disks forms redundancy tolerate disk failures perform migration blocks disks load balancing transparently buffer writes non-volatile ram writing disk perform transparent remapping blocks hide failure result sophistication significant amount low-level information control storage system failure boundaries disks exact mapping logical blocks physical disk blocks storage systems intelligent complex narrow block-based interface remained unchanged due massive industrywide effort required change legacy issues result file systems longer understand storage system simplistically continue view storage system dumb disk file system implement functionality requires low-level knowledge control low-level details block layout storage system locale information required provide functionality placing functionality storage systems limited due narrow interface storage systems simply observe raw stream block reads block writes semantic meaning semantic knowledge logical grouping blocks files type block data metadata liveness blocks unavailable storage system research efforts limited applying disk-system intelligence manner oblivious nature meaning file system traffic improving write performance writing data closest block priest majority set lasttried priest initiates ballot number quorum decree chosen satisfy sends beginballot message priest receipt beginballot message nextbal priest casts vote ballot number sets prevvote vote sends voted message abeginballot message nextbal received voted message priest quorum ballot number lasttried writes decree ballot ledger sends success message priest receiving success message priest enters decree ledger basic protocol restricted version preliminary protocol meaning action allowed basic protocol allowed preliminary protocol preliminary protocol satisfies consistency condition basic protocol satisfies condition preliminary protocol basic protocol require action address question progress derivation basic protocol made obvious consistency condition satisfied similarly obvious ancient wisdom turned false skeptical citizens demanded rigorous proof paxon mathematicians proof protocol satisfies consistency condition reproduced appendix complete synod protocol basic protocol maintains consistency ensure progress states priest require complete protocol consists steps conducting ballot basic protocol achieve progress includes obvious additional requirement priests perform steps protocol meet progress condition priest required perform step initiates ballot key complete protocol lay determining priest initiate ballot initiating ballot prevent progress initiating ballots prevent progress larger ballot number receipt nextballot message priest step elicit promise prevents voting step previously initiated ballot initiation ballot prevent previously initiated ballot succeeding ballots lamport acm transactions computer systems vol continually initiated increasing ballot numbers previous ballots chance succeed progress made achieving progress condition requires ballots initiated succeeds initiated frequently develop complete protocol paxons long messengers deliver messages priests respond determined messenger leave chamber deliver message minutes priest remained chamber perform action minutes event caused action chamber event caused send message responded reply receive reply minutes messenger left chamber priest send message minutes event receive message minutes respond minutes reply reach minutes suppose single priest initiating ballots sending message priest step protocol initiated ballot majority set priests chamber expect execute step minutes initiating ballot execute step minutes unable execute steps times priest messenger left chamber initiated ballot larger-numbered ballot previously initiated priest priest initiate ballots handle possibility learn ballot numbers greater lasttried priests extending protocol require priest received nextballot beginballot message nextbal send message nextbal priest initiate ballot larger ballot number assuming priest initiating ballots suppose required initiate ballot iff executed step step previous minutes learned priest initiated higher-numbered ballot chamber doors locked majority set priests inside decree passed recorded ledgers priests chamber minutes minutes start ballot minutes learn priest initiated larger-numbered ballot minutes complete steps successful ballot progress condition met single priest leave chamber initiating ballots assuming seconds ifi paxon unit time range determined studies hourglass shards reaction time priests long respond message minutes ifi number messages arrived simultaneously part-time parliament acm transactions computer systems vol complete protocol included procedure choosing single priest called president initiate ballots forms government choosing president difficult problem difficulty arises governments require president time united states chaos result election people thought bush elected president thought dukakis decide sign bill law decided veto paxon synod multiple presidents impede progress inconsistency complete protocol satisfy progress condition method choosing president needed satisfy presidential selection requirement entered left chamber minutes priest chamber president presidential selection requirement met complete protocol property majority set priests chamber entered left chamber minutes end period priest chamber decree written ledger paxons chose president priest alphabetical order names priests chamber presidential selection requirement satisfied priest chamber message priest minutes priest considered president received message higher-named priest minutes complete synod protocol obtained basic protocol requiring priests perform steps promptly adding method choosing president initiated ballots requiring president initiate ballots times details protocol simple methods selecting president deciding president initiate ballot undoubtedly paxos rules require president initiating ballots decree chosen ensuring priests entered chamber learn chosen decree ways make priests learned decree chosen selecting president priest lasttried priests allowing president choose large ballot number paxons realized protocol achieve progress condition involve measuring passage time protocols centuries pass rigorous proof result fischer lamport acm transactions computer systems vol selecting president initiating ballots easily formulated precise algorithms set timers perform actions time-outs occur assuming perfectly accurate timers closer analysis reveals protocols made work timers bound accuracy skilled glass blowers paxos difficulty constructing suitable hourglass timers sophistication paxon mathematicians widely believed found optimal algorithm satisfy presidential selection requirement hope algorithm discovered future excavations paxos multidecree parliament parliament established protocol satisfy consistency progress requirements derived synod 
protocol derivation properties original parliamentary protocol sections section discusses evolution protocol protocol passing decree paxon parliament pass series numbered decrees synod protocol president elected wanted decree passed inform president assign number decree attempt pass logically parliamentary protocol separate instance complete synod protocol decree number single president selected instances performed steps protocol key deriving parliamentary protocol observation synod protocol president choose decree quorum step newly elected president send set legislators single message serves nextballot message instances synod protocol infinite number instances decree number legislator reply single message serves lastvote messages step instances synod protocol message finite amount information voted finite number instances president received reply member majority set ready perform step instance synod protocol finite number instances decree numbers choice decree step determined president immediately performs step instances passing decrees receives request pass decree chooses lowest-numbered decree free choose performs step decree number instance synod protocol pass decree part-time parliament acm transactions computer systems vol modifications simple protocol lead actual paxon parliament protocol reason synod protocol decree number outcome newly elected president decrees numbers equal written ledger sends nextballot message serves nextballot message instances synod protocol decree numbers larger response message legislator informs decrees numbered greater ledger addition sending usual lastvote information decrees ledger asks send decrees numbered ledger suppose decrees introduced late friday afternoon decree passed written ledgers legislators home weekend suppose monday dfvrk elected president learns decree knowledge decree previous president legislators voted chamber hold ballot passes decree leaves gap ledgers assigning number decree earlier ledger decree passed previous week passing decrees order confusion citizen proposed decree knew decree passed dfvrk attempt pass ides february national olive day traditional decree made absolutely difference paxos general president fill gaps ledger passing olive-day decree consistency progress properties parliamentary protocol follow immediately properties synod protocol derived knowledge paxons bothered writing precise description parliamentary protocol easily derived synod protocol properties protocol ordering decrees balloting place concurrently decree numbers ballots initiated legislators thinking president initiated ballot precisely order decrees passed knowing president selected important property ordering decrees deduced lamport acm transactions computer systems vol decree proposed chosen president step instance synod protocol decree passed written time ledger president propose decrees learn members majority set decrees voted decree passed voted legislator majority set president learned previously passed decrees initiating decree president fill gap ledgers important decree decree olive-day decree propose decrees order protocol satisfied decree-ordering property decrees important decree passed decree proposed lower decree number closed doors details involved choosing president parliament functioned president chosen entering leaving chamber receiving request pass decree directly citizen relayed legislator president assigned decree number passed exchange messages numbers refer steps synod protocol president beginballot message legislator quorum legislator quorum voted message president president success message legislator total message delays messages assuming parliament legislators quorum parliament busy president combine beginballot message decree success message previous total messages decree developments governing island turned complex task paxons realized number problems arose solutions required protocol important picking president president parliament originally chosen method synod based purely alphabetical ordering names legislator vki returned six-month vacation immediately made president idea happened absence parliamenthe part-time parliament acm transactions computer systems vol tary activity halt vki slow writer laboriously copied months worth decrees bring ledger date incident led debate choose president paxons urged legislator president remain president left chamber influential group citizens wanted richest legislator chamber president afford hire scribes servants presidential duties argued rich legislator brought ledger date reason assume presidency argued upstanding citizen made president wealth upstanding meant dishonest paxon publicly admit possibility official malfeasance outcome debate record exists presidential selection protocol ultimately long ledgers years progressed parliament passed decrees paxons pore longer list decrees find current olive tax color goat sold legislator returned chamber extended voyage bit copying bring ledger date eventually legislators forced convert ledgers lists decrees law books contained current state law number decree passage reflected state learn current olive tax looked law book taxes learn color goat sold looked mercantile law legislator ledger contained law decree learned decree set olive tax drachmas ton changed entry olive-tax law noted ledger complete decree learned decree write back ledger wait learned decrees incorporating decree law book enable legislator short time catch copying entire law book legislators list past week decrees back book list slip paper convenient legislator enter decrees back ledger passed update law book times week bureaucrats paxos prospered legislators busy parliament longer handle details government bureaucracy established passing decree declare lot cheese fit sale parliament passed decree appointing cheese inspector make decisions evident selecting bureaucrats simple parliament passed decree making kstra cheese inspector months merchants complained kstra lamport acm transactions computer systems vol strict rejecting perfectly good cheese parliament replaced passing decree gvyda cheese inspector kstra pay close attention parliament learn decree period confusion cheese market kstra gvyda inspecting cheese making conflicting decisions prevent confusion paxons guarantee position held bureaucrat time president included part decree time date proposed decree making kstra cheese inspector read jan kstra cheese inspector months declares term begin january previous inspector term ended whichever term end march explicitly resigned president pass decree mar kstra resigns cheese inspector bureaucrat appointed short term replaced quickly left island parliament pass decree extend bureaucrat term satisfactory job bureaucrat needed time determine held post mechanical clocks unknown paxos paxons time accurately minutes position sun stars kstra term began start inspecting cheese celestial observations easy make method appointing bureaucrats work higher-numbered decrees proposal times parliament passed decrees apr fransez wine taster months apr pnyeli wine taster month proposed legislators thought president out-of-order proposal times easily prevented parliamentary protocol satisfies property decrees passed presidents presidents proposed decree learning decree proposed cloudy days rare paxos balmy climate part-time parliament acm transactions computer systems vol property satisfied suppose ballot number successful decree ballot number successful decree legislator voted ballots balloting began nextballot message sender message decree number reply nextballot message state voted learning law addition requesting passage decrees ordinary citizens needed inquire current law land paxons thought citizen simply examine ledger legislator incident demonstrated sophisticated approach needed centuries legal sell white goats farmer named dvlef parliament pass 
decree sale black goats permitted dvlef instructed goatherd sell black goats merchant named skeen law-abiding citizen skeen asked legislator stvkmei sale legal stvkmei chamber entry ledger past decree advised skeen sale illegal current law skeen refused buy goats incident led formulation monotonicity condition inquiries law inquiry precedes inquiry inquiry reveal earlier state law citizen learns decree passed process acquiring knowledge considered implicit inquiry condition applies interpretation monotonicity condition changed years initially monotonicity condition achieved passing decree inquiry der wanted current tax olives parliament pass decree citizen der reading law read ledger complete decree learn olive tax decree citizen greez inquired olive tax decree inquiry proposed decree passed decree-ordering property section implies received decree number greater greez obtain earlier olive tax der method reading law satisfied monotonicity condition precedes interpreted inquiry precedes inquiry finished earlier time began lamport acm transactions computer systems vol passing decree inquiry proved cumbersome paxons realized simpler method weakened monotonicity condition changing interpretation precedes decided event precede event happen earlier time causally affect event weaker monotonicity condition prevents problem encountered farmer dvlef merchant skeen causal chain events end implicit inquiry dvlef beginning inquiry skeen weaker monotonicity condition met decree numbers business transactions inquiries farmer dvlef flock included nonwhite goats parliament pass decree sale brown goats permitted selling brown goats skeen informed merchant sale legal decree number skeen asked legislator stvkmei sale legal law decree stvkmei ledger complete decree wait skeen stvkmei ledger decree skeen sale legal decree number merchant skeen remember number business transaction inquiry law paxons satisfied monotonicity condition ordinary citizens disliked remember decree numbers paxons solved problem reinterpreting monotonicity condition time changing meaning state law divided law separate areas legislator chosen specialist area current state area law determined specialist ledger suppose decree changed tariff law decree changed tax law tax law change tax-law specialist learned decrees tariff-law specialist learned yielding state law obtained enacting decrees numerical order avoid conflicting definitions current state paxons required specialist time area requirement satisfied method choose specialists choose bureaucrats section inquiry involved single area law monotonicity achieved directing inquiry area specialist answered ledger learning law passed constituted result implicit inquiry paxons required decree change area law notification decree passage area specialist part-time parliament acm transactions computer systems vol inquiries involving multiple areas hard handle merchant liskvf asked tariff imported golden fleece higher sales tax purchased locally tax-law tariff-law specialists cooperate provide answer tax specialist answer liskvf tariff specialist tariff golden fleeces long made ledger receiving reply method proved satisfactory make sweeping change areas law time paxons realized requirement maintaining monotonicity decree affect single area area affects specialist parliament change areas law single decree appointing single legislator specialist areas area multiple specialists long area law allowed change income taxes due parliament appoint tax-law specialists handle seasonal flood inquiries tax law dishonest legislators honest mistakes official assertions contrary dishonest legislators history paxos caught exiled sending contradictory messages malicious legislator legislators ledgers inconsistent disk modern storage stack precludes entire class functionality requires information file system storage system thesis proposes solution fundamental limitation thesis propose class storage systems bridges information gap file systems storage requiring change existing block-based interface operating underneath unmodified scsi interface storage systems automatically track higher-level semantic information file system database system running utilize information provide classes functionality impossible provide existing systems call storage system aware semantics higher layer semantically-smart disk system sds requiring existing storage interface cases file system semanticallysmart disk systems present pragmatic solution storage systems extremely easy deploy adopt thesis dissertation feasible block-based storage system track higher level semantic information file system show information tracked high level accuracy enables classes innovation storage systems motivation motivate semantic intelligence storage systems provide functionality provided today semantically-smart disk system relates improving availability storage face kinds failures rest thesis present examples semantic disk functionality improve security availability performance storage ways modern storage systems tolerate disk failures employing forms raid storing redundant information small number disks storage system recover small fixed number failures losing data availability guarantee provided existing raid systems simple fewer disks fail raid continues operate correctly disks fail raid unavailable raid schemes small disks working users observe failed disk system failures disk raid system result complete unavailability fact disks working ideally availability loss scenario availability cliff behavior existing raid systems storage system lays blocks oblivious semantic importance relationship files corrupted inaccessible extra disk failure storage system information semantic importance blocks treats root directory block regular file block extra failed disk happened important block large fraction file system rendered unavailable file system address problem views storage system single large disk information failure boundaries disks control exact physical disk logical block mapped chapter present system called d-graid availability multiple failures exploiting semantic knowledge storage system specifically d-graid enables graceful degradation availability multiple failures selective replication key metadata structures file system fault-isolated placement semantically-related data acquiring semantic knowledge important constraint semantically-smart disk system existing block-based interface storage modified sds infers semantic information carefully monitoring block-level read write traffic order bootstrap inference storage system relies minimal understanding file system specifically sds embedded static knowledge key on-disk structures file system sds static knowledge build extract sophisticated kinds information monitoring write traffic specific on-disk structures observing structures change sds correlate higher level file system activities led change key challenge tracking semantic information based observing on-disk structure asynchrony exhibited modern file systems file systems typically buffer data memory arbitrarily re-order block writes crucial file system performance asynchrony effectively obfuscates information storage system writes made disk reflect effect multiple file system operations cancel show asynchrony imposes basic limits extent accuracy semantic information tracked sds thesis present detailed techniques extract kinds semantic information sds bring limits accurately information tracked find dynamic behavior file system significantly affects effectiveness simplicity dynamic inference identify file system properties simplify techniques improve accuracy experiment variety file systems linux ext linux ext vfat smaller extent windows ntfs order explore generality techniques analyze conformance file system properties identify thesis successively refine model file system starting simple synchronous file system complex file systems dynamic properties exploiting semantic knowledge disk system semantic knowledge file system relevant question utility demonstrate utility semantic disks built prototype implementations semantic disk case studies show semantic knowledge improve storage systems fundamental ways specifically present case studies targeted improving availability security performance storage case studies present d-graid storage system graceful degradation availability multiple failures ensuring semantically-meaningful data catastrophic failures d-graid enables faster recovery failure exploiting semantic knowledge preferentially recover blocks file system blocks live file system show d-graid significantly improves storage availability modest performance cost demonstrate semantic disks provide functionality extreme correctness requirements designed implemented semantic disk system called faded performs secure deletion makes deleted data irrecoverable performing repeated overwrites inferring logical deletes occurring file system case study utilizes semantic information directly impacts correctness error detecting delete result irrevocable deletion valid data missed secure deletion deleted data showing reliable secure deletion implemented inherently uncertain information show functionality correctness-sensitive implemented semantic disks case studies find limits accuracy semantic inference complex functionality implemented semanticallysmart disk system key implementing complex functionality sds conservatism identifying conservative techniques abstractions circumvent inherently inaccurate information provide guarantees functionality implemented based information find performance cost conservatism small case studies results overhead cost conservatism find modest cpu cost tracking semantic information quantify costs discussion case studies semantic disks underneath dbms database management systems dbms prime client storage file systems investigated techniques semantic inference underneath dbms implemented major case studies d-graid faded dbms case bring key differences find semantic inference underneath dbms easier writeahead logging performed dbms log communicates semantic disk complete time-ordered list operations dbms find database systems track fewer general purpose statistics file systems limits effectiveness case studies identify propose minor database systems address limitation reasoning semantic disks developing functionality semantic disks entails careful reasoning accuracy pieces semantic information functionality relies accuracy information turn depends specific properties file system running process developing techniques case studies found reasoning challenging recognized systematic formal framework model semantic disks interaction file systems end thesis present formal logic representing proving properties file systems semantic disks intended initial 
goal logic model semantic disks identified reasoning information semantic disk strong parallel reasoning file system consistency management cases information purely pertains on-disk state present logic model file systems general reason correctness properties show reason semantic disks evaluation methodology evaluate techniques semantic inference case studies prototype implementations prototype sds employ softwarebased infrastructure implementation inserts pseudo-device driver kernel interposing file system disk similar software raid prototype appears file systems device file system mounted prototype observes exact block-level information disk controller functionally identical hardware prototype contributions key contributions dissertation formulation design techniques block-based storage system infer pieces semantic information underneath modern file systems uncertainty caused due file system asynchrony design implementation evaluation variety prototype case studies demonstrate semantic disks significantly improve storage systems axes availability security performance case studies serve explore costs providing functionality semantic disk terms performance implementation complexity design conservative techniques circumvent fundamental uncertainty semantic inference techniques ensure sds functionality provide correctness guarantees secure delete based inherently inaccurate information identification dynamic file system properties impact effectiveness accuracy semantic inference exploration limits semantic inference properties formulation logic framework proof system reasoning file systems semantic disks demonstration effectiveness framework representing file system properties proving high-level correctness guarantees file systems interaction semantic disks outline rest dissertation organized chapter background information modern storage systems file systems chapter present overview basic sds approach compare alternative approaches addressing information gap storage describe techniques sds track semantic information chapter present simple case studies exploit information improve functionality chapter chapter present d-graid system improves storage availability exploiting semantic knowledge chapter presents faded secure deleting disk shreds deleted data inferring logical deletes chapter extend semantic disk technology work underneath database systems chapter presents formal logic framework reason file systems semantic disks discuss related work chapter conclude discuss future directions chapter chapter background chapter provide background information modern storage systems file systems describe range functionality modern storage systems provide hinting complexity extent low-level knowledge storage systems provide background file systems semantic inference modern storage systems storage systems today constitute huge industry ranging desktop hard drives mid-range high-end storage servers due availability cheap processing power memory level intelligence storage rise exemplified high-end storage systems today emc symmetrix storage system hundreds processors ram storage systems processing power implement wide range optimizations transparent higher layers system section briefly describe common examples functionality modern storage systems provide raid layout common feature enterprise storage systems today tolerance small number disk failures spreading data multiple disks redundant information storage system tolerate fixed number failures losing data spreading data multiple disks improves performance storage system exploit parallelism disks levels raid varying exact layout strategy employed redundancy common raid levels raidi mirroring raidi parity raideach disk block mirrored disks resulting space overhead raidachieves space efficiency computing parity block row data blocks disks ith parity block xor ith block disks common variant raidextended inconsistency result lapse memory honest legislator messenger inconsistencies recognized easily corrected passing decrees disagreement current olive tax eliminated passing decree declaring tax difficult problem lay correcting inconsistent ledgers aware inconsistency existence dishonesty mistakes legislators inferred redundant decrees began appearing ledgers years founding parliament decree olive tax drachmas ton passed decree set olive tax drachmas ton intervening decree changed parliament apparently cycled laws months legislators ledgers initially inconsistent legislators agree current law land months believed redundant decrees paxons made parliament selfstabilizing self-stabilizing modern term due dijkstra clear precisely self-stabilization meant parliament legislators coming paxons satisfied definition required legislators chamber time consistency guaranteed achieving consistency required legislator entry ledger lamport acm transactions computer systems vol decree number legislator eventually fill entry sort self-stabilization property paxon parliament possessed achieved paxon mathematicians undoubtedly addressed problem work found hope future archaeological expeditions paxos give high priority search manuscripts self-stabilization choosing legislators membership parliament hereditary passing parent child elder statesman parnaz retired gave ledger son carried interruption made difference legislators parnaz communicated families emigrated immigrated system change paxons decided add remove members parliament decree posed circularity problem membership parliament determined decrees passed passing decree required knowing constituted majority set turn depended member parliament circularity broken letting membership parliament passing decree law decree president pass decree knew decrees decree practice passing decree strvng legislator president immediately pass olive-day decree decrees changing composition parliament dangerous care consistency progress conditions hold progress condition guaranteed progress majority set chamber guarantee majority set fact mechanism choosing legislators led downfall parliamentary system paxos scribe error decree supposed honor sailors drowned shipwreck declared members parliament passage prevented decrees passed including decrees proposed correct mistake government paxos halt general named lampsvn advantage confusion stage coup establishing military dictatorship ended centuries progressive government paxos grew weak series corrupt dictators unable repel invasion east led destruction civilization relevance computer science state machine approach paxos parliament destroyed centuries ago protocol simple distributed database system part-time parliament acm transactions computer systems vol server state database consists assignment values names copies database maintained multiple servers client program issue server request read change assigned kinds read request slow read returns assigned fast read faster reflect recent change database table shows obvious correspondence database system paxon parliament client request change performed passing decree slow read involves passing decree section fast read performed reading server current version database paxon parliament protocol distributed fault-tolerant implementation database system method implementing distributed database instance state machine approach proposed lamport approach defines state machine consists set states set commands set responses function assigns response state pair pair consisting response state command state pair intuitively state machine executes command producing response changing state command machine current state determine response state distributed database state machine state database state state machine commands function response state figure state machine approach system implemented network server processes servers transform client requests state machine commands execute commands transform state machine responses replies clients general algorithm ensures servers obtain sequence commands ensuring produce sequence responses state assuming start initial state database client request perform slow read change transformed state machine read update command command executed state machine response transformed reply client server received request 
servers perform sequence state machine commands maintain consistent versions database time servers earlier versions state machine command executed time servers server table parliament distributed database legislator server citizen client program current law database state lamport acm transactions computer systems vol current version state reply fast read request executing state machine command functionality system expressed state machine function command state pairs response state pairs problems synchronization fault-tolerance handled general algorithm servers obtain sequence commands designing system state machine servers obtain state machine commands standard distributed algorithm proved correct functions easier design distributed algorithms algorithm implementing arbitrary state machine lamport algorithms devised tolerate fixed number arbitrary failures lamport algorithms guarantee fewer processes fail state machine commands executed fixed length time algorithms suitable applications requiring real-time response failures occur servers inconsistent copies state machine inability servers communicate equivalent failure system low probability losing consistency algorithm large turn implies large cost redundant hardware communication bandwidth response time paxon parliament protocol implement arbitrary state machine legislators law book corresponds machine state passing decree corresponds executing state machine command resulting algorithm robust expensive earlier algorithms tolerate arbitrary malicious failures guarantee bounded-time response consistency maintained benign failure number processes communication paths paxon algorithm suitable systems modest reliability requirements justify expense extremely fault-tolerant real-time implementation state machine executed algorithm guarantees bounded-time response time made part state machine actions triggered passage time system granting ownership resources state algorithms derived military protocols mediterranean state fig state machine simple database part-time parliament acm transactions computer systems vol include time client granted resource state machine automatically execute command revoke ownership client held resource long paxon algorithm time made part state natural failures occur arbitrarily long execute command pass decree command executed earlier sequence decrees command issued earlier state machine real time paxon parliament method section deciding current cheese inspector decide current owner resource commit protocols paxon synod protocol similar standard three-phase commit protocols bernstein skeen paxon ballot three-phase commit protocol involve exchange messages coordinator president quorum members legislators commit protocol chooses values commit abort synod protocol chooses arbitrary decree convert commit protocol synod protocol sends decree initial round messages commit decision means decree passed abort decision means olive-day decree passed synod protocol differs converted commit protocol decree phase parliamentary protocol execute phase decrees exchange messages needed pass individual decree theorems synod protocol based similar results obtained dwork lynch stockmeyer dwork algorithms execute ballots sequentially separate rounds unrelated synod protocol lamport acm transactions computer systems vol appendix proof consistency synodic protocol basic protocol synod basic protocol informally section stated modern algorithmic notation begin variables priest maintain variables represent information ledger convenience vote prevvote section replaced components prevbal prevdec outcome decree written ledger blank written lasttried number ballot begin prevbal number ballot voted voted prevdec decree voted blank voted nextbal number ballot agreed participate agreed participate ballot variables representing information priest slip paper status values idle conducting begin ballot begin ballot number lasttried polling conducting ballot number lasttried lost slip paper status assumed equal idle values variables irrelevant prevvotes set votes received lastvote messages current ballot ballot number lasttried quorum status polling set priests forming quorum current ballot meaningless voters status polling set quorum members received voted messages current ballot meaningless part-time parliament acm transactions computer systems vol decree status polling decree current ballot meaningless history variable set ballots started progress priests cast votes history variable development proof algorithm implemented actions priest actions assumed atomic meaning action begun completed priest begins action action enabling condition list effects enabling condition describes action performed actions receive message enabled messenger arrived message list effects describes action algorithm variables message sends individual action sends message recall ballot numbers partitioned priests ballot number paxons defined owner priest allowed ballot number actions basic protocol allowed actions protocol require priest attempt efficiency made actions silly things sending beginballot message priest received lastvote message ballot enabled set lasttried case disks raidwhere data striped mirrored pairs -disk raidsystem mirrored pairs data striped pairs complex performance space trade-offs raid schemes storage systems adaptively choose ideal raid level workload migrate data raid levels based access patterns optimizations result dynamic mappings logical physical blocks storage system explicitly track nvram buffering storage systems today perform buffering writes non-volatile ram performance write request arrives host system storage system simply writes nvram returns success write actual propagation write disk occurs time nvram filled delay threshold enables queueing writes disk resulting scheduling performance block migration traditional raid systematic pattern choosing disk physical block logical block mapped disk system logical block mapped disk modulo storage systems today break mapping migrating blocks disks reasons load balancing eliminate specific hotspots avoiding bad blocks disk track logical-to-physical mapping storage systems maintain internal indirection table tracks location blocks migrated summary storage systems smarter providing wide range optimizations higher layers system view simple disks decades ago linear address space abstraction provided scsi hides complexity modern storage result modern storage systems exclusively rich amount low-level knowledge number disks raid array raid layout strategy logical-to-physical block mapping control choosing blocks buffered nvram file system background techniques tracking semantic information storage system dependent characteristics file system study range techniques required inference experimenting underneath file systems ext ext vfat ext modes operation synchronous asynchronous modes ext modes writeback ordered data journaling modes update behaviors form rich set file systems chapter report limited experience underneath windows ntfs file system section provide background information file systems study discuss key on-disk data structures update behavior common properties begin properties common file systems viewpoint tracking semantic information basic level file systems track kinds on-disk metadata structure tracks allocation blocks bitmap freelist index structures inodes map logical file groups blocks directories map human-readable path names logical files file systems manage data domains main memory disk time file system caches subset blocks memory modifying block block read memory written back disk time modification common aspect update behavior modern file systems asynchrony data metadata block updated contents block immediately flushed disk buffered memory interval delayed write interval blocks dirty longer delayed write interval periodically flushed disk order delayed writes committed potentially arbitrary file systems enforce ordering constraints linux ext ext file system intellectual descendant berkeley fast file system ffs disk split set block groups akin cylinder groups ffs inode data blocks allocation status live dead data blocks tracked bitmap blocks information file including size block pointers found file inode accommodate large files pointers inode point indirect blocks turn block pointers committing delayed writes ext enforces ordering whatsoever crash recovery requires running tool fsck restore metadata integrity data inconsistency persist ext synchronous mode operation metadata updates synchronously flushed disk similar early ffs linux ext linux ext file system journaling file system evolved ext basic on-disk structures ext ensures metadata consistency writeahead logging metadata updates avoiding perform fscklike scan crash ext employs coarse-grained model transactions operations performed epoch grouped single transaction ext decides commit transaction takes in-memory copy-on-write snapshot dirty metadata blocks belonged transaction subsequent updates metadata blocks result in-memory copy transaction ext supports modes operation ordered data mode ext ensures transaction commits data blocks dirtied transaction written disk data journaling mode ext journals data blocks metadata modes ensure data integrity crash 
mode data writeback order data writes data integrity guaranteed mode vfat vfat file system descends world operating systems work linux implementation vfat vfat operations centered file allocation table fat entry allocatable block file system entries locate blocks file linked-list fashion file block address entry fat find block file entry hold end-of-file marker setting block free unlike unix file systems information file found inode vfat file system spreads information fat directory entries fat track blocks belong file directory entry information size type information pointer start block ballot number greater previous owner set status set prevvotes send nextballot message enabled status send nextballot lasttried message priest receive nextballot message nextbal set nextbal send lastvote message enabled nextbal prevbal send lastvote nextbal message priest owner nextbal pst bal prevbal dec prevdec lamport acm transactions computer systems vol receive lastvote message lasttried status set prevvotes union original start polling majority set enabled status pst prevvotes majority set set status polling set quorum set voters set decree decree chosen maximum element prevvotes ifv bal dec equal decree file set similar union ext vfat preserve ordering delayed updates dec windows ntfs qrm ntfs default vot file system bal lasttried windows today send beginballot ext message ntfs enabled status journaling file system polling send journaling beginballot mode lasttried ntfs decree employs metadata message journaling priest metadata quorum operations receive journaled beginballot message equivalent nextbal data journaling prevbal mode set ext prevbal fundamental piece metadata ntfs master file table mft record mft information unique file set piece prevdec metadata ntfs treated ballot regular file file bal mft choose obtained setting vot equal union send voted message enabled prevbal send voted prevbal message owner prevbal receive voted message lasttried status polling set voters union succeed enabled status polling quorum voters outcome blank set outcome decree send success message enabled outcome blank part-time parliament acm transactions computer systems vol send sucess outcome message priest receive success message outcome blank set outcome algorithm abstract description real protocol performed paxon priests algorithm actions accurately model actions real priests kinds actions priest perform atomically receiving message writing note ledger entry sending message represented single action algorithm receive actions receive message set variable pretend receipt message occurred priest acted message left chamber acting pretend message received pretense affect consistency condition infer consistency basic synod protocol consistency algorithm proof consistency prove consistency condition show outcome outcome blank equal rigorous correctness proof requires complete description algorithm description complete missing variable multiset messages transit send action adds message multiset receive action removes needed actions represent loss duplication messages forget action represents priest losing slip paper additions algorithm defines set behaviors change state corresponds allowed actions paxons proved correctness finding predicate true initially implies desired correctness condition allowed action leaves true predicate written conjunction turn conjunction predicates priests variables mentioned conjuncts variable status naturally conjunct conjunct thought constraint variables definitions individual conjuncts figure list items marked symbols denotes conjunction multiset set multiple copies element lamport acm transactions computer systems vol items variables conjunct listed bracketed comments paxons prove satisfies conditions condition holds initially 
requires checking conjunct true initial values variables stated fig individual conjuncts predicate part-time parliament acm transactions computer systems vol explicitly initial values inferred variables descriptions checking condition straightforward condition implies consistency conjunct theorem hard part proving condition invariance meant proving left true action condition proved showing conjunct executing action true leaves conjunct true proofs sketched proof sketch changed adding ballot adding priest vot falsify file recovery log similar ext journal allocation status blocks volume maintained file called cluster bitmap similar block bitmap tracked ext addition ntfs extensible metadata form attribute lists logical file chapter semantic disks overview details semantically-smart disk system extracts knowledge file system present overview approach discuss merits demerits comparison alternative approaches basic approach basic idea presented thesis build storage system understands higher-level semantic information file system database system requiring existing block-based scsi-like interface storage semantically-smart disk system infers semantic knowledge file system carefully observing block-level read write traffic combining observation minimal amount embedded knowledge file system embedded knowledge techniques specific file system running top level dependency created file system storage system benefits concerns implementing functionality semantically-smart disk system key benefit enabling wide-scale deployment underneath unmodified scsi interface modification working smoothly existing file systems software base desire evolve interface file systems storage reality current interfaces survive longer anticipated bill joy systems protocols live forever similar modern processors innovate beneath unchanged instruction sets semantic disk-level implementation non-intrusive existing infrastructure individual storage vendor decide provide functionality sell enhanced storage system interact layers system achieve industry consensus semantically-smart storage systems require detailed knowledge file system concerns arise commercial feasibility systems main concerns concern arises placing semantic knowledge disk system ties disk system intimately file system on-disk structure file system storage system change issue problematic on-disk formats evolve slowly reasons backwards compatibility basic structure ffs-based file systems changed introduction period twenty years linux ext file system introduced roughly exact layout lifetime finally ext journaling file system backwards compatible ext on-disk layout extensions freebsd file system backwards compatible evidence storage vendors maintain support software specific file system emc symmetrix storage system client-side software understand format common file systems similarly storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically illustrating storage vendors put minimal amount knowledge specific higher layers concern storage system semantic knowledge file system interacts fortunately large number file systems supported cover large fraction usage population semantic storage system file system support storage system detect turn special functionality revert revert normal storage system detection simple techniques observing file system identifier partition table magic number super block similar host operating system detect file system disk starts final concern arises processing required disk system major issue general trend increasing disk system intelligence processing power increases disk systems substantial computational abilities modern storage arrays exhibit fruits moore law emc symmetrix storage server configured processors upto ram alternative approaches section discuss alternative approaches addressing problem information divide file system storage system compare sds approach describe approach explicitly changing interface storage convey richer information implicit forms information extraction sds approach explicit explicit approach involves changing interface file systems storage convey richer information layers instance storage system expose low-level information layout failure boundaries file system file system utilize knowledge layout alternatively file system explicitly communicate semantic information storage system notify storage system logical operations deletes storage system implement functionality techniques conceivably complex approach major drawbacks changing interface storage raises legacy issues terms huge existing investment blockbased interface adopting interface storage requires broad industry consensus extremely slow occur finally demand interface requires industry-wide agreement clear benefits interface difficult achieve interface deployed chicken-and-egg problem key weakness approach explicit approach problems regard practicality deployment benefits benefit axis system complexity explicit approach conceivably results simpler systems built semantic inference semantic disk approach incurs performance costs inferring file system information explicit communication information potentially efficient chapter quantify costs sds approach relative explicit approach context specific case study implicit implicit approaches intended address bootstrapping issue explicit approach implicit approach requires existing storage interface involves inference additional information adhering existing interfaces semantically-smart disks implicit approach subsection discuss alternative implicit approaches sds approach alternative file system infer information storage system contrast sds approach storage system infers information file system main problem approach inadequacy implicit observation channels file system infer information implicit techniques rely careful observations implicit channel efficacy inference depends richness implicit channel viewpoint storage system channel rich file system inevitably store data storage system contents data written file system constitutes rich information channel storage system semantic disks channel make inferences contrast information channel viewpoint file system fragile file system observe specific timing characteristics requests limited channel insufficient range optimizations modern storage systems perform implicit approach pertinent black-box approach storage system simply logical block stream make inferences correlating related blocks based observing sequences blocks accessed approach advantage requiring information file system main disadvantage black-box approach applicability limited small class functionality implementing functionality correctness paramount black-box approach fundamentally heuristic approximate techniques provide guarantees terms accuracy information show rest thesis sds approach enables implementing aggressive classes functionality utilize semantic knowledge ways impact correctness black-box techniques fragile concurrent interleavings independent streams evaluation methodology evaluate techniques semantic inference case studies utilizing information prototype implementations prototype sds employ software-based infrastructure implementation inserts pseudodevice driver kernel interpose traffic file system disk similar software raid prototype appears file systems device file system mounted primary advantage prototype observes block-level information traffic stream actual sds file system conceptually transferring functionality pseudo driver actual hardware prototype straightforward current infrastructure differs important ways true sds importantly prototype direct access lowlevel drive internals current head position information made difficult sds 
runs system host interference due competition resources performance overheads incurred prototype pessimistic estimates actual overheads performance characteristics microprocessor memory system actual sds high-end storage arrays significant processing power processing capacity trickle lower-end storage systems platform experimented prototype sds linux linux operating systems underneath ext ext vfat file systems limited experience underneath windows ntfs file system interpose underneath virtual machine running windows initial case studies work underneath ext file system case studies d-graid faded operate underneath file systems experiments paper performed processor slow modern standards mhz pentium iii processor k-rpm ibm lzx disks experiments employ fast system comprised ghz pentium gauge effects technology trends chapter acquiring semantic knowledge true knowledge confucius chapter discuss semantically-smart disk system tracks file system-specific semantic information underneath block-based interface identify classes semantic information pertinent storage system static dynamic present techniques track classes information discuss file system asynchrony significantly limits extent accuracy semantic information tracked sds finally evaluate costs techniques semantic inference static information basic piece information sds requires knowledge key on-disk data structures file system information static change version file system structure inode linux ext file system specific fields superblock examples outcome changed succeed receive success message actions enabling condition imply left true succeed action enabling condition conjunct imply left true receive success message action proof sketch conjunct depends lasttried status ballot action lasttried action set status action increases lasttried owner leaves true completely element added start polling action conjunct specification action imply adding element falsify conjunct changed adding priest vot affect proof sketch votes removed action change maxvote adds vote cast receive beginballot message action action prevbal prevdec beginballot conjunct implies action add vote conjunct implies ballot vote added enabling condition assumption holds executing action definition maxvote imply action leaves conjuncts true conjunct left true prevbal changed setting nextbal nextbal decreased proof sketch conjunct depends values status prevvotes lasttried nextbal priests status changed idle idle ballot action sets prevvotes making vacuously true actions change prevvotes forget action leaves true sets status idle lamport acm transactions computer systems vol receive lastvote message action enabling condition lastvote conjunct receive lastvote message action preserves lasttried changed ballot action leaves true sets status nextbal increase make false finally maxvote lasttried pst changed pst added vot bal lasttried pst added vot receive beginballot message action nextbal pst bal case implies bal lasttried proof sketch status set polling start polling action action enabling condition guarantees conjunct true adds ballot makes conjunct true action quorum decree lasttried leaving status equal polling prevvotes changed status polling changed adding elements adding priest vot remaining possibility falsifying addition element voters receive voted message action voted conjunct conjunct action enabling condition imply element added voters vot ballot existence asserted proof sketch bal qrm changed conjunct falsified adding ballot start polling majority set action status equals conjunct action leaves true assertion enabling condition majority set implies action leaves conjunct true ways falsifying changing maxvote bal qrm adding vote adding ballot vote added receive beginballot message action implies action adds vote vote cast soit change maxvote bal qrm conjunct implies ballot added start polling action falsify proof sketch falsified adding message changing variable depends lasttried nextbal decreased changing make false outcome changed blank changing falsify changed adding ballots adding votes change make part-time parliament acm transactions computer systems vol false addition vote pst makes lastvote conjunct false changing maxvote pst happen pst votes ballot bal pst vote ballot number nextbal pst assumption conjunct holds initially implies nextbal pst check message satisfies condition conjunct nextballot definition send nextballot message action conjunct lastvote enabling condition send lastvote message action imply maxvote nextbal maxvote lastvote message action satisfies condition beginballot definition send beginballot message action voted definition maxvote definition send voted message action success definition send success message acknowledgments daniel duchamp pointed state machine implementation discussions mart abadi andy hisgen tim mann garret swart led paxos levni gki mpa provided invaluable assistance paxon dialect bernstein hadzilacos goodman concurrency control recovery database systems addison-wesley longman publ reading prisco lampson lynch revisiting paxos algorithm proceedings international workshop distributed algorithms mavronicolas tsigas eds lecture notes computer science vol springer-verlag berlin germany dijkstra self-stabilizing systems spite distributed control commun acm dwork lynch stockmeyer consensus presence partial synchrony acm apr fekete lynch shvartsman partitionable group communication service proceedings annual acm symposium principles distributed computing acm press york fischer lynch paterson impossibility distributed consensus faulty process acm jan gray cheriton leases efficient fault-tolerant mechanism distributed file cache consistency sigops oper syst rev dec lamport acm transactions computer systems vol keidar dolev efficient message ordering dynamic networks proceedings annual acm symposium principles distributed computing acm press york ladin liskov shrira ghemawat providing high availability lazy replication acm trans comput syst nov lamport time clocks ordering events distributed system commun static acm information lamport important note static information include entire realm information on-disk layout file systems store file data inodes small files cases format inode strictly static file system case determining inode stores data inline based specific field inode field static fields inode constitute static information file system immutability static knowledge file system imparting information sds straight-forward directly embed information sds hardcoding information sds communicate separate administrative channel offline fashion modern storage systems separate administrative channels alternative encapsulate information add-on hardware card plugs storage system summary techniques assume white-box knowledge static information approach impart static information automatically fingerprint on-disk structures file system gray box techniques techniques enable automatic detection minor on-disk formats remainder thesis assume sds static information on-disk format white-box techniques mentioned embedding static information file system sds raises concern tying storage system specific file system discussed chapter stability on-disk formats small number file systems popular creating dependency reasonable current storage systems instances dependency file systems databases dynamic information static information on-disk structures file system crucial semantic disk static information sufficient provide large class functionality enable functionality sds requires dynamic information operation file system dynamic information involves properties blocks constantly changing file system higher level operations file system performs knowing sds block live dead valid data free dynamic information constantly blocks allocated deleted file system section describe techniques infer kinds dynamic information sds techniques involve exploiting static knowledge on-disk structures sds carefully watch updates structures sds snoops traffic inode blocks data bitmap blocks cases sds requires functionality identify block changed order correlate higher level file system activity led sds infer data block allocated file system observes bit change data bitmap sds tracks blocks block differencing change detection potentially expensive operations sds reasons compare current block version block sds fetch version block disk avoid overhead cache blocks employed comparison expensive find location difference byte block compared byte block cost reduced metadata blocks skipping portions block uninteresting differencing inode block sds skip scanning inodes marked deleted quantify costs section describe techniques common inferences made sds classification tracking type block association inode block discuss sds track higher level file system activity file creations deletions process operation inferencing sds techniques implement desired functionality simplicity discuss techniques based assumption file system synchronous meta-data updates bsd ffs file system linux ext synchronous mode fit assumption section discuss implications asynchrony complicate techniques tracking dynamic information classification type block pieces information sds exploit identifies block important metadata block replicate block high degree reliability knowing block type enables sophisticated inferences built top block indirect block pointers block interpreted meaningfully block type determined direct indirect classification direct classification blocks easily identified location disk indirect classification blocks identified additional information identify directory data indirect blocks inode examined permissions ptr block type dir inference block directory inode figure indirect classification direct classification direct classification simplest efficient form on-line block identification sds information requires static knowledge file system sds determines type statically-typed blocks performing simple bounds check calculate set block ranges block falls ffs-like file system superblock bitmaps inode blocks identified technique indirect classification indirect classification required type block vary dynamically simple direct classification precisely determine type block ffs-like file systems linux ext ext block data region file system data block directory block indirect pointer block single double triple indirect block indirect classification cases determine precise type block illustrate concepts focus directory data differentiated file data steps identifying indirect blocks versus pure data similar describe briefly identifying directory data basic challenge identifying data block belongs file directory track inode points data check type file directory perform tracking sds snoops inode traffic disk directory inode observed data block numbers inserted dir blockshash table sds removes data blocks hash table observing blocks freed block differencing bitmaps sds identify block file directory block presence table directory data problem approach sds perform excess work obliviously inserts data blocks hash table directory inode read written inode recently passed sds causing hash table updated optimize performance sds infer block added modified deleted time directory inode observed ensure blocks added deleted hash table process operation inferencing detail section identifying indirect blocks process identifying indirect blocks identical identifying directory data blocks case sds tracks indirect block pointers inodes read written maintaining hash tableindirect blocks single double triple indirect block addresses sds determine block data region indirect block association association refers ability sds re-establish semantic relationship blocks types associations sds sds associate inode directory inode sds infer pathname inode pathname file block belongs information sds decides perform optimizations specific type file inferred pathname association connect data blocks inodes chapter show sds employ smarter layout data raid systems availability set blocks belong single file association achieved simple space-consuming approach similar indirect classification sds snoops inode traffic inserts data pointers address inode hash table concern table size accurate association table grows proportion number unique data blocks read written storage system system booted approximate information tolerated sds size table bounded operation inferencing block classification association provide sds efficient identifying special kinds 
blocks operation inferencing understand semantic meaning observed blocks outline sds identify file system operations observing key illustrative purposes section examine sds infer file create delete operations discussion specific ext similar techniques applied file systems file creates deletes steps identifying file creates deletes actual detection create delete determining inode affected describe detection mechanisms logic determining inode detection mechanism involves inode block inode block written sds examines determine inode created deleted valid inode non-zero modification time deletion time modification time non-zero deletion time non-zero means inode newly made valid created similarly reverse change newly freed inode deleted file inode number calculated physical position inode disk on-disk inodes inode numbers detection mechanism involves inode bitmap block bit set inode bitmap file created inode number represented bit position similarly newly reset bit deleted file update directory block indication newly created deleted file directory data block written sds examines block previous version directory entry dentry added inode number file obtained dentry case removed dentry contents dentry inode number deleted file newly created deleted file choice mechanism combinations thereof depends functionality implemented sds newly created deleted file directory block-based solution readily convey exact file added removed file system operations general technique inferring logical operations observing blocks versions detect file system operations note cases conclusive inference specific logical operation sds observe correlated multiple meta-data blocks sds infer file renamed observes change directory block entry inode number stays note version number inode stay similarly distinguish creation hard link normal file directory entry file inode examined accuracy inference techniques assume file system synchronous reflects metadata updates disk immediately file system tracking dynamic information accurate long information pertains on-disk state sds implicitly track in-memory state file system contents file system buffer cache inference uncertain synchronous file system subsection show modern file systems exhibit synchronous metadata updates tracking dynamic information on-disk state uncertain inaccurate dealing asynchrony techniques previous section classification association operation inferencing basic trait modern file systems asynchrony modern file systems exhibit asynchrony updates data metadata file systems typically delay updates disk reorder writes disk scheduling asynchrony significantly complicates techniques tracking dynamic information indirect classification indirect classification depends sds observing inode block observing actual block write specifically data block present indirect blocks hash table sds infers data corresponds regular file block cases concerned inode sds result hash table situation occur large file created blocks allocated existing files file system guarantee inode blocks written data blocks sds incorrectly classify newly written data blocks problem occur classifying data blocks read case reads file system read inode block data block find data block number sds inode correctly identify subsequent data blocks solve problem sds buffer writes time classification made deferred classification occurs inode written disk data block freed inferred monitoring data bitmap traffic move subtle problem indirect classification due asynchrony detecting indirect pointer block order detect block indirect block sds observe inode indirect pointer field pointer address block formally identify indirect block semantic disk inode block indirect pointer field relevant inode block written disk disk infers indirect block records information observes block written information classify treat block indirect block due delayed write reordering behavior file system time disk writes block freed original inode reallocated inode type normal data block disk operations place memory reflected disk inference made semantic disk block type wrong due inherent staleness information tracked association similar uncertainty arises tracking forms dynamic information association associating block file uncertain observing inode owning block observing actual block block deleted inode reassigned inode operation inferencing techniques operation inferencing impacted asynchrony sds relies inode bitmap differencing technique identify file creates deletes section miss create quickly subsequent delete inode sds observe change bitmap operations grouped due delayed write file system sds track information partially change version number valid inode delete create occurred asynchrony file system effectively results suppression information sds imposes hard limits accuracy sds impact uncertainty asynchrony imposes fundamental limits accuracy information tracked sds preclude utilizing information provide interesting functionality kinds functionality easily amenable inaccurate information utilize semantic information impact correctness sds utilize information block type selectively cache metadata blocks non-volatile ram case misclassification directory block data block worst lead sub-optimal utilization nvram cache space performance issue concern terms correctness functionality utilizes semantic information track contents cache host system disk system avoid caching blocks optimizations inherently tolerant inaccuracy challenging class functionality semantic information directly affect correctness set guarantees system provide chapters describe case studies levels stringency degree correctnesssensitive techniques circumventing inaccuracy specific funcindirect block-inode operation classification association inferencing sync async sync async sync async create create delete delete mkdir rmdir table sds time overheads table breaks costs indirect classification block-inode association operation inferencing microbenchmarks row stress aspects action create benchmark creates files size delete benchmark similarly deletes files mkdir rmdir benchmarks create remove directories result presents average overhead operation extra time sds takes perform classification association inferencing experiments run slow system ibm lzx disk linux ext mounted synchronously sync asynchronously async tionality implemented sds show case studies build complex functionality correctness-sensitive potentially inaccurate information evaluation section evaluate costs tracking dynamic information sds specifically examine time space overheads classification association operation inferencing chapter software-based prototype environment time overheads reported pessimistic real hardware-based implementation lower overhead due absence cpu memory contention time overheads classification association operation inferencing potentially costly operations sds subsection employ series microbenchmarks illustrate costs actions results experiments sds underneath linux ext presented table action microbenchmark cases case file system mounted synchronously ensuring meta-data operations reach sds order allowing sds guarantee correct classification additional effort synchronous mounting linux ext similar traditional ffs handling meta-data updates metadata updates written immediately disk case file system mounted asynchronously case correct classification association guaranteed microbenchmarks perform basic file system operations including file directory creates deletes report per-file per-directory overhead action test experiments make number observations operations tend cost order tens microseconds file directory operations require complete cost due per-block cost operation inferencing synchronous mode create workload takes roughly corresponds base cost create workload cost approximately block costs rise file size increases sds incurs small per-block overhead compared actual disk writes number milliseconds complete cases overheads ext file system run 
asynchronous mode lower run synchronous mode asynchronous mode numerous updates meta-data blocks batched costs block differencing amortized synchronous mode meta-data operation reflected disk system incurring higher overhead sds observe synchronous mode classification expensive association expensive inferencing asynchronous mode relative difference time overheads forms dynamic inference insignificant space overheads sds require additional memory perform classification association operation inferencing specifically hash tables required track mappings data blocks inodes caches needed implement efficient block differencing quantify memory overheads variety workloads table presents number bytes hash table support classification association operation inferencing sizes maximum reached run workload netnews postmark modified andrew benchmark netnews postmark vary indirect block-inode operation classification association inferencing netnews netnews netnews postmark postmark postmark andrew table sds space overheads table presents space overheads structures performing classification association operation inferencing workloads netnews postmark modified andrew benchmark workloads netnews postmark run amounts input correspond roughly number transactions generates netnews implies transactions run number table represents maximum number bytes stored requisite hash table benchmark run hash entry bytes size experiment run slow system linux ext asynchronous mode ibm lzx disk workload size caption table dominant memory overhead occurs sds performing block-inode association classification operation inferencing require table sizes proportional number unique meta-data blocks pass sds association requires information unique data block passes worst case entry required data block disk memory disk space space costs tracking association information high prohibitive memory resources scarce sds choose tolerate imperfect information swap portions table disk addition hash tables needed perform classification association operation inferencing cache data blocks perform block differencing effectively recall differencing observe pointers allocated freed inode indirect block check time fields inode changed detect bitwise bitmap monitor directory data file creations deletions performance system sensitive size cache cache small difference calculation fetch version block disk avoid extra size cache roughly proportional active metadata working set postmark workload found sds cache approximately blocks hold working set cache smaller block differencing operations disk retrieve older copy block increasing run-time benchmark roughly summary chapter presented basic techniques sds track semantic information file system based minimal amount static information on-disk layout file system sds successively builds information track complex pieces information shown fundamental limits extent accuracy semantic information tracked sds asynchrony meta-data data updates file system results obfuscation information semantic disk chapters explore sds utilize potentially inaccurate information implement kinds functionality precluded today storage systems chapter exploiting semantic knowledge tis knowing makes wise man thomas fuller chapter describe simple case studies implementing functionality sds implement drive raid semantic knowledge case studies presented chapter built file system proper present case studies illustrate range functionality provided sds chapters describe complex case studies represent pieces functionality implemented storage system file system today file system model functionality implemented semantically-smart disk system based assumptions dynamic behavior file system functionality caching fundamentally tolerant inaccuracy afford lax assumptions dynamic file system behavior functionality utilizes semantic information impacts correctness precise file system assumptions based crucial case study chapter belongs non-correctnesscritical category case study correctness-sensitive works underneath specific file system behavior assumes file system synchronous words file system writes metadata blocks synchronously linux ext file system synchronous mount mode fits assumption chapters successively refine simplistic file system model general detailed dynamic properties file system implement similar correctness-sensitive functionality d-graid chapter considers general asynchronous file system faded chapter considers refinements asynchronous file systems provide kinds guarantees update behavior semantic caching case study explores semantic information caching sds examine sds semantic knowledge store important structures non-volatile memory specifically exploit semantic knowledge store nvram journal linux ext journaling file system variants basic idea caching metadata blocks inodes directories nvram case study meant illustrative potential semantically-aware nvram caching implement journal caching sds sds sds recognize traffic journal redirect nvram straightforward blocks allocated file system journal part static information file system designated set blocks assigned journal specific inode number points journal inode pre-allocated point set journal blocks part file system initialization classifying caching data reads writes journal file sds implement desired functionality prototyping purposes treat portion volatile ram nvram tables shows performance sds simple nvram caching journal ext effective reducing run times dramatically greatly reducing time write blocks stable storage lru-managed cache effective case cache large working set worse performance lru compared default ext points overhead introduced pseudodevice driver layer main benefits structural caching nvram size cached structures sds guarantees effective cache utilization hybrid combine worlds storing important structures journal meta-data nvram managing rest cache space lru fashion create create sync ext lru sds lru sds journal caching sds table journal caching table shows time seconds create -kb files ext sds sds performs lru nvram cache management cache journal caching sds storing journal nvram create benchmark performs single sync files created create sync benchmark performs sync file creation inducing journaling-intensive workload experiments run slow system running linux utilizing ibm lzx disk semantic knowledge efficient management main memory cache storage system simple lru management disk cache duplicate contents file system cache wastes memory storage system waste onerous storage arrays due large amounts memory contrast sds understanding file system cache blocks intelligently specifically sds cache exclusive set blocks cached file system avoid wasteful replication caching sds made effective sds identifies blocks deleted removes cache freeing space live blocks related optimization sds smarter prefetching sds file awareness make guess block read sds observes read directory block prefetch inode blocks pertaining inodes pointed directory tolerance inaccuracy optimizations discussed smarter caching sds belong category semantic disk functionality naturally tolerant inaccurate information optimizations utilize semantic information impact correctness work underneath asynchronous file systems occasional inaccuracy make optimizations marginally effective correctness implications optimizations simpler build sds ignore possibility inaccuracy information case study explore stringent requirements accuracy journaling case study complex demanding semantic information semantic knowledge sds implement journaling underneath unsuspecting file system chapter journaling makes crash recovery simple efficient committing metadata operations atomic transactions journaling avoids expensive disk scans found ffs ffs-like file systems main difficulty addressed case study track information disk efficient manner view journaling sds extreme case helps understand amount information obtain disk level unlike smarter caching case study journaling sds requires great deal precise information file system view 
extreme requirements accuracy information required journaling sds implement journaling sds underneath synchronous file system linux ext mounted synchronous mode simplification focus challenges implementing complex functionality assuming accurate information file system chapters explore similarly complex case studies working general asynchronous file system behaviors design implementation fundamental difficulty implementing journaling sds arises fact disk transaction boundaries blurred instance file system file create file system inode block parent directory block inode bitmap block updated part single logical create operation block writes grouped single transaction straight-forward fashion sds sees stream meta-data writes potentially interleaved logical file system operations challenge lies identifying dependencies blocks handling updates atomic transactions journaling sds maintains transactions coarser granularity type purpose currcache cache needed blocks current epoch nextcache cache needed blocks epoch addptrs hash tracks added block ptrs inodes addblks hash tracks added blocks bitmaps addind hash tracks write indirect blocks freeblks hash tracks freed inode pointers bitmaps addfile hash tracks time timeout fault-tolerant distributed systems acm trans program lang syst apr lampson build highly system consensus distributed algorithms babaoglu marzullo eds springer lecture notes computer science vol springer-verlag berlin germany oki liskov viewstamped replication general primary copy proceedings annual acm symposium principles distributed computing toronto ontario august acm press york schneider implementing fault-tolerant services state machine approach tutorial acm comput surv dec skeen crash recovery distributed database system thesis california berkeley berkeley received january accepted march part-time parliament acm transactions computer systems vol 
files inode dentry inode bitmap delfile hash tracks deleted files inode dentry inode bitmap dirty hash tracks dirty blocks fast hash performance optimization table journaling sds structures journaling file system basic approach buffer meta-data writes memory sds write disk in-memory state meta-data blocks constitute consistent meta-data state logically equivalent performing incremental in-memory fsck current set dirty meta-data blocks writing disk check succeeds current set dirty meta-data blocks form consistent state treated single atomic transaction ensuring on-disk meta-data contents remain previous consistent state fully updated current consistent state benefit coarse-grained transactions batching commits performance improved traditional journaling systems coarse-grained approach journaling brings important requirements efficient identifying keeping track dependencies meta-data blocks checking current in-memory state blocks consistent entire set dirty meta-data blocks constitute single transaction ensure continuous metadata-intensive workload indefinitely prevent reaching consistent state leading unbounded loss data crash meet requirements journaling sds maintains cache contents meta-data blocks efficiently identifying detects interesting change occurred meta-data blocks records change multiple hash tables hash table represents single type meta-data operation newly added bit inode bitmap newly added directory entry separate hash table kind meta-data operation occur detecting multiple meta-data blocks prune hash tables canceling lead consistent state file created journaling sds sees write inode-bitmap block detects inode bit set records inode number suitable hash table subsequently directory block write records directory entry pointing inode number added finally inode block write initialized inode state blocks constitute consistent state assuming place record current state consistent journaling sds removes inode number hash tables recorded similar fashion manage hash tables meta-data operations technique keeping track checking consistency involves checking hash tables empty efficient guarantee bounded loss data crash journaling sds mechanism limiting time elapse successive journal transaction commits journaling daemon wakes periodically configurable interval takes copy-on-write snapshot current dirty blocks cache current hash table state point subsequent meta-data operations update copy cache hash tables introduce additional dependencies current epoch commit decision-point actual commit meta-data operation treated part current epoch operation contributes resolving existing dependencies current epoch slightly complicated coarse block-level granularity cache multiple meta-data operations part current epoch rest epoch additional dependencies allowed current epoch maximum delay till commit limited configured periodicity journaling daemon delayed write interval file system sds idiosyncrasies ext file system made implementation complex instance synchronous mode ext write inode blocks size count inode requires journaling sds track additional state update based operations similarly writing inode blocks ext preserve temporal ordering updates taking place inode instance inode block write reflects deleted inode reflect creation inodes block happened past table displays data structures journaling sds fair amount complexity tracking needed information detect consistency mentioned journaling sds implementation assumes file system mounted synchronously robust sds requires verify assumption holds turn journaling meta-data state written disk journaling sds consistent synchronous asynchronous mount problem imposed asynchronous mount sds miss operations reversed file create delete lead dependencies resolved indefinite delays journal transaction commit process avoid problem journaling sds suspicious sequence meta-data blocks single change expected multiple inode bitmap bits change part single write turns journaling cases fall-back journaling sds monitors elapsed time commit dependencies prolong commit time threshold suspects asynchronous mount aborts journaling aborts journaling journaling sds relinquishes control clean flag file system fsck program forcing file system perform consistency check subsequent crashes journaling sds make consistency worse traditional ext irrespective mounted synchronously asynchronously evaluation evaluate correctness performance journaling sds verify correctness implementation inserted crash points specific places test workloads ran journaling sds recovery program replays log sds propagate committed transactions ran fsck file system verified inconsistencies reported repeated verification numerous runs confident existing implementation strong consistency guarantee ext fsck avoids high cost fsck-based recovery performance journaling sds summarized table interesting aspect note performance journaling sds requiring create read delete ext sync ext async ext ext sync journaling sds table journaling table shows time complete phase lfs microbenchmark seconds -kb files configurations compared ext linux mounted synchronously mounted asynchronously journaling ext linux journaling sds synchronously mounted ext linux experiment place slow system ibm lzx disk sds infrastructure initialization hash table cache direct classification indirect classification association operation inferencing case studies journal cache journaling sds table code complexity number lines code required implement aspects sds presented file system mounted synchronously performance similar asynchronous versions file system effect fact journaling sds delays writing meta-data disk due buffering operations reach consistent state read test sds similar performance base file system ext delete test similar performance journaling file system ext file creation sds pays significant cost relative ext overhead block differencing hash table operations noticeable impact purpose case study demonstrate sds implement complex functionality small overhead acceptable complexity analysis briefly explore complexity implementing software sds table shows number lines code components system case studies table code complexity found basic cache hash tables operation inferencing code smarter caching case study trivial implement top base infrastructure journaling sds requires thousand lines code due inherent complexity conclude including type functionality sds pragmatic summary chapter case studies functionality provided sds case studies provide file system-like functionality disk system existing disk systems fundamentally provide case study naturally tolerant inaccurate information case study journaling stringent requirements accuracy accommodate stringent requirements journaling case study required synchronous file system top 
chapters relax assumption building similar case studies complex correctness-sensitive work general asynchronous file system behaviors chapter improving availability d-graid tree falls forest hears make sound george berkeley chapter demonstrate feasibility building complex functionality sds asynchrony file system chapter presents design implementation evaluation d-graid gracefullydegrading quickly-recovering raid storage array exploits semantic information enable availability compared existing storage systems d-graid ensures files file system remain unexpectedly high number faults occur d-graid achieves high availability aggressive replication semantically critical data fault-isolated placement logically related data d-graid recovers failures quickly restoring live file system data hot spare introduction storage systems comprised multiple disks backbone modern computing centers storage system entire center grind halt downtime expensive on-line business world millions dollars hour lost systems storage system availability formally defined time failure mtbf divided sum mtbf time recovery mttr mtbfmtbf mttr order improve availability increase mtbf decrease mttr surprisingly researchers studied components storage availability increase time failures large storage array data redundancy techniques applied keeping multiple copies blocks sophisticated redundancy schemes parity-encoding storage systems tolerate small fixed number faults decrease time recovery hot spares employed failure occurs spare disk activated filled reconstructed data returning system normal operating mode quickly problem reduced availability due semantic ignorance techniques proposed improve storage availability narrow interface file systems storage curtailed opportunities improving mtbf mttr raid redundancy schemes typically export simple failure model fewer disks fail raid continues operate correctly disks fail raid unavailable problem corrected time-consuming restore tape raid schemes small disks working users observe failed disk system availability cliff result storage system laying blocks oblivious semantic importance relationship files corrupted inaccessible extra disk failure storage array information blocks live file system recovery process restore blocks disk unnecessary work slows recovery reduces availability ideal storage array fails gracefully disks system data unavailable ideal array recovers intelligently restoring live data effect important data disappear failure data restored earlier recovery strategy data availability stems berkeley observation falling trees file isn process access recovered failure solution d-graid explore concepts provide storage array graceful failure semantics present design implementation evaluation d-graid raid system degrades gracefully recovers quickly d-graid exploits semantic intelligence disk array place file system structures disks fault-contained manner analogous fault containment techniques found hive operating system distributed file systems unexpected double failure occurs d-graid continues operation serving files accessed d-graid utilizes semantic knowledge recovery specifically blocks file system considers live restored hot spare aspects d-graid combine improve effective availability storage array note d-graid techniques complementary existing redundancy schemes storage administrator configures d-graid array utilize raid level single disk fail data loss additional failures lead proportional fraction unavailable data built prototype implementation d-graid refer alexander alexander semantically-smart disk system exploits semantic knowledge implement graceful degradation quick recovery alexander functions underneath unmodified linux ext vfat file systems running general asynchronous file system behaviors d-graid demonstrates feasible build complex functionality fundamentally imperfect information key techniques key aspects alexander implementation graceful degradation selective meta-data replication alexander replicates naming system meta-data structures file system high degree standard redundancy techniques data small amount overhead excess failures render entire array unavailable entire directory hierarchy traversed fraction files missing proportional number missing disks fault-isolated data placement strategy ensure semantically meaningful data units failure alexander places semantically-related blocks blocks file storage array unit fault-containment disk observing natural failure boundaries found array failures make semanticallyrelated groups blocks unavailable leaving rest file system intact fault-isolated data placement improves availability cost related blocks longer striped drives reducing natural benefits parallelism found raid techniques remedy alexander implements access-driven diffusion improve throughput frequentlyaccessed files spreading copy blocks hot files drives system alexander monitors access data determine files replicate fashion finds space replicas pre-configured performance reserve opportunistically unused portions storage system evaluate availability improvements d-graid trace analysis simulation find d-graid excellent job masking arbitrary number failures processes enabling continued access important data evaluate prototype alexander microbenchmarks trace-driven workloads find construction graid feasible imperfect semantic knowledge powerful functionality implemented block-based storage array find run-time overheads d-graid small storage-level cpu costs compared standard array high show access-driven diffusion crucial performance live-block recovery effective disks under-utilized combination replication data placement recovery techniques results storage system improves availability maintaining high level performance rest chapter structured section present extended motivation graceful degradation present design principles d-graid section present trace analysis simulations section section outline file system model implementation d-graid based section present prototype implementation evaluate prototype section section present custom policies levels d-graid discuss resilience d-graid incorrect information section summarize section extended motivation section motivate graceful degradation multiple failures motivation graceful degradation arises fact users applications require entire contents volume present matters set files question arises realistic expect catastrophic failure scenario raid system raidsystem high mtbf reported disk manufacturers disk failure highly occur failed disk repaired multiple disk failures occur primary reasons correlated faults common systems expected raid carefully designed orthogonal manner single controller fault component error render fair number disks unavailable redundant designs expensive found higher end storage arrays gray points system administration main source failure systems large percentage human failures occur maintenance maintenance person typed wrong command unplugged wrong module introducing double failure page evidence suggests multiple failures occur ibm serveraid array controller product includes directions attempt data recovery multiple disk failures occur raidstorage array organization data stored file servers raidin computer science department servers single disk failed indicator informed administrators problem problem discovered disk array failed full restore backup ran days scenario graceful degradation enabled access large fraction user data long restore approach dealing multiple failures employ higher level redundancy enabling storage array tolerate greater number failures loss data techniques expensive three-way data mirroring bandwidth-intensive write redundant store graceful degradation complementary techniques storage administrators choose level redundancy common case faults graceful degradation enacted worse expected fault occurs mitigating ill effect design d-graid expectations discuss design d-graid present background information file systems data layout strategy required enable graceful degradation important design issues arise due layout process fast recovery graceful degradation ensure partial availability data multiple failures raid array graid employs main techniques fault-isolated data placement strategy d-graid places semantically-related set blocks unit fault containment found storage array simplicity discussion assume 
file semantically-related set blocks single disk unit fault containment generalize easily generalized failure boundaries observed scsi chains refer physical disk file belongs home site file disk fails fault-isolated data placement ensures files disk home site unavailable files remain accessible files technique selective meta-data replication d-graid replicates naming system meta-data structures file system high degree directory inodes directory data unix file system d-graid ensures live data reachable orphaned due multiple failures entire directory hierarchy remains traversable fraction missing user data proportional number failed disks d-graid lays logical file system blocks availability single file depends disks traditional raid array dependence set entire set disks group leading entire file system unavailability unexpected failure unixcentric typical layout fault-isolated data placement selective metadata replication depicted figure note techniques d-graid work meaningful subset file system laid single graid array file system striped multiple d-graid arrays single array meaningful view file system scenario d-graid run logical volume manager level viewing arrays single disk techniques remain relevant d-graid treats file system block type differently traditional raid taxonomy longer adequate describing d-graid behaves fine-grained notion raid level required d-graid employ redundancy techniques types data d-graid commonly employs n-way mirroring naming system meta-data standard redundancy techniques mirroring parity encoding raidfor user data note administrative control determines number failures d-graid degrade gracefully section explore data availability degrades varying levels namespace replication design considerations layout replication techniques required enable graceful degradation introduce number design issues highlight major challenges arise foo bar inode foo inode bar data bar data bar data root data foo inode root foo bar inode foo inode bar data bar data bar data root data foo inode root inode foo inode bar data bar data bar foo data root bar data foo inode root inode root inode root inode root inode fooinode fooinode foo foo data root foo data root foo data root bar data foo bar data foo bar data foo figure comparison layout schemes figures depict layouts file foo bar unix file system starting root inode directory tree file data vertical column represents disk simplicity assumes data redundancy user file data top typical file system layout non-d-graid disk system blocks pointers spread file system single fault render blocks file bar inaccessible left figure bottom fault-isolated data placement files directories scenario access inode file access data indirect pointer blocks constrained disk finally bottom selective meta-data replication replicating directory inodes directory blocks d-graid guarantee users files requisite pointers removed rightmost figure simplicity color codes white user data light shaded inodes dark shaded directory data semantically-related blocks fault-isolated data placement d-graid places logical unit file system data file fault-isolated container disk blocks graid considers related determines data remains failure basic approach file-based grouping single file including data blocks inode indirect pointers treated logical unit data technique user find files directory unavailable frustration confusion groupings preserve meaningful portions file system volume failure directory-based grouping d-graid ensures files directory unit fault containment automated options allowing users arbitrary semantic groupings d-graid treats unit load balance fault-isolated placement placing blocks file disks blocks isolated single home site isolated placement improves availability introduces problem load balancing space time components terms space total utilized space disk maintained roughly level fraction disks fail roughly fraction data unavailable balancing addressed foreground data allocated background migration files directories larger amount free space single disk handled potentially expensive reorganization reserving large extents free space subset drives files larger single disk split disks pressing performance problems introduced fault-isolated data placement previous work striping data disks performance compared sophisticated file placement algorithms d-graid makes additional copies user data spread drives system process call access-driven diffusion standard d-graid data placement optimized availability access-driven diffusion increases performance files frequently accessed surprisingly access-driven diffusion introduces policy decisions d-graid including place replicas made performance files replicate create replicas meta-data replication level degree meta-data replication d-graid determines resilient excessive failures high degree replication desirable meta-data replication costs terms space time space overheads trade-offs obvious replicas imply resiliency difference traditional raid d-graid amount space needed replication naming system meta-data dependent usage volume directories induces greater amount overhead time overheads higher degree replication implies lowered write performance naming system meta-data operations observed lack update activity higher levels directory tree lazy update propagation employed reduce costs fast recovery main design goal d-graid ensure higher availability fast recovery failure critical straightforward optimization d-graid recover live file system data assume restoring data live mirror hot spare straightforward approach d-graid simply scans source disk live blocks examining file system structures determine blocks restore process readily generalized complex redundancy encodings d-graid potentially prioritize recovery number ways restoring important files importance domain specific files users manner similar hoarding database coda exploring graceful degradation section simulation trace analysis evaluate potential effectiveness graceful degradation impact semantic grouping techniques quantify space overheads d-graid demonstrate ability d-graid provide continued access proportional fraction meaningful data arbitrary number failures importantly demonstrate d-graid hide failures users replicating important data simulations file system traces collected labs cover days activity data spread logical volumes level replication -way -way -way ext ext vfat vfat table space overhead selective meta-data replication table shows space overheads selective meta-data replication percentage total user data level naming system meta-data replication increases leftmost column percentage space overhead meta-data replication shown columns depict costs modest -way paranoid -way schemes row shows overhead file system ext vfat block size set space overheads examine space overheads due selective meta-data replication typical d-graid-style redundancy calculate cost selective metadata replication percentage overhead measured volumes trace data laid ext vfat file system running underneath ext selective meta-data replication applied superblock inode data block bitmaps inode data blocks directory files blocks replicated case vfat comprise fat directory entries calculate highest percentage selective meta-data replication overhead assuming replication user data user data mirrored overheads cut half table shows selective meta-data replication induces mild space overhead high levels meta-data redundancy linux ext vfat file systems -way redundancy meta-data space overhead incurred worst case vfat blocks increasing block size ext space due internal fragmentation larger directory blocks overheads decrease vfat phenomenon due structure vfat fixed-sized file system block size grows 
file allocation table shrinks blocks directory data grow static availability examine d-graid availability degrades failure semantic grouping strategies strategy file-based grouping information single file failure boundary disk directory-based grouping allocates files directory analysis place entire files directories trace simulated -disk system remove simulated disks measure percentage directories assume user data redundancy d-graid level figure shows percent directories directory files accessible subdirectories files figure observe graceful degradation works amount data proportional number working disks contrast traditional raid disk crashes lead complete data unavailability fact availability degrades slightly expected strict linear fall-off due slight imbalance data placement disks directories modest level namespace replication -way leads good data availability failure conclude file-based grouping files directory disappear failure leading user dissatisfaction dynamic availability finally simulating dynamic availability examine users applications oblivious d-graid operating degraded mode specifically run portion trace simulator number failed disks record percent processes observed failure run experiment find namespace replication files needed processes replicated experiment set degree namespace replication full replication vary level replication contents popular directories usr bin bin lib figure shows replicating contents directories percent processes run ill-effect lower expected results figure directories replicated percentage processes run completion disk failure expected reason clear substantial number processes require executable libraries run correctly popular direcpercent directories completely number failed disks static data availability directory-based -way directory-based -way file-based -way directory-based -way figure static data availability percent entire directories shown increasing disk failures simulated system consists disks loaded trace strategies semantic grouping shown file-based directory-based line varies level replication namespace metadata point shows average standard deviation trials trial randomly varies disks fail tory replication excellent availability failure fortunately popular files read directories wide-scale replication raise write performance consistency issues space overhead due popular directory replication minimal sized file system trace directories account total file system size file system model journaling case study previous chapter considered synchronous file system committed metadata updates synchronously disk case study general file system model tune modern percent unaffected processes number failed disks dynamic per-process availability popular replication -way -way -way figure dynamic data availability figure plots percent processes run unaffected disk failure busy hour trace degree namespace replication set aggressively line varies amount replication popular directories -way implies directories replicated -way -way show modest extreme amount replication means deviations trials shown file system behaviors specifically implementation d-graid works asynchronous file systems ext asynchronous mode generic behaviors hold asynchronous file system arbitrary ordering arbitrary ordering file system orders updates file system arbitrarily update order relied garner extra information nature disk traffic ffs meta-data updates forced disk synchronously arrive disk data file systems careful order updates disk ordering assumed remain general avoid assumptions chapter analyze assumption greater detail explore semantic inference techniques simplified file system simple ordering guarantees delayed update asynchronous file system delays updates disk performance reasons delays found writing data disk file systems including lfs buffers data memory flushing disk improves performance batching small updates single large avoiding write data created deleted contrast delayed updates file system immediately reflects file system disk ext mounted synchronously behave manner hidden operation hidden operation property refers file system reflecting operations disk hand-in-hand delayed updates file system delay disk update file creation subsequent delete obviates reflect create disk implementation making d-graid discuss prototype implementation d-graid alexander alexander fault-isolated data placement selective meta-data replication provide graceful degradation failure employs access-driven diffusion correct performance problems introduced availability-oriented layout alexander replicates namespace system meta-data administratorcontrolled stores user data raidor raidmanner refer systems d-graid levels section present implementation graceful degradation live-block recovery complexity discussion centered graceful degradation simplicity exposition focus construction alexander underneath linux ext file system end section discuss differences implementation underneath vfat figure anatomy write figure depicts control flow sequence write operations alexander figure inode block written alexander observes contents inode block identifies newly added inode selects home site inode creates physical mappings blocks inode home site inode block aggressively replicated figure alexander observes write data block inode mapped write directly physical block figure alexander write unmapped data block defers writing block alexander finally observes inode fourth figure creates relevant mappings observes blocks deferred issues deferred write relevant home site graceful degradation present overview basic operation graceful degradation alexander describe simple cases proceeding intricate aspects implementation indirection map similar scsi-based raid system alexander presents host systems linear logical block address space internally alexander place blocks facilitate graceful degradation control placement alexander introduces transparent level indirection logical array file system physical placement disks indirection map imap similar structures unlike systems imap maps live logical file system block replica list physical locations unmapped blocks considered free candidates d-graid reads handling block read requests d-graid level straightforward logical address block alexander imap find replica list issues read request replicas choice replica read based criteria alexander randomized selection presence access-driven diffusion diffused copy preference fault-isolated copy writes contrast reads write requests complex handle alexander handles write request depends type block written figure depicts common cases block static meta-data block inode bitmap block unmapped alexander allocates physical block disks replica reside writes copies note alexander easily detect static block types inode bitmap blocks underneath unix file systems simply observing logical block address inode block written d-graid scans block newly added inodes understand inodes d-graid compares newly written block copy process referred block differencing inode d-graid selects home site lay blocks belonging inode records inode-to-homesite hashtable selection home site balance space allocation physical disks d-graid greedy approach selects home site free space write unmapped block data region data block indirect block directory block allocation d-graid file block belongs actual home site case d-graid places block deferred block list write disk learns file block crash inode write make block inaccessible file system in-memory deferred block list reliability concern d-graid newly added block pointers inode indirect block written newly added block pointer refers unmapped block d-graid adds entry imap mapping logical block physical block home site assigned inode newly added pointer refers block deferred list d-graid removes block deferred list issues write physical block writes deferred blocks written owner inode blocks inode written subsequent data writes mapped disk directly block type 
interest d-graid data bitmap block data bitmap block written d-graid scans newly freed data blocks freed block d-graid removes logical-to-physical mapping exists frees physical blocks block deferred list freed block removed deferred list write suppressed data blocks written file system deleted inode written disk generate extra disk traffic similar optimizations found file systems removing blocks deferred list important case freed blocks alexander observe owning inode deferred block stays deferred list bounded amount time inode owning block written bitmap block indicating deletion block written exact duration depends delayed write interval file system block reuse discuss intricate issues involved implementing graceful degradation issue block reuse existing files deleted truncated files created blocks part file reallocated file d-graid place blocks correct home site reuse blocks detected acted graid handles block reuse manner inode block indirect block written d-graid examines valid block pointer physical block mapping matches home site allocated inode d-graid mapping block correct home site write block made context file home site copied physical location location blocks copied added pending copies list background thread copies blocks home site frees physical locations copy completes dealing imperfection difficulty arises semantically-smart disks underneath typical file systems exact knowledge type dynamically-typed block impossible obtain discussed section alexander handle incorrect type classification data blocks file data directory indirect blocks d-graid understand contents indirect blocks pointers place file blocks home site due lack perfect knowledge fault-isolated placement file compromised note data loss corruption issue goal dealing imperfection conservatively avoid eventually detect handle cases specifically block construed indirect block written assume valid indirect block live pointer block graid action cases case pointer refer unmapped logical block mentioned d-graid creates mapping home site inode indirect block belongs indirect block pointer valid mapping correct mapping indirect block misclassified pointer invalid d-graid detects block free observes data bitmap write point mapping removed block allocated file bitmap written d-graid detects reallocation inode write file creates mapping copies data contents home site discussed case potentially corrupt block pointer point mapped logical block discussed type block reuse results mapping copy block contents home site indirect block pointer valid mapping correct block indirect block misclassification alexander wrongly copies data home site note data accessible original file block belongs blocks incorrect home site fortunately situation transient inode file written d-graid detects reallocation creates mapping back original home site restoring correct mapping files accessed properly laid infrequent sweep inodes rare cases improper layout optimizations d-graid eventually move data correct home site preserving graceful degradation reduce number times misclassification occurs alexander makes assumption contents indirect blocks specifically number valid unique pointers null pointers alexander leverage assumption greatly reduce number misclassifications performing integrity check supposed indirect block integrity check reminiscent work conservative garbage collection returns true pointers -byte words block point valid data addresses volume nonnull pointers unique set blocks pass integrity check corrupt data contents happened evade conditions test run data blocks local file system small fraction data blocks pass test blocks pass test reallocated file data block indirect block misclassified access-driven diffusion issue d-graid address performance fault-isolated data placement improves availability cost performance data accesses blocks large file directory-based grouping files directory longer parallelized improve performance alexander performs access-driven diffusion monitoring block accesses determine block ranges hot diffusing blocks replication disks system enhance parallelism sensitive data contents semantically-smart disks place requirement file system traces include user data blocks privacy concerns campaign encounter difficult overcome access-driven diffusion achieved logical physical levels disk volume logical approach access individual files monitored considered hot diffused per-file replication fails capture sequentiality multiple small files single directory pursue physical approach alexander replicates segments logical address space disks volume file systems good allocating contiguous logical blocks single file files directory replicating logical segments identify exploit common access patterns implement access-driven diffusion alexander divides logical address space multiple segments normal operation gathers information utilization segment background thread selects logical segments remain hot number consecutive epochs diffuses copy drives system subsequent reads writes replicas background updates original blocks imap entry block copy date policy deciding segments diffuse simplistic prototype implementation detailed analysis policy space access-driven diffusion left future work amount disk space allocate performance-oriented replicas presents important policy decision initial policy alexander implements reserve minimum amount space system administrator replicas opportunistically free space array additional replication approach similar autoraid mirrored data autoraid identify data considered dead file system written contrast d-graid semantic knowledge identify blocks free live-block recovery implement live-block recovery d-graid understand blocks live knowledge correct block live considered dead lead data loss alexander tracks information observing bitmap data block traffic bitmap blocks liveness state file system reflected disk due reordering delayed updates uncommon observe write data block bit set data bitmap account d-graid maintains duplicate copy bitmap blocks sees write block sets bit local copy bitmap duplicate copy synchronized file system copy data bitmap block written file system conservative bitmap table reflects superset live blocks file system perform live-block recovery note assume pre-allocation state bitmap written disk subsequent allocation locking linux modern systems ensures technique guarantees live block classified dead disk block live longer situation arise file system writes deleted blocks disk implement live-block recovery alexander simply conservative bitmap table build list blocks restored alexander proceeds list copies live data hot spare aspects alexander number aspects implementation required successful prototype subsection briefly describe key aspects physical block allocation logical array blocks exported scsi property block numbers contiguous logical address space mapped contiguous physical locations disk property empowers file systems place data contiguously disk simply allocating contiguous logical blocks data traditional raid property straightforward preserve physical blocks assigned round-robin fashion disks contiguity guarantees continue hold physical block assign logical block simple arithmetic calculation logical block number d-graid deciding physical block allocate newly written logical block straightforward decision depends file logical block belongs logical offset file fault-isolated placement set contiguous logical blocks belong single file map contiguous physical blocks disk logical block set mapped physical block block set mapped physical block order preserve contiguity expectations larger granularity d-graid balances space utilization files allocation policy large values block map physical block number disks array choice policies requires estimates 
file size dynamic prototype addresses issue simple technique space reservations alexander utilizes knowledge inodes indirect blocks priori estimates exact size entire file large segment file case indirect block observes inode written file size blocks reserves contiguous blocks home-site assigned file actual logical blocks written subsequently reserved space note blocks deferred inodes indirect blocks observed write logical block prior reservation inodes indirect blocks written periodically seconds size information obtained writes stable just-in-time commit space reservations depend size information extracted inode indirect blocks indirect block detection fundamentally inaccurate misclassified indirect block result spurious reservations hold physical space prevent alexander employs lazy allocation actual physical blocks committed logical block written reservation priori reservations viewed soft space reclaimed required interaction deferred writes sync alexander defers disk writes logical blocks observed owning inode arbitrary deferral potentially conflict application-level expectations sync operation issued sync returns application expects data disk preserve semantics d-graid handles inode indirect block writes specially d-graid return success write inode indirect block deferred writes blocks pointed inode indirect block reached disk sync operation complete inode block write returns deferred writes guaranteed complete sync returns argument extends fsync return writes pertaining file complete weakness approach application performs equivalent fdatasync flushes data blocks disk metadata technique preserve expected semantics inconsistent fault behavior linux ext interesting issue required change design behavior linux ext partial disk failure process read data block unavailable ext issues read returns failure process block recovery process issues read ext issue read works expected process open file inode unavailable ext marks inode suspicious issue request inode block alexander recovered block avoid change file system retain ability recover failed inodes alexander replicates inode blocks namespace meta-data collocating data blocks file persistence data structures number structures alexander maintains imap reliably committed disk preferably good performance buffered small amount non-volatile ram note nvram serve cache actively accessed entries data structures space requirements acceptable level current prototype simply stores data structures memory complete implementation require backed persistently popular directory replication important component missing alexander prototype decision popular read-only directories usr bin replicate widely alexander proper mechanisms perform replication policy space remains unexplored initial experience simple approach based monitoring frequency inode access time updates effective alternative approach administrators directories treated manner alexander fat surprised similarities found implementing d-graid underneath ext vfat vfat overloads data blocks user data blocks directories alexander defer classification blocks manner similar ext implementation expected implementation basic mechanisms graid physical block allocation allocation home sites files tracking replicas critical blocks shared versions d-graid instances vfat implementation graid differed interesting ways ext version fact pointers file located file allocation table made number aspects d-graid simpler implement vfat indirect pointers worry copy fat block written version directly compared previous contents block accurate information blocks newly allocated deleted ran occasional odd behavior linux implementation vfat linux write disk blocks allocated freed avoiding obvious common file system optimization behavior vfat estimate set live blocks strict superset blocks live indicative untuned nature linux implementation served indicator semantic disks wary assumptions make file system behavior evaluating alexander present performance evaluation alexander similar sds prototype section alexander prototype constructed software raid driver linux kernel focus primarily linux ext variant include baseline measurements vfat system answer questions alexander work correctly time overheads introduced effective access-driven diffusion fast live-block recovery benefits expect d-graid complex implementation misplaced blocks time sec misplaced blocks remapping remapping close-up close-up figure errors placement figure plots number blocks wrongly laid alexander time running busy hour trace experiment run disks total number blocks accessed trace alexander work correctly alexander complex simple raid systems gain confidence alexander operates correctly put system numerous stress tests moving large amounts data system problems extensively tested corner cases system pushing situations difficult handle making system degrades gracefully recovers expected repeatedly crafted microbenchmarks stress mechanisms detecting block reuse handling imperfect information dynamically-typed blocks constructed benchmarks write user data blocks disk worst case data data appears valid directory entries indirect pointers cases alexander detect blocks indirect blocks move files directories proper fault-isolated locations verify alexander places blocks disk instrumented file system log block allocations addition alexander logs events interslowdown versus raidoperational overheads ext fat create read overwrite unlink figure time overheads figure plots time overheads observed d-graid level versus raid level series microbenchmarks tests run disk systems experiment operations enacted file creations operation file est assignment home site inode creation mapping logical block re-mapping blocks homesite receipt logical writes file system evaluate behavior alexander workload run workload alexander obtain time-ordered log events occurred file system alexander process log off-line number blocks wrongly laid time ran test hours traces found hours examined number blocks misplaced temporarily low blocks report detailed results hour trace observed greatest number misplaced blocks hours examined figure shows results figure parts bottom part shows normal operation alexander capability react block reuse remapping copying blocks correct homesite figure shows alexander quickly detect wrongly blocks remap appropriately run-time blocks written seconds total meta uniquedata raidd-graid d-graid d-graid d-graid table performance postmark table compares performance d-graid level raidon postmark benchmark row marked d-graid specific level metadata replication column reports benchmark run-time column shows number disk writes incurred column shows number disk writes metadata blocks fourth column number unique metadata blocks written experiment run disks number blocks misplaced temporarily total number blocks accessed trace top part figure shows number misplaced blocks experiment assuming remapping occur expected delinquent blocks remain misplaced dip end trace occurs misplaced blocks assigned file homesite preceding delete accidentally correcting original misplacement time overheads introduced explore time overheads arise due semantic inference primarily occurs blocks written file system file creation figure shows performance alexander simple microbenchmark allocating writes slower due extra cpu cost involved tracking fault-isolated placement reads overwrites perform comparably raidthe high unlink times d-graid fat fat writes data pertaining deleted files processed graid newly allocated data implementation untuned infrastructure suffers cpu memory contention host worst case estimates overheads cost d-graid explore overhead metadata replication purpose choose postmark metadata intensive file system benchmark slightly modified postmark perform sync deletion phase metadata writes accounted making pessimistic evaluation costs table shows performance alexander degrees metadata replication table 
synchronous replication metadata blocks significant effect performance metadata intensive workloads file sizes postmark range bytes note alexander performs default raidfor lower degrees replication physical block allocation ext contiguous free chunk blocks allocate file layout sub-optimal small files pack table shows number disk writes incurred benchmark percentage extra disk writes roughly accounts difference performance replication levels extra writes metadata blocks count number unique physical writes metadata blocks absolute difference replication levels small suggests lazy propagation updates metadata block replicas idle time freeblock scheduling greatly reduce performance difference cost added complexity lazy update propagation replicas updated d-graid incur extra disk writes played back portion traces minutes standard raidsystem d-graid disks playback engine issues requests times trace optional speedup factor speedup implies idle time requests reduced factor speedup factors d-graid delivered per-second operation throughput raidutilizing idle time trace hide extra cpu overhead scaling factor operation throughput lagged slightly d-graid showing slowdown one-third trace execution caught due idle time effective access-driven diffusion show benefits access-driven diffusion trial experiment perform set sequential file reads files increasing size compare standard raidstriping d-graid access-driven diffusion figure shows results experiment figure access-driven diffusion sequential access larger files run rate single disk system benefit potential parallelism access-driven diffusion performance bandwidth file size access-driven diffusion raidd-graid file-based access-driven diffusion d-graid directory-based access-driven diffusion d-graid file-based d-graid directory-based figure access-driven diffusion figure presents performance d-graid level standard raidunder sequential workload experiment number files size read sequentially total volume data fixed graid performs smaller files due physical block layout improved reads directed diffused copies disks system note case arrange files diffused start experiment reading threshold number times investigating sophisticated policies initiate access-driven diffusion left future work fast live-block recovery explore potential improvement live-block recovery figure presents recovery time d-graid varying amount live file system data figure plots lines worst case case live-block recovery worst case live data spread disk case compacted single portion volume graph live-block recovery successful reducing recovery time reconstruction time live volume percentage costs reconstruction d-graid level worst case d-graid level case idealized raid level figure live-block recovery figure shows time recover failed disk hot spare d-graid level mirrored system live-block recovery lines d-graid plotted worst case live data spread entire volume case compacted smallest contiguous space plotted recovery time idealized raid level disk half full note difference worst case case times difference suggests periodic disk reorganization speed recovery moving live data localized portion benefits expect d-graid demonstrate improved availability alexander failures figure shows availability performance observed process randomly accessing files running d-graid raidto ensure fair comparison d-graid raidlimit reconstruction rate figure shows reconstruction volume live data completes faster d-graid compared raids extra failure occurs availability raiddrops availability ops succeed file throughput files sec time sec raid availability raid throughput d-graid availability d-graid throughput operation failure firstfailure reconcomplete failureafter recon secondfailure restore offailed disk re-mount reconcomplete figure availability profile figure shows operation d-graid level raid failures array consists data disks hot spare failure data reconstructed hot spare d-graid recovering faster raid failures occur raid loses files graid continues serve files workload consists read-modify-writes files randomly picked working set d-graid continues availability surprisingly restore raidstill fails files linux retry inode blocks fail remount required raidreturns full availability complex implementation briefly quantify implementation complexity alexander table shows number statements required implement components alexander table core file system inferencing module ext requires lines code counted number semicolons core mechanisms d-graid contribute lines semicolons total d-graid generic setup fault-isolated placement physical block allocation access driven diffusion mirroring live block recovery internal memory management hashtable avl tree file system specific sds inferencing ext sds inferencing vfat total table code size alexander implementation number lines code needed implement alexander shown column shows number semicolons column shows total number lines including white-spaces comments code rest spent hash table avl tree wrappers memory management compared tens thousands lines code comprising modern array firmware added complexity d-graid significant academic prototype complexity numbers slight under-estimate required production quality implementation analysis intended approximate estimate d-graid levels discussion focused implementing d-graid storage system redundancy user data raidor mirrored storage system raidhowever mentioned layout mechanisms d-graid orthogonal underlying redundancy scheme section formalize levels d-graid popular traditional raid levels present custom policies graid level tailored underlying redundancy mechanism note contrast traditional raid levels levels d-graid differ type redundancy normal user data system meta-data maintained raidwith configured replication degree d-graidno redundancy simplest d-graid level redundancy mechanism employed normal user data single disk failure results data loss contrast traditional raidwhere single disk failure results complete data loss graidensures proportional data availability failure figure shows d-graidconfiguration absence redundancy normal data additional storage required access-driven diffusion d-graidneeds separate performance reserve section reserve fixed percentage storage volume size tunable administrator tuning parameter administrator control trade-off performance storage efficiency issue changing size performance reserve dynamically file systems equipped deal variable volume size limitation addressed simple technique administrator creates file file system reserved diffuse size file implicitly conveys d-graid size performance reserve file system blocks assigned reserved file file d-graid free storage space file system runs short storage administrator prune size special file dynamically reducing size performance reserve d-graidmirroring mirrored d-graid system stripes data multiple mirrored pairs similar raidnote d-graid meaningful storage system comprised single mirrored pair raidbecause system fundamentally partial failure mode access-driven diffusion policy d-graidis similar d-graidwhere dynamic performance reserve hold diffused copies figure depicts configuration note diffused copies mirrored d-graidrequires half percentage space d-graidrequires order achieve level diffusion slight variant d-graidcan make access-driven diffusion effective cost slight degradation reliability disks mirrored pair physical mirrors discussed employ logical mirroring impose logical disk block copies disks relaxed definition d-graid store copy file traditional striped fashion copy file diffused copies dgraid diffused copies dgraid physical mirrored pair mirrored pair dgraid logical striped copy fault isolated copy diffused copies parity disk dgraid figure d-graid levels figures depict data layout d-graid redundancy schemes style shading represents file graidfigure color shading physical raidstripe diffusion segments striped region d-graidlogical separate regions disk simplicity practice interleaved fault-isolated copies stored fault-isolated fashion figure depicts configuration file fault-isolated copy laid single disk copy striped disks single disk 
failure result data loss logical mirroring data achieves benefits fault isolated placement impact performance parallelism striped copies note scenario extra space required access-driven diffusion variant d-graidimproves performance efficient access-driven diffusion reduces reliability compared traditional graidin traditional d-graidi physical mirroring single disk failure failure mirror disk lead loss data logical mirroring failure results loss data proportionally irrespective disk incurred failure d-graidparity d-graidis counterpart traditional raidredundancy user data maintained form parity encoding small number disks resulting space efficiency fine grained blocklevel striping fundamental raidwould conflict fault isolated placement d-graid techniques orthogonal finegrained striping required raidoccurs physical level actual physical disk blocks fault isolated placement logical assignment files physical blocks d-graidwould maintain invariant kth parity block xor kth block disk difference kth block disk data pertaining file d-graid raid part file configuration shown figure blocks belonging physical raidstripe shaded color fault isolated placement raidlike redundancy leads performance issue blocks raidstripe longer part single file logically related full stripe writes uncommon block allocation policies writes partial stripes small writes performance problem requiring disk operations block written address small write problem d-graidwe customized block allocation policy allocation policies section targeted preserving logical contiguity perceived file system graidrequires policy minimizes impact small writes policy log-structured allocation blocks written place allocated empty segments invalidating locations log structured allocation d-graidwould simply divide disk multiple segments time d-graidwould operate segment stripe comprises kth segment disk write arrives fault isolation module d-graidwould decide disk block laid allocate tail physical block segment logical block typical workload writes spread multiple files d-graid balances space utilization disks writes multiple files spread segments current segment stripe resulting full stripe writes note technique effective log cleaner co-ordinate cleaning entire set disks set freed segments comprise full segment stripes summary summary find basic layout techniques d-graid orthogonal underlying redundancy mechanism building top physical redundancy scheme d-graid strictly improves availability storage array custom policies access driven diffusion physical block allocation make d-graid effective redundancy mechanism discussion impact wrong section fair amount complexity identifying logical file block belongs order place correct home site graceful degradation interesting question arises light complexity d-graid makes wrong inference d-graid permanently associates block wrong file places wrong home site incorrect inferences affect parts d-graid design differently graceful degradation component d-graid robust incorrect inferences incorrect association block wrong file affect fault isolation impact correctness d-graid miscalculates large fraction associations reliability resulting storage layout strictly traditional raid level d-graid builds top existing raid redundancy incorrect association lead layout completely fault isolated layout exhibit fault isolation compared traditional raid face incorrect inference storage system correctness affected making d-graid ideal candidate make aggressive semantic information contrast live block recovery component d-graid depend semantic information correctness requires conservative estimate set live blocks volume d-graid requires estimate strictly conservative live block inferred dead lead loss data section tracking block liveness information conservatively simple straightforward realize liveness tracking accurate file system d-graid reconstruct blocks thinks live conservative recover remaining blocks background d-graid requires accuracy simple piece semantic information implementing fast recovery design complexity d-graid fault isolation graceful degradation component robust incorrect inference wrong bad summary d-graid turns simple binary failure model found storage systems continuum increasing availability storage continuing operation partial failure quickly restoring live data failure occur chapter shown potential benefits d-graid explored limits semantic knowledge shown successful d-graid implementation achieved limits simulation evaluation prototype implementation found d-graid built semantically-smart disk system file system modification delivers graceful degradation live-block recovery access-driven diffusion good performance chapter exploiting liveness knowledge faded life pleasant death peaceful transition troublesome isaac asimov d-graid presented previous chapter naturally amenable approximate semantic inference wrong information lead correctness issues chapter present aggressive piece sds functionality stringent requirements correctness specific functionality involves performing secure deletion disk system exploiting knowledge liveness data discuss techniques tracking forms liveness general apply techniques context secure deletion introduction liveness blocks key piece semantic information storage systems previous work demonstrated utility knowledge dead blocks store rotationally optimal replicas data provide zero-cost writes chapter describe semantically-smart disk system acquire information liveness demonstrate application case study presenting specific techniques tracking liveness formalize notion liveness storage specifically identify classes liveness content block generation liveness present techniques explicit implicit tracking type techniques tracking liveness dependent characteristics file system study range file systems including ext ext vfat identify key file system properties impact feasibility complexity techniques demonstrate utility applicability techniques describing design implementation evaluation prototype secure deleting disk infers logical deletes occurring file system shreds deleted blocks making deleted data irrecoverable unlike d-graid case study explored previous chapter secure delete poses challenges due extreme requirements type accuracy liveness information case study show underneath modern asynchronous file systems implement sds functionality extreme correctness requirements finally compare sds approach tracking liveness alternative approach interface disk system changed add explicit free command comparison helps bring complexity performance costs semantic inference approach comparison explicit approach chapter organized present extended motivation taxonomy liveness list file system properties impact techniques tracking liveness information discuss specific techniques tracking liveness present secure delete case study describe initial experience liveness tracking ntfs closed-source file system finally describe explicit notification approach compare merits approaches conclude extended motivation liveness information enables variety functionality performance enhancements storage system enhancements implemented higher layers require low-level control storage system eager writing workloads write-intensive run faster storage system capable eager writing writing free block closest disk arm traditional in-place write order select closest block storage system information blocks live existing proposals function long exist blocks written file system writes block storage system identify subsequent death block result delete disk empowered liveness information effective eager writing adaptive raid information block liveness storage system facilitate dynamic adaptive raid schemes autoraid system autoraid utilizes free space store data raidlayout migrates data raidwhen runs short free space knowledge block death make schemes effective optimized layout techniques optimize on-disk layout transparently storage system explored adaptive reorganization blocks disk replication blocks rotationally optimal locations examples knowing blocks free greatly facilitate techniques live blocks collocated minimize seeks free space dead blocks hold rotational replicas smarter nvram caching buffering writes nvram common optimization storage systems synchronous write workloads benefit in-memory delayed writes 
file system nvram buffering improves performance absorbing multiple overwrites block delete-intensive workloads unnecessary disk writes occur absence liveness information deleted blocks occupy space nvram written disk nvram fills real file system traces found writes deleted typical delayed write interval seconds unnecessarily written disk knowledge block death storage removes overhead intelligent prefetching modern disks perform aggressive prefetching block read entire track block resides prefetched cached internal disk cache aged fragmented file system subset blocks track live caching track result suboptimal cache space utilization reading track efficient disk knowledge liveness enable disk selectively cache blocks live faster recovery liveness information enables faster recovery storage arrays storage system reduce reconstruction time disk failure reconstructing blocks live file system previous chapter self-securing storage liveness information storage build intelligent security functionality storage systems storage level intrusion detection system ids perimeter security monitoring traffic suspicious access patterns deletes truncates log files detecting patterns requires liveness information secure delete ability delete data manner makes recovery impossible important component data security government liveness description type utility content data block versioning block block holds eager write valid data fast recovery generation block lifetime secure delete context file storage ids table forms liveness regulations require strong guarantees sensitive data forgotten requirements expected widespread government industry future secure deletion requires low-level control block placement storage system implementing storage level secure delete requires liveness information storage system explore secure deletion section liveness storage taxonomy discussed utility liveness information storage system present taxonomy forms liveness information relevant storage liveness information classified dimensions granularity accuracy timeliness granularity liveness depending specific storage-level enhancement utilizes liveness information logical unit liveness tracked vary identify granularities liveness information meaningful content block generation summary presented table content liveness content liveness simplest form liveness unit liveness actual data context block death granularity occurs overwrite block block overwritten data storage system 
infer contents dead approximate form content liveness readily existing storage systems explored previous work wang virtual log disk frees past location block block overwritten contents tracking liveness granularity on-disk versioning self-securing storage systems completely accurate storage system block freed file system contents stored block dead overwritten block liveness block liveness tracks disk block valid data data accessible file system unit interest case container contents block liveness granularity required applications intelligent caching prefetching eager writing deciding propagate block nvram disk storage system block live granularity form liveness information tracked traditional storage systems storage system unaware blocks file system thinks live weak form liveness tracked block written inferred dead generation liveness generation disk block lifetime block context file death generation block written disk context file free reallocated file tracking generation liveness ensures disk detect logical file system delete block contents reached disk context deleted file storage level functionality requires generation liveness secure delete track block live contained data belonged file generation longer alive application requires generation liveness information storage-based intrusion detection generation liveness tracked existing storage systems accuracy liveness information dimension liveness accuracy refer degree trust disk place liveness information inaccuracy liveness information lead disk overestimating underestimating set live entities blocks generations degree accuracy required varies specific storage application delete-squashing nvram acceptable storage system slightly overestimate set live blocks performance issue correctness issue hand underestimating set live blocks catastrophic disk lose valid data similarly generation liveness detection secure delete acceptable miss intermediate generation deaths block long latest generation death block timeliness information final axis liveness timeliness defines time death occurring file system disk learning death periodicity file system writes metadata blocks imposes bound timeliness liveness information inferred applications eager writing delete-aware caching delayed knowledge liveness acceptable long information changed meantime applications secure delete timely detection provide stronger guarantees file system model d-graid previous chapter considered asynchronous file system file system model based assumed worst-case effects asynchrony arbitrary reordering delayed writes modern file systems provide kinds guarantees terms update data disk case study study range dynamic update properties hold modern file systems study properties affect techniques semantic inference goal experimented underneath file systems ext ext vfat experimented ntfs limited scale due lack source code access ntfs experience section ext modes operation synchronous asynchronous modes ext modes writeback ordered data journaling modes update behaviors form rich set file systems based experience file systems identify key high level behavioral properties file system relevant context tracking liveness information table summarizes properties property syn reuse ordering block exclusivity generation marking delete suppression consistent metadata data-metadata coupling table file system properties table summarizes properties exhibited file systems study sections discuss properties influence techniques storagelevel liveness tracking reuse ordering file system guarantees reuse disk blocks freed status block bitmaps metadata pointed block reaches disk file system exhibits reuse ordering property sufficient ensure data integrity absence property file end partial contents deleted file crash journaling file system vfat asynchronous mode ext reuse ordering modes ext ext synchronous mode exhibit reuse ordering block exclusivity block exclusivity requires disk block dirty copy block file system cache requires file system employ adequate locking prevent update in-memory copy dirty copy written disk property holds file systems ext vfat ext conform property snapshotbased journaling dirty copies metadata block previous transaction committed current transaction generation marking generation marking property requires file system track reuse file pointer objects inodes version numbers ext ext file systems conform property inode deleted reused file version number inode incremented vfat exhibit property delete suppression basic optimization found file systems suppress writes deleted blocks file systems discuss obey property data blocks vfat obey property directory blocks consistent metadata property file system conveys consistent metadata state storage system journaling file systems exhibit consistent metadata property transaction boundaries on-disk log implicitly convey information ext vfat exhibit property data-metadata coupling data-metadata coupling builds consistent metadata property requires notion consistency extended data blocks words file system conforming property conveys consistent metadata state set data blocks dirtied context transaction file systems ext data journaling mode conforms property techniques liveness detection section analyze issues inferring liveness information storage system semantic inference file system dependent discuss feasibility generality implicit liveness detection file systems ext ext vfat section discuss initial experience detecting liveness underneath windows ntfs file system forms liveness 
address granularity accuracy axes mentioned section accuracy axis accurate approximate inferences approximate instance refers strict over-estimate set live entities timeliness axis address common complex case lack timely information modern file systems asynchronous delay metadata updates timeliness guaranteed content liveness discussed section disk observes write contents live data block infer previous contents stored block suffered content death completely accurate content liveness inference requires information block liveness block liveness block liveness information enables storage system block valid data time track block liveness storage system monitors updates structures tracking allocation ext ext specific data bitmap blocks convey information vfat information embedded fat entry fat block free file system writes allocation structure storage system examines entry concludes relevant block dead live allocation bitmaps buffered file system written periodically liveness information storage system stale account allocations deletes occurred interval table depicts time line operations leads incorrect inference storage system bitmap block tracking liveness written step indicating dead subsequently allocated file written disk indicating live buffered memory point disk wrongly believes dead on-disk contents valid address inaccuracy disk tracks shadow copy bitmaps internally chapter file system writes bitmap block disk updates shadow copy copy written addition data block written disk disk pro-actively sets bit shadow bitmap copy block live write leads disk live preventing incorrect conclusion drawn operation in-memory on-disk initial free write disk free alloc alloc write disk written liveness belief live free table naive block liveness detection table depicts time line events leads incorrect liveness inference problem solved shadow bitmap technique file system properties block liveness shadow bitmap technique tracks block liveness accurately underneath file systems obey block exclusivity data-metadata coupling property block exclusivity guarantees bitmap block written reflects current liveness state relevant blocks file system tracks multiple snapshots bitmap block ext write version bitmap block indicating dead subsequent allocation write disk wrongly infer dead fact on-disk contents valid belongs newer snapshot uncertainty complicates block liveness inference file system exhibit block exclusivity block liveness tracking requires file system exhibit data-metadata coupling group metadata blocks bitmaps actual data block contents single consistent group file systems typically enforce consistent groups transactions observing transaction boundaries disk reacquire temporal information lost due lack block exclusivity ext data journaling mode transaction newly allocated data blocks bitmap blocks indicating allocation part consistent group commit point disk conclusively infers liveness state state bitmap blocks transaction data writes actual in-place locations occur transaction commits disk guaranteed transaction commit blocks marked dead previous transaction remain dead absence data-metadata coupling newly allocated data block reach in-place location transaction commits live disk disk detects operation in-memory on-disk initial alloc live write disk written delete free alloc alloc write disk live liveness belief missed gen death table missed generation death block liveness table shows scenario illustrate simply tracking block liveness insufficient track generation deaths accuracy block liveness requires file system conform delete suppression property delete suppression hold write block imply file system views block live shadow bitmap technique overestimate set live blocks bitmap write table ext vfat ext data journaling mode readily facilitate block liveness detection generation liveness generation liveness stronger form liveness block liveness builds shadow bitmap technique generation liveness goal find on-disk block generation data file stored block dead block liveness special case generation liveness block dead latest generation stored dead conversely block liveness information sufficient detect generation liveness block live stored dead generation past table depicts case block initially stores generation inode disk thinks block live deleted freeing immediately reallocated file written time continues marked live disk missed generation death occurred bitmap writes generation liveness reuse ordering tracking generation liveness general challenging file system reuse ordering property makes simple track reuse ordering block reused file deleted status block reaches disk reused bitmap block written disk detect dead presence reuse ordering tracking block liveness accurately implies accurate tracking generation liveness file systems ext conform reuse ordering facilitate accurate tracking generation liveness generation liveness reuse ordering underneath file systems ext vfat exhibit reuse ordering property tracking generation liveness requires disk detailed information specifically disk monitor writes metadata objects link blocks single logical file inode indirect blocks ext directory fat entries vfat disk explicitly track generation block belongs inode written disk records block pointers belong specific inode extra knowledge file block belongs disk identify generation deaths ownership table disk tracked belongs eventually written disk observe change ownership owns block 
owned past disk conclude generation death occurred complication arises reused reused representing file belongs generation scenario detected generation death ownership change monitor miss detect case require file system track reuse inodes generation marking property ext maintains version number enables detection cases generation deaths version numbers disk tracks block generation belonged generation number combination inode number version number disk observes inode written incremented version number concludes blocks belonged previous version inode incurred generation death call technique generation change monitoring finally pertinent note generation liveness detection generation change monitoring approximate assume disk observes block belongs generation time observes belongs generation generation change monitoring disk conclude generation death occurred liveness type properties blockapprox block exclusivity data-metadata coupling blockaccurate blockapprox delete suppression generationapprox blockapprox generation marking generationaccurate blockaccurate reuse ordering table properties liveness inference approx set live entities over-estimated disk generation deaths occurred relevant period freed allocated freed reallocated disk owning due delayed write show case study weaker form generation liveness summary file system properties required forms liveness inference presented table case study secure delete demonstrate techniques imparting liveness storage present design implementation evaluation secure deleting disk primary reasons chose secure deletion case study secure delete requires tracking generation liveness challenging track secure delete liveness information context correctness paramount false positive detecting delete lead irrevocable deletion valid data false negative result long-term recoverability deleted data violation secure deletion guarantees secure deletion prototype called faded file-aware data-erasing disk faded works underneath file systems ext vfat ext complete lack ordering guarantees ext presented challenges specifically ext reuse ordering property detecting generation liveness requires tracking generation information disk section focus implementation faded underneath ext finally discuss key differences implementation file systems goals faded desired behavior faded block reaches disk context file delete file trigger 
secure overwrite shred block behavior corresponds notion generation liveness defined section shred involves multiple overwrites block specific patterns erase remnant magnetic effects past layers recovered techniques magnetic scanning tunneling microscopy recent work suggests overwrites sufficient ensure non-recoverability modern disks traditionally secure deletion implemented file system implementations unreliable modern storage systems high security overwrites off-track writes writes straggling physical track boundaries external erase programs file system perform storage system buffers writes nvram multiple overwrites file system collapsed single write physical disk making overwrites ineffective finally presence block migration storage system overwrite file system overwrite current block location stray copies deleted data remain storage system proper locale implement secure deletion note faded operates granularity entire volume control individual files shredded limitation dealt storing sensitive files separate volume secure delete functionality enabled basic operation discussed section faded monitors writes inode indirect blocks tracks inode generation block belongs augments information block liveness information collects shadow bitmap technique note ext obeys block exclusivity delete suppression properties block liveness detection reliable block death detected faded safely shred block hand faded detects generation death ownership change generation change monitors block live block liveness module faded simply shred block faded current contents block belong generation deleted generation subsequently allocated block due block reuse current contents block valid shredding block catastrophic deal uncertainty conservative approach generationdeath inference conservative convert apparent correctness problem performance problem end performing overwrites required fundamental approach notion conservative overwrite conservative overwrites conservative overwrite block erases past layers data block leaves current contents intact faded subsequent valid write occurred predicted generation death conservative overwrite block safe shred valid data perform conservative overwrite block faded reads block nonvolatile ram performs normal secure overwrite block specific pattern ultimately restores original data back block problem conservative overwrite block contents restored conservative overwrite fact data shredded conservative overwrite ineffective case faded guaranteed observe things block reused file system file valid data written eventually delayed write interval file system faded receives write buffers write writing data disk faded performs shred concerned block time faded restore data recent contents block identify writes treat special manner faded tracks list blocks subjected conservative overwrite suspicious blocks list write block list committed secure overwrite block overwrite block removed suspicious list note suspicious list stored persistently nvram order survive crashes block reused file system immediately faded guaranteed observe bitmap reset block flagged block death block liveness detector block liveness tracking reliable faded shred block destroying data cases wrongful restore data faded guaranteed opportunity make error cost conservatism conservative overwrites performance cost conservative overwrite results concerned block treated suspicious data restored conservative overwrite data faded information find stage uncertainty data restored data overwritten subsequent write block context file lead redundant shredding block performance cost faded pays circumvent lack perfect information coverage deletes previous subsection showed generation deaths detected faded ensures block version overwritten compromising valid data faded achieve goals detection techniques sufficient identify cases deletes file system level shredded section show faded detect deletes requires minor modifications ext undetectable deletes weak properties ext deletes missed faded present specific situations identification deletes impossible propose minor ext fix scenarios file truncates generation change monitor assumes version number inode incremented inode reused version number ext incremented complete delete reuse partial truncates affect version number block freed due partial truncate reassigned file faded misses generation death reuse partial truncate argued logical overwrite file delete adopt complex conservative interpretation treating delete handle deletes propose small change ext incrementing version number reallocation inode increment truncate alternatively introduce separate field inode tracks version information non-intrusive change effective providing disk requisite information technique result extra overwrites rare case partial truncates correctness guaranteed operation in-memory on-disk initial bind bind delete free alloc write disk bind wrong type table misclassified indirect block table shows scenario normal data block misclassified indirect block bind treated indirect block reuse ordering indirect blocks prevents problem spurious overwrites conservative leave data intact reuse indirect blocks subtle problem arises due presence indirect pointer blocks indirect blocks share data region file system user data blocks file system reuse normal user data block indirect block vice versa presence dynamic typing disk reliably identify indirect block shown chapter faded identify block indirect block observes inode indirect pointer field faded records fact indirect block observes write faded contents indirect block deleted reused user data block inode scenario illustrated table faded trust block pointers suspected indirect block uncertainty lead missed deletes cases prevent occurrence data block misclassified indirect block ensure file system allocates immediately file system frees indirect block bind concerned data bitmap block mbind flushed disk disk block freed note weak form reuse ordering indirect blocks show change impact performance indirect blocks tend small fraction set data blocks practicality discussed minimal non-intrusive required modification lines code ext required weak ordering guarantees ext file systems ext exhibit reuse ordering operation in-memory on-disk initial free free alloc write disk written delete free alloc write disk missed delete table missed delete due orphan write table illustrates delete missed orphan block treated carefully block initially free allocated memory written disk written deleted reallocated written faded associate miss overwrite required study ext aimed limit study minimal set file system properties required reliably implement secure deletion disk orphan allocations orphan allocations refer case file system considers block dead disk considers live assume block newly allocated file written disk context file crash occurs point metadata indicating allocation written disk disk assume block live restart file system views block dead on-disk contents block belong file longer extant file system block suffered generation death disk implicit block liveness tracking faded addresses case ext ext recovers crash fsck utility writes copy bitmap blocks block liveness monitor faded detect death orphan allocations orphan writes due arbitrary ordering ext faded observe write newly allocated data block observes owning inode orphan writes treated carefully owning inode deleted written disk faded block belonged inode block reused inode faded miss overwriting concerned block written context inode table depicts scenario address problem defer orphan block writes faded observes owning inode potentially memory-intensive solution suspicious block list conservative overwrites track orphan blocks faded observes 
write orphan block marks suspicious subsequent write arrives contents shredded inode owning block deleted reaching disk write block context file trigger shred block reused bitmap reset delete technique results redundant secure overwrite anytime orphaned block overwritten file system context file cost pay conservatism note overhead incurred time orphan block overwritten delayed overwrites multiple overwrites block additional disk hurt performance incurred critical path performance faded delays overwrites idle time workload optionally minutes detection faded decides shred block queues low priority thread services queue faded observed foreground traffic duration delayed overwrites faded present writes disk sequential ordering reducing impact foreground performance delaying reduces number overwrites block deleted multiple times notion conservative overwrites crucial delaying overwrites arbitrarily block overwritten written context file note shredding required user perform sync summary key data structures components faded presented figure guaranteed detection deletes demonstrate basic techniques outlined ensure faded captures relevant cases deletes prove block deleted file system reached disk faded overwrites deleted contents persistent liveness monitor block block inode mapping overwrite thread datadelayed overwrites suspicious list shadow bitmaps monitor generation change figure key components faded delete inode occurs ext set blocks freed file results increment version number reset relevant bits data bitmap block pertaining freed blocks block freed assume written disk context written disk disk perform overwrite case bitmap block status block inode possibilities reused file system written disk reused write case block reused reused immediately file bitmap block dirtied eventually written disk disk immediately delete bitmap reset indicator overwrite case block reused case reused inode possibilities case point receiving write disk thinks belongs thinks free belongs inode case disk thinks disk knew disk tracked previous version number eventually observes write dirtied version number increment disk note version number increased overwrite blocks thought belonged case includes overwritten restoring newer discussed section conservative overwrite contents guaranteed shredded case disk thinks free disk thinks free treat orphan block written mark suspicious written context inode contents shredded case disk thinks disk observed pointing point current write disk observed allocated file system case c-i disk observed allocated thinks written context means disk case block deleted time past order allocated led version number incrementing disk observes written perform overwrite thinks belong case c-ii occurs means written disk owning deleted written case written context deleted overwritten discussed section true block exclusivity property ext note case block deleted file quickly reallocated file special case case cases block written disk context file delete block file lead shred deleted contents indirect block detection uncertain disk wrongly corrupt pointer false indirect block file system modification reuse ordering indirect blocks case occur faded file systems implemented faded underneath file systems case validated implementation testing methodology section sake brevity point key differences observed relative ext faded vfat ext vfat conform reuse ordering faded track generation information block order detect deletes key difference vfat compared ext pre-allocated uniquely addressable inodes version information dynamically allocated directory blocks pointer start block file fat chains start block blocks file detecting deletes reliably underneath unmodified vfat impossible introduced additional field vfat directory entry tracks globally unique generation number generation number incremented create delete file system newly created file assigned current generation number small change lines code vfat generation change monitor accurately detects deletes interest faded ext ext exhibits reuse ordering tracking generation liveness ext tracking block liveness ext obey block exclusivity property tracking block liveness accurately impossible data journaling mode property data-metadata coupling ordered writeback modes make small change metadata transaction logged made ext log list data blocks allocated transaction change lines code coupled reuse ordering property enables accurate tracking deletes evaluation section evaluate prototype implementation secure delete enhanced disk implemented pseudo-device driver linux kernel chapter config delete overwrite excess miss 
analysis internet content delivery systems himani apte distributed systems wisconsin madison january spring overview paper analyzes content delivery systems arena client-server oriented world-wide web content delivery networks peer-to-peer file sharing systems important features peer-to-peer systems symmetry peers behave servers clients scalability millions machines dynamic membership wide-area network heterogeneity participating systems terms bandwidth connectivity performance distinguishing characteristics peer-topeer systems systems tend application specific peer-to-peer systems hierarchy members instance peers kazaa supernodes maintain indexes content peers nearby neighborhood problem statement paper examined traffic flow content delivery systems focusing largely web versus peer-to-peer traffic flows specifically http web traffic akamai kazaa gnutella delivery systems methodology methodology employed passive network monitoring traffic coming border routers washington rest internet tcp flows reconstructed monitoring hosts extract information categorize http non-http traffic http traffic distinguished kazaa gnutella based destination ports akamai traffic identified based served akamai server methodology misses internal traffic local network instance file sharing traffic kazaa users network analysis account nonhttp tcp traffic total tcp traffic non-tcp traffic total network traffic peer-to-peer systems bittorrent napster excluded study observations section summarizes observations made class analysis presented paper data characteristics http trace summary statistics presented table unavailable outbound akamai traffic akamai servers hosted kazaa highest outbound traffic terms net bytes transferred spite smaller server client population total tcp bandwidth consumed http transfers content delivery systems presented figure show typical diurnal pattern traffic peaks true daylight hour opposed kazaa traffic peaks late night http trace collected may-june day period trace significantly vary based timing collected network behavior students widely summer break final exams week longer data sample desirable analysis client server tcp bandwidth presented figure kazaa peers act servers web servers reason high connectivity kazaa peers content delivery characteristics bytes transferred video objects requests gif jpeg images median object size peer-to-peer systems top bandwidth consuming clients figure servers figure kazaa peers caching beneficial kazaa file sharing system large bandwidth consumption large size objects popular objects result large number requests result small number clients consume large amount bandwidth peer-topeer systems kazaa gnutella servers perfectly loadbalanced spite scalability peer-to-peer systems existence highly popular content single server availability large-size objects peers incorrect make conclusions kazaa gnutella servers load-balanced based data trace include internal file sharing traffic network role caching studying role caching cdns systems authors simulated infinite-capacity caches cache misses popularly cold capacity conflict misses case infinitely large cache kind misses occur cold misses result cache miss rate measure proportion unique bytes accessed net bytes accessed ideal byte hit rate figure outbound kazaa traffic found stabilize inbound traffic stabilize end trace cache byte hit rate function population size presented figure hand increasing number kazaa clients increase number requests indirect version table correctness accuracy table shows number overwrites performed faded configurations ext columns order number blocks deleted file system total number logical overwrites performed faded number unnecessary overwrites number overwrites missed faded note deletes occurred data write require overwrite correctness accuracy test faded implementation detected deletes interest instrument file system log delete correlate log writes overwrites faded capture cases unnecessary missed overwrites tested system workloads technique including busy hours file system traces table presents results study trace hour experiment ran faded versions linux ext marked default ext file system indirect ext modified obey reuse ordering indirect blocks version ext modified increment inode version number truncate configuration represents correct file system implementation required faded column measure extra work faded order cope inaccurate information column number missed overwrites correct system fourth column cost inaccuracy reasonable faded performs roughly overwrites minimal amount note version number modification ext faded misses deletes reason missed overwrites reported version configuration rarity case involving misclassified indirect block config reads writes run-time version table impact performance performance file system configurations busy hour trace shown configuration show number blocks read written trace run-time run-time postmark trace default securedelete securedelete securedelete table foreground impact postmark trace run-times postmark trace shown faded overwrite passes postmark configured files transactions performance impact evaluate performance impact made ext running trace versions ext table shows results performance reduction number blocks written marginally higher due synchronous bitmap writes indirect block reuse ordering conclude practical performance secure delete explore foreground performance faded cost overwrites foreground performance impact tracking block generation liveness requires faded perform extra processing cost reverse engineering directly impacts application performance incurred critical path disk operation quantify impact extra processing required faded foreground performance software prototype competes cpu memory resources host worst case estimates overheads run postmark file system benchmark trace file system running top faded postmark metadata intensive small-file benchmark heavily exercises inferencing mechanisms faded arrive pessimistic estimate perform syncat end phase postmark causing disk writes complete account time results note wait completion delayed overwrites numbers performance perceived foreground task table compares performance lowering faded cache default hit disk rate table hand leads complementary overwrite passes effect foreground caching performance numerous affected clients extra improving cpu cache processing hit rate faded preliminary investigation lower presented performance compared paper suggests modified file caching system running large normal effect disk idle wide-scale time required system potentially quantify reducing wide-area cost bandwidth performing demands overwrites dramatically shredding run feasible microbenchmark employ caches repeatedly creates systems due file legal flushes issues pertaining disk deletes content distributed waits system delay period paper repeating give steps insight total realistic size iterations cache varying amount sufficient delay obtain improvements phase bandwidth measure usage time conclusions paper benchmark presents including 
quantification domination systems modern day internet traffic global characteristics easily small part network case makes interesting revelations network traffic flows future expect traffic show similar characteristics larger traffic 
delayed overwrites complete run experiment block reuse occurs file system iterations creates files directory ext places directories block groups overwrite process overwrite iteration matter long overwrites delayed left graph figure shows results expected extra overwrite pass extra time required finish overwrites roughly equal default benchmark run-time delay amount data written single overwrite pass written due create faded delays overwrites presents disk sequential order overwrites achieve close sequential disk performance run experiment allowing block reuse file system representative typical workloads deletes creates occur roughly set directories experiment phases create file directory roughly set blocks repeatedly assigned files delaying overwrites yield significant benefit overwriting blocks synchronously incurred overwrite phase delayed overwrites ideally incur overhead graph figure shows benchmark run time faded varying number overwrite passes note time overwrites fixed case previous experiment idle time benchmark runtime delay phase create-delete-delay benchmark block reuse default faded faded faded benchmark runtime delay phase create-delete-delay benchmark block reuse default faded faded faded figure idle time requirement microbenchmark figure plots time create-delete-delay microbenchmark left graph shows block reuse graph includes block reuse baseline execution time benchmark default line faded performs unnecessary overwrites intermediate phases presence block reuse idle time required explore time required overwrites postmark configuration measure time benchmark complete including delayed overwrites postmark deletes files end run face worst case scenario entire working set benchmark run-time overwrites postmark trace default securedelete securedelete securedelete table idle time requirement table shows total run-time benchmarks postmark trace time reported includes completion delayed overwrites overwritten accounting large overwrite times reported table hp-trace overwrite times reasonable blocks deleted trace reused subsequent writes overwrites performed conservative accounts steep increase overwrite passes implicit case implicit detection ntfs section present experience building support implicit liveness detection underneath windows ntfs file system main challenge faced underneath ntfs absence source code file system basic on-disk format ntfs details update semantics journaling behavior publicly result implementation tracks block liveness requires knowledge on-disk layout generation liveness tracking implemented details ntfs journaling mechanism fundamental piece metadata ntfs master file table mft record mft information unique file piece metadata ntfs treated regular file file mft file recovery log allocation status blocks volume maintained file called cluster bitmap similar block bitmap tracked ext block allocations deletions ntfs regularly writes modified bitmap blocks prototype implementation runs device driver linux similar setup earlier file systems virtual disk interpose exported logical disk virtual machine instance windows running vmware workstation track block liveness implementation shadow bitmap technique mentioned section detailed empirical observation long-running workloads found ntfs exhibit violation block exclusivity delete suppression properties mentioned section due absence source code assert ntfs conforms properties limitation points general difficulty implicit techniques underneath closed-source file systems file system conforms properties guaranteed file system vendor absence guarantees utility implicit techniques limited optimizations afford occasionally wrong implicit inference experience ntfs points utility characterizing precise set file system properties required forms liveness inference set properties constitutes minimal interface communication file system storage vendors ntfs confirmed conformance block exclusivity delete suppression properties storage system safely implement aggressive optimizations rely implicit inference explicit liveness notification considered infer liveness sds costs pertinent issue liveness inference techniques compare approach luxury changing interface support command explicitly deletes section discuss explicit notification approach assume special allocate free commands added scsi optimization obviate explicit allocate command treating write previously freed block implicit allocate describe issues incorporating explicit command existing file systems modifying file systems interface trivial find supporting free command ramifications consistency management file system crashes implement secure deletion explicit notification scenario compare costs secure delete semantic inference approach modified linux ext ext file systems free command communicate liveness information discuss issues free command implemented ioctl pseudo-device driver serves enhanced disk prototype granularity free notification issue arises explicit notification exact semantics free command granularities liveness outlined section block liveness content liveness tracked file system lazy initiating free commands suppressing free blocks subsequently reused generation liveness file system notify disk delete block contents reached disk context deleted file multiple intermediate layers buffering file system contents block reached disk context file simplify file system implementation file system concerned form liveness disk functionality requires approach file system invokes free command logical delete receiving free command block disk marks block dead internal allocation structure bitmap write marks block live responsibility mapping free commands form liveness information lies disk disk track generation deaths interested free command block thinks live internal bitmaps redundant free block free disk block deleted written disk viewed generation death correct operation file system guarantee write block disk prior allocation write treated implicit allocate guarantee delete suppression property write freed block allocation result incorrect conclusion generation liveness disk note free issued block disk safely block possibly erasing contents timeliness free notification important issue arises explicit notification afreeis file system issues notification option notification file system issues free immediately block deleted memory solution result loss data integrity crash scenarios crash occurs immediately free notification block metadata indicating delete reaches disk disk considers block dead recovery file system views block live delete reached disk live file freed block scenario violation data integrity violations acceptable file systems ext weak data integrity guarantees file systems preserve data integrity ext delay notification effect delete reaches disk delayed notification requires file system conform reuse ordering property block reused live file system effect previous delete reaches disk delayed free command suppressed means disk miss generation death orphan allocations finally explicit notification handle case orphan allocations file system considers block dead disk considers live assume block newly allocated file written disk context file crash occurs point metadata indicating allocation written disk disk assume block live restart file system views block dead on-disk contents block belong file longer extant file system block suffered generation death disk freenotification mechanism enable accurate tracking liveness orphan allocations handling orphan allocations file system specific describe explicit notification ext mentioned ext provide data integrity guarantees crash notification deletes ext invokes free command synchronously block freed memory dealing orphan allocations ext requires simple expensive operation recovery fsck utility conservatively issues free notifications block dead file system explicit notification ext ext guarantees data integrity ordered data 
journaling modes free notification ext delayed effect delete reaches disk words notification delayed transaction performed delete commits record in-memory list blocks deleted part transaction issue freenotifications blocks transaction commits ext conforms reuse ordering property delayed notification feasible crash occur invocation free commands immediately commit transaction free operations redo-able recovery purpose log special free records journal replayed recovery part delete transaction recovery multiple committed transactions propagated on-disk locations block deleted transaction reallocated subsequent committed transaction replay logged free commands guarantee completing free commands transaction committing transaction replay freecommands successfully committed transaction log earlier committed transactions replayed deal orphan allocations log block numbers data blocks written written disk recovery ext issue free commands set orphan data blocks part uncommitted transaction explicit secure delete built secure deletion distributed operating explicit systems notification suresh sridharan framework modified distributed systems ext ext file wisconsin systems madison notify january disk spring logical delete overview paper aims providing complete survey contemporary distributed systems time authors define distributed operating system distinguishing networked systems paper endeavours explain detail key design issues involved building systems examples research projects considered light issues discussed distributed system defined users ordinary centralized operating system runs multiple independent cpus words distributed operating system appears users single coherent system networked system users aware specific system service computer running private operating system fault tolerance distinguishing characteristic transparency argued transparent system utility display information processes executed files stored necessarily brand system non-transparent goals assumptions distributed operating systems tend leverage availability cheap microprocessor technology achieve performance similar expensive counterparts obtaining computing power proportionate processors added system reliability availability face failure system components key advantages distributed operating systems characteristics system listed model client server workload general purpose membership static scale machines network local-area homogenity heterogenity clients servers heterogeneous machines clients homogeneous attributes listed table differ distributed systems systems host client server workload application specific membership dynamic scale larger millions wan involved resources hosts heterogeneous design issues communication primitives due availability high bandwidth network links price computationally iso model prohibitive distributed systems standard protocols tcp udp avoided due reason protocol stack addition larger number bytes headers checksum data paper discusses fundamental tradeoffs reliable unreliable primitives blocking nonblocking primitives operations system idempotent problems semantics guaranteed rpc semantics impossible achieve order make message passing efficient rpc avoid copying data levels messages made longer amortize overhead allowing acks replaced subsequent request reply messages higher-level knowledge saves number messages network rpc implementation avoids reliable connection oriented protocol tcp naming protection problem naming associate logical names actual physical storage locations system physical names simplest model centralized server creates bottleneck case systems larger scale problems arise domain protection names identifiers derived global space possibility entity identifying attributes making difficult enforce protection resource management deals capability balance load execute processes picking processor execute processes objective minimize communication costs requiring knowledge future behaviour processes load balancing assuming future behaviour processes unknown jobs typically run local machine interactive attributes fast response time short completion times detection interactive job based statistics measured process cpu bound bound interactive migration remote execution tradeoff explored harder migration history maintained analyzed deciding job short perform processor allocation suitably load information maintained propagated due constantly changing nature system information information inaccurate ways averaging number runnable processes computing residual running time provide fair estimates load information propageted frequent broadcasts takes bandwidth processor heavily loaded diffusion pairwise exchange processors pick machine random share load information polling requesting load information processor events distributed deadlocks detected readily due absence centralized tables giving status resources problem scheduling complex due dependencies processes machines communicate continue work coscheduling works communicating processes allocated time slices row process blocked waiting time slice instant requires synchronized time slices turn requires master timer single point failure important latency small compared time-slice scheduling time slices range typical systems case file system distributed file system involves decision file service stateless virtual-circuit oriented server connection oriented clients susceptible failure case server failure stateless case message information kind failure accounted idempotency read write operations utilized fundamental trade-off stateless connection oriented robustness performance research system implementations interesting features systems systems mechanism similar rpc communication protocols established protocols tcp udp performance primary goal systems cambridge naming centralized server protection active table amoeba naming protection achieved capabilities access rights object checksum constant encrypted random keys internal table kernel trusted establish protection replay attacks obtained capabilities capabilities rights created obtained capability dynamic allocation processors pool servers charge services bank account scheme limit resource usage address fault tolerance eden protection capabilities unencrypted form reliability systems complete objects checkpointed time time incremental checkpointing objects efficient usage capabilities represents components centralized research distributed systems research applications distributed systems limited parallelized compilation parallelized version travelling salesman problem implemented amoeba cambridge distributed system project practical accomodated number users system creating fairly stable system communication primitives made systems potentially capable achieving reliability systems address reliability large extent summary paper presents comprehensive summary ideals distributed operating system ideas fault tolerance received attention systems systems implement basic versions required features naming protection half paper general objectives guidelines distributed systems systems paper addressed range issues discussed part 
file system modifications accounted lines code receiving notification disk decides shred block similar faded disk delays overwrites idle time minimize impact foreground performance performance explicit secure delete evaluate explicit secure delete implementation run hour trace results presented table file system require faded required default performance corresponds row table terms foreground performance explicit implementation performs incur overhead inference require file system modifications reported table corresponds row table note model cost sending free command scsi bus overheads explicit case system run-time foreground overwrites default table trace performance explicit secure delete table shows foreground run time total completion time overwrites trace explicit secure delete optimistic explicit mode perfect information avoids unnecessary overwrites resulting lower overwrite times compared faded conclude extra costs semantic inference comparison explicit approach reasonable discussion section reflect lessons learned case study refine comparison strengths weaknesses explicit implicit approaches ideal scenario implicit approach required storage system file system interface practice accurate liveness detection requires file system properties means file system modified conform requisite properties face storage system file system implicit approach pragmatic explicit approach changing interface main reasons implicit approach file system required file system conforms requisite properties file systems ext vfat ext -data journaling ntfs amenable block liveness detection change file system ext file system data journaling mode conforms properties required generation liveness detection cases implicit approach enables non-intrusive deployment functionality modifying file system conform set welldefined properties general modifying file system interface convey specific piece information discussed file system properties viewpoint implicit liveness detection properties enable richer information inferred association block owning inode required applications file-aware layout tracked accurately file system obeys reuse ordering consistent metadata properties ultimate goal arrive set properties enable wide variety information tracked implicitly outlining file systems designed enable transparent extension storage system contrast approach changing interface requires introducing interface time piece information required summary chapter explored techniques infer liveness information sds utilized techniques implement reliable secure delete implementation demonstrated functionality relies semantic information correctness embedded storage system fundamental uncertainty due asynchrony conservative dealing imperfect information storage system convert apparent correctness problems minor performance degradation quantified performance complexity costs implementing functionality sds comparing alternative implementation explicitly interface achieve functionality inference techniques case studies assumed storage system file system popular usage scenario storage system context dbms chapter explore applying similar semantic inference techniques underneath database system chapter semantic disks database systems today sort simple-minded model disk arm platter holds database fact holding database raid arrays storage area networks kinds architectures underneath hood masked logical volume manager written operating system people databases transparency good makes productive care details hand optimizing entire stack fields talk hand accept things -pat selinger considered range functionality sds provide understood semantic information file system chapter extend philosophy storage system understand higher layers system specifically techniques sds infer information database management system utilize techniques provide improved functionality introduction order explore applicability semantic disk technology realm databases describe earlier case studies applied database management systems case studies based work successfully applied idea underneath file system specifically study improve storage system availability building dbms-specific version d-graid raid system degrades gracefully failure implement dbms-specialized version faded storage system guarantees data unrecoverable user deleted evaluate case studies prototype implementations underneath predator dbms built shore storage manager d-graid find dbms-specific version work file-system-specific counterpart investigation found fundamental underlying reasons unexpected discrepancy data structures inter-relationships data items complex databases file systems improving reliability proved difficult anticipated databases tend general-purpose metadata file systems reflect information stable storage conclude discussing set evolutionary database management systems incorporate amenable semantically-smart disk technology specifically dbms export knowledge data relationships storage system general purpose usage statistics periodically reflect disk incremental place database systems poised advantage intelligent storage systems rest chapter organized section discuss semantic disk extracts information dbms sections present dbms-specific case studies present general discussion additional dbms support required semantic disks section conclude section extracting semantic information implement database-specific functionality storage system storage system understand higher level semantic information dbms storage system similar file system case database-specific semantic information broadly categorized types static dynamic section describe types information semantic disk requires underneath dbms discuss information acquired experience primarily predator dbms built shore storage manager illustrate techniques specific examples predator techniques general database systems static information static information comprised facts database change database running structure log record database format database page examples static information information embedded storage system firmware communicated storage system out-of-band channel system installation file systems case on-disk formats change slowly embedding information storage system firmware practical gain insight storage vendor deliver firmware updates order pace dbms-level studied development postgres times revision history dump restore required migrate version database found dump restore needed months average number higher expected commercial databases store terabytes data requiring dump restore migrate tolerable users recent versions oracle great lengths avoid on-disk format assume storage system static information relevant data structures dbms main challenge lies building minimal information automatically infer higher level operations performed database system dynamic information dynamic information pertains information dbms continually database system operates tracking set disk blocks allocated table instance dynamic information changing records inserted deleted table similarly tracking disk block belongs table index unlike static information dynamic information continually tracked disk track dynamic information semantic disk utilizes static information data structure formats monitor key data structures correlated specific 
higher level dbms operations caused change case file systems showed process dynamic inference significantly complicated buffering reordering writes file system fortunately tracking dynamic information accurately reliably underneath dbms straightforward write-ahead log wal databases log records operation leads change on-disk contents wal property log operation reaches disk effect operation reaches disk strong ordering guarantee makes inferences underneath dbms accurate straightforward describe semantic disk infers kinds dynamic information dbms note case study requires types information quantify discuss section log snooping basic technique semantic disk track dynamic information dbms snoop log records written dbms log record log sequence number lsn byte offset start record log volume lsn semantic disk accurate information exact ordering events occurred database track events disk maintains expected lsn pointer tracks lsn log record expected disk semantic disk receives write request log block block log record processes log record advances expected lsn pointer point record dbms group commits log blocks arrive order semantic disk utilizes lsn ordering process order log blocks arriving order deferred expected lsn reaches block tracking block ownership important piece information semantic disk logical grouping blocks tables indices involves associating block table index store logically owns block allocation block update on-disk data recoverable dbms logs performing allocation shore writes create ext log record block number store allocated semantic disk observes log entry records information internal block store hash table semantic disk numerical store page belongs map store actual table index mapping disk utilizes static knowledge system catalog table tracks mapping dbms case predator mapping maintained tree called rootindex disk monitors btree add records log records store-id b-tree entry added logical store rootindex part static information semantic disk identifies newly created mappings tracks astore namehash table tracking block type piece information semantic disk require type store block block data page index page track information semantic disk watches updates system catalog tables track information semantic disk detects inserts table page insert records log tuple added page number disk store page belongs block ownership straightforward monitor inserts store ids catalogs note table names catalog tables part static information disk predator interested sindxs table list indexes database tuple sindxs table index table attribute built relationship stores type relationship table set indices built table tracking information involves monitoring updates sindxs catalog table association note relationship maintained catalogs terms table index names store ids track store-to-name mapping earlier fine-grained information aforementioned dynamic information extracted log snooping tracking pieces information requires semantic disk probe contents page find b-tree page internal page leaf page disk relevant field page similarly find byte ranges page deleted semantic disk scan page holes fine-grained information requires static knowledge storage system understand format b-tree data pages partial availability d-graid section describe case study d-graid prototype semanticallysmart storage system utilizes dbms-specific information lay blocks ensures graceful degradation database availability unexpected multiple failures d-graid enables continued operation database complete unavailability multiple failures motivated partial availability showed approach significantly improves availability file systems chapter design basic goal d-graid make semantically meaningful fragments data failures queries access parts data run completion oblivious data loss parts database key idea lay blocks semantic fragment single disk arbitrary disk failures semantic fragments unavailable entirety data present live disks meaningful isolation choice blocks constitute semantic fragment dependent database workload d-graid exports linear logical address space dbms running scsi disk internally place logical blocks physical disk ensure partial availability file system case key structure d-graid enable flexibility block placement indirection map maps logical block physical block similar structures modern arrays logical block replicated bit indirection map entry tracks date copy present layout strategies partial dbms availability discuss structures aggressively replicated partial availability explore classes techniques fragmentation targeting widely database usage patterns coarse-grained fragmentation fine-grained fragmentation finally discuss partial availability interacts transaction recovery mechanism dbms issue availability database log system metadata replication delving semantic fragmentation data structures dbms query system run system catalogs information table index frequently consulted structures unavailable partial failure fact data remains accessible practical d-graid aggressively replicates system catalogs extent map database tracks allocation blocks stores experiments employ -way replication important meta-data -way replication feasible read-mostly nature meta-data minimal space overhead entails coarse-grained fragmentation database large number moderately sized tables semantic fragment defined terms entire tables subsection presents layout strategies improved availability scenario scans queries selection queries filter non-indexed attribute aggregate queries single table involve sequential scan entire table scan requires entire table order succeed simple choice semantic fragment set blocks belonging table entire table single disk failures occur subset tables entirety scans involving tables continue operate oblivious failure index lookups index lookups form common class queries selection condition applied based indexed attribute dbms index find tuple rids reads relevant data pages retrieve tuples traversing index requires access multiple pages index collocation index improves availability index table viewed independently placement index query fails index table unavailable decreasing availability strategy improve availability collocate table indices call strategy dependent index placement joins queries involve joins multiple tables queries typically require joined tables order succeed improve availability join queries d-graid collocates tables joined single semantic fragment laid single disk note identification join groups requires extra access statistics tracked dbms specifically modified predator dbms record set stores tables indexes accessed query construct approximate correlation matrix access correlation pair stores information written disk periodically seconds d-graid information collocate tables accessed fine-grained fragmentation collocation entire tables indices single disk enhanced availability scenarios preclude layout single table index large fit single disk table popular accessed queries placing hot table single disk lead significant loss availability failure disk scenarios require fine-grained approach semantic fragmentation approach d-graid stripes tables indexes multiple disks similar traditional raid array adopts techniques enable graceful degradation detailed scans scans fundamentally require entire table striping strategy impact availability scan queries disk capacities roughly doubling year tables large fit single disk increasingly rare cases hierarchical approach large table split minimal number disks hold disk group treated logical fault-boundary d-graid applied logical fault-boundaries finally database supports approximate queries provide partial availability scan queries missing data index lookups large tables index-based queries common oltp workload involves index lookups small number large tables queries require entire 
index table d-graid simple techniques improve availability queries internal pages b-tree index aggressively replicated failure instance root b-tree index page collocated data pages tuples pointed index page collocation d-graid probabilistic strategy leaf index page written d-graid examines set rids contained page rid determines disk tuple places index page disk greatest number matching tuples note assume table clustered index attribute page-level collocation effective case non-clustered indices joins similar indices page-level collocation applied tables join group collocation feasible tables join group clustered join attribute alternatively tables join group small replicated disks larger tables striped diffusion performance coarse-grained fragmentation entire table single disk table large accessed frequently performance impact parallelism obtained disks wasted remedy graid monitors accesses logical address space tracks logical segments benefit parallelism d-graid creates extra copy blocks spreads disks array normal raid blocks hot d-graid regains lost parallelism due collocated layout providing partial availability guarantees reads writes diffused copy background updates actual copy transactions recovery pertinent issue partial availability interacts transaction recovery mechanisms dbms databases declare transaction committed recorded log potentially actual data blocks written acid guarantees impose commit durable face database crashes recovery process read log redo operations committed disk storage system underneath supports partial availability blocks missing blocks referred redo record missing recovery process execute redo problem considered solved aries context handling offline objects deferred restart specifically solution ignore redo blocks blocks restored tape instance pagelsn unavailable page remained unavailable redo record recent pagelsn page redo applied accesses page prevented recovery takes place page similarly undo operations handled logical physical level key idea turn access pages normal transactions holding long-standing lock pages recovered side-effect partial availability semantics occurs normal transaction execution transaction committed recording log actual data pages updated disk blocks unavailable forcing database violate durability guarantees prevent d-graid guarantees writes fail long space atleast disks alive write request arrives dead disk automatically remaps block disk alive updates imap space d-graid worse normal storage system multiple failure occured middle query execution database abort redo log replay disk recovered restore sanity contents block availability database log database log plays salient role recoverability database ability make partial availability important log multiple failures providing high availability log size active portion log determined length longest transaction factored concurrency workload portion log highly reasonable modern storage arrays large amounts persistent ram obvious locales place log high availability replicating multiple nvram stores addition normal on-disk storage log ensure log remains accessible face multiple disk failures evaluation evaluate availability improvements performance d-graid prototype implementation d-graid prototype functions software raid driver linux kernel operates underneath predator shore dbms availability improvements evaluate availability improvements d-graid d-graid array disks study fraction queries database serves successfully increasing number disk failures layout techniques d-graid queries succeed failed disks table scans availability queries succeed failed disks index lookup queries dependent index placementindependent index placement queries succeed failed disks join queries join collocationwithout collocation figure coarse-grained fragmentation graphs show availability degradation scans index lookups joins varying number disk failures -disk d-graid array steeper fall availability higher number failures due limited -way replication metadata straight diagonal line depicts ideal linear degradation complementary existing raid schemes parity mirroring show graid level redundancy data measurements simplicity microbenchmarks analyze availability provided layout techniques d-graid coarse-grained fragmentation evaluate availability improvements due coarse-grained fragmentation techniques d-graid figure presents availability types queries synthetic workloads graph figure shows degradation availability scan queries multiple disk failures database tables tuples workload query chooses table random computes average non-indexed attribute requiring scan entire table percentage queries complete successfully shown graph shows collocation tables enables database partially serving proportional fraction queries comparison failure traditional raidsystem results complete unavailability note redundancy maintained parity mirroring d-graid traditional raid tolerate failure availability loss middle graph figure shows availability index lookup queries similar workload layouts layouts entire store index table collocated disk independent index placement d-graid treats index table independent stores possibly allocates disks dependent index placement d-graid carefully allocates index disk table dependent placement leads availability failure finally evaluate benefits join-group collocation microbenchmark database pairs tables joins involving tables pair join queries randomly selecting pair joining tables bottom-most graph figure shows collocating joined tables d-graid achieves higher availability note experiment dbms modified report additional statistics access correlation tables fine-grained fragmentation evaluate effectiveness implementing remote procedure calls andrew birrell bruce jay nelson xerox palo alto research center remote procedure calls rpc paradig providing communication network programs written high-level language paper describes package providing remote procedure call facility options face designer package decisions made describe structure rpc mechanism facilities binding rpc clients transport level communication protocol performance measurements include descriptioro optimizations achieve high performance minimize load server machines clients categories subject descriptors computer-communication networks network protocols--protocol architecture computer-communication networks distributed systems-distributed applications network operating systems operating systems communications management--message sending network communication operatiug systems organization design--distributed systems general terms design experimentation performance security additional keywords phrases remote procedure calls transport layer protocols distributed naming binding inter-process communication performance communication protocols introduction background idea remote procedure calls hereinafter called rpc simple based observation procedure calls well-known wellunderstood mechanism transfer control data program running single computer proposed mechanism extended provide transfer control data communication network remote procedure invoked calling environment suspended parameters passed network environment procedure execute refer callee desired procedure executed procedure finishes produces results results passed backed calling environment execution resumes returning simple single-machine call calling environment suspended processes machine possibly authors address xerox palo alto research center coyote hill road palo alto permission copy fee part material granted provided copies made distributed direct commercial advantage acm copyright notice title date notice copying permission association computing machinery copy republish requires fee specific permission acm acm transactions computer systems vol february pages birrell nelson execute depending details parallelism environment rpc implementation attractive aspects idea clean simple semantics make easier build distributed computations efficiency procedure calls simple communication rapid generality singie-machine computations procedures important mechanism communication parts algorithm idea rpc years discussed public literature times back nelson doctoral dissertation extensive examination design possibilities rpc system previous work rpc full-scale implementations rpc rarer paper designs notable recent efforts include courier xerox family protocols current work mit paper results construction rpc facility cedar project felt earlier work nelson thesis experiments understood choices designer rpc facility make task make choices light aims environment practice found areas inadequately understood produced system design aspects major issues facing designer rpc facility include precise semantics call presence machine communication failures semantics address-containing arguments absence shared address space integration remote calls existing future programming systems binding caller determines location identity callee suitable protocols transfer data control caller callee provide data integrity security desired open communication network building rpc package fine-grained fragmentation d-graid focus availability index lookup queries interesting category workload study consists index lookup queries randomly chosen values primary key attribute single large table plot fraction queries succeed varying number disk failures top graph figure shows results layouts examined graph lowermost line shows availability simple striping replication system catalogs availability falls drastically multiple failures due loss internal b-tree nodes middle line depicts case internal b-tree nodes replicated aggressively expected achieves availability finally line shows availability data index pages collocated addition internal b-tree replication techniques ensure linear degradation availability queries succeed failed disks split tables indexes replication colocationwith replication plain striping queries succeed failed disks split tables indexes hot-cold tuples hot-cold -wayhot-cold -way hot-cold -way figure index lookups fine-grained fragmentation graph figure considers similar workload small subset tuples hotter compared specifically tuples accessed queries workload simple replication collocation provide linear degradation availability hot pages spread uniformly disks addressed hotcold workload d-graid issues improve availability describe replicating data suitable index depth pages single paper hot tuples paper includes lines discussion depict availability issues hot major pages decisions replicated factors describes structure small solution fraction read describe data detail hot binding d-graid mechanism utilizes information transport level enhance communication availability protocol selective plan replication produce d-graid subsequent papers raidslowdown describing table facilities scan encryption-based security index providing lookup information bulk manufacture load stub modules table responsible insert interpretation arguments table time results overheads rpc d-graid calls table experiences compares practical performance graid facility default environment raidunder remote-procedure-call microbenchmarks package array built disks developed primarily performance overheads evaluate cedar programming performance environment implications communicating fault-isolated layout xerox research d-graid internetwork experiments building section package characteristics mhz p-iii environment system inevitably -disk impact graid array design comprised environment ibm summarized ultrastar lzx disks cedar peak large project throughput concerned developing database programming environment single table powerful records convenient sized building bytes experimental index programs systems primary key emphasis time uniform space highly overheads interactive user explore interfaces time ease space construction overheads incurred debugging programs d-graid cedar prototype designed tracking information acm transactions database computer laying systems vol blocks facilitate february graceful implementing degradation remote table procedure compares calls performance single-user d-graid workstations fine-grained fragmentation linux software construction raid servers shared basic computers query providing workloads common services workloads accessible examined scan communication entire network table index computers lookup random cedar key dorados table bulk dorado load entire powerful indexed machine table inserts simple algol-style indexed call table return d-graid takes performs microseconds raidfor equipped workloads scans -bit virtual poor address performance space scans -bit due words predator -megabyte anomaly disk scan dorado workload completely power saturated cpu ibm processor dedicated table single disks user communication extra cpu computers cycles required typically d-graid means impacts scan 
performance interference prototype competes resources host hardware raid system interference exist find overheads d-graid reasonable evaluated space overheads due aggressive metadata replication found minimal overhead scales number tables database tables overhead -way replication important data parallelism collocated tables evaluate benefits diffusing extra copy popular tables table shows time scan table coarsegrained fragmentation d-graid simple collocation leads poor scan performance due lost parallelism extra diffusion aimed performance d-graid performs closer default raiddiscussion comparing implementation d-graid underneath dbms file system work discussed chapter uncovered fundamental challenges unique dbms notion semantically-related groups complex dbms inter-relationships exist tables indexes file system case files directories reasonable approximations semantic groupings dbms goal d-graid enable serving higher level queries notion semantic grouping dynamic depends query workload identifying popular data aggressively replicated easier file systems standard system binaries libraries obvious targets independent specific file system running dbms set popular tables varies dbms dependent query workload effective implementation d-graid underneath dbms requires modification dbms record additional information d-graid enables database system serve large fraction queries multiple failures traditional storage arrays lead complete unavailability database simple modifications dbms enable d-graid reasonable feasible outline exact pieces information required section secure delete faded section describe faded prototype semantically-smart disk detects deletes records tables dbms level securely overwrites shreds relevant data make irrecoverable file system instance faded discussed chapter table-level deletes simplest granularity secure delete table drop table command issued faded shred blocks belonged table faded log snooping identify log records freeing extents scan time raidd-graid d-graid diffusion table diffusing collocated tables table shows scan performance observed -disk array configurations stores shore free ext list log record written extent freed faded list freed blocks issue secure overwrites pages contents freed pages required database transaction aborts undoing deletes faded delay overwrites transaction performed delete commits faded track list pending transactions snoops commit records log identify transactions committed relevant secure overwrites initiated record-level deletes handling record level deletes faded challenging specific tuples deleted sql delete statement specific byte ranges pages contained tuples shredded delete dbms typically marks relevant page slot free increments freespace count page freeing slots logged faded learn record deletes log snooping faded shred page detecting delete records page valid shredding deferred faded receives write page reflects relevant delete receiving write faded shreds entire page disk writes data received past layers data disk pertaining deleted records disappear main problems implementing technique problem relates identifying correct version page contained deleted record assume faded observed record delete page waits subsequent write written faded detect version written reflects version stale dbms wrote page delete block reordered disk scheduler run time workload workload default faded faded faded table overheads secure deletion table shows performance faded overwrites workloads workload deletes contiguous records workload deletes records randomly table arrives late disk identify current contents reflects delete faded pagelsn field page pagelsn page tracks sequence number latest log record describing change page faded simply compare pagelsn lsn delete tracking issue -megabitper-second ethernet computers -megabit-per-second ethernet computers running cedar ethernet ethernets research internetwork internetwork consists large number -megabyte -megabyte ethernets presently connected leased telephone satellite links data rates bps envisage rpc communication follow pattern experienced protocols communication local ethernet lower data rates internet links inconvenience users ethernets overloaded rarely offered loads percent capacity ethernet percent typical pup family protocols uniform access computer internetwork previous pup protocols include simple unreliable highprobability datagram service reliable flow-controlled byte streams computers ethernet lower level raw ethernet packet format essentially programming high-level languages dominant language mesa modified purposes cedar smalltalk interlisp assembly language dorados aims primary purpose rpc project make distributed computation easy previously observed research community construction communicating programs difficult task undertaken members select group communication experts researchers substantial systems experience found difficult acquire specialized expertise required build distributed systems existing tools undesirable large powerful communication network numerous powerful computers environment makes building programs easy existing communication mechanisms appeared major factor constraining development distributed computing hope providing communication ease local procedure calls people encouraged build experiment distributed applications rpc hope remove unnecessary difficulties leaving fundamental difficulties building distributed systems timing independent failure components coexistence independent execution environments secondary aims hoped support purpose wanted make rpc communication highly efficient factor acm transactions computer systems vol february birrell nelson transmission times network important communication expensive application designers strenuously avoid applications developed distorted desire avoid communicating additionally felt important make semantics rpc package powerful loss simplicity efficiency gains single unified communication paradigm lost requiring application programmers build extra mechanisms top rpc package important issue design resolving tension powerful semantics efficiency final major aim provide secure communication rpc previously implemented protocols provision protecting data transit networks true extent passwords transmitted clear-text belief research protocols mechanisms secure communication open network reached stage reasonable desirable include protection package addition distributed systems previously provided secure end-to-end communication dbms typically bytes belonged deleted records result data remains page faded observes page write scan page free space deleted byte ranges page reside dbms cache subsequent writes page scanned zeroed freespace performance subsection briefly evaluate cost secure deletion faded prototype implementation similar d-graid prototype faded implemented device driver linux kernel works underneath predator workloads operating table -byte records workload perform delete rows half table deleted deleted pages contiguous workload similar tuples deleted selected random table compares faded number overwrite passes default case plain disk expected secure deletion performance cost due extra disk multiple passes overwrites overhead incurred deletes sensitive data deleted manner costs reasonable situations additional security required discussion fine record-level granularity deletes dbms makes secure deletion complex case dbms compared file system counterpart implementation require dbms requires detailed information on-disk page layout dbms make assumption on-disk page formats stable dbms releases alternatively format storage system require firmware upgrade secure delete beginning recognized crucial piece functionality context recent legislations data retention semantic disk ideal locale implement incurring extra dependency worth cost semantic disk-friendly dbms section discuss exact types dbms-specific information required storage system case studies suggest evolutionary make dbms amenable semanticallysmart disks information required section presented techniques storage system employ learn dbms techniques general case studies require techniques listing exact types information required case studies intimate relationship storage dbms semantic-disk technology successful exact information required case studies listed table table presents case study variants listed column shows type information needed database columns broken major categories static dynamic extra information discussed static information type information change dbms running format log record information change version database installed dynamic information continually operation blocks disk static dynamic extra embedded automatically provided tracked dbms tal tab les eco tre orm ers hip lat ion tor sac tio tat ces tat ist ics d-graid basic fine-grained frags join-collocation faded basic record-level delete table dbms information required case studies table lists pieces information dbms case studies require column specifies static information embedded semantic disk column lists dynamic state automatically tracked disk column lists additional information tracked communicated disk dbms table allocated finally extra information needed add dbms order storage system implement classes functionality biggest concern perspective database vendors types static information required storage system assumes understands contents catalog table database vendor loathe change feel constrained dependency table amount static information needed case study varies bit case studies structure catalog tables log records addition implementing features fine-grained fragmentation d-graid record-level delete faded required understanding detailed aspects dbms including b-tree page format data page format level detail weigh additional functionality provided merits increased level interdependence dbmses semantic disks simplicity tracking dynamic information underneath dbms encountered fundamental challenges implementing case studies underneath dbms arise file system counterparts general-purpose file systems track richer information implicitly files accessed fact set files lies single directory implicitly conveys information storage system files accessed similarly file systems track time file accessed information required effective implementation case studies database systems typically track detailed information tables indexes accessed subsection draw experiences case studies identify simple ways dbms assist semanticallysmart disks tracking information provide key suggestions applied easily incorporated rpc dbms systems design demonstrated provide case research studies insights fundamental require dbms decisions track statistics relationships 
logical entities tables indexes write periodically disk dbms record query set tables indexes accessed aggregate information queries statistics capture semantic correlation accesses tables inform storage system general semantic groupings collocated improved reliability statistics written additional catalog tables updates primarily meant performance hints transactional avoid logging overhead dbms track statistics popularity pieces data number queries accessed table duration piece information conveys storage system importance tables indexes storage system information aggressively replicate hot data improve availability interestingly modern database systems track modest amount similar access information coarser granularity purposes performance diagnosis automatic workload repository oracle maintains detailed access statistics commits disk periodically finally key point note stability on-disk data layout dbms format data page chapter case file systems on-disk formats rarely change case dbms format concern facilitate semantic disks dbmses judicious format trend holds extent modern dbmses format require dump restore existing data arduous task large databases modern versions oracle great care preserving on-disk format summary demonstrated chapter semantic knowledge enables construction powerful functionality storage system tailored system complex modern dbms avoiding complex database order harness functionality shown simple evolutionary dbms make database-specific storage systems effective viewing storage systems database systems black boxes relative sds approach enables classes functionality making storage system dbms-aware chapter logic file systems semantic disks theory difference theory practice practice anonymous previous chapters presented detailed techniques extract semantic information underneath modern file systems database systems case studies utilize information provide enhanced functionality storage description case studies extracting utilizing semantic information modern file systems complex building functionality sds utilizes information impact correctness faded clear understanding kinds information tracked accurately semantic disk set file system behaviors order simplify systematize process reasoning kinds information tracked accurately semantic disk formulate chapter logic framework formally reasoning file systems semantic disks intended initial goal logic model semantic disks clear reasoning information consequence aims procedure calls paradigm expressing control data transfers message passing plausible alternative belief choice semantic disk strong parallel reasoning file system consistency management cases information purely pertains ondisk state present logic model file systems reason correctness properties show reason semantic disks introduction reliable data storage cornerstone modern computer systems file systems responsible managing persistent data essential ensure function correctly modern file systems evolved extremely complex pieces software incorporating sophisticated performance optimizations features disk key bottleneck file system performance optimizations aim minimizing disk access cost complicating interaction file system storage system early file systems adopted simple update policies easy reason modern file systems significantly complex interaction disk stemming asynchrony updates metadata reasoning interaction file system disk paramount ensuring file system corrupts loses data complex update policies precise set guarantees file system obscured reasoning behavior translates manual alternatives intuitive exploration make major scenarios difference developers problems faced hoc exploration design arduous solutions possibly adopted error-prone problems reliable recent efficient work transmission found major message correctness errors widely reply file systems similar ext problems reiserfs encountered jfs remote procedure chapter calls present problems formal passing logic arguments modeling results interaction network file security system essentialy disk unchanged formal overriding modeling consideration show made reasoning choose file procedure system calls correctness simple foolproof major control data transfer formal mechanism model imbedded illustrated major language existence mesa similar frameworks areas parallel paradigm correctness paramount communication existing models form authentication remote protocols fork database language reliability includes database construct recovery forking parallel examples computations general theories chosen modeling concurrent point systems exist add communication frameworks semantics general model file systems changed effectively major design domain-specific problems logic greatly significantly simplifies discarded modeling possibility logic emulating file systems form serves shared address important space purposes computers enables previous prove work properties shown existing file sufficient system care designs moderate resulting efficiency achieved understanding set guarantees enabling aggressive performance approach optimizations employing shared preserve addresses guarantees feasible potentially significantly major lowers diff culties barrier spring providing mind mechanisms functionality representation remote file system addresses enabling integrated rigorous reasoning programming languages correctness possibly absence underlying machine framework architecture designers tend undue stick upheaval time-tested acceptable alternatives efficiency finally logic achieved helps design functionality host semantically-smart pup disk internet systems represented facilitating precise -bit characterization address proof naive implementation properties shared key address goal space logic extend framework width simplicity language order addresses -bits general file hand system designers barrier careful entry terms address-mapping applying mechanisms logic virtual low memory 
hardware shared address space changing address acm transactions computer systems vol february implementing remote procedure calls width megabit ethernets minimum average round trip time packet exchange microseconds approach form paging system summary shared address space participants rpc feasible undertake research subsequent design assumes absence shared addresses intuition hardware cost shared address space exceed additional benefits principle times making design choices semantics remote procedure calls close local single-machine procedure calls principle attractive ensuring rpc facility easy programmers familiar single-machine languages packages violation principle lead complexities made previous communication packages protocols difficult principle occasionally caused deviate designs attractive experienced distributed computing chose time-out mechanism limiting duration remote call absence machine communication failures communication packages worthwhile feature argument local procedure calls time-out mechanism languages include mechanisms abort activity part parallel processing mechanism designing time-out arrangement rpc needlessly complicate programmer world similarly chose building semantics based closely existing cedar mechanisms preference presented nelson thesis structure program structure rpc similar proposed nelson thesis based concept stubs making remote call pieces program involved user user-stub rpc communications package rpcruntime server-stub server relatidnship shown figure user user-stub instance rpcruntime execute caller machine server server-stub instance rpcruntime execute callee machine user wishes make remote call makes perfectly normal local call invokes procedure user-stub user-stub responsible placing specification target procedure arguments packets rpcruntime transmit reliably callee machine receipt packets rpcruntime callee machine passes server-stub server-stub unpacks makes perfectly normal local call invokes procedure server calling process caller machine suspended awaiting result packet call server completes returns serverstub results passed back suspended process caller machine unpacked user-stub returns user rpcruntime responsible retransmissions acknowledgments packet routing encryption effects multimachine binding machine communication failures call user acm transactions computer systems vol february birrell nelson caller machine network callee machine user user-stub rpcruntime importer exporter interface transmit wait receive call packet result packet rpcruntime server-stub server receive r--nt importer exporter interface transmit fig components system interactions simple call invoked procedure server directly user server code brought single machine bound directly stubs program work rpcruntime standard part cedar system user server written part distributed application user-stub server-stub automatically generated program called lupine generation mesa interface modules basis mesa cedar separate compilation binding mechanism interface module list procedure names types logic achieves enabling incremental modeling complete model file system starting logic simply model piece functionality mechanism isolation prove properties case studies demonstrate utility efficacy logic reasoning file system correctness properties represent prove soundness important guarantees provided existing techniques file system consistency soft updates journaling logic prove linux ext file system needlessly conservative transaction commits resulting sub-optimal performance case study demonstrates utility logic enabling aggressive performance optimizations illustrate utility logic developing file system functionality propose file system mechanism called generation pointers enable consistent undelete files prove correctness design incremental modeling mechanism logic demonstrating simplicity process implement mechanism linux ext file system verify correctness logic empirically show inconsistency occur undeletes absence mechanism finally demonstrate logic reasoning semantic disks specific type semantic information block type logically prove set file system behaviors inference information guaranteed accurate rest chapter organized present extended motivation quick context file systems present logic represent common file system properties logic logic prove consistency properties existing systems prove correctness unexploited performance optimization ext reason technique consistent undeletes apply logic semantic disks summarize extended motivation systematic framework reasoning interaction file system disk multifarious benefits describe key applications framework reasoning existing file systems important usage scenario logic model existing file systems key benefits modeling enables clear understanding precise guarantees mechanism assumptions guarantees hold understanding enables correct implementation functionality system layers disk system ensuring adversely interact file system assumptions write-back caching disks results reordering writes media negate assumptions journaling based benefit enables aggressive performance optimizations reasoning complex interactions hard file system developers tend conservative perform unnecessarily synchronous writes logic helps remove barrier enables developers aggressive performance optimizations confident correctness section analyze real opportunity aggressive performance optimization linux ext file system show logic framework prove correctness final benefit logic framework potential implementationlevel model checkers clear model expected behavior validate existing file system enable comprehensive efficient model checking current technique relying fsck mechanism expensive cost fsck explored state limits scalability model checking building file system functionality recovery consistency arguments results sufficient information caller callee independently perform compile-time type checking generate calling sequences program module implements procedures interface export interface program module calling procedures interface import interface writing distributed application programmer writes interface module write user code imports interface server code exports interface presents interface lupine generates user-stub exports interface server-stub imports interface binding programs caller machine user bound user-stub callee machine server-stub bound server programmer build detailed communication-related code designing interface write user server code lupine responsible generating code packing unpacking arguments results details parameter result semantics dispatching correct procedure incoming call server-stub rpcruntime responsible packet-level communications programmer avoid arguments results incompatible lack shared address space lupine checks avoidance programmer steps invoke intermachine binding section handle reported machine communication failures binding aspects binding turn client binding mechanism bound acm transactions computer systems vol february implementing remote procedure calls caller determine machine address callee callee procedure invoked primarily question naming question location naming binding operation offered rpc package bind importer interface exporter interface binding calls made importer invoke procedures implemented remote exporter parts interface type instance type intended level abstraction interface caller expects callee implement instance intended implementor abstract interface desired type interface correspond abstraction mail server instance correspond mail server selected reasonable default type interface derived mesa interface module fundamentally semantics interface dictated rpc package--they agreement exporter importer fully enforceable rpc package means exporter interface locate exporter dictated rpc package plan describe bell locating labs rob pike dave exporter presotto sean dorward grapevine bob distributed flandrena ken database thompson howard rpc trickey binding phil winterbottom major bell attraction laboratories murray grapevine hill jersey widely usa reliably motivation grapevine mid distributed trend multiple computing servers strategically located large centralized time-shared computers networks smaller personal machines typically unix worksta tions people grown weary overloaded bureaucratic traditionally timesharing viewed machines tricky issues eager reason move small self-maintained systems classic illustration meant view net arises loss computing context power database microcomputers recovery widely faster loss aries algorithm recovered pointed correctness style issues computing remains earlier popular proposals today success rush aries personal workstations stalled innovation database weaknesses recovery looked notion operating system hard run confident unix correctness timesharing system techniques trouble significant adapting range ideas innovation born file graphics system deals networking added interaction unix disk lifetime correctness remain implications poorly integrated inertia difficult changing admin time-tested ister alternatives stifles important early incorporation focus functionality private file machines made systems difficult systematic framework networks reason machines serve piece seamlessly functionality greatly monolithic reduce timesharing barrier systems timesharing entry centralized section management propose amortization file costs system functionality resources sonal logic computing prove fractured democratized correctness ultimately illustrate amplified administrative prob efficacy lems logic choice reasoning timesharing functionality operating system examine run section personal machines common made file difficult bind things smoothly plan began late attempt ways build sys tem centrally administered cost-effective cheap modern microcom puters computing elements idea build time-sharing system workstations computers handle tasks small cheap machines people offices serve terminals providing access large central shared resources internet topology configured maintain copies database entry grapevine servers highly reliable data replicated extremely rare unable database entry alternatives database find unsatisfactory include application programs network addresses machine communicate bind machine early applications alternatively form broadcast protocol locate desired machine acceptable general mechanism interference innocent bystanders convenient binding machines local network grapevine database consists set entries keyed character string grapevine rname varieties entries individua groups grapevine items information database entry rpc package concerned individual connect-site network address group member-list list rnames rpc package maintains entries grapevine database interface type instance type instance grapevine rnames database entry instance grapevine individual connect-site network address specifically network address machine instance exported database entry type grapevine group members grapevine rnames instances type acm transactions computer systems vol february birrell nelson exported remote interface type fileaccess alpine instance ebbets alpine exported server running network address remote interface type fileaccess alpine instance luther alpine exported server running network address members grapevine group fileaccess alpine include ebbets alpine luther alpine grapevine individual ebbets alpine connect-site luther alpine exporter wishes make interface remote clients server code calls server-stub turn calls procedure exportlnterface rpcruntime exportlnterface interface type instance procedure dispatcher implemented server-stub handle incoming calls interface exportlnterface calls grapevine ensures instance members grapevine group type connect-site grapevine individual instance network address exporting machine involve updating database optimization database updated correct information--this true typically interface previously exported typically network address export interface type fileaccess alpine instance ebbets alpine network address rpcruntime ensure ebbets alpine grapevine database connect-site ebbets alpine member fileaccess alpine rpcruntime records information export table maintained exporting machine exported interface table interface dispatcher procedure server-stub -bit serves permanently unique machine-relative identifier export table implemented array indexed small integer identifier guaranteed permanently unique successive values -bit counter start-up counter initialized one-second real time clock counter constrained subsequently current clock constrains rate calls exportlnterface single machine computing servers file servers cen tral machines coming wave shared-memory multiprocessors obvious candidates philosophy cambridge distributed system nehe early catch phrase build unix lot systems system lot unixes problems unix deep fix ideas brought file system coordinate naming appeared slightly form computing systems vol summer access resources devices traditionally treated files plan adopted idea designing network-level protocol called enable machines access files remote systems built naming system lets people computing agents build customized views resources network plan began plan user builds private computing environment recreates desired comput ing private machine clear model richer foreseen ideas per-process spaces file-system-like resources extended system processes graphics network system solid began exclusive computing environment meant bringing services applications unix opportunity revisit issues kernel-resident felt unix addressed badly plan pilers languages libraries window systems applications tools dropped brought polished rewritten all-encompassing distinction operating system library application important operating system researcher uninteresting user matters clean functionality building complete system solve problems thought solved real tty driver kernel job window system modern world multi-vendor multi-architecture computing essential usual pilers tools assume program built run locally needed rethink issues important test system computing environ ment producing efficient run unix warhorses empty engineering interested ideas suggested architecture underlying system encourage effective working plan emulation environment running posix commands backwater system vast majority system software developed native plan environment benefits all-new system laboratory history building experimental peripheral boards make easy write device drivers system source form longer guaranteed unix laboratory born redistribute work means software locally produced vendors compilers system overcome problems cross-compilation average rate averaged time exporting machine restarted burst rate calls exceed figure importer wishes bind exporter user code calls userstub turn calls procedure importlnterface rpcruntime giving desired interface type instance rpcruntime determines network address exporter grapevine network address connect-site interface instance rpcruntime makes remote procedure call rpcruntimepackage machine binding information interface type instance machine exporting interface fact returned importing machine binding fails machine exporting interface table system current exports feature maintained journaling rpcruntime show yields starting unique identifier simple logical identifier model table journaling index systematically returned arrive importing 
machine acm transactions computer systems vol february implementing remote procedure calls caller machine grapevine callee machine user user-stub rpcruntime import import record return result importer exporter interface ---getconnect bind transmit update update lookup rpcruntime server-stub server -export export record table setconnect addmember return table lookup check uid table importer exporter interface fig sequence events binding subsequent call callee machine exports remote interface type instance caller machine imports interface show caller initiating call procedure procedure interface return shown binding succeeds exporter network address identifier table index remembered user-stub remote calls subsequently user-stub making call imported remote interface call packet manufactures unique identifier table index desired interface entry point number desired procedure relative interface rpcruntime callee machine receives call packet index table current exports efficiently verifies unique identifier packet matches table passes call packet dispatcher procedure table variants binding scheme clients importer calling importlnterface specifies interface type instance rpcruntime obtains grapevine members grapevine group named type rpcruntime obtains network address grapevine individuals addresses turn find instance accept binding request efficiently acm transactions computer systems vol february birrell nelson order locate closest responsive running exporter importer bound closest running instance replicated service importer care instance importer free enumerate instances enumerating members group named type instance network address constant grapevine importer bind exporter interaction grapevine cost including explicit address application programs discussion important effects scheme notice importing interface effect data structures exporting machine advantageous building corner cases handled involve difficulty redistributing result servers hundreds users avoids problems server information relation subsequent importer crashes unique identifier scheme means bindings implicitly broken exporter crashes restarts currency identifier checked call implicit unbinding correct semantics user notified crash happening calls finally note scheme calls made procedures explicitly exported rpc mechanism alternate slightly efficient scheme issue importers exporter internal representation server-stub dispatcher procedure considered undesirable unchecked access procedure server machine make impossible enforce protection security schemes access controls restrict updates grapevine database effect restricting set users export interface names desired semantics random user claim workstation mail server intercept message traffic case replicated service access control effect critical client replicated service priori names instances service client wishes twoway authentication assurance service genuine avoid single password identifying instance service client securely obtain list names instances service achieve security employing secure protocol client interacts grapevine interface imported grapevine access controls provide client assurance instance service genuine authorized allowed choices binding time flexible importer specifies type interface instance decision interface instance made dynamically common interface instance rname delaying choice exporting machine restrictive facility network address instance binding machine compile time provide facilities allowing importer dynamically instantiate interfaces import detailed description acm transactions computer systems vol february implementing remote procedure calls complicated paper summary importer bind program exporting machines importer statically machines wishes bind proved open-ended multimachine algorithms implementing manager distributed atomic transaction allowed binding finer grain entire interface option considered light inutility mechanism packages systems observed packet-level transport protocol requirements semantics rpcs achieved designing specialized packetlevel protocol built package pup byte stream protocol xerox sequenced packet protocol transport layer previous experiments made pup byte streams xerox courier rpc protocol sequenced packet protocol grapevine protocols essentially similar remote procedure calls pup byte streams measurements experience implementations convinced approach unsatisfactory nature rpc communication means substantial performance gains designs implements transport protocol specially rpc experiments performance gain factor ten intermediate stance tenable experiment existing transport protocol building implementation specialized rpc request-response nature communication rpc sufficiently unlike large data transfers bytes streams employed intermediate position tenable aim emphasized protocol design minimizing elapsed real-time initiating call results protocols bulk data transfer important time spent transferring data strove minimize load imposed server substantial numbers users performing bulk data transfers acceptable adopt schemes lead large cost setting taking connections require maintenance substantial state information connection acceptable costs small relative data transfer untrue rpc envisage machines serve substantial numbers clients unacceptable require large amount state information expensive connection handshaking level rpc package defines semantics guarantees give calls guarantee call returns user procedure server invoked precisely exception reported user procedure invoked all--the user told exception reported user server crashed problem communication network provided rpcruntime server machine acm transactions computer systems vol february birrell nelson responding upper bound long wait results abort call communication breakdown crash server code deadlocks loops identical semantics local procedure calls simple calls make call communication efficient situation arguments fit single packet buffer results frequent calls made make call caller sends call packet call identifier discussed data desired procedure connection binding arguments callee machine receives packet procedure invoked procedure returns result packet call identifier results back caller machine transmits packet responsible retransmitting acknowledgment received order compensate lost packets result call sufficient acknowledgment call packet received call packet sufficient acknowledge result packet previous call made process situation duration call interval calls transmission interval transmit precisely packets direction call lasts longer longer interval calls additional packets retransmission explicit acknowledgment packet acceptable situations clear communication costs longer limiting factor performance call identifier serves purposes caller determine result packet result current call delayed result previous call callee eliminate duplicate call packets caused retransmissions call identifier consists calling machine identifier permanent globally unique machine-relative identifier calling process sequence number term pair machine identifier process activity important property activity activity outstanding remote call time--it initiate call received results preceding call call sequence number monotonic activity necessarily sequential rpcruntime callee machine maintains table giving sequence number call invoked calling activity call packet received call identifier looked table call packet discarded duplicate possibly acknowledgment sequence number greater table figure shows packets transmitted simple calls interesting compare arrangement connection establishment maintenance termination heavyweight transport protocols protocol connection shared state information activity calling machine rpcruntime package server machine accepting calls activity require special connection establishment acm transactions computer systems vol february implementing remote procedure calls caller machine user rpc stub call send callpkt await ack result return callee machine cali caliid dispatcherhint dispatcherld procedure arguments result callld results rpc stub server invoke proc call send results mtum fig packets transmitted simple call protocol compared two-packet handshake required protocols receipt call packet previously unknown activity sufficient create connection implicitly connection active call handled result packet call acknowledged 
ends maintain significant amounts state information connection idle state information server machine entry table sequence numbers caller minimal state information connection idle single machine-wide counter sufficient initiating call sequence number counter sequence numbers calls activity required monotonic sequential connection idle process machine concerned connection communications pinging packet exchanges required maintain idle connections explicit connection termination protocol connection idle server machine discard state information interval longer danger receiving retransmitted call packets minutes interacting caller machine scheme guarantees traditional connection-oriented protocols costs note rely unique identifier introduced remote admittedly complex interactions key attraction logic framework enables incremental modeling complete model file system order start proving properties model specific subset file system impacted 
functionality designing semantically-smart disks logic framework significantly simplifies reasoning semanticallysmart disk systems showed case studies inferring information accurately underneath modern file systems complex dependent dynamic properties file system section show logic simplify reasoning semantic disk turn enable aggressive classes functionality semantic disks background section provide basic context information file systems chapter provided detailed background information file systems section high-level background general model file system file system organizes disk blocks logical files directories order map blocks logical entities files file system tracks forms metadata correct maintenance metadata crucial enabling proper retrieval data file system section describe forms metadata file systems track discuss issue file system consistency finally describe asynchrony file systems major source complexity interaction disk file system metadata file system metadata classified types directories directories map logical file per-file metadata file mapped directory directories enable hierarchy files user opens file path file system locates per-file metadata file reading directory path required file metadata file metadata information specific file examples information set disk blocks comprise file size file file systems fat file metadata embedded directory entries file systems file metadata stored separately inodes pointed directory entries pointers file metadata actual disk blocks indirected indirect pointer blocks turn pointers actual data blocks allocation structures file systems manage resources disk set free disk blocks allocated files set free file metadata instances track resources file systems maintain structures bitmaps free lists instance resource free busy addition file systems track metadata super block bootstrapping focus major types discussed file system consistency proper operation internal metadata file system data blocks consistent state consistency state metadata structures obeys set invariants file system relies directory entry point valid file metadata structure directory points file metadata uninitialized marked free file system inconsistent file systems provide metadata consistency crucial correct operation stronger form consistency data consistency file system guarantees data block contents correspond file metadata structures point discuss issue section modern file systems linux ext reiserfs provide data consistency file system asynchrony important characteristic modern file systems asynchrony exhibit updates data metadata updates simply buffered memory written disk delay interval reordering writes asynchrony crucial performance complicates consistency management due asynchrony system crash leads state arbitrary subset updates applied disk potentially leading inconsistent on-disk state asynchrony updates principal reason complexity interaction file system disk formalism section define formal model file system define basic entities model relationships present basic operations logical postulates basic entities basic entities model containers pointers generations file system simply collection containers containers linked pointers file system differs exact types containers defines relationship container types abstraction based containers pointers general describe file system containers file system freed reused container considered free pointed container live instance container reuse free called binding generation identifier unable detect duplicates server crashed restarted caller retransmitting call packet plausible assuming call sequence number activity repeat calling machine restarted call restarted machine eliminated duplicate practice achieve side effect -bit conversation identifier connection secure calls nonsecure calls conversation identifier thought permanently unique identifier distinguishes incarnations calling machine conversation identifier passed call sequence number call generate conversation identifiers based -bit clock maintained machine initialized network time servers machine restarts experience previous systems anticipate light-weight connection management important building large busy distributed systems acm transactions computer systems vol february birrell nelson complicated calls mentioned transmitter packet responsible retransmitting acknowledged packet modified request explicit acknowledgment handles lost packets long duration calls long gaps calls caller satisfied acknowledgments caller process waits result packet waiting caller periodically sends probe packet callee callee expected acknowledge caller notice callee crashed communication failure notify user exception provided probes continue acknowledged caller wait indefinitely happy knowledge callee claims working call implementation probes issued delay slightly approximate round-trip time machines interval probes increases gradually minutes probes minutes probe subject retransmission strategies similar packets call communication failure caller told fairly relative total time caller waiting result call note detect failures communication levels detect callee deadlocked working call keeping principle making rpc semantics similar local procedure call semantics language facilities watching process aborting facilities suitable process waiting remote call alternative strategy retransn ssions acknowledgments recipient packet spontaneously generate acknowledgment doesn generate packet significantly sooner expected retransmission interval save retransmission packet dealing long duration calls large gaps calls decided saving packet large gain merit extra cost detecting spontaneous acknowledgment needed implementation extra cost form maintaining additional data structure enable extra process server generate spontaneous acknowledgment computational cost extra process deciding generate acknowledgment difficult avoid incurring extra cost acknowledgment needed analogous extra cost caller caller necessarily retransmission algorithm case call packet lost arguments results large fit single packet multiple packets requesting explicit acknowledgment transmitting large call argument packets alternately caller callee caller sending data packets callee responding acknowledgments implementation packet buffer end call avoids necessity including buffering flow control strategies found normal-bulk data transfer protocols permit duplicate elimination multiple data packets call call-relative sequence number figure shows packet sequences complicated calls acm transactions computer systems vol february implementing remote procedure calls caller machine user rpc stub send call pkt wait ack build pkt transmit wait retransmit wait wait esult return acknowledge callee machine call callld pkt pleaseack ack callld pkt data callld pkt dontack data callld pkt pleaseack ack callid pkt result callld pkt dontack result callld pkt pleaseack ack callld pkt rpc stub start arg record acknowledge wait pkt invoke call acknowledge send result wait ack retransmit wait ack idle server fig complicated call arguments occupy packets call duration long require paper serves overview system discusses architecture lowest building blocks computing environment users serves introduction rest plan programmer manual accompanies detail topics paper found manual design view system built principles resources named accessed files hierarchical file system standard proto col called accessing resources disjoint hierarchies provided services joined single private hierarchical file space unusual properties plan stem consistent aggressive application principles large plan installation number computers networked providing class service shared multiprocessor servers provide comput ing cycles large machines offer file storage machines located air-conditioned machine room connected high-performance networks lower bandwidth networks ethernet isdn connect servers officeand home-resident workstations pcs called terminals plan terminology figure shows arrangement cpu cpufile file internet gateway gateway term term term term ethernet fiber network datakit termtermterm figure structure large plan installation cpu servers file servers share fast local-area networks terminals slower wider-area networks ethernet datakit telephone lines connect gateway machines cpu servers connected multiple net works machines network modern style computing offers user dedicated workstation plan approach machines screens keyboards mice provide access resources network functionally equivalent manner terminals attached timesharing systems system terminal temporarily personalized user customizing hardware plan offers ability customize view sys tem provided software customization accomplished giving local sonal names publicly visible resources network plan mech anism assemble personal view public space local names globally accessible resources important resources network files model view file-oriented client local space customize user view network services network export file hierarchies impor tant user gathered custom space interest style idea uniform global space plan names services uniform names files exported services view local analogy difference phrase house precise address speaker home easier makes sense spoken meaning depending confusion similarly plan dev cons refers user terminal bin date correct version date command run files names represent depends circumstances architecture machine executing date plan local spaces obey globally understood conventions conventions guarantee sane behavior pres ence local names protocol structured set transactions send request client local remote server return result controls file systems files includes procedures resolve file names traverse hierarchy file system provided server hand client space held client system server distinction systems sprite ocdnw file access level bytes blocks distinguishes protocols nfs rfs paper welch compares sprite nfs plan network file system structures welc approach designed traditional files mind extended resources plan services export file hierarchies include devices backup services window system network interfaces exam ple process file system proc clean examine con trol running processes precursor systems similar idea kill plan pushes file metaphor ppttw file system model well-understood system builders general users services present file-like interfaces easy build easy understand easy files agreed-upon rules protection naming access local remote services built ready-made distributed system distinction objectoriented models issues faced anew class object examples sections follow illustrate ideas action command level view plan meant machine screen running window sys tem notion teletype unix sense keyboard handling bare system rudimentary window system pike running text edited cut paste operations pop-up menu copied win dows permits editing text past current input line text-editing capabilities strong displace special features history shell paging scrolling mail editors windows support cursor addressing terminal emulator simplify connecting traditional systems cursor-addressing software plan window created separate space adjustments made space window affect windows programs making safe experi ment local modifications space substitute files dump file system debugging debugging window deleted trace experimental apparatus similar arguments apply private space window environment variables notes analogous unix signals window created running application shell standard input output connected editable text 
window window private bitmap multiplexed access keyboard mouse graphical resources files dev mouse dev bitblt dev cons analo gous unix dev tty files provided implemented file server unlike windows application typically creates window run graphics application runs window starts possi ble efficient application create window style system contrasting remote application makes network call server start running remote application sees mouse bitblt cons files window usual dev files local reads writes control window network connection multiplexed intended style run interactive applications window sys tem text editor terminal run computationor file-intensive applica tions remote servers windows running programs machines networks making space equivalent win dows transparent commands resources names computation performed command set plan similar unix commands fall sev eral broad classes programs jobs programs cat familiar names functions simpler implementations shell script lines code commands essentially unix ancestors awk troff con verted ansi extended handle unicode familiar tools programs niches shell text editor sam debugger acid displace better-known unix tools similar jobs finally half commands compatibility requirement system commands notation good didn replaced file server central file server stores permanent files presents network file hierarchy exported server stand-alone system accessible network designed job runs user processes fixed set routines compiled boot image set disks sepa rate file systems main hierarchy exported server single tree represent ing files disks hierarchy shared users wide area variety networks file trees exported server include special-purpose sys tems temporary storage explained backup service file server levels storage central server installation megabytes memory buffers gigabytes magnetic disks giga bytes bulk storage write-once-read-many worm jukebox disk cache worm memory cache disk faster sees order magnitude traffic level caches addressable data file system larger size generation magnetic disks specific incarnation container cache generations main file server reused container gigabytes reused active previous storage generation unusual container feature freed file server generation container worm device life generation stable storage fully morning defined clock container dump logical file generation system number occurs tracks auto matically times file container system frozen reused note blocks generation modified refer dump contents queued container written worm abstraction blocks current incarnation queued contents service change restored affecting read-only root generation dumped illustrate file notion system appears containers hierarchy generations dumps simple named typical unix-based date file system directory file system dump fixed set root designated directory inodes image inode slot file system container appeared point early inode morning slot march takes inode minutes generation queue corresponds blocks specific file process copy file blocks deleted worm inode runs generation background deleted forever hours inode container ways simply dump marked file free system file created reuse users inode container browse logically dump file inode system directly generation similarly attach pieces directory container block space directory track entries bug stored block straightforward compiler generation months time ago pointers link program directory yesterday inodes library pointers daily respective snapshots inode containers files directory easy entries find typically change inode number made note single container made inode date point people feel multiple free containers make large data speculative blocks single files container knowledge pointed backed multiple containers single copy hard command links unix backup file system systems data block containers typically dump point file space backup container problems symbol description solved set standard entities tools point container grep set diff entities pointed rare container complete system container backup tracks event container disaster live active set file entities system point initialized generation dump set clearing entities pointed disk cache generation set ting denotes root container active file pointer system denotes copy entity dumped points root easy kth epoch container lightly type kth losing epoch change container made date generation kth dump epoch recovery container method results container slow system generation cache generation reloaded container worm table notations slower containers magnetic disks generations notations file system notations takes days depict reload basic entities working set relationships regain full listed performance access table permissions containers files denoted dump upper case letters generations dump denoted lower made case normal letters utilities entity normal permissions description dump represents container special arrangement generation note dump file system notations read-only table defined means files section memory disk versions containers file system manage structures domains volatile memory disk accessing contents container file system read on-disk version container memory subsequently file system make modifications in-memory copy container modified contents periodically written disk duration residence memory file system writes modified container disk contents modified container memory disk beliefs actions formulate logic terms beliefs actions belief represents state memory disk action operation performed file system resulting set beliefs belief represents creation existence state statement enclosed represents belief beliefs memory beliefs disk beliefs denoted belief file system memory container points memory means disk belief memory beliefs represent state file system tracks memory on-disk beliefs defined belief holds disk time crash file system conclude belief purely based scan on-disk state time on-disk beliefs solely dependent on-disk data file system manages free reuse containers beliefs terms generations valid note refers generation container on-disk beliefs deal containers generation information lost disk sections propose techniques expose generation information disk show enables guarantees component logic actions result set beliefs hold time actions defined logic performed file system disk read operation file system read contents on-disk container current generation memory file system container memory modify read contents memory on-disk write operation results flushing current contents container disk operation contents memory on-disk ordering beliefs actions fundamental aspect interaction file system disk ordering actions ordering actions determines order beliefs established operation file system viewed partial ordering beliefs actions order actions resulting beliefs operators means occurred time note ordering beliefs notation indicating event creation belief state existence belief belief represents event file system assigns pointers special ordering operator called precedes belief left operator operator defined means belief occurs means belief holds occurs implies intermediate action event invalidates belief operator transitive imply belief hold necessarily note simply shortcut proof system primitives sequencing beliefs actions define rules logic terms implication event sequence sequence traditional operators implication double implication logical operator logical combine sequences logical rule notation means time event action occurs event occurs point occurrence rule rule denotes time occurs occurred note rule event occurs sides event constitutes temporal point referring time instant lhs rhs temporal interpretation identical events crucial rule serving intended implication rhs 
refer instant rules logical proofs event sequence substitution rule subsequence occurs sequence events logically implies event apply rule event sequence replacing subsequence matches left half rule half rule postulate attributes containers make logic expressive modern file systems extend vocabulary logic attributes container generation attributes container define attributes epoch type sharing epoch epoch container defined time contents container change memory epoch incremented file system batch multiple contents due buffering set epochs visible disk subset total set epochs container denote epoch superscript notation denotes kth epoch note definition epoch expressivity logic imply expect file system track epoch note distinction epoch generation generation change occurs reuse container epoch change contents container reused type containers type type container static change lifetime file system dynamic container belong types points time typical ffs-based file systems inode containers statically typed block containers change type data directory indirect pointers denote type container notation shared unshared container pointed container called shared container container pointer leading unshared default assume containers shared denote unshared containers operator unshared note unshared property container type file system ensures container belonging type unshared pointer pointing file systems designate data block containers unshared logical postulates subsection present fundamental rules govern behavior event sequences beliefs define composition operators present rules logic operators operator composition define rules operators logic compose operator distributive precedes operator ordering operators operator distributes rule note parentheses group beliefs transitive basic rules present basic rules govern transition beliefs memory disk actions leading container points memory current generation points memory points memory write lead disk belief points write converse states disk belief implies belief occurred memory similarly points disk read result file system inheriting belief read on-disk contents container pertain epoch generation pointed generation memory write converse holds write write hold memory points time container freed instants negationslash note rule includes scenario intermediate generation occurs container pointed disk subsequently file system removes pointer memory write lead disk belief point write unshared container write lead disk belief container points free write dynamically typed container type instants freed negationslash file system properties file systems provide guarantees update behavior guarantee translates rules logical model file system complement basic rules reasoning file system section discuss properties container exclusivity file system exhibits container exclusivity guarantees on-disk container dirty copy container contents file system cache requires file system ensure in-memory contents container change container written disk file systems bsd ffs linux ext vfat exhibit container exclusivity journaling file systems ext exhibit property equations refer containers memory refer latest epoch container memory case file systems obey container exclusivity means retransmission argument packet requesting acknowledgment result packet retransmitted requesting acknowledgment subsequent call arrived section protocol concentrates handling simple calls local networks call requires packet arguments results protocol sends packets logically required acceptable protocols designed efficient transfer bulk data incorporate rpc bulk data single protocol transferring large amount data direction protocol sends packets good bulk data protocol send acknowledge packet inappropriate long haul networks large delays high data rates communication activity represented procedure calls protocol desirable characteristics long haul networks practical rpc bulk data transfer networks multiplexing data processes making single packet calls--the penalty extra acknowledgment packet situations acceptable dominant advantage requiring acknowledgment argument packet simplifies optimizes implementation acm transactions computer systems vol february birrell nelson protocol simple calls switch automatically conventional protocol complicated explored possibility exception handling mesa language elaborate facilities procedure notify exceptions caller exceptions called signals thought dynamically bound procedure activations exception raised mesa runtime system dynamically scans call stack determine catch phrase exception body catch phrase executed arguments exception raised catch phrase return results causing execution resume exception raised catch phrase terminate jump lexically enclosing context case termination dynamically newer procedure activations call stack unwound most-recent-first order rpc package faithfully emulates mechanism facilities protocol process server machine handling call dump transmit exception written packet place permission result bits packet fact packet directories part handled read-only rpcruntime structure caller permissions machine approximately changed file written call worm packet removed invoking users call raises clean files exception messages process command regard worm catch jukebox unlimited phrase resource executed issue catch long phrase returns results fill worm served passed back community callee users machine events years proceed absorbed daily dumps consuming catch phrase total terminates jump storage callee jukebox machine time notified manufacturer unwinds improved technology procedure doubling activations capacity individual emulated disks semantics local upgrade calls media true fact free space permit callee machine original empty communicate jukebox technology exceptions created storage defined faster mesa interface unusual file callee servers exported plan simplifies characterized implementation variety servers translating offer exception file-like names interface unusual callee services machine environment caller implemented user-level protection processes debugging distinction assistance unimportant programming convention clients single machine service programs provided kernel package user process communicate exception remote server caller irrelevant exception defined dozens package interface servers exceptions section present handled representative debugger maintained remarkable enforced file server convention plan rpc exceptions window addition system exceptions raised discussed length callee rpcruntime pike raise deserves call failed explanation exception pro vides communication interfaces difficulty user seated primary terminal offers clients note traditional style difference interaction local multiple remote windows calls running application processes mesa controlled cedar mouse parallel keyboard processes client programs built-in view language feature fairly process creation traditional pro changing grams running processor state window process set swap files considered dev inexpensive names mouse forking screen process cons costs programs print ten text local procedure window calls write process dev swap cons involves read swapping mouse evaluation stack read dev mouse register invalidating plan style cached bitmap information graphics imple mented scale providing remote file procedure dev call bitblt process creation clients write process encoded swaps messages amount execute graphical significant operations cost bitblt shown rasterop nelson unusual experiments care file server serving cost low files building dev package clients running designing win dow protocol acm transactions window computer systems vol client window february distinct implementing set remote files procedure calls dev multiplexes step clients reducing access cost resources maintaining terminal machine serving stock multiple idle sets server files processes client handle incoming private packets space means call set files handled behave incurring cost process windows creation cost advantages initializing structure state serves server process files server process implementation finished multiplexes call interface reverts idle run state recursively client implementation dev tty unix requires special code kernel redirect open calls device equivalent service falls automatically serves dev cons basic function extra program read keyboard opens dev cons private file shared special properties local spaces make conventions consis tency files make natural unique feature made design implemented file server power postpone answering read requests win dow behavior toggled reserved key keyboard toggling sus pends client reads window toggling resumes normal reads absorb text prepared line time user edit multi-line input text screen application sees obviating invoke separate editor prepare text mail messages related property reads answered directly data structure defining text display text edited final newline makes prepared line text readable client line read text client read changed typing make shell user backspace final newline time make fin ishes holding execution command point mouse type command executed ftp command plan user-level file server called ftpfs dials ftp site logs behalf user ftp protocol examine files remote directory local user offers file hierarchy attached ftp local space mirroring contents ftp site words translates ftp protocol offer plan access ftp sites implementation tricky ftpfs sophisticated caching efficiency heuristics decode remote directory information result worthwhile local file management tools grep diff avail ftp-served files local files systems jade prospero exploited opportunity rao neu local spaces simplicity implementing approach fits naturally plan environments server exportfs user process takes portion space makes processes translating requests system calls plan kernel file hierarchy exports files multiple servers exportfs run remote server started local program import cpu import makes network call remote machine starts exportfs attaches connection local space import helix net makes helix network interfaces visible local net directory helix central server dying excess idle server processes kill created response transient peak number network interfaces permits machine network access helix time container latest epoch memory points similarly write means latest epoch time written referring specific version epoch notation container exclusivity holds epoch container exists memory container exclusivity stronger converse assume unshared stronger equation equation disk belief hold written file system note containers typical file systems data blocks unshared write reuse ordering file system exhibits reuse ordering ensures reusing container commits freed state container disk pointed generation memory freed generation made point freed state container generation pointer removed written disk reuse occurs write reuse results commit freed state extend rule write ffs soft updates linux ext examples file systems exhibit reuse ordering pointer ordering file system exhibits pointer ordering ensures writing container disk file system writes containers pointed write write write ffs soft updates file system exhibits pointer ordering modeling existing systems defined basic formalism logic proceed logic model reason file system behaviors section present proofs properties important file system consistency discuss data consistency problem file system model journaling file system reason non-rollback property journaling file system data consistency problem data consistency file system crash data consistency contents data block containers consistent metadata data blocks words file end data file file system recovers crash assume file metadata container pointers data blocks respective file data block container disk belief points holds on-disk contents written generation epoch pointed time past kth generation memory generation rule summarizes simplicity make assumption data containers file system nonshared files share data block pointers assume file system obeys container exclusivity property modern file systems ext vfat properties block exclusivity rewrite rule rule hold means file represented generation points generation contents written generation case data corruption show rule hold assume negation prove reachable sequence valid file system actions write event sequences implied lhs write order prove prove interleaving sequences clause negationslash invalid disprove prove interleavings valid negationslash event occur events due container exclusivity unshared similarly occur write interleavings write write case applying write negationslash applying write step valid sequence file system execution generation freed due delete file represented generation subsequent generation block reallocated file represented generation memory shown violation occur assume file system obeys reuse ordering equation additional constraint equation imply write write write contradiction initial assumption started reuse ordering shown scenario arise case write negationslash applying negationslash write eqn write valid file system sequence file generation pointed data block generation generation deleted generation container assigned file generation consistency violation occur scenario interestingly apply write write apply case belief hold rule led belief immediately write belief overwritten sequence invalidate sequence reuse ordering guarantee data consistency case make assumption file system obeys pointer ordering assume unshared container exclusivity holds apply equation write write applying pointer ordering rule eqn write write write write write contradiction implies contents disk belong generation started assumption negationslash reuse ordering pointer ordering file system suffers data consistency violation file system obey ordering ext data consistency compromised crashes note inconsistency fundamental fixed scan-based consistency tools fsck modeling file system journaling extend logic rules define behavior journaling file system model reason key property journaling file system journaling technique commonly file systems ensure metadata consistency single file system operation spans multiple metadata structures file system groups transaction guarantees transaction commits atomically reach disk reach disk preserving consistency provide atomicity file system writes write-ahead log wal propagates actual on-disk location transaction committed log transaction committed logged special commit record written log indicating completion transaction file system recovers crash checkpointing process replays belong committed transactions model journaling logical transaction object determines set log record containers belong transaction logically pointers log copies containers modified transaction 
denote log copy journaled container symbol top container container log journal file system note assume physical logging block-level logging ext physical realization transaction object commit record logically points containers changed transaction wal property hold commit container written log copy modified containers transaction points written commit container wal property leads rules write write write write write write rule states transaction committed commit record written containers belonging transaction written disk rule states on-disk copy container written transaction container modified committed disk note unlike normal pointers considered point containers generations pointers container rules point epochs epoch pointers commit record specific epoch snapshot container replay checkpointing process depicted rules write write rule container part transaction transaction committed disk on-disk copy container updated logged copy pertaining transaction rule container part multiple committed transactions on-disk copy container updated copy pertaining transactions belief transitions hold write write rule states points belongs transaction commit leads disk belief rule disk belief holds immediately commit transaction part creation belief require checkpoint write happen disk belief pertains belief file system reach start current disk state journaling file systems containers types journaled updates containers directly disk transaction machinery proofs cases complete journaling containers journaled selective journaling containers type selective case address possibility container changing type journaled type non-journaled type vice versa container belongs journaling type converse equation write show complete journaling data inconsistency occurs omit due space constraints non-rollback property introduce property called non-rollback pertinent file system consistency formally define property reason conditions required hold journaling file system non-rollback property states contents container disk overwritten older contents previous epoch property expressed rule states on-disk contents move epoch logically imply epoch occurred epoch memory rpc calls packet process identifier source destination packets caller machine source process identifier calling process packets callee machine source process identifier server process handling call call process transmits packet sets destination process identifier packet source process identifier preceding packet call process waiting packet call process notes 
fact simple data structure shared ethernet interrupt handler interrupt handler receives rpc packet destination process identifier process machine time waiting rpc packet incoming packet dispatched directly process packet dispatched idle server process decides packet part current call requiring acknowledgment start call server process handle duplicate discarded means cases incoming packet process process swap arrangements resilient incorrect process identifier calling activity initiates call attempts destination identifer process handled previous call activity beneficial process waiting acknowledgment results previous call non-rollback call property packet crucial sufficient journaling file acknowledgment systems absence slight performance property degradation lead result data corruption caller disk believes xth epoch networks import possibilities local machine type make calls journaled type networks connected belonged helix transaction import helix disk proc makes observed helix commit processes record visible transaction local proc permitting local debuggers belief examine remote occurs processes immediately cpu commit command connects local point terminal actual remote contents cpu server works written opposite file system direction part import calling checkpoint propagation server 
starts local exportfs mounts space process typically newly created shell server rearranges space make local device files served terminal window system visible server dev directory effect running cpu command start shell fast machine tightly coupled file server space analogous local local device files visible remotely remote applications full access local services bitmap graphics dev cons rlogin reproduce local space remote system file sharing nfs achieve space equivalence combination access local hardware devices remote files remote cpu resources cpu command uniquely actual transparent on-disk mechanism location re-establishing reasonable belief start window system set window running journaled cpu types mand windows created automatically start write processes cpu write server configurability administration uniform possibility interconnection components type plan makes journaled config case ure plan installation disk learnt ways single laptop prior commit function stand-alone plan system write extreme setup central multiprocessor cpu journaled servers file assume servers scores terminals belong ranging journaled small pcs type high-end prove graphics non-rollback workstations large property installations lhs represent plan oper ates system software journaled portable operating system sequence runs events led hard ware beliefs performance appearance system write sgi workstation write laptop computing file write services centralized write termi nals omitting permanent write file actions storage terminals sequences functionally simplicity identical sequences plan events good properties timesharing systems user note sit front sequence machine instances system disk belief modern workstation created commu nity instance machines tend created owned people transaction customize committed storing private instance infor mation checkpoint local propagation disk reject style time snapshot-based coarse-grained system journaling systems ext transactions group committed laboratory order epoch public-access machines occurred ter minal room committed user sit instance work occur central file servers instance centralize files property true journaling administration maintenance checkpointing fact in-order server main server holding committed transactions system files copies servers provide extra data storage version pertaining debugging transaction special propagated system checkpoint software resides machine sequences events means lead program interleavings sin gle depending copy epoch binary occurs epoch architecture vice trivial versa install updates ordering bug epoch fixes single user fixed database rest events synchronize constrained distinct single passwd sequence files interleaving hand depending single central server limit size interleaving installation power centralized file service plan adminis ters interleaving network results information contradiction central server initial statement directory started lib ndb information administer interleaving local legal ethernet net sequences works events machines combined database talk interleaving implies network manage proved distributed naming system epochs journaled parallel files non-rollback property date holds install journaled machine local ethernet choose address add single file lib ndb machines installation talk immediately start running plug machine network turn bootp tftp load kernel automatic finally automated dump file system frees users maintain systems providing easy access backup files tapes special mands involvement support staff difficult overstate improvement lifestyle afforded service plan runs variety hardware constraining configure installation laboratory chose central servers amortize costs administration sign good decision cheap terminals remain comfortable places work years longer workstations provide complete computing environment upgrade central machines computation plan terminals improves time money saved avoiding regular upgrades terminals spent newest fastest multiprocessor servers estimate costs half money networked workstations general access powerful machines programming plan utilities written languages scripts shell duff handful written c-like concurrent language called alef wint great majority written dialect ansi ansic programs originate pre-ansi code research unix system unix updated ansi reworked portability cleanliness plan dialect minor extensions pike major restrictions important restriction compiler demands function definitions ansi prototypes function calls scope prototyped declaration function stylistic rule prototyped declaration header file included files call function sys tem library header file declaring functions library standard plan library called libc source files include libc rules guarantee functions called arguments expected types true pre-ansi programs restriction compilers accept subset preprocessor directives required ansi main omission nec essary abused effect achieved means instance toggle feature compile time written regular statement relying compile-time constant folding dead code elimination dis card object code conditional wrong compilation destination process ifdef caller maintains sparingly plan single destination process architecture-dependent ifdefs calling process system summary low-level normal routines sequence graph events ics library process wishing avoid make dependencies call manufactures isolate packet separate call source guesses files plausible libraries making code destination hard process read identifier ifdefs sets make source impossible source presents compiled packet binary ethernet output source device protected waits compile incoming work packet properly callee make machine harder interrupt maintain handler software receives packet standard plan notifies library overlaps server ansi process server posix process posix handles diverges packet manufactures plan response goals packet implementation destination process semantics identifier function packet change change process waiting instance caller unix machine creat plan response packet create arrives function caller takes machine arguments interrupt handler original passes directly calling argument process open calling defines process returned file process descriptor identifier server opened process reading writing subsequent design packets forced call initiating imple ments creation call effect simplifies scheme common simple create calls initialize processes tempo created rary file departure typically ansi process swaps plan call -bit inherently character set minimum called uni number code process swaps iso unicode busy-wait stopped --we short incurred full extra internationalization plan incoming treats packets representation handled major interrupt languages handler uniformly dispatched software correct simplify process exchange directly text device programs microcode characters decided packed write byte specialized stream microcode acm encoding transactions designed computer called systems utfwhich vol accepted february standard birrell fssutf attractive nelson properties optimizations including byteorder discussion independence shows backwards compatibility optimizations ascii adopted ease subsequent implementation packets implicit problems acknowledgment adapting previous existing packets software attempt large minimize character set costs maintaining encoding connections represents characters avoid costs variable number establishing bytes terminating ansi connections addresses reduce issues number falls process short switches solving involved call pick detailed char optimizations acter set encoding significant payoff define transmitting receiving rpc library packets bypass routines software layers functions correspond define normal engineering layers problems protocol stan hierarchy dard left problems cases unsolved decided caller build callee interface network--we sepa rate paper protocol hierarchy details pike internetwork small routing class plan substantial programs performance follow gains conventions discussed sense cheating section successful programs optimization imported rpc maintained package unix community tex modified representative network-driversoftware treat rpc packets special case profitable ten special cases aims imply rpc special case intend dominant communication protocol utility optimization artifact implementation layered protocol hierarchy transport level protocol improve performance significantly by-passing full generality lower layers reasonable optimizations refrain internet packet format local network communication specialized packet formats simple calls implement special purpose network microcode forbid non-rpc communication save process switches busy-waits avoided optimizations inconvenient achieved sufficient efficiency purposes provided extra factor performance security rpc package protocol include facilities providing encryption-based security calls facilities grapevine authentication service key distribution center federal data encryption standard callers guarantee identity callee vice versa provide full end-to-end encryption calls results encryption techniques provide protection eavesdropping conceal patterns data detect attempts modification replay creation calls insufficient space describe additions modifications made support mechanism reported paper performance mentioned nelson thesis included extensive analysis rpc protocols implementations included examination contributing factors differing performance characteristics repeat information made measurements rpc package measurements made remote calls rados connected acm transactions computer systems vol february implementing remote procedure calls table performance results examples remote calls procedure minimum median transmission local-only args results arg result args results args results args results word array word array word array word array word array resume unwind ethernet ethernet raw data rate megabits dorados running cedar measurements made ethernet shared users network lightly loaded tests ten percent capacity times shown table microseconds measured counting dorado microprocessor cycles dividing crystal frequency accurate ten percent times elapsed times include time spent waiting network time interference devices measuring user program invokes local procedure exported userstub return procedure call interval includes time spent inside user-stub rpcruntime machines server-stub server implementation procedures transmission times directions test procedures exported single interface encryption facilities measured individually elapsed times calls procedure table shows minimum elapsed time observed median time present total packet transmission times call calculated packet sizes protocol direct measurement finally present elapsed time making calls user program bound directly server program making purely local call involvement rpc package time purely local calls provide reader calibration speed dorado processor mesa language times local calls part total time due rpc procedures arguments results argument result bits long procedures argument result argument result array size words line bottom shows call procedure raises exception caller resumes line procedure raising exception caller unwound transferring large amounts data direction protocols rpc advantage transmit fewer packets acm transactions computer systems vol february birrell nelson direction interleaving parallel remote calls multiple processes achieved data rate megabits transferring dorado main memories megabit ethernet equal rate achieved highly optimized byte stream implementation written bcpl measured cost exporting importing interface operations dominated time spent talking grapevine server locating exporter machine calling exporter determine dispatcher identifier rpc call words data status discussions package fully implemented cedar programmers entire rpcruntime package amounts cedar modules packet exchange packet sequencing binding security totalling lines source code lupine stub generator substantially larger clients rpc projects including complete communication protocol alpine file server supporting multimachine transactions control communication ethernet-based telephone audio project network games providing real-time communication players multiple machines clients found package convenient projects full-scale implementations protocol made bcpl interlisp smalltalk early stages acquiring experience rpc work confidence strength design appropriateness rpc earnest projects committing circumstances rpc wrong communication paradigm correspond situations solutions based multicasting broadcasting distributed environment times procedure calls language parallel processing coroutine facilities sufficiently powerful tool situations single machine hopes providing rpc package high performance low cost encourage development distributed applications infeasible present hard justify insistence good performance lack examples demonstrating importance performance belief examples present lack due fact historically distributed communication inconvenient slow starting distributed algorithms developed considered major undertaking trend continues successful question undecided sufficient level performance rpc aims achieved general purpose transport protocol implementation adopts strategies suitable rpc suitable bulk data transfer convincing argument impossible hand achieved acm transactions computer systems vol february implementing remote procedure calls parts rpc package discussed general interest ways represent point design spectrum rpc achieved good performance adopting extreme measures sacrificing call parameter semantics techniques managing transport level connections minimize communication costs state maintained server important experience servers dealing large numbers users binding semantics powerful conceptually simple programmer familiar single machine binding easy efficient implement birrell levin needham schroeder grapevine exercise distributed computing commun acm april boggs internet broadcasting phd dissertation department electrical engineering stanford jan boggs shoch taft metcalf pup internetwork architecture ieee trans commun april courier remote procedure call protocol xerox system 
integration standard xsisxerox corporation stamford connecticut dec data encryption standard fips national bureau standards department commerce washington january deutsch taft requirements exceptional programming environment tech rep csl- xerox palo alto research center palo alto calif ethernet local area network data link layer physical layer specifications version digital equipment corporation intel corporation xerox corporation sept lampson pier processor high-performance personal computer proc ieee symposium computer architecture ieee york lampson schmidt practical polymorphic applicative language proc tenth annual acm symposium principles programming languages austin texas jan acm york liskov primitives distributed computing oper syst rev dec metcalfe boggs ethernet distributed packet switching local computer networks commun acm july mitchell maybury sweet mesa language manual version tech rep csl- xerox palo alto research center palo alto calif nelson remote procedure call tech rep csl- xerox palo alto research center palo alto calif spector performing remote operations case type epochs belongs journaled type start statement equations sequences events write write write omitting write actions sake readability sequences prove non-rollback property show interleaving sequences results contradiction co-exist interleavings scenarios imply invalid interleavings scenarios valid interleavings contradict initial assumption disk beliefs time imply scenarios violate non-rollback property dynamic typing journaling mechanism guarantee non-rollback due violation file contents corrupted stale metadata generations scenario occur checkpoint propagation earlier epoch journaled occurs overwritten non-journaled epoch prevent impose checkpoint propagation container context transaction happen on-disk contents container updated commit journal revoke records ext precisely guarantee revoke record encountered log replay block propagated actual disk location scenario epoch committed disk transaction modified earlier epoch committed prevent form reuse ordering imposes container type reused memory transaction freed previous generation committed transactions commit order freeing transaction occur transaction guarantee write additional rule scenario scenarios handled revoke record solution properties non-rollback property guaranteed redundant synchrony ext examine performance problem ext file system transaction commit procedure artificially limits parallelism due redundant synchrony disk writes ordered mode ext guarantees newly created file point stale data blocks crash ext ensures guarantee ordering commit procedure transaction committed ext writes disk data blocks allocated transaction waits writes complete writes journal blocks disk waits complete writes commit block inode container file data block container transaction commit container commit procedure ext efficiently expressed equation write write write write examine condition ensure no-stale-data guarantee formally depict guarantee ext ordered mode seeks provide equation equation states disk acquires belief contents data container disk pertain generation pointed memory note ext obeys reuse ordering ordered mode guarantee cater case free data block container allocated file prove equation examining conditions hold equation true lhs equation applying equation write applying equation write write write equation write write current ext commit procedure equation guarantees nostale-data property waits procedure required reorder actions write write write write write applying equation ordering actions write write inconsequential guarantee ext ordered mode attempts provide conclude wait ext employs write data blocks redundant unnecessarily limits parallelism data journal writes severe performance implications settings log stored separate disk ext unnecessarily limited single disk bandwidth specific ext points general problem file system design developers rigorous frameworks reason correctness tend conservative conservatism translates unexploited opportunities performance optimization systematic framework logic enables aggressive performance optimizations compromising correctness support consistent undelete section demonstrate logic enables quickly formulate prove properties file system features mechanisms explore functionality traditionally considered part core file system design ability undelete deleted files consistency guarantees ability recover deleted files demonstrated large number tools purpose tools rebuild deleted files scavenging on-disk metadata extent file systems freed metadata containers simply marked free unix file system block pointers deleted inode blocks belong deleted file existing tools undelete guarantee consistency assert recovered contents valid undelete fundamentally best-effort files recovered blocks subsequently reused file user trustworthy recovered contents demonstrate logic existing file systems consistent undelete impossible provide simple solution prove solution guarantees consistent undelete finally present implementation solution ext undelete existing systems order model undelete logic capable expressing pointers containers holding dead generation introduce thea notation pointer call dead pointer define operator container denotes set dead live entities pointing container undel action initiate undelete container undelete process summarized equation undel words dead free container points disk container alive dead pointing undelete makes generation live makes point guarantee hold consistency dead pointer brought alive on-disk contents time pointer brought alive generation epoch originally pointed memory similar data consistency formulation note 
clause required lhs cover case generation brought life true undelete show guarantee hold necessarily negation rhs negationslash show condition co-exist conditions required undelete equation words show undel negationslash arise valid file system execution utilize implications proof write write interleaving event sequences write write valid file system sequence file represented generation points written disk block freed killing generation generation allocated generation deleted written disk disk beliefs initial state disk sequence simultaneously lead disk belief shown conditions negationslash hold simultaneously undelete point lead violation consistency guarantee associate stale generation undeleted file shown reuse ordering pointer avoid reconverting programs time version released built porting environment called ansi posix envi ronment ape tric ape comprises separate include files libraries mands conforming strict ansi base-level posix speci fications port network-based software windows add extensions specifications bsd networking functions portability compilation plan portable variety processor architectures single puting session common architectures window system running intel processor connected mips-based cpu server files resident sparc system heterogeneity transparent conventions data interchange programs software maintenance straightfor ward conventions cross-architecture compilation avoid byte order problems data communicated programs text practical amount data high binary format data communicated byte stream predefined encoding multi-byte values rare cases format complex defined data structure structure communicated unit decomposed individual fields encoded ordered byte stream reassembled recipient conventions affect data ranging kernel application program state information object file intermediates generated compiler programs including kernel present data file system inter face access mechanism inherently portable system clock represented decimal number file dev time time library function time system call reads file converts binary similarly encoding state application process series flags bits private memory kernel presents text string file named status proc file system process plan command trivial prints contents desired status files minor reformatting import helix proc local command reports status helix processes supported architecture compilers loader alef compilers produce intermediate files portably encoded contents unique target architecture format file independent compiling proces sor type compiler architecture compiled type pro cessor compile program intermediate produced architecture identical intermediate produced native processor compiler point view compilation cross-compilation architecture loader accepts intermediate files produced compilers architecture files generated compiler exe cuting type processor instance run mips compiler mips loader sparc produce mips executable plan runs variety architectures single installation distin guishing compilers intermediate names simplifies multi-architecture development single source 
tree compilers loader architec ture uniquely named command names derived concate nating code letter target architecture piler loader letter code letter intel processors compiler named alef compiler loader called similarly compiler intermediate files suffixed plan build program relative make reads names current target architectures environment variables called cputype objtype default current processor target setting objtype architecture invoking results cross-build objtype sparc builds program sparc architecture executing machine objtype selects file architecture-dependent variable definitions configures build compilers loader simpleminded technique works practice applications plan built single source tree build architectures parallel conflict parallel programming plan support parallel programming aspects kernel pro vides simple process model carefully designed system calls synchroniza tion sharing local computer network commun acm april white high-level framework network-based resource sharing proc national computer conference june received march revised november accepted november acm transactions computer systems vol february 
ordering guarantee consistency case undelete generation pointers propose notion parallel programming language called alef supports concurrent programming generation pointers show pointers consistent undelete guaranteed assumed pointers disk point containers discussed section pointer pointed specific generation leads set file system properties implement generation pointers on-disk container generation number incremented time container reused addition on-disk pointer embed generation number addition container generation pointers on-disk contents container implicitly generation valid belief means disk contents belong generation generation pointers criterion undelete undel introduce additional constraint negationslash left hand side equation previous subsection negationslash negationslash denote on-disk container holds generation number equation negationslash contradiction means on-disk container generations simultaneously undelete occur scenario alternatively flagged inconsistent undeletes occurring generation pointers consistent implementation undelete ext proof consistent undelete implemented generation pointer mechanism linux ext block generation number incremented time block reused generation numbers maintained separate set blocks ensuring atomic commit generation number block data straightforward data journaling mode ext simply add generation update create transaction block pointers inode extended generation number block implemented tool undelete scans on-disk structures restoring files undeleted consistently specifically file restored generation information metadata block pointers match block generation data blocks ran simple microbenchmark creating deleting directories linux kernel source tree observed roughly deleted files files roughly detected inconsistent undeletable remaining files successfully undeleted illustrates scenario proved section occurs practice undelete tool generation information wrongly restore files corrupt misleading data application semantic disks move application logic framework reasoning semanticallysmart disk systems case studies reasoning process hard complex functionality formalism memory disk beliefs fits sds model extra file system state tracked sds essentially disk belief section logic explore feasibility tracking block type semantic disk show usage generation pointers file system simplifies information tracking sds block typing important piece information required semantic disk type disk container identifying type statically-typed containers straightforward dynamically typed containers hard deal type dynamically typed container determined contents parent container indirect pointer block identified observing parent inode block indirect pointer field tracking dynamically typed containers requires correlating type information type-determining parent information interpret contents dynamic container accurate type detection sds guarantee hold words disk interprets contents epoch belonging type contents belonged type memory guarantees disk wrongly interpret contents normal data block container indirect block container note equation impose guarantee disk identifies type container states association type contents correct prove state algorithm disk arrives belief type sds snoops metadata traffic typedetermining containers inodes container written observes pointers container concludes type pointers assume pointer type points container disk examines container written time freed interprets current contents belonging type written time contents type equation words interpret belonging type disk container points current on-disk epoch type function abstracts indication disk contents epoch order associate contents type explore logical events led components side equation applying similarly component write verify guarantee equation assume hold observe leads valid scenario add clause negationslash equation equation prove event sequences write type epoch unique write container implies type write write sequences interleaved ways epoch occurs epoch interleaving write write valid sequence container freed disk acquired belief version written actual type changed memory leading incorrect interpretation belonging type order prevent scenario simply reuse ordering rule rule sequence imply write write write written disk treating free wrongly associate type interleaving proceeding similarly interleaving epoch occurs assigned type arrive sequence write simply applying write parallel programs alef parallel language choice trend operating systems implement classes processes normal unix-style processes light-weight kernel threads plan single class process fine control sharing process resources memory file descriptors single class process feasible approach plan kernel efficient system call interface cheap process cre ation scheduling parallel programs basic requirements management resources shared processes interface scheduler fine-grain process synchroniza tion spin locks plan processes created rfork system call rfork takes single argument bit vector specifies parent process resources shared copied created anew child resources controlled rfork include space environment file descriptor table memory segments notes plan analog unix signals bits controls rfork call create process bit resulting modification resources occurs process making call exam ple process calls rfork rfnameg disconnect space parent alef fine-grained fork resources including memory shared parent child analogous creating kernel thread systems indication rfork model variety ways canonical library routine fork hard find calls rfork bits set programs create forms sharing resource allocation system types processes regular processes threads handle variety ways share memory flag rfork mem ory segments parent shared child stack forked copy-on-write alternatively segment memory attached segattach system call segment shared par ent child rendezvous system call processes synchronize alef implement communication channels queuing locks multiple reader writer locks sleep wakeup mechanism rendezvous takes arguments tag process calls rendezvous tag sleeps process presents matching tag pair tags match values exchanged processes rendezvous calls return primitive suffi cient implement full set synchronization routines finally spin locks provided architecture-dependent library user level processors provide atomic test set instructions implement locks notable exception mips sgi power series multiprocessors special lock hardware bus user processes gain access lock hardware mapping pages hardware locks address space segattach system call plan process system call block weight means program wishes read slow device blocking entire cal culation fork process read solution start satellite process delivers answer main program shared memory pipe sounds onerous works easily efficiently practice fact interactive plan applications ordinary writ ten text editor sam pike run multiprocess programs kernel support parallel programming plan hundred lines portable code handful simple primitives enable problems handled cleanly user level primitives work fine expressive alef creation management slave processes written lines alef providing foundation consistent means multiplexing data flows arbitrary processes implementing language kernel ensures consistent semantics devices gen eral multiplexing primitive compare unix select system call select applies restricted set devices legislates style multiprogramming kernel extend networks difficult implement hard reason parallel programming important plan multi-threaded user-level file servers preferred implement services examples servers include programming environment acme pike space exporting tool exportfs ppttw http daemon network servers dns prwi complex applications acme prove careful operating system support reduce difficulty writing multi-threaded applications moving threading synchronization primitives kernel implementation spaces user processes construct spaces system calls mount bind unmount mount system call attaches tree served file server current space calling mount client means acquire connection server form file descriptor written read transmit messages file descriptor represents pipe network connection mount call attaches hierarchy existing space bind system call hand duplicates piece existing space point space unmount system call components removed bind mount multiple directories stacked single point space plan terminology union directory behaves concatenation constituent directories flag argument bind mount specifies position directory union permitting elements added front rear union replace file lookup performed union directory component union searched turn match likewise union directory read contents component directories read turn union directories widely organizational features plan space instance directory bin built union cputype bin program binaries bin shell scripts directories provided user construction makes shell path variable unnecessary question raised union directories element union receives newly created file designs decided default direc tories unions accept files create system call applied existing file succeeds directory reuse ordering rule prevent sequence stronger form reuse ordering freed state includes containers pointed allocation structure tracking liveness rule sequence write write add behavior sds states sds observes allocation structure indicating free inherits belief free write applying sds operation eqn write sequence sds observe write treated free associate type subsequently written shown sds accurately track dynamic type underneath file system ordering guarantees shown file system exhibits strong form reuse ordering dynamic type detection made reliable sds utility generation pointers subsection explore utility file system-level generation pointers context sds illustrate utility show tracking dynamic type sds straightforward file system tracks generation pointers generation pointers equation causal event sequences explored previous subsection write sequences imply generation types violates rule straightaway arrive contradiction proves violation rule occur summary dependability computer systems important essential systematic formal frameworks verify reason correctness file systems critical component system dependability formal verification correctness largely making file systems vulnerable hidden errors absence formal framework stifles innovation skepticism correctness proposals proclivity stick time-tested alternatives chapter step bridging gap file system design showing logical framework substantially simplify systematize process verifying file system correctness shown logic systematizes process reasoning semantic disks chapter related work related work semantically-smart disk systems grouped classes work smarter storage general work implicit systems discuss turn finally discuss related work pertaining case studies logic framework smarter storage idea embedding intelligence storage systems well-explored dating back logic track devices suggested slotnick database machines proposed early range previous work smart disks fall categories category assumes interface file storage systems fixed changed category sds belongs research group proposes storage interface requiring file systems modified leverage interface group proposes interface programming model applications finally fourth group advocates placing storage-like smartness file systems reverting back model dumb storage fixed interfaces focus thesis integration smart disks traditional file system environment environment file system narrow scsi-like interface storage disk persistent store data structures early smart disk controller loge harnessed processing capabilities improve performance writing blocks current diskhead position wang log-based programmable disk extended approach number ways quick crash-recovery free-space compaction systems assume require knowledge file system structures limited range optimizations provide storage system interfaces developed provided local setting opportunities functionality network packet filter slice virtual file service slice interpose nfs traffic clients implement range optimizations preferential treatment small files interposing nfs 
traffic stream simpler scsi-disk block stream contents nfs packets well-defined high-end raid products perfect place semantic smartness typical enterprise storage system substantial processing capabilities memory capacity emc symmetrix server processors configured memory high-end raid systems leverage resources perform bare minimum semantically-smart behavior storage systems emc recognize oracle data block provide extra checksum assure block write comprised multiple sector writes reaches disk atomically thesis explores acquisition exploitation detailed knowledge file system behavior expressive zap interfaces vmmigration zap virtual primary machine factors process limits migration todd addition tannenbaum functionality distributed systems smart disk wisconsin narrow madison interface feb file spring systems overview storage class discussed surprising paper design research implementation investigates zap changing system interface mime migrating investigates computing enhanced environments interface zap osman context intelligent columbia raid controller live specifically mirgration mime virtual adds primitives machines vmmigration clients clark control cambridge updates copenhagen storage guest visible lecturers traffic class streams greg commit joe order addition operations logical discussion disks expand interface approaches allowing process file migration system presented express grouping papers preferences compared lists approach file systems sprite simplified system problem statement maintain assumptions zap information presents raid system exposes per-disk process information migration strives informed file meet system goals lfs migrate providing legacy performance applications optimizations modification control limitations redundancy common improved operating manageability system storage services finally leverage ganger existing suggests commodity operating systems avoid creating residual dependencies overhead small normal execution migration zap assumes environment consisting machines connected network commodity operating systems offer loadable kernel modules explicitly mentioned anticipated workload legacy unmodified networked applications expected zap requires process migrate initially created pod zap assumes user advance processes candidates migration contrasts sprite process machine told migrate zap assumes presence network file servers store applications user data dynamic dns servers support network address virtualization finally authors assume machine independent independent operating system stark contrast sprite vmmigration paper goal design options migration active operating systems hosting live services attention paid minimize downtime migration minimize resource contention service migration mechanism factors degrade quality service perspective client implementation address migration widearea network essentially assumed environment cluster data center charged providing interactive network-based services machines assumed local switch data accessed locally attached storage design implementation architecture fundamental zap design introduction pod abstraction pod collection processes host-independent virtualized view operating system pods self-contained units suspended secondary storage migrated machine transparently resumed pods private virtual namespace namespace consistent virtual resource names place host-dependent resource names pids processes pod interact usual processes pod namespace interact processes inside pod ipc mechanisms shared memory signals vmmigration mechanisms presented xen virtual machine monitor leveraged migrate operating system instances physical hosts words xen transparency checkpoint functionality sprite zap developers develop design focuses minimization downtime services unavailable total migration time state added union flag bind mount enables create permission property space directory file created machines synchronized affect reliability heavy lifting performed reevaluation interface needed outlines relevant case studies track-aligned extents freeblock scheduling distributed storage domain researchers considered changing interface storage order provide functionality petal storage system proposes sparse virtual disk abstraction file system counterpart frangipani recently boxwood storage system higher level abstractions b-trees software layers high level interface storage naturally conveys rich semantic information storage system recent work storage community suggests evolution storage place disks general-purpose network standard scsi bus suggested network disks export higherlevel object-like interface moving responsibilities low-level storage management file system drives specific challenges context fixed object-based interface file systems storage provide interesting avenue research utility semantic awareness programming environments contrast integration underneath traditional file system work focused incorporating active storage parallel programming environments recent work active disks includes acharya riedel amiri sivathanu research involves shipping computation storage system focuses partition applications host disk cpus minimize data transferred system busses reduce latency smarter file systems approach considered custom file systems circumvent narrow interface limitation revert back model storage treat storage collection dumb disks file system manages storage-like functionality raid layout placement migration data google file system category gfs implements smarts file system simply built top collection ide disks interesting alternative approach fraught limitations storage-level placement optimizations depend low-level details storage system power supply actual busses connecting disks gfs successful custom-made specific information control low-level details general purpose environments customizing file system low-level details infeasible surprise large enterprise storage systems today constitute multi-billion dollar industry general-purpose customers banks e-commerce site tend deploy sophisticated enterprise storage systems care low-level complexity implicit systems semantically-smart disk systems based general philosophy implicit inference information existing interfaces general approach formalized term gray-box systems arpaci-dusseau authors propose techniques infer state operating system based high-level gray-box assumptions expected behavior operating system approach subsequently applied infer control aspects operating system cache replacement file placement cpu scheduling implicit inference information explored boundaries application operating system boundary programming language research hsieh investigate automatic extraction bit-level instruction encodings feeding permutations instructions assembler analyzing resulting machine code schindler proposed implicit techniques tracking internal parameters disk drives utilizing timing information carefully chosen microbenchmarks recent work looked deconstructing internal information raid systems similar approach careful microbenchmarking chapter microbenchmarking techniques tracking information storage system implicit channel rely fragile timing channel techniques track wealth dynamic information existent modern storage systems examples systems usage implicit information explored inferring tcp behavior inferring policies commodity storage cluster co-ordinated process scheduling distributed systems partial availability case study d-graid partial availability draws related work number areas including distributed file systems traditional raid systems distributed file systems designers distributed file systems long ago realized problems arise spreading directory tree machines system walker discuss importance directory namespace replication locus distributed system coda mobile file system takes explicit care regard directory tree specifically file cached coda makes cache directory root directory tree coda guarantee file remains accessible disconnection occur interesting extension work reconsider host-based inmemory caching availability mind slice route namespace operations files directory server recently work wide-area file systems re-emphasized importance directory tree pangaea file system aggressively replicates entire tree root node file accessed island-based file system points fault isolation context wide-area storage systems island principle similar fault-isolated placement d-graid finally systems past place entire file single transparent process migration design alternatives sprite implementation meenali rungta distributed systems wisconsin madison january spring overview paper describes process migration mechanism sprite operating system offload work idle machines evict migrated processes idle workstations reclaimed owners authors made trade-off factors designing implementing mechanism transparency residual dependencies performance complexity authors claim emphasized transparency performance implementation lead mix factors sacrificed point transparency importance providing high degree transparency cost factors reliability system warranted problem statement assumptions network personal workstations machines typically idle time idle hosts represent substantial pool processing power harnessed authors implemented process migration sprite operating system purpose assumptions made authors environment workload listed users workstations means user returns workstation migrated processes automatically evicted migration occur automatically reduce load processor pool model user-invoked ownership model implies idle hosts plentiful suggests sophisticated policies idle host selection required cluster machines underlying system comprises hundered hosts noteworthy figure paper shows fileserver bottleneck hosts performing compilations open files write back cached object files workstations sprite system diskless dirty pages virtual memory dirty pages file cache written file server time migration workloads cited paper largely paralled make simulations network applications memory-intensive workloads included performance analysis trade-offs transparency transparency defined paper process behaviour affected migration execution environment produce results migrated process appearance rest world affected migration operation unmigrated process stopping signalling migrated process tranparent migration correct results makes user perpective intuitive importance workload-dependent performance performance defined efficiency act migration efficiency execution migrated remote processes dominates design virtual memory open files migration transfer flushing dirty pages file server discarding address space makes act migration fast migration open files chosen transferring entire state remote machine forwarding kernel calls home machine seeks avoid cost forwarding network residual dependencies residual dependency defined ongoing host maintain data structures provide functionality process process migrates host authors leave residual dependencies achieving transparency case fork exit resulting distributed process state degrades performace reliability system makes system complex complexity complexity system measured maintainability authors mention migration code broken easily due sprite kernel implementation turned pretty complex techniques main techniques employed achieve migration maintaining global state authors changed file system machine network sees namespace transferring state source machine target migrate virtual memory open files process identifiers forwarding kernel calls home technique employed cases gettimeofday getpgrp noted slight optimization authors saved penalty forwarding gettimeofday involve forwarding storing skew time remote machine call served locally case studies migrating virtual memory approaches transfer freezing process monolithic transfer pre-copying pages lazy-copying pages authors chosen variant lazy copying process freezes source machine dirty pages flushed process starts target resident pages paging ondemand build address space scratch target machine migrating open files main components state open file transferred file open file opened target machine closing source delete-of-an-opened-file semantics unix caching information file open source target machines simultaneously noted write-shared caused server disable caching file unnecessarily due pre-existing consistency semantics sprite file system access position shared machines processes sharing access position migrated case authors chose disable storing access position machine forwarding operations file server maintains position opinion makes subsequent file operations tediously slow undesirable design choice conclusions paper step forward distributed systems idea targeted workloads applications take-off authors understandably concerned designing system satisfied users compiling sprite kernels performing simulations goals easily achieved simpler system targeted making specific applications fast machine xen xen move state focus paper performance core union created direc tory union create permission creation fails full-fledged process migration system 
similar load balancing issues problem difficult space due constraints file placement block migration simpler centralized storage array traditional raid systems draw long history research classic raid systems autoraid learned complex functionality embedded modern storage array background activity utilized successfully environment afraid learned flexible trade-off performance reliability delaying updates raid research focused redundancy schemes early work stressed ability tolerate single-disk failures research introduced notion tolerating multiple-disk failures array stress work complementary line research traditional techniques ensure full file system availability number failures d-graid techniques ensure graceful degradation additional failures related approach parity striping stripes parity data parity striping achieve primitive form fault isolation layout oblivious semantics data blocks level redundancy irrespective importance meta-data data multiple failures make entire file system inaccessible file systems typically spread large files logical address space parity striping ensure collocation blocks file number earlier works emphasize importance hot sparing speed recovery time raid arrays work semantic recovery complementary approaches finally note term graceful degradation refer performance characteristics redundant disk systems failure type graceful degradation discuss thesis systems continues operation unexpected number failures occurs logical modeling systems section examine prior work related logic framework modeling file systems semantic disks previous work recognized modeling complex systems formal frameworks order facilitate proving correctness properties logical framework reasoning authentication protocols proposed burrows related work spirit paper authors formulate domain-specific logic proof system authentication showing protocols verified simple logical derivations domainspecific formal models exist areas database recovery database reliability body related work involves generic frameworks modeling computer systems well-known tla framework automaton framework frameworks general model complex systems generality curse modeling aspects file system extent paper tedious generic framework tailoring framework domain-specific knowledge makes simpler reason properties framework significantly lowering barrier entry terms adopting framework specifications proofs logic lines contrast thousands lines tla specifications automated theorem-proving model checkers benefits generic framework tla previous work explored verification correctness system implementations recent body work model checking verify implementations body work complementary logic framework logic framework build model invariants hold model implementation 
verified chapter conclusions future work beginning end end beginning winston churchill philosophy building systems hierarchy layers oldest influential system design layer system communicates well-defined interfaces layering clear advantages enabling independent innovation evolution system components cost interfaces layers formulated based implicit assumptions layers layers evolve time invalidate assumptions making interface obsolete sub-optimal overly limiting evolving system layers interfaces ideally evolve practical concerns preclude retard interface evolution problem narrow interface storage addressed thesis instance general problem system evolution thesis presented approach interface evolution implicit inference information fixed existing interfaces implicit interface evolution approach addresses fundamental bootstrapping problem explicit interface change facilitating demonstration benefits due potential interface actual interface change implicit inference techniques move industry rapidly interface catalyzing explicit interface change long time-scales explicit interface occur implicit techniques provide innovate short term sds approach storage vendor provide smart functionality today ship systems waiting industry embrace storage interface implicit interface evolution costs terms added system complexity small amount performance overhead costs justify benefits potential functionality decision case-by-case basis remainder chapter outline key lessons learned work presented thesis explore avenues future research based aspects thesis lessons learned section reflect key lessons learned experience designing implementing semantically-smart disk systems lessons broader relevance specific techniques case studies limited knowledge disk imply limited functionality main contributions thesis demonstration limits semantic knowledge tracked underneath modern file systems proof implementation limitations interesting functionality built inside semantically-smart disk system semantic disk system careful assumptions file system behavior hope work guide pursue similar semantically-smart disks easier build file systems reorder delay hide operations disks reverse engineering scsi level difficult found small modifications file systems substantially lessen difficulty file system inform disk believes file system structures consistent on-disk state challenges disk lessened similarly file system careful reuses blocks avoid significant source uncertainty semantic disk chapter summarize dynamic properties hold file systems file systems conform properties small alterations ease burden semantic disk development dependence dynamic properties file system double-edged blade semantic disks significantly simplified make assumptions dynamic ordering guarantees provided file system dependence dynamic properties concern dependence static on-disk layout file system d-graid general assumptions arbitrary reordering delay writes robust dynamic properties file system aggressive functionality secure deletion make assumptions dynamic properties order guarantee correctness added level dependence careful deciding extra functionality enables worth cost semantically-smart disks stress file systems unexpected ways file systems built operate top semantic disks behave graid specifically behave part volume address space unavailable heritage inexpensive hardware linux file systems handle unexpected conditions fairly exact model dealing failure inconsistent data blocks missing reappear true inodes semantically-smart disks push functionality storage file systems potentially evolve accommodate conservative techniques crucial working fundamental uncertainty points work stumbled appeared fundamental limitation terms semantic inference resolved based conservative technique abstraction pronounced secure delete strong requirements correctness semantic information based fundamentally imprecise mechanism conservative overwrites solve apparent limitation crucial aspects developing complex correctness-sensitive functionality semantic disks tolerance uncertainty conservative mechanisms abstractions detailed traces workload behavior invaluable excellent level detail traces simulate analyze potential d-graid realistic settings traces per-process information anonymize file extent pathnames included trace utilize study remaining challenge tracing include user data blocks semantically-smart disks sensitive contents privacy concerns campaign encounter difficult overcome domain specific formal models granularity greatly simplify reasoning logic framework modeling file systems semantic disks pointed utility formalizing complex systems making framework domain specific coarse granularity equations proofs logic extremely intuitive understand conceivably accessible real file system designers comparison generic low-level frameworks tla found framework significantly helped uncovering implicit assumptions made helped systematize process showing set mechanisms precisely guarantees claims similar frameworks domain knowledge significantly aid construction complex systems future work section describe extensions work thesis interesting relate general applications implicit inference approach semantic disks directions logic framework direct extensions semantic disk technology implicit inference domains thesis explored implicit inference semantic inference underneath modern file systems general philosophy techniques apply layer boundaries system stack subject problem stuck obsolete limiting interfaces instance similar problem occurs area virtual machine monitors vmm narrow interface guest operating systems perspective vmm exports virtual disk guest operating system observes block-level reads writes virtual disk storage system underneath scsi narrow interface vmm information prioritize requests multiple guest operating systems vmm distinguish foreground writes application guest waiting background delayed writes scheduling effective similarly semantic knowledge disk space allocation vmm compacting free space due deleted files guest operating systems interesting aspect semantic inference vmm unlike storage system underneath scsi passively observe requests vmm active entity vmm change memory contents guest operating system observes actively control operating system application semantic information vmm robust intrusion detection tracing virtual machines additional protection boundary isolated operating system provide attractive vantage point perform tracing analysis detect intrusions cases operating system compromised today intrusion analysis vmm fundamentally limited due lack semantic knowledge semantic information vmm-level tools monitor key files file system registry contents circumvented intruder integrating logic implementation checkers logic framework modeling file systems presents interesting avenue future research interesting explore framework augment existing techniques verifying file system implementations logic construct set precise constraints update traffic file system conform layer software underneath file system verify online constraints violated ambitious goal explore start logic specification file system interaction disk automatically arrive implementation preserves ordering guarantees mentioned specification general problem logic implementation hard tractable logic sufficiently domain specific recent research demonstrated correctness bugs widely file systems automating implementation file system consistency management alleviate problem semantic disk functionality investigated wide range case studies thesis demonstrating utility semantic knowledge storage system plenty opportunities left unexplored d-graid built mechanisms required policy space remains largely uninvestigated design points d-graid interesting policy decisions arise deciding perform access-driven diffusion blocks diffuse difficult vmmigration problem design depending introduces aspects precopy approach update frequency pages popularity memory blocks iteratively copied deciding source degree destination metadata host replication stopping problem execution deciding degree virtual replication machine migrated viewed design highlights optimization problem utility replicating block depends importance high directory tree block arrive optimal level replication type block fixed space budget performance tolerance semantic knowledge applied kinds functionality enhancements storage systems power consumption critical issue enterprise systems storage system semantic knowledge collocate active data smaller set disks similar d-graid spin remaining disks conserving power semantic information storage system predict blocks active future pro-actively power disks anticipation directory block 
read disk system anticipate blocks belonging inodes pointed directory files entirety accessed perform needful functionality semantic disks consistent block-level snapshotting incremental snapshotting popular feature file server appliances today block-level storage systems provide functionality information on-disk contents represents consistent file system state semantic inference storage systems acquire information journaling case study chapter detects consistent state underneath synchronous file systems extending underneath general purpose asynchronous file systems interesting making semantic disks semantic work thesis pertained inferring higher level knowledge file system natural extension approach infer higher-level information case d-graid collocation policy explored straight-forward considered files directories storage system aggressive semantic inference looked file names file content perform interesting collocation identifies file html extension contents place embedded links fault boundary similarly looked file called makefile infer entire source code base related collocate files database-specific storage work chapter instance general approach direction work considered judiciously extending stack result dependencies fragile assumptions similar work drew line static format information change challenge identifying stable dependencies worth creating summary semantically-smart disk systems enable classes functionality improvements storage exploiting information higher layers system pragmatism driving goals sds approach deployment innovative functionality successful intrusive existing infrastructure requiring storage interface cases layers semantic disks achieve general approach implicit evolution ossified interfaces realm file systems storage lead thinking system evolution bibliography acharya uysal saltz active disks proceedings international conference architectural support programming languages operating systems asplos viii san jose california october agrawal kiernan srikant hippocratic databases proceedings international conference large databases vldb hong kong china august alvarez burkhard cristian tolerating multiple failures raid architectures optimal storage uniform declustering proceedings annual international symposium computer architecture isca denver colorado amiri petrou ganger gibson dynamic function placement data-intensive cluster computing proceedings usenix annual technical conference usenix san diego california june anderson chase vahdat interposed request routing scalable network storage acm transactions computer systems arpaci-dusseau arpaci-dusseau information control gray-box systems proceedings acm symposium operating systems principles sosp banff canada october arpaci-dusseau culler mainwaring scheduling implicit information distributed systems proceedings joint international conference measurement modeling computer systems sigmetrics performance madison wisconsin june attie lynch dynamic input output automata formal model dynamic systems acm symposium principles distributed computing pages bairavasundaram sivathanu arpaci-dusseau arpacidusseau x-ray non-invasive exclusive caching mechanism raids proceedings annual international symposium computer architecture isca munich germany june bauer priyantha secure data deletion linux file systems proceedings usenix security symposium august jfs overview ibm developerworks library l-jfs html bitton gray disk shadowing proceedings international conference large data bases vldb pages los angeles entire create fails scheme enables common placing private directory union public allowing creation private directory convention kernel device file systems bound dev directory bootstrap space building process notation mits direct access devices existing space root directory tree served device driver accessed syntax unique character typically letter identifying type device simple device drivers serve single level directory files serial port represented data control file bind dev dev eia bootes bootes feb eia bootes bootes feb eia ctl bootes bootes feb eia bootes bootes feb eia ctl bind program encapsulation bind system call flag positions directory end union data files eia eia read written communicate serial line special operations files control devices commands written files eia ctl eia ctl california control august device boehm writing weiser text garbage string collection dev uncooperative eia ctl environment sets software speed practice line baud experience compare september unix boral ioctl system call dewitt plan database devices machines controlled idea textual time messages passed free byte order international problems workshop clear database semantics reading machines brown writing common yamaguchi oracle configure hardware debug assisted devices resilient shell data scripts oracle universal technical bulletin protocol connects note plan burkhard components menon form disk distributed array system storage system reliability inventing unique proceedings protocol service international symposium rlogin ftp fault-tolerant tftp computing windows ftcspages plan toulouse implements france services terms june operations file burnett objects bent arpaci-dusseau single well-documented arpaci-dusseau protocol exploiting exchange gray-box information knowledge computers buffer-cache unlike contents nfs treats proceedings files usenix sequence annual bytes technical conference blocks usenix unlike monterey nfs california stateful june clients perform burrows remote proce abadi dure calls needham establish logic pointers objects authentication proceedings acm symposium operating systems principles sosp litchfield park arizona december chao english jacobson stepanov wilkes mime high performance parallel storage device technical report hpl-csp- labs chapin rosenblum devine lahiri teodosiu gupta hive fault containment shared-memory multiprocessors proceedings acm symposium operating systems principles sosp copper mountain resort colorado december chase anderson thakar vahdat doyle managing energy server resources hosting centers proceedings acm symposium operating systems principles sosp bolton landing lake george york october chen lee gibson katz patterson raid high-performance reliable secondary storage acm computing surveys june jonge kaashoek hsieh logical disk approach improving file systems proceedings acm symposium operating systems principles sosp asheville north carolina december denehy arpaci-dusseau arpaci-dusseau bridging information gap storage protocol stacks proceedings usenix annual technical conference usenix monterey california june denehy bent popovici arpaci-dusseau arpaci-dusseau deconstructing storage arrays proceedings international conference architectural support programming languages operating systems asplos pages boston massachusetts october dijkstra structure multiprogramming system communications acm dowse malone recent filesystem optimisations freebsd proceedings usenix annual technical conference freenix track monterey california june dragovic fraser hand harris pratt warfield barham neugebauer xen art virtualization proceedings acm symposium operating systems principles sosp bolton landing lake george york october emc corporation symmetrix enterprise information storage systems http emc english stepanov loge self-organizing disk controller proceedings usenix winter technical conference usenix winter san francisco california january ganger blurring line oses storage devices scs cmu-cs- dec ganger mckusick soules patt soft updates solution metadata update problem file systems acm transactions computer systems ganger patt metadata update performance file systems proceedings symposium operating systems design implementation osdi monterey california november ganger worthington hou patt disk subsystem load balancing disk striping conventional data placement hicss ghemawat gobioff leung google file system proceedings acm symposium operating systems principles sosp pages bolton landing lake george york october gibson nagle amiri butler chang gobioff hardin riedel rochberg zelenka cost-effective highbandwidth storage architecture proceedings international conference architectural support programming languages operating systems asplos viii san jose california october gibson nagle amiri chang gobioff riedel rochberg zelenka filesystems network-attached secure disks technical report cmu-cs- carnegie mellon golding bosch staelin sullivan wilkes idleness sloth proceedings usenix winter technical conference usenix winter pages gray computers stop international conference reliability distributed databases june gray horst walker parity striping disc arrays lowcost reliable storage acceptable throughput proceedings international conference large data bases vldb pages brisbane australia august gribble robustness complex systems eighth workshop hot topics operating systems hotos viii schloss elmau germany grochowski emerging trends data storage magnetic hard disk drives datatech september gunawi agrawal arpaci-dusseau arpaci-dusseau schindler deconstructing commodity storage clusters proceedings annual international symposium computer architecture isca madison wisconsin important remote distinction file server migration time pointers -vstotal called file identifiers fids migration time operations files design supply fid memory identify moved object faster remote file sprite system zap protocol migration process defines memory messages state providing means written disk authenticate users checkpoint navi gate fids pulled file system vmmigration hierarchy pre-copy copy mechanism fids perform combination change file push attributes pull create delete discussion files felt complete total specification migration time section push system programmer 
wold manual man procedure gain access hierar chy supplied server file server connection established pipe network connection initial session message performs bilateral authentication client server attach message connects fid suggested client root server file tree attach message includes identity user performing attach henceforth fids derived root fid permissions user multiple users share connection form attach establish identity walk message moves fid single level file system hierarchy clone message takes established fid produces copy points file original purpose enable walking file directory losing fid directory open message locks fid specific file hierarchy checks access permissions prepares fid read write messages arbitrary offsets file maximum size transferred defined protocol clunk message client fid remove message behaves clunk file fid removed resources server deallocated forms rpc messages pipe network connection pro cedural interface kernel kernel device drivers directly addressable pass messages communicate transac tion implemented direct procedure call fid kernel maintains local representation data structure called channel operations files performed kernel involve channel connected fid simplest user process file descriptors indexes array channels table kernel list entry points messages device system call read user translates pro cedure calls table indexed type character stored channel procread eiaread call takes channel argument special kernel driver called mount driver translates procedure calls messages converts local procedure calls remote effect special driver local proxy files served remote file server channel pointer local call translated fid transmitted message mount driver sole rpc mechanism employed system seman tics supplied files operations performed create par ticular service cpu command mount driver demultiplexes protocol messages clients sharing communication channel file server outgoing rpc message mount driver allocates buffer labeled small unique integer called tag reply rpc labeled tag mount driver match reply request kernel representation space called mount table stores list bindings channels entry mount table pair channels channel channel time walk succeeds moving channel location space mount table consulted channel matches channel cloned substituted original union directories implemented converting channel list channels successful walk union directory returns channel forms head list channels representing component directory union walk fails find file directory union list component cloned walk directory file plan uniquely identified set integers type channel index function call table server device number distinguishing server type decided locally driver qid formed -bit numbers called path version path unique file number assigned device driver file server file created version number updated file modified section maintain cache coherency clients servers type device number analogous unix major minor device num bers qid analogous i-number device type connect channel device driver qid identifies file device file recovered walk type device qid path entry mount table file substitution mount table made space implemented file caching protocol explicit support caching files client large memory central file server acts shared cache clients reduces total amount memory needed machines network nonetheless sound reasons cache files client slow connection file server version field qid changed file modified makes weakly coherent forms caching important client caching text data segments executable files process execs pro gram file re-opened qid version compared cache match local copy method build local cach ing june file server gutmann secure user-level deletion server interposes data magnetic connection solid-state remote memory server proceedings monitors sixth traffic usenix copying data security symposium local disk july sees hadzilacos read theory data reliability answers database directly systems writes journal passed immediately cache acm write-through hagmann reimplementing central copy cedar file date system logging transparent group processes commit terminal proceedings requires acm change symposium works operating systems home principles machines connected sosp austin serial texas lines similar november method applied hellerstein build haas general client cache wang online unused local aggregation memory proceedings acm sigmod plan international networks conference communication devices management network data interfaces sigmod kernel-resident file tucson systems arizona analogous eia device holland gibson earlier call setup siewiorek fast shutdown on-line achieved failure recovery writing text redundant strings disk arrays control file proceedings international device symposium information fault-tolerant received computing reading ftcstoulouse writing france data file june structure hsiao semantics devices dewitt chained common declustering net works availability strategy file multiprocessor substitution database machines procedure makes international call data tcp engineering ethernet conference urp datakit hsieh fra engler illustrates back structure reverse-engineering 
instruction encodings proceedings usenix annual technical conference usenix boston massachusetts june hughes personal communication hughes coughlin secure erase disk drive data idema insight magazine ibm serveraid recovering multiple disk failures http ibm qtechinfo migrhtml felten wang singh archipelago island-based file system highly scalable internet services usenix windows symposium august katcher postmark file system benchmark netapp troctober keeton wilkes automating data dependability proceedings acm-sigops european workshop pages saint-emilion france september kistler satyanarayanan disconnected operation coda file system acm transactions computer systems february kuo model verification data manager based aries acm transactions database systems lamport temporal logic actions acm transactions programming language systems lee thekkath petal distributed virtual disks proceedings international conference architectural support programming languages operating systems asplos vii cambridge massachusetts october chen srivivasan zhou c-miner mining block correlations storage systems proceedings usenix symposium file storage technologies fast pages san francisco california april lumb schindler ganger freeblock scheduling disk firmware proceedings usenix symposium file storage technologies fast monterey california january carey shoring persistent applications proceedings acm sigmod international conference management data sigmod minneapolis minnesota maccormick murphy najork thekkath zhou boxwood abstractions foundation storage infrastructure proceedings symposium operating systems design implementation osdi pages san francisco california december mckusick joy leffler fabry fast file system unix acm transactions computer systems aug mckusick joy leffler fabry fsck unix file system check program unix system manager manual bsd virtual vaxversion april menon mattson comparison sparing alternatives disk arrays proceedings annual international symposium computer architecture isca gold coast australia mogul update policy proceedings usenix summer technical conference usenix summer boston massachusetts june mohan haderle lindsay pirahesh schwarz aries transaction recovery method supporting fine-granularity locking partial rollbacks write-ahead logging acm transactions database systems march musuvathi park chou engler dill cmc pragmatic approach model checking real code proceedings symposium operating systems design implementation osdi boston massachusetts december nugent arpaci-dusseau arpaci-dusseau controlling place file system gray-box techniques proceedings usenix annual technical conference usenix pages san antonio texas june oracle self-managing database automatic performance diagnosis https oracleworld pub-lished doc orji solworth doubly distorted mirrors proceedings acm sigmod international conference management data sigmod washington ousterhout aren operating systems faster fast hardware proceedings usenix summer technical conference usenix summer june padhye floyd inferring tcp behavior proceedings sigcomm san diego california august park balasubramanian providing fault tolerance parallel secondary storage systems technical report cs-tr- princeton november patterson gibson katz case redundant arrays inexpensive disks raid proceedings acm sigmod conference management data sigmod chicago illinois june patterson availability maintainability performance focus century key note fast january pennington strunk griffin soules goodson ganger storage-based intrusion detection comparing vmmigration sprite zap difficult vmmigration moving entire machine state processes fundamental question comparing works approaches pre-copy overcome fact state moved migration zap performs migration checkpointing memory areas allocated processes pod sounds difficult port maintain file state zap pod virtualized file system private file system namespace leveraging file systems nfs store file data approach takes advantage distributed file systems reduce file state moved migration requiring global file system host machines zap provide mechanism move file data checkpoint require file system host machines file namespace virtualization local system mount global filesystem arbitrary local mount point network file system zap case sprite migration mechanism required sprite network file service sprite adding residual dependencies support access local devices migration zap supports virtualization device namespace support devices pseudo terminals zap vmmigration worried access physical devices cd-rom drives migration vmmigration migration performed level virtual machine xen mechanism checkpoint restart migrate state including process kernel machine state xen heavy lifting respect migration vmmigration authors focused primarily performance network file state migration class talked nuances migration performance watching storage activity suspicious behavior proceedings usenix security symposium popek walker chow edwards kline rudisin thiel locus network transparent high reliability distributed system proceedings acm symposium operating systems principles sosp pacific grove california december postgres postgresql database http postgresql prabhakaran arpaci-dusseau arpaci-dusseau analysis evolution journaling file systems proceedings usenix annual technical conference usenix anaheim california april r-undelete r-undelete file recovery software http undelete reddy banerjee gracefully degradable disk arrays proceedings international symposium fault-tolerant computing ftcspages montreal canada june regehr inferring scheduling behavior hourglass proceedings usenix annual technical conference freenix track monterey california june reiser reiserfs namesys restorer restorer data recovery software http bitmart net riedel gibson faloutsos active storage large-scale data mining proceedings international conference large databases vldb york york august riedel kallahalla swaminathan framework evaluating storage system security proceedings usenix symposium file storage technologies fast monterey california january roselli lorch anderson comparison file system workloads proceedings usenix annual technical conference usenix san diego california june rosenblum ousterhout design implementation logstructured file system acm transactions computer systems february rowstron druschel storage management caching past large-scale persistent peer-to-peer storage utility proceedings acm symposium operating systems principles sosp banff canada october ruemmler wilkes disk shuffling technical report hpl- hewlett packard laboratories saito karamanolis karlsson mahalingam taming aggressive replication pangaea wide-area file system proceedings symposium operating systems design implementation osdi boston massachusetts december savage wilkes afraid frequently redundant array independent disks proceedings usenix annual technical conference usenix pages san diego california january schindler ganger automated disk drive characterization cmu-cs- schindler griffin lumb ganger track-aligned extents matching access patterns disk drive characteristics proceedings usenix symposium file storage technologies fast monterey california january selinger winslett pat selinger speaks sigmod record december seshadri paskin predator or-dbms enhanced data types proceedings acm sigmod international conference management data sigmod tucson arizona sivathanu arpaci-dusseau arpaci-dusseau evolving rpc active storage proceedings international conference architectural support programming languages operating systems asplos pages san jose california october sivathanu bairavasundaram arpaci-dusseau arpacidusseau life death block level proceedings symposium operating systems design implementation osdi pages san francisco california december sivathanu prabhakaran arpaci-dusseau arpacidusseau improving storage system availability d-graid proceedings usenix symposium file storage technologies fast pages san francisco california april sivathanu prabhakaran popovici denehy arpacidusseau arpaci-dusseau semantically-smart disk systems proceedings usenix symposium file storage technologies fast san francisco california april slotnick logic track devices volume pages academic press sourceforge srm secure file deletion posix systems http srm sourceforge net sourceforge wipe secure file deletion http wipe sourceforge net sourceforge linux ntfs project http linux-ntfs net strunk goodson scheinholtz soules ganger self-securing storage protecting data compromised systems proceedings symposium operating systems design implementation osdi san diego california october swartz brave toaster meets usenet lisa pages chicago illinois october thekkath mann lee frangipani scalable distributed file system proceedings acm symposium operating systems principles sosp pages saint-malo france october tweedie future directions ext filesystem proceedings usenix annual technical conference freenix track monterey california june tweedie ext journaling file system http olstrans sourceforge net release ols -ext ols -ext html july vmware vmware workstation http vmware products waldspurger memory resource management vmware esx server proceedings symposium operating systems design implementation osdi boston massachusetts december wang anderson patterson virtual log-based file systems programmable disk proceedings symposium operating systems design implementation osdi orleans louisiana february wilhelm-olsen desai melvin federwisch data protection strategies network appliance storage systems netapp technical report wilkes golding staelin sullivan autoraid hierarchical storage system acm transactions computer systems february wolf placement optimization problem practical solution disk file assignment problem proceedings acm sigmetrics conference measurement modeling computer 
systems sigmetrics pages berkeley california wong wilkes cache making storage exclusive proceedings usenix annual technical conference usenix monterey california june worthington ganger patt wilkes on-line extraction scsi disk drive parameters proceedings acm sigmetrics conference measurement modeling computer systems sigmetrics ottawa canada yang twohey engler musuvathi model checking find file system errors proceedings symposium operating systems design implementation osdi san francisco california december gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings symposium operating systems design implementation osdi san diego california october manolios lamport model checking tla specifications lecture notes computer science zhou philbin multi-queue replacement algorithm level buffer caches proceedings usenix annual technical conference usenix boston massachusetts june 
discussed idea migration downtime -vstotal migration time migration downtime amount time service executing process migrated total migration time amount time takes complete migration process source destination machines ideally remove residual dependencies source machine zap migration downtime total migration time equal vmmigration due pre-copy approach migration downtime minimized smaller total migration time previously mentioned vmmigration approach move lot memory state zap sprite authors explored memory migration techniques push source running pages pushed destination pages modified process pushed ensure consistency stop-and-copy source stopped pages copied destination execution resumes destination simple approach zap complexity minimized cost increased migration downtime similar sprite wrote entire memory image disk source read destination pull execution started destination page accessed copied fault occurs page pulled source decreases migration downtime cost total migration time authors developed technique called precopy consists bounded iterative push phase small stop-n-copy pre-copy technique works memory pages copied destination host faster dirtied migrating virtual machine happening vmmigration authors propose process stunning technique limits process write faults moved wait queue found process stunning interesting idea essentially tuneable knob negatively impact performance running service order improve migration time network resources vmmigration authors observed cluster environment interfaces source destination typically exist single switched lan solution generate unsolicited arp reply migrated host advertise address location solution limits deployment data-center type environments suitable departmental organizational-wide deployment hand remove residual dependency originating machine storage vmmigration relies networkattached storage sprite zap transparency fundamental transparency issue zap vmmigration advance processes candidates migration processes started pod virtual machine case sprite discussed internal -vsexternal transparency internal viewpoint processes migrated external viewpoint processes moved sprite external transparency zap run bin source machine consistent view migration felt good external transparency cost increased residual dependencies network operation transparency sprite network operations transparent cost residual dependency entire process lifetime zap residual dependency finite number minutes major transparency issue client server side connection pods order virtual network address translation vnat mechanism work vmmigration places restrictions client side requires server side migrate machines subnet arp broadcasts work previously mentioned implementation vmmigration implementation integrated xen virtual machine monitor xen special management virtual machine administration control machine design lends managed migration approaches zap virtualization largely implemented providing mechanism intercepting system calls translate pod operating system namespaces zap migration mechanism based crak linux kernel module written earlier zap authors linux netfilter network tcp device net tcp bootes bootes feb bootes bootes feb bootes bootes feb clone net tcp rob bootes feb ctl rob bootes feb data rob bootes feb listen bootes bootes feb local bootes bootes feb remote bootes bootes feb status top directory net tcp clone file directory connec tion numbered connection directory corresponds tcp connection opening clone reserves unused connection returns control file reading control file returns textual connection number user process construct full newly allocated connection directory local remote status files diagnostic remote address tcp address port number remote side call initiated virtualization felt zap implementation writing connect message network-specific address argument open telnet session port remote machine address string connect write control file blocks connection established destination unreachable write returns error connection established telnet application reads writes data file talk remote telnet dae mon end telnet daemon start writing announce control file willingness receive calls port daemon called listener plan uniform structure network devices hide details addressing communication dissimilar networks datakit textual hierar chical addresses unlike -bit addresses application control file network represents make application addressing network plan hides details connection server called file system mounted place supplies single control file application discover connect host application writes symbolic address service connection wishes make reads back clone file open address present multiple networks machines presents list networks addresses sequence heuristics decide order instance presents highest-bandwidth choice single library function called dial talks establish connection application dial recompilation adapt networks interface hides details uniform structure networks plan makes import command needed construct gateways kernel structure networks kernel plumbing build plan communications channels called streams rit presotto stream bidirectional channel connecting physical pseudo-device user process user process inserts removes data end stream kernel process acting behalf device operates end stream comprises linear list processing modules module upstream process downstream device put routine calling put routine module end stream inserts data stream module calls succeeding send data stream unix streams rit plan streams dynamically configured protocol protocol run reliable transport protocol delimited mes sages mechanism recover transmission errors system assumes read communication channel return single message parse data stream discover message boundaries pipes net work protocols properties standard protocols tcp delimit messages udp rfc provide reliable in-order deliv ery ordering events distributed systems wisconsin-madison computer sciences department distributed systems andrea arpaci-dusseau papers time clocks ordering events distributed system distributed snapshots determining global states distributed systems motivation develop distributed algorithm participants conclusion helps inputs order questions event precedes distributed system impossible doesn matter event occurs machine event occurs machine communication event event happen terminology distributed system collection distinct processes spatially separated communicate exchanging messages differ previous definitions process sequence events instructions sending messages receiving messages events process total ordering partial ordering happened rules events process sending message process receiving message causally affect concurrent ordering causally affect space-time diagram time relationship logical clocks abstract view logical clock assigning number event express ordering relation logical clock physical time clock process function assigns number event clock condition events converse condition hold concurrent events logical time implementation logical clocks process increments successive events event sending message process timestamp receiving process sets greater equal presents greater logical clocks logical clock values assume initial total ordering logical clocks obtain total ordering processes events process ids break ties partial ordering unique total ordering concurrent operations order depends implementation depends tie designed breaking rules protocol distributed called state machines internet link mutual transmit exclusion messages process runs distributed connection-based algorithm protocol relies total reliable ordering transmission requests agreed sequenced messages participants machines ensure process events single inputs outstanding order request flow make control decisions tcp idea send timestamped adaptive request timeouts scales processes acknowledge handle request retransmission times total order match network speed request received protocol request perform participants problems internet physical clocks local motivation ethernets 
blind retransmission avoid adding congestion busy networks full details paper prwi plan implementation smaller faster tcp main internet transport protocol overview authentication authentication establishes identity user accessing resource user requesting resource called client user granting access resource called server auspices attach message user client authentication exchange server servers act behalf user normal client administrative entity authentication defined users machines plan user des nbs authentication key user iden tity verified ability encrypt decrypt special messages called challenges knowledge user key access user resources plan authen tication protocols transmit message cleartext key authentication bilateral end authentication exchange side convinced identity machine begins exchange des key memory case cpu file servers key user domain server read permanent storage non-volatile ram case terminals key derived password typed user boot time special machine authentication server maintains database keys users administrative domain participates authentication protocols authentication protocol exchanging challenges party contacts authentication server create permission-granting tickets encrypted party secret key conversation key party decrypts ticket conversation key encrypt party challenge structure kerberos mbss avoids reliance syn chronized clocks unlike kerberos plan authentication supports speaks relation labw enables user authority cpu server runs processes behalf clients plan authentication structure builds secure services depending firewalls firewalls require special code service penetrating wall plan approach permits authentication single place ser vices cpu command works securely internet authenticating external connections regular plan authentication protocol suitable text-based services telnet ftp cases plan users authenticate hand-held des cal culators called authenticators authenticator holds key user distinct user normal authentication key user logs authenticator digit pin correct pin enables authenticator challenge response exchange server correct challenge response exchange valid keys network procedure susceptible replay attacks compatible protocols telnet ftp special users plan super-user server responsible maintaining secu rity permitting access console protected password file servers unique administrative user called adm special priv ileges apply commands typed server physical console privi leges concern day-to-day maintenance server adding users configuring disks networks privileges include ability modify examine change permissions files file read-protected user user grant access cpu servers equivalent user administrative access resources server control files user processes permission kill rogue processes extend server hand means key held protected non-volatile ram identity administrative user proven authentication server cpu server authenticate remote users access server cpu server acting proxy behalf finally special user called password allowed con nect claim restricted permissions allowed examine dump files read world-readable files idea analogous anonymous user ftp services plan guest ftp servers confined special restricted space disconnects guest users system programs contents bin makes make local files guests binding explicitly space restricted space secure usual technique export ing hoc directory tree result kind cage untrusted users cpu command proxied authentication call made cpu server user peter intent peter wishes run processes authority implement property cpu server call received listener forks process handle call process user avoid giving permis sions compromised performs authentication protocol verify calling user peter prove peter machine trustwor thy finally reattaches relevant file servers authentication protocol identify peter case cpu server client file server forms client portion authentication exchange behalf peter authen tication server give process tickets accomplish cpu server administrative user allowed speak peter speaks relation labw table authentication server simplify management users computing authentication domains mappings user names domains user rtm domain person user rtmorris file permissions advantages constructing services file systems solutions ownership permission problems fall naturally unix file direc tory observe anomalous behavior communication channels exist processes physical clock meaning physical world synchronize independent physical clocks running slightly rates skew implementation idea send timestamp message receiver update clock timestamp minimal network delay clock increase lots work area distributed snapshots goal record global state distributed system state process state communication channel observe system properties computation terminated system deadlocked number tokens amount money complication distributed system shared state shared clock record global state simultaneously distributed snapshot record local state times combine meaningful picture obtain cut logical time remain consistent preserving logical ordering ordering physical time system model distributed system finite set processes channels graph processes set states initial state set events channels fifo error-free infinite buffers arbitrary finite delay distributed snapshot algorithm goal record local state process adjoining channels produces meaningful global system state idea send marker channels show messages snapshot receiver records messages channel marker initial process decides initiate snapshot performed periodically marker rules marker-sending rule send marker channel recording state sending messages marking-receiving rule channel recorded state record state record state empty recorded state record state msg sequence arrived recorded state termination state recorded processes channels algorithm collect assemble information banking stable property banking state empty channels banking state empty channels state empty channels total money banking banking actual state banking simultaneously properties recorded global state recorded global state occurred bothers doesn exist permutation actual sequence events reachable sinit sfinal reachable stable properties hold permute sequence events goal snapshot correspond single logical cut slide events snapshots logical time events processes switch order specifically postrecorded events prerecorded events prerecorded events occurred state recorded banking swap sending receiving logically consistent observe difference pre post 
separate read write execute search permissions file owner file group idea group unusual user potentially group group user list users group conven tions make distinction people user names group members groups long lists attached names sys group traditionally system programmers system files accessible group sys lines user database stored server pjw pjw sys pjw ken philw presotto establishes user pjw regular user establishes user sys group lists users members group empty colon-separated field space user named group leader group leader user special permissions group freedom change group missions files group leader member group considered equal leader pjw add mem bers group sys members equal partners group regular files owned user creates group inherited directory holding file device files treated specially kernel arrange ownership permissions file user accessing file good generality offers process files owned read-protected owner process owner access memory process author program debug broken image standard chmod command applied process files job unusual application file permissions dump file system served file server original data represented user database files dump identical protection files regular file system file owned pjw read-protected dump file system owned pjw read-protected dump file sys tem immutable file changed read-protected forever drawbacks file readable read-protected readable user names hard re-use performance simple measure performance plan kernel compared time simple operations plan sgi irix release running sgi challenge mhz mips -megabyte secondary cache test program written alef compiled compiler run iden tical hardware variables operating system libraries program tests time context switch rendezvous plan blockproc irix trivial system call rfork nap lightweight fork rfork rfproc sproc sfds saddr measures time send byte pipe process throughput pipe processes results table test plan irix context switch system call light fork pipe latency pipe bandwidth table performance comparison plan times spectacular show kernel competitive commercial systems discussion plan conventional kernel system novelty lies pieces kernel interact building plan considered aspects system solving problems solution fit times solution spanned components problem heteroge neous instruction architectures addressed compilers code characters portable object code environment cputype objtype space binding bin components issues solved single place centralizes naming access authentication core system fair plan kernel primarily multiplexer plan focus files naming central expressiveness distributed computing things named profound influence system nee combination local spaces global conventions interconnect networked resources avoids difficulty maintaining global uniform space naming file makes system easy understand nov ices dump file system trivial familiar hier archical file systems deeper level building resources single uni form interface makes interoperability easy resource exports interface combine transparently part system build unusual applica tions details hidden sound object-oriented distinc tions defines fixed set methods extensible protocol important files well-defined well-understood prepackaged familiar methods access protection naming networking 
objects generality attributes defined reducing object file plan technology free nonetheless push idea file-based computing con verting resource system file system kind metaphor meta phors abused good restraint proc view process representation run processes usual fork exec calls bin date proc clone mem problem examples require server things control ability assign meaning command imply meaning fall naturally structure answering requests generates related plan put machine network names file space network interfaces provide model naming open create read write files offer suitable place encode details call setup arbitrary network network interface file-like tightly defined struc ture differently time elements implementation unsatisfactory streams implement network interfaces kernel pro tocols connected dynamically attach tty driver tcp urp connections plan makes configurability exploited research unix system streams invented replacing streams static queues simplify code make faster main plan kernel portable machines file server implemented separately caused problems drivers writ ten bugs fixed weaker portability file system code solution easy file server kernel maintained variant regu lar operating system user processes special compiled-in kernel processes implement file service improvement file system change internal structure worm jukebox reliable piece hardware holds metadata file system present order serve files system restructured worm backup device file system proper residing magnetic disks require change external interface plan per-process spaces mechanism give description process space process direct inheritance cpu command general reproduce terminal space re-interpret user login profile make substitutions things binary directory load misses local modifications made running cpu capture terminal space transmit description remote process problems plan works matured system supports research subject research experimental work includes developing interfaces faster networks file caching client kernel encapsulating exporting spaces ability re-establish client state server crash attention focusing system build distributed applications reason plan success daily work research tool active forces address shortcomings arise adapt system solve problems process plan comfort productive programming environment vehicle systems research man plan programmer manual volume bell laboratories murray hill ansic american national standard information systems programming language amer ican national standards institute york duff tom duff shell plan unix systems proc summer ukuug conf london july reprinted form volume fra fraser datakit modular network synchronous asynchronous traffic proc int conf commun june boston fssutf file system safe ucs transformation format fss utf open preliminary specification iso designation iso iec jtc dated iso iso iec dis information technology universal multiple octet coded character set ucs part architecture basic multilingual plane kill killian processes files usenix summer conf proc june salt lake city labw butler lampson mart abadi michael burrows edward wobber authentication distributed systems theory practice proc acm symp sys princ asilomar mbss miller neumann schiller saltzer kerberos authentication authorization system massachusetts institute technology nbs national bureau standards federal information processing standard national technical information service springfield nee needham names indistributed systems mullender addison wesley nehe needham herbert cambridge distributed computing system addison-wesley london neu clifford neuman prospero file system usenix file systems workshop proc ann arbor ocdnw john ousterhout andrew cherenson fred douglis mike nelson brent welch sprite network operating system ieee computer feb pike rob pike text editor sam software practice experience nov reprinted volume pike rob pike plan window system usenix summer conf proc nashville june reprinted volume pike rob pike ken thompson world usenix winter conf proc san diego reprinted volume pike rob pike acme user interface programmers usenix proc winter conf san francisco pike rob pike plan compiler plan programmer manual volume bell laboratories murray hill posix information technology portable operating system interface posix part system application program interface api language ieee york ppttw rob pike dave presotto ken thompson howard trickey phil winterbottom spaces plan sys rev vol april reprinted volume presotto dave presotto multiprocessor streams plan ukuug summer conf proc july prwi dave presotto phil winterbottom organization networks plan usenix proc winter conf san diego reprinted volume prwi dave presotto phil winterbottom protocol plan programmer manual volume bell laboratories murray hill rfc postel rfc user datagram protocol darpa internet program protocol specification august rfc rfc transmission control protocol darpa internet program protocol specification september rao herman chung-hwa rao jade file system dissertation dept comp sci arizona rit ritchie stream input-output system bell laboratories technical journal october complex tric fragile howard trickey vmmigration ape ansi posix environment plan programmer manual volume bell laboratories murray hill unicode unicode standard worldwide character encoding version volume unicode consortium addison wesley york unix unix time sharing system programmer manual research version eighth edition volume bell laboratories murray hill welc brent welch comparison distributed file system architectures vnode sprite plan computing systems spring wint phil winterbottom alef language manual plan programmer manual volume bell laboratories murray hill 
implementation largely xen shielded vmmigration clean level indirection deal kernel state crak sensitive kernel similar sprite migration mechanism constantly failed regression testing kernel changed zap authors made zap publically source binary form embarrassingly fragile constantly referred implementation proof-of-concept makes speculate actual production-quality zap implementation complex difficult working correctly evaluation zap authors provide benchmarks simple synthetic applications apache web server vnc virtual server similar benchmarking sprite authors necessarily indicative real-world work load vmms proposed migration 
zap authors performed experiments vmware workstation linux measurements made execution times inside pod -vsinside vmware virtual machine times vmware checkpoint restart -vszap migrate zap authors state performance overhead process running inside zap pod smaller running vmware virtual machine loved performance numbers -vsxen addition vmware suspect performance overhead visor xen lower vmware made performance comparisons zap vmmigration easier migration zap authors found generally bound pod image size faster vmware checkpoint restart instances zap saves state unlike vmmigration sprite authors zap authors techniques overlap migration continued program execution policy usage sprite performed trivial apparently effective policies decide migrate process sprite authors work investigate policy instance sprite select candidate resource based long unused zap vmmigration purely mechanism address questions migrate conclusions class felt fundamental question arises papers entire virtual machine migrate process felt answer virtual machine advantages migration sandboxing security isolation running operating systems hardware discussion lead talking direct migration vmmigration -vsmigration checkpoint restart sprite zap direct migration performance advantage helps issues availability felt checkpoint restart advantageous enables additional functionality free fault tolerance migration storage processor queuing multiple snapshots time incremental debugging long running services applications 
caching sprite network file system peter couvares scribe distributed systems wisconsin madison friday february spring overview paper describes caching sprite network operating system compares filesystem semantics performance tradtional local filesystems distributed filesystems afs earlier sprite paper authors make number tradeoffs design goals consistency performance scalability reliability general sacrifice problem statement assumptions sprite operating system features transparent process migration rely distributed filesystem client read write single shared filesystem distributed filesystem caching overhead processing file operation machines client server communicating network result significant performance disadvantage relative local disk likewise distributed consistency applications care coordinate order guarantee don operate stale data goal sprite provide single uniform filesystem view workstation clients performance comparable traditional local-disk filesystems wished preserve traditional local-disk filesystem semantics key requirement clients view global filesystem local machine contrast afs fits earlier sprite paper goals remote operation local sprite designed assuming departmentscale network consisting primarily high-powered potentially multi-cpu diskless workstations supported central file service running sprite retrospect strange assumption multiple cpus common typical departmental workstation disk person commented obsession diskless workstations machines small amount memory compared today workloads equivalently small workload assumed typical computer science department userbase primarily interactive software development designed workload sequential file access typically whole-file access smaller files files open short time methodology methodology paper design implement filesystem sprite measure performance filesystems number simulated workloads actual traces inform design comparison performance systems weakness paper design implementation sprite caches individual file blocks files system memory cache disk block caching works memory file caching afs works disk cache system memory improves access speed limits size cache order cache memory interfering running applications allowed cache size dynamically grow shrink ultimately dynamic cache powerful ideas paper near-universal feature sprite implemented kernel level opposed user-level afs improves performance cost increased complexity decreased reliability sprite developers admitted earlier paper describing modifications area kernel problems places long writers sprite performs read caching clients order improve performance avoids disk network latency decreases load file server sprite performs write-back writethrough caching readers writers write-back caching acheives dramatic performance improvement increases liklihood data loss event crash data disk unlike afs nfs flush dirty files close sprite flushes dirty blocks periodically seconds alarming writers assume data safe disk close sprite sprite sequential write sharing forcing writers open calls block previous writer data flushed concurrent write sharing disabling caching altogether writer readers writers nfs block caching memory guarantees consistency flush close server stateless clients robust server crashes afs flushes close readers stale data updates open files readers guaranteed latest file open sprite disabling write caching readers writers ensures clients files time sprite lack callbacks simplifies implementation means server load opens claim faster kernel-based networking made expensive afs rejected approach conclusions sprite performance scalability reliability important consistency cache disabled instances afs concurrent write sharing hurting performance common scenarios mistake consistency ensure higher level scalability performance limited sprite adoption compared afs student pointed sprite paper encourages files communication sprite concurrent write sharing scenario disable caching perforn poorly 
petal distributed virtual disks garrett kolpin distributed systems wisconsin madison lecture date spring overview petal aims provide distributed fault tolerant scalable data storage system clients view petal system group virtual disks clients interface petal block device virtual disk offset file system petal data storage requires change current disk driver problem statement assumptions goal petal designers provide globally accessible storage system tolerate single component failures easy administer load balancing capabilities possibly geographically distributed petal abstract block device form virtual disk important distinction made petal distributed file systems nfs afs petal distributed storage system distributed file system interface provided petal block-level interface important implications petal blocklevel interface existing software works block level rewrite software work storage system instance existing file systems rely block-level interfaces work fine top petal block interface makes system simpler designers focus providing mechanisms policies policy decisions left designers file systems interface petal distributed nature petal assumptions disks accessed made current file systems assume sole access disk volume case petal multiple clients access disk volumes time design virtual physical translation problem case clients blocks form disk-identifier offset translated form server-identifier disk-identifier diskoffset translation occurs global map retrieving redirect server respond physical location request essentially enumeration servers virtual disk spread find actual physical location block directory global map small tables physical map information stored tables stored reliable remain consistent servers general rule thumb share information don share distributed nature petal assumptions made normal disk interfaces longer apply assumed blocks close virtual address necessarily physically close disk fact virtually close blocks physically machines space allocated differs substantially file systems regular file system local disk amount space required file system allocated file system creation time petal utilizes lazy allocation scheme space doesn allocated write operation initially allocate huge amount space space needed immediately allocated required amount physical storage storage demands reach limit physical disks disks simply added lazy allocation helps avoid fragmentation problems data written large contiguous blocks application absolutely needed knowledge required space exists order preallocate space simply write data number blocks needed snapshots goals snapshot system don recreate data require space store unnecessarily large amounts data essentially track storing deltas create snapshot space efficient snapshot process exist data offset order track recent data epoch number incremented petal differentiate snapshot data implication snapshot lowest level petal track epoch numbers server receive virtual disk offset client clear client knowledge epoch numbers necessitates server keeping track information reconfiguration petal goal scale system easily administrative overhead easy add disks machines add remove machines network process adding disk machine essential goal method place data disk couple approaches data existing disks forcefully disk order create balanced load disks approach rely system place newly written data disk necessarily assumption disks machine amount disk capacity disks varying capacity single machine machine local maps maintained order disks capacity deal case adding machine network handled step process global map created affects servers virtual disk directories changed global map referenced affects servers finally data redistributed newly added machine important striping disks writes global map reads reader check global map make reading stale data location downside readers check data network traffic reads result reconfigurations hours complete policy decision made minimize performance pains problem solved moving data incrementally virtual disks address range divided types fenced reads bound regions global maps reads bound fenced region redirected maps downside tie disk hot spot portions disk moved selected randomly hot spot data moved data access recovery basic method store data spread data node load balancing operation failure chained declustering primary secondary copy blocks reads serviced primary secondary copies write requests serviced primary server blocks locked writing ordering avoid deadlocks order concurrent writers obtain exclusive locks primary secondary copies blocks reach deadlock point release lock hold server determined primary server block data send write request secondary copy busy bits set writing recovery failure failure occur busy bits determine blocks written time failure occurred point decision made bring copies data consistent state recovery primary secondary copies block inconsistent half block written bring back reasonable state log absence log copies chosen consistent copy bring consistency evaluation table shows comparison petal local disk table apparent local disk faster blocks sizes petal performs small reads writes performance degrades larger block sizes writes slower reads writes neighboring servers clients don issue writes parallel primary sends data secondary results higher latency waiting log properly updated source added latency write operations figure glimpse scalability petal show petal scaling servers nice petal scales environment servers authors give indication performance reconfiguration recovery nice performance numbers reconfiguration time spent describing reconfiguration mechanisms design section conclusion conclude petal easy manage distributed storage system ability scaled easily simple block interface give clients view multiple virtual disks replication data distribution completely invisible user run specialized client software petal device driver 
serverless network file systems himani apte distributed systems wisconsin madison february spring overview paper presents design serverless network file system dynamically distribute control processing data storage caching set cooperating workstations aim improve performance scalability availablility peer-based distributed system approach adopted paper build monolithic system handles aspects storage caching data layout recovery contrast technique distributed file systems petal frangipani two-layer structure order reduce complexity building single monolithic system problem statement assumptions traditional distributed file systems centralized server file system services large number clients server performance bottleneck clients added limiting scalability system file server single point failure system reduces system reliability xfs avoids problems making distributed file system serverless xfs assumes availability fast switched network cooperating workstations distributed system lan assumed lower latency compared physical disk data access remote machine memory expected faster local disk generation multimedia parallel applications higher requirements expect performance file system satisfied current centralized file system designs necessarily true distributed file system multiple servers load-balancing workloads assumed good locality multiple clients workstations expected large memory cooperative caching good performance xfs assumes workstations mutually trust provide security primitives file system methodology xfs borrows ideas earlier systems design serverless file system raid-style disk striping single parity disk improve reliability availability parity disk reconstruct data case single disk failure xfs appears raidstriping parity blocks written single disk shown figure sub-optimal compared raidstriping distributed parity xfs incorporate raidstriping raid systems typically large performance overhead small writes xfs solves problem small writes writing data disk large log segments lfs stripe group reconfiguration remains expensive operartion xfs xfs incorporates log-structured file system lfs design writes client buffered memory written disk contiguous log segment good write performance avoids raid small write problem log simple means failure recovery simply roll forward checkpoint re-build important data structures problems lfs approach complicated reads latest copies inodes tracked imaps overhead log cleaners create free disk space log segments xfs draws heavily design zebra striped network file system advantages log private client parity computation local operation avoiding expensive operation tranferring log segments clients respect zebra features incorporated xfs parallel log cleaner stripe groups distributed manager design design xfs central theme leverages location independence load balancing increase locality metadata data distribution xfs distributes main tasks central server system components system data blocks distributed stripe groups storage servers metadata manager handles disk location cache consistency metadata cooperative caching clients replaces central server caches finally cleaner component responsible recycling disk space log number mapping tables manage highly distributed data metadata firstly globally replicated manager map locates manager file global replication manager map managers clients system performance bottleneck number clients increase frequent system load membership managers potentially limit scalability system manager maintains imap files order locate disk log address inode file inode structure xfs similar ffs order avoid clients inode number xfs divide inode space clients static partitioning imap collocated manager cached main memory checkpinted persistent storage stripe group map maps disk log address list storage servers size stripe group essentially number storage servers system advantages improving availability parity server stripe group cost reduction disk storage space bandwidth write performance optimal match network disk bandwidth single cleaner stripe group system operation figure detailed depiction steps involved reading data block xfs control flow read operation neatly shown elaborate mechanisms needed handle race conditions occuring due multiple requests data block clients due unpredictable network delays delegating client transfer cached data block client makes assumption clients network entire operation system optimizes path assumption access local memory faster access remote machine memory turn faster access disk strong data locality assumptions exist order reduce number network hops xfs block level caching file caching finer granularity caching semantics give performance avoid false sharing observed spritefs requires complex cache consistency metadata maintained xfs writer policy order collocate manager client dramatically reduces number network hops operations potential network hops data block read small file shown simulation read requests served local cache reducing avearge network hops expected higher large files additionally access indirect blocks implementation paper reports partial implementation xfs design distributed log cleaners automatic crash recovery dynamic reconfiguration components implemented empirically evaluate design terms usability evaluation test environment consists workstations connected high-speed switched myrinet network tcp throughput disk bandwidth performance xfs compared nfs nfs tests performed single disk server peak bandwidth unfair nfs server multiple disks processors add additional hardware nfs server fair comparison aggregate large write bandwidth expected performance system bandwidth single client study storage server scalability give insight decide stripe group size conclusions xfs design brings important considerations developing location independent serverless distributed system collocation data manager importance cooperative caching stripe groups scalability highly cited paper important research commercial systems drawn lessons learnt xfs including now-famous peer-to-peer cluster based systems 
mapreduce simplified data processing large clusters garrett kolpin distributed systems wisconsin madison spring overview mapreduce programming model designed hide complexities scheduling parallelization failure handling computation distribution cluster nodes programming model suited large sets data program written specifies map reduce function map function produces set intermediate key pairs reduce function input reduce function merges intermediate values intermediate key motivation assumptions motivation mapreduce advantage distributed file system gfs give model user user worry issues stated parallelization failure handling user supplies map reduce functions linked mapreduce library library hook mapreduce system paper notes implementations mapreduce specific implementation google consists cluster nodes node built commodity hardware cheap inexpensive ide disks makes failures common common run mapreduce operation data set order terabytes size important mapreduce implementation handle run data fit cluster memory design mapreduce started spawning master set workers job creation time parameters tune runtime behavior scheduling order loads entire cluster consideration entity paper global load balancing single master programs lesser local masters mapreduce job high level work-flow scheduler output phase complete needed input phase mapreduce user mapreduce operation set input files gfs locations found gfs chunkservers hosting input file master start mappers reducers master run map function chunkserver hosting input file impossible locate mapper switch reduce network transfer overhead mappers reducers intermediate files produced mapping function reside local disk mapper intermediate map files replicated failure completed mapper node trigger entire re-computation intermediate file worker failures detected master periodic heartbeat messages reducer reads intermediate files produced mappers rpc calls intermediate files local reducer reducer remote worker intermediate input file reducer fails computation complete computation restarted output files reducer workers written gfs files atomically renamed function sort atomic commit reducer failure occurs final output completed affect output gfs replication master master crashes user resubmit job crashed master abort computation master tracks location intermediate files workers working task master checkpoint runs state involved designers favored re-computation bit added complexity performance overhead taking checkpoints single worker worker failure happen worker failures gfs gfs intermediate files replication files slow general system simpler concerned replication persistence case simpler re-run computation providing intermediate files persistence gfs output files making multiple replicas slow process benefit persistent output data case outweighs cost recomputing entire mapreduce operation mapreduce designers importance local bus data keeping data local important order good performance gfs nodes mappers master attempts input data local mappers intermediate file output local disk final output stored gfs replica local disk probable reason stage mapreduce begin sooner replicas non-local stage competing execution current stage load balancing river programming environment data flexible runs producer consumer running slowly data requested idea mapreduce straggler reducer mapper co-scheduled separate node output fastest node wins faster mappers reducers grab jobs slower mappers increasing performance entire computation mapreduce sort sort mapreduce sort written roughly lines code impressive complexity parallelization computation mapreduce processes keys sorted order work mapreduce infrastructure sort byte records byte keys keys separated records order partition inputs workers modulo function top bits finding remainder divided number mappers case reducer needed data sorted end map phase reducer simply identity function sort mapreduce sort input data local disk initially methods assume initial distribution keys nodes nowsort reads keys sends key correct machine bucket remote mapreduce sends local disk shuffle phase begins final phase methods simply local sort data nowsort finishes writing output data local disk mapreduce hand gfs store output data replica produced means writes needed end decreases performance robust failure evaluation grep grep application mappers work reducer required writes output file results mappers takes grep application full minute start acceptable startup time consists sending data workers conflicts dealing gfs interesting performed fewer total workers faster startup time make fewer peak workers sort sort terabyte data required seconds complete current terasort record mapreduce achieving good performance interesting note mapreduce memory entire cluster keys memory designers designed system common case generally memory data written disk map reduce operations conclusion conclude mapreduce operation impressive thinks amount complexity hidden user evaluation appears mapreduce good performance scale thousand nodes emphasis simplicity system nice concrete examples applications written mapreduce framework simplicity code speak 
disconnected operation coda file system james kistler satyanarayanan school computer science carnegie mellon pittsburgh improve state affairs ideally abstract enjoy benefits shared data repository disconnected operation mode operation enables continue critical work repository client continue accessing critical data temporary inaccessible call mode operationfailures shared data repository important exclusive application disconnected operation disconnected operation represents temporary supporting portable computers paper show deviation normal operation client shared disconnected operation feasible efficient usable repository describing design implementation coda file system central idea work caching paper show disconnected operation file data widely performance system feasible efficient usable central exploited improve availability idea work caching data widely improve performance exploited enhance availability implemented disconnected introduction operation coda file system carnegie mellon user distributed system faced situations critical work impeded remote failure frustration acute initial experience coda confirms viability workstation powerful standalone disconnected operation successfully operated configured dependent remote disconnected periods lasting hours resources important instance dependence disconnection duration process reconnecting data distributed file system propagating typically takes minute local disk adequate placing data distributed file system simplifies periods disconnection trace-driven simulations collaboration users delegate disk half size administration data growing popularity adequate disconnections lasting typical workday distributed file systems nfs afs attests compelling nature considerations users systems accept design overview fact remote failure critical juncture coda designed environment consisting large inconvenience collection untrusted unix clients smaller number trusted unix file servers design optimized access sharing patterns typical academic research environments specifically notthis work supported defense advanced research projects agency avionics lab wright research development center aeronautical systems intended applications exhibit highly concurrent division afsc air force wright-patterson afb ohio fine granularity data access contract -carpa order national science foundation pyi award grant ecd ibm corporation faculty development award graduate fellowship research initiation grant digital equipment coda client local disk communicate corporation external research project grant bellcore information servers high bandwidth network networking research grant times client temporarily unable communicate servers due server network failure due detachment portable client network unix trademark mahler vivaldi ravel flute harp mahler vivaldi ravel flute harp viola mahler vivaldi ravel flute harp viola mahler vivaldi ravel flute harp mahler vivaldi ravel flute harp mahler vivaldi ravel harp flute viola viola viola viola servers mahler vivaldi ravel replicas volume file file potentially interest users clients flute viola harp flute capable wireless communication dotted line regular network communication proceeding clockwise steps show node connectivity system note step flute operating disconnected figure disconnected operation relates server replication clients view coda single location-transparent shared cache cache misses serviced masked unix file system coda namespace mapped failures application programs users individual file servers granularity subtrees called disconnection ends venus propagates modifications volumes client cache manager venus reverts server replication figure depicts typical dynamically obtains caches volume mappings scenario involving transitions server replication disconnected operation coda distinct complementary mechanisms achieve high availability mechanism server earlier coda papers server replication volumes read-write replicas replication depth contrast paper restricts server set replication sites attention disconnected operation discuss server volume volume storage group vsg subset replication areas presence vsg accessible client accessible significantly influenced design disconnected vsg avsg performance cost server replication operation low caching disks clients parallel access protocols venus cache coherence protocol based callbacks guarantee open design rationale file yields latest copy avsg guarantee high level factors influenced strategy high provided servers notifying clients cached availability wanted conventional off-thecopies longer valid notification referred shelf hardware system wished callback break modifications coda preserve transparency seamlessly integrating high propagated parallel avsg sites eventually availability mechanisms coda normal unix missing vsg sites environment disconnected operation high availability detailed level considerations influenced mechanism coda takes effect avsg design include scale gracefully empty disconnected venus services file advent portable workstations system requests relying solely contents resource integrity security assumptions made clients servers strike balance portable machines gave insight availability consistency examine fact people operate extended issues sections periods isolation good predicting future file access turn suggests reasonable seek user assistance augmenting cache management policy disconnected scalability operation successful distributed systems tend grow size experience coda ancestor afs impressed functionally involuntary disconnections caused failures prepare growth priori voluntary disconnections caused treating afterthought brought unplugging portable computers coda experience bear coda ways single mechanism cope disconnections adopted mechanisms enhance scalability qualitative differences user drew set general principles guide expectations extent user cooperation design choices cases mechanism adopted scalability callback-based cache coherence mechanism whole-file caching offers added advantage class replication simpler failure model cache miss occur disconnected operation feasible server open read write seek close replication needed answer question turn substantially simplifies implementation depends critically assumptions made disconnected operation partial-file caching scheme clients servers coda afsecho mfs clients appliances turned complicated implementation made disconnected unattended long periods time operation transparent limited disk storage capacity software hardware scalability principle considerable influence tampered owners design placing functionality clients diligent backing local disks servers servers integrity security public utilities greater disk capacity compromised violated principle physically secure carefully monitored scalability principle adopted administered professional staff avoidance system-wide rapid change distinguish class rejected strategies require election agreement replicas servers class replicas cache large numbers nodes avoided copies clients class replicas higher quality algorithms locus depend persistent widely secure nodes achieving consensus current partition state complete accurate class replicas contrast network inferior dimensions periodic revalidation respect class replica class replica portable workstations powerful lightweight compact laptop computers function cache coherence protocol combine commonplace today instructive observe performance scalability advantages person data shared file system class replica quality class replica machine typically identifies files interest disconnected quality class replica downloads shared file system local degraded class replica space isolated returns contingent inaccessible longer duration copies modified files back shared file system disconnection greater potential degradation user effectively performing manual caching server replication preserves quality data write-back reconnection face failures disconnected operation forsakes quality availability server replication important early design coda realized disconnected reduces frequency duration operation substantially simplify portable disconnected operation properly viewed clients users space measure resort isolated manually propagate reconnection portable machines server replication expensive requires champion application disconnected operation additional hardware disconnected operation contrast costs server replication purpose 
disconnected operation provide tradeoff quality cost coda permit high availability worse updates made volume sole server replica disconnected discarded installation rely exclusively disconnected operation optimistic approach disadvantages chooses update made disconnected client conflict update disconnected connected client optimistic replication viable system optimistic pessimistic replica control sophisticated machinery definition network partition exists system detecting conflicts automating resolution disconnected class replica class confining damage preserving associates choice families replica evidence manual repair repair conflicts control strategies pessimistic optimistic manually violates transparency annoyance users central design disconnected operation reduces usability system pessimistic strategy avoids conflicting operations disallowing partitioned writes restricting reads chose optimistic replication felt writes single partition optimistic strategy strengths weaknesses matched design goals higher availability permitting reads writes dominant influence choice low degree deals attendant danger write-sharing typical unix implied conflicts detecting resolving optimistic strategy lead occurence conflicts optimistic strategy consistent goal providing highest pessimistic approach disconnected operation availability data require client acquire shared exclusive control cached object prior disconnection principle chosen pessimistic strategy retain control reconnection possession server replication choosing optimistic exclusive control disconnected client preclude strategy disconnected operation reading writing replicas possession reduced transparency user faced shared control reading replicas anomaly update data disconnected writes forbidden unable connected subset servers previous arguments favor acquiring control prior voluntary disconnection optimistic strategy apply server replication simple difficult disconnection involuntary system arbitrate optimistic strategy presents uniform multiple requestors information model system user perspective needed make wise decision readily time read latest data accessible system predict requestors universe updates immediately visible object release control universe accessible universe relative costs denying access entire set servers clients failures occur accessible universe shrinks set servers retaining control reconnection acceptable contact set clients turn case disconnections unacceptable contact limit operating disconnected case extended disconnections disconnected client accessible universe consists machine shared control object force rest reconnection updates visible system defer updates reconnected now-enlarged accessible universe exclusive control prevent users making copy object coercing client reconnect feasible whereabouts detailed design implementation entire user community describing implementation disconnected mercy single errant client unbounded operation focus client amount time complexity lies section describes physical placing time bound exclusive shared control structure client section introduces major states case leases avoids problem venus sections discuss states introduces lease expires disconnected detail description server support needed client loses ability access cached object disconnected operation contained section system interested turn defeats volumes depending failure conditions client structure system complexity venus made user level process part kernel approach yielded performance portable considerably difficult debug figure illustrates high-level structure coda client hoarding emulation reintegration disconnection physical reconnection logical reconnection disconnected venus emulation state transits reintegration successful reconnection avsg member hoarding resumes connected operation figure venus states transitions coda minicache application venus system call interface vnode interface coda servers hoarding figure structure coda client hoarding state named key responsibility venus state hoard data anticipation venus intercepts unix file system calls widely-used disconnection sun vnode interface interface imposes responsibility venus manage cache heavy performance overhead user-level cache managers manner balances connected tiny in-kernel minicache filter disconnected operation instance user kernel-venus interactions minicache set files critical support remote access disconnected operation server files provide good replication functions handled venus performance venus cache files prepared disconnection cache system call coda object forwarded vnode set files interface minicache call serviced minicache control returned application factors complicate implementation hoarding minicache contacts venus service file behavior call turn involve contacting coda servers distant future predicted control returns venus minicache certainty application program updating minicache state side effect minicache state initiated disconnections reconnections unpredictable venus events callback breaks coda servers measurements implementation confirm true cost cache miss minicache critical good performance disconnected highly variable hard quantify activity clients accounted venus states latest version object logically venus operates states hoarding cache disconnection emulation reintegration figure depicts states transitions venus cache space finite availability critical objects sacrificedhoarding state relying server replication favor critical objects alert disconnection disconnection enters emulation state remains address concerns manage cache duration disconnection reconnection venus prioritized algorithm periodically reevaluate enters reintegration state resynchronizes cache objects merit retention cache process avsg reverts hoarding state hoard walking volumes replicated set servers venus states respect personal files files venus source files coda usr jjk maintainer shared coda developers coda usr jjk papers usr bin coda project coda src venus coda usr jjk papers sosp usr bin xvga coda project coda include usr bin mwm coda project coda lib system files usr bin startx usr bin usr bin xclock usr usr bin xinit usr include usr bin xterm usr lib usr include bitmaps usr local gnu usr lib app-defaults usr local rcs usr lib fonts misc usr ucb usr lib system mwmrc typical hoard profiles provided coda user application maintainer group project developers profile interpreted separately hdb front-end program beginning line add-entry command commands delete entry clear entries list entries modifiers pathnames non-default priorities default meta-expansion entry note pathnames beginning usr symbolic links coda figure sample hoard profiles prioritized cache management resolve pathname cached object venus combines implicit explicit sources disconnected imperative ancestors information priority-based cache management object cached venus ensure algorithm implicit information consists recent cached directory purged history traditional caching algorithms descendants hierarchical cache management explicit information takes form per-workstation needed traditional file caching schemes cache hoard database hdb entries pathnames misses translation serviced albeit identifying objects interest user performance cost venus performs hierarchical cache workstation management assigning infinite priority directories cached children automatically forces simple front-end program user update replacement occur bottom-up hdb command scripts called hoard profiles shown figure hoard profiles files hoard walking simple application maintainer provide cache equilibrium signifying common profile users users collaborating meets user expectations availability project maintain common profile user uncached object higher priority cached object customize hdb combinations equilibrium disturbed result normal activity profiles executing front-end commands interactively suppose object brought facilitate construction hoard profiles venus cache demand replacing object suppose record file observed pair start mentioned hdb time stop events user activity ceases priority decay hoard priority cache longer equilibrium reduce verbosity hoard profiles effort cached object lower priority needed maintain venus supports meta-expansion uncached 
object hdb entries shown figure letter pathname command applies venus periodically restores equilibrium performing children descendants operation hoard walk hoard walk occurs command applies minutes current implementation future present children descendents hoard explicitly requested user prior voluntary entry optionally hoard priority higher disconnection walk occurs phases priorities indicating critical objects bindings hdb entries reevaluated reflect update activity coda clients current priority cached object function children created directory hoard priority metric representing recent usage pathname option hdb updated continuously response priorities entries cache hdb serves age priority objects reevaluated objects fetched evicted needed longer working set objects lowest priority restore equilibrium chosen victims cache space reclaimed logginghoard walks address problem arising callback emulation venus records sufficient information tobreaks traditional callback-based caching data replay update activity reintegrates maintainsrefetched demand callback break information per-volume log mutating operationscoda strategy result critical object called replay log log entry copy theunavailable disconnection occur system call arguments versionreference refetching immediately callback state objects referenced call break avoids problem ignores key characteristic unix environments object modified venus number optimizations reduce length modified times user replay log resulting log size typically short interval refetch percent cache size small log conserves disk policy increase client-server traffic considerably space critical resource periods disconnection reducing scalability improves reintegration performance reducing latency server load strategy compromise balances availability consistency scalability files symbolic links important optimization reduce log length pertains venus purges object callback break refetches write operations files coda whole-file demand hoard walk whichever caching close open file occurs earlier disconnection occur modification installs completely copy file refetching object unavailable logging open close intervening directories venus purge callback break write operations individually venus logs single marks cache entry suspicious stale cache entry store record handling close disconnection occur hoard walk acceptability stale optimization consists venus discarding directory data callback previous store record file semantics callback break directory typically means appended log fact entry added deleted store renders previous versions file superfluous directory case directory entries store record copy file objects unchanged contents points copy cache saving stale copy event untimely disconnection consistency suffer implementing optimizations increases availability considerably reduce length replay log generalizes optimization previous paragraph operation overwrites effect earlier emulation operations cancel log records emulation state venus performs actions cancelling store handled servers venus subsequent unlink truncate assumes full responsibility access semantic checks optimization exploits knowledge inverse operations responsible generating temporary file cancel inverting inverted log records identifiers fids objects pending assignment rmdir cancel log record permanent fids reintegration venus mkdir functioning pseudo-server updates accepted revalidated respect integrity protection persistence real servers coda policy trusting disconnected user restart machine servers clients minimize unpleasant delayed shutdown continue left case surprises disconnected user behooves venus crash amount data lost greater faithful emulation failure occurred connected operation provide guarantees venus cache andcache management emulation related data structures non-volatile storage priority algorithm hoarding mutating operations directly update cache entries objects meta-data consisting cached directory symbolic involved cache entries deleted objects freed link contents status blocks cached objects types immediately modified objects assume replay logs hdb mapped venus address infinite priority purged space recoverable virtual memory rvm reintegration cache miss default behavior transactional access memory supported venus return error code user optionally rvm library linked venus actual contents request venus block processes cache misses serviced cached files rvm stored local unix reintegration files reintegration transitory state venus passes changing roles pseudo-server cache transactions manipulate meta-data simplifies manager state venus propagates made venus job enormously maintain invariants venus emulation updates cache reflect current ensure transaction takes meta-data server state reintegration performed volume time consistent state concerned update activity volume suspended crash recovery rvm handles transparently completion chosen obvious alternative placing metadata local unix files follow replay algorithm strict discipline carefully timed synchronous writes propagation client avsg ad-hoc recovery algorithm accomplished steps step venus obtains permanent fids objects replace rvm supports local non-nested transactions temporary fids replay log step avoided independent control basic transactional properties cases venus obtains small supply atomicity permanence serializability permanent fids advance hoarding application reduce commit latency labelling state step replay log shipped commit no-flush avoiding synchronous write parallel avsg executed independently disk ensure persistence no-flush transactions member server performs replay single application explicitly flush rvm write-ahead log transaction aborted error detected time time manner rvm bounded persistence bound replay algorithm consists phases phase period log flushes log parsed transaction begun objects referenced log locked phase venus exploits capabilities rvm provide good operation log validated executed performance constant level persistence validation consists conflict detection integrity hoarding venus initiates log flushes infrequently protection disk space checks case copy data servers servers store operations execution replay identical accessible emulating venus conservative execution connected mode store empty flushes log frequently lowers shadow file created meta-data updated performance amount data lost client data transfer deferred phase consists crash acceptable limits exclusively performing data transfers process back-fetching final phase commits resource exhaustion transaction releases locks venus exhaust non-volatile storage emulation significant instances reintegrations succeeds venus frees replay log file cache filled modified files resets priority cached objects referenced log rvm space allocated replay logs full reintegration fails venus writes replay log local replay file superset unix tar format current implementation graceful log cache entries purged handling situations file cache full space subsequent refetch current freed truncating deleting modified files contents avsg tool provided log space full mutations allowed user inspect contents replay file compare reintegration performed nonthe state avsg replay selectively mutating operations allowed entirety plan explore alternatives free disk reintegration finer granularity volume space emulating possibility compress file reduce latency perceived clients improve cache rvm contents compression trades concurrency load balancing servers reduce user computation time space recent work shown effort manual replay end revising promising tool cache management implementation reintegrate granularity possibility users selectively back updates subsequences dependent operations volume made disconnected approach dependent subsequences identified portions file cache rvm written precedence graph approach davidson revised removable media floppy disks implementation venus maintain precedence graphs emulation pass servers replay log conflict handling sections provide qualitative optimistic replica control means quantitative answers important questions disconnected operations client conflict pertaining 
disconnected operation activity servers disconnected clients long reintegration class conflicts concerned write write large local disk conflicts read write conflicts relevant unix file system model notion atomicity conflicts boundary single system call check conflicts relies fact replica duration reintegration object tagged storeid uniquely identifies experience typical disconnected sessions editing update phase replay server program development lasting hours require compares storeid object mentioned log minute reintegration characterize reintegration entry storeid replica object speed precisely measured reintegration times comparison equality objects operation disconnected execution well-defined tasks performed mutated objects tagged task andrew benchmark widely storeid log entry basis comparing file system performance task compiling linking current storeid comparison fails action depends version venus table presents reintegration times operation validated case store tasks file entire reintegration aborted directories conflict declared newly created collides time reintegration consists components existing object updated client time allocate permanent fids time replay server deleted directory servers time phase attributes modified server client update protocol server replication strategy resolving partitioned directory updates component disconnections due consistent strategy server replication preallocation fids hoarding expect originally suggested locus time component fall considerably cases incorporate replay log original design disconnected operation called optimizations section preservation replay files servers clients component avoided server replication approach damage confined marking conflicting replicas inconsistent forcing manual repair case server make interesting secondary observations replication awaiting usage experience table total time reintegration determine correct approach roughly tasks andrew disconnected operation benchmark smaller elapsed time andrew benchmark file system intensively reintegration venus make status evaluation takes longer number entries today coda high-performace sorting networks workstations meenali rungta distributed systems wisconsin madison march spring overview paper demonstrates parallel sorting network workstations competitive sorting largescale smps clusters consist commodity hardware software authors evaluate suitability existing hardware interfaces mechanisms sorting application problem statement assumptions runs paper presents ibm rts approach decstation disk-to-disk replay sorting log problem smaller authors show nows file data -based laptops well-suited toshiba o-intensive applications back-fetched phase single application replay mind finally small user system community easier coda build daily optimize basis system task involves build time assumptions result workstations primary cluster data repository connected april high-speed reintegration switched times ethernet comparable workstation runs independent development unix work based coda disparate coda resources unified longer distributed typical glunix disconnected session authors july primarily parallel program launcher triply-replicated communication environment layer data coda nodes plans cluster expand lightweight activemessage months layer cache built size specially version advantage disconnected operation low latency minimal local high disk bandwidth capacity switch-based networks clients proved message functionality arrives tcp demonstrated buffers till october reaches adequate application layer initial sessions application disconnected decides operation complete version functional arriving message january active messages hand sender handler message executed remotely obtain atomically respect understanding message cache arrivals size regular layers tcp successfully operated present requirements round-trip latency network disconnected operation analyzed authors file measure disconnected performance periods lasting now-sort key hours values uniform traces distributions environment assume traces experience initial number system records positive workstation obtained equal instrumenting methodology authors develop collection sorting algorithms increasing complexity single-pass single node final two-pass parallel algorithm implement algorithm study system detail tune configure optimal performance design implementation one-pass single-node sorting single-pass single node sort turns building block sorting application implementation contributes studying optimal disk configuration authors develop library enable stripe larger blocks faster disks make heterogenity disk hardware expected common phenomenon large clusters once-deployed clusters mix disks due time study suggests existing hardware -disk configuration bandwidth notable studying parallel version sort authors find due communication disk traffic common s-bus interconnect bottleneck disks wide scsi turns disk configuration -disk disks fast-narrow scsi due hardware limitations variablilty optimal configurations variety hardware cluster environment noteworthy usefulness software interface find mmap madvise adequate reading records disk develop tool determining amount memory tool determine number records sorted one-pass sort one-pass parallel sorting interesting contrast active messages compared distributed file system case nowsort doesn dfs distributed glunix launch parallel programs simultaneously activemessage layer utilizes locality data input files read local disks keys remote buckets needed communication layer final output files written local disks dfs hand read remote files information locality send keys remote buckets needed write local disks dfs built petal write final files locally virtual disk interface exposed petal completely abstracts location information frangipani authors exploit overlapping operations reading local disks communicating remote nodes develop synchronous interleaved threaded versions algorithm observe threaded overlaps reading sending means separate threads performs host disk configurations note previous optimal disk configuration -disks doesn communication involved s-bus due saturation reading -disks writing configuration saturation observed s-bus pretty acute theoretical peak observed peak two-pass single-node sorting consists phases create runs phase one-pass sort repeated create multiple sorted runs disks merge phase sorted runs merged single sorted file authors exploit overlapping read subsequent run write previous run create runs phase exploit overlap reading records individual runs merging writing part final sorted file two-pass parallel sorting authors explore overlapping reading sending one-pass parallel algorithm reading writing two-pass single node sort operations overlapped performance case disks s-bus point contention case disks read write network traffic passing time -disks configuration overlaps reading sending performs reading writing synchronously evaluation paper evaluates system incrementally sorting implementation configurations interesting nowsort sets record minutesort sorted minute processors amount sorting passes owing insufficient memory sets datamation benchmark record seconds node cluster conclusions paper successfully demonstrates suitability cluster environment intensive applications argument favour clusters developing parallel applications strong provide performance isolation making easy determine performance bottleneck compared black-boxed smps enable incremental scalabilty commodity hardware added easily effectively existing clusters nowsort works idealized conditions issue fault tolerance addressed utilizes soft state re-sorting effectively case node failures issues writing part file arise writer thread fails middle operation cluster bigger difficult imagine machines time disks perfect condition techniques developed paper proof-of-concept readily applied real world problems lack robustness 
workstations record confident refinements development information file system operation result usable system file coda afs local file system andrew benchmark venus make elapsed time seconds reintegration time seconds total allocfid replay cop size replay log records bytes data back-fetched bytes data obtained toshiba client memory disk reintegrating ethernet ibm rt-apc server memory disk values shown means trials figures parentheses standard deviations table time reintegration analysis based simulations driven traces plan extend work trace-driven simulations writing validating simulator precisely models ways investigate cache size complex caching behavior venus requirements longer periods disconnection difficult avoid difficulty modified venus sampling broader range user act simulator running simulator activity obtaining traces machines venus driven traces requests environment evaluate effect kernel code communicate servers hoarding simulating traces hoard profiles code perform physical local file system ante users stubbed simulation likelihood conflicts optimistic server replication coda year virtually conflicts due multiple users updating object network partitions gratifying observation subject criticisms users cautious knowing dealing experimental system conflicts problem coda user community grows larger extended voluntary disconnections lead conflicts time hours high-water mark megabytes max avg min obtain data likelihood conflicts larger scale graph based total traces active instrumented afs servers environment coda workstations curve labelled avg servers computer science corresponds values obtained averaging faculty staff graduate students research program high-water marks workstations curves development education usage profile includes alabelled max min plot highest lowest values high-water marks workstations significant amount collaborative activity coda note high-water mark include space descended afs makes kind usage needed paging hdb replay logs assumptions data estimate frequent figure high-water mark cache usage conflicts coda replace afs environment figure shows high-water mark cache usage time user modifies afs file directory function time actual disk size needed compare identity user made disconnected operation larger previous mutation note time interval explicit implicit sources hoarding information mutations file close open imperfect data appears disk update counted mutation individual write adequate operating disconnected operations counted directories operations typical workday user activity modify directory counted mutations drastically recorded traces produce significantly results type volume number volumes type object total mutations user total min min day user files project directories files directories system files directories user data obtained june afs servers cmu cell servers stored total data column entitled user percentage mutations user performing mutation performing immediately preceding mutation file directory remaining mutations contribute column entitled user table sequential write-sharing afs table presents observations period twelve providing tools link local remote spaces months data classified volume type user cedar file system provided rudimentary support volumes private user data project volumes disconnected operation collaborative work system volumes primary goal cedar provide support hoarding program binaries libraries header files similar transparent reintegration conflict detection files data average project volume files versioned immutable cedar cache manager directories system volume substitute cached version file files directories user volumes tend smaller unqualified remote file server inaccessible averaging files directories users implementors cedar observe place data project volumes capability exploited remote files referenced specific version number table shows modifications previous writer chances birrell schroeder pointed possibility users modifying object day stashing data availability early discussion expected highest degree echo file system recent description write-sharing project files directories echo stashing surprised occurs system files highest levels naming hierarchy conjecture significant fraction sharing arises face file system stashing modifications system files operators integrate caching lack integration change shift periodically system files excluded negative consequences reduces absence write-sharing striking transparency users applications deal mutations previous writer spaces consistency chances users modifying object properties utilization local disk space week data highly worse recent usage information encouraging point view optimistic cache management manage stash replication suggests conflicts literature face report problem afs replaced coda lack integration detracted usability environment system application-specific form disconnected operation related work implemented pcmail system mit lambert coda unique exploits caching pcmail allowed clients disconnect manipulate performance high availability preserving high existing mail messages generate redegree transparency aware system synchronize central repository reconnection published unpublished duplicates 
key aspect relying heavily semantics mail pcmail coda transparent coda required manual re-synchronization pre-registration clients made disconnected replay servers reconnection optimistic replication distributed file systems implementing disconnected operation simple pioneered locus locus peer-toinvolves major modifications careful attention detail peer model client-server model availability aspects cache management hoarding achieved solely server replication surprisingly large volume variety interrelated state notion caching disconnected operation maintained emulating persistence integrity client data structures critical coda benefited general sense large body reintegration dynamic choices made work transparency performance distributed file granularity reintegration systems coda owes afs inherits model trust integrity hindsight realize extent mechanisms design philosophy scalability implementations traditional caching schemes simplified guaranteed presence lifeline first-class replica purging refetching demand strategy handle pathological situations future work implementations viable supporting disconnected operation coda facility active disconnected operation obstacles development earlier sections paper realizing disconnected operation insurmountable work progress areas log optimization central message paper granularity reintegration evaluation hoarding disconnected operation feasible efficient additional work lower levels usable system section ways scope work broadened view work regard extension idea write-back caching write-back excellent opportunity exists coda adding caching hitherto performance transactional support unix explicit transactions shown extended mask temporary failures desirable systems scale hundreds broader view disconnected operation thousands nodes informal concurrency control graceful transitions states autonomy unix effective mechanisms interdependence distributed system favorable supporting disconnected operation operation conditions approach benefits remote logging precedence graph maintenance conflict data access unfavorable conditions checking transfer directly transactional system continued access critical data optimistic concurrency control disconnected operation increasingly important transactional file systems idea distributed systems grow scale diversity system scalability availability performance vulnerability properties coda proposed built opportunity exists extending coda support weakly-connected operation environments acknowledgments connectivity intermittent low bandwidth lily mummert invaluable assistance collecting postprocessing file traces conditions found networks rely voice-grade section dimitris varotsis helped instrument lines wireless technologies packet afs servers yielded measurements section radio ability mask failures provided express appreciation past present disconnected operation weak contributors coda project puneet kumar hank connectivity techniques exploit adapt mashburn maria okasaki david steere communication opportunities hand needed techniques include aggressive write-back policies compressed network transmission partial file transfer caching intermediate levels burrows efficient data sharing phd thesis cambridge computer conclusion laboratory december disconnected operation tantalizingly simple idea pre-load cache critical data continue normal operation disconnection log cate gross ousterhout costa harrison kunze combining concepts compression caching kupfer thompson two-level file system trace-driven analysis bsd file system proceedings acm symposium proceedings acm symposium operating architectural support programming languages system principles december operating systems april sandberg goldberg kleiman walsh lyon cova resource management federated computing design implementation sun network environments filesystem phd thesis department computer science princeton summer usenix conference proceedings october satyanarayanan davidson influence scale distributed system optimism consistency partitioned distributed proceedings international conference database systems software engineering april acm transactions database systems september satyanarayanan kistler kumar okasaki siegel steere davidson garcia-molina skeen coda highly file system distributed consistency partitioned networks workstation environment acm computing surveys september ieee transactions computers april floyd satyanarayanan transparency distributed file systems scalable secure highly distributed file technical report department computer access science rochester ieee computer gray cheriton schroeder gifford needham leases efficient fault-tolerant mechanism caching file system programmer workstation distributed file cache consistency proceedings acm symposium operating proceedings acm symposium operating system principles december system principles december steere kistler satyanarayanan hisgen birrell mann schroeder swart efficient user-level cache file management sun availability consistency tradeoffs echo vnode interface distributed file system summer usenix conference proceedings june proceedings workshop workstation decorum file system operating systems september transarc corporation howard kazar menees nichols walker popek english kline thiel satyanarayanan sidebotham west locus distributed operating system scale performance distributed file system proceedings acm symposium operating acm transactions computer systems february system principles october kleiman vnodes architecture multiple file system types sun unix summer usenix conference proceedings kumar satyanarayanan log-based directory resolution coda file system technical report cmu-cs- school computer science carnegie mellon mashburn satyanarayanan rvm recoverable virtual memory user manual school computer science carnegie mellon needham herbert report european sigops workshop autonomy interdependence distributed systems sigops review april 
frangipani scalable distributed file system shreepadma venugopalan distributed systems wisconsin madison feb spring overview paper describes design implementation frangipani scalable distributed filesystem built top petal petal incrementally scalable highly automatically managed storage service multiple machines run frangipani top shared virtual disk distributed locking service ensure coherence frangipani inherits scalability fault tolerance easy administration petal design constrained petal frangipani replicated petal disk implies logging occurs solve problem double logging frangipani care replication introduce complexity file system problem arises frangipani running virtual disk unlike file system exploit location information placing data petal influences aspects frangipani design generally speaking ways petal makes building filesystems easier ways petal make file system design ideal sitting top petal frangipani make correct tradeoffs system structure machines running frangipani servers access petal virtual disk network alternatively machine run frangipani petal clients form tier system access frangipani server network clients reside machine server case accesses local individual servers dont talk directly petal backplane method communication reduces global state greatly simplifies system locks stored petal frangipani makes reliable underlying distributed store provide consistent data access frangipani system doesnt provide form security restrict accesses sensitive data frangipani assumes environment trust petal frangipani lax view security provide basic solution security petal implement form block level security higher level trusted entity give authorization clients lower levels implement block level problem arising lack block level security buggy implementation lead clients writing data wrong location provide security sparse address space petal make location file inode exponentially difficult disrtibuted file systems care security widely system nfs provided security design-the piece details section presents discussion aspects frangipani design-disk layout logging recovery synchronization locking disk layout frangipani multiple file systems access petal volume region disk holds shared configuration parameters housekeeping information region holds logs frangipani server private log region hold upto logs region allocation bitmap blocks remaining region free fourth region consists inodes inodes bytes long avoid false sharing servers occur servers access inodes block leads space waste tradeoff reasonable simplicity scheme region hold small blocks size file stored small blocks file grows rest stored large block remainder petal address space holds large blocks frangipani block sizes greatly simplifies design eliminating indirect pointers indirect blocks indirect blocks complicate metadata tracked frangipani afford allocate blocks worrying internal fragmentation allocation lazily petal blocks lead internal fragmentation issue log size large sparse address space petal log bigger frangipani make space pressure commit transactions remove log bigger log increase recovery time tradeoff frangipani makes doesnt bad frangipani sets static limit constrictive limits limit number servers number large files large million limit work application photo server frangipani dynamically set limits number small files size large files fair authors limits worked workloads time paper written logging recovery frangipani write ahead logging log metadata writes metadata logging log sizes small reasonable write performance logging metadata file system data structures consistent reduces cost recovery metadata logging data losses inconsistencies log records flushed petal order updates describe requested permanent location updated seconds unix update daemon presence multiple frangipani servers important maintain cache consistency frangipani doesnt make meta data reaches disk data lead mis ordering inconsistencies ordered journaling reduces chances mis ordered data minimal performance loss frangipani ordered journaling guarantee metadata updates frangipani introduces additional level logging petal levels logging serve purposes petal logging needed mirrors sync frangipani logging metadata consistent incurrs additional physical writes logical write dirty inode flushed disk frangipani issues logical writes petal issues physical writes logical write incurring total phyical writes solution solve problem frangipani issue writes replicas introduce additional complexity frangipani petal expose underlying details frangipani frangipani server crashes failure detected client server lock server recovery daemon explicit ownership failed server log locks daemon finds log start runs recovery lease expired network failure lead false recovery abstarction extended single system distributed nature failures synchronization locking frangipani multiple reader single writer locks ensure synchronized access shared disk data structures lock server detects conflict current holder lock asked release downgrade remove conflict disk structres divided segments locks segment frangipani locking granularity files restrictive applications database runs top frangipani database systems require locks record level maintain concurrency system lock server deals client failure leases client contacts lock server obtains lease additional locks added existing lease lease expiration time creation renewal client renew lease expiration server failed discussed server renew lease network failure access petal volume frangipani server checks lease valid issuing write petal checking write request arrives solve problem authors plan add expiration timestamp write request make write valid summary frangipani file system users coherent shared access set files frangipani leaverages scalability ease administration availability underlying petal frangipani locking synchronization complicated distributed nature system frangipani pretty system scalably large storage space provided petal 
byzantine generals wisconsin-madison computer sciences department distributed systems andrea arpaci-dusseau paper byzantine generals problem lamport shostak pease acm transactions programing languages systems july motivation build reliable systems presence faulty components common approach multiple potentially faulty components compute function perform majority vote outputs result majority faulty good components total assumption good nonfaulty components input trust output result majority voting work nonfaulty processors input input nonfaulty nonfaulty processes byzantine failure primary differences fail-stop failure component produce arbitrary output fail-stop produces correct output detect output faulty fail-stop detect component stopped components work maliciously collusion components byzantine generals algorithm achieve agreement loyal generals working components traitors faulty components agreement loyal generals decide plan small number traitors loyal generals adopt bad plan terminology information communicated ith general combine values form plan rephrase agreement conditions generals method combining information decision majority function values key step agree inputs generals communicate values loyal general obtain loyal generals traitor loyal generals ith general loyal sends general problem general send commanding general send order lieutenants loyal lieutenants obey order commanding general loyal loyal lieutenant obeys order sends interactive consistency conditions impossibility result generals solution work traitor oral messages commander attack retreat commander traitor option loyal commander commander attack retreat attack obey commander attack option loyal commander attack retreat retreat obey order retreat problem distinguish scenarios general impossibility result solution fewer generals cope traitors paper details oral messages assumptions message delivered correctly receiver message absence message detected oral message algorithm commander sends lieutenant commander sends lieutenant lieutenant receives commander act commander send lieutenants lieut received lieut lieut computes majority vnexample bad lieutenant scenario traitor decision attack bad commander scenario traitor decision attack bigger bad lieutenants scenario traitors decision messages loyal lieutenants attack bigger bad commander scenario traitors decision messages decision bad commander attack retreat attack retreat attack problem loyal lieutenants choose action step algorithm verify lieutenants thing requires rounds msg lieut form messages receive total confusion messages attack signed messages assumption cryptography loyal general signature forged contents altered verify authenticity signature simplifies problem lieutenant passes signed message lie lieutenants harm forge loyal general orders check traitor commander cryptographic primitives implement byzantine agreement nodes signed messages algorithm commander signs sends lieut receive order send receive add send msgs obey order choose bad commander scenario bad commander trust orders apply decision bad commander scenario bad commander goal make decision decision variations handle missing communication paths paper details assumptions message nonfaulty processor delivered correctly network failure processor failure handle connectivity graph processor determine sender message communication fixed dedicated lines switched network absence message detected fixed max time send message synchronized clocks msg received fixed time default processors sign msgs nonfaulty signatures forged randomizing function cryptography make liklihood forgery small importance assumptions separating agreement execution byzantine fault tolerant services sosp goal reduce replication costs agreement replicas execution replicas costly part replicate software versions potentially long running time protocol assumes cryptographic primitives switched environment problem 
fail-stop processors wisconsin-madison computer sciences department distributed systems andrea arpaci-dusseau paper byzantine generals action implementing failstop processors fred schneider tocs motivation goal build systems continue work presence component failure difficulty building systems depends components fail fail-stop components make building reliable systems easier components byzantine failures fail-stop processors failure output behavior inconsistent specification byzantine failure arbitrary malicious behavior components collude necessarily detect output faulty fail-stop processor halts performing erroneous transformations detect halted state obtain uncorrupted stable storage real processors fail-stop build building fail-stop processors easier distributed state machine common approach building reliable systems idea replicate servers coordinate client interactions replicas failure model components determines replicas needed interactions rrr client input sequence state machine output build t-fault tolerant state machine t-fault tolerant satisfies specification long components servers fail inputs key replicas receive process sequence inputs agreement nonfaulty replica receives sam request interactive consistency byzantine agreement ordering nonfaulty replica processes requests order logical clocks outputs number replicas output fail-stopbyzantine build t-fault tolerant state machine t-fault tolerant satisfies specification long components servers fail inputs key replicas receive process sequence inputs agreement nonfaulty replica receives sam request interactive consistency byzantine agreement ordering nonfaulty replica processes requests order logical clocks outputs number replicas output majority output fail-stopbyzantine build t-fault tolerant state machine t-fault tolerant satisfies specification long components servers fail inputs key replicas receive process sequence inputs agreement nonfaulty replica receives sam request interactive consistency byzantine agreement ordering nonfaulty replica processes requests order logical clocks outputs number replicas output majority output fail-stopbyzantine build t-fault tolerant state machine t-fault tolerant satisfies specification long components servers fail inputs key replicas receive process sequence inputs agreement nonfaulty replica receives sam request interactive consistency byzantine agreement ordering nonfaulty replica processes requests order logical clocks outputs number replicas output output majority output fail-stopbyzantine build t-fault tolerant state machine t-fault tolerant satisfies specification long components servers fail inputs key replicas receive process sequence inputs agreement nonfaulty replica receives request interactive consistency byzantine agreement ordering nonfaulty replica processes requests order logical clocks outputs number replicas output output majority output fail-stopbyzantine building fail-stop processor assumption storage volatile lost failure stable affected lost corrupted failure read processor benefit recover work failed process drawback minimize interactions slow build approximation fail-stop processor finite hardware finite failures disable error detection hardware k-fail-stop processor behaves fail-stop failures implementation k-fsp overview components p-processes program s-processes storage process runs processor connected network p-processes runs program state machine interacts s-processes read write data fail disagreement stop detect failures implementation continued s-processes contents stable storage reliable data failures stop detects disagreements failures p-processes assumptions messages authenticated digital signatures byzantine agreement protocol signed messages requires processes synchronized clocks processes fsp fsp algorithm writes p-process write byzantine agreement s-processes agree input s-process write ensure p-process writes time bound update stable storage halt p-processes set failed variable true future writes fsp algorithm reads p-process read broadcast request s-processes result majority fsp read p-process determine halted failed read failed variable s-process majority fsp code p-processes read s-processes fail s-processes fail p-processes write p-process slow p-process incorrect results p-process incorrect results p-processes give bad result failed higher-level goal run app handling faults good processors solution k-failstop processors fsp fsp fsp fsp fsp p-processes fsp fail p-processes fsp fail p-process fsp fsp fsp fail fsp p-processes fsp fsp fsp fail s-process fails s-processes fail fail stop processors metric hardware cost state machines fail-stop components worst-case assuming process processor processors best-case assuming s-processes fsp share processor processors byzantine components fail-stop s-processes share metric frequency byzantine agreement protocol fail-stop access stable storage byzantine input read fewer input reads summary build fail-stop components easier higher layers model deal matches assumptions distributed protocols hardware agreements needed higher-levels cope slightly faulty components end-to-end argument 
deadlock lecture deadlock happen entities threads processes waiting resource held entity group release hold waiting unordered mutex threads accessing locks thread grabs thread grabs semaphore binary semaphore thread thread access shared data access representing deadlock ways represent problem waits-for graph waiting waiting resource-allocation graph holds waits-for holds waits-for conditions mutual exclusion resource held non-sharable requests delayed release hold wait exists process holding resource waiting held process preemption resources released voluntarily circular wait exists set processes pnp conditions hold deadlock occur handling deadlock options prevention ensure system enters deadlock make condition hold detection recovery deadlocks detect recover continue ignore fairly common approach bad solution prevention stopping mutual exclusion required read-only file needed hold wait guarantee grab resources atomically bad idea prevention cont preemption holding resources wait problem force release hard general undo state preempted prevention cont circular wait impose total order locks follow order circular wait occurs locks acquired order advantages simple follow works common practice disadvantages arbitrary ordering avoidance prevention knowledge processes request schedule carefully avoid deadlock maximal requests process banker algorithm commonly knowledge detect recover detection notice waiting processes dependencies inform human handle automatically expensive run infrequently recovery abort processes abort deadlocked good bad abort time deadlock doesn exist hard undo effects process lock remove account lock put account release release starve repeatedly aborted locks summary deadlock mutual exclusion hold wait preemption circular wait required solve preventing conditions avoidance clever scheduling detect recover aborting processes ignoring altogether 
afast file system unix marshall kirk mckusick william joy samuel leffler robert fabry computer systems research group computer science division department electrical engineering computer science california berkeley berkeley abstract areimplementation unix file system reimplementation substantially higher throughput rates flexible allocation policies allowbetter locality adapted wide range peripheral processor characteristics newfile system clusters data sequentially accessed twoblock sizes allowfast access large files wasting large amounts space small files file access rates ten times faster traditional unix file system experienced long needed enhancements programmers interface discussed include mechanism place advisory locks files extensions space file systems ability long file names provisions administrative control resource usage revised february categories subject descriptors operating systems file systems management file organization directory structures access methods operating systems storage management allocation deallocation strategies secondary storage devices operating systems performance measurements operational analysis information systems information storage file organization additional keywords phrases unix file system organization file system performance file system design application program interface general terms file system measurement performance unix trademark bell laboratories william joyiscurrently employed sun microsystems garcia avenue mountain view samuel leffler employed lucasfilm box san rafael work grants national science foundation grant mcs defense advance research projects agency dod arpaorder monitored navalelectronic system command contract -csmm fast file system unix table contents introduction file system file system organization optimizing storage utilization file system parameterization layout policies performance file system functional enhancements long file names file locking symbolic links rename quotas acknowledgements introduction paper describes original byte unix file system newone released berkeleysoftware distribution presents motivations methods effect rationale design decisions description implementation discussion summary results obtained directions future work additions made facilities programmers original unix system runs pdphas simple elegant file system facilities file system input output buffered kernel alignment constraints data transfers operations made synchronous transfers disk byte blocks arbitrarily data area file system virtually constraints disk space file growth ritchie thompson vaxtogether unix enhancements original byte unix file system incapable providing data throughput rates manyapplications require forexample applications vlsi design image processing small amount processing large quantities data high throughput file system high throughput rates needed programs map files file system large virtual address spaces paging data file system occur frequently ferrin requires file system providing higher bandwidth original byte unix twopercent maximum disk bandwidth kilobytes arm white smith modifications made unix file system improve performance unix file system interface understood inherently slow development retained abstraction simply changed underlying implementation increase throughput users system faced massive software conversion problems file system performance dealt extensively literature smith survey previous work improve unix file system performance ferrin unix operating system drewmanyofits ideas multics large high performance dec pdp vax massbus unibus trademarks digital equipment corporation inpractice file ssize constrained gigabyte afast file system unix smm operating system feiertag work includes hydra almes spice thompson file system lisp environment symbolics agood introduction physical latencies disks pechura file system file system developed bell laboratories traditional file system disk drive divided partitions disk partitions file system afile system neverspans multiple partitions afile system super-block basic parameters file system include number data blocks file system count maximum number files pointer free list alinked list free blocks file system file system files files distinguished directories pointers files directories file descriptor called inode inode information describing ownership file time stamps marking modification access times file array indices point data blocks file forthe purposes section assume blocks file directly referenced values stored inode inode indirect blocks data block indices file system byte block size singly indirect block block addresses doubly indirect block addresses singly indirect blocks triply indirect block addresses doubly indirect blocks megabyte traditional unix file system consists megabytes inodes megabytes data organization segregates inode information data accessing file incurs long seek file sinode data files single directory typically allocated consecutive slots megabytes inodes causing manynon-consecutive blocks inodes accessed executing operations inodes files directory allocation data blocks files suboptimum traditional file system nevertransfers bytes disk transaction finds sequential data block cylinder forcing seeks byte transfers combination small block size limited read-ahead system manyseeks severely limits file system throughput work berkeleyonthe unix file system attempted improve reliability throughput reliability improvedbystaging modifications critical file system information theycould completed repaired cleanly program crash kow alski file system performance improvedbyafactor twobychanging basic block size bytes increase twofactors disk transfer accessed data files access indirect blocks direct blocks contained data file system henceforth referred file system performance improvement gav astrong indication increasing block size good method improving throughput throughput doubled file system percent disk bandwidth main problem free list initially ordered optimal access quickly scrambled files created removed eventually free list random causing files blocks allocated randomly overthe disk forced seek block access file systems provided transfer rates kilobytes theywere created rate deteriorated kilobytes fewweeks moderate randomization data block placement wayofrestoring performance file system dump rebuild restore file system possibility assuggested maruyama process periodically partition refer subdivision physical space disk drive inthe traditional file system newfile system file systems located logical disk partitions overlap overlapping made allowprograms copyentire disk drivescontaining multiple file systems actual number vary system system range smm fast file system unix reorganized data disk restore locality file system organization newfile system organization file system organization disk drive file systems afile system super-block located beginning file system sdisk partition super-block critical data replicated protect catastrophic loss file system created super-block data change copies referenced head crash hard disk error default super-block unusable insure create files large bytes twolev els indirection minimum size file system block bytes size file system blocks anypower greater equal block size file system recorded file system ssuper-block file systems block sizes simultaneously accessible system block size decided time file system created subsequently changed rebuilding file system newfile system organization divides disk partition areas called cylinder groups acylinder group comprised consecutive cylinders disk cylinder group bookkeeping information includes redundant copyofthe super-block space inodes bit map describing blocks cylinder group summary information describing usage data blocks cylinder group bit map blocks cylinder group replaces traditional file system sfree list foreach cylinder group static number inodes allocated file system creation time 
default policyistoallocate inode bytes space cylinder group expecting everbeneeded cylinder group bookkeeping information beginning cylinder group howeverifthis approach redundant information top platter single hardware failure destroyed top platter loss redundant copies super-block cylinder group bookkeeping information begins varying offset beginning cylinder group offset successive cylinder group calculated track beginning cylinder group preceding cylinder group redundant information spirals pack anysingle track cylinder orplatter lost losing copies super-block cylinder group space beginning cylinder group beginning cylinder group information data blocks optimizing storage utilization data laid larger blocks transferred single disk transaction greatly increasing file system throughput file newfile system composed byte data blocks file system file composed byte blocks increasing block size disk accesses newfile system transfer times information disk transaction large files byte blocks allocated cylinder evenlarger data transfers requiring seek main problem larger blocks unix file systems composed manysmall files uniformly large block size wastes space table shows effect file system block size amount wasted space file system files measured obtain figures reside appears cylinder group laid super-block location work file systems blocks sizes kilobytes greater requirement kilobytes disk reserved bootstrap program separate requirement cylinder group information begin file system block boundary tostart cylinder group file system block boundary file systems block sizes larger kilobytes toleave anempty space end boot block beginning cylinder group knowing size file system blocks system knowwhat roundup function find beginning cylinder group afast file system unix smm time sharing systems roughly gigabytes on-line storage measurements based active user file systems megabytes formatted space space waste organization data noseparation files data file starts byte boundary data inodes byte block unix file system data inodes byte block unix file system data inodes byte block unix file system data inodes byte block unix file system table amount wasted space function block size space wasted calculated percentage space disk user data block size disk increases waste rises quickly toanintolerable waste byte file system blocks beable large blocks undue waste small files stored efficient newfile system accomplishes goal allowing division single file system block fragments file system fragment size time file system created file system block optionally broken fragments addressable lower bound size fragments constrained disk sector size typically bytes block map cylinder group records space cylinder group fragment level determine block aligned fragments examined figure shows piece amap file system bits map xxxx xxoo ooxx oooo fragment numbers block numbers figure layout blocks fragments file system bit map records status fragment shows fragment shows fragment allocation fragments fragments free fragments adjoining blocks full block eniftheyare large fragments allocated full block fragments coalesced full block file system block size bytes fragment size bytes file represented byte blocks data possibly single fragmented block file system block fragmented obtain space small amount data remaining fragments block made allocation files byte file stored byte file system file twofull size blocks fragment portion block block aligned fragments time file created full size block split yielding fragments single unused fragment remaining fragment allocated file needed space allocated file program write system call time data written file system checks size file increased file expanded hold newdata conditions exists space left allocated block fragment hold newdata data written space file fragmented blocks block file insufficient space hold newdata space exists block allocated space filled newdata remainder newdata full block data full block allocated aprogram overwriting data middle existing file case space allocated smm fast file system unix full block newdata written process repeated full block newdata remains remaining newdata written fit full block block fragments located full block located remaining newdata written located space file fragments fragments insufficient space hold data size newdata size data fragments exceeds size afull block newblock allocated contents fragments copied beginning block remainder block filled newdata process continues newdata written fit full block block fragments located full block located contents existing fragments appended newdata written allocated space problem expanding file fragment time data copied manytimes afragmented block expands full block fragment reallocation minimized user program writes full block time partial block end file file systems block sizes reside system file system interface extended provide application programs optimal size read write forfiles optimal size block size file system file accessed forother objects pipes sockets optimal size underlying buffer size feature standard input output library apackage user programs feature system utilities archivers loaders input output management highest file system bandwidth amount wasted space byte newfile system organization empirically observed byte file system organization file system byte blocks byte fragments amount wasted space byte block unix file system newfile system space byte byte file systems indexing information large files amount space small files savings offset space keeping track free blocks net result disk utilization newfile system sfragment size equals file system sblock size order layout policies effective afile system completely full foreach file system parameter termed free space reserve givesthe minimum acceptable percentage file system blocks free number free blocks drops belowthis levelonly system administrator continue allocate blocks parameter changed time evenwhen file system mounted active transfer rates section measured file systems full reserveof number free blocks falls file system throughput cut half inability file system localize blocks file file system sperformance degrades overfilling restored removing files amount free space reaches minimum acceptable level access rates files created periods free space restored moving data space free space reservemust added percentage waste comparing organizations givenintable percentage waste byte unix file system roughly comparable byte file system free space reserveset compare wasted file system waste reserved space newfile system file system parameterization initial creation free list file system ignores parameters underlying hardware information physical characteristics mass storage device hardware interacts agoal newfile system parameterize processor capabilities mass storage characteristics blocks allocated optimum configurationdependent parameters include speed processor hardware support mass storage transfers characteristics mass storage devices disk technology constantly improving agiv eninstallation sev eral disk technologies running single processor file system parameterized adapted characteristics disk formass storage devices disks newfile system allocate newblocks cylinder previous block file optimally newblocks rotationally afast file system unix smm positioned distance rotationally optimal blocks varies greatly consecutive block rotationally delayed block 
depending system characteristics processor input output channel require anyprocessor intervention mass storage transfer requests twoconsecutive disk blocks accessed suffering lost time intervening disk revolution processors input output channels main processor field interrupt prepare newdisk transfer expected time service interrupt schedule newdisk transfer depends speed main processor physical characteristics disk include number blocks track rate disk spins allocation routines information calculate number milliseconds required skip overablock characteristics processor include expected time service interrupt schedule newdisk transfer giv enablock allocated file allocation routines calculate number blocks skip oversothat block file position disk head expected amount time takes start newdisk transfer operation forprograms sequentially access large amounts data strategy minimizes amount time spent waiting disk position ease calculation finding rotationally optimal blocks cylinder group summary information includes count blocks cylinder group rotational positions rotational positions distinguished resolution summary information milliseconds typical revolution minute drive super-block vector lists called rotational layout tables vector indexedbyrotational position component vector lists indexinto block map data block contained rotational position allocatable block system summary counts rotational position non-zero block count indexofthe rotational position find list indexthrough relevant parts block map find free block parameter defines minimum number milliseconds completion data transfer initiation data transfer cylinder changed anytime file system mounted active ifafile system parameterized lay blocks rotational separation milliseconds disk pack movedtoasystem processor requiring milliseconds schedule disk operation throughput drop precipitously lost disk revolutions block eventual target machine file system parameterized eventhough initially created processor evenifthe move isnot advance rotational layout delay reconfigured disk movedsothat allocation based characteristics newhost layout policies file system layout policies divided twodistinct parts top levelare global policies file system wide summary information makedecisions placement newinodes data blocks routines responsible deciding placement newdirectories files theyalso calculate rotationally optimal block layouts decide force long seek newcylinder group insufficient blocks left current cylinder group reasonable layouts belowthe global policyroutines local allocation routines locally optimal scheme lay data blocks methods improving file system performance increase locality minimize seek latencyasdescribed trivedi improve layout data makelarger transfers nevalainen global layout policies improve performance clustering related information theycannot attempt localize data spread unrelated data cylinder groups localization attempted local cylinder group run space forcing data scattered non-local cylinder groups takentoan extreme total localization result single huge cluster data resembling file system global policies balance twoconflicting goals localizing data concurrently accessed spreading unrelated data allocatable resource inodes inodes describe files directories inodes files directory frequently accessed list directory command smm fast file system unix accesses inode file directory layout policytries place inodes files adirectory cylinder group ensure files distributed disk policyisused directory allocation anew directory cylinder group greater erage number free inodes smallest number directories intent policyistoallowthe inode clustering policytosucceed time allocation inodes cylinder group free strategy allocates inodes randomly cylinder group inodes cylinder group read disk transfers disk transfers required cylinder group nomore inodes puts small constant upper bound number disk transfers required access inodes files directory incontrast file system typically requires disk transfer fetch inode file directory major resource data blocks data blocks file typically accessed policyroutines place data blocks file cylinder group preferably rotationally optimal positions cylinder problem allocating data blocks cylinder group large files quickly space cylinder group forcing spill overto areas space cylinder group future allocations anyfile cylinder group spill areas ideally cylinder groups everbecome completely full heuristic solution chosen redirect block allocation cylinder group afile exceeds kilobytes megabyte newly chosen cylinder group selected cylinder groups greater average number free blocks left big files tend spread overthe disk megabyte data typically accessible long seek performed cost long seek megabyte small global policyroutines call local allocation routines requests specific blocks local allocation routines allocate requested block free allocates free block requested size rotationally closest requested block global layout policies complete information theycould request unused blocks allocation routines reduced simple bookkeeping maintaining complete information costly implementation global layout policyuses heuristics employonly partial information requested block local allocator levelallocation strategy block rotationally closest requested block cylinder itis assumed head switching time disk controllers case incorporate time required switch disk platters constructing rotational layout tables blocks cylinder block cylinder group cylinder group full quadratically hash cylinder group number choose cylinder group free block finally hash fails apply exhaustive search cylinder groups quadratic hash speed finding unused slots full hash tables knuth file systems parameterized maintain free space rarely strategy file systems run maintaining anyfree space typically sofew free blocks anyallocation random important characteristic strategy conditions strategy fast spill overpoint kilobytes point file byte block file system requires single indirect block appears natural point redirect block allocation spilloverpoints chosen intent forcing block allocation redirected file data blocks cylinder group observing newfile system day day heuristics work minimizing number completely filled cylinder groups afast file system unix smm performance ultimately proof effectiveness algorithms previous section long term performance newfile system empirical studies shown inode layout policyhas effective running list directory command large directory manydirectories force system access inodes multiple cylinder groups number disk accesses inodes cut factor improvements evenmore dramatic large directories files disk accesses inodes cut factor encouraging programs spooling daemons access manysmall files programs tend flood disk request queue file system table summarizes measured throughput newfile system comments made conditions tests run test programs measure rate user programs transfer data file performing anyprocessing programs read write data insure buffering operating system affect results theyare run times succession system state twotoinsure experiment stabilized repeatable tests results discussed detail kridle systems running multi-user quiescent contention cpu disk arm difference unibus massbus tests controller tests ampex capricorn megabyte winchester disk table shows file system test runs vax file systems production month measured number system calls performed tests basic system call overhead negligible portion total running time tests type processor read file system bus measured speed bandwidth cpu unibus kbytes sec unibus kbytes sec unibus kbytes sec massbus kbytes sec massbus kbytes sec table reading rates newunix file systems type processor write file system bus measured speed bandwidth cpu unibus kbytes sec unibus kbytes sec unibus kbytes sec massbus kbytes sec massbus kbytes sec table writing rates newunix file systems unlikethe file system transfer 
rates newfile system change time throughput rate tied strongly amount free space maintained measurements table based file system free space reserve synthetic work loads suggest throughput deteriorates half rates givenintable file systems full percentage bandwidth givenintable measure effective utilization disk file system upper bound transfer rate disk calculated multiplying number bytes track number revolutions disk bandwidth calculated comparing data rates file system achieve asapercentage rate metric file system disk bandwidth newfile system bandwidth aunix command similar reading test file dev null file iseight megabytes long smm fast file system unix reads writes faster newsystem system biggest factor speedup larger block size newfile system overhead allocating blocks newsystem greater overhead allocating blocks system howeverfewer blocks allocated newsystem theyare bigger net effect cost byte allocated systems newfile system reading rate fast writing rate expected kernel work allocating blocks simply reading note write rates read rates byte block file system write rates slower read rates byte block file system slower write rates occur kernel manydisk allocations making processor unable disk transfer rate contrast file system faster writing files reading write system call asynchronous kernel generate disk transfer requests faster theycan serviced disk transfers queue disk buffer cache disk buffer cache sorted minimum seek distance average seek scheduled disk writes data blocks written random disk order theyare generated howev erwhen file read read system call processed synchronously disk blocks retrievedfrom disk non-optimal seek order theyare requested forces disk scheduler long seeks resulting lower throughput rate newsystem blocks file optimally ordered disk reads synchronous requests presented disk order eventhough writes asynchronous theyare presented disk minimum seek order gain reordering disk seek latencies limited file system effect newfile system cost allocation factor newsystem writes slower reads performance newfile system limited memory memory copyoperations required move data disk buffers system saddress space data buffers user saddress space copyoperations account time spent performing input output operation buffers address spaces properly aligned transfer performed copying vax virtual memory management hardware desirable transferring large amounts data implement change user interface file system twomajor ways user programs required allocate buffers page boundaries data disappear buffers written greater disk throughput achievedbyrewriting disk drivers chain kernel buffers allowcontiguous disk blocks read single disk transaction manydisks unix systems byte sectors track track holds twoor byte file system blocks byte file system blocks inability contiguous disk blocks effectively limits performance disks bandwidth block file laid contiguously minimum spacing allocatable block anyplatter sixth half revolution implication layout contiguous blocks half bandwidth anygiv entrack track odd number sectors resolvethe rotational delay anynumber sectors finding block begins desired rotational position track reason block chaining implemented require rewriting disk drivers system current throughput rates limited speed processors block allocated file time atechnique demos file system finds file growing rapidly istopreallocate blocks releasing file closed theyremain unused batching allocations system reduce overhead allocating write cut number disk writes needed block pointers disk synchronized block allocation powell technique included block allocation accounts time spent write system call current throughput rates limited speed processors afast file system unix smm file system functional enhancements performance enhancements unix file system require anychanges semantics data structures visible application programs sev eral generally desired time introduced theywould require users dump restore file systems newfile system required existing file systems dumped restored functional enhancements introduced time long file names file names nowbeofnearly arbitrary length programs read directories affected change promote portability unix systems running newfile system set directory access routines introduced provide consistent interface directories newsystems directories allocated byte units called chunks size chosen allocation transferred disk single operation chunks broken variable length records termed directory entries adirectory entry information map file inode directory entry allowed span multiple chunks fields directory entry fixed length inode number size entry length file contained entry remainder entry variable length null terminated file padded byte boundary maximum length file directory characters ailable space directory recorded entries accumulate free space entry size fields results directory entries larger required hold entry fixed length fields space allocated directory completely accounted totaling sizes entries entry deleted directory space returned previous entry directory chunk increasing size previous entry size deleted entry ifthe entry directory chunk free entry sinode number set unallocated file locking file system provision locking files processes needed synchronize updates file separate lock file process create lock file creation succeeded process proceed update creation failed process wait mechanism drawbacks processes consumed cpu time looping overattempts create locks locks left lying system crashes manually removed system startup command script finally processes running system administrator permitted create files forced mechanism problems solutions straight forward mechanism locking files added general schemes allowmultiple processes concurrently update file techniques discussed peterson asimpler technique serialize access file locks attain reasonable efficiency applications require ability lock pieces file locking byte levelhas implemented onyx file system bass standard system applications mechanism locks granularity file sufficient locking schemes fall twoclasses hard locks advisory locks primary difference advisory locks hard locks extent enforcement ahard lock enforced program access file advisory lock applied requested program advisory locks effective programs accessing file locking scheme hard locks override policyimplemented kernel advisory locks policyisleft user programs unix system programs system administrator privilege allowed override anyprotection scheme manyofthe programs locks run system administrator wechose implement advisory locks create additional protection scheme inconsistent unix philosophyorcould system smm fast file system unix administration programs file locking facilities allowcooperating programs apply advisory shared exclusive locks files process anexclusive lock file multiple shared locks present shared exclusive locks present file time anylock requested process holds exclusive lock exclusive lock requested process holds lock lock request block lock obtained shared exclusive locks advisory evenifaprocess obtained lock file process access file locks applied removedonly open files means locks manipulated needing close reopen file process wishes apply shared lock read information determine update required apply exclusive lock update file arequest lock process block lock immediately obtained instances unsatisfactory process check lock present require separate mechanism find information aprocess locking request return error lock immediately obtained conditionally request lock daemon processes service spooling area instance daemon locks directory spooling takes place daemon processes easily check active daemon 
exists locks exist locking processes exist lock files neverbeleft active processes exit system crashes deadlock detection attempted deadlock detection system file lock applied lock type twosuccessive calls apply lock type fail symbolic links traditional unix file system multiple directory entries file system single file directory entry links afile sname inode contents link concept fundamental inodes reside directories exist separately referenced links links inode removed inode deallocated style referencing inode allowreferences physical file systems support inter-machine linkage avoid limitations symbolic links similar scheme multics feiertag added asymbolic link implemented file pathname system encounters symbolic link interpreting component pathname contents symbolic link prepended rest pathname interpreted yield resulting pathname unix pathnames relative tothe root file system hierarchy orrelative toaprocess scurrent working directory pathnames relative tothe root called absolute pathnames pathnames relative tothe current working directory termed relative pathnames asymbolic link absolute pathname absolute pathname contents symbolic link evaluated relative tothe location link file hierarchy programs aware symbolic link pathname theyare howevercertain system utilities detect manipulate symbolic links system calls provide ability detect read write symbolic links sevensystem utilities required calls future berkeleysoftware distributions file systems located remote machines pathnames occurs create symbolic links span machines rename programs create newversion existing file typically create newversion temporary file rename temporary file target file unix file system renaming required calls system program interrupted system crashed calls target file left temporary eliminate possibility rename system call added rename call rename operation fashion guarantees afast file system unix smm existence target rename works data files directories renaming directories system special validation checks insure directory tree structure corrupted creation loops inaccessible directories corruption occur parent directory movedinto descendants validation check requires tracing descendents target directory insure include directory moved quotas unix system traditionally attempted share resources greatest extent anysingle user allocate space file system environments unacceptable aquota mechanism added restricting amount file system resources user obtain quota mechanism sets limits number inodes number disk blocks user allocate aseparate quota set user file system resources givenboth hard soft limit program exceeds soft limit warning printed users terminal offending program terminated exceeds hard limit idea users stay belowtheir soft limit login sessions theymay resources theyare actively working toencourage behavior users warned logging theyare overany oftheir soft limits users fails correct problem manylogin sessions eventually reprimanded soft limit enforced hard limit acknowledgements robert elz ongoing interest newfile system adding disk quotas rational efficient manner wealso acknowledge dennis ritchie suggestions modifications user interface weappreciate michael powell sexplanations howthe demos file system worked manyofhis ideas implementation special commendation peter kessler robert henry acting likereal users early debugging stage file systems stable theyshould criticisms suggestions reviews contributed significantly coherence paper finally sponsors national science foundation grant mcs defense advance research projects agency dod arpaorder monitored navalelectronic system command contract -creferences almes almes robertson extensible file system hydra proceedings international conference software engineering ieee bass bass implementation description file locking onyx systems trimble san jose jan feiertag feiertag organick multics input-output system proceedings symposium operating systems principles acm oct ferrin ferrin performance robustness improvements version unix computer graphics laboratory technical report school pharmacy california san francisco january presented winter usenix conference santa monica california ferrin ferrin performance issuses vmunix revisited login usenix association newsletter vol november kridle kridle mckusick performance effects disk subsystem choices vax systems running bsd unix computer systems research group smm fast file system unix dept eecs berkeley technical report kow alski kow alski fsck unix system check program bell laboratory murray hill march knuth knuth art computer programming volume sorting searching addison-wesleypublishing companyinc reading mass maruyama maruyama smith optimal reorganization distributed space disk files cacm nov nevalainen nevalainen vesterinen determining blocking factors sequential files heuristic methods computer journal aug pechura pechura schoeffler estimating file access time floppydisks cacm oct peterson peterson concurrent reading writing acm transactions programming languages systems acm jan powell powell demos file system proceedings sixth symposium operating systems principles acm nov ritchie ritchie thompson unix time-sharing system cacm july smith smith input output optimization disk architectures survey performance evaluation jan smith smith bibliographyonfile system optimization related topics operating systems review oct symbolics symbolics file system symbolics desoto ave chatsworth aug thompson thompson unix implementation bell system technical journal part july-august thompson thompson spice file system carnegie-mellon department computer science pittsburg cmu-cssept trivedi trivedi optimal selection cpu speed device capabilities file assignments journal acm july white white disk storage technology scientific american august 

erential power analysis paul kocher joshua benjamin jun cryptography research market street floor san francisco usa http cryptography e-mail fpaul josh beng cryptography abstract cryptosystem designers frequently assume secrets manipulated closed reliable computing environments actual computers microchips leak information operations process paper examines speci methods analyzing power consumption measurements secret keys tamper resistant devices discuss approaches building cryptosystems operate securely existing hardware leaks information keywords erential power analysis dpa spa cryptanalysis des background attacks involvemultiple parts security system cult predict model cipher designers software developers hardware engineers understand review work security assumptions made level system design incomplete unrealistic result security faults involveunanticipated interactions components designed erent people manytechniques designed testing cryptographic algorithms isolation erential cryptanalysis linear cryptanalysis exploit extremely small statistical characteristics cipher inputs outputs methods studied applied analyzing part system architecture algorithm mathematical structure correct implementation strong protocol necessarily secure failures caused defective computations information leaked secret key operations attacks timing information data collected invasive measuring techniques havebeen demonstrated government invested considerable resources classied tempest program prevent sensitive information leaking electromagnetic emanations introduction power analysis modern cryptographic devices implemented semiconductor logic gates constructed transistors electrons silicon substrate charge applied removed transistor gate consuming power producing electromagnetic radiation measure circuit power consumption small ohm resistor inserted series power ground input voltage erence resistor divided resistance yields current well-equipped electronics labs equipment digitally sample voltage erences extraordinarily high rates ghz excellent accuracy error devices capable sampling mhz faster transferring data bought simple power analysis spa technique involves directly interpreting cryptographicoperations spa yield information device operation key material figure spa trace showing entire des operation trace refers set power consumption measurements cryptographic operation millisecond operation sampled mhz yields trace points figure shows spa trace typical smart card performs des operation note des rounds visible figure spa trace showing des rounds figure detailed view trace showing rounds des encryption operation details des operation visible -bit des key registers rotated round left arrow round arrows figure small variations rounds perceived discernable features spaweaknesses caused conditional jumps based key bits computational intermediates figure shows higher resolution views trace showing power consumption regions clock cycles mhz visible variations clock cycles result primarily erences power consumption erent microprocessor instructions upper trace figure shows execution path spa feature jump instruction performed lower trace shows case jump point divergence clock cycle visible figure spa trace showing individual clockcycles spa reveal sequence instructions executed break cryptographic implementations execution path depends data processed des key schedule des key schedule computation involves rotating bit key registers conditional branch commonly check bit shifted end bits wrapped resulting power consumption traces bit bit erent spa features execution paths takedi erentbranches des permutations des implementations perform variety bit permutations conditional branching software microcode signi power consumption erences bits comparisons string memory comparison operations typically perform conditional branch mismatch found conditional branching large spa timing characteristics multipliers modular multiplication circuits tend leak great deal information data process leakage functions depend multiplier design strongly correlated operand values hamming weights exponentiators simple modular exponentiation function scans exponent performing squaring operation iteration additional multiplication operation exponent bit equal exponent compromised squaring multiplication operations erentpower consumption characteristics erent amounts time separated erent code modular exponentiation functions operate exponent bits time mayhave complex leakage functions preventing spa techniques preventing simple power analysis generally fairly simple implement avoiding procedures secret intermediates keys conditional branching operations mask manyspacharacteristics cases algorithms inherently assume branching require creative coding incur performance penalty microcode microprocessors large operand-dependent power consumption features systems constant execution path code spa vulnerabilities hard-wired hardware implementations symmetric cryptographic algorithms ciently small power consumption variations spa yield key material erential power analysis des implementations addition large-scale power variations due instruction sequence ects correlated data values manipulated variations tend smaller overshadowed measurement errors noise cases break system statistical functions tailored target algorithm widespread data encryption standard des examined detail rounds des encryption algorithm performs box lookup operations sboxes takeasinputsix key bits exclusive-ored bits register produce output bits output bits reordered exclusive-ored halves exchanged detailed description des algorithm dpa selection function ned computing bit des intermediate beginning round ciphertext key bits entering box bit represented note incorrect evaluating yield correct bit probability ciphertext implementthedpa attack attacker rst observes encryption operations captures power traces samples addition attacker records ciphertexts noknowledge plaintext required dpa analysis power consumption measurements determine key blockguessk correct attackercomputes ak-sample erential trace nding erence average traces average traces istheaverage ect due represented selection function power consumption measurements point incorrect bit computed actual target bit half ciphertexts selection function ectively uncorrelated computed target device random function divide set subsets erence averages subsets approach subset sizes approach nity incorrect lim trace components uncorrelated diminish causing erential trace actual trace completely incorrect mayhaveaweak correlation correct correct computed equal actual target bit probability selection function correlated bit manipulated round result approaches ect target bit power consumption data values measurement errors correlated approach power consumption correlated data bit values plot spikes regions correlated values processed correctvalue ofk identi fromthe spikesin erential trace values correspond box providing con rmation key block guesses finding yields entire -bit round subkey remaining key bits found easily exhaustive searchorby analyzing additional round triple des keys found analyzing outer des operation rst resulting key decrypt ciphertexts attacking des key dpa plaintext ciphertext encryption decryption keys figure shows traces prepared plaintexts entering des encryption function smart card top power trace showing average power consumption des operations erential traces rst produced correct guess lower traces produced incorrect values traces prepared samples signal visible erential trace modest amount noise figure dpa traces correct incorrect power figure shows average ect single bit detailed power consumption measurements top power consumption trace center trace shows standard deviation power consumption measurements finally lower trace shows erential trace prepared note regions correlated bit order magnitude closer indicating noise error remains size dpacharacteristic times standard deviation observed point rise standard deviation clock cycle coinciding strong characteristic operand signi ect instruction power consumption considerable variation operand values manipulated low-level instructions manipulate bits selection function simultaneously select values multiple bits resulting dpa characteristics tend larger peaks 
necessarily signal-to-noise ratios fewer samples included averaging figure quantitativedpa measurements sources introduce noise dpa measurements including electromagnetic radiation thermal noise quantization errors due mismatching device clocks sample clocks additional errors finally uncorrected temporal misalignment traces introduce large amount noise measurements improvements applied data collection dpa analysis processes reduce number samples required circumventcountermeasures helpful correct measurement variance yielding signi cance variations magnitude variant approach automated template dpa des keys fewer traces smart cards sophisticated selection functions importance high-order dpa functions combine multiple samples trace selection functions assign erentweights erent traces divide traces categories selection functions defeat manycountermeasures attack systems partial information plaintexts ciphertexts data analysis functions ordinary averaging data sets haveunusual statistical distributions erential power analysis algorithms public key algorithms analyzed dpaby correlating candidate values computation intermediates power consumption measurements modular exponentiation operations test exponent bit guesses testing predicted intermediate values correlated actual computation chinese remainder theorem rsa implementations analyzed ning selection functions crt reduction recombination processes general signals leaking asymmetric operations tend stronger symmetric algorithms high computational operations result implementing ectivespa challenging dpa break implementations symmetric asymmetric algorithm wehaveeven technique reverse-engineer unknown algorithms protocols dpa data test hypotheses device computational processes mayeven automate reverse-engineering process preventing dpa techniques preventing dpa related attacks fall roughly categories rst approach reduce signal sizes constant execution path code choosing operations leak information power consumption balancing hamming weights state transitions byphysically shielding device signal size reduction generally reduce signal size attacker nite number samples perform dpa heavily-degraded signal practice aggressive shielding make attacks infeasible adds signi cantly device cost size approachinvolves introducing noise power consumption measurements signal size reductions adding noise increases numberofsamples required attack possibly infeasibly-large number addition execution timing order randomized designers reviewers approach temporal obfuscation great caution techniques canbeusedtobypass compensate ects vulnerable products passed reviews data processing methods safety disable temporal obfuscation methods review certi cation testing nal approachinvolves designing cryptosystems realistic assumptions underlying hardware nonlinear key update procedures employed ensure power traces correlatedbetween transactions simple hashing -bit key sha ectively destroy partial information attacker gathered key similarly aggressive exponent modulus modi cation processes public key schemes prevent attackers accumulating data large numbers operations key counters prevent attackers gathering large numbers samples leak-tolerant design methodology cryptosystem designer dene leakage rates functions cryptographycan survive leakage functions analyzed oracles providing information computational processes data leakage rate upper bound amountof information provided leakage function implementers leak reduction leak masking techniques needed meet speci parameters finally reviewers verify design assumptions correspond physical characteristics completed device related attacks electromagnetic radiation issue devices pass keys secret intermediates bus simple radio detect strong signals cryptographic devices wide variety signal measurement techniques superconducting quantum imaging devices show promise statistical methods related spaanddpa signals noisy data conclusions power analysis techniques great concern large number vulnerable products deployed attacks easy implement low cost device non-invasive making cult detect dpa automatically locates correlated regions device power consumption attack automated information target implementation required finally attacks theoretical limited smart cards lab power analysis techniques extract keys form factors reliable solution dpa involves designing cryptosystems realistic assumptions underlying hardware dpa highlights people design algorithms protocols software hardware work closely producing security products anderson kuhn lowcostattacks tamper resistant devices security protocol workshop april http cam ftp users rja tamper anderson kuhn tamper resistance cautionary note usenix workshop electronic commerceproceedings november biham shamir erential cryptanalysis data encryption standard springer-verlag biham shamir erential fault analysis secret key cryptosystems advances cryptology proceedings crypto springer-verlag august boneh demillo lipton importance checking cryptographic protocols faults advances cryptology proceedings eurocrypt springer-verlag jameco electronics pc-multiscope part february catalog kocher timing attacks implementations e-hellman rsa dss systems advances cryptology proceedings crypto springerverlag august matsui experimental cryptanalysis data encryption standard advances cryptology proceedings crypto springer-verlag august national bureau standards data encryption standard federal information processing standards january national institute standards technology secure hash standard federal information processing standards april dhem koeune leroux mestr quisquater willems practical implementation timing attack ucl crypto group technical report series cgr rivest shamir adleman method obtaining digital signatures public-key cryptosystems communications acm 
cactdactd ctb bxd cvcxd ctctd cxd crd cxd bxd crd cscxd cvd cfcxd bvba cxctcw cdd cxdactd cxd cdd cpcw dbcxd bscrd bad cpcwbactcsd bwcpdbd caba bxd cvd ctd cbd cpd cud cdd cxdactd cxd ctd cvd ctd bscrd bad cpd cud csbactcsd bzd csd cpd bucpcrcz cdd cxdactd cxd cdd cpcw cvcqcpcrczbscrd bad cpcwbactcsd btcqd cpcrd bucxd cpd crcwcpd cscxd cpd ctd cqd ctd cyd cxd cxd crd cxd ctd cpd ctdcctcrd cpcqd crd csct ctdbd cxd ctd ctctcs cwcpdact cpd ctdcd cxcrcxd ctd ctd ctd cpd cxd cwd cpcrcwcxd cxd crb cxd cpd ctd crd csctcsba cdd cud cpd ctd ddb dbd cxd cxd ctd crd cscxd cvd cud cpd ctd cxd cxd crd cxd ctd cqdd cwcpd cxd cqd ctcscxd cpd ctd ctba cfct csctd crd cxcqct csctd cxdactb cwcpd ctdcd cpcrd cqcxd ctdactd cxd crd cxd ctd crd cscxd cxd cud cpd cxd cud cpd ctd cqd ctd cccwct ctd dacxcsctd csctd cxdact dbcxd cpd ctd cqd ddb ctdactd cxd cud cpd cxd cpcqd dacpd cxd cxd crb cxd bwctd cxdact cpd cpd cxcrcpd ctdactd ctb ctd cvcxd ctctd cwct ctd crd cscxd cvd cud cwd cxd crd cxd cud cpd cpd ctd cqd ctd cqdd cuctctcscxd cxd ctd cpd cxd cxd crd cxd cpd cpd cpd dddecxd cwct ctd cxd cpcrcwcxd crd csctba bwctd cxdact dactd cwct ctd cxd ctc cbb cbc btcabvb btd cwcpb cpd dbctd cxd crd cxd ctd cpd cpd cpd cwct btcac cpd dcbkbi cxd crd cxd ctd crd cxd csctcrd cpd cpd cxd cwcpd crcpd cqct ctcs cqdd cqcxd cpd ccd csctd cpd cwct cxd cxd ddd csctd cxdactb dbctcwcpdact cqd cxd crd csct ctd cxd ctd cvctd ctd cpd cwcpd cpczctd csctd cxdactb cpd csd crctd bvd cpcrd cud crd csct ctd cxd cxd dbcwcxcrcwdbctcwcpdactd cwctd ctcs ctdbd cxd cpdacpc cqcpcrczctd csba csd crd cxd bucxd cpd crcw cpd cpd ctd cqd ctd csctcqd cvcvctd cscxd cpd ctd cqd ctd csddd cpd cxcr crd csct cvctd ctd cpd cxd ddd ctd cjbfb bkb bdbcb bdbhb bdbkclb cyd cxd cxd crd cxd ctd cjbgb bjb bdbeclb cpd ctdcctcrd cpcqd crd csct ctdbd cxd ctd cjbdbgb bebdb bebgcl ctctcs crd cpcxd ctd ctd ctd cpd cxd cwd cpcrcwcxd cxd crd cxd cpd ctd crd csctcsba byd ctdccpd ctb ctctcsd czd dbd cwcpd cwct dcbkbi cxd crd cxd cpcscsd ctcrdcb ctcqdc crd ctd csd cwct cqcxd bcdcbcbdcrcq cud ctcs cqdd cqcxd dbcxd cad cud cwct cpcscsd crd csct bcdcbcbdcrbc dbcxd bcdcbk cud cwct ctcrdc cpd cvd ctd cpd bcdcbf cud cwct ctcqdc cpd cvd ctd cdd cud cpd ctd ddb ctcrcxcuddcxd cxd crd cxd ctd crd cscxd cvd dbcxd crd ctd ctd cxd ctd czcxd cpd csctd cpcxd cxd cwct abd ctd cxdectd cpd dacpd ctd cpd cxd crd cxd acctd csd cdd cxd cxd cvd ddb cwcxd crctd cxd cqd ctd cpd ctcscxd ctd ctcrcxcpd cud bvc cbbv cpcrcwcxd ctd crcw cpd cwct dcbkbiba bvd ctd ddb ddd ctd cqd cxd csctd cpd crd cxcqct cxd btd cwd dactd cxd cwcxd cpd ctd cwcpd csctd crd cxcqctcs cwct csctb cxdact dactd cpd ctcpd ctcs cxd cwct crctctcscxd cvd cwct bwchc btc bebcbcbc dbd czd cwd cjbiclba crd cxd ctd crd cscxd cvd cud cpd cpd crcwcxd ctcrd ctcuctd ctd crct cpd cpd cfct cwcpdact cqd cxd crcpd ctcs csctd cxdact cwcpd ctd cxd cxd cpd ctd cwct ctctcs ctcrcxcudd cpd cwct cqcxd ctdactd csctd cpcxd cxd crd cxd cpddd cccwct ctd cxctd csctb cxdact dbcxd cpd cpd ctd cqd ddb ctdactd csctd crd cxd cxd cpd cxd crd cxd ctd cxd crd cxd cpd ctd cpd ctd cpd ddd ctd ctcvcxd ctd cxd ctcscxcpd ctd cpcqctd bwctd cxdact csctd crd cxd cxd cwd ctcpcrcw cxd crd cxd cxd ctd crd csctcsb dbcwcxcrcw cxd cvcxdactd cxd cwct cud crd csctcrd cpd cpb cxd cccwctd ctd csd ctd ctcrcxcudd cqcxd cpd ddb ctdactd csctd cpcxd crcwcpd ctd cpd dbcxcsd cwd ctd cpd abd ctd crd csct dacpd ctd cpd ctcvcxd ctd dacpd ctd crd cscxd cvd btd ctd cwct ctd cxcpd cud cxd ctcrcxaccrcpd cxd cqddd cwctd ctd cxd ctd cwcpd cwcpd dbcxd cwctd bwctd cxdact cxd cqcpd ctcs cxd cqd ctd dacpd cxd dacxd cpd cpd cpd crcwcxd ctcrd ctd cud dbcwcxcrcwcpd cvd cpd ctd ctctcsd cqcxd cpd ctd crd cscxd cvd dbcxd cpd ctcpcsdd cwcpdactd cvd cpd cpd ctd cqd ctd cwcpd crd cpcxd cwcxd cxd cud cpd cxd cccwctd ctcud ctb dbct cwd cqct cpcqd ctdcd cpcrd cwct cxd cud cpd cxd cud cwct cpd ctd cqd ctd dbcwcxcrcw cxd dbcwcpd csctd cxdact csd ctd btd cwcxcvcw ctdactd csctd cxdact csd ctd cqdd cuctctcscxd ctd cpd cxd ctcpcrcw cxd crd cxd cwct ddd ctd cpd ctd cqd ctd cpd csd cxd ctd cpd cxd dacxd cwct cpd ctd cqd ctd csctd ctd cxd cwd cwct cxd crd cxd cxd ctd crd csctcs cxd crd csct cpd cxd ctd cpd ctd crd cscxd cvd cccwctcsctd cxdactcxd ctb ctd cpd cxd dactd cwct ctd cxd cbb cbc btcabvb btd cwcpb cpd dbctd cxd crd cxd ctd cwcpd csd ctd cwct btcac cpd dcbkbi cxd crd cxd ctd cxd csd ctd ddctd cwcpd csd cxd crd cxd crcw cpd cwd dbcxd crd cxd acctd csd bwctd cxdact csd crctd cscpd crd csctcrd cpd cpd cxd cwcpd csctd crd cxcqct cwd cpd cxd crd cxd ctd cxd ctd crd csctcsba dbd cqct cscx crd csd crct csctcrd cpd cpd cxd cxd cwctd cpd cvd cpcvctd cccwctd csctcrd cpd cpd cxd crcpd cqctd ctcs cqdd cwcpd cxd ctd ctd cpd cxd cpd cqcxd cpd cxctd btd cpd ctdccpd ctb dbctcwcpdact cqd cxd crd csct ctd cxd ctd cvctd ctd cpd cwcpd cpczctd cwct ctdactd ctb ctd cvcxd ctctd ctcs csctcrd cpd cpd cxd cpd cvctd ctd cpd ctd cpcrd cud cucpd crd csct ctd cxd cxd ccd csctd cpd cwct cxd cxd cwctd dbct cwcpdact ctdbd cxd ctd cpabctb cjbebhcl crd cxd ctd cwctd cpcrd cdd cxd cpd cpd cxcrcpd cvctd ctd cpd ctcs crd csct ctd cxd ctd ctcsd crctcs cwct cxdect cwct cpabct 
cqcpcrczctd csctd crd cxd cxd cqddbgbcb cpd cxcsctd ctb csctd cxdact crcpd cqct dacxctdbctcs cpd cpd cpd ctd cqd ctd ctd ctd buctcrcpd ctd cucwd dbcxd ctdactd ctb ctd cvcxd ctctd cxd crd cxd cxd crcpd cxcrczd acd cscxabctd ctd crctd cqctd dbctctd dbcwcpd cpd cpd crcwcxd ctcrd ctd cpd cpd cpddd cpd dbcwcpd cpd cpd ctd cqd ctd cpcrd cpd cxd ctd ctd cbctcrd cxd dbct cscxd crd ctd cpd ctcs dbd czbm bvd cqctd cvb crd cxd ctd ctdactd ctb ctd cvcxd ctctd cxd ddd ctd cpd cwct ctdb ctd ctdd cpcrcwcxd ctb bvd csct ccd czcxd cbctcrd cxd csctd crd cxcqctd cpd cxd cpcqd cxd crd cxd ctd ctd crd cscxd cvd cpd ctdcd cpcxd cwd csctd cxdact dbd czd cbctcrd cxd cscxd crd ctd cwct crd csct ctd cxd ctd cvctd ctd cpd cwcpd dbctcwcpdactcqd cxd csctd cxdactba cbctcrd cxd cpd cxdectd crd crd cxd cpd csctd crd cxcqctd cud dbd czba btd ctd cscxdc cvcxdactd crd ctd cxd ctcrcxaccrcpd cxd cwct cxd crd cxd ctd cud csctd cxdactba cactd cpd ctcs cfd cccwct cxd cpd ctcrctcsctd csctd cxdact cxd bvd cqctd cvb dbd ctdactd ctb ctd cvcxd ctctd cxd crd cxd ctd cjbdclba bvd cqctd cvb ddd ctd csd ctd ckd ctdactd cxd ctd ctd cpd cxd cxd cuctd dbcwcpd cpd ctd cqd cxd crd cxd cwd cqct ctcs cxd ctd ctd cwcxcvcwb ctdactd cpd cvd cpcvctba cxctcrctd cpd ctcs cpd ctd cqd ddcrd csctd ctct cwd cwct ctd cpd cxcrd cwct cxd crd cxd crcwcpd cvctd cxd dbd cxd cpcxd ctcs cpd cqd cxd csb cxd cpd cpd cxcrcpd ctd cpd cvctd cxd crd cxd ctd dbcwctd cwct csctd crd cxd cxd cwct ctd cpd cxcrd cpd ctd cqd cxd crb cxd cpd ctcs cpd cpd cxcrcpd ctd cpd cvctd cwct bubxbz cqcpcrczb ctd cvctd ctd cpd cjbhclba bwctd cxdact cxd crd ctd ctd cpd bvd cqctd cvb dbd czba bwctd cxdact crd ctd cxd cud cpd cxd cwcpd crcpd cqctd ctcsd cqddd cpd cwct cpd ctd cqd ctd dbcwcxcrcw cxd cxd cpd cud cqd cxd csb cxd ccd cpd cqcxd cpd cpd cxd cpd cxd bwctd cxdact cpcscsd ctd ctd cxd ctd cqd ctd csd cpcxd cwcpd bvd cqctd cvb ddd ctd cxd crct cxd ctdactd ctb ctd cvcxd ctctd ddd cpdc cpd cud cpd cxd cpd ctd cpd cxcr cpd cud cpd cxd cwct cwctd cwcpd csb cqd ctd csd cpcxd cxd crcwd cpcrd cpcqd ctbm cxd cpczctd cwct csctd cxd ctd cwd cud csctd cxdact ctdactd ctb ctd cvcxd ctctd cpd cxd crd cxd ctd ctd crd cscxd cvb dbcwctd ctcpd bvd cqctd cvb ddd ctd cpczctd cscpddd ctdactd ctb ctd cvcxd ctctd cpd cxd crd cxd ctd ctd cpd cxcrd cccwct dbd dbcwd cvd cpd crd ctd cpd crcw csctd cxdact cxd cwct ctdbc ctd ctdd cpcrcwcxd ctb bvd csct ccd czcxd ccb cjbebcclba cccwct cpd cpd cxcrcpd cvctd ctd cpd ctd cxd ctd cpd cxd cpd cpcrcwcxd ctb crd csct cud ctd ctcrcxaccrcpd cxd dbd cxd ctd cxd cpd cvd cpcvctcrcpd ctcs cbc bxbwba cbc bxbw ctcrcxaccrcpd cxd cpd ctdccpcrd csctd crd cxd cxd cxd crd cxd cpddd dbd cxd ctd cpd ctdactd cpd ctdactd cpcqd cpcrd cxd btd cwct dbctd ctdactd csctd crd cxd cxd cbc bxbw ckacctd csd cpd ctcs csctd crd cxcqct cxd crd cxd cqcxd acctd csd byd ctdccpd ctb cwct csctd crd cxd cxd cwct cbc btcabv cxd crd cxd acctd csd cxd cbc bxbwd ctcpcsd cpd cud dbd cjbdblclbm cxd bcbmbfbd bfbcbmbfbd cscxd bfbc bcbmbebl bebhbmbebl bebebmbebg cxd bebe bcbmbebd beblbmbebl crd bebhbmbebk cscxd bebe bcbmbebd bdblbmbebg bdbgbmbdbk bdbfbmbdbf cpd bhbmbdbe bcbmbg cxd bdbf bcbmbdbe bhbmbdbf bwctd cxdact crd cscqctd ctcsd ctd cxd cxd cpd cwct acctd ctdactd ctcrcxaccrcpd cxd cxd ctcrcxaccrcpd cxd cdd ctd csctd cxdact ctctcs dbd cpcqd cpd cxd ctd cucpcrct cwcpd cxd cpd cwct ctdactd cpd ctd cqd cpd cvd cpcvct cwct ctdactd cpd dbcwcxcrcw ccb ckcrd crd cpd ctdbd cxd ctd bacactd dacxd crcwctdcd ctdactd csctd cpcxd ctd cxd cxd cpd ctd ctd cxcpd crctd ctcrcxaccrcpd cxd ctd cccwct cpd cwd cwct cpd ctd cqd ctd csd ctd cxd ctcrcxacctcs ctd crd cscxd cvd cjblclb cqdd crcwd cxd cpd csd dacpd ctd cud dbcxd cwcxd cwct cpcrct cuctd crd cscxd cvd cfct cwct crctd cpd cpd cwct cpd ctd cqd ctd csctd cxdact ctd crd cscxd cvd buctcrcpd ctd ctcrcxacctd cwct dacpd cxb crd cpd ctd ctd crd cscxd cvd cwct ctcrcxaccrcpd cxd ctd ctd csd ctd ctctcs ctcpd crcw cpd crcw cwct ctd crd cscxd cpcrct cpd csctd cxdact csd ctd ctdactd cpd cuctcpd ctd cwcpd csctd cxdact csd ctd crcwcpd cud ctd crcpd cxd byd cpd cvctd crcw cpd ccd dbcwcxcrcwcpd cxd cpd cxd ctd ctd ctd crcpd cxd cxd cpd cxd ctba btd cpd cwctd ctdccpd ctb cccrcpd cwcpd csd ddd cwctd cxcr cpd ctd cqd cxd crd cxd crd ctcpd ddba cccwct crd ctd cxd ctd ctd cpd cxd csctd cxdact csd ctd ddctd cwcpd csd crd ctdc ddd cwctd cxcr cxd crd cxd dbctd cwd dbctdactd dbct crd ctcpd cxd cpddctd cwcpd csctd crd cxcqctcs ddd cwctd cxcr cxd crb cxd csctd cxdactba ctd ctd cpd cxd bwctd cxdact cpczctd cwcxcvcwb ctdactd csctd crd cxd cxd cpd cxd crb cxd ctd cpd cpd cpd ctd cqd ctd cpd cvctd ctd cpd ctd cwctcpcsctd acd ctd cwcpd csctd crd cxcqct cwct cxd crd cxd ctd bycxcvd cxd cpd ctd cwct crcwcpcxd cud csctd cxdactba cxcscr cxd csctd crd cxcqctcs cxd cbctcrd cxd bfbabdbn cwct dactd cxd csctd crd cxcqctcs cxd cbctcrd cxd bfbabebn cpd cwct crd csct ctd cxd ctd cvctd ctd cpd cxd csctd crd cxcqctcs cxd cbctcrb cxd bgba bwctd cxdact ctdcd cpcrd cwct ctd crd cscxd cuctcpcrcw cxd crd cxd cqddd cpd cxd cpd cqctd cxcqd cxd ctd cfctcpd ctcpcrcw cxd crd cxd cpczctd acdcctcsd cqctd acctd csd cpd ctd cpd csd cfctcpd cwcpd acctd csd cwcpdactd cwd ctct ddd ctd ctcvcxd ctd cxd ctcscxcpd ctd cpd cpcqctd bbcyd cpd cvctd cactcvcxd ctd cpd cpd cpd ctd ctdcd cpd cpd ctd 
cxd ctcscxcpd ctd cpd cxd ctcvctd dacpd ctd cwcpd crcpd cpd cvct dactd cpd cvct ctd cud cxcqcxd cxd cxctd ctbacvbab cwct dcbkbib cxd crd cxd cpczctd ctdactd cpd bfbeb cqcxd cxd ctcscxcpd ctd cpd cpb cqctd cpd ddd cqd cxcr cxd crd cxd cpcscsd ctd ctd cccwctd cpd cxd cpd crcxctd csctd crd cxcqct cwct cxd crd cxd ctd cud dbcwcxcrcwdbctcwcpdact dbd cxd ctd ctcrcxaccrcpd cxd dbcwcxcrcw crd dactd dbcxcsct cpd cvctd csctd cbbtd cfct cpczct cwct cud dbcxd cpd cxd cpcqd cxd crb cxd ctd crd cscxd cvd bdba btd ctd cqd ctd crd cscxd cvd cpd cpd ctd ctd cpd cud cpb cxd cccwcpd cxd cpd cxcrd cpd cxd crd cxd cpd dbcpddd cwcpd cwct cpd ctd crd cscxd cvba cpcscscxd cxd cpd ctd cqd cxd dbd crd ctcrd cxd crd cxd cvctd cwctd cvcxdactd cwct cpd ctd cpd crd crcpd ctd cpd cxd cwct ctd cpd instruction description idc derive solver encoding description system assembler binary rewriter disassembler debugger code emitter generator bycxcvd ctbdbm btd ctdccpd crcwcpcxd cwcpd ctd csctd cxdactba cfctcwcpdact cqd cxd cwct cwcpd cpd ctcxd csd cqd cqd dcctd ctd cqd cxd cwctd ctd cpd cpd ctd ddba ccd cpd cxd cudd cwcxd cpd cxd cpd ctd cqd ctd cqct ctdactd ctcs cud cpcscscxd cvb ctd dacxd cvb ctd csctd cxd cxd crd cxd cxd cvctd ctd cpd cwct ctd dacxcsct csctd cxdact dbcxd cwct cpd cxcpd cpd ctd cqd cscxd ctcrd cxdactd cccwcxd cpd cxd ctd cqcpd crcw cvctd cwctd cwct cpd ctd cqd cpd cxd crd cxd cxd acd ctb dbcwcxcrcw ctcsd crctd dacxd cxd cqddcpd cpd cvct crd cpd byd ctdccpd ctb cpcrcwcxd csd cpd crctd bibcbc ctd cxd dbct crcpd cpd ctd cqd bebcbcbc cxd crd cxd cxd cyd cpcqd dbcxcrct cpd crcw cxd cpd dbct crcpd cpd ctd cqd cxd crd cxd dgd crctd crd ctcpd cxd cpd acd bbc csd cxd cpd cwct crd cucpd ctd cqd ddba beba cfctcpd cwcpd cwctdacpd cxcscxd cpd ctd crd cscxd dacpd cxd acctd csd ctd csctd ctd cwct dacpd cpd ddd cwctd acctd csba cfctcrcpd cwd crd acctd csb ctd crd cscxd cqdd ctd ctd cpd cxd ctdactd ctcvcpd dacpd cud cxd dbcwcxd cwd cscxd cpd cwctd acctd csd crd cpd cccwcxd cpd cxd cpd dbd dactcud ctcpcrcw acctd ctd cpb cpd ctd ddba cbd cxd crd cxd cxd crctd cpcxd cxd crd cxd ctd crcw cpd cwct btcac csd cpd cxd cudd cwcxd cpd cxd cxd dbcwcxcrcwcrcpd csctd cxdact csd cpd ctdcd ctd cxdact ctcpd crcwba bfba cccwct acctd cud cpd cqcxd cxd ctcscxcpd dbcwd ctcwcxcvcwcqcxd cxd ctd cxd cxd csctd ctd csctd cud cwctd cqcxd cud cwctcxd ctcscxcpd ctba cccwcpd cxd crd cpd cqctd dbctctd cpd bdcrcpd cpd cqct ctd crd csctcs cxd cwct cpd cxd crd cxd acctd csba cccwcxd cpd cxd ctd dact cud cxd ctb cscxcpd acctd csd cxcrczd ddbn cwctd dbcxd ctb dactd dbd cwcpdactd ctd ctd cpd cpd ctcvcpd ctd cpd dacpd ctd bgba ctcscxcpd ctd cpd ctd crd csctcs dbcxd ckd cxd ctay cpd cud cpd cxd ckcbcxd ctay ctcpd cwcpd cpd cxd ctcscxb cpd ctb dacpd crcpd cwcpdactd ctcpcscxd cpcxd cxd dectd ctb dactcsb crd cpd cqd cpcrd ctcs cud cxd ctcpcscxd cqcxd cpcscsctcs cud cxcvd ctdcd ctd cxd csctd cxdact csctd ctcrd crd ctdc cpd cud cpd cxd cwcpd cxd crcpd ctd ctd ctd crcw cpd cwct crcpd cucpcrd cxd dcbkbi ctd cxd crd cxd dbcwcxcrcw cxd ctd crd csctcs cpd cwct cvcpd cxd cwd cwct crcpd cucpcrd cxd cxd ctcscxcpd ctd cpd ctd cwct ctd dbcwd crcpd dacxcsct csctb cxdact dbcxd cwct cpd cud cpd cxd ctdcd cxcrcxd ddba cxd ctd ctd cpd cxd crd ctd cpczctd cwct cpd cxd cwcpd ctcvcpd cxdact cxd ctcscxcpd ctd cpd ctd crd csctcs cxd dbd crd ctd ctd cwcxd cpd cxd crd cqct crcwcpd cvctcs cucpcxd ctcpd cxd ddba bfbabd cbd ctcrcxcuddcxd crd cxd cccwct cud ctd csd csctd cxdactb cxcscrb cpd cpd ctd cpd ctd cqd ddb ctdactd cxd crd cxd ctd csctd crd cxd cxd cxd cwct cxd ctd ctcscxb cpd cud cwcpd csctd cxdact ctd cxcscr cxd cpcqd blbcbc cxd ctd ctdc cpd ddcpcrcr crd csctba cdd ctd csctd crd cxcqct cxd crd cxd dbcxd ddcpcrcrb cxczct csctd crd cxd cxd cwcpd crd cpcxd cxd cxd crd cxd cvctd ctd cpd cpd cud cpd csctd crd cxd cxd cwcpd csctd crd cxcqctd cwct cpd ctd cqd ddd cpdc cpd cwct ctd cpd ddd ctd cwcpd cwct cud cpd cpd ctcvd ctcs cqddcpd ctd cqd ddd cpdcb cqdd ctd crd cscxd crd cpd dbct cqctd cxctdact cwcpd cwct cud ctd cxd cucpd ctcpcscpcqd ctb cpd cxd ctdcd ctd ctd cvcxb crcpd ctd cpd cxd cwcxd cpd cwctd cwcpd ctd crd cscxd ctd cpd cxd cwcxd cccwct csctd crd cxd cxd cbc btcabv btc cxd crd cxd ctcpcsd cpd cud dbd cwcpd cwct acctd csb ctdactd cxd cud cpd cxd ctcrb ctd cpd dbcxd csd ctd cwcpdactd cqctd ctcrcxacctcs cqdd cwct ctd cxd ctcvd cvbcb cvbdb cvbeb cvbfb cvbgb cvbhb cvbib cvbjb bcb bdb beb bfb bgb bhb bib bjb bcb bdb beb bfb bgb bhb bib bjb cxbcb cxbdb cxbeb cxbfb cxbgb cxbhb cxbib cxbj cpd csb cpd cscrcrb cpd csd cpd csd crcrb crcrb crcrb dcd dcd crcrb dcd dcd crcrb cpb cpcscsb cpcscscrcrb cpcscsdcb cpcscsdccrcrb cpcscscrcrb cpcscscrcrd dab cqb cqcrcrb cqdcb cqdccrcrb cqcrcrb cqcrcrd dab crcrb crcrb crcrb cscxdab cscxdab cscxdacrcrb cscxdacrcrb cpdactb ctd cnbdbmcxd ctcvd cnbebmcxd ctcvd cncsctd bmcxd ctcvd cnbdbmcxd ctcvd cxd cncsctd bmcxd ctcvd cccwct csd crd cxd csctd crd cxcqctd cwct cpd ctd cqd ctd ddd cpdccpd cwct ddd ctd cwct ctd cpd csd cud cxd cxd crd cxd cpd ctd cwcpd cwctd ctcpd ctd ctcvcxd ctd dfd ctcvcxd ctd cpd ctcvcxd ctd cxd ctcscxcpd cud cpd cwct cxd crd cxd cxd ctcsba cccwct cpcrctcwd csctd cxd cscxcrcpd ctd dbcwctd cwct cxd crb cxd cpd cpd ctcpd cccwct ctd cpd ctcrcxaccrcpd cxd cpddd cwcpd cwct cud cpd cxd cqd cxd cocod bdbmcxd ctcvd cxd ctb cpcrctcs cqdd ctd cqctd cwct cxd ctcvcxd ctd cxd 
ctcvd cbctdactd cpd crd dactd cxd cpd ddbm ctcvcxd ctd acctd cpd ctd cqctcvcxd dbcxd cyd cpd cvctd cwcpd cwct cxd cpd cpcqctd cpd cpd cxd ctcscxcpd acctd cxd cpd ddd cwctd acctd csba bfbabe cccwct cbd dactd bwctd cxdact cxd crd ctcs cwd ctct dactd ctcpcrcw ctcrcxcpd cxdectcs csctd cxdact ctcrcxaccr ctd cpd ddd ctbm cwct ctcvcxd ctd dactd dactd cud ctcvcxd ctd acctd csd bbd ctd cpd csd cwct cxd ctcscxcpd dactd dactd cud cxd ctcscxcpd acctd csd cpd cwct cyd dactd dactd cud cyd cpd cvctd acctd csd bxcpcrcwd dactd ctd cwct cpd ctd cqd ctd crd cxd crd cxd ctd crd csb cxd cvd bwctd cxdact ctd cxd crd csct cxd cpd cpd ctd cqd ctd acd ctb cwct cpd ctd cqd ctd cpd acd csd cwct crd csct cxd cwct ctd cxd ctdcctcrd cpcqd ctba bwctd cxdact ctd cud cpd cscxabctd ctd crct cxd ctd cscxb cpd ctd cqctd dbctctd cwct cpd cvctd cpd cwct dacxd cpd crcwcxd ctcrb ctd cpd cpd cxcpd ctd dbcpd cqddd ctd cxd cwct cqcyctcrd crd csct cqctcud ctcsd cxd cpd ddd dacxd cvba ccd csctd cpd crcpd cwct cpcrcwcxd crd csct cxd cwct ctdcctcrd cpcqd ctb csctd cxdact ctdcd cxcrcxd ctd cxd cuctd crctd cpd cwct crd csct cxd cpd ctd cqd ctd cscpd cscxd ctcrd cxdactd cuctd crctd cpd cpd cpd csd crcwd ctd ctd ctd crct cucqddd ctd cxd cxczctd cwcpd csctd cxdact ctd cxd ctd ctd crct cxd crd cxd cwcpd cpd crcw cwct cuctd crctd cxd crct cxd csd ctd ctdccwcpd cxdactd ctcpd crcw cwct ctd cxd crd cxd dbct ctcpd crcw cwct ctd cxd cpcrct dacpd ctd cud ctcvcxd ctd acctd csd cpd cud cxd ctcscxcpd cpd cyd acctd csd btd cwd cvcw dbct csd crd ctd csd cxd dbcpdd csctd ctcrd cpd cpdad cxcs cuctd crctd crd adcxcrd dbd cqct dactctcpcrcw cxd crd cxd dbcxcrct dbcxd cscxabctd ctd cuctd crctd dacpd ctd bwctd cxdact ctd ctd ctd cxd crd cxd ctd crd cscxd cvd cpd cpd crd csct cpd cpd cpd cpd cqcxd cpd cqctd ctd cpd acctd csd cccwct crd csct cpd crd cpcxd cwct cqcxd cpd czd cubdb cwcpd cqct ctd cxd cpd cxd crd cxd ctcrcxcudd cvcxdactd crd csctba cactcvcxd ctd acctd csd cpd ctcrcxacctcs cpd ctd ctd crct cpd czd cud ctcpcrcwd ctcvcpd ctcvcxd ctd dacpd ctbn cxd ctcscxcpd ctd cpd cpcqctd acctd csd cpd ctd ctd ctd ctd ctcs cpd cxdectb cpd abd ctd cpd cxd cpd cud cccwct dactd cpd dbd cxd cwct cpd cqcpd cxcr cpd ctd bxcpcrcw crcpd ctd acctd cpd csctd ctd cxd ctd cxd cxdect cqdd ctd cxd cxd cxd crd cxd cud ctcpcrcwd ctcvcpd ctd cpd dacpd cwcpd acctd dbcwcxd cwd cscxd cpd cwctd acctd csd dacpd ctd cpd cvbj cvbi cvbcbn bcdcbkbcbcbl bcdccrbcbcbi cpd cvbj cvbi cvbdbn bcdcbkbebcbl bcdccrbcbcbi cpd cvbj cvbi cvbebn bcdcbkbgbcbl bcdccrbcbcbi cpd cvbj cvbi cvbfbn bcdcbkbibcbl bcdccrbcbcbi cpd cvbj cvbi cvbgbn bcdcbkbkbcbl bcdccrbcbcbi cpd cvbj cvbi cvbhbn bcdcbkcpbcbl bcdccrbcbcbi cpd cvbj cvbi cvbibn bcdcbkcrbcbl bcdccrbcbcbi cpd cvbj cvbi cvbjbn bcdcbkctbcbl bcdccrbcbcbi cpd cvbj cvbi bcbn bcdcblbcbcbl bcdccrbcbcbi cpd cvbj cvbi bdbn bcdcblbebcbl bcdccrbcbcbi cpd cvbj cvbi bebn bcdcblbgbcbl bcdccrbcbcbi cpd cvbj cvbi bfbn bcdcblbibcbl bcdccrbcbcbi cpd cvbj cvbi bgbn bcdcblbkbcbl bcdccrbcbcbi cpd cvbj cvbi bhbn bcdcblcpbcbl bcdccrbcbcbi cpd cvbj cvbi bibn bcdcblcrbcbl bcdccrbcbcbi cpd cvbj cvbi bjbn bcdcblctbcbl bcdccrbcbcbi cpd cvbj cvbi bcbn bcdccpbcbcbl bcdccrbcbcbi bababa bycxcvd bebm cccwct cpd ctd cqd cwcpd csctd cxdact cvctd ctd cpd ctd dactcud cwct cpd ctcvcxd ctd acctd cwct cbc btcabv cpcscs cxd crd cxd cpd cwct ctd cxd cqcxd cpd cxd crd cxd cwcpd cxd cpd cpd dddectd acdcctcsb cpd acd cscxd cpd cqcxd cwcpd crcwcpd cvct cxd cwct cqcxd cpd ctd crd cscxd cvba bycxcvd cxd cpd ctd cwcxd crctd cud cwct cpd ctd cpd cud cwctcbc btcabv cpd cxd crd cxd cccwd cqcxd cwcpd crcwcpd cvct cqctd cwct acctd csba abd ctd cxd cvcxdactd cqddd cwctd dbctd crcwcpd cvcxd cqcxd cxd cxdect cqdd cwct cscxcub cuctd ctd crct cqctd dbctctd cxd dbctd cpd cwcxcvcwctd cqcxd ctcrcxaccr ctd cpd csb dacpd ctdccpcrd ctd cpd cwct dacpd cwctd cqcxd dbcwctd cxd cxd ctcsba btd cwctd cqcxd cqctd cwctd acctd csd cwct crd csct cpd czba btd ctcpcrcwd dactd cxd cwct crd csct cpd cxd ctacd ctcsba cccwcpd cxd ctcpcrcwd dactd ckcrd cpcxd cqcxd cud dacpd cxd acctd csd btcud ctd cpd acctd csd cwcpdactcqctctd dactcs cud cwct crd csct cpd cxd ctd cwct ctd cpcxd cxd crd cpcxd ctcs cqcxd cxbactbab cwd cwcpd cpd ctd cxd ctdactd ctd cxd ctcs cxd crd cxd cfct cpczct cvctd ctd cpd cpd ctd cqd ctd csctd ctd csctd cpd cxd dbcwcxcrcwcxd cwcpd cwct cpd ctd cqd ctd dbcxd csd crct ctd ckdbcwctd ctdcd ctcrd ctcsbaay byd ctdccpd ctb dbct cpd cwcpd cpd ctd cqd ctd dbcxd ctd cpd ctd dbcwctd cxd ctcvcpd ctcvb cxd ctd cpd ctcs cxd cpd cxd crd cxd dbcwctd crd cpd cpd ctd cpd cvctba bwctd cxdact acd csd cwct cxdectd cxd ctcscxcpd acctd csd cqdd ctd cxd cpd cvctd cpd cpd cvctd cxd ctcscxcpd ctd cpd cxd ctdcd ctcrd cwcpd cwctcpd ctd cqd ctd dbcxd ctdactd cpd ctd cpd ctd ctd cpcvctba cdd cud cpd ctd ddb cpd ctd cqd ctd csd cpd dbcpddd ctd ctd dbcwctd cwctdd cwd csba bzcpd cwd dbd ctdcd ctcrd ctcs cqctcwcpdacxd cxd cwcpd cxd dbcxd cpcrcrctd cxd cxdact crd cpd cwcpd cpd cpd cvct cud cxcvd ctcs acctd csd cud ctdcb cpd ctb cud bdbib cqcxd cxcvd ctcs acctd cxd cpcrcrctd crd cpd cqctd dbctctd bfbebjbibkcpd bibhbhbfbhb cvctd ctd cpd dbct ctb dact crcw csctd ctd csctd crcxctd cqdd crcwctcrczcxd cwct cvctd ctd cpd ctcs crd csctbm bwctd cxdact cvctd cpd cwcxd cpd cxcrd cpd cqd cqdd cpczcxd ctd cwcpd ctcpcrcwcrd cpd dacpd ctdccpd cxd ctcs cpcrb cpd ctd cxd cscxabctd ctd cxd crd cxd bfbabebabd cactcvcxd ctd cbd dactd cccwct ctcvcxd ctd dactd cxd csctd cxdactb cqcpd cxcr dactd cpd cxd crcpd ctcs cqdd cwct cwctd dactd cwcpd dbd cpd czd bycxd cxd crd ctd cpd cxd crd cxd crd 
csctd cpd dbcxd ctd ctcrd cwct ctcvcxd ctd acctd csd cccwct crd csct cpd crd cxd cpd cwct cqcxd cwcpd cpd ctd ctcs cqdd cwct ctcvb cxd ctd acctd csd cbctcrd csb cxd acd csd cqd cwct crcpd cxd cpd cxdect ctcpcrcwckd ctcvcxd ctd ayd ctd cpd acctd csb cpd dbcxd cwct cqcxd cpd cwcpd cqct ctd ctcrcxcudd cvcxdactd ctd cpd dacpd ctba btd ctcvcxd ctd ctd cpd cxd cpd ddd ctd cpd cud dbcwcxcrcw cwct crd cxctd ctd ctd cpd ctd cwct cxcqd ctdcd cpd dacpd ctd cccwct ctcvcxd ctd dactd dbd czd cxd cwct cud dbcxd cpd ctd cud cpd cxcrd cpd ctcvcxd ctd acctd csbm bdba ctd cpd dactd cpd ctcvcpd ctcvcxd ctd cxd cwct acctd csba byd ctcpcrcw ctcvcxd ctd crd ctcpd crd cwct cxd crd cxd cud cpd cxd cvba cactd cpcrct cwct ctd cpd dbcxd cwct ctcvcxd ctd dacpd ctb cpd cpd cwctd ctd cpd csd dbcxd cwd ctcvcpd dacpd ctd bxd cxd cwct cxd crd cxd cpd ctcpcs cwctd cqcpcrczcxd cqd abctd cxd beba cfcwcxd cxd ctd cpd cxd dactd ctcpcrcwd ctcvcxd ctd dacpd ctb cxd crd ctb ctd cpd ctcsd crct cwct crd csct cpd cqddbtc bwcxd cvcxd dbcxd ctcpcrcw cqcxd cpd cxd crd cxd cnd cpd dibcbn cud ctcpcrcw cxd ctcvcxd ctd cnd cpd cxd cjd clbn cccwcxd crctd dbcxd ctcpdact cwct cpd dbcxd bdb cxd ctdab ctd cqcxd cwcpd cwcpd ctd cud cpd cxd crd cxd cxd cpd crctd bfba bxdccpd cxd cwct ctd cxd ctcs cxd crd cxd ctcpd cpd cud cpd cqcxd cwcpd crcwcpd cvct cqctd dbctctd cpd bdba cbd crcw cqcxd cqctd cwct crd ctd acctd csb cxd crct cpd cwctd ctd cpd dacpd ctd dbctd ctacdcctcsb cpd cpd dacpd ctd cud cwct acctd dbctd ctctd ctd cpd ctcsba ccd acd cwctd cqcxd cqcxd dbcxd btc ctcpcrcw cxd crd cxd dbcxd cwct crd ctd ctd cwct crd csct cpd cpd cvcxcrcpd cxd cwct ctd cucxctd bcbn cud ctcpcrcw cxd ctcvcxd ctd cucxctd dgbp cxd cjd cnd cpd czb btd cwct ctd csb cwct cucxctd cpd cwcpd ctd cud ctdactd cqcxd cxd cwct acctd csba cccwct cxdect cwct acctd cxd cqd csctcs cqddd cwctd cpd ctcpd cxcvd cxaccrcpd cqcxd ctd cxd cwcxd crd ctcs cpd czba cccwct acctd csb abd ctd cxd cvcxdactd cqdd cxd ctcpd cxcvd cxaccrcpd cqcxd bgba ccd csctd cxdact cwct bdb cwcpd cqct ctd ctd crd csct ctcpcrcwd ctcvcxd ctd cud cwct crd ctd acctd csb cxd ctd cpd cpcvcpcxd dactd cpd ctcvcpd ctcvcxd ctd cpd cqcxd dbcxd btc ctcpcrcw cxd crd cxd dbcxd cwct crd ctcs acctd cpd czbm cud ctcpcrcw cxd ctcvcxd ctd cud cpd czcjd cxd cjd cucxctd csbn cfctcwcpdact cxd ctd ctd ctcs dbd cpcxd ctdcd ctd cxd cwcxd cxd crcwctd ctba bycxd cpd crcwcxd ctcrd ctd cpd ctb crcxaccr ctcvcxd ctd dacpd crcpd crcwcpd cvctd cwctcpcrd cpd cxd crd cxd ctd crd cscxd cvba byd ctdccpd ctb cwct dcbkbib cscxabctd ctd cxd crb cxd cud cpd ctcs dbcwctd cwct ctcpdc ctcvcxd ctd cxd ctcs cpd cpd ctd cpd csba cccwct dactd csctd ctcrd crcw cscxd crd cxd cxd cxctd cqddcrcwctcrczcxd cwcpd cpd cxd cpd crctd cpd cxd crd cxd cpd ctd cud cwctd cpd ctd ctd cvd cwba cccwcpd cxd dbcwctd cxd ctd cxd cwct ctd ctd crct cxd crd cxd cpd cwd dbd cxd bycxcvd beb cxd crcwctcrczd cwcpd cwctd cqctd cxd crd cxd cqddd ctd ctd cxd ctcs ctd cpd cwct cqctd cucxd crd cxd cxd cxctcs cqdd cwct cxdect cud cxd crd cxd cxd csd ctd cxd dactd cud cwct ctcvcxd ctd dacpd ctd cxd csctd ctd csctd ddb cpd ctd cxd cxd cxd crd cxd ctcrcxaccrcpd cxd ctcpcrcw ctcrcxaccrcpd cxd cxcsctd cxacctd dbcwcxcrcwd ctcrcxaccr ctcvcxd ctd dacpd ctd cxd crd ctd csd cbctcrd csb cwct dactd cpd dbd ctd ctcvcxd ctd ctd cpd cxd cwcpd crd cpcxd cxd ctcvcpd dacpd ctd dbcwcxcrcwcxd cpd cpd cxcrcpd crd cccwcxd ddd cpdccxd ctcud cud cxd cpb cxd dbcwctd cxd crd cxd cpcrcrctd cqd ctd cxcqd ctcvcxd ctd dacpd ctd byd ctdccpd ctb cwct cbc btcabv cpd crcwcxd ctcrd add cpd cxd cvb cxd cxd crd cxd cwcpd cpczct cscxabctd ctd crd cqcxd cpd cxd ctcvcxd ctd csctd ctd csb cxd dbcwctd cwctd cwct cxd cpd cpd cxd cvd ctb csd cqd ctb cpcsb ctcrcxd cxd cbd ctcrcxcuddcxd ctdccpcrd dbcwcxcrcw ctd cpd csd cpd ctcvcpd dbd cxd crd ctcpd cwct cxdect cwct cbc btcabv ctcrcxaccrcpd cxd cqddcpcqd bgbcb cpd dbd cpd cxd crd ctcpd cwct cqcpcqcxd cxd ddd cuctd ctcpcsb crd cxctd crcpd cpd cwcpd ctdactd add cpd cxd cvb cxd cxd crd cxd cpczctd cpd add cpd cxd cvb cxd ctcvcxd ctd cpd cpd ctd cpd csbm cud ctcvd cubcb cubdb cubeb cubfb cubgb cubhb cubib cubjb cubkb cublb cubdbcb cubdbdb cubdbeb cubdbfb cubdbgb cubdbhb cubdbib cubdbjb cubdbkb cubdblb cubebcb cubebdb cubebeb cubebfb cubebgb cubebhb cubebib cubebjb cubebkb cubeblb cubfbcb cubfbd cucpcscsd cud cqd cud cucscxdad cucpcscscsb cud cqcsb cud csb cucscxdacsb cucpcscsd cud cqd cud cucscxdad cud csb cucsd cnbdbmcud ctcvd cnbebmcud ctcvd cnbfbmcud ctcvd bwctd cxdact cpd cpd cxcrcpd ctd cxd cxd cpd ctd ckcqcpcsay ctcvcxd ctd cqdd acd cpd csd ctd ctcrd cxd ctd cpd dacpd ctd cxd cxd acd csd cpd ctd ctd crct cwcpd cwct cpd ctd cqd ctd cpcrcrctd cwctd acd csd cpd ctcvcpd dacpd ctd cud acctd cqdd ddcxd cpd cxd dacpd ctd dbcwcxd cwd cscxd cwct cwctd ctd cpd csd acdcctcs ctcvcpd dacpd ctd bfbabebabe ctcscxcpd cbd dactd cccwct cxd ctcscxcpd dactd crd ctd cwct dbcxcsd cpd cxd cxd ctcpcrcw cxd ctcscxcpd acctd csb cpd cpd cpd cpd cud cpd cxd cwct dacpd ctd cxd cxd ctcscxcpd acctd csd cccwct cxd ctcscxcpd dactd cxd crcpd ctcs cpcud ctd cwct cyd dactd cxcu cwctd cpd ctd cpd cxdact cyd cpd cvctd cxd cxd ctcs cscxd ctcrd dact cud cpcqd cyd cpd cvctd cxd crct cwd cpd cyd cpd cud ctcs cxd ctcscxcpd dacpd ctd cfct ctdcd cpcxd cxd cqctcud cwct cyd dactd cqctcrcpd cxd cqctcwcpdacxd cxd crd ctd cwcpd cud cwctd ctcvcxd ctd dactd cccwct cpcxd cscxabctd ctd crct cqctd dbctctd 
cwcxd dactd cpd cwct ctcvcxd ctd dactd cxd cwcpd cxd cxd cxd cpcrd cxcrcpd ctd ctd cpd cpd ctcvcpd dacpd ctd cud cpd cxd ctcscxcpd ctd cpd csbm cpd cqcxd cxd ctcscxcpd acctd dbd ctd cxd ctd cpd cxd cactcvcxd ctd acctd csd ctd cqct cpd cwct csctd cqcxd cud csctd cpd crcwcxd ctcrd ctd dbcwctd ctcpd cxd ctcscxcpd acctd csd crcpd cqct cqd cpd cxcpd cpd cvctd btd ctd dbct dact cud ctcpcrcw cqcxd cxdect cwct cxd ctcscxcpd acctd csb cpd cwctd cwcpd ctcpcrcwd cxcqd dacpd ctba cccwct dactd acd acd csd cpd cxd ctcscxcpd acctd csb cxdect cqdd cxd ctd cpd cxd dbcpd csd cud cqcxd cqcxd ctd crbab cxd cwct cpd ctd cqd ctd ctcud ctd cpd ctd cqd cwct cxd crd cxd cwctd cxd ctd cpd ctd csd dbd cud cwct cpdccxd cqctd cqcxd crcpd cxd cpd dactd cud ctcpcrcw cxd ctcscxcpd cxdectba cccwct cxd ctcscxcpd dactd dbd czd cxd cwct cud dbcxd cpd ctd cud ctcpcrcwcqcxd cxdectd bdba bvcwd ctcpd cpd csd bdcqcxd dacpd cpd crd ctcpd dbd cqcxd crd cpd dabccpd dabdb cqdd ctd cxd dabc cwct dacpd cpd dabdd cxd crd ctd ctd cccwctd ctd cwct cqcxd cxd cqd dabc cpd dabdbm bobo bdb cpd csd cxdect cqcxd dabc cpd csd dcbn dabd crd ctd ctd cwd cqcxd dabd didabc dcbn ctd cqcxd cxd dabc cpd dabd dabc dabc dcbn dabd dabd dcbn budd ctd cxd cxd cxd crd cxd dbcxd cwctd dbd crd cpd dbctcud crct cwct cqcxd cwct cxd ctb cscxcpd acctd csd cwcpdact crd ctd ctd cpd cqcxd dacpd ctd dbcwcxd cpd cwctd cqcxd ctd cpcxd crd cpd ctdcd crd ctb cpd dbd cxd crd cxd cwct acd dbcxd cwct cxd ctcscxb cpd ctd cpd ctd cpcrctcs dbcxd dabcb cwct ctcrd dbcxd dabdba cdd cwct cwct ctcvcxd ctd dactd csctd cxdact cwct ctcvcxd ctd acctd ctd crd cscxd cvd cud cwctd dbd cxd cxd cpd czcxd cxd crd cxd crd cxd crd cxd cxd cxd cxd ctd cpcrct cxd ctcscxcpd ctd cpd dbcxd dabc ctdbd cxd ctb cxd bcbacud bacucxctd cscnd cpd ctb dabcb ctd cpcrct cxd ctcscxcpd ctd cpd dbcxd dabd ctdbd cxd ctb cxd bdbacud bacucxctd cscnd cpd ctb dabdb dact ctcpcrcw crd ctcvcxd ctd cnd dactb bcb cxd bcb ctcvcxd ctd cnd dactb bdb cxd bdb cccwct dbd ctcrcxaccrcpd cxd cpd cwd cwcpdact cwct cpd ctcvcxd ctd cpd czd cpd cwct cpd cxdect cxd cqddd ctd cccwct cscxabctd ctd crct cqctd dbctctd cwct dbd cwd cqct cwct crd csct cpd czd crd ctcs cqdd cwct ctcvcxd ctd dactd dbcwcxcrcwdbcxd cscxabctd cqdd ctdccpcrd cwct cqcxd cwcpd cscxabctd cxd dabc cpd dabdba beba bycxd cwct cxd ctcscxcpd acctd cqdd acd cgc cacxd cwct dbd crd csctd cpd czd cbcxd crct cwct dbctd cqcxd cud cwctacctd cscxabctd cwcxd cpcrd cxd ctd bdb ctdccpcrd cxd cwct crcpd cxd cwctd cqcxd cpd bcd ctdactd dddbcwctd ctd ctbn dbctcpcscs cwct cqcxd cxd ctdcd cxcrcxd cwcpd dbctcpd cwcpd cwcxd cqcxd cxd crd cxcvd dbcxd cwct ctd cwct acctd csb cccwct ctcpd cxcvd cxaccrcpd cqcxd cvcxdactd cwct acctd csb abd ctd cactacd cwct crd csct cpd cqdd ctd dacxd cpd acctd cqcxd cud cxd ctd cwct cqcxd cxd cucxctd cucxctd cscncqcxd bcbad cnd cpd bdbad cnd cpd czbn cucxd ctcpd cxcvd cxcucxcrcpd cqcxd cucxctd cscnd cucud ctd cqb cucxctd cscncqcxd cpcscs cqcpcrcz cucxctd cqcxd cucxctd cscncqcxd dgbp bobo cucxctd cscnd cucud ctd bdb cucxdc crd csct cpd bcbad cnd cpd bcbad cnd cpd dicucxctd cscncqcxd bfba bvcwctcrczd cwctdacpd cwct acctd cpcvcpcxd cwct dacpd cwct ctd crd csctcs acctd ctct cxcu cpd cxd cpd cud cpd cxd cpd ctcsba crcw cpd cwcxcud cwct cxcvcwd cqddcpd cpd crd cpd bgba bvcwctcrcz ctct cxcu ctdacxd ctd crd cscxd cpd crcwctd cwcxd ctba cbcxd crct dbct dbd dbcpdd csd dbd cud cpd cvctd dacpd ctcs cxd ctcscxcpd ctd cpd ctd dacpd ctcs cxd ctcscxcpd ctd dbct cpdd cwcpdact cpd ctcpcsdd cscxd crd dactd ctcs cwct ctd crd cscxd cud cwct acctd csba byd ctdccpd ctb cbc btcabv bdbfb cqcxd cxcvd ctcs cxd ctcscxcpd acctd ctd crd csctd cpd cxcvd ctcs dacpd ctd cqctd dbctctd cqcxd cpd bdbf cqcxd crd cpd cwct dcbkbibgb cqddd ctd cscxd cpcrctd ctd cpd ctd crd csctcs cscxabctd ctd cwcpd beb cqddd ctd cscxd cpcrctb ctd dbctcwcpdactcpd ctcpcsdd cud cpd ctd crd cscxd cvb cwct crd ctd ctd crd cscxd cxd cxcvd ctcsbn cwctd dbcxd ctb cxd cxd cpcscsctcs cwct cxd cucsctd cxdactcs ctd crd cscxd cvd cfctd ctdacpd cpd cwct ctd crd cscxd cud ctcpcrcw cxd ctcscxcpd cxdectb cqctb crcpd ctd cxd crd cxd ctd crd cscxd cvd crctd cpcxd cpd crcwcxd ctcrd ctd dcbkbib dacpd dbcxd cwct cxdect cwct cxd ctcscxcpd dacxcsctcsba cpcrd cpd dactd cxd cvctd ctd cpd cwcpd cwcxd czctd crcwb cpd cwcpd csd ctd cxd crd cxd dbcxd cpd cpd cqcxd cpd ddd cqctd cxd ctcscxcpd ctd cpd csd byd cxd crd cxd dbcxd cwcpd cxd ctcscxcpd ctd cpd csb cpd dacpd cxcpcqd ctb ctd cvd cxd ctcscxcpd acctd csd cpd cxd csctd ctd csctd cuctcpcrcwd cwctd cwct ctd cvd dbcxd cpabctcrd cwct cxd cxd cucpd ddd cwb ctd cccwctd ctcud ctb csctd cxdact crcpd dactcud cxd ctcscxcpd acctd csd ctd cpd cpd ctd ddba cccwct crd ctd cxd ctd ctd cpd cxd cwcpd dbd cxd cxd cpd cxd bycxd cxd cpd ctd cwcpd ctdactd cxd ctcscxcpd dacpd cwcpd acd cxd cvcxdactd acctd cscxd ctcvcpd cwcpd cxd cwctd ctcpd ctd ckcwd ctd cxd cwct dacpd cpcrct cud cpd cxd ctcscxcpd acctd csba cccwcxd ctb cxcrd cxd cxd cpd ctcpd cqd ctd cbctcrd csb cxd csd ctd cwcpd csd cxd ctcscxcpd ctd cwcpd cpd ctd crd csctcs cpd cxd crd cxcvd cqcxd cpd cvctd cwd cqct cscx crd ctdcd ctd csctd cxdact cwcpd csd crcwcxd ctcscxcpd ctd bfbabebabf cbd dactd cccwct cyd dactd csctd cxdactd cwct ctd crd cscxd cud ctd cpd cxdact cyd cpd cvctd acctd csd cpcqctd crcpd cqct crd cpd cxacctcs cxd dbd dbcpddd bdb ctd cpd cxdact dad cpcqd cyd cpd beb cyd cwcpd cpczct cxd ctcscxcpd ctd dad bad cwd cwcpd cpczct cpcqctd cccwct cyd dactd 
acd csd ctd crd cscxd cvd cud ctd cpb cxdact cyd cwcpd cpczct cpcqctd cpd ctd cpd csd cwcpd cpcrcrctd cxd ctcscxcpd ctd cpd csd cpd ctcwcpd csd ctcs cqddcxd dad czcxd cwct cxd ctcscxcpd dactd btcqd cyd cwcpd cpczct cpcqctd csd ctctd crcrd cxd cpcrd cxcrctba cccwct cscxabctd ctd crct cqctd dbctctd cwct cyd dactd cpd cwct cxd ctcscxcpd dactd cxd cwcpd cwct cyd dactd cvctd ctd cpd ctd dacpd ctd cpcqctd cscxabctd ctd ddba ccd ctd cpd cqcxd cxd cpd cqcxd cxd ctcscxcpd acctd csb cwct cxd ctcscxcpd dactd crcpd ctd cxd cwct cxd ctcscxcpd cscxd ctcrd ddbn cwct cyd dactd cpdd cwcpdactd cpcrct cpcqctd cpcqd cxd crd cxd cpdbcpddba btd ctd cxd dbd cqct cxd cpcrd cxcrcpd dact cud cpd cvct abd ctd cscxd ctcrd ddba cfct cpd cwcpd cqcpcrczdbcpd csd cyd cpd ctctd crd csctcs cxd dbd crd ctd ctd cwcpd dbct crcpd dactcud cwct cxdectd abd ctd bwctd cxdact crd ctd dbcwctd cwctd cyd cxd cpcqd ctd cpd cxdact cqdd ctd cxd cxd dbd crd ctcrd cxdact cyd cwct cpd cpd cvctd cpd crd cpd cxd cwct ctd cxd ctcs crd csct dacpd ctd btcqd cyd dbcxd cwcpdact cxcsctd cxcrcpd cqcxd cxd crct cqd cxd crd cxd ctd crd csct cwct cpd cpd cvctd cpcscsd ctd cactd cpd cxdact cyd dbcxd cscxabctd cxd crct cwctdd cpd cscxabctd ctd cscxd cpd crctd cud cwct cpd cvctd cpd cwd dbcxd cwcpdact cscxabctd ctd abd ctd dacpd ctd bzcxdactd cpd cxd crd cxd cwcpd cxd ctd cpd cxdactcyd cwct dactd acd cwct cyd cpd cvctd acctd csb dbcxcsd cpd abd ctd cxd cwct cxd crd cxd cpd cxd cxd cwd cpd cqddd ctd cyd cqddd ctd cpcrd cpd cyd cud cwct cyd cxd crd cxd cpd cxd cxd cyd cxdectba cccwct cpd cvctd cpcscsd ctd cwct cyd cxd cpd cvctd cpd ctd crd csctcsacctd csa cyd cxdectba bdba bycxd cwct cxd cxd cyd cxdect cqdd ctd cxd cxd cyd bdb beb bababacqddd ctd cxd cwct cyd acctd crcwcpd cvctd beba bycxd cwct cpd cxd cxd cud cwct cyd cwct cpcrctb ctd cud cwctd cpcqctd cwcpd ctd cxd cpd abd ctd bcb cccwct cpd cxd cxd cxd cud cqdd ctcpd crcwcxd cud cpcqctd cpcrctd ctd cwcpd ctd cxd cpd cxd crd cxd cxb dbcwctd cacxd cpcvcpcxd cxd cwct cxcsctd cxd cud crd cxd cwctd dbd csd ctd cxd cyd cscxabctd ctd cpcqctd cpd cwct cyd cpd ctcpcrcw cwct cyd cpcvcpcxd cwct cwctd acd csd dbcwd abd ctd acctd cqct cpd bcb bfba bycxd cwctd cpcqctd abd ctd cxd cwctcxd crd cxd cqdd ctd cxd cxd cud dbcpd cyd cyd cpd cwct cpd cxd cxd cud cwct cyd cccwctd dbd cxd crd cxd dbcxd cscxabctd cqdd cxd cvd cqcxd dbcwcxcrcw cxd cwct dbctd cqcxd cxd cwct cpd cvctd acctd csba bgba bycxd cwct cpcqctd cxdect cxd cwct cxd crd cxd cqdd ctd cxd cxd ctcvcpd cxdactb abd ctd cpcqctd btd cxd cwcpd ctd cpb cxdactcyd cpd ctd crd csctcs cxd dbd crd ctd ctd ctdactd ctd crd ctd ctd cwcxd cpcqctd ctd cwct cwcxcvcw cqcxd cwct cpcqctd cfct cpd cwcpd cwct cpb cqctd cxd crd cxcvd cxd cwct cxd crd cxd cpd cwcpd cxd crd cxd cwct cqcxd cqctd dbctctd cwct cpcqctd cwcxcvcw cpd cqcxd bhba bycxd cpd ddb csctd ctd cxd cwd cwct cyd cscxd cpd crct cxd ctd crd csctcs cqdd ctcuctd ctd crcxd cpcqctd cpd czd dbd abd ctd cpd crd cpd cxd cwct acctd csb dacpd cwctd crcpb cxd cfct crcwctcrcz cud cwct cud dbcxd cpd cud cpb cxd cqd cpcrd cxd cqddcpcrd cpd crcpd cxd cpcxd cxd dectd cwcpdacxd ctcpcscxd cqcxd crcpd ctcsba byd ctdccpd ctb crcpd cxd cpcxd cxd dectd cud cpcqddd abd ctd cwcpd ctd cxd cbc btcabv cyd cxd crb cxd cxd crct cxd crd cxd cpd ctdbd csb cpd cxcvd ctcsba cxczct cwct cwctd dactd cwct cyd dactd cpd ctd cwcpd cpcqctd acctd csd cpd crd cxcvd dbd cqct crcwcpd ctd cvcxd csctcpd dbcxd cwd crd cxcvd cpcqctd acctd csd cqctcrcpd cwct ctctcs cvctd ctd cpd cpcqctd cpd cpd cvct cscxd cpd crctd cud cyd bfbabf cdd ctd bxdcd ctd cxd cbd cxd crd cxd ctd crcw cpd cwct btcac csd cpd cxd cudd cpd cwct cpd cxd dbct cwcpdact csctd crd cxcqctcsba cpcscscxd cxd cxd crd ctcrd cpd ctd cqd ctd crcpd dacxcsct cqcpcs cxd cud cpd cxd csctd cxdactba cfctcvcxdact cwct ctd ctcrcwcpd cxd cpcscsd ctd cwctd cqd ctd cwct ctd crcpd cxd cud csctb cxdact crd ctdc cxd ctcscxcpd ctd crd cscxd cvd ctd cxd dbcwctd ctcvcxd ctd acctd dacpd ctd cpdd csctd ctd ctcpcrcw cwctd cpd dacxcsct ctdcd cxcrcxd acctd dbcxcsd cwd dbcwctd ctcrctd cpd ddba cccpcqd cxd cwct crcpd ctd dbcwctd dbct cwcpdact ctctcsctcs cwctd ctcrcwcpd cxd cccwct cud dbcxd csctd crd cxd cxd cud cpcvd ctd cwd dbd cwd ctd crcpd ctcrcxcudd crd ctdc ctd crd cscxd cvd cccwct ctd dacxcsctd bvcrd csctd cwcpd cpd cpd ctd cqctd dbctctd cwct dacpd cwcpd cxd cpd ctcs cxd cwct cxd ctcscxcpd acctd cpd cwct cxd cecxd cpd ctcs btd cxd btd crcwcxd ctcrd crd cxd bvd cpd bycxctd csd dacpd ctd btcac cactcvcxd ctd ctb bbd cxd csctdcctcs cpcscsd ctd cxd csctd cpd cxd csctd ctd csctd bucpd cpd cxd csctdc ctcvcxd ctd cscxabctd dbctd cpcs cxd cxd crd cxd btcscsd ctd ctcvcxd ctd cqctcpd cpd cvctd dbctd cdd cscpd cxd crd cxd bucpd ctcvcxd ctd ctd cpd cpd cvctd ctcvcxd ctd cbcxd cpd cud dcbkbi cbcrcpd cucpcrd cxd ctd ddcpcscsd ctd cxd cbcrcpd cxd ctd crd csctcs cpd cbc btcabv cbctd cwcx cpd cvd ctd cwcpdactbdbcd dbdectd cqcxd btcac bvctd cpcxd cpcscsd ctd cxd csctd ctdcd ctcrd abd ctd cccpcqd bdbm bxdccrctd cxd cwcpd dbctcwcpdact cud ctd crd cscxd csctd bwctd cxdactb cwd czd ctd crd cxctd ctcpd cxd ctdcd ctd cxd csctd cwcpd csd cwctd crcpd ctd byd acctd csd cwcpd csctd ctd ctcpcrcw cwctd ctd cpd cpd csctd ctd csctd ctcvcxd ctd cxd cwctcxd ctcrcxaccrcpd cxd cpd cud crd cxd cwcpd cpczctd cpd cxd cud ddd cqd cxcr ctcvcxd ctd cpd ctd cpd ctd 
cccacdbx cxab cwctdd cpd ctcpd ctcvcpd crd cqcxd cpd cxd cdd ctd cpcscs cxd cxd cpd cud cpd cxd cqddd dacxcscxd cud crd cxd cwcpd cpczctd cpd cxd ctcscxcpd cpd ctd cwct cpd cud ctcs dacpd ctb cpd cpd cpd cxd cxd ctcscxcpd ctd cwcpd cwcxd ctd crd cscxd cxd cwct ctcrcxaccrcpd cxd cwcpd cwct cpd ctd cqd ctd ctdcd ctcrd cwcxd ctdccpd ctb cwct dcbkbi cxd crd cxd ctd crd csctd cwct cvcpd cxd cwd cwct crcpd cucpcrd cwcpd cxd cvcxdactd cwct cpd ctd cqd ctd cccwcxd ctdccpd crd cpd cqctcwcpd csd ctcs cqdd cpczcxd cwct crcpd cucpcrd ctcvcxd ctd ddd cpd ctd ctd cpd cxd cwct cxcqd crcpd dacpd ctd bab cxcvd ctcs dbbeb cxcvd ctcs dcb ctd bobo dcbn cnbecnd ctd cnbdbmd ctcvd cscxd cqcpd ctbmd ctcvd cxd csctdcbmcxd csctdccnd ctcvd crcpd ctbmd dbbeb cccwct cud dbcxd ctdccpd cwd dbd cwd ctd dacxcsctd cxd cud cpd cxd cpcqd cxd csctd ctd csctd acctd csd byd cwct dbctd cxd crd cxd ctd cxd ctcsb cwct cqcpd ctcvcxd ctd cqctd cwctd cpd cpd cwct cpd cvctd ctcvcxd ctd csba cqd cscpd ctcncscxd crcwcpd cpd cvd cjclb crcwcpd cpd cvd cjbcclb cpd cvd cjbdclbn cxcu axd crd cpb ayd bcayb dgdg axd crd cpb csb ctd bybtc cbbxbn ctd ctd cccacdbxbn cqded cwded cwcpd dbded bmd cscpd ctcncscxd cacncsbmd ctcvd cscxd cacncpbmd ctcvd bycxd cpd ddb bwctd cxdact dacxcsctd cwd czd cwcpd ctd cpcqd cwct ctd dactd crd cpd ctd cqd ctd cqd cvd cwcpd dbctcwcpdact ctd crd ctd ctcsba byd ctdccpd ctb bzc cpd cpd dbd cwct cbc btcabv cebl cxcrcr cxd crd cxd cpczct cpd cxd ctcscxcpd cwcpd cxd cpd cvctba bwctd cxdact ctd cwcpd acctd csd dactd cpd cpd cwct ctd crcpd ctdcd cxcrcxd ctd csctd cxdact cwct acctd dbcxcsd cqcxd cpd cud dbd crctd cad cxd bwctd crd cxd cxd ctd cvd cxd ctd cxd ctd btd cwcp bibabf bdbcbg btcac apbgbfba bebebj bebabh bkbd dbctd bgbabk bdbkbi cbc btcabv bgbabk blbj dcbkbi apbebgbcba bebebd dcbkbib czcpabct bgbabl bdbcbi cccpcqd bebm cccwct cxd cxd cpczctd csctd cxdactd cwd cvcw ctcpcrcw cpd crcwcxd ctcrd ctb cpd cwd dbd cpd crcwcxd ctcrd csctd crd cxd cxd cpd ctba dcbkbib czcpabct cxd cwct cqd ctd cudcbkbidbct ctctcsctcs ctd cpd cvctd cpabctb csctd cxdactb cvctd ctd cpd ctcs ctd cxd ctd cvd ctd crcrb crd ctcvb dacrb dad cncrbmcrcrb cxd bmbmbjbn bfbabg cdd cxd csctd cxdact cccpcqd cpd cxdectd cwct cxd ctd cxd cpczctd cud csctd cxdact ctdactd cpd cxd crd cxd ctd cpd cpd cwd dbd cwct ctd cvd csctd cxdactb ctcrcxaccrcpd cxd btd cwct cqctd cucpd crcwcxd ctcrd ctd cxd cwct cpcqd cwd dbd csctd cxdactb cpd cxd dacxdactdbctd csctd ctba cccwct cpd cxd cxd csctd cpczct csctd cxdact ctcpd cpcqd cucpd btcac cxd cqctcrcpd cxd cxd crd cxd cpcscsd ctd cxd csctd dacxd cpd cwct cxd csctd ctd csctd crct cpd cxd btd ctd dacxd cwd cxd crd cxd cpczctd cpd cxd cscxb cpd cpd cxd ctb cqctcrcpd csctd cxdact crcwctcrczcpd crd cqcxd cpd cxd ctcvcxd ctd dacpd ctd dcbkbicxd dbcqctcrcpd cwct cpd cvct cqctd cxd crd cxd cpd cpcscsd ctd cxd csctd cpd dbctd cpd cwct ctcrcxcpd ctd crd cscxd cvd cud crctd cpcxd ctcvcxd ctd cccwct cqd ctd cwct dcbkbi cbbt ctcrctd cpd ctd cpd cvctd cwct cpabct cec cjbebhcl dbcpd cpd ctd cvcw cxcrczd ddba cfcwcxd cxd csctd cxdact ctdactd ctb ctd cvcxd ctctd ctdactd cpd cxd crd cxd ctd dbct cwcpdact crd cpcrd ctdactd cpd ctd cxd crd cxd ctd crcxctd cxd cvcpd cpd dacpd cxd cpd crcwcxd ctcrd cpd cpd cpd cccwct cud dbcxd cxd csctd crd cxcqctd cwctd ctd cpd cxd crd cxd ctd crcxctd cpd csctd cpd ctd cwcpd csctd cxdactb ctdactd ctb ctd cvcxd ctctd cxd ctd cwd csd cvdd crcpd cpd cqct dacxctdbctcs cpd ctcud ctd cxd ctd cwd csd cvddba bzc cpd csd ctd cpd ctd cqd cwct btd cwcp dbcwbibg cxd crd cxd bzc cpd csd ctd cwcpd csd cwct btd cwcp csb cxd cvbbd cpd cxd csctd add cpd cxd cvb cxd cxd crd cxd bzc cpd cxd ctd crcpd ctd cwct cqcxd cpcqd cpcscsd ctd ctd cpd cvctd cwcpd bebk cqcxd bzc cpd csd ctd cxd cwcpd csd ctd cxd cwct ctd csct cqcxd cxd btcac cpcscsd ctd cxd csct crd ctcrd ddba bzc cpd cpcrcrctd cxd ctcscxcpd ctd cwcpd cpd cpd cvct cud cbc btcabv cxcrcr cxd crd cxd bzc cpd cud ctd cpcrcrctd cqcxd cxd cxdact dacpd ctd cud cqcxd cxcvd ctcs cxd ctcscxcpd ctd cpd csd ckcbctct cbcad aycjbebeclb cccpcqd bkbabib cxd cxd crd ctcrd cud crbd cpd csd crbdba cwct btcac cpd cpd cjbdbfclb cpcscsd ctd cxd csct ctcvcxd ctd ctb bbd cxd csctdcctcs cxd crd cxd csd cwcpdactd cwctd cpd cxd ctcs ctd cxcrd cxd cpd cwd cpd cxd crd cxd cxd cpcscsd ctd cxd csct beb cpd cwd cvcw cvcpd ctd cud crctd cwd ctd ctd cxcrd cxd cccwct btd cwcp cpd cpd cjbecl csctd crd cxd cxd cud cwctcrdad bxbxbxcrd dactd cxd cxd crd cxd ctctd cxd crd ctcrd cqctcrcpd cxd cxd cwct bbd cebtcg cscxd csctb cpd cpd cxd cdd cxd bwctd cxdactb btd cxd cpd cxdacpd cxd cqctcwcxd cqd cxd cscxd csctd cxdact dbcpd csctd cxd cpdad cxcs cwcpd csb ctcrcxcuddcxd cwct dcbkbi cxd crd cxd ctd cccwcxd cscxd cpd dbcpd cpd cxd cpd ctcpb dbcwdddbctcwcpdactd ctd cpd cvctd ctcs dbd cud ddd ctd cjbjb bkcl cwct dcbkbib csctd cxd ctd ctcpd ctcs ctd ctd csctd cxdactb cvctd ctd cpd ctcs ctcrcxaccrcpd cxd cxd cvctd ctd cpd crd csct ctd cxd ctd cud cwctd cfct cwcpdact dbd cxd ctd cpd ctd cxd ctd cvctd ctd cpd cwcpd crctd ctd cxd crd cxd ctcrcxaccrcpd cxd cpd cvctd ctd cpd ctd bvd crctcsd ctd cpcrd cwcpd crcpd ctd cxd cxd crd cxd cxd crd csct cqd abctd bycxcvb cwd dbd cwct crd csctcrd cpd cpd cxd cud csctd cxdactb bycxcvd cwd dbd cpd ctcrcxaccrcpd cxd cwcpd csctd cxdact cvctd ctd cpd ctd cud cwct cqd ctcpcz cxd crd cxd dbcwcxcrcwcrcpd ctd cqd ctcpczd cxd bxcpcrcwcxd crd cxd 
ctcrcxacb crcpd cxd cxd cpd cud ctcs cxd cud crd cxd cpcrd dbcwd cpd cvd ctd cpd cpd cxd csctdc cxd cwctcrd csctcqd abctd cpd cwct ctcvcxd ctd cxd ctcscxcpd ctd cpcqctd cwcpd cpd ctd cpd csd cud cwcpd cxd crd cxd cccwct crcwd cxcrct cpcrd cpd dbd cwct crd cxd ctd cpcvcpd crd cpd cxcu cpd cpd ctd ctd crcwcpd ctcvcxd ctd dacpd ctd cpd czd dbd cpd crd cxd ctb cxd ctba cccwct cud dbcxd ctdccpd cwd dbd cwct cpcrd cvctd ctd cpd ctcs cud cwct dcbkbi cpcscsd cxd crd cxd cpd cwd cxd cxd ctcsbm azcsctcucxd bxcncpcscsd cnd cnbdb cncrd csctb cub csd cuck ctcvcxd ctd cxcvd ctcs cwd cnbc bcdccrbcbcbdck cub bobo bdbdb bobo bkb bnck cxcvd ctcs cwd crcwcpd cncrd csctb cnbcbnck cncrd csct dad cxcs crcwcpd cncrd csct beb bnck dbcwcxd ctb bcb bbb ctd cxd aycpcscsd ctcrdcb ctcqdcay cxd crd csctcncqd cucuctd bxcncpcscsd cnd cnbdb crd csctcncqd cucuctd cabxbzctcrdcb cabxbzctcqdcb cfcxd cxd cwctd cxd cxcrd dbct cpd cpcqd cvctd ctd cpd crd csct cpd cpd cxcrcpd cwcpd cxd crcpd ctd cxd cpd crcxctd cpd csd ctcpcscpcqd cpd cwct crd csct cwd cpd dbd dbd cxd ctba cccwctd ctcpd ctd dbd crcwcpd ctd cvctd cxd cvctd ctd cpd cxd crcxctd ctd cxd ctd crd csctba bycxd dbct czctctd cwct cqctd cucpd cxd cwb ctd cxcr cpd cqcxd cwcxcud ctd cpd cxd cwct cxd crd cxd ctd cpd csd cpd cbctcrd csb dbct cxd cxd cxdect cwct cqctd ctd cwct crd csct cqd abctd byd cpd crcwcxd ctcrb ctd dbcxd crd cpd cxd crd cxd ctd cvd cwd crcwcpd cpd cac cbbv cpd crcwcxd ctcrd ctd cvctd ctd cpd ctcsctd cxd ctd cxd crd ctd dddbd cxd ctd cxd crd cxd cvctd ctd cpd ctcsba ctd cpd csd cpd cxd crd cxd cpd cpd dbcpddd czd dbd dbcwctd cwct cxd crd cxd cxd cvctd ctd cpd ctcsba bwddd cpd cxcr crd csct cvctd ctd cpd cud ctdccpd ctb csd czd cwct dacpd cud dbcpd csb ctcuctd ctd crctcs cpcqctd cbd crcw cpcqctd cqct cpd crcwctcs cpd ctd cqdd cxd cxd czctd crct cwct cpcrd cpd dacpd cwct ctd cpd cqctcrd ctd czd dbd bwctd cxdact cpd dbd cxd crd cxd ctd cpd csd cqct cpd czctcs cwcpd cwctdd cpd ctcs cxd cwct ctd cxd ctd cpcrd ctcpcsb cwct ctd cxd ctd cvctd ctd cpd cvctd ctd cpd ctd cpd cpcscscxd cxd cpd cpcrd cwcpd crcpd cqct crcpd ctcs acd cxd cwct ctd cpd csba bwctd cxdact crd cxctd crcpd cqd cxd cwctcxd dbd cxd czctd cud cwcxd ctcrcwcpd cxd ctd cxd ctd cvctd ctd cpd crcpd ctd cxd ctdcd csctcqd cvcvcxd cxd cud cpd cxd bycxd ctd cxd ctd cvctd ctd cpd cxd cpcqd cvctd ctd cpd ctd cxd ctd cwcpd crcwctcrczd cwctdacpd cxcscxd ddd cud cwctcxd cpd cvd ctd cccwcxd cuctcpd cxd ctcud crcpd crcw cqd cvd cpd ctcpd cpd cxcqd csd cxd csddd cpd cxcr crd csct cvctd ctd cpd cxd cpcscscxd cxd cwct cvctd ctd cpd crcpd cpd csd crct ctd cxd ctd cwcpd cxd ctdcd cpd csctd crd cxd cxd ctcpcrcw cxd crd cxd cvctd ctd cpd ctcs csd cxd csddd cpd cxcr crd csct cvctd ctd cpd cxd crd cxd cnd ctcr crcwcpd cxd bbb cxd crd cxd cpd crcwcpd cud bbb cpd ctd cqd cud cpd cxcvd ctcs cwd cnd bbb ctd cpd crd cxcvd ctcs cqddd ctd bbb cxd crd cxd cxdect crcwcpd cpd ctd cucucxdcbn bbb cucucxdc ctcs cqdd ctd cxd ctd cvctd ctd cpd cxcvd ctcs crcwcpd cnd cpd czcjc btcgcnbuchccbxcbclbn bbb crd csct cpd crd ctd cpd crcwcpd ddd cnd bbb ddd cqd cxcr cpd ctd cpd ctd cnd ddd cabxbzb bxbwb buc cncab buc cnbt ddd ctbn bbb ctd cpd ddd cnd ddd ctd crd cscxd cvbn bbb cxd cxd ctcscxcpd cpd cud ctcsbr cxcvd ctcs bbb dbctd ctcvcpd dacpd cucxctd cxcvd ctcs cqcxd bbb cqctd cqcxd cxd cucxctd cxcvd ctcs cpd czcjc btcgcncabxbzcbclbn bbb cucxctd cpd cxd cucud ctd bbb cucud ctd cxd cxd crd cxd cxcvd cnd ddd cxcvd ctcscncucxctd csbn bbb cxcvd ctcs cxcvd ctcs cucxctd csbr cxd ctd cpd cxdactcnd cucud ctd bbb dbcwctd ctd cpd cxdact cyd cpd cud cucud ctd cud ctd cyd cxd dbcpd cnd ctcubn bbb cvctd ctd cpd ctd cpd cpd ctd cxd ctd ctd cwcxd cucxctd cjc btcgcnc cbclbn cvbn bycxcvd bfbm cccwct ctd crd cscxd csctd crd cxd cxd cwcpd csctd cxdact cxd crd csctd cwct cxd crd cxd cud cpd cxd cxd ctcs cvctd ctd cpd cwct cxd crd cxd cwct cxd crd cxd cpd czb cpd cxd ctd cpd ctcrcxaccrcpd cxd ddd cxd cpd ctd cwcpd ctd ctd ctd cxd cpd cud cpd cxd cxd ctcscxcpd ctd cccwct csctd cxdact crd cqct cscxacctcs cud dbcxd cwctd cpd cvd cpcvctd aycqd ctcpczayb bbb cxd crd cxd cpd ayb cxd ayb bbb cpd ctd cqd cud cpd bdb bbb ctd cpd crd bgb bbb cxd crd cxd cxdect ayayb bbb cucucxdc ctcs cqdd ctd cxd ctd cvctd ctd cpd bcdccsb bcdcbcb bcdcbcb bcdcbcb cvb bbb crd csct cpd aycxd ayb bbb cpd ctd cpd bxbwb bbb ddd ctd cpd bwbxc ccb bbb ctd cpd cpd cud cpd cxd bcb bbb dbctd ctcvcpd dacpd bdbcb bbb cqctd cqcxd cvb bbb cucxctd cpd bdbib bbb cucud ctd cxd cxd crd cxd cncdc cbc bzc bxbwb bbb cxcvd ctcs bcb bbb cxcvd ctcs cud cyd bbb csd cvctd ctd cpd cpd ctdcd ctd cxd ctd cvb bycxcvd bgbm csctd cxdactb cvctd ctd cpd ctcs ctcrcxaccrcpd cxd cud cwct cqd ctcpcz cxd crd cxd cccwct ctd cxd ctd cpd cpd ctcud cpd ctcpd ctd cxd csctd cxdactba byd ctcpcrcw cxd crd cxd dbct cvctd ctd cpd cwct ctd cxd ctd cpd ctd cvd cpd cwcpd cxd dad czctd cxd dbcxd cvcxdactd cqd ctd cpd cpd ctd ctd cccwct cxd crd cxd ctd crd cscxd cvd cvctd ctd cpd ctcs cqddd cwctctd cxd ctd cud crd cxd cpd cwctd crd cpd ctcs cwct cvctd ctd cpd ctcs cqdd cwct cpd cvctd cpd ctd cqd ctd cccwcxd crctcsd cpd 
dbd ctd csctd cxdact dbcxd cwd cpcrd cpd cxd crd csct cwct cpd cvctd cpd cud csctd cxdactb ctd cxd ctd cvctd ctd cpd crcpd crd crd cxd cqctd dbctctd cpd crcwcxd ctcrd ctd dbcwd ctd cscxcpd ctd ctd cpd crcwba ccd csctd cpd cwcpd dbd czb dbctcwcpdactd ctb cpd cvctd ctcs cpabctb cjbebhcl dcbkbi cqcpcrczctd cpd cpd cxcrcpd cvctd ctd cpd ctcs ctd cxd ctd cud cyd cqd ctd cwct dcbkbi cbbtba cfct ctcsd crctcs cwct cqctd cxd ctd cxd cwct cqcpcrczctd csctd crd cxd cxd cud beb bcbkbg bdb bebibjba cfct cpd cscxd crd dactd ctcs cwcpd cwct cxcvcxd cpd crd csctd cxd ctcs cwd ctd cxd crd cxd ctd crd cscxd cvd cxd crcpd ctba cactd cpd cvctd cxd cwct cpd dccxd cpd ctd cscpddb dbcwcxcrcw cxd cscxcrcpd ctd cwcpd cwct ctd cxd ctd cud crd cxd cvctd ctd cpd ctcs cqdd csctd cxdact cpd cpcqd cxd ctcpd cpd cxcrcpd cxd bwctd cxdactb cvctd ctd cpd ctcs ctd cxd ctd crcpd cqctd ctcscxd ctdactd cpd cwctd dbcpddd csddd cpd cxcr crd csct cvctd ctd cpd cxd byd ctdccpd ctb cwctddcrcpd cqct ctcs cqcpcrczctd cscrd crd cxd cud cvctd ctd cpd csddd cpd cxcr crd csct cvctd ctd cpb cxd ddd ctd crcwcpd dacrd csct cjbkcl cpd crcrcv cjbdbjclba cccwctd ddd ctd dacxcsct cpd ctd cqd ddb cxczctcxd ctd cucpcrctd cud csddb cpd cxcr crd csct cvctd ctd cpd cxd cpd ctctcs ctd crd cscxd cxd cud cpb cxd cpcrd cpd cvctd ctd cpd crd csctba bwctd cxdact crd cpd cqct ctcs crd ctd cpd ctd cud cpd cxcrcpd cxd ctcrcxaccr ddd ctd crcw cpd bwc cjbjclba ccctd cpd ctd crcpd cqct ctcrcxb acctcs cxd ctd ddd cqd cxcr cxd crd cxd ctd ctd crctd cuctcs csctd cxdact cvctd cwct crd ctd cscxd cqcxd cpd ctd crd csctd cpd cwctd ctcxd crd cpd ctcs cxd cwct ddd ctd bvd crd cxd cccwct csctd cxdact ddd ctd ctdactd ctb ctd cvcxd ctctd cxd crd cxd ctd crd cscxd cvd cud cwctd ddd ctd cpd ctd cqd ctd cdd ctd ctctcs cvcxdact cpd ctd cqd ddb ctdactd cxd cud cpd cxd cpcqd cwct cxd crb cxd cud dbcwcxcrcw cwctdd dbcpd ctd crd cscxd cvd cpd dbb ctdactd cxd cud cpd cxd cpcqd cqcxd acctd cpddd bwctd cxdact crb crctd cud ctdactd ctb ctd cvcxd ctctd cxd crd cxd ctd crd cscxd cvd cwct cbc btcabvb cbb btd cwcpb btcac dbctd bvb cpd dcbkbiba cwct cpd crcpd ctb cxd cwcpd csd ctd dacpd cxcpcqd ctb cxdectcs cxd crd cxd cpd cvct cxd crd cxd bdbicqddd ctd cxd cxd crd cxd ctd crd cscxd cvd csctd ctd cxd ctcs cqddd ctd cpd cxdectb cpd cwctd bvc cbbv cuctcpd ctd btd cpd cud cxd cxd cxd ddb dbctcwcpdact cqd cxd crd csct ctd cxd ctd cvctd ctd cpd cucsctd cxdactba cfctd cpd ctdcd ctd csctd cxdactb ctcrcwd cxd ctd ctdactd ctb ctd cvcxd ctctd cqcyctcrd crd csct acd cud cpd cxd crd cscxd csctcqd cvb cvcxd cpd cxd czcpcvct cxd cud cpd cxd cbd crcw cxd cud cpd cxd dbcxd ctd cpcqd cqd cxd ctd ctdactd ctb ctd cvcxd ctctd ctcsd cxd crd cscxd dactd cxd cubtccc cjbebdclb csddd cpd cxcr cxd czcxd cxb cqd cpd cxctd cjbdbdclb cqcyctcrd ctdactd cpd cscqd dcctd cjbebfclb ctdcctcrd cpcqd cxd cxdectd cpd cxd czctd bud cxd csctd crcw cpd cpcvd ctcs cqdd cwct ctctcs ctd ctcpd ctcsd ctcxd ctd ctd cud crb cxd cpd cxd crd cpcxd ctcs cxd ctdccxd cxd cud dbcpd ctba byd ddd ctd cxd cxd ctdcd ctd cxdact crcpd ctdccxd cxd cvd cpd csddd cpd cxcr crd csct cvctd ctd cpd cxd ddd ctd crcpd cpabd cwct cxd crcpd cpd cpd ctd cqd ctd cwctd crcpd ctd cwct cud dbcpd cwcpd cpd cxd cpd cxcpd cud cpd cqct ctdbd cxd ctd cud crd cpd crcwba byd ctdccpd ctb crd ckd cxcrczay cwcpd crd ctd crcxcpd crd cpd cxctd cscxd crd cpcvct cwcxd csb cpd dactd csd cxd cwcpdact cxctd cpd ddd cqd cpcqd cpddd dbcwcxcrcwcrcwcpd cvct ctdactd ddd cud dbcpd ctd ctd ctcpd cjbdbiclba cccwct crd cpd cpd ctdactd ctb ctd cvcxd ctctd cxd cwctd cud cpd cwcpd cud crctcsd cxd ctd ctd cpdad cxcs cqcyctcrd ctdactd cscxaccrcpd cxd cxd cxd cwct cpcsdacpd cpcvctd cud crcwcpd cpd cpcrcwba cccwct crct crd csct cud cwct crd ctd dactd cxd csctb cxdact cxd cud ctctd cpdacpcxd cpcqd cpd cwctcud dbcxd cdcac cwd bbbbdbdbdbbacrd bad cpcwbactcsd bbdidbcxd bbcsctd cxdactbad cpd bacvdeba btcrczd dbd ctcscvd ctd cfctd cwcpd cwd bvcpd ctd bwcpdacxcs bvcwctd bxcscscxct cwd ctd cpdc ctd cpd cxcrczccd cpd dacpd cxd cpd ddd ctcuctd ctctd cpd cwctd cwctd cectd cpdcd cud cwctcxd crd ctd ctcpd cxctd csd cpcud cwcxd cpd ctd cfcxd cxctcw dbcpd ctcs cxd cpd cqdd cpd cbby bvbtcabxbxca cpdbcpd csb bvbvcadfblbkbjbibdbdbjba bwcpdbd bxd cvd ctd dbcpd ctcscxd cpd cqddbwbtcac btcrd cpcrd bwbtblbcbgdfblbkdfbvdf btblbfbf cpd cqdd ccctd cpd byctd dbd cwcxd bzd csd cpd bucpcrcz dbcpd ctcs cqdd bwbtcac crd cpcrd bybfbcbibcbedfblbldfbddf bcbhbcbfba cactcuctd ctd crctd cjbdcl bvba bvd cqctd cvba cactdactd cxd ctd ctd cpd cxd cpd cxd cpd cpd ddd cxd cpd cpd cxcr ctd cpd cvctd cxd cvba crba bwc blbjb bdblblbjba cjbecl btba btba bvd cxd ctctba btd cwcp btd crcwcxd ctcrd ctcactcuctd ctd crct cpd cpd bwcxcvcxd cpd ctd cwcxd ctcscxd cxd bdblblbkba cjbfcl bvba bvd ctd cpd byba ctd btcvctd ctd cpd cpd cpcrcwcud cxd ctcrcxcpd cxdecpd cxd cpd cxd cpd cxcrcpd cxd bvba crba bebfd csc cbd bac ctd ctd cqd cvb byc cpd bdblblbiba cjbgcl bwctd crcw cpd btba cbcrcwcxabd cpd crcxctd cxd ctb ctd cpd cxd cwct cbd cpd cpd czb bkbc ddd ctd crba bdbdd babeblbjdfbfbcbeb cbcpd cpczct bvcxd ddb cdccb cpd bdblbkbgba cjbhcl bxd ctd cpd bybab cfba cbcrcwd djd ctd cpd caba cpd csdbctcwd bubxbzdgcp cvctd ctd cpd cud crcxctd cqcpcrcz ctd csd crba bwc bkblb bebebjdfbebfbjb bdblbkblba cjbicl bwba bxd cvd ctd cpd cfba cxctcwba bwbxcac cebxbm cwcpd cpd cpd cxcrcpd ctdactd ctb ctd cvcxd ctctd cxd crd cxd ctd crd cscxd cvd crba bwchc btc bud btb cpd 
bebcbcbcba cjbjcl bwba bxd cvd ctd cpd cpcpd cwd ctczba bwc bybm bycpd adctdccxb cqd ctd cpcvct csctd cxd ctdccxd cxd csddd cpd cxcr crd csct cvctd ctd cpd cxd crba cbc bzbvc bdblblbib bhbfdfbhblb cbd cpd cud csb bvbtb cdcbbtb btd cvba bdblblbiba cjbkcl bwba caba bxd cvd ctd dacrd csctbm ctd cpd cvctd cpcqd ctb ctdcd ctd cxb cqd ctb dactd cucpd csddd cpd cxcr crd csct cvctd ctd cpd cxd ddd ctd crba bwc blbib bdbibcdfbdbjbcb cwcxd cpcsctd cwcxcpb btb cdcbbtb cpddbdblblbiba cjblcl byctd ajcpd csctde cpd cacpd ctddba btd cpd cxcr crcwctcrczcxd cxd crd cxd ctcrcxaccrcpd cxd crba bvcbbxb bdblblbjba cjbdbccl bubabzd cpd cwcxd cxd ctb bac crczb bvba bvcwcpd cqctd cpd cbba bxcvcvctd btd ctdacpd cpd cxd cud cpcvctcs cxd cxd cxdecpd cxd cxd bwddbvba crba bwc blblb beblbfdfbfbcbgb btd cpd cpb bzbtb cpddbdblblblba cjbdbdcl cfba cfba cpd caba btba btd cpd cpcrcwd cvctd cxd csddd cpd cxcr cxd czcxd cvba cbd cud dbcpd ctbm cpcrd cxcrct cpd bxdcd ctd cxctd crctb bebgb bgb bmbfbjbhdfbfblbcb btd babdblblbdba cjbdbecl cdba djd ded cpd bwba cdd cvcpd cxd cxdecxd csddd cpd cxcrcpd ddb cscxd cpd crcwctcs crcpd dbcxd cxd ddd cuctctcscqcpcrczba crba bwc blbgb bfbebidf bfbfbhb cpd csd byd cxcscpb bdblblbgba cjbdbfcl bwba cpcvcvcpd ctcscxd btcac btd crcwcxd ctcrd ctcactcuctd ctd crct cpd cpd ctd cxcrct cpd bdblblbiba cjbdbgcl caba cpd cpd ccba bucpd cactdbd cxd cxd ctdcctcrd cpcqd acd ctd ctcpd cvd cpd cqctcwcpdacxd cbc bxb bebgb beb bmbdblbjdfbebdbkb cuctcqbdblblbgba cjbdbhcl ctd cpd ctctba cxd cxdecxd dbcxd cxd crd csct cvctd ctd cpd cxd crba bwc blbib cpddbdblblbiba cjbdbicl cbba crcrd ctd cpd crd cxcrcpd cxd cdd ctd cud csd crd ctd ctcs cxctd cpd cud cpd cpd ctcrcwd cxd cxd ctcsct cwcxd csb cpd ddcpcscscxd cxd btd cvba bdblblbjba cjbdbjcl cxd cpd cpba crcrcvbm csddd cpd cxcr crd csct cvctd ctd cpd cxd cud cpd bvb cwd bmbbbbdbdbdbb bacxd cxcpbacud bbb cyctcrd bbdadad bbcrcrcvbbb cpd babdblblblba cjbdbkcl ctd cfba cxctcwb bwba bxd cvd ctd cpd byba cpcpd cwd ctczba cobv cpd crcrbm cpd cvd cpcvct cpd crd cxd ctd cud csddd cpd cxcr crd csct cvctd ctd cpd cxd ccc btcbb bebdb beb bmbfbebgdfbfbiblb cpd babdblblblba cjbdblcl cacpd ctdd cpd byctd ajcpd csctdeba ctdb ctd ctdd cpcrcwcxd ctb crd csct czcxd cpd crcwcxd ctcrd ctcrcxacb crcpd cxd cwd bmbbbbdbdbdbbactctcrd bacwcpd dacpd csbactcsd bbb czcxd bbd ctcrd bbd ctcrd bad daba bdblblbiba cjbebccl cacpd ctdd cpd byba byctd ajcpd csctdeba cccwct ctdb ctd ctdd cpcrcwcxd ctb crd csct czcxd bdblblbh cfcxd ctd cdcbbxc cgb bwctcrba bdblblbhba cjbebdcl btba cbd cxdacpd cpdacp cpd btba bxd cpcrctba btd ddd ctd cud cqd cxd cscxd crd cxdectcs cvd cpd cpd cpd ddd cxd crba bwc blbgb bdblblbgba cjbebecl bwba cbdbctctd cpd cbctct cad cvcpd cpd cub cpd bdblblblba cjbebfcl caba cfcpcwcqctb cbba crcrd ccba btd csctd cpd cbba bzd cpb cwcpd crcxctd cud dbcpd ctb cqcpd ctcs cucpd cxd cpd cxd crba cwct byd ctctd btbvc cbc cbc bebcbfdf bebdbib btd cwctdacxd ctb bvb cdcbbtb bwctcrba bdblblbfba cjbebgcl bwba cfcpd cbddd ctd cud cpd crd csct cscxaccrcpd cxd bvc bwbx blbd cfd czd cwd bvd csct bzctd ctd cpd cxd bdblblbdba cjbebhcl ccba cfcxd czcxd cpabct cpdacpdacxd cpd cpcrcwcxd ctba cwd bmbbbbdbdbdbbad cpd dacxd cpd bacrd csctd crd cxd cxd crcwcpd cscxd ctcrd cxdact aybad ctd ctd csctd ckd bad ctd cpcrd ckd aybad ctd cpd ckd aybn ctcvd bcb bdb beb bfb bgb bhb bib bjb bkb blb bdbcb bdbdb bdbeb bdbfb bdbgb bdbhb bdbib bdbjb bdbkb bdblb bebcb bebdb bebeb bebfb bebgb bebhb bebib bebjb bebkb beblb bfbcb bfbd cud ctcvd cubcb cubdb cubeb cubfb cubgb cubhb cubib cubjb cubkb cublb cubdbcb cubdbdb cubdbeb cubdbfb cubdbgb cubdbhb cubdbib cubdbjb cubdbkb cubdblb cubebcb cubebdb cubebeb cubebgb cubebhb cubebib cubebjb cubebkb cubeblb cubfbcb cubfbd cucrd cucrcrbcb cucrcrbdb cucrcrbeb cucrcrbfb cucrcrbgb cucrcrbhb cucrcrbib cucrcrbj ddd crb cqd cqdbcxb cqdbd cqd ctd ctd dacub dad cncsbmd ctcvd cnd bmd ctcvd cncrbmcucrd csbn cyd cycpd cucwcxb cwcxb cud cnd bmd ctcvd cycpd cvctb cvctd ctd ctb cucrbcb csd cucrbcb crcucrbcb crbcb csd crbcb crd crbcb crcucrbdb crbdb cucrbeb crcucrbeb crbeb crd crbeb csd csd cncsbmd ctcvd cnd bmd ctcvd cpb csd csd csd cpb csd bfbeb csd bfbeb csd cpbfbe cncsbmd ctcvd cndbbmd ctcvd cxd dab dab cpdab dadeb dad csd dab csd dab csd cpdab cpcscsb cpcscsd cqb cqd cpd csb dcd cscpcscsb cscpcscsd csd cqb csd cqd cncsbmd ctcvd cndbbmd ctcvd cnd bmd ctcvd bbbb cpd ctd cqd ctd aycscxdaay cpd cpcrd bbbb cwcxd ddd cpdc cxd cwd cwct ctcpd cwcpd csdbcpd bbbb cxd crd cxd cwct cpd cpd ctd crcpd cqct cvctd ctd cpd ctcs cscxdab cscxdad cscscxdab cscscxdad bcayb cndbbmd ctcvd cnd bmd ctcvd cqd ctcpczb ddd crcpd cxd cyb cycpd cpcqctd cqd deb cqcvctdeb cqd ded cqcvctded cqd decpd cqcvctdecpd cqd decpd cqcvctdecpd cqd ctded cqd ctded cqcvd ded cqd ctdeb cqcvd cnd bmd ctcvd ctcub cpcqctd cvctcxb cvctcxd cxb cxd ctd cxb ctcx cnd bmd ctcvd cxd cqctd cqd ctb cqctd cqd ctd cnd bmd ctcvd cnd bmd ctcvd cpcqctd cpcscscxb cpcscscxd cxb cxd cpd cscxb cxb dcd cxb cncsbmd ctcvd ctcub cxd cscpcscscxb cscpcscscxd cncsbmd ctcvd cndbbmd ctcvd ctcub cxd bbbb dbct crcpd csctcucxd cwcxd cud crct cpd cxd bbbb cxd cscxcucuctd ctd cpcrctd csctd ctcs cqdd cxd cvd cpd dbb cqd cpd crcw cpcqctd cqd cpd crcwcr cnd bmcucrd csb cpcqctd bbbb cwctd cpd crd crctd cqd cpd crcwctd cqcrbccub cqcrbcd cqcrbccud cqcrbcd cqcrbecub cqcrbed cqcrbecud cqcrbed cqd cpd crcwbn bbbb cwctd cxd crd cxd crd cxd dbd cud cpdad cqcrbdcub cqcrbdd cqcrbdcud cqcrbdd cqd cpd crcwb cqd cpd crcwcrbn cpcscsbad cpcscsbacsb cqbad cqbacsb bad bacsb cscxdabad cscxdabacs cncsbmcud ctcvd cndbbmcud ctcvd cnd bmcud ctcvd cucrbdb csd cucrbdb csd crbdb crd crbd cnd bmd ctcvd cnd bmcud ctcvd bad bacsb cpcqd bad cpcqd bacsb dabad dabacsb ctcvbad ctcvbacsb csbad bad csbad bacsb crbad bad crbad bacsb crctcxd bad bad crctcxd bad bacsb cud bad bad cud bad bacsb csbadbbad csbadbbacsb crbadbbad crbadbbacsb crctcxd badbbad crctcxd badbbacsb cud badbbad cud badbbacsb ctcrcxd bad ctcrcxd bacsb 
bad bacsb crdad bad bacsb crdad bad badbb crdad bad bad crdad bacsbad crdad bacsbadbb crdad bacsbad crdad badbbad crdad badbbacsb crdad bad bad crdad bad bacs cncsbmcud ctcvd cnd bmcud ctcvd dacubad dad bad dacubacsb dad bacs cncsbmcud ctcvd cndbbmcud ctcvd cnd bmcucrd csbn dadebad dadebacsb dad bad dad bacs cncsbmcud ctcvd cndbbmcud ctcvd cnd bmd ctcvd crbacubad crbacubacsb crbad bad crbad bacsb crbactd bad crbactd bacsb crbad ctd bad crbad ctd bacsb crbad bad crbad bacsb crbad bad crbad bacsb crbad ctbad crbad ctbacsb crbad ctbad crbad ctbacsb crbad cubad crbad cubacsb crbad ctd bad crbad ctd bacsb crbad cvd bad crbad cvd bacsb crbad bad crbad bacsb crbad cvctbad crbad cvctbacsb crbad ctbad crbad ctbacsb crbad cvd bad crbad cvd bacs cncsbmcucrd csb cndbbmcud ctcvd cnd bmcud ctcvd bbbb cwct cpd ctd cqd ctd ctdcd ctcrd cpd ctd cwctd ctd dbdccrbdb csdccrbdb dbdccrbdb csdccrbd cncsbmcud ctcvd cndbbmd ctcvd cnd bmd ctcvd bbbb ctcuctd crcw cxd crd cxd dbcxd cwcxd ctcu cncwbmcwcxd cxd cndbbmd ctcvd ctcudc cncwbmcwcxd cndbbmd ctcvd cnd bmd ctcvd cwcxd bcb bdb bgb bhb bib cpcscsbad cpcscsbacsb cqbad cqbacsb cpcscsbad cpcscsbacsb cqbad cqbacs cncsbmcud ctcvd cnd bmcud ctcvd cnd bmcud ctcvd cnd bmcud ctcvd csd csd cqb cwb dbd dbb cqd cwd dbd dbd cqb cwb dbd dbb csd csd dbd dbcrbeb csb cscrbeb csb crb dbcrbeb crcsb cscrbeb cncsbmd ctcvd ctcub cxd cndbbmd ctcvd bad bacsb bad dbcrbdb bacsb cscrbd cncsbmcud ctcvd cxd cndbbmd ctcvd crcpcrcwct cncsbmcrcpcrcwctcnd cxd cndbbmd ctcvd crcpcrcwctcnd bcb bdb beb bfb bgb bhb bib bjb bkb blb bdbcb bdbdb bdbfb bdbhb bdbib bdbjb bdbkb bdblb bebcb bebdb bebfb bebgb bebhb bebjb bfbcb bfbd 
microbenchmark-based extraction local global disk characteristics nisha talagala remzi arpaci-dusseau david patterson computer science division california berkeley abstract obtaining timely accurate information low-level characteristics disk drives presents problem system design implementation alike paper presents collection disk microbenchmarks combine empirically extract relevant subset disk geometry performance parameters efficient accurate manner requiring priori information drive measured benchmarks utilization linearly-increased stride glean spectrum low-level details including head-switch cylinderswitch times factoring rotational effects bandwidth benchmark extracts zone profile disks revealing previously preferred linear model zone bandwidth accurate quadratic model seek profile generated completing trio benchmarks data collected broad class modern disks including scsi ide simulated drives introduction theories fundamental data remains mary leakey sustained innovation hard-drive industry spurred incredible advances disk technology performance capacity benefited bandwidth increasing sixty percent year capacity growing rate disk drive industry moves quickly drive appears market twelve months due rapid evolution clients modern disks left quandary obtain accurate detailed information inner-workings recently manufactured disks system implementors knowledge low-level performance characteristics lead improved policy decisions system researchers simulations parameterized latest disk attributes facilitating timely relevant research straight-forward methods obtaining performance characteristics prove successful detailed specifications complete accurate solution put literature employ microbenchmarks characterize hardware software systems alike carefully crafted microbenchmarks utilized wide range environments accurately describe performance uniprocessor multiprocessor memory systems discover cost communication mechanisms parallel machines measure performance operating system primitives evaluate file systems extract parameters scsi disk drives calculate megahertz rating processors applying microbenchmarks disk drives vexing problem complex drive mechanism involving cooperating mechanical electronic parts benchmarks adequate domains translate disk drives rotational factor affects measurement results renders current position drive unpredictable seek develop microbenchmarks suitable extracting performance parameters modern disk drives ideally disk microbenchmarks exhibit properties general runs vast array systems specialized specific kind disks ideal benchmark requires priori information drive measured complete extracts relevant parameters including disk geometry performance parameters low-level parameters including head cylinder switch times overlooked accurate extracts parameters excellent precision fast runs quickly giving information seconds minutes hours days paper introduce microbenchmarks designed extract performance parameters hard disk drives sum total microbenchmarks approach ideal microbenchmark axes general running scsi ide drive raw-device interface run quickly extracting parameters seconds completely characterize physical properties disk drive finally produce accurate drive geometry performance parameters percent manufacturer-reported values contributions paper three-fold simple method based linearly increased step-size extracting localized disk parameters including platter count sectors track rotational delay head switch time cylinder switch time minimum time media slowly ramping step-size factor rotational effects unveil host drive performance characteristics empirical characterization large collection modern disk drives including scsi ide drives previous work focused solely scsi-drive extraction update results zoned nature modern disks including correction proposed linear model accurate quadratic model present results executing microbenchmarks diverse collection modern drives including scsi ide simulated drives study uncovered numerous interesting results discovered minimum overhead write disk media widely varies drives generation found family drives manufacturer exhibited similar strengths weaknesses seagate drives tend excellent switching times multi-zoned nature modern disks pronounced outer tracks delivering bandwidth tracks surprisingly scsi disks performance characteristics ide disks measured scsi bandwidth switching characteristics programmed dma renders ide drive overhead lower rest paper organized section give background disk terminology related work section section presents overview collection microbenchmarks results range disks presented section conclusions section background explaining functionality disk characterization tool give overview modern disk drives in-depth excellent summaries modern disk drive behavior basic internal structure disk drive rotating disks coated sides magnetic media rotating disk called platter side disk called recording surface data stored recording surface concentric circles called tracks track divided sectors sector minimum unit data accessed disk media typical modern disks -byte sectors tracks surface equidistant center form cylinder disks zoned bit recording zbr outer tracks disk higher sectors track ratio tracks read write heads surface ganged disk arm time move arm proper cylinder called seek time time required sector rotate head referred rotational latency time transfer data media called transfer time modern disks head active time sector track track cylinder access spans tracks disk complete portion track switch heads continue track sector mappings consecutive tracks skewed head switch time switching heads requires short repositioning time skew prevents request crosses track boundaries missing logical block wait full rotation similarly access spans cylinders disk arm seek forward cylinder consecutive cylinders skewed cylinder switch time related work inspired separate works literature saavedra presents microbenchmarking technique memory systems paper worthington describes extract performance geometry parameters scsi disks seek combine simplicity speed accuracy saavedra introduces simple powerful method extract performance characteristics multi-level memory hierarchy benchmark repeatedly performs basic loop reading memory locations fixed-size array stride surprisingly characteristics memory hierarchy including number caches capacity associativity block size access times extracted simply changing size array length stride technique applied disk yield results desired disk subsystems regular memory hierarchies complex interaction rotation seek time leave direct application saavedra disk infeasible study worthington describes partially automated tools extracting parameters scsi disk drives twofold approach interrogative empirical extraction interrogative extraction library scsi access functions read mode pages disk mode pages describe disk parameters sectors track ratio prefetch buffer size information extracted mode pages construct test vectors empirical extraction process measure minimum time requests mtbrc kinds comparing mtbrcs test vectors calculate switching times parameters main disadvantage approach reliance user send low-level scsi commands disks highly non-portable requires user trust disk manufacturer information parameter extracted requires separate group test vectors algorithms outlined worthington minutes hours extract parameters contrast benchmarks require low-level access disk interface sense closer true black box microbenchmarks drive parameters extracted skippy benchmark single fast experiment benchmarks section present collection disk characterization tools table summarizes constituent benchmarks microbenchmark extracts skippy linearly increases step distance platter count sectors track writes sector-sized block rotational delay head switch time cylinder switch time minimum time media writes zoned streams entire disk bandwidth function location reading large blocks seeker repeatedly writes sector-sized blocks seek cost function distance start disk locations table microbenchmarks table describes collection microbenchmarks paper skippy extract parameters runs small portion disk zoned produces bandwidth versus location characterization extracting zone profile disk finally seeker generates seek profile function 
distance standard techniques innovative component benchmark suite skippy utilizing technique linearly increasing stride writing disk factor rotational effects extract surprising amount information disk including sectors track ratio rotation time minimum time access media disk head positioning time head switch time cylinder switch time number recording surfaces impressive run-time characterization completes roughly skippy completely characterize behavior modern disk nature local benchmark crucial pieces global information missing cost seeks function distance effect zones function location derive final pieces information utilize microbenchmarks seeker zoned constructed benchmarks similar found due global nature benchmarks time consuming skippy taking minutes complete benchmarks rely raw device interface order bypass file system optimization activities caching buffering read ahead access raw interface benchmarks difficult impossible construct skippy skippy microbenchmark implements approach disk measurement linearly increasing strides counteract disk natural rotation figure shows pseudocode algorithm benchmark writes sector disk forwards file pointer writes iteration distance increases single sector resulting latency versus step size curve distinctive sawtooth shape extract parameters sectors track ratio rotation time minimum time access media disk head positioning time head switch time cylinder switch time number recording surfaces gathered results read variant skippy sake space present write version benchmark read results analysis presented full version paper intuition analytical foundation traditionally extracting parameters head switch time disk drive difficult request incurs unpredictable rotational latency intuition linearly increasing stride method write accesses drive rotates distance forward requests incur latencies time successive requests reaching disk roughly step size requests linearly increasing eventually match distance disk rotates successive requests point observable requests prior incur extra rotation requests afterward basically access pattern designed advantage rotational mechanism separate rotational latency request contributing latencies result disk characteristics including head cylinder switch times observable describe skippy works simple analytical model model terms time full rotation rotational latency hand time request spends waitingfor required sector rotate head rotationallatency vary time transfer data media byte cost including overheads minimum time media minimum time access data disk surface disk request completes incurs rotational seek latency number sectors disk rotates time mtm equation defines terms note equation assumes linear relationship latency number sectors rotated seek time increase linearly seek distance stated earlier step sizes generate arm movement delay purely rotational disk rotates fixed speed delay increases linearly number sectors rotated figure shows expected sequence events single sector writes track figure shows stages write disk open raw disk device measurements time sequence output time lseek single sector seek cur write buffer single sector close figure skippy algorithm basic algorithm skips disk increasing distance seek sector write outputs distance time write raw device interface order avoid file system optimizations single sector size single sector case bytes seek cur argument lseek moves file pointer amount relative current pointer start atdisk atsurface underhead rotationaldelay end start atdisk atsurface rotationaldelay underheadw end step size latency distance disk rotates requests figure behavior figure shows expected sequence events sector writes disk media writes labeled rotates sectors stage illustration show focusing single sector accesses transfer time negligible starts time time scsi subsystem processed request command reached disk time disk positioned head track time required sector disk head difference rotational latency write system call returned short time write begins disk rotated distance forward illustration step size greater distance required sector ahead request served rotation time time start loop iteration execute lseek call time negligible compared disk access times system microseconds average entire write takes assume time negligible make interesting observations rotational delay approaches rotational delay eliminated figure disk rotates approximately time sectors requests words extra rotation complete rotation logic model latency write request access track prior access request satisfied current rotation latency latency minimum time media time rotate remaining sectors substituting equation simpler term latency equation shows latency linear function step size request satisfied rotation latency equation equation simplified substituting substitution equation latency linear sectors trackwith offset equal rotational latency step puts request track request incurs extra head switch delay tracks skewed head switch disk wait full rotation case latencies calculated equations equations cylinder switch similar place note equations assume long distance seeks model skippy intended step sizes seeks greater single cylinder point significant arm movement latency scale linearly step size illustrate expected graphical result mock disk mock disk rpm recording surfaces sectors track minimum time access media head cylinder switch times benchmark create long distance seeks seek profile figure shows expected graphical result accompanying illustrations shown figures reveal points graph illustration shows writes write shows request pattern marked point graph track shown concentric circles rotational delay marked outer circle rotational delay marked circle increases linearly latency sawtooth pattern point figure causing large rotational delay making equal increases latency increases linearly equation approaches equation shows latency approaches point figure time disk head lowered track required sector missed full rotation takes place latency overhead steps reach point figure slightly larger case disk head lowered time rotational latency latency time distance sectors mock disk base cylinder switch head switch base heads minimal time media transfer time rotational latency cylinder switch head switch figure mock disk output mock disk shown graph constructed strictly models developed text illustrate output benchmark startw atsurface rotationaldelay underhead underhead endw start end atsurface rotationaldelay figure point startw atsurface rotationaldelay underhead end underhead end atsurface start rotational delay figure point rotationaldelay min timeto media start atsurface underhead end start underhead atsurface end figure point underhead end prior start atsurface rotationaldelay end start atsurface track end start ofnew track track underhead end track cylskew figure point increases latency increases linearly equation graph sawtooth shape transition graph shows series upward spikes correspond head cylinder switches point figure illustrates head switch case rotational latency increased equation extracting parameters figure exposes disk details coordinate point coordinate transfer time small single sector coordinate point good estimate difference coordinates points latency step size height transition point information calculate number sectors track point reached reverse equation calculate note calculate sectors track ratio region written modern disks employ zone bit recording zbr outer cylinders packed sectors track cylinders due circular nature disks sectors track ratio regions disk run benchmark regions increases latencies form distinct lines 
slope offsets figure shows lines labeled conforms equation conforms equation taking difference offsets lines calculate slope line extract slope point represents head switch latencies conform equation vertical offset point corresponds cylinder switch vertical offset line finally step size number sectors track number recording surfaces counting number head switches cylinder switches larger number steps successive head cylinder switches decreases figure shows eventually step results head switch sample result prior section showed mock disk parameters extracted figure apply techniques ibm ultrastar disk drive manufacturer specifications learn disk rpm rotational latency recording surfaces recording zones outermost sectors track head cylinder switch times figure shows result running benchmark disk figure similar model result figure result behavior predicted equations equation completely explain result head cylinder switches figure head switches upward latency spikes consistent figure shows upward spikes small downward spikes approaches variation affect ability extract parameters require refinement analytical model refine model subsection focus extracting parameters figure error rates calculated comparing extracted values manufacturer values parameter extraction techniques earlier measured values coordinate point actual latency error height sawtooth wave estimate case error techniques yield extremely accurate results coordinate point coordinate equation states offset writing bytes transfer time small subtracting coordinate point estimate fact transfer time small effect virtually indistinguishable measurement noise coordinate good estimate hand estimate difference time distance sectors ultrastar skippy base base head switch cylinder switch heads minimal time media transfer time rotational latency cylinder switch head switch figure ibm ultrastar sample write result ibm ultrastar disk drive values points counterpart specification represents important estimate system overhead ratio actual sectors track error measured error compared specification similarly measured error compared specification finally counting number head switches cylinder switches find disk recording surfaces matches disk specification disk drive extracted values close actual values cases error rate refined analytical model writes figure showed simple model inadequate describing parts benchmark behavior graph shows downward spikes region explained equation section present refinement initial model explain effects figure downward spikes point happen head switch occurs close normal circumstances head switch mechanics equation apply time position head service write rotation prior write head switch occurs track skew disk head slightly extra time enabling disk service writes waiting extra rotation writes complete latencies close figure shows downward spikes extend head switch line left point observation confirms hypothesis spikes caused head switches adjusting model account effects create model graph identical sample result extra downward spikes reveal interaction disk head positioning head switch property estimate disk head positioning time full paper details refinement limitations limitation write skippy technique work disks delayed write optimizations limitation applies microbenchmarks attempt measure write latencies disks measurable read skippy variant open raw disk device read buffer large size large size transfer large size transfer report size output location bandwidth achieved region transfer close figure zoned algorithm benchmark simply reads disk sequentially blocks size large size threshold amount read report size benchmark outputs location bandwidth achieved region full paper show read result slightly write result interaction read-ahead mechanism smaller cases cases parameters extractable read benchmark slight variant basic read benchmark backwards read benchmark strides disk reverse direction measures parameters basic read write versions avoiding read-ahead optimizations tend obscure results plan investigate utility future work developed tool automatically extract parameter values graphical result extraction algorithm utilizes work statistics image-processing communities process latency versus data extract parameters listed table details final version paper extraction tool made online benchmark tool set zoned subsection briefly describes zoned microbenchmark designed extract bandwidth profile recording zones disk basic algorithm depicted figure straight-forward figure shows algorithm result ultrastar disk drive manufacturer specification learn disk recording zones sectors track ranging outermost zone innermost zone graph shows recording zones earlier demonstrated skippy extract sectors track local area run running skippy zone defined figure extract sectors track zone drive technique learn largest zone average sectors track sectors track values subsequent zones values match specifications observe large difference delivered bandwidth zones drive outermost zone bandwidth roughly tracks deliver roughly increase outer tracks seeker global disk characteristic missing seek profile fortunately seek delays based solely mechanical movements disk arm explored prior studies limit discussion seeks present variant skippy technique make seek experiments easier factoring rotational latency component measured time present seek curves function sector distance cylinder distance skippy local benchmark directly measure seek distances utilize slight variant figure measurements algorithm writes fixed location beginning disk variant disk space reused creates similar identical sawtooth wave minimum estimate seek time rotational latency bandwidth location ibm ultrastar zones largest size inner-most zone covers delivers roughly bandwidth figure ibm ultrastar zoned benchmark run ibm ultrastar figure shows seek latency versus distance sector seagate barracuda shape curve slightly seek curves papers textbooks seek time versus sectors versus cylinders note minimal time media included values reported true seek values obtained subtracting derived skippy benchmark basic algorithm suffers limitations similar skippy work drive immediately write data disk disk area reused read version work nearby disk sectors found buffer cache results section presents results range modern scsi ide drives simulated drives table lists drives measured real drive measured pentium memory running freebsd version scsi drives connected fast-wide scsibus figures show results skippy seeker zoned drive table summarizes extracted numbers benchmarks disk skippy scsi disk drives scsi disk cases clear height sawtooth wave rpm hawk wave high error rate rpm barracuda disk error rate estimated micropolis disk error rate finally rotation time rpm giving error measurements show vary disks rpm generation hawk average rpm disks ranged lowest seagate barracuda highest micropolis drive finally latest disk rpm lowest disks measured testbed open raw disk device base base disk size base large size measurements lseek seek set write single sector time sequence output location time lseek base single sector seek set write buffer single sector close figure seeker algorithm pseudocode seek algorithm presented benchmark jumps betweeen beginning disk target locale writing single sector time time write timed performed repeatedly parts disk shown loop base seek set argument moves file pointer absolute relative location call seek time distance seagate barracuda seeker figure seagate barracuda seek curve seek profile seagate barracuda note seek 
time non-linear small seeks linear model long-distance seeks note seek time reported includes time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure seagate hawk results presented seagate referred hawk note disk older generation head cylinder switch times good disk large number zones typical seagate disks finally seek curve standard time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure seagate barracuda results presented seagate referred barracuda note excellent head cylinder switch times skippy curve large number platters seagate devices odd number platters hypothesis extra platter position-sensing information zoned curve shows large number zones small indistinguishable finally seeker curve ranges time distance sectors skippy base cylinder switch head switch base heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure micropolis micropolis disk worst performers scsi class poor switch times exceptionally high zone profile odd zone delivers notably higher performance explanation behavior point time distance sectors skippy base basehead switchcylinder switch heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure ibm ultrastar disk presents typical output curve reasonable switch times zones disk outermost zones occupy half disk seek numbers noisy repetition explanation effect time distance sectors skippy base base head switch cylinder switch heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure ibm modern disks study fastest rotating disk revolution rpm low rotational latencies low small-seek costs head cylinder switch times prominent zoning ibm disks distinct zoning found seagate disks time distance sectors skippy base cylinder switchhead switch base heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned axis axis data figure quantum fireball ide ide disks study low profile disk recording surfaces recent drives high sectors track ratio single zone definition suggests drive manufacturers chose simplicity performance unable generate seek profile figure full paper time distance sectors skippy base cylinder switch head switch base heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure ibm ide ide drive study recent scsi drives considerably lower bandwidth higher switching times time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure simulated dartmouth simulator accurately simulates disk generation high rotational latency switch times minimal time media true disks era zone entire disk finally seek profile regular matches formula utilized authors note data simulator cleaner real-world disks time distance sectors skippy base cylinder switch head switchbase heads minimal time media transfer time rotational latency cylinder switch head switch bandwidth location zoned seek time distance seeker figure dec simulated data disksim michigan disk simulator presented disk modern dartmouth simulator show realistic behavior text zone profile uninteresting seek curve expected model year interface capacity dimensions rpm rotational recording latency surfaces seagate scsi hawk seagate scsi barracuda micropolis scsi ibm ultrastar scsi ibm scsi quantum fireball ide ibm-dttaide simulated dec simulated table disks disks years range rpm rpm detailed specifications ibm ultrastar disk drive table relevant information gather disk drives on-line specification sheets drives excepting simulated disks inch half height low profile table describes simulated disk drives drive represents trial benchmarks dartmouth disk drive simulator dec represents trial disksim simulator developed michigan assume operating system scsi overheads similar results show ibm drive lowest overhead access media seagate hawk barracuda drives interestingly seagate hawk considerably older rpm drives ibm ultrastar micropolis drive measured drives employ zbr extract sectors track ratio outermost zone drive hawk roughly sectors track barracuda ultrastar micropolis finally compare head cylinder switch times graphs show seagate barracuda drive lowest head cylinder switch times hawk cylinder switch time comparable ultrastar hawk older drive counting number head switches cylinder learn hawk recording surfaces barracuda micropolis match specification data table seagate drives odd number recording surfaces suggesting dedicate surface track mentioned ide disk drives figures show write behavior quantum ibm ide disks graphs show caching activity lower step sizes fact appears drives write buffer cache requests empties cache additional request reached behavior entire result graph shift graphs slightly shifted measure rotational latency height transition point measured quantum fireball error specification drive recording surfaces consistent disk specifications table quantum drive head switch time cylinder switch time measured rotation time ibm ide disk error compared specification sectors track ratio disk recording surfaces head switch time cylinder switch time disk low values bit scsi disks reflects programmed disk rotation mtm sectors heads head cylinder bandwidth seek time switch switch outer max min track time time seagate hawk seagate barracuda micropolis ibm ultrastar ibm quantum fireball ibm-dtta sim dec sim table extracted values table lists extracted parameters disk drive including ranges bandwidth seek curves note values seek curve adjusted minimum time media reflect actual seek characteristics dma common ide drives improves overhead cost discussing achieved bandwidth ide simulated drives figures show results dartmouth disk simulator disksim experiments verify skippy technique matches disks expected work disk simulators values extracted measurements match simulator disk specifications simulated results noticeably cleaner measurement results comparing simulation results disksim result shows downward spikes sawtooth transition real disks dartmouth result suggesting newer disksim simulates drive closely older dartmouth simulator zoned drive skippy result accompanied zoned result make general observations zoned results older simulated drives newer ide drives show recording zone ide drives implies drive manufacturers sacrifice performance simplicity achieved bandwidth ide drives low reflect programmed dma scsi drives seagate drives noticeably finely zoned ibm micropolis drives finally disks multiple zones difference outer-track inner-track bandwidth ranges recent comprehensive discussion disk drive zoning behavior observed relationship transfer rate disk position linear function single examining zone results curve closer parabolic linear quadratic function form fit zone 
graph linear function fact fitting linear quadratic functions data standard linear regression techniques learned quadratic function factor factor error simple linear fit linear fit explored extra advantage required highest lowest bandwidth values drive found quadratic fit values factor linear fit values fact cases quadratic fit values linear fit values factor model employed recommend usage quadratic fit simple construct linear model requiring data points matches profiles linear fit disks zones exact step function utilized disksim simulator makes exact characterization seeker figures table show seek latency start drive areas function seek distance sectors seeks tenth disk seek latency appears increase linearly sector distance seek latency increases larger numbers cylinders close examination data reveals seeks reaching innermost zones latency increase higher linear observable ibm seek result seek time increases rapidly sectors track ratio decreases rapidly area requiring arm movement sector distance conclusions paper presents disk benchmarks skippy zoned seeker extract range parameters modern disk drives skippy illustrates approach measuring disks linearly increasing stride patterns technique extensions filter rotational effect kinds disk measurements knowledge present benchmark utilizes disk rotational mechanism characterizing disk defeat benchmarks run scsi drives ide drives disk simulators revealing numerous results modern disk drives find minimum time access drive media vary widely drives generation seagate drives show excellent switching time characteristics ibm drives bandwidth results show similarities drives made manufacturer odd number recording surfaces present seagate drives scsi drives older show performance ide drives switching times bandwidth overhead ide reads writes lower improvements linear areal density reflected sectors track number recording surfaces measured drives modern drives higher sectors track ratio older drives number recording surfaces concurrently decreasing rotational latency seek times decrease head-switch cylinder-switch times important hawk switch times roughly rotation time percentages ibm simple models disk behavior characterize disks seek rotation time longer full paper provide details skippy read write benchmarks future work includes exploration backwards read variant retains benefits read benchmark avoiding interaction read-ahead mechanism benchmarks extraction tool measured data made public website hope run benchmark contribute data active archive disk characteristics remzi arpaci david culler arvind krishnamurthy steve steinberg kathy yelick empirical evaluation cray-t compiler perspective annual international symposium computer architecture iscapages june peter chen david patterson approach performance evaluation self-scaling benchmarks predicted performance proceedings acm sigmetrics conference pages david culler lok tin liu richard martin chad owen yoshikawa logp performance assessment fast network interfaces ieee micro gregory ganger bruce worthington yale patt disksim simulation environment version manual technical report cse-tr- department electrical engineering computer science michigan february cristina hristea daniel lenoski john keen measuring memory hierarchy performance cache-coherent multiprocessors micro benchmarks supercomputing san jose november ibm ultrastar hardware functional specification models rpm version document number ibm storage products division june ibm ibm disk drive specifications http storage ibm david kotz song bac toh sriram radhakrishnan detailed simulation model disk drive technical report pcs-tr department computer science dartmouth college july larry mcvoy carl staelin lmbench portable tools performance analysis proceedings winter usenix january rodney van meter observing effects multi-zone disks proceedings usenix conference january micropolis micropolis disk drive specifications http procom homepage tech quantum quantum disk drive specifications http quantum chris ruemmler john wilkes introduction disk drive modeling ieee computer march rafael saavedra stockton gaines michael carlton characterizing performance space shared-memory machines micro-benchmarks hot interconnects san jose august rafael saavedra-barrera cpu performance evaluation execution time prediction narrow spectrum benchmarking phd thesis berkeley computer science division february david schwarderer andrew wilson understanding subsystems adaptec press edition january seagate seagate disk drive specifications http seagate carl staelin larry mcvoy mhz anatomy micro-benchmark proceedings usenix annual technical conference pages berkeley usa june usenix association bruce worthington greg ganger yale patt john wilkes on-line extraction scsi disk drive parameters proceedings acm sigmetrics conference pages 
scale performance distributed file system john howard michael kazar sherri menees david nichols satyanarayanan robert sidebotham michael west carnegie mellon andrew file system location-transparent distributed tile system eventually span workstations carnegie mellon large scale affects performance complicates system operation paper present observations prototype implementation motivate areas cache validation server process structure translation low-level storage representation quantitatively demonstrate andrew ability scale gracefully establish importance whole-file transfer caching andrew comparing performance sun microsystem nfs tile system show aggregation files volumes improves operability system categories subject descriptors operating systems file systems managementdistributed file systems operating systems performance-measurements general terms design experimentation measurement performance additional key words phrases andrew caching operability scalability venus vice volumes file transfer introduction andrew distributed computing environment development carnegie mellon comprehensive overview system presented morris characteristic andrew pertinent paper expected final size individual cmu eventually possess andrew workstation implying scale nodes fundamental component andrew distributed file system constitutes underlying information sharing mechanism detailed work performed joint project carnegie mellon ibm corporation satyanarayanan supported writing paper national science foundation contract ccrthe views conclusions contained document authors interpreted representing official policies ibm corporation national science foundation carnegie mellon direct correspondence satyanarayanan department computer science carnegie mellon pittsburgh permission copy fee part material granted provided copies made distributed direct commercial advantage acm copyright notice title date notice copying permission association computing machinery copy republish requires fee specific permission acm acm transactions computer systems vol february pages howard description file system presented earlier paper set trusted servers collectively called vice andrew file system presents homogeneous location-transparent file space client workstations clients servers run berkeley software distribution bsd unix operating system operating system workstation intercepts file system calls forwards user-level process workstation process called venus caches files vice stores modified copies files back servers venus contacts vice file opened closed reading writing individual bytes file performed directly cached copy bypass venus file system architecture motivated primarily considerations scale maximize number clients supported server work performed venus vice functions essential integrity availability security file system retained vice servers organized loose confederacy minimal communication venus workstation locating file specific server initiates dialogue server intent paper examine design andrew file system level detail concentrate features design decisions bear scalability system large scale affects distributed system ways degrades performance complicates administration day-to-day operation paper addresses consequences scale andrew shows mechanisms incorporated cope successfully concerns section paper describes initial prototype implementation experience section introduces synthetic benchmark basis performance comparison rest paper basis experience made design rationale presented section section discusses effect design performance place design perspective quantify relative merits section presents results running benchmark alternative contemporary distributed file system sun microsystem nfs section shows operability system enhanced design finally section discuss issues related peripherally scale examine ways present design enhanced prototype primary goal building prototype validate basic file system architecture implementation carefully balance opposing constraints desire obtain feedback design rapidly build system usable make feedback meaningful retrospect prototype successful respects prototype users unix trademark bell laboratories avoid ambiguity bsd paper specific version unix system acm transactions computer systems vol february scale performance distributed file system peak usage workstations servers workstations sun local disks servers sun vaxs disks rest paper illustrates experience gained prototype invaluable developing considerably improved implementation andrew file system description prototype venus client workstation rendezvous process listening well-known network address server process created dedicated process deal future requests client dedicated process persisted client terminated network connection steady state server typically operated processes clients contacted bsd sharing address spaces processes communication manipulation data structures server processes place files underlying file system userlevel file locking implemented dedicated lock-server process serialized requests separate server processes maintained lock table address space data vice status information stored separate files server contained directory hierarchy mirroring structure vice files stored vice file status information access list stored shadow directories called admin directories directory hierarchy contained stub directories represented portions vice space located servers location database maps files servers embedded file tree file server search end stub directory identified server file top levels vice tree files subtree located server clients cached pathname prefix information basis heuristic direct file requests servers vice-venus interface named files full pathname notion low-level inode bsd rudimentary form read-only replication restricted topmost levels vice tree present replicated directory single server site updates directed asynchronous slow-propagation mechanism reflected made site read-only replicas sites cached copies files considered suspect venus cached file venus verify timestamp server responsible file open file resulted interaction server file cache date qualitative observations preliminary experience prototype positive application program workstations vice files recompilation relinking put rest key concerns successful emulation bsd file system semantics caching wholefile transfer areas incompatibility standard bsd semantics discourage prototype acm transactions computer svstems vol februarv howard command execution involving vice files noticeably slower similar commands involving local files performance heavily loaded timesharing systems general user community cmu users willingly suffered anticipated performance degradation uniform operations cpu-bound operations compilation large program fast stand-alone system operations recursive directory listing large subtree files larger subtree vice puzzled application programs ran slower expected relevant files local cache turned programs stat primitive bsd test presence files obtain status information opening pathological cases file stat-ed thrice opened stat call involved cache validity check total number client-server interactions significantly higher number file opens factor increased total running time programs load servers attempted alleviate problem placing upper bound frequency checked validity cache entry performance improve satisfactory found performance acceptable limit active users server occasions persons file system intensely caused performance degrade intolerably prototype turned difficult operate maintain dedicated process client server caused critical resource limits exceeded number occasions resulted excessive context switching overhead high virtual memory paging demands virtue simplicity resulted robust system failure individual server process affected client remote procedure call rpc package built top reliable byte-stream abstraction provided kernel simplified implementation frequently caused network-related resources kernel exeeeded decision embed file location database stub directories vice tree made difficult move users directories servers disk storage server exhausted easier add disk move users server inability enforce disk storage quotas individual users exacerbated problem benchmark quantify performance penalty due remote access ran 
series controlled experiments synthetic benchmark benchmark consists command script operates collection files constituting application program operations intended representative sample kinds actions average user perform demonstrate statistical similarity file observed real systems convenient yardstick comparing variety file system implementations acm transactions computer systems vol february scale performance distributed file system table stand-alone benchmark performance machine type benchmark phase sun ibm sun makedir copy scandir readall make notes table shows elapsed time seconds benchmark run local tile systems machines remote file accesses made differences times due solely hardware operating system implementation amount real memory machine types sun mbytes ibm mbytes sun mbytes machines configured workstations servers low performance disks experiments repeated times figures parentheses standard deviations paper term loud unit refers load server single client workstation running benchmark server load varied initiating benchmark simultaneously multiple client workstations waiting complete refrain term client reporting benchmark results avoid misinterpretation referring human user observations network traffic load unit corresponds andrew users input benchmark read-only source subtree consisting files files source code application program total kilobytes size distinct phases benchmark makedir constructs target subtree identical structure source subtree copy scandir copies file source subtree target subtree recursively traverses target subtree examines status file read contents file readall scans byte file target subtree make compiles links files target subtree sun workstation local disk benchmark takes seconds complete files obtained locally times machines shown table performance observations fundamental quantity interest caching file system hit ratio observed actual venus caches files status information files snapshot caches machines indebted jerry saltzer alerting danger acm transactions computer systems vol february howard table distribution vice calls prototype call distribution testautb getfilestat fetch store setfilestat listdir server total calls cluster cluster emuemu- emumean notes data shown gathered one-month period figures parentheses standard deviations table iii prototvne benchmark performance load units benchmark time time testauth call relative relative absolute absolute notes data point trials clients servers sun client -entry cache figures parentheses standard deviations row column marked relative ratio absolute load load part data presented reproduced figure showed average file-cache hit ratio percent standard deviation percent average status-cache hit ratio percent standard deviation percent interest relative distribution client server interactions profile valuable improving server performance attention focused frequent calls table shows observed distribution vice calls accounted percent total data gathered l-month period servers distribution dramatically skewed calls accounting percent total testauth call validated cache entries getfilestat obtained status information files absent cache table shows percent calls vice fetch store involved file transfer ratio fetch calls store calls approximately performed series controlled experiments benchmark table iii presents total running time benchmark function server load table shows average response time frequent vice operation testauth experiments important observation table benchmark percent longer acm transactions computer systems vol february scale performance distributed file system table prototype server usage cpu utilization disk disk utilizautiliza- server samples total user system tion kbytes xfers tion kbytes xfers cluster cluster emuemu- notes data shown gathered servers weeks weekdays figures parentheses standard deviations load stand-alone case observation time testauth rose rapidly load indicating server saturation benchmark server load maximum feasible measuring server usage installed software servers maintain statistics cpu disk utilization data transfers disks table presents data servers -week period data restricted observations made weekdays period greatest system cpu utilizations table show servers loads evenly balanced fact confirmed table shows spread total number vice calls presented server circumstances moving users heavily loaded servers improved quality service considerably table reveals heavily servers showed average cpu utilization percent high figure average g-hour period closer examination raw data showed higher short-term cpu utilization figures neighborhood percent -minute averaging period common disk utilizations lower g-hour average percent short-term peaks rarely percent concluded figures server utilization data obtained benchmarks performance bottleneck prototype server cpu basis profiling servers deduced factors chiefly responsible high cpu utilization frequency context switches server processes time spent servers traversing full pathnames presented workstations summarize measurements reported section significant performance improvement reduce frequency cache validity checks reduce number server processes require workstations servers pathname traversals balance server usage reassigning users acm transactions computer systems vol february howard performance basis experience prototype set build revised version andrew file system constraint reuse code ideas resulting design fundamental architectural principle prototype workstations cache entire files collection dedicated autonomous servers analysis convinced shortcomings prototype due inadequacies realization basic architecture convinced promising path goal supporting clients server aspects prototype implementation remained unchanged venus server code run user-level processes communication servers clients based rpc paradigm independently optimized protocol transfer bulk data mechanism workstation kernels intercept forward file requests venus prototype retained aspects prototype changed details fall categories made enhance performance made improve operability system section describe made performance defer discussion operability section performance distinct areas -cache management -name resolution -communication server process structure -low-level storage representation orthogonal small degree interdependency inevitable discuss individual sections describe synthesis section cache management caching key andrew ability scale exploited redesign venus caches contents directories symbolic links addition files separate caches status data venus simple least-recently-used lru algorithm bounded size status cache virtual memory rapid servicing stat system calls entry information size file modification timestamp data cache resident local disk bsd buffering mechanism caching disk blocks memory transparent venus modifications cached file locally reflected back vice file closed mentioned earlier venus intercepts opening closing files participate reading writing individual bytes cached copy reasons integrity modifications directory made directly server responsible directory venus reflects change cached copy avoid refetching directory acm transactions computer systems vol february scale performance distributed file system significant point departure prototype manner cache entries consistent checking server open venus assumes cache entries valid notified workstation caches file directory server promises notify allowing modification workstation promise called callback dramatically reduces number cache validation requests received servers small amount cache validation traffic present replace callbacks lost machine network failures workstation rebooted venus considers cached files directories suspect generates cache validation request entry callback complicates system server venus maintain callback state information modifying file directory server notify workstation callback file amount callback state maintained server excessive performance degrade circumstances servers break callbacks reclaim storage finally potential 
inconsistency callback state maintained venus sync state maintained servers spite complications convinced importance callback reducing cache validation traffic callback reduces load servers considerably callback makes feasible resolve pathnames workstations section absence callback lookup component pathname generate cache validation request resolution conventional bsd system file unique fixed-length inode variable-length pathnames map inode routine performs mapping namei heavily time consuming parts kernel prototype venus aware pathnames notion inode vice file data representation servers vice pathname presented venus involved implicit namei operation server locate file resulted considerable cpu overhead servers obstacle scaling made full emulation bsd semantics difficult alleviate problems reintroduced notion two-level names vice file directory identified unique fixed-length fid entry directory maps component pathname fid venus performs logical equivalent namei operation maps vice pathnames fids servers presented fids fact unaware pathnames discussed section performed optimizations ensure implicit namei operations performed server accessing data fid bits long components -bit volume number -bit vnode number -bit uniquifier volume number identifies collection files called volume located server volumes discussed section vnode number index array tile storage information files single volume actual accessing file data fid efficient operation uniquifier guarantees fid acm transactions computer systems vol february howard history file system reuse vnode numbers keeping critical server data structures compact important note fid explicit location information moving files server invalidate contents directories cached workstations location information contained volume location database replicated server slowly changing database server identify location volume system aggregation files volumes make location database manageable size communication server process structure context switching paging overheads prototype server process client scale related problem server processes cache critical shared information address spaces bsd permit processes share virtual memory redesign solves problems single process service clients server multiple threads control provide convenient programming abstraction built user-level mechanism support multiple nonpreemptive lightweight processes lwps process context switching lwps order procedure-call times number lwps typically determined server initialized remains fixed lwp bound client duration single server operation client long-term state server thread control venus lwp mechanism act concurrently remote file access requests multiple user processes workstation prototype clients servers communicate rpc mechanism unlike prototype implementation kernel capable supporting hundreds thousands clients server integrated lwp mechanism server continue servicing client requests lwps blocked network events rpc mechanism runs variety workstations exactly-once semantics absence failures supports whole-file transfer optimized bulk transfer protocol secure authenticated communication workstations servers low-level storage representation decision retain bsd servers implied files hold vice data prototype mentioned section wary cost namei operations involved accessing data pathnames decided access files inodes pathnames internal inode interface visible user-level processes add set system calls vnode information vice file identifies inode file storing data data access server rapid consists index fid table vnode information iopen call read write data acm transactions computer systems vol february scale performance distributed file system efficiency venus mechanism local directory workstation cache directory files names placeholders cache entries venus accesses files directly inodes eliminated pathname lookups workstations servers explicit performed cached directories venus explicit lookups fact faster kernel lookups improved internal organization vice directories design result redesign understood examining remote file access detail suppose user process opens file pathname workstation kernel resolving detects vice file passes venus workstation lwps comprising venus cache examine directory component succession -if cache callback network communication -if cache callback server contacted copy fetched updated callback established -if cache fetched server callback established target file identified current cache copy created manner venus returns kernel opens cached copy returns handle user process end pathname traversal intermediate directories target tile cache callbacks future file involve network communication callback broken component venus regains control file closed modified locally updates server lru replacement algorithm periodically run reclaim cache space processing pathname component venus identifies server contacted examining volume field fid component entry volume present mapping cache venus contacts server connection requests location information enters mapping cache venus connection server responsible volume establishes connection connection fetch file directory connection establishment future requests workstation serviced lwps comprising server process description simplified view actual sequence events authentication protection checking network failures complicate matters considerably lwps venus concurrently servicing file access requests processes accesses cache data structures synchronized initial access file complex expensive accesses simpler cheaper locality inherent actual file access patterns makes strategy viable acm transactions computer systems vol february howard table vice interface fetch returns status optionally data file directory store remove create rename symlink link makedir removedir setlock releaselock getrootvolume getvolumeinfo getvolumestatus setvolumestatus connectfs disconnectfs removecallback gettime getstatistics checktoken disablegroup enablegroup breakcallback places callback stores status optionally data file deletes file creates file places callback file directory cross-volume renames illegal creates symbolic link tile directory creates hard link file cross-directory links illegal creates directory deletes directory empty locks file directory shared exclusive mode locks expire minutes unlocks file directory returns volume root vice returns servers store volume returns status information volume modifies status information volume initiates dialogue server terminates dialogue server specifies file venus flushed cache synchronizes workstation clock returns server cpu memory utilization determines authentication token user valid temporarily disables membership protection group enables membership temporarily disabled protection group revokes callback tile directory made server venus complexity implementation arises desire provide efficient notion file consistency multiple machines examined variety choices ranging strict serializability operations typically provided database systems laissez-faire attitude exemplified sun nfs file system file created workstation visible workstation seconds design converged consistency semantics -writes open file process workstation visible processes workstation immediately invisible network -once file closed made visible opens network open instances file reflect -all file operations protection visible network immediately operation completes -multiple workstations perform operation file concurrently conformance bsd semantics implicit locking performed application programs cooperate performing synchronization care serialization operations actual usage convinced easily understood model consistency distributed file system successfully implemented performance penalty acm transactions computer systems vol february scale performance distributed file system table andrew benchmark times load units time time phase seconds seconds relative absolute makedir copy scandir readall make notes clients ibm token ring server sun ethernet clients router hop server hops experiments repeated times figures parentheses standard deviations row column marked relative ratio absolute load 
load finally important note describe paper relevant scale typically bsd emulation security discussed current interface venus vice summarized table effect performance revised implementation andrew file system operational year evaluation system focuses questions effective anticipated improvement scalability realized characteristics system normal operation question addressed section information pertinent question presented section scalability investigate behavior system repeated experiments performed prototype server sun experiments prototype clients ibm-rts table shows absolute relative times benchmark function server load times individual phases benchmark shown table figure presents data graphically compares prototype data table iii performance penalty remote access reduced considerably data tables show andrew workstation percent slower stand-alone workstation prototype percent slower improvement scalability remarkable prototype benchmark times long load load current system takes long load load load takes percent longer table shows copy make phases susceptible server load files written phases interactions server acm transactions computer systems vol february howard prototype --revised andrew file system load units fig relative running time benchmark figure compares degradation performance prototype current andrew file system function load clients sun prototype ibm rts current file system server sun cases tables iii present information greater detail table vii andrew server utilization benchmark load units utilization percent cpu disk notes table shows sun server cpu disk utilization function load utilizations averaged entire duration benchmark data obtained experiment table experiments repeated times figures parentheses standard deviations part data reproduced figure file stores copy phase files fetched callbacks established contrast scandir readall phases barely affected load callback eliminates interactions server phases table vii figure present cpu disk utilization server benchmark cpu utilization rises percent load acm transactions computer systems vol february scale performance distributed file system andrew cpu utilization --andrew disk utilization -e---- e---load units fig andrew server utilization benchmark server cpu disk utilization presented figure function load clients ibm rts single sun server table vii presents information greater detail percent load disk utilization percent load server cpu limits performance system bottleneck prototype performance load require efficient server software faster server cpu figure shows anomaly load data table vii show high standard deviation suspect server activity unrelated experiments occurred trials summary results section demonstrate design improved scalability considerably load system saturated load unit corresponds typical andrew users scale goal users server met general observations table viii presents server cpu disk utilizations andrew figures shown averages a-hour period weekdays servers show cpu utilizations percent servers vice shows utilization percent disk utilization correspondingly high high standard deviation cpu utilization leads anomaly caused system maintenance activities unexpectedly performed day night server vice hand shows cpu utilization percent small standard deviation disk utilization percent highest server high utilization explained fact acm transactions computer systems vol february howard table viii andrew server usage cpu utilization disk disk disk utiliutili- utiliserver samples total user system zation kbytes xation kbytes zation kbytes vice vice vice vice vice vice vice vice vice vice vice vice vice k-g notes data gathered week february february omitted table servers experimental versions system servers table vicell vicel vicel fourth disk cases utilization disk percent servers listed sun figures parentheses standard deviations server stores bulletin boards collection directories frequently accessed modified users distribution vice calls three-day period shown table servers calls vice stores common system files workstations vice server stores bulletin boards frequent call gettime workstations synchronize clocks implicit keepalive frequent call fetchstatus conjecture calls generated users listing directories parts file space cached interesting caching fetches dominate stores call removecb made venus flushes cache entry server vice shows highest occurrences removecb files stores exhibit poor locality precisely behavior expect bulletin boards users tend read bulletin board entries vice special server operations staff shows higher occurrence removecb acm transactions computer systems vol february howard table active users andrew servers average active users server type volumes peak period vice read-only system vice read-write user vice read-write user vice read-only read-write vice read-only system vice read-write system vice special vice read-write bboard vice read-write user vice read-write user vice read-write user vice read-write user vice read-write user notes column describes kind volumes stored server server vice stores bulletin boards frequently updated set directories shared users server vice read-write volumes read-only clones vice vice vice vice common system volumes read-only workstations system data servers running experimental version system shown active user server behalf workstation interacted server past minutes peak period defined weekdays basis measurements modified venus remove callback groups files file time modification reduced observed frequency removecb considerably table derived set observations table shows type data stored server average number persons actively server peak period servers active users behalf request gettime received minutes interpreting data mind user files servers present detailed data network utilization low typically neighborhood percent lo-megabit ethernet percent -megabit token ring routers interconnect segments local area network occasionally shown signs overload problem concern require attention future comparison remote-open file system caching entire files local disks andrew file system motivated primarily considerations scale -locality tile typical users makes caching attractive server load network traffic reduced whole-file transfer approach contacts servers opens closes read write operations numerous transparent servers network traffic acm transactions computer systems vol february scale performance distributed file system -the study ousterhout shown tiles bsd environment read entirety whole-file transfer exploits property allowing efficient bulk data transfer protocols -disk caches retain entries reboots surprisingly frequent event workstation environments files accessed typical user modified system amount data fetched reboot small -finally caching entire files simplifies cache management venus track tiles cache individual pages approach drawbacks diskless operation workstations require local disks acceptable performance files larger local disk cache accessed strict emulation bsd concurrent read write semantics workstations impossible reads writes intercepted building distributed database file system difficult impossible disadvantages persisted approach believed provide superior performance large scale system drawbacks listed previous paragraph proved significant actual usage environment discussions section established andrew file system scale alternative design produced equivalent results critical scaling caching whole-file transfer rest section examines questions detail remote open number distributed file systems locus ibis newcastle connection research literature surveyed svobodova design systems matured point vendor-supported implementations sun microsystem nfs rfs locus details systems vary considerably share 
fundamental property data file fetched masse remote site potentially participates individual read write operation buffering read-ahead employed systems improve performance remote site conceptually involved operation call property remote open reminiscent situation file opened remote site local site andrew file system cedar file system employ caching entire files remote access mechanism explore vital approach scaling compared andrew controlled conditions representative set remote open file systems selected sun microsystem nfs candidate comparison reasons nfs mature product successful vendor distributed computing hardware software research prototype sun spent considerable amount time effort tune refine nfs deficiencies performance due acm transactions computer systems vol february howard basic architecture inadequacies implementation comparison andrew nfs yield significant insights scaling characteristics caching remote-open file systems nfs andrew run precisely hardware operating system fact coexist machine simultaneously nfs allowed conduct controlled experiments significant variable file system component performance differences observed due design implementation distributed file systems artifacts hardware network operating system variation perception bsd user community nfs facto standard curious andrew measured fair pointed nfs designed operation large environment designed distributed file system small collection trusted workstations emphasized comparison based single benchmark benchmarks yield results emphasize focus comparison scalability question interest performance perceived workstation degrade load server increases justifies comparison nfs andrew identical hardware configurations question compare cost nfs andrew configurations level performance load price hardware subject variety factors scope paper address issue sun network file system section present minimal overview nfs details relevant paper discussed information obtained documentation nfs distinguish client server machines workstation export subtree file system server servers identified mounted individually transparent file location facility andrew client server components nfs implemented kernel efficient counterparts andrew nfs caches inodes individual pages file memory file open kernel checks remote server fetch revalidate cached inode cached file pages cached inode up-to-date validity check directory inodes suppressed check made seconds file open remote site treated local disk read-ahead write-behind pages difficult characterize consistency semantics nfs files created workstation visible seconds indeterminate writes file site visible sites file open reading opens file flushed server caching file pages processes workstations perform interleaved write file produce acm transactions computer systems vol february scale performance distributed file system result sequence writes processes workstation nfs strict emulation bsd semantics open close action consistency andrew results comparison benchmark section basis comparison nfs andrew eighteen sun workstations local disks experiments added andrew kernel intercepts workstations venus run modifications orthogonal nfs sun server andrew nfs trials clients servers communicated lo-megabit ethernet set experiments operating files nfs set operating files andrew run andrew experiments consisted subsets cold cache set workstation caches cleared trial warm cache set caches left unaltered target subtree re-created trial benchmark benefit warm cache avoids fetching tiles source subtree cases trials performed experiment ran functional problems nfs high loads loads ten greater consistently observed workstations terminated final phase benchmark prematurely file system errors examination nfs source code revealed problem caused lost rpc reply packets servers periods high network activity rpc protocol nfs based unreliable datagrams depends retries operation level rpc level nonidempotent file system calls retried nfs failed reflected file system errors running benchmark effective server load lower nominal load phase experiments results presented biased favor nfs high loads encounter functional problems nature andrew table figure present running time seconds benchmark function server load nfs performs slightly andrew low loads performance degrades rapidly increasing load crossover point load warm cache case cold cache case close examination table reveals scandir readall make phases contribute difference nfs andrew performance caching callback andrew result time phases slightly affected load nfs lack disk cache check server file open time phases considerably load dependent warm cache andrew improves time copy phase figure table xii present data server cpu utilization experiments load server cpu utilization percent nfs andrew approximately percent cold warm cache cases load server cpu utilization saturates percent nfs andrew percent cold cache case percent warm cache case acm transactions computer systems vol february howard table benchwork times nfs andrew phase load units nfs time seconds andrew andrew cold warm standalone copy make makedir scandir readall notes data correspond set experiments table xii describes hardware configuration problems encountered nfs loads stand-alone numbers reproduced table part data reproduced figure figures parentheses standard deviations acm transactions computer systems vol february scale performance distributed file system andrew cold cache andrew warm cache nfs -a- load units fig nfs andrew benchmark times figures compares benchmark times nfs andrew file system function load table presents data greater detail table xii describes conditions data obtained c-o ---b-- andrew cold cache andrew warm cache nfs load units fig nfs andrew server cpu utilization figure compares server cpu utilizations nfs andrew function load table xii presents data greater detail describes conditions obtained acm transactions computer systems vol february scale performance distributed file system andrew warm cache nfs disk nfs diik load units fig nfs andrew server disk utilization figure compares server disk utilizations nfs andrew function load table xii presents data greater detail describes conditions obtained table xiii network traffic andrew nfs andrew nfs total packets packets server client packets client server notes table presents observed network traffic generated benchmark single client server andrew ease server sun ethernet connected router ibm client token ring nfs case server sun ethernet cable sun cliant esperiments repeated times figures parentheses standard deviations data server disk utilization presented figure table xii nfs disks server utilizations rising percent load percent load andrew oniy server disks utilization rising percent load percent load cold cache case disk utilization slightly substantially lower warm cache case quantity interest relative amount network traffic generated nfs andrew execution benchmark table xiii presents information table nfs generates times packets andrew load acm transactions computer systems vol febnary howard table xiv latency nfs andrew file size byte andrew cold time milliseconds andrew warm nfs stand-alone notes table shows latency milliseconds function file size latency defined total time open file read byte close file sun server single sun client cases andrew warm cache case file accessed cache cold cache numbers correspond cases file fetched server figures parentheses standard deviations low latency obvious 
advantage remote-open file systems quantify fact ran series experiments opened file read byte closed table xiv illustrates effect file size latency nfs andrew latency independent tile size nfs thrice local file andrew file cache latency close nfs file cache latency increases file size interpreting andrew data important note close-system call completes venus transfers tile server conclude observations clear andrew scaling characteristics superior nfs improved scaling andrew achieved price substantially poorer small-scale performance andrew implemented user space nfs kernel anticipate significant reduction overhead move andrew code kernel untapped potential improved performance andrew similar potential nfs finally andrew well-defined consistency semantics support security operability pleased observe additional functionality incorporated detriment primary goal scalability operability scale system grows users increasingly dependent operability assumes major significance prototpye paid scant attention operability imperative address aspect system redesign goal build system easy small operational staff run monitor minim inconvenience users heart operability problems prototype inflexible mapping vice files server disk storage mapping section deficient number ways vice constructed collections files glued bsd mount mechanism entire disk partitions mounted acm transactions computer systems vol february scale performance distributed file system sets files disk partitions independently located vice minimize internal fragmentation disks partitions large typically consisting files ten users fact repartitioning disk off-line reduced flexibility embedding file location information file storage structure made movement files servers difficult required structural modifications storage servers modifications files move progress lost implement quota system important system large number users mechanisms file location file replication cumbersome lack well-defined consistency guarantees embedded location database wrong failures propagation replicated files left inconsistent standard utilities create backup copies files system utilities adequate single-site system convenient distributed environment files moved backed wiring-in location information made restoration files difficult backup complicated fact consistent snapshot user files made entire disk partition files off-line felt factor unacceptable imposition users address problems redesign data structuring primitive called volume rest section describe volumes show improved operability system volumes volume collection files forming partial subtree vice space volumes glued mount points form complete space mount point leaf node volume specifies volume root directory attached node mount points visible pathnames venus transparently recognizes crosses mount points resolution mount mechanism vice conceptually similar standard bsd mount mechanism volume resides single disk partition server grow shrink size volume sizes small volumes partition found convenient associate separate volume user mentioned section volume-to-server mapping information maintained volume location database replicated servers volume movement balancing disk space utilization servers accomplished redistributing volumes partitions servers volume moved volume location database updated update synchronous servers temporary forwarding acm transactions computer systems vol february howard information left original server move workstation identify server responsible volume volume update moved actual movement accomplished creating frozen copy-on-write snapshot volume called clone constructing machine-independent representation clone shipping site regenerating volume remote site process volume updated original site volume change procedure repeated incremental clone shipping files changed finally volume briefly disabled incremental shipped volume made site requests directed volume move operation atomic server crashes operation aborted quotas quotas implemented system volume basis user system assigned volume volume assigned quota responsibility managing allocated space volume left user access lists permitting user store files volume belonging user owner volume charged usage system administrators change quotas easily volumes created read-only replication executable files system programs files upper levels vice space frequently read seldom updated read-only replication files multiple servers improves availability balances load callbacks needed files making access efficient read-only replication supported granularity entire volume volume location database specifies server read-write copy volume list read-only replication sites section read-only clone volume created propagated efficiently replication sites volume propagation atomic operation mutual consistency files read-only volume guaranteed replication sites period time replication sites copy volume copy read-only volumes valuable system administration form basis orderly release process system software easy back release event unanticipated problem collection servers identical sets read-only volumes read-write volumes introduced withdrawn service virtually impact users additional measure availability serviceability backup volumes form basis backup restoration mechanism redesign backup volume read-only clone made creating frozen snapshot files cloning efficient operation users rarely notice loss access volume asynchronous mechanism transfers acm transactions computer systems vol february scale performance distributed file system clone staging machine dumped tape staging software aware internal structure volumes dumps restores entirety volumes restored server server-specific information embedded volume experience shown large fraction file restore requests arise accidental deletion users handle common special case cloned read-only backup volume user files made read-only subtree user home directory restoration files -hour period performed users normal file operations cloning copy-on-write conserve disk storage convenient backup strategy achieved modest expense summary experience volumes data structuring mechanism positive volumes provide level operational transparency supported file system aware operational standpoint system flat space named volumes file system hierarchy constructed volumes orthogonal ability associate disk usage quotas volumes ease volumes moved servers proved considerable actual operation system backup mechanism simple efficient seldom disrupts normal user activities observations lead conclude volume abstraction similar indispensible large distributed file system conclusion scale impacts andrew areas performance operability large number users workstations system resulted sizable authentication network databases system grows existing mechanisms update query databases inadequate fault-tolerance area scaling stresses andrew access vice file worst case involve multiple servers network elements components file access succeed read-only replication system files alleviates problem extent solve uniform location-transparent file space major conceptual simplification failure modes arise difficult naive user comprehend issue software version control orderly release critical software workstations increase importance system grows size choosing focus scale omitted discussion important aspects evolution andrew file system security emulation unix semantics areas fundamental file system network topology hardware software examples pay close attention similar areas design implementation time paper written early workstations servers workstations public acm transactions computer systems vol february howard terminal rooms registered users system andrew regularly data stored servers approximately megabytes distributed volumes andrew sole computing facility cmu primary computational environment courses research projects future usage experience confidence system scale minimal workstations eventual goal workstations large gap performance data presented paper confirms high-level architecture scaling inevitable significant made quantum increase size system thought address variety issues moving venus server code 
kernel improve performance considerably changing kernel intercept mechanism industry standard simplify maintenance portability system ability users define protection groups simplify administration users dependent system availability increasingly important form replication writable files eventually distributed nature system inherent complexity make difficult system troubleshoot monitoring fault isolation diagnostic tools span levels hardware software increasingly important finally system grows decentralized administration physical dispersal servers conclusion present state andrew file system satisfaction pleased current performance fact compares favorably prominent alternative distributed file system time growth stress skill patience ingenuity acknowledgments andrew file system developed information technology center carnegie-mellon colleagues contributions project users patience dealing system development express special appreciation vasilis apostolides assisted running benchmarks compared andrew nfs suggestions james kistler richard snodgrass james peterson sosp referees improved paper ways brownbridge marshall randell newcastle connection softw pratt exper kazar synchronization caching issues andrew file system tech rep cmuitc- information technology center carnegie mellon univ pittsburgh june morris satyanarayanan conner howard rosenthal smith andrew distributed personal computing environment comnun acm mar ousterhout costa harrison kunze kupfer thompson trace-driven analysis unix bsd file system proceedings acmsymposium acm transactions computer systems vol february scale performance distributed file system operating system principles orcas island wash dec acm york rifkin forbes hamilton sabrio shah yueh rfs architectural overview usenix conference proceedings atlanta summer usenix association berkeley calif satyanarayanan howard nichols sidebotham spector west itc distributed file system principles design proceedings acm symposium operating system principles orcas island wash dec acm york schroeder gifford needham caching file system programmer workstation proceedings acm symposium operating system principles orcas island wash dec acm york sidebotham volumes andrew file system data structuring primitive european unix user group conference proceedings august tech rep cmuitc- information technology center carnegie mellon univ pittsburgh sun microsystems networking sun workstation sun microsystems mountain view calif svobodova file servers network-based distributed systems comput suru dec tichy ruan distributed file system tech rep csd-trcomputer science dept purdue univ west lafayette walker popek english kline thiel locus distributed operating system proceedings acm symposium operating system principles bretton woods oct acm york received revised august accepted september acm transactions computer systems vol february 
mimo feedback control enforce policies interrelated metrics application apache web server yixin diao neha gandhi joseph hellerstein sujay parekh dawn tilbury ibm watson research center fdiao hellers sujayg ibm department mechanical engineering michigan fgandhin tilburyg umich abstract policy-based management means systems operate business impedance mismatch policies administrators controls apache web server administrators control cpu memory utilizations indirectly manipulating tuning parameters maxclients keepalive interest feedback control bridge impedance mismatch efforts focused single metric manipulated single control considered interactions controls common computing systems paper shows multiple-input multiple-output mimo control theory enforce policies interrelated metrics mimo model target system apache case design feedback controllers mimo model captures interactions identify infeasible metric policies addition mimo control techniques provide considerable benefit handling trade-offs speed metric convergence sensitivity random fluctuations enforcing desired policies keywords web server resource management policy-based management control theory mimo control introduction wide-spread exploitation information technology motivated policy-based management resources business policies limit memory cpu consumed web servers policies originate business including providing sufficient capacity co-located applications file server database server avoiding thrashing failures result overutilization ensuring sufficient capacity remaining handle workload surges server failures describe control-theory based approach enforcing policies interrelated metrics apply approach apache web server frequently impedance mismatch policies administrators controls apache web server administrators control cpu memory utilizations denoted cpu mem apache application directly administrators operate indirectly adjusting tuning parameters operations staff conduct experiments determine desired utilizations achieved controls widely parameters maxclients maximum number clients connect apache server keepalive timeout determines long idle connection maintained http time time light workload heavy workload sess sec sess sec figure effect static settings note cpuincreases light workload heavy workload surprisingly time-consuming error-prone skills intensive manually adjust achieve desired settings cpu mem worse effort repeated workload illustrated figure shows cpu mem affected workload light heavy precise definition light heavy discussed mem cpu increases light workload heavy workload ooh figure block diagram feedback system control cpu memory utilizations propose feedback control bridge gap administrative goals controls specifics shown figure assume policies cpu mem terms desired values metrics denoted cpu mem controller reads measured values cpu mem computes control error thecpucontrol error denoted ecpu cpu cpu emem defined analogously mem controller current past values time-averaged values reduce measurement overhead inherent variability metrics cpu utilization makes instantaneous control impractical emem siso memmem kac siso cpu cpu ecpu emem memmem cpu cpu ecpu mimoc model siso model mimo controller siso controller mimo figure architectures feedback control apache control error adjustmcandkawith goal achieving desired target utilization values ecpu emem control theory sound rigorous mathematical principles analyze dynamical systems design controllers widely mechanical aeronautical chemical engineering concern applying standard linear control techniques computer systems domain non-linearities abound well-developed theory non-linear control difficult apply generalize systems insight linear control theory adopt pragmatic perspective construct analyze properties real-life closed-loop computer systems linear control theory prior work applying control theory computing systems focused singleinput single-output siso techniques single control single metric regulate examples include flow congestion control differentiated caching multimedia streaming differentiated web services control server cases system multiple-input multipleoutput mimo approach decompose multiple simpler independent siso control loops cases complex systems decomposed interactions controls main contribution paper general mimo techniques design analysis feedback control enforce policies presence interactions approaches controlling mimo system shown figure figure mimo ways model predict behavior apache server design controller show siso models accurately model cpu interaction mimo model works show accurate model cpu mem accurately determining space feasible policies metrics terms control turns system independent simpler siso controllers figure works surprisingly inspite siso modeling inaccuracies due limited nature interactions show mimo techniques linear quadratic regulation lqr provide considerable benefit terms balancing trade-off speed convergence desired result sensitivity random fluctuations note aforementioned studies focus principles models empirical validation contrast proceed lines work empirical models developed validated server research improving web server performance areas admission control schemes adaptive content delivery caching mechanisms focus client-perceived metrics response time knowledge work discusses tuning server parameters goals related administrative policies remainder paper organized section background apache architecture modifications enabling feedback control section details approach mimo modeling compares results models obtained siso model section presents evaluates controller designs conclusions contained section apache architecture enabling dynamic control apache unix structured pool worker processes monitored controlled master process shown figure worker processes responsible handling communications web clients including work required generate responses worker process handles connection time continues handle connection connection terminated worker idle consecutive requests connected client parameter limits size worker pool imposing limitation processing capacity server higher apache process client requests large excessive resource consumption degrades performance clients maxclientskeepalive figure apache architecture session flow figure state transition diagram worker processes idle state worker waiting client connection point enters user state waits http request worker busy processing request sending reply time sending http reply receipt request http persistent connections spent user state user time apache tuning parameter controls maximum time worker remain user state client tcp connection closed allowing worker handle clients large cpu memory underutilized clients requests process connect server reducing timeout means workers spend time user state cpu increases timeout small tcp connection terminates prematurely reduces benefits persistent connections order dynamically control apache needed programmatic access performance metrics set tuning parameters on-line facilitate capabilities implemented control module set interface special tcp port metrics stored apache scoreboard shared memory area common apache processes updated asynchronously control converted static configuration parameters variables accessed scoreboard picked asynchronously affected components convenient time processing major change apache dynamic control concerns effect unmodified apache incorporates heuristic dynamically change server pool size based parameters minspareservers maxspareservers controller replace heuristic modified effect directly controls size worker pool master process creates kills worker processes ensure total pool size matches controller note slightly changed semantics affect web administrators set anymore modeling apache section describes approach modeling apache based statistical black box models quantify relationship tuning parameters metrics cpu mem describe experimental environment obtain data models constructed describe models briefly evaluate section show models determine feasible policies realizable combinations cpu mem predict dynamic behavior system experimental environment testbed consists server running apache connected mbps lan clients running synthetic workload generator server pentium 
iii mhz ram running linux client machine runs synthetic workload generator simulates activity clients lph lph figure depiction workload model sessions displayed solid dashed arrows long arrows denote clicks short arrows objects burst workload generator publicly-available httperf core generate http requests manage user sessions written wrapper scripts emulate stochastic user behavior wagon model liu shown figure wagon structures workload multiple sessions represent series user interactions multiple clicks end-user requests generates burst http requests represents web pages multiple objects workload parameters session arrival rate number clicks session burst size number objects burst time distribution time clicks table summarizes parameters based data reported file access distributions webstone benchmark based findings session arrivals poisson rate simulates heavier sessions sec lighter sessions sec workload table workload parameters parameter distribution parameters session length clicks lognormal burst length urls gaussian user time lognormal session inter-arrival time exponential heavy light heavier workload servercpucan utilized hundred server processes experiments increased default linux process limit control code ensures integer range values integral seconds minimum maximum values larger found small effect cpu mem metrics time-averaged values choice averaging interval commonly called sample time control theory affect performance controller chose experimentally sample time seconds order balance competing goals reacting quickly avoiding excessive measurement overhead reaction random fluctuations system identification steps black-box system identification determining experiments run collecting data constructing model data experiments varied tuning parameters heavy workload first-order linear time invariant arx model fit data squares regression form model shown equation parameters estimated squares regression indexes time vector siso model scalars apache system stochastic nonlinear found first-order linear model sufficient control design data collection tuning parameters varied manner properties satisfied sufficient variability frequency content excite dynamics system addition dense uniform coverage range values parameters parameters varied cover input space time time input input coverage input space figure inputs system coverage input space inputs concurrently pairs plotted diamonds model care required avoid highly non-linear regions poor model fit result separate models constructed regions case apache server input space constructed saturation limits tuning parameters discrete sine waves high frequency components form steps low frequency components frequency sine wave sufficient identify sine wave period seconds amplitude values greater saturated control implementation noted section sine wave period seconds amplitude figure shows input signals plotted versus time coverage provide input space multiple siso identifications begin modeling apache siso models figure siso model captures relationship cpu design controller model quantifies relationship mem design identifying model cpu sine wave figure model mem sine wave figure important factor affects outcome siso identification tuning parameter run siso models valid operating region set sine wave construct model mem identifying model cpu similarly set identifying model mem figure plot data testbed runs inputs equation displays resulting model time series form note term model negative result inverse relationship cpu range cpu range term includes scaling factor makes order magnitude smaller term similarly data figure identify relationship mem displayed equation term includes scaling factor range mem range cpuk cpuk kak cpu time cpu time cpu time siso data siso data mimo data varying varying figure experimental input-output data system memk memk mck mentioned earlier performing separate siso identifications assumed tuning parameters interact figure sine wave significant effect cpu assumption invalid motivates mimo techniques system identification mimo identification apache mimo models shown figure figure identifying mimo model requires simultaneously varying order capture interactions parameters discrete sine waves frequencies prime good coverage input space obtained figure figure plots results mimo identification experiments data first-order linear mimo model constructed shown equation note matrices scalars matrix approach model combined effect tuning parameters utilizations specifically elements matrix non-zero effect mem effect cpu cpuk memk cpuk memk kak mck model evaluation model evaluations one-step prediction metric time predicted based measured values time multi-step prediction predicted based prediction time focus cpu low variability mem makes easy predict figure plots predicted versus measured values one-step predictions cpu perfect model observations diamonds line unit slope part plots siso results siso data fit good part mimo model mimo data fit good good siso model actual actual actual siso model siso data mimo model mimo data siso model mimo data figure results one-step ahead predictions cpu utilization plot x-axis actual y-axis predicted line actual equals predicted occurs model perfect cpu mem time cpu mem time siso model prediction mimo model prediction figure results multiple step prediction plot solid line experimental data dashed line model prediction tuning parameters varied experiment siso model provide fit mimo model answer siso identification vary tells mimo model siso model mimo data vary siso model considerably worse shown part figure multiple step predictions data set figure plots response real system siso mimo models series clear figure siso model account interaction effect cpu contrast mimo model figure accurate predictions cpu regions accuracy mimo model degrades limitation linear model accurate center operating region accurate edges region keepalive maxclients cpu memory range inputs predicted range feasible outputs figure determining feasible regions steady state values metrics markers rectangle parallelogram correspondance input values model predictions feasible policy settings common problem practice determining feasibility interrelated policies utilization policies cpu mem workload combinations cpu mem achieved tuning parameters desirable administrators policy feasible controls equation equation equation predict feasible steady-state values metrics cpu mem based valid ranges tuning parameters illustrated figure part displays range part shows range predicted steady-state values siso models dashed rectangle mimo model solid parallelogram bold represent candidate cpu mem policies apache system feasible circled fall rectangle predicted feasible siso models predicted feasible mimo model inputs part point cpu mem invert mimo model determine inputs model predicts point realized range inputs considered part experimental results confirm show realized larger values cpu mem mimo model determines achieved negative experimental results confirm cpu mem feasible combination metric values feasible test profile section models make predictions dynamic behavior scalar version time-series model equation solution equation apparent parameter plays large role determining response system input control theory parameter called pole system referring siso models note pole cpu model equation pole mem model equation property determined pole stability jaj system unstable grows bound terms explode approaches infinity jaj system stable terms 
remain bounded noted model predict steady-state values output constant inputs finding fixed-points equation dynamic property determined pole speed response jaj approaches speed response input faster takes time step output converge input jaj approaches takes time steps output converge steady-state input figure acpu amem mem settles faster cpu final property determined pole system respond oscillatory manner input behavior system oscillatory due terms equation term positive odd term negative vector case scalar parameters equation replaced matrices solution vector time-series equation similar equation case response system governed eigenvalues poles vector case note vector case poles complex real imaginary parts presence imaginary part oscillations real part positive multiple poles slowest dominant poles magnitude closest determine settling times poles mimo model equation indicating model predicts slower response predicted siso model control design section describes methodology designing feedback controllers applies apache architectures shown figure section introduces methodology section applies methodology designing siso controllers section mimo design lqr section discusses controller robustness workloads design methodology methodology controller design proportional integral controller robustness control widely mechanical engineering process control control operates control law kpek tuning parameter control error apache system desired policy cpu mem measured metric cpu mem siso control policies cpu mem considered individually mimo control considered control parameters proportional gain integral gain rule thumb proportional term increase speed response table closed loop models magnitude settling controller dominant pole time siso stable siso unstable unstable mimo lqr integral term eliminate steady-state error siso control scalars mimo control vectors matrices design controller set achieve desired control specifications steady-state error small settling times commonly approach controller design pole placement closed-loop system poles chosen meet desired criteria steps pole placement control design outlined desired transient performance settling time closed loop system determine required closed loop pole locations performance specifications desired settling time sampling time closed-loop poles magnitude steady-state error nonzero derive closed loop system model open loop system model obtained section control law calculate control gains matching poles closed loop system model desired closed loop poles desired closed loop poles step poles closed loop system model found functions step control gains found equating solving control design based model system predict behavior true system control design evaluated experimentally determine suitability multiple siso controller control design multiple siso controller shown figure control loops controls cpu controls mem begin settling time closed loop system criteria settling time closed loop system open loop system experimental measurements suggest settling time cpu seconds mem seconds determine proportional integral gains required achieve settling times refer table summary control gains multiple siso controller diag diag figure shows experimental evaluation multiple siso control system cpu mem time cpu mem time siso controllers siso controllers proper gains large gains figure control performance multiple siso controller thin thick solid lines experimental data mimo model prediction equation dashed lines cpu mem desired policies clarity model prediction omitted shows similar degree oscillation cpu system run open loop controllers started heavy workload sessions sec applied entire experimental period excellent job regulating mem regulating cpu stochastic behavior metric note mimo model predicts dip cpu utilization occurs due interaction reject disturbance regulate cpu back desired level control theory needed result answer choose arbitrary values gains performance good figure gains table magnitude dominant pole exceeds control theory predicts choice gains controller unstable confirmed figure large oscillations turn induces oscillations mem cpu mimo nature system simplicity siso controllers analytically designed siso controller works surprisingly factor contributing success unidirectional coupling affects mem cpu affects cpu controller robust achieve desired utilization policies explicit model interaction mimo controller design lqr approach control trade-off short settling times overreacting random fluctuations end approach controller design choosing gains based cost function specifically lqr linear quadratic regulator finds control gains minimize quadratic cost function ruk cpu mem time cpu mem time mimo lqr mimo lqr heavy workload light workload figure control performance mimo controllers thin thick solid lines experimental data mimo model prediction equation dashed lines cpu mem desired policies cost function includes control errors accumulated errors control inputs matrices determine trade-off control error control gain intuition large compared larger control gains smaller poles controller react quickly deviations desired policy metrics hand large compared reverse controller responds slowly deviations avoiding over-reactions random fluctuations efficient numerical methods solve optimization problem implemented computer-aided design tools control design problem shifted determining desired closedloop poles choosing weighting matrices lqr problem common approach normalize terms cost function cpu mem utilization range valid region vary choose diag scale inputs order magnitude control errors choose diag weight control errors heavily accumulated control errors method designed aggressive controller smaller control gains table lqr design method setting time design criteria transient performance closed loop system inferred closed loop pole locations case dominant closed loop pole located predicts settling time seconds closed loop pole imaginary part closed loop system oscillatory control performance shown figure variability control inputs reduced cpu mem utilizations oscillatory time controller fast track desired utilization policies interesting compare control performance multiple siso controller figure mimo lqr controller figure generally mimo lqr controller results variability cpu memory utilizations handle interactions tuning parameters metrics system controller robustness workload experimental results presented single workload heavy workload section practice workload unknown priori time determine feedback control design performs presence unknowns ran experiment lighter workload table mimo lqr controller section redesign experimental results shown figure model prediction accurate controller performs conclusions paper describes multiple-input multiple-output mimo techniques address policies interrelated metrics ways mimo model target system apache case show siso sufficient obtain accurate model cpu interaction mimo captures interactions accurate model accurate model cpu mem benefit determining feasible policies show mimo model predict infeasible policies cpu mem siso model fails mimo control turns system multiple siso controllers works surprisingly limited nature interactions mimo techniques linear quadratic regulation lqr provide benefit handling trade-off speed metric convergence sensitivity random fluctuations results controller siso case system unequal numbers inputs outputs siso techniques applicable mimo design techniques paper apply future work extend results ways understand limits models employ dealing notoriously non-linear metrics response times adapt controller models modeling errors discovered system configuration workload apache software foundation http apache franklin powell emani-naeini 
feedback control dynamic systems reading massachusetts addison-wesley keshav control-theoretic approach flow control proceedings acm sigcomm sept hollot misra towsley gong control theoretic analysis red proceedings ieee infocom conference anchorage alaska apr saxena abdelzaher differentiated caching services control-theoretic approach international conference distributed computing systems apr nahrstedt control-based middleware framework quality service applications ieee journal selected areas communication abdelzaher stankovic son feedback control architecture design methodology service delay guarantees web servers tech rep cs- virginia department computer science parekh gandhi hellerstein tilbury bigus jayram control theory achieve service level objectives performance management proceedings ifip ieee international symposium integrated network management chen mohapatra chen admission control scheme predictable server response time web accesses proceedings world wide web conference hong kong abdelzaher bhatti adaptive content delivery web server qos international workshop quality service london june iyengar challenger improving web server performance caching dynamic data usenix symposium internet technologies systems dec krishnamurthy wills analyzing factors influence end-to-end web performance ninth international world wide web conference amsterdam netherlands fielding gettys mogul frystyk masinter leach berners-lee rfc hypertext transfer protocol http internet engineering task force ietf june http ietf rfc rfc txt mosberger jin httperf tool measuring web server performance workshop internet server performance wisp acm june liu niclausse jalpa-villanueva barbier traffic model performance evaluation web servers tech rep inria dec mindcraft webstone web server benchmark http mindcraft webstone ljung system identification theory user upper saddle river prentice hall franklin powell workman digital control dynamic systems reading massachusetts addison-wesley 
exploiting gray-box knowledge buffer-cache management nathan burnett john bent andrea arpaci-dusseau remzi arpaci-dusseau department computer sciences wisconsin madison ncb johnbent dusseau remzi wisc abstract buffer-cache replacement policy significant impact performance intensive applications paper introduce simple fingerprinting tool dust uncovers replacement policy specifically identify initial access order recency access frequency access long-term history determine blocks replaced buffer cache show fingerprinting tool identify popular replacement policies literature fifo lru lfu clock random segmented fifo lru-k found current systems netbsd linux solaris demonstrate usefulness fingerprinting cache replacement policy modifying web server knowledge specifically web server infers contents file cache modeling replacement policy set page requests show servicing web pages believed resident buffer cache improve average response time throughput introduction specific algorithms manage buffer cache significantly impact performance o-intensive applications knowledge hidden user processes determine behavior buffer cache implementors forced rely documentation access source code general knowledge buffer caches behave relying hoc methods propose fingerprinting automatically uncover characteristics buffer cache paper describe dust simple fingerprinting tool identify buffer-cache replacement policy specifically identify initial access order recency access frequency access historical information fingerprinting microbenchmarking techniques identify algorithms policies system test idea fingerprinting insert probes underlying system observe resulting behavior visible outputs carefully controlling probes matching resulting output fingerprints algorithms identify algorithm system test key challenge inject probes create distinctive fingerprints algorithmic characteristics isolated significant advantages fingerprints automatically identifying internal algorithms fingerprinting eliminates developer obtain documentation source code understand underlying system fingerprinting enables programmers sophisticated experience algorithmic knowledge improve performance fingerprinting uncover bugs hidden complexities systems development deployed finally fingerprinting run-time allowing adaptive application modify behavior based characteristics underlying system paper investigate algorithmic knowledge exposing current contents buffer cache recent work shown o-intensive applications improve performance information contents file cache specifically applications handle data disk flexible order access blocks buffer cache disk current approaches suffer limitations require underlying export information accurately identify presence small files buffer cache observe application model simulate state buffer cache replacement policy file accesses dedicated web server greatly benefit knowing contents buffer cache servicing requests hit buffer cache implemented cache-aware web server based nest storage appliance show web server improves average response time throughput paper make contributions introduce dust fingerprinting tool automatically identifies cache replacement policies based prioritize initial access order recency access frequency access historical information demonstrate simulations dust distinguish variety replacement policies found literature fifo lru lfu random clock segmented fifo lru-k fingerprinting software identify replacement policies operating systems netbsd linux solaris show knowing replacement policy cache-aware web server service requests satisfied buffer cache obtain substantial performance improvements rest paper organized begin section describing fingerprinting approach section show simulation identify range popular replacement policies section identify replacement policies current operating systems section show web server exploit knowledge buffer-cache replacement policy improved performance briefly discuss related work section conclude section fingerprinting methodology describe dust software identifying page replacement policy employed operating system manipulating blocks accessed forcing evictions observing blocks replaced dust identify parameters page replacement policy algorithm dust relies probes infer current state buffer cache measuring time read byte file block determine block previously buffer cache intuitively probe slow infers block previously disk probe fast infers block cache dust correctly distinguish replacement polices identify file block attributes existing policies select victim block replacement search database research literature documentation existing operating systems identified attributes replacement order initial access block fifo recency accesses lru frequency accesses lfu historical accesses blocks correctly identify combinations attributes replacement policy note operating systems replacement policies attributes dust considers replacement policies pages dirty size file page replacement cost replacement pages performed global process basis finally real systems file pages cached file meta-data systems prefer evict pages files meta-data longer cached future replacement policies utilize attributes fingerprint dust identify parameters basic framework dust extended goal identifying replacement policies primary components dust size buffer cache measured simple microbenchmark input remaining steps short-term replacement algorithm fingerprinted based initial access recency access frequency access dust determines long-term history replacement algorithm microbenchmarking buffer cache size manipulate state buffer cache interpret contents dust size buffer cache information readily common interface systems dust simple microbenchmark dust accesses progressively larger amounts file data notices blocks longer fit cache increase tested size steps step dust touches file blocks newly increased size fetch buffer cache step dust probes block measuring time probe verify block cache technique similar technique determine memory now-sort important features approach probing file block step algorithm independent replacement policy manage buffer cache algorithm works buffer cache integrated virtual memory system assuming dust memory buffer cache grow maximum size show fingerprinting algorithm robust slight inaccuracies estimation buffer cache size initial accces order higher recent file position block initial access order lru queue position higher newer file position block access recency access count file position block access frequency figure short-term attributes blocks graphs show priority block test region metrics order initial access recency access frequency access x-axis block number file forming test region y-axes initial accesses order left recency access center frequency access fingerprinting replacement attributes buffer cache size dust determines attributes file blocks shortterm replacement policy fingerprinting stage involves simple steps dust reads file blocks buffer cache simultaneously controlling replacement attributes block accessing blocks initial access recency frequency orders dust forces blocks evicted buffer cache accessing additional file data finally contents buffer cache inferred probing random sets blocks cache state file blocks plotted illustrate replacement policy describe steps detail configuring attributes step moves buffer cache well-controlled state data blocks resident initial access recency frequency attributes resident block control imposed performing pattern reads blocks single file refer blocks test region ensure data resident size test region set slightly smaller estimate buffer cache size precisely estimated cache size adjust size ten stripes discussed page aligned controlling initial access parameter block dust identify replacement policies based initial access order blocks fifo exert control access pattern begins sequential scan test region resulting initial access queue ordering shown graph figure specifically blocks end file priority remain buffer cache fifo-based policy dust identify replacement policies based temporal locality lru controlling recently block accessed ensuring ordering match initial access ordering ensure criteria pattern 
reads ten stripes file performed specifically indices file maintained left pointer starts beginning file pointer starts center test region workload alternates reading stripe left pointer stripe pointer pattern continues left pointer reaches center test region pointer reaches end controlled pattern access induces recency queue order shown middle graph figure specifically blocks end left regions priority lru-based policy finally identify policies frequency based component dust ensures stripes test region distinctive frequency counts reading stripes recency ordering dust touches stripe multiple times frequency ordering pattern stripes center test region read beginning end test region read number reads area test region shown right-most graph figure blocks middle priority lfu-based policy impose frequencies parts file part motivation dividing test region fixed number stripes instance block test region frequency count runtime dust exponential size file simulation experiments determined ten good number stripes precise fingerprint greater variety frequency recency regimes greater number stripes makes stripe smaller making data susceptible noise forcing evictions state buffer cache configured dust performs eviction scan file data read portion test region evicted cache goal evicting pages give information ability differentiate replacement policies dust evict approximately half cached data note eviction scan read page multiple times frequency counts pages higher pages test region dust identify frequencybased replacement policies eviction region replace pages illustrates limitations approach differentiate lifo mru mfu replacement policies replace eviction region feel limitation acceptable policies streaming large files tend behave similarly conditions probing file-buffer contents determine state buffer cache eviction scan perform probes measuring time read byte selected pages read call returns quickly assume block file resident cache read returns slowly assume disk access required noted perform probe block determine state state buffer cache specifically dust probes block disk block replace block previously buffer cache changing state perform probes selectively obtain number samples probe stripe times total twenty probes probes spaced evenly test region location chosen randomly half stripe keeping probes ensure interfere probe due prefetching choosing random offset probes run benchmark multiple times generate picture cache state running dust multiple times platform accurately determine cache replacement policy chooses victim pages based initial access recency access frequency access precisely size eviction scan set equal difference size cache size test region cache size half size cache hot cold evict evict figure access pattern fingerprint history distinct regions file blocks hot cold evict evict accessed set attributes evictions order toidentify history replacement algorithm arrow region accessed reads time move page width arrow number shows number times block read set frequency attributes fingerprinting history fingerprinting tool identify replacement policies single queue ranking blocks based attributes previous step controls short-term attributes blocks identify algorithms track blocks longer memory track recency block lru-k determine long-term tracking performed dust observes preference pages referenced evicted describe long-term history identified shown figure regions file blocks accessed test region divided separate regions half total cache size hot cold portion algorithm begins touching hot pages evicting touching evict region evict region sufficient blocks fill buffer cache hot pages longer cache historical information tracked dust touches hot cold regions times touches cold times point evict evicted cold preferred initial access recency frequency attributes replacement policy cold touched cold region preferred traditional lru lfu hot retouched additional hot region preference policies history step prior eviction rereference hot cold regions sequentially notice point hot region touched number times cold region touched migrated long-term queue lrucache cold region short-term fingerprint phase dust probe test region determine blocks read time microseconds file position simulated first-in first-out fingerprint read time microseconds file position simulated recently fingerprint read time microseconds file position simulated frequently fingerprint figure fingerprints basic replacement policies fifo lru lfu graphs show time required probe blocks test region file depending buffer cache replacement policy x-axis shows offset probed block y-axis shows time required probe low times block cache high times block cache left graphs simulate fifo lru lfu file cache hot region remains cache infer history cold region remains cache infer history identification history attributes specific replacement algorithm focus simple historical fingerprint simulation fingerprints illustrate ability dust accurately fingerprint variety cache replacement policies implemented simple buffer cache simulator section describe simulation framework present number results simulation results verify distinctive short-term replacement fingerprints produced pure replacement policies fifo lru lfu simple replacement policies random segmented fifo explore impact internal state replacement policy investigate clock two-handed clock demonstrate ability identify historical information replacement policy focusing lru-k conclude section showing dust robust inaccuracy estimate buffer-cache size simulation methodology simulator meant illustrate ability dust identify buffer cache replacement policies rest system simple specifically assume process running fingerprinting software ignore irregularities due scheduling interference model buffer cache fixed size contention virtual memory system simulations model buffer cache approximately pages finally assume reads hit file cache require constant time reads disk require basic replacement policies begin showing simulation results strict fifo lru lfu replacement policies precisely matches derive ordering graphs shown figure fingerprints simulations shown figure show dust identify random replacement segmented fifo fingerprints shown figure graphs observe levels probe times blocks cache verify approximately half test data remains cache examine basic policies turn fifo fingerprint shows half test region remains cache matches initial access ordering shown figure blocks end file priority lru fingerprint shows roughly quarter fourth quarter test region remains buffer cache expected behavior blocks accessed recently finally lfu fingerprint shows middle half file remains resident expected blocks highest frequency counts lfu fingerprint small discontinuous regions remain cache left main in-cache area behavior due fact stripe blocks frequency count in-cache regions part stripe beginning evicted fingerprinting random replacement policy stresses importance running dust multiple times single fingerprint run twenty probes exists probability random replacement behaves identically fifo lru lfu fingerprinting system times definitively random pages selected replacement illusread time microseconds file position simulated random fingerprint read time microseconds file position simulated segmented fifo fingerprint read time microseconds file position simulated segmented fifo fingerprint figure fingerprints random segmented fifo left-most graph shows random page replacement policy distinctive fingerprint run fingerprint pages evicted buffer cache middle graph shows segmented fifo buffer cache devoted secondary queue resulting fingerprint cyclic shift 
fifo fingerprint right-most graph shows segmented fifo buffer cache devoted secondary queue queue managed lru fingerprint identical lru trated graph figure horizontal lines indicating fast slow access times original vms system implemented segmented fifo sfifo page replacement policy sfifo divides buffer cache queues primary queue managed fifo non-resident pages faulted primary queue page evicted primary queue moved secondary queue page accessed secondary queue moves back primary queue key parameter sfifo fraction buffer cache devoted secondary queue denoted fraction devoted primary queue traditional choice fingerprinted middle graph figure resulting sfifo fingerprint cyclic shift pure fifo fingerprint reason pattern initial read test area sets contents primary secondary queues pages accessed left portion test area shifted secondary queue tail primary queue portion head primary queue pages touched set recency frequency attributes left portion test area moved back head primary queue portion shifted secondary queue end primary queue blocks evicted portion evicted blocks left portion queue sizes sfifo produces distinctive fingerprint uniquely identify policy asa increases sfifo behaves lru fingerprint identical lru shown figure secondary queue large time page touched time progressed secondary queue fingerprint reveals lru behavior policy matches lru fingerprint feel segmented fifo approximate lru high acceptable fingerprint distinguished lru replacement policies initial state clock replacement algorithm popular approach managing unified file virtual memory caches modern operating systems ability approximate lru replacement simpler implementation clock algorithm interesting policy fingerprint pieces internal initial state initial position clock hand bit set ensure clock identified fingerprint initial state describe small modifications methodology guarantee behavior basic implementation clock buffer cache viewed circular buffer starting current position clock hand single bit page frame page accessed bit set replacement needed clock hand cycles page frames frame cleared bit clearing bits inspects frame clock approximates lru replacing pages bit set accessed time clock treats buffer cache circular initial position clock hand affect current fingerprint initial position clock hand simply determines block test region subsequent actions relative initial position positionis transparent dust modify fingerprinting methodology account hand position state bits impact fingerprint depending fraction set bits clock fingerprint fifo lru specifically extremes read time microseconds file position simulated clock fingerprint bits set read time microseconds file position simulated clock fingerprint bits set figure fingerprints clock replacement policy identify clock basic fingerprinting algorithm run time run bits set case clock behaves identically fifo shown graph left time run half bits set case clock fingerprint lru shown graph fingerprint fifo fingerprint lru describe intuition behavior simplest case frame starting clock hand allocated sequential pages test region result clock hand wraps back beginning buffer cache allocation dust touches page set attributes bit page set eviction pages test region replaced matching behavior fingerprint fifo policy note results identical behavior clock hand sweep frames clearing bits allocates test region sequentially left portions test region data randomly interleaved memory interleaving occurs pages allocated passes pass frames cleared bits allocated left-hand portion test region bits frames set bits remaining frames cleared pass remaining frames allocated righthand portion test region accesses set locality frequency attributes pages bits frames set eviction phase begins half pages left portions test region replaced frames set bits uniformly distributed coincidentally matches evictions lru policy distribution bits uniform fingerprint show blocks frames bits initially clear replaced case uniformly distributed consistent recognizable fingerprint identify clock dust brings initial state bits configurations observes resulting fingerprints steps configure bits dust sets bits allocating warmup region pages fills entire buffer cache touching pages intervening allocations bits set setting half bits slightly complex step set bits previous scenario step dust allocates pages warmup region bits set point clock hand pass entire buffer cache clearing bits find page evict final step randomly touch half pages setting bits dust configure state bits summary modify dust slightly account internal state running fingerprint dust allocates warmup region effect setting bits replacement policy implements resulting fingerprint fifo dust runs half bits set fingerprint fifo conclude bits underlying policy fifo fingerprint lru conclude clock underlying policy result running steps clock replacement policy shown figure replacement policies history show dust distinguish replacement policies long-term history begin briefly showing policies examined fifo lru lfu random segmented fifo clock history discuss detail behavior policies lru-k history figure shows long-term fingerprints representative policies history graph left lru fifo lfu segmented fifo identical shown graph shows results probing hot cold regions test data expected hot data evicted shown high probe times initial portion cold data evicted due size eviction region cold data preferred policies middle graph shows read time microseconds file position simulated lru history fingerprint read time microseconds file position simulated random history fingerprint read time microseconds file position simulated clock history fingerprint random initial bits figure history fingerprint short-term policies probes performed pages hot blocks left cold blocks test regions graph left shows fingerprint fifo lru lfu segmented fifo cold test region remains buffer cache policies prefer pages history graph middle shows random preference pages history history finally graph shows historical fingerprint clock ambiguous bits set bits properly set fingerprint identical leftmost graph read time microseconds file position simulated lrufingerprint correlated count read time microseconds file position simulated lrufingerprint correlated count read time microseconds file position simulated lruhistory fingerprint figure fingerprints lruthe graph shows short-term fingerprint lruwhen correlated count set case lrudisplaces pages frequency count second-to-last oldest graph shows short-term fingerprint lruwhen correlated count increased pages eviction frequency count higher evicted finally graph shows history fingerprint lruverifying prefers hot pages random preference hot cold data finally graph shows historical behavior clock difficult determine bits explicitly controlled graph bits set result hot cold regions interleaved file buffer region replaced sequentially illustrate clock history dust ensure bits cleared set initialization step history fingerprint clock identical graph figure fifo lru lfu segmented fifo random clock history making replacements lru-k replacement policy introduced database community address problem lru discriminate frequently infrequently accessed pages idea lru-k tracks -th page past replaces page oldest -th page -th traditional lru equivalent lrugiven exhibits benefits general case commonly lrufurther lruis sensitive parameter correlated period intuition accesses page period counted distinct setting correctly non-trivial task default lruis 
complex note implementation derived version provided original authors begin briefly exploring sensitivity lruto correlated period short-term fingerprints lruare shown graphs figure default resulting fingerprint variation pure lru shown left-most graph specifically stripe test region evicted lrusince stripe accessed second-to-last page initially referenced correlated period increased thata fingerprint similar lfu shown middle graph setting pages eviction region classified correlated replace pages frequency count greater read time microseconds file position simulated two-queue fingerprint read time microseconds file position simulated two-queue history fingerprint read time microseconds file position simulated secondary queue fingerprint figure fingerprints fingerprint shows short-term replacement policy fifo fingerprint shows history preferring pages accessed evicted fingerprint shows replacement policy pages main queue lru memory finally large accesses treated correlated pages second-to-last case behavior degenerates pure lru shown summary lruproduces distinctive fingerprint uniquely identifies approximate setting correlated period verify lruuses history graph figure shows historical fingerprint lruas desired hot region preference data cold region occurs second-to-last pages hot region recent second-to-last cold region replacement made hot region oldest secondto-last chosen algorithm proposed simplification lruwith run-time overhead similar performance basic intuition removing cold pages main buffer admits hot pages main buffer buffer cache divided buffers temporary queue shortterm accesses managed fifo main buffer managed lru pages initially admitted thea queue evicted reaccessed admitted structure remember pages accessed longer buffer cache experiments set buffer cache remember number past equal number pages cache show fingerprints figure graph shows short-term fingerprint identical fifo queue managed fifo short-term fingerprint access pages evicted expected result easily distinguished pure fifo observing history fingerprint shown graph historical fingerprint hot region remains buffer cache accesses moved buffer finally identify replacement policy employed long-term buffer setting initial access recency frequency attributes hot region forcing evictions methodology specific replacement policy describe detail fingerprint shown graph figure correctly identifies lru policy buffer note lruor policies history similar technique determine replacement strategy long-term queue explicitly setting state long-term queue requires knowledge policy short-term queue policy moving block queue fingerprintingtechnique long-term queue nature specific policy short-term queue sensitivity buffer size estimate set experiments verify robustness dust inaccuracies estimate size buffer cache estimate buffer cache size significantly actual resulting fingerprints identifiable estimate cache small dust touch pages force evictions occur estimate large dust evicts entire region short-term fingerprint sensitive estimate historical fingerprint short-term fingerprint observe presence absence stripes buffer cache historical fingerprint observe hot cold region half buffer cache figure shows short-term fingerprint lru distinguishable estimates real sizes replacement policies exception clock robust similar read time microseconds file position simulated lru fingerprint overestimate read time microseconds file position simulated lru fingerprint perfect estimate read time microseconds file position simulated lru fingerprint underestimate figure sensitivity lru fingerprint cache size estimate graphs show short-term fingerprints lru estimate size buffer cache varied graph estimate high graph estimate perfect graph estimate low fingerprints uniquely identify lru read time microseconds file position simulated clock fingerprint underestimate read time microseconds file position simulated clock fingerprint perfect estimate read time microseconds file position simulated clock fingerprint overestimate figure sensitivity clock fingerprint cache size estimate graphs show short-term fingerprints clock half bits set estimate size buffer cache varied clock expected lru graph estimate high graph estimate perfect graph estimate low clock fingerprint robust inaccuracies estimate algorithms degree clock replacement algorithm sensitive estimate due configure state bits specifically size warm-up region dust fill buffer cache accurate figure shows dust tolerant errors cache-size estimate identifying clock robust identifying algorithms platform fingerprints buffer caching modern operating systems complex simple replacement policies operating systems textbooks part complexity due fact filesystem buffer cache integrated virtual memory system current systems amount memory dedicated buffer cache change dynamically based current workload control effect dust minimizes amount virtual memory maximize amount memory devoted file buffer cache run dust idle system minimize disturbances competing processes section describe experience fingerprinting unix-based operating systems netbsd linux solaris fingerprints real systems variation simulations addition fingerprinting replacement policy buffer cache dust reveals cost hit versus miss buffer cache size buffer cache buffer cache integrated virtual memory system dust takes considerable amount time run real system generating sufficient number data points requires running iterations test scan eviction scan probes experiments allowed iterations found iteration seconds minutes depending system test note systems smaller buffer caches tested shorter period time test region smaller feel long running time acceptable system configuration dust run results stored made applications programmers experiments section run systems dual pentium iii-xeon processors physical ram scsi storage subsystem ultra rpm disks read time microseconds file position netbsd dust fingerprint read time microseconds file position netbsd history fingerprint figure fingerprints netbsd graph shows short-term fingerprint netbsd indicating lru replacement policy graph shows longterm fingerprint indicating history netbsd netbsd straightforward replacement policy systems examined begin fingerprint shown figure simulations examine shortterm long-term fingerprints graph figure shows expected pattern pure lru replacement dust produces fingerprint attempts manipulate bits infer netbsd implements strict lru clock conclusion verified graph figure showing netbsd history documentation inspection source code confirm finding fingerprints infer parameters specifically time reading byte page buffer cache order time disk varies machine physical memory netbsd devotes buffer cache easily shown fact history fingerprint devotes memory hot cold regions infer file buffer cache segregated system read time microseconds file position linux dust fingerprint bits cleared read time microseconds file position linux dust fingerprint random bits figure fingerprints linux graph shows short-term fingerprint linux bits set graph shows fingerprint bits untouched linux linux popular version linux kernel production environments section run nest web server top important understand fingerprint short-term fingerprint linux shown figure graph left shows results dust attempts set bits graph fifo investigate determine clock graph shows fingerprint bits left random state fingerprint noisy priority pages recently referenced pages fourth quarters filtering data verify pages quarters cache cache fingerprint similar lru fingerprint expected clockbased replacement algorithm examination source code documentation confirms replacement policy clock based finally buffer cache size close amount physical ram system conclude buffer cache integrated linux memory management system linux underwent large revision version fingerprint linux complex replacement scheme linux netbsd short-term fingerprint shown graph figure suggests linux recency frequency 
component clock graph dust shows linux history decision examination linux source code existing documentation confirms results linux maintains separate queues active inactive list memory scarce linux shrinks size buffer cache pages recently referenced bit moved active list inactive list inactive list scanned replacement victims form page aging age counter frame indicating desirable page memory scanning page evict page age decreased considered eviction page age reaches page candidate eviction age incremented page referenced solaris solaris presented greatest challenge platforms studied subsystem solaris studied believed two-handed global clock algorithm researchers noted non-intuitive behavior twohanded clock hand clears bits hand fixed distance selecting page replacement bit clear hands advanced unison bit page cleared opportunity re-referenced candidate eviction implemented simulator fingerprint two-handed clock identical fifo shown short-term fingerprint solaris shown graph figure out-of-cache areas left fingerprint strongly suggests solaris frequency aging component eviction decision addition clock graph figure shows historical fingerprint solaris data noisy shows clear preference hot region suggesting history page aging solaris fingerprint shows time service buffer cache hit significantly higher solaris linux fingerprint shows hit time read time microseconds file position linux dust fingerprint read time microseconds file position linux history fingerprint figure fingerprints linux graph shows short-term fingerprint linux indicating combination lru lfu graph shows long-term fingerprint indicating history hit time linux platform cache-aware web server section describe knowledge buffer cache replacement algorithm exploited improve performance real application modifying web server re-order accesses serve requests hit file system cache serve miss idea handling requests non-fifo service order similar introduced connection scheduling web servers work scheduled requests based size request schedule based predicted cache content re-ordering based cache content lowers average response time emulating shortest-job scheduling discipline improves throughput reducing total disk traffic approach key challenge implementing cache-aware server gray-box knowledge file caching algorithm determine files cache keeping track file access stream presented kernel web server simulate read time microseconds file position solaris dust fingerprint read time microseconds file position solaris history fingerprint figure fingerprint solaris graph shows short-term fingerprint solaris graph shows history fingerprint operating system buffer cache predict time data cache term algorithmic mirroring general powerful manner exploit gray-box knowledge important assumption algorithmic mirroring application induces traffic file system mirror cache accurately represent state real cache assumption hold general case multi-application environment feasible single application dominates filesystem activity server applications web server database management system perfect match mirroring methods nest storage appliance supports http access protocols nest configurable number requests serviced simultaneously requests received number queued pending requests completes default nest services queued requests fifo order term default behavior cache-oblivious nest modified nest request scheduler model current state buffer cache model updated time request scheduled nest bases model underlying file cache algorithm exposed dust nest model reorder requests requests files believed cache serviced note nest perform caching files relies strictly buffer cache cache mirror accurately reflect internal state nest reasonable estimate cache size current approach nest static estimate produced dust disadvantage approach estimate produced contention virtual memory system larger amount web server running increase robustness estimate plan modify nest dynamically estimate size buffer cache measuring time file access time low file cache high file disk comparing timings prediction provided mirror cache nest adjust size mirror cache performance evaluate performance benefits cache-aware scheduling compare performance cacheaware nest cache-oblivious nest workloads tests web server run dual pentium iii-xeon machine main memory ultra disks clients machines identical server main memory running client threads clients connected server gigabit ethernet server clients running linux shown section clock replacement algorithm cache-aware nest configured model clock algorithm configuration server approximately memory dedicated buffer cache experiments explore performance cache-aware nest vary estimate size buffer cache experiment workload client thread repeatedly requests random file set files figures show average response time throughput web servers apache web server cache-oblivious nest cache-aware nest function estimate cache-size begin comparing response time throughput nest apache figures nest incurs overhead flexible structure nest handle multiple transfer protocols ftp nfs achieves respectable performance web server reasonable platform studying cache-aware scheduling importantly adding cache-aware scheduling signifaverage response time cache size estimate cache-aware scheduling working set cache-oblivious apache cache-aware figure response time function cache size estimate response time cache-aware nest lowest estimate cache size closest true size cache icantly improves response time throughput nest servicing requests hit cache cache-aware scheduling improves average response time servicing short requests dramatically cache-aware scheduling improves throughput reducing number disk reads verified proc interface in-cache requests handled data evicted cache finally performance cache-aware nest improves estimate cache size closer real robust large range cache size estimates experiment workload created surge http workload generator surge workload approximately distinct files sizes zipf distribution approximately surge representative web workload presented surge workload measure qualitatively similar results main differences performance cache-oblivious nest relative apache degrades slightly average response time cache-oblivious nest seconds apache seconds result expected nest designed staging data grid optimized large files small files typical web workloads performance cache-aware nest sensitive estimate cache size performance improves approximately cache size estimate improved apache achieves future plan experiment web servers workloads related work idea algorithmic knowledge underlying operating system improve performance throughput cache size estimate cache-aware scheduling working set cache-aware apache cache-oblivious figure sensitivity cache estimate accuracy performance cache aware nest improves estimate cache size approaches true size buffer cache buffer cache approximately cache-oblivious nest apache shown comparison recently explored context gray-box systems work showed os-like service implemented information control layer icl algorithmic knowledge probes statistical analysis concrete solutions proposed developers icls obtain algorithmic knowledge paper show fingerprinting obtain graybox knowledge simple automatic manner fingerprinting system components determine behavior successfully contexts notably networking storage specifically fingerprinting uncover key parameters tcp protocol identify remote host primary difference fingerprinting tcp context identify policies arbitrary behavior implementations expected adhere specifications techniques similar dust determine characteristics disks size prefetch window prefetching algorithm caching policy fingerprinting shares common microbenchmarking specifically perform requests underlying system order characterize behavior simple probes microbenchmarks determine parameters memory hierarchy processor cycle time characteristics disk geometry view key difference fingerprinting microbenchmarking fingerprint discover policy algorithm employed underlying layer microbenchmark typically uncover specific system parameters idea discovering 
characteristics lower layers system knowledge higher layers improve performance traxtents file system layer operating system modified avoid crossing disk track boundaries minimize cost incurred due head switching exploit zero-latency access developed method predicting position disk head hardware support information determine rotational replicas service request giving software expanded knowledge hardware state approach involves informing application buffer cache replacement policy operating system sleds dynamic sets seek increase knowledge application operating system approach embellishing interface application explicit exchange types information case dynamic sets application ability provide knowledge future access patterns reorder fetching data improve cache performance sleds export performance data application enabling application modify workload based performance characteristics underlying system idea servicing requests web server order explored connection-scheduling web servers main thesis research performance obtained controlling scheduling requests web server approach static file size schedule requests cache-aware nest dynamic estimate contents buffer cache future work hope investigate interactions scheduling requests based file size cache content cache-aware web server similarities locality-aware request distribution lard clusterbased web servers lard front-end node directs page requests specific back-end node based back-end recently served page modulo load-balancing constraints front-end simple model cache contents backend improve cache hit rates approaches complementary lard partitions requests nodes cache content service requests order single node conclusions future work shown buffer cache replacement algorithms uniquely identified simple fingerprint fingerprinting tool dust classifies algorithms based initial access locality frequency history choosing block replace simple simulator shown fifo lru lfu clock random segmented fifo lru-k produce distinctive fingerprints allowing uniquely identified begun address challenging problem fingerprinting real systems running dust netbsd linux solaris shown determine attributes considered page replacement algorithm finally shown algorithmic knowledge revealed dust predicting contents file cache specifically implemented cache-aware web server services requests predicted hit file cache improving response time bandwidth future extend range policies dust recognize specifically adaptive policies eelru lrfu identified policies attributes size page cost replacing page current system visually interpret fingerprint graphs produced dust automate process well-known replacement policies long-term plan continue exploring fingerprinting subsystems cpu scheduler determine algorithmic knowledge user processes main challenge performing model simulation access inputs required accuracy finally investigating algorithmic knowledge infer contents file cache change contents acknowledgments brian forney tim denehy muthian sivathanu florentina popovici helpful discussion comments paper shepherd greg ganger anonymous reviewers helpful comments work sponsored nsf ccrngs- ccrccr- itrand wisconsin alumni research foundation apache foundation apache web server http apache arpaci culler krishnamurthy steinberg yelick empirical evaluation cray-t compiler perspective annual international symposium computer architecture iscapages santa margherita ligure italy june arpaci-dusseau arpaci-dusseau information control gray-box systems symposium operating systems principles sosp october arpaci-dusseau arpaci-dusseau culler hellerstein patterson high-performance sorting networks workstations sigmod tucson barford crovella generating representative web workloads network server performance evaluation proceedings sigmetrics conference june bent venkataramani leroy roy stanley arpaci-dusseau arpaci-dusseau livny flexibility manageability performance grid storage appliance hpdcj bertoni understanding solaris filesystems paging technical report tr- sun microsystems cao felten implementation performance application-controlled file caching proceedings symposium operating systems design implementation pages crovella frangioso harchol-balter connection scheduling web servers usenix symposium internet technologies systems forney arpaci-dusseau arpaci-dusseau storage-aware caching revisiting caching heterogeneous storage systems usenix symposium file storage technologies fast monterey january glaser tcp stack fingerprinting principles http sans newlook resources idfaq tcp fingerprinting htm october johnson shasha low overhead high performance buffer management replacement algorithm proceedings international conference large databases pages september lee choi kim noh min cho kim existence spectrum policies subsumes recently lru frequently lfu policies sigmetrics atlanta georgia levy lipman virtual memory management vax vms operating system ieee computer march linux kernel archives linux source code http kernel mckusick bostic karels quarterman design implementation bsd operating system addison wesley netbsd kernel archives netbsd source code http netbsd nicola dan dias analysis generalized clock buffer replacement scheme database transaction processing sigmetrics performance neil neil weikum lru-k page replacement algorithm database disk buffering proceedings acm sigmod conference pages neil lrusource code ftp ftp umb pub lru-k lruk tar padhye floyd identifying tcp behavior web servers sigcomm june pai aron banga svendsen druschel zwaenepoel nahum locality-aware request distribution cluster-based network servers eighth international conference architectural support programming languages operating systems san jose california robinson devarakonda data cache management frequency-based replacement proceedings acm sigmetrics conference measurement modeling computer systems pages saavedra smith measuring cache tlb performance effect benchmark runtimes ieee transactions computers schindler ganger automated disk drive characterization technical report cmu-cs- carnegie mellon schindler griffin lumb ganger trackaligned extents matching access patterns disk drive characteristics proceedings usenix conference file storage technologies fast monterey smaragdakis kaplan wilson eelru simple effective adaptive page replacement sigmetrics conference measurement modeling computer systems atlanta staelin mcvoy mhz anatomy micro-benchmark proceedings usenix annual technical conference pages berkeley june steere exploiting non-determinism asynchronyof set iterators reduce aggregate file latency proceedings acm symposium operating systems principles sosp pages saint-malo france october talagala arpaci-dusseau patterson microbenchmark-based extraction local global disk characteristics technical report csd- california berkeley turner levy segmented fifo page replacement acm sigmetrics international conference measurement modeling computer systems vahalia unix internals frontiers prentice hall van meter gao latency management storage systems proceedings fourth symposium operating systems design implementation osdi october van riel page replacement linux memory management http surriel lectures linux -vm html june worthington ganger patt wilkes online extraction scsi disk drive parameters proceedings acm sigmetrics performance conference measurementand modeling computer systems pages gum chen wang krishnamurthy anderson trading capacity performance disk array proceedings fourth symposium operating systems design implementation osdi san diego 
